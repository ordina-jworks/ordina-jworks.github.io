<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://ordina-jworks.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ordina-jworks.github.io/" rel="alternate" type="text/html" /><updated>2023-06-29T07:37:23+00:00</updated><id>https://ordina-jworks.github.io/feed.xml</id><title type="html">Ordina JWorks Tech Blog</title><subtitle>We build innovative solutions with Java and JavaScript. To support this mission, we have several Competence Centers. From within those Competence Centers, we provide coaching to the employee and expert advice towards our customer. In order to keep in sync with the latest technologies and the latest trends, we frequently visit conferences around the globe.
</subtitle><entry><title type="html">Cloud Kickstart Components</title><link href="https://ordina-jworks.github.io/cloud/2023/06/14/cloud-kickstart-components.html" rel="alternate" type="text/html" title="Cloud Kickstart Components" /><published>2023-06-14T00:00:00+00:00</published><updated>2023-06-14T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/cloud/2023/06/14/cloud-kickstart-components</id><content type="html" xml:base="https://ordina-jworks.github.io/cloud/2023/06/14/cloud-kickstart-components.html"><![CDATA[<h1 id="table-of-contents">Table of contents</h1>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#architecture">Architecture</a></li>
  <li><a href="#docker">Docker</a></li>
  <li><a href="#github-actions">GitHub Actions</a></li>
  <li><a href="#aws">Amazon Web Services(AWS)</a></li>
  <li><a href="#amazon-ecr">Amazon ECR</a></li>
  <li><a href="#amazon-eks">Amazon EKS</a></li>
  <li><a href="#amazon-cloudwatch">Amazon CloudWatch</a>
    <ul>
      <li><a href="#cloudwatch-dashboard">CloudWatch Dashboard</a></li>
      <li><a href="#cloudwatch-logs">CloudWatch Logs</a></li>
      <li><a href="#cloudwatch-alarms">CloudWatch Alarms</a></li>
      <li><a href="#cloudwatch-container-insights">CloudWatch Container Insights</a></li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>Introducing Cloud Kickstart Components: Simplifying Application Deployment on AWS (Internship Project)</p>

<p>We are excited to introduce Cloud Kickstart Components, an internship project aimed at simplifying the process of deploying applications on Amazon Web Services (AWS). As interns, we have developed a template that enables the automatic deployment of applications to AWS, harnessing the power of multiple AWS services effortlessly.</p>

<p>Our internship project focuses on automating the deployment process, eliminating the need for manual configurations and reducing the chances of errors. With this template, developers can seamlessly deploy their applications to AWS, allowing them to concentrate on their core application logic and development tasks.</p>

<p>Through Cloud Kickstart Components, we provide an easy-to-use template that integrates with various AWS services, including EKS and ECR instances, Amazon CloudWatch, Amazon S3 for storage and Amazon IAM. This integration empowers developers to take advantage of multiple AWS services without individual setup and configuration complexities.</p>

<h1 id="architecture">Architecture</h1>

<p>The first step in our deployment process is for the developer to push their code to the designated repository, such as GitHub. This ensures that the latest changes and updates are available for deployment. Once the code is pushed, our deployment pipeline kicks into action. The Spring Boot application is built, packaged, and transformed into a Docker image. This image encapsulates the application and its dependencies, making it portable and ready for deployment. The Docker image is then stored in a Docker registry, such as Docker Hub. This step ensures the image is prepared and available for deployment across different environments.</p>

<p>We use the power of GitHub Actions, a workflow automation tool, to streamline the deployment process. Using predefined workflows, we configure GitHub Actions to automatically trigger the deployment process whenever a new Docker image is available. GitHub Actions pulls the Docker image from the registry, fetching the latest version of the Spring Boot application built in the previous steps. This ensures that the deployment uses the most up-to-date version of the application.</p>

<p>With the Docker image available, we utilize AWS services to deploy the application. Depending on the specific requirements, it can automatically provision resources such as Amazon Elastic Kubernetes Service (EKS) instances, Amazon Elastic Container Registry (ECR), Amazon Simple Storage Service (S3) buckets and many more. After deployment, we implement continuous monitoring and testing mechanisms using CloudWatch. We set up custom metrics, create dashboards for visualization, and define alarms to detect anomalies or performance issues. Additionally, we leverage CloudWatch Logs to collect application logs for troubleshooting and analysis.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img.png" style="margin:0px auto; max-width: 750px;" /></p>

<h1 id="docker">Docker</h1>

<p>Docker Hub is a central repository allowing us to store, manage, and distribute our Docker images. At the same time, AWS provides the ideal platform for deploying and running our containerized applications.
To begin the deployment process, we build and package our application code into a Docker image, encapsulating all the necessary dependencies and configurations. 
This Docker image acts as a self-contained unit, ensuring consistent deployment across various environments. We leverage the power of Docker Hub, a trusted and widely used container registry, to store and manage our Docker images. 
By utilizing Docker Hub, we can easily version our images, making tracking and managing changes over time simple.
This ensures that our Docker images are always up to date, incorporating the latest changes and enhancements. Once our Docker images are prepared on Docker Hub, we will deploy them to AWS.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_1.png" style="margin:0px auto; max-width: 750px;" /></p>

<h1 id="github-actions">GitHub Actions</h1>

<p>We use the capabilities of GitHub Actions, a workflow automation tool. We have automated various tasks with GitHub Actions, including deploying to Docker and seamlessly integrating our applications with multiple Amazon Web Services (AWS).</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_2.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>One of the key benefits of GitHub Actions is its ability to automate the deployment of applications using Docker images. Using a simple configuration, we have set up workflows that automatically build our applications, package them into Docker images, and push those images to a Docker registry. 
This automation saves us valuable time and effort, ensuring our applications are always up-to-date and readily available for deployment.</p>

<p>This automation allows us to seamlessly deploy our applications, configure the necessary settings, and utilize the full capabilities of AWS without manual intervention. For example, when triggering a deployment workflow, GitHub Actions pulls the Docker image from the registry, distributes it to ECR and deploys it to an Elastic Kubernetes Service (EKS) cluster. Simultaneously, it can create S3 buckets for storage, create/add some CloudWatch metrics, and set up the alarms and necessary permissions and configurations, all in an automated and reliable manner.
This level of automation significantly reduces the complexity and time required to deploy and integrate our applications with AWS services.</p>

<p>In conclusion, GitHub Actions has become an invaluable tool, empowering us to automate the deployment to Docker and seamlessly utilize multiple AWS services.</p>

<h1 id="amazon-web-services-aws">Amazon Web Services (AWS)</h1>

<p>We have chosen Amazon Web Services (AWS) as our preferred cloud computing platform. With AWS, we have access to a comprehensive suite of cloud services that enable us to build, deploy, and manage our applications and infrastructure with flexibility, scalability, and reliability.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_3.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>By leveraging AWS as our cloud computing platform, we can take advantage of a vast array of services and features that enable us to build and scale our applications efficiently. 
AWS’s flexibility, scalability, and reliability empower us to focus on innovation and deliver exceptional experiences to our users while benefiting from the robust infrastructure and services AWS provides.</p>

<h1 id="amazon-ecr">Amazon ECR</h1>

<p>We utilize AWS Elastic Container Registry (ECR) as a pivotal component in our deployment and containerization strategy. AWS ECR is a secure and fully managed container registry, enabling us to store, manage, and deploy container images effortlessly. With AWS ECR, we can securely store our Docker container images, ensuring their availability for deployment across various environments.
The integration of ECR within the AWS ecosystem allows us to seamlessly incorporate it into our deployment pipelines, simplifying the process of deploying containerized applications.</p>

<p>Moreover, AWS ECR provides powerful monitoring and management capabilities. We can track image usage, monitor repository activity, and gain insights into resource utilization through integration with AWS CloudWatch. 
This allows us to monitor the performance of our container images and repositories, enabling proactive management and optimization.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_4.png" style="margin:0px auto; max-width: 750px;" /></p>

<h1 id="amazon-eks">Amazon EKS</h1>

<p>To deploy these containers effectively, we have adopted Amazon Elastic Kubernetes Service (EKS) from Amazon Web Services (AWS). 
AWS EKS provides a robust and reliable platform to deploy, scale, and easily manage our containerized applications.
AWS EKS is a fully managed Kubernetes service that simplifies the deployment and management of containerized applications. 
This allows us to streamline our development and operations processes, enabling faster time-to-market and improved agility.</p>

<p>Amazon Elastic Kubernetes Service (EKS) pod is the smallest and most basic unit of deployment within a Kubernetes cluster.
A pod represents a single or multiple instance(s) of a running application workload within the cluster. 
It encapsulates one or more tightly coupled containers that share the same network namespace, IP address, and storage volumes.
These containers within a pod often work together to fulfill a specific task or service.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_5.png" style="margin:0px auto; max-width: 750px;" /></p>

<h1 id="amazon-cloudwatch">Amazon CloudWatch</h1>

<p>AWS CloudWatch plays a vital role in our operations by providing us with real-time insights into the performance and health of our AWS resources and applications. 
With AWS CloudWatch, we can collect and analyze a wide range of metrics across various AWS services, including compute instances, databases, storage, and networking. 
This comprehensive monitoring solution allows us to gain deep visibility into the performance and utilization of our resources, enabling us to make informed decisions and optimize our infrastructure.</p>

<h3 id="1-cloudwatch-dashboard">1. CloudWatch Dashboard</h3>

<p>Additionally, AWS CloudWatch provides us with the flexibility to create customized dashboards. 
These dashboards offer a consolidated view of our key metrics, allowing us to monitor and analyze critical aspects of our infrastructure and applications in a centralized and intuitive manner.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_6.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>The example image above shows the different metrics we included, such as CPU usage, Incoming log events,…</p>

<h3 id="2-cloudwatch-logs">2. CloudWatch Logs</h3>

<p>AWS CloudWatch also supports log management and analysis through CloudWatch Logs. This feature enables us to centralize and collect logs generated by our applications and services. We can then search, filter, and analyze these logs, making troubleshooting and debugging more efficient. 
CloudWatch Logs simplifies investigating issues and monitoring application behavior by consolidating logs in a single location.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_7.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>As shown image above, different logs are being shown inside CloudWatch Logs. Every endpoints inside our application sends the following logs :</p>
<ul>
  <li>The log level</li>
  <li>The message</li>
  <li>The context</li>
  <li>The sender</li>
  <li>The IP address</li>
  <li>The endpoint</li>
</ul>

<p>Depending on the endpoint itself, the values are going to be different.</p>

<h3 id="3-cloudwatch-alarms">3. CloudWatch Alarms</h3>

<p>One of the key features we leverage in AWS CloudWatch is the ability to set up custom alarms. These alarms enable us to define specific thresholds and conditions for our metrics. 
When a metric breaches a threshold for a specific timeframe or meets a predefined condition.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_8.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>CloudWatch triggers an alarm, notifying us of potential issues or deviations from expected behavior. This proactive monitoring approach empowers us to address potential problems before they impact our applications or services. In the example image above, the CPU usage is being monitored. The red line represents the upper CPU usage limit, meaning the maximum value that can be reached.
If the CPU surpasses the maximum value indicated by the red line, as shown in the image, a notification will be sent to a specific email address.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_9.png" style="margin:0px auto; max-width: 750px;" /></p>

<h3 id="4-cloudwatch-container-insights">4. CloudWatch Container Insights</h3>

<p>Container Insights, powered by Amazon CloudWatch, offers real-time monitoring and deep visibility into the performance and health of your containers. Integrating Container Insights into our deployment template enables developers to gain valuable insights and make data-driven decisions to optimize their containerized applications. With Container Insights, you can effortlessly monitor crucial metrics such as CPU and memory utilization, network performance, disk I/O, and container-level resource allocation. This level of observability empowers you to identify performance bottlenecks, proactively troubleshoot issues, and optimize resource allocation for better efficiency. CloudWatch automatically collects metrics for many resources, such as CPU, memory, disk, and network. At the same time, Container Insights supports collecting metrics from clusters deployed on Fargate for both Amazon ECS and Amazon EKS.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_10.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>For example, we utilize Container Insights to monitor our EKS cluster .</p>

<h1 id="terraform">Terraform</h1>

<p>In our project, we use the capabilities of Terraform to perform a wide range of tasks automatically. Terraform is a powerful infrastructure as code (IaC) tool that allows us to define, provision, and manage our project resources seamlessly and efficiently. With Terraform, we can customize our infrastructure requirements and represent them in a declarative configuration language. This enables us to specify the desired state of our infrastructure, including the resources, dependencies, and configurations needed for our project.</p>

<p>One of the key advantages of using Terraform is its ability to automate the provisioning and management of resources across various cloud providers, including Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). This eliminates manual interventions, reduces human error, and ensures consistent deployments across environments.</p>

<p><img alt="architecture" src="/img/2023-06-14-cloud-kickstart-components/img_11.png" style="margin:0px auto; max-width: 750px;" /></p>

<p>The code shown above shows a snippet of terraform code inside our project. This snippet code will configure the required provider and backend settings. It ensures that the project can interact with AWS resources using the specified provider and store the state of the infrastructure in an S3 bucket.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Our cloud kickstart project has been an extraordinary journey, and we express deep appreciation for the exceptional support and guidance offered by our mentors, Pieter Vincken and Sigriet Van Breusegem. This experience has truly been transformative, allowing us to acquire invaluable knowledge and skills, significantly enhancing our proficiency in cloud computing and automation.</p>

<p>Through this project, we have developed a comprehensive understanding of various aspects, including:</p>

<ol>
  <li>Cloud infrastructure and its intricacies.</li>
  <li>Proficiency in working with Amazon Web Services (AWS), a leading cloud computing platform.</li>
  <li>Extensive knowledge and hands-on experience with multiple AWS services, enabling us to leverage their full potential.</li>
  <li>Mastery of Terraform, an essential tool for infrastructure automation and provisioning.</li>
  <li>Expertise in automation techniques, empowering us to streamline and optimize various processes.</li>
  <li>Proficiency in setting up Continuous Integration/Continuous Deployment (CI/CD) pipelines using GitHub Actions, ensuring efficient code deployment and delivery.</li>
  <li>Strong grasp of Agile methodologies, allowing us to adapt swiftly to changing requirements and deliver high-quality results.</li>
  <li>Effective communication and teamwork skills, fostering collaboration and synergy among team members.</li>
</ol>

<p>Undoubtedly, this Cloud Kickstart project has been instrumental in broadening our horizons and equipping us with the expertise needed to thrive in cloud computing and automation. We are deeply grateful for the opportunity and look forward to applying our newfound knowledge to future endeavors.</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Gabriel&quot;, &quot;last_name&quot;=&gt;&quot;Dela Peña&quot;, &quot;permalink&quot;=&gt;&quot;/author/gabriel-dela-peña/&quot;, &quot;avatar&quot;=&gt;&quot;gabriel-dela-pena.jpg&quot;, &quot;title&quot;=&gt;&quot;Java Developer&quot;, &quot;linkedin&quot;=&gt;&quot;gabriel-dela-peña-4abb09223&quot;, &quot;github&quot;=&gt;&quot;GabrielDelaPena&quot;, &quot;email&quot;=&gt;&quot;Gabriel.DelaPena@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Gabriel is a  Developer with a strong interest in back-end and front-end.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2023-06-14-cloud-kickstart-components.md collection=posts&gt;]}</name><email>Gabriel.DelaPena@ordina.be</email></author><category term="Cloud" /><category term="cloud" /><category term="aws" /><category term="internship" /><category term="kubernetes" /><category term="terraform" /><category term="docker" /><summary type="html"><![CDATA[Table of contents]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-06-14-cloud-kickstart-components/cloud-kickstart-component-banner.jpg" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-06-14-cloud-kickstart-components/cloud-kickstart-component-banner.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Secure cloud foundation tooling</title><link href="https://ordina-jworks.github.io/cloud/2023/06/07/secure-cloud-foundation.html" rel="alternate" type="text/html" title="Secure cloud foundation tooling" /><published>2023-06-07T00:00:00+00:00</published><updated>2023-06-07T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/cloud/2023/06/07/secure-cloud-foundation</id><content type="html" xml:base="https://ordina-jworks.github.io/cloud/2023/06/07/secure-cloud-foundation.html"><![CDATA[<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#the-warden-open-policy-agent">The Warden: Open Policy Agent</a></li>
  <li><a href="#the-janitor-cloud-custodian">The Janitor: Cloud Custodian</a></li>
  <li><a href="#the-watchdog-snyk">The Watchdog: Snyk</a></li>
  <li><a href="#the-container-detective-trivy">The Container Detective: Trivy</a></li>
  <li><a href="#the-composer-fugue">The Composer: Fugue</a></li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>If you ever came in contact with an enterprise cloud environment,
you know that keeping it secure and compliant can be a challenging task.</p>

<p>Every company has their own security and compliance requirements,
and they should at least follow a best practice standard like CIS AWS Foundations Benchmark.</p>

<p>Often, you’ll find that because of the segmented responsibilities across many teams and projects,
gaps exist in the security and compliance posture of the organization.
Add to that the rate of change that companies need to handle to stay competitive,
and you end up with a puzzle that no-one can oversee.</p>

<p>Originally, we started investigating solutions to scan your infrastructure as code and cloud environment.
We thought we’d look at a handful of tools, evaluate them and select a clear winner.</p>

<p>We quickly discovered there are quite some interesting tools out there.
Some that are free and some that cost you a lot of money,
and some not only do cloud, but can do much more.</p>

<p>We’ll be summarizing the tools we found and how they can help you to secure your cloud environment or other resources.
What they can do, what they can’t do, what you can use them with and for.</p>

<h2 id="the-warden-open-policy-agent">The Warden: Open Policy Agent</h2>

<p>Cloud computing has made it easy for organizations to manage their IT infrastructure on a large scale. However, with the ease of cloud computing comes the challenge of securing the environment and preventing accidental misconfigurations. This is where <a href="https://www.openpolicyagent.org/" target="_blank" rel="noopener noreferrer">Open Policy Agent</a> (OPA) comes into play.</p>

<p><img src="/img/2023-02-16-secure-cloud-foundation/warden.png" alt="Gandalf, You shall not pass" class="image fit" style="margin:0px auto; max-width:60%" /></p>

<p>OPA is an open-source, general-purpose policy engine that can be used to enforce policies across various systems, including cloud infrastructure.</p>

<p>Lets look at some of the strengths and advantages that OPA has to offer.</p>

<h3 id="policy-management">Policy Management</h3>

<p>OPA provides a simple and flexible policy language used by
writing <a href="https://www.openpolicyagent.org/docs/latest/policy-language/" target="_blank" rel="noopener
noreferrer">Rego</a>.
That allows organizations to define and manage policies across multiple cloud platforms.
Policies can be created to enforce security controls, compliance, and cost optimization rules.
The policies can be customized to meet specific operational needs.
These policies are available as code and can be managed like any other code, including automated testing in an
organisation.</p>

<h3 id="automated-compliance-enforcement">Automated Compliance Enforcement</h3>

<p>OPA can be integrated into the cloud environment using various methods, such as by deploying it as a sidecar container in a Kubernetes cluster or by using Google Cloud functions, <a href="https://aws.amazon.com/blogs/opensource/easily-running-open-policy-agent-serverless-with-aws-lambda-and-amazon-api-gateway/" target="_blank" rel="noopener noreferrer">AWS Lambda</a> or Azure Automation to run OPA policies. 
Once integrated, OPA can provide real-time policy evaluation and enforcement across the AWS environment, enabling organizations to maintain compliance and security posture.</p>

<h3 id="integrations">Integrations</h3>

<p>OPA can integrate with various cloud native tools, including Kubernetes, Istio, and Envoy. This allows organizations to extend their policy management across various systems, helps to ensure end-to-end compliance, making it easier to enforce policies across multiple platforms and cover broad ecosystems.</p>

<h2 id="the-janitor-cloud-custodian">The Janitor: Cloud Custodian</h2>

<p><a href="https://cloudcustodian.io/" target="_blank" rel="noopener noreferrer">Cloud Custodian</a> is an open-source tool that helps organizations manage their cloud infrastructure in a secure and compliant way. One of the key features of Cloud Custodian is its ability to scan cloud resources across multiple cloud platforms, including Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).</p>

<p><img src="/img/2023-02-16-secure-cloud-foundation/janitor.png" alt="Gandalf, You shall not pass" class="image fit" style="margin:0px auto; max-width:60%; height:60%;" /></p>

<p>Cloud Custodian uses a policy-driven approach to scan cloud resources. Policies are written using a simple and flexible policy language that allows organizations to define and manage policies.</p>

<p>Following are some features with which Cloud Custodian can make governance and compliance easier.</p>

<h3 id="automated-remediation">Automated Remediation</h3>

<p>Cloud Custodian not only identifies policy violations but also automates the remediation process. For example, if a policy violation is identified, such as an unsecured storage bucket in AWS, Cloud Custodian can automatically take corrective actions, such as deleting the unsecured bucket or encrypting it.</p>

<h3 id="continuous-compliance">Continuous Compliance</h3>

<p>Cloud Custodian helps organizations maintain continuous compliance by ensuring that policies are enforced at all times. The tool can detect any changes in the cloud infrastructure that may violate the policies and take corrective actions in real-time.</p>

<h3 id="cost-optimization">Cost Optimization</h3>

<p>Cloud Custodian also helps organizations optimize cloud costs by automating the deletion of unused resources, enforcing tagging policies to identify unused resources, and providing reports on cost savings.</p>

<h2 id="the-watchdog-snyk">The Watchdog: Snyk</h2>

<p><a href="https://snyk.io/" target="_blank" rel="noopener noreferrer">Snyk</a> is a cloud security platform that helps you to get end-to-end insight into your security footprint.</p>

<p><img src="/img/2023-02-16-secure-cloud-foundation/snyk.png" alt="Gandalf, You shall not pass" class="image fit" style="margin:0px auto; max-width:60%; height:60%" /></p>

<h3 id="code-scanning">Code scanning</h3>

<p>Snyk can scan your code for vulnerabilities and compliance issues,
by example security issues like sql injection or path traversal vulnerabilities.</p>

<p>You might then ask: “How can it help me solve them?”
They have a data flow window that shows you the entry point and method invocation of the vulnerability, which shows you
the entire stack path to the vulnerability in your source code.
This helps you in assessing what the impact of a vulnerability is and how urgent you need to provide a patch for it.
This way you can, for example, determine if a method is publicly accessible or not.</p>

<p>To help you further with solving the vulnerability, you can read the details of the vulnerability
and the best practice for preventing it, if available.
If you thought that would be enough, they have another tab that shows 3 open source projects that had the vulnerability
and how they fixed it in their code base.</p>

<h3 id="license-scanning">License Scanning</h3>

<p>You can configure Snyk to scan your open source dependencies for license issues with your dependencies,
which can be useful, for example, if your company wants to avoid using dependencies with a certain license,
because they want to commercialize the software in the future.
For example, a library that uses patents,
but its software rights don’t include that you may use their patents when using their library.
The company might for various reasons not allow AGPLv3 libraries, for example, because of various reasons.</p>

<h3 id="container-scanning">Container scanning</h3>

<p>Snyk can scan your container images for vulnerabilities it can do this from docker images, Amazon ECR, Docker hub, …
You can set this up in your Kubernetes cluster, but currently Fargate is not supported.</p>

<p>You can automate the image updating process by using container scanning in your Git repository.
By using this method, Snyk can automatically create pull requests for you that you can test and then merge if satisfied.
This reduces your effort to stay safe and up to date.</p>

<h3 id="infrastructure-as-code-scanning">Infrastructure as code scanning</h3>

<p>Snyk can scan your infrastructure as code against the CIS AWS Foundations Benchmark or you can write custom policies.
To scan your IaC, you simply have to add your Git repository that contains your IaC
and the Snyk platform will start scanning if for you.
You can also use the Snyk CLI to scan your IaC if you want to make it part of your CI/CD pipelines.
By using it this way you can make this a requirement before pull requests are merged that you pass the CLI tool’s scan
or even block deployment to environments.</p>

<h3 id="custom-policies">Custom policies</h3>

<p>Snyk allows you to write custom policies in Rego, but only for IaC scanning and platform policies.</p>

<p>OPA is easy to use because applications can easily delegate policy validation to OPA if needed.
Snyk IaC leverages OPA to do its policy scanning, according to one of their blogposts.
You get a preset of policies out the box from Snyk, and you can add your own custom policies written in Rego.</p>

<h2 id="the-container-detective-trivy">The Container Detective: Trivy</h2>

<p><a href="https://trivy.dev/" target="_blank" rel="noopener noreferrer">Trivy</a> is an open-source cli tool provided by Aqua
Security.</p>

<p><img src="/img/2023-02-16-secure-cloud-foundation/Trivy.png" alt="Gandalf, You shall not pass" class="image fit" style="margin:0px auto; max-width:60%; height:60%" /></p>

<h3 id="container-scanning-1">Container scanning</h3>

<p>Trivy can scan container images against well-known vulnerabilities.
Trivy will scan files inside container images and container image metadata.</p>

<p>Trivy scans the files inside container images for:</p>

<ul>
  <li>Vulnerabilities</li>
  <li>Misconfigurations</li>
  <li>Secrets</li>
  <li>Licenses</li>
</ul>

<p>The image metadata will be scanned for:</p>

<ul>
  <li>Misconfigurations</li>
  <li>Secrets</li>
</ul>

<h3 id="dependency-scanning">Dependency scanning</h3>

<p>Trivy can scan your dependencies for well known vulnerabilities.</p>

<p>It has a mode that automatically discovers, declarations files for various package managers.
This dependency scanning is very powerful it scans the file system for typical files used to declare dependencies,
like a <code class="language-plaintext highlighter-rouge">pom.xml</code>, but can also scan into jar and war files.</p>

<p>If you thought “that’s nice”, that’s not all it can do!
It can also scan your linux systems package managers by fetching what packages are installed,
apt and apk are supported by default for alpine and ubuntu based images.</p>

<h3 id="cicd-integration">CI/CD integration</h3>

<p>Because Trivy is a CLI tool, it can easily be integrated in new or existing CI/CD pipelines.
To integrate it into GitHub, you could tell trivy the run should fail (exit code 1 instead of 0) only for HIGH and
Critical issues.</p>

<p>Trivy also maintains a <a href="https://github.com/aquasecurity/trivy-action">GitHub action</a> to integrate it in GitHub actions.
But the community has created 2 additional GitHub actions.</p>

<p>This action has some examples of how you can integrate this with GitHub Advanced Security.</p>

<h3 id="aws-integration">AWS integration</h3>

<p>Trivy can be run locally to scan your AWS environment using the AWS CLI.
The default included check scans against AWS CIS 1.2.0 benchmark.
It shows summarizes a lists of issues, and gives description of how to resolve the issue; it won’t automatically fix it.</p>

<p>The benefit compared to AWS security hub is that here you can stop the issue from being created before merge or deploy.
While security hub would tell you after the resource already exists in AWS.</p>

<h3 id="secret-scanning">Secret scanning</h3>

<p>Trivy can scan your code for secrets,
because it’s not like you have ever had a developer push your precious AWS access key.
(I really wonder why we suddenly have EC2s with GPU’s booting up?)
It can scan for:</p>

<ul>
  <li>AWS access key</li>
  <li>GCP service account</li>
  <li>GitHub personal access token</li>
  <li>GitLab personal access token</li>
  <li>Slack access token</li>
  <li>etc.</li>
</ul>

<p>It can do this either on the file system or inside a container image.</p>

<h3 id="configuration-issues">Configuration issues</h3>

<p>Trivy can scan your configuration files against known configuration issues it support files like:</p>

<ul>
  <li>Dockerfiles</li>
  <li>Kubernetes manifests</li>
  <li>Terraform</li>
  <li>CloudFormation</li>
  <li>etc.</li>
</ul>

<h3 id="custom-policies-1">Custom policies</h3>

<p>You can add your own custom policies, but you will have to write them in Rego.
Trivy uses Defsec their cloud rules engine for Docker and Kubernetes and tfsec a static analysis scanner for terraform
code,
both of these rule engines are open-source and use OPA under the hood.</p>

<h2 id="the-composer-fugue">The Composer: Fugue</h2>

<p><a href="https://www.fugue.co/" target="_blank" rel="noopener noreferrer">Fugue</a> is a cloud security platform that helps you
to secure your cloud environment,
it was bought by Snyk some time ago, and after this take-over Snyk started working on Snyk Cloud.</p>

<p><img src="/img/2023-02-16-secure-cloud-foundation/Fugue.png" alt="Gandalf, You shall not pass" class="image fit" style="margin:0px auto; max-width:60%; height:60%" /></p>

<h3 id="baseline-enforcement">Baseline enforcement</h3>

<p>Fugue allows you to take a snapshot of your cloud environment and use it as baseline.</p>

<p>This prevents anyone from making modifications to your environment that are not compliant with your baseline.</p>

<p>It can’t recreate or delete resources, it only enforces by modifying them back to the original state of the snapshot.</p>

<p>A snapshot captures complete cloud resource configurations, attributes, relationships, and drift.
As an added bonus, snapshots enable deep visualization and reporting capabilities.</p>

<h3 id="policy-scanning">Policy scanning</h3>

<p>Fugue allows you to write policies to scan your AWS environment for compliance,
or you can use one of the pre-defined policies like CIS AWS Foundations Benchmark.</p>

<p>It does not provide automatic solutions to fix the violations, but has descriptions on how to fix them.</p>

<h3 id="cicd-integration-1">CI/CD integration</h3>

<p>Fugue can be integrated with your CI/CD pipeline to scan your infrastructure as code for compliance using their cli.</p>

<p>They have a guide on how to set this up with CircleCI,
but it should be possible to set this up with any other CI/CD tools.</p>

<h3 id="custom-policies-2">Custom policies</h3>

<p>Fugue allows you to write custom policies in Rego.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In conclusion, managing and securing cloud environments can be a complex and challenging task due to segmented responsibilities across different teams and projects. Open Policy Agent, Cloud Custodian, and Snyk are three tools of many that can help organizations enforce policies, maintain continuous compliance and governance, and optimize costs across multiple cloud platforms.</p>

<p>While these tools can be valuable additions to any organization’s cloud security and compliance toolset, it’s worth
noting that cloud providers also offer native solutions like AWS Security Hub, Azure Security Center/Sentinel that can
offer similar functionality.
However, the native solutions may lack the flexibility and customization options of third-party tools like OPA, Cloud
Custodian, and Snyk, which may be essential for meeting specific organizational requirements.</p>

<p>If you find this post helpful, be sure to keep an eye out for our upcoming follow-up post,
we’ll be diving deeper into the practical applications of OPA and sharing some real-world use cases.
Be sure to stay tuned, so you don’t miss out on valuable insights and tips.</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Lander&quot;, &quot;last_name&quot;=&gt;&quot;Marien&quot;, &quot;permalink&quot;=&gt;&quot;/author/lander_marien/&quot;, &quot;avatar&quot;=&gt;&quot;lander_marien.jpg&quot;, &quot;title&quot;=&gt;&quot;Intern&quot;, &quot;linkedin&quot;=&gt;&quot;lander-marien&quot;, &quot;email&quot;=&gt;&quot;lander.marien@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Lander is a Cloud Engineer with an elevated interest in everything Kubernetes, Automation and AWS.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2022-07-07-snitching-on-your-colleagues-using-cloud-magic.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-05-17-kubecon-2023.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-06-07-secure-cloud-foundation.md collection=posts&gt;]}</name><email>lander.marien@ordina.be</email></author><category term="Cloud" /><category term="aws" /><category term="cloud" /><summary type="html"><![CDATA[Introduction The Warden: Open Policy Agent The Janitor: Cloud Custodian The Watchdog: Snyk The Container Detective: Trivy The Composer: Fugue]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-02-16-secure-cloud-foundation/header.png" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-02-16-secure-cloud-foundation/header.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Back to Terraform</title><link href="https://ordina-jworks.github.io/cloud/2023/06/05/back-to-terraform.html" rel="alternate" type="text/html" title="Back to Terraform" /><published>2023-06-05T00:00:00+00:00</published><updated>2023-06-05T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/cloud/2023/06/05/back-to-terraform</id><content type="html" xml:base="https://ordina-jworks.github.io/cloud/2023/06/05/back-to-terraform.html"><![CDATA[<blockquote>
  <p>“Fail fast” does not imply lack of commitment to a mission or goal, but on the contrary, indicates a willingness to experiment in the process, learn quickly from the results, and make adjustments to better achieve an enhanced customer experience.</p>
</blockquote>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#what-is-terraform">What is Terraform?</a></li>
  <li><a href="#what-is-pulumi">What is Pulumi?</a></li>
  <li><a href="#what-is-terragrunt">What is Terragrunt?</a></li>
  <li><a href="#developer-adoption-excluding-the-batteries-pulumi">Developer adoption, excluding the batteries: Pulumi</a></li>
  <li><a href="#additional-complexity-without-the-promised-benefit-terragrunt">Additional complexity without the promised benefit: Terragrunt</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>Public cloud computing has revolutionized the way organizations approach their IT infrastructure. 
Rather than investing in and maintaining their hardware and software, businesses can now access computing resources through the internet, on an as-needed basis. 
This shift towards public cloud adoption has created a demand for new management and deployment approaches, with Infrastructure as Code (IaC) emerging as a critical tool for cloud infrastructure management.</p>

<p>IaC is the process of defining and provisioning computing infrastructure through code, rather than through manual processes. 
This approach allows for consistent, repeatable, and scalable infrastructure deployment, while also providing the ability to automate management tasks and enforce governance policies. 
With IaC, developers can write code that defines the desired state of infrastructure, which can then be deployed and improved over time.</p>

<p>As organizations increase their footprint in the cloud, the need for IaC becomes increasingly important. 
Without proper automation and governance, manual infrastructure management becomes time-consuming, error-prone, and difficult to scale. 
Developers can easily collaborate and share their infrastructure code, while also ensuring that resources are deployed consistently and securely. 
By automating infrastructure provisioning and deployment, organizations can more easily scale resources up or down as needed, optimize resource usage, and rapidly respond to changing business needs.</p>

<p>In this blog post, we’ll discuss 3 different IaC tools: Terraform, Pulumi and Terragrunt. 
We’ll discuss two real-world cases where Pulumi and Terragrunt were replaced by Terraform. 
We’ll explain why they weren’t the correct fit and what lessons we learned from using them.
If you’re already familiar with the 3 tools, feel free to skip the next 3 sections and dive straight into the <a href="#use-cases">use cases</a></p>

<h2 id="what-is-terraform">What is Terraform?</h2>

<p>Terraform is an open-source IaC tool, created by Hashicorp, that enables developers to provision and manage cloud infrastructure across various cloud providers. 
It uses a declarative language to define infrastructure resources, allowing developers to specify the desired state of resources such as virtual machines, load balancers, databases, and more. 
The tool then manages the entire lifecycle of these resources, from initial provisioning to improvements over time to deletion.
Terraform’s use of code to manage cloud infrastructure provides several benefits.</p>

<p>First, it allows for consistent and repeatable infrastructure deployment, eliminating manual errors.
Second, it enables collaboration and version control, as infrastructure code can be shared among teams and tracked through Git repositories. 
This allows teams to execute infrastructure changes using the same processes as code changes in their application.
Deploying a single instance of an application or 100s becomes as simple as a code change.
Terraform’s versatility and portability make it an attractive choice for managing multi-cloud environments.
Its state management features ensure that infrastructure changes are auditable and transparent. 
With Terraform, organizations can achieve greater efficiency, scalability, and agility in their public cloud infrastructure management.</p>

<h2 id="what-is-pulumi">What is Pulumi?</h2>

<p>Pulumi is another IaC platform that allows developers to define and manage cloud infrastructure using familiar programming languages such as Python, JavaScript, and Go.
Pulumi takes a different approach compared to Terraform, which uses a declarative language to define infrastructure resources.</p>

<p>With Pulumi, developers can create infrastructure resources using a variety of programming languages, leveraging the full power of those languages to manage infrastructure. 
This approach provides more flexibility and control compared to Terraform, as developers can use programming language constructs such as loops, conditionals, and functions to create more dynamic and complex infrastructure resources.
While Pulumi provides more flexibility and control compared to Terraform, it may also require more programming knowledge and experience to use effectively.
Terraform, on the other hand, provides a simpler and more standardized approach to infrastructure management, which can be easier for beginners to learn.</p>

<p>In essence, Pulumi and Terraform aren’t that different, they just use a different <code class="language-plaintext highlighter-rouge">interface</code> to determine what the desired state of the infrastructure is. 
Pulumi even uses the Terraform libraries in the backend to create the desired state model.</p>

<h2 id="what-is-terragrunt">What is Terragrunt?</h2>

<p>Terragrunt is a thin wrapper around Terraform that provides extra functionality and simplifies the management of multiple Terraform modules.
It is essentially a tool for managing Terraform code and configurations, and it uses a similar syntax to Terraform.</p>

<p>One of the main benefits of Terragrunt is that it provides a more modular approach to infrastructure management compared to Terraform. 
With Terragrunt, developers can define common configurations and modules that can be reused across multiple Terraform projects.
This makes it easier to maintain consistent infrastructure across an organization and reduces duplication of effort.</p>

<p>Another key benefit of Terragrunt is that it supports the automatic generation of Terraform configuration files, making it easier to manage and scale large infrastructure projects. 
It also includes a feature called “apply-all”, which applies Terraform changes across all configured environments, simplifying the management of complex environments.
While Terragrunt provides several benefits, it does come with a learning curve, as it requires developers to learn a new syntax and understand its unique features.
Additionally, it adds another layer of complexity to infrastructure management, which may not be necessary for smaller projects.</p>

<h1 id="use-cases">Use cases</h1>

<h2 id="developer-adoption-excluding-the-batteries-pulumi">Developer adoption, excluding the batteries: Pulumi</h2>

<h3 id="background">Background</h3>

<p>This story starts with the IT operations team of a large corporation.
The team is responsible for managing all on-premise infrastructure and supporting teams in using public cloud providers such as Amazon Web Services (AWS) and Microsoft Azure.
For the scope of this story, the on-premise side of things isn’t relevant.</p>

<p>For the public cloud resources, the team supplied the development teams with accounts to manage and allowed them to either request infrastructure from the operations team or manage their infrastructure on their own. 
This meant that some development teams chose AWS CloudFormation to manage their infrastructure, some vendors supplied scripts (including Terraform) and some teams clicked the setup together in the AWS Console.</p>

<p>The issue where temporary proofs-of-concept (PoCs) became <code class="language-plaintext highlighter-rouge">no such thing as temporary</code> production environments, was also a common occurrence.
Due to this approach, the governance and security footprint of applications depended highly on the AWS knowledge available in the teams.</p>

<p>Finally, just a side note, the CI/CD solution consisted of AWS CI/CD services: CodeBuild, CodePipeline, CodeCommit and CodeArtifact.</p>

<h3 id="managing-expectations-utopia">Managing expectations: Utopia</h3>

<p>To improve governance and security of the landscape, standardized building blocks would be built.
These building blocks would adhere to the security and governance requirements by design. 
The idea was that project teams could use the building blocks directly in their project and could maintain them over time.</p>

<p>The operation team evaluated multiple tools, including Terraform and Pulumi. 
As the project teams generally didn’t know Terraform, but did know programming languages, Pulumi was chosen as a standard tool to build and distribute the building blocks.</p>

<p>Pulumi supports a handful of languages at the time of writing: Go, Python, NodeJS (JavaScript/TypeScript), .NET and Java.
One of the assumptions made by the operations team was that the development teams would know at least one of these languages.</p>

<p>Another advantage, especially compared to AWS CloudFormation or plain Terraform, is that programming languages generally have great support for testing, especially unit testing. 
This would allow the operations team to create components, validate them using traditional testing methods and supply components to teams with a high level of confidence that they would work as intended.</p>

<p>Finally, the last relevant assumption for this story was the assumption that development teams would like to manage their infrastructure and would maintain the code needed to deploy, run and maintain that infrastructure.</p>

<p>By now you might have figured out that these assumptions might be the cause of our <code class="language-plaintext highlighter-rouge">change of heart</code></p>

<h3 id="change-of-heart">Change of heart</h3>

<p>The change of heart occurred after multiple difficulties and unexpected limitations.</p>

<p>The first red flag was discovered during the evaluation phase. 
While setting up Pulumi, most of the documentation assumes that the Pulumi Service product is used. 
This is most visible in the default behavior of the <code class="language-plaintext highlighter-rouge">pulumi login</code> command.
This command is needed to initialize the Pulumi context and its default behavior is to login into the Pulumi Service product.
It’s possible to use AWS S3, Azure Storage Accounts or local files instead <a href="https://www.pulumi.com/docs/intro/concepts/state/#using-a-self-managed-backend" target="_blank" rel="noopener noreferrer">as is documented here</a>.</p>
<blockquote>
  <p>Note that for AWS, profile support is implemented by adding it to the query parameters of the S3 and KMS connection strings. This has implications on the “shareability” of the configuration across developers.</p>
</blockquote>

<p>As there was support for the desired self-managed approach with some additional configuration, this wasn’t considered an issue at the time.</p>

<p>During the implementation of the pipelines to deploy the Pulumi setup, another red flag occurred.
Reviewing infrastructure rollouts is a key feature of any IaC tool.
(Infrastructure) Developers want to be able to review the changes Pulumi wants to make to the infrastructure to match the new desired state. 
This feature has proven to be invaluable in the past to prevent costly or even unrecoverable mistakes in the code. 
Unfortunately, this is another area where Pulumi isn’t strong at the time of writing. 
The support for reviewing the changes that Pulumi will make is limited to the CLI and saving a <code class="language-plaintext highlighter-rouge">change plan</code> that can be reviewed is supported, <a href="https://www.pulumi.com/docs/intro/concepts/update-plans/" target="_blank" rel="noopener noreferrer">but behind an experimental flag</a>, which suggests it’s not ready for production use. 
Currently, the change plan is also not in a review-friendly format.</p>

<p>Remember the assumption that development teams would know a programming language that was supported by Pulumi?
Good news, they do and Pulumi supports their language of choice.
The problem is that the most commonly used language is Java, the language that is currently only experimentally supported by Pulumi. 
This shouldn’t be a problem as Pulumi supports cross-language component creation. 
This means that the operation teams can create Pulumi components in Python for example and allow development teams to use them in Java once that reaches prime-time status. 
Unfortunately, a theme starts to occur in the feature set of Pulumi. 
The cross-language component support is also a feature that’s <a href="https://github.com/pulumi/pulumi/issues/6804" target="_blank" rel="noopener noreferrer">still under heavy development</a> and cannot be used properly at the time of writing.</p>

<p>Finally, for the sake of completeness, there were a few additional issues that are worth mentioning.</p>

<p>The assumption that (unit) tests could be easily written to validate the Pulumi code, doesn’t hold true today. 
There is <a href="https://www.pulumi.com/docs/guides/testing/" target="_blank" rel="noopener noreferrer">support for testing</a> in Pulumi, but due to the lack of <code class="language-plaintext highlighter-rouge">real</code> mocking capabilities (including any side-effects) in Pulumi, the tests feel like testing getters and setters instead of proper tests. 
A solution like the mocking server in the <a href="https://github.com/fabric8io/kubernetes-client#mocking-kubernetes" target="_blank" rel="noopener noreferrer">Fabric8 Kubernetes client</a> would solve this issue.</p>

<p>Another side-effect of having a regular programming language is that the order of execution of statements is important. 
When the output of a previous statement is needed as input for the next, one cannot simply use the output of the first command to pipe it to the next. 
This is a limitation because Pulumi needs to build a desired state model in the backend before it can know the actual output for some statements. 
When the output of a statement is needed in the next statement, it needs to have a way to postpone the execution of that statement.
<a href="https://www.pulumi.com/docs/intro/concepts/inputs-outputs/#outputs-and-strings" target="_blank" rel="noopener noreferrer">Pulumi does supply helper functions</a> to make this less of an issue, but it’s something that needs to be considered when developing the code.</p>

<h3 id="pulumi-to-terraform-conclusion">Pulumi to Terraform conclusion</h3>

<p>These previous issues combined caused the team to switch to Terraform as their preferred IaC tool.</p>

<p>Pulumi has a ton of potential and might become the next big multi-cloud IaC tool in the coming years. 
Unfortunately, too many parts don’t feel ready enough to be used in a corporate context where infrastructure developers come at a premium.</p>

<p>Terraform has a proven track record. 
It supports shared modules in a variety of ways: S3, Git, and local files, … 
It properly supports a split between the <code class="language-plaintext highlighter-rouge">plan</code> and <code class="language-plaintext highlighter-rouge">apply</code> phases, meaning that the change plan can be reviewed.
Terraform Cloud is a SaaS solution provided by Hashicorp, but it doesn’t feel like a requirement to properly use Terraform.</p>

<p>Terraform isn’t perfect either though.
There is no real testing automated capability in Terraform, especially without deploying the infrastructure.
Developers need to learn yet another language (HCL) to use Terraform and it doesn’t have the flexibility of a real programming language.</p>

<h2 id="additional-complexity-without-the-promised-benefit-terragrunt">Additional complexity without the promised benefit: Terragrunt</h2>

<h3 id="background-1">Background</h3>

<p>When starting a new infrastructure project, it’s important to choose the right tools for the job. But what happens when you inherit an existing project with infrastructure as code (IaC) already in place, using a tool like Terragrunt that your team is not familiar with? 
This is the situation the team in this story found themselves in.</p>

<p>The project they were working on was based on an existing infrastructure stack that had been built using Terragrunt, a popular wrapper around Terraform. 
While Terragrunt can be a powerful tool for managing complex infrastructure stacks, the new team found that it added unnecessary complexity and overhead to their workflow. 
They struggled to read and understand the existing Terragrunt code, which had been split into multiple Terraform modules in different git repositories and brought back together using a Terragrunt configuration repository.</p>

<p>Adding to the complexity, the Terragrunt configuration was loading different versions of the Terraform modules in different environments, and the Terraform state was split into modules as well, with custom scripts to read and manipulate the state. 
All of these factors made it difficult for the new team to make progress on the project, and they began to question the decision to use Terragrunt in the first place.</p>

<h3 id="deciding-to-return-to-terraform">Deciding to Return to Terraform</h3>

<p>After struggling to work with the complex and fragmented Terragrunt setup, the new infrastructure team wanted to find a simpler, more streamlined approach. 
They sought a setup with a low threshold, low maintenance, and easy to understand.</p>

<p>As they dug deeper into the Terragrunt configuration, they found that even the company’s architects had some doubts about its usefulness. 
So they began discussing alternatives, and eventually, someone asked the question: “Why are we using Terragrunt?”</p>

<p>The answer was surprising: the team wasn’t even using any of Terragrunt’s real advantages and was instead dealing with unnecessary overhead. 
The configuration of all external modules was just a single key-value file, with no clear indication of which value was passed to which module.</p>

<p>It became clear that continuing to use Terragrunt was simply pointless, and only adding to the team’s frustration. 
They, together with the company’s architects, decided to switch back to using Terraform directly, to simplify their workflow and make progress on the project.</p>

<h3 id="returning-to-terraform-without-breaking-the-existing-infrastructure">Returning to terraform without breaking the existing infrastructure</h3>

<p>After deciding to switch back to using Terraform directly, the team realized that a complete overhaul of their infrastructure setup would be necessary. 
Rather than throwing out all of the existing Terragrunt code, however, they decided to take a hybrid approach.</p>

<p>The first step was to set up a new mono git repository that would contain all of the different Terraform modules that had previously been spread across multiple repositories. 
They also created a new Terraform root configuration setup, which would enable them to manage the entire infrastructure as a single entity.</p>

<p>Next, the team began moving these Terraform modules into their new mono repo. 
As they did so, they took the opportunity to clean them up, removing any unused input variables or features. 
They also improved the security setup, as the project required the infrastructure to be publicly accessible instead of hidden behind an on-premises network.</p>

<p>By consolidating all of the Terraform modules in one place, the team made it easier for existing teams to import them into their Terragrunt setup. 
The hybrid approach proved to be a successful strategy, enabling the team to manage their infrastructure more effectively and efficiently, but also not breaking what was already in place.</p>

<h3 id="some-other-unexpected-advantages">Some other (un)expected advantages</h3>

<p>Moving to a mono repo from the existing Terragrunt setup not only simplified the project’s infrastructure but also brought several expected and unexpected advantages.</p>

<p>One of the most significant issues with the Terragrunt setup was its complexity and slow performance on the CI/CD pipelines, which checked out multiple git repositories. 
However, by consolidating the Terraform modules into a single repository, the team eliminated the need to check out different repositories during CI/CD, significantly speeding up the pipeline.</p>

<p>The development process also benefited from the switch to a mono repo. 
A simple change to the infrastructure no longer required updates to multiple git repositories, making the process more efficient and reducing the chance of errors.</p>

<p>In addition to these expected advantages, the team discovered other unexpected benefits. 
For example, setting up a new environment only requires a simple configuration file with around ten configuration values. 
This means that setting up a new environment is much quicker and more straightforward than before.</p>

<p>Overall, the decision to move from Terragrunt to a hybrid approach with a mono repo proved to be a wise one. 
The team enjoyed a simpler, more efficient infrastructure setup, improved performance on CI/CD pipelines, and unexpected benefits such as a streamlined environment setup.</p>

<h3 id="conclusion">Conclusion</h3>

<p>The decision to move away from Terragrunt and towards a hybrid approach of using Terraform and a mono repo had several (un)expected benefits for the team. Not only did it simplify the setup and reduce maintenance overhead, but it also significantly sped up the development process and CI/CD pipelines.</p>

<p>The team was able to clean up and improve the security of the existing Terraform modules, and consolidate them into a single repository, making it much easier to manage and make changes. The move to a simpler configuration file for new environments also made it easier to spin up new instances of the infrastructure.</p>

<p>While it can be tempting to stick with existing tools and setups, sometimes a reevaluation of the current setup can lead to significant improvements in productivity and efficiency.</p>

<h1 id="conclusion-1">Conclusion</h1>

<p>The main takeaways of this blog post are twofold:</p>
<ul>
  <li>use the right tool for the job, technically and,</li>
  <li>even more importantly, use a tool that the users can understand and/or have knowledge on</li>
</ul>

<p>Often tools are selected based on a first impression and a very small PoC, as in the first story. 
These PoCs are often a good indicator of the feasibility of a tool, but they don’t show real use of course.
Therefore we recommend including the maturity of a tool and the general availability of knowledge about the tool in the market. 
Look up how many TerraGrunt or Pulumi developers there are near you and compare that to the number of Terraform developers you can find.</p>

<p>Next to selecting the right tool for the right reasons, your existing team also needs to be able to work with the tool.
Selecting a tool therefore as much of a technical as a people decision.</p>

<p>Finally, selecting a well-established tool is always a safe bet. 
We’d like to highlight that you can and should select more innovative and less established tools as well but don’t use them for your core business or as something that will support (or fail to) your entire application landscape.
Pulumi and TerraGrunt might pop up on our horizon in the future again, but for now, we’ll stick with good old (<a href="https://en.wikipedia.org/wiki/Terraform_(software){:target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;}">28 July 2014</a>) Terraform.</p>

<p>If you want help with your Infrastructure As Code, cloud adoption in your company or want to provide feedback, feel free to contact <a href="https://be.linkedin.com/in/yannick-bontemps-885379b0" target="_blank" rel="noopener noreferrer">Yannick</a> or <a href="https://be.linkedin.com/in/pieter-vincken-a94b5153" target="_blank" rel="noopener noreferrer">Pieter</a> on LinkedIn!</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Pieter&quot;, &quot;last_name&quot;=&gt;&quot;Vincken&quot;, &quot;linkedin&quot;=&gt;&quot;pieter-vincken-a94b5153&quot;, &quot;twitter&quot;=&gt;&quot;PieterVincken&quot;, &quot;github&quot;=&gt;&quot;pietervincken&quot;, &quot;permalink&quot;=&gt;&quot;/author/pieter_vincken/&quot;, &quot;avatar&quot;=&gt;&quot;pieter-vincken.jpeg&quot;, &quot;title&quot;=&gt;&quot;Cloud Automation Engineer&quot;, &quot;email&quot;=&gt;&quot;pieter.vincken@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Pieter rarely sees problems, just too many solutions sometimes. That’s why K’nex was his favorite pastime as a child. Nowadays, he builds cloud platforms so organizations don’t have to worry about their IT infrastructure but can focus on their clients.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2019-05-03-istio-service-mesh-s2s.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-09-18-kustomize.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-11-18-devoxx-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-06-02-terraform.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-08-28-kubernetes-clients-comparison.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-10-15-5-reasons-not-to-go-to-the-cloud.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-02-14-postgres-ai.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-06-10-kubecon.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-10-27-renovate.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-05-17-kubecon-2023.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-06-05-back-to-terraform.md collection=posts&gt;]}</name><email>pieter.vincken@ordina.be</email></author><category term="Cloud" /><category term="cloud" /><category term="automation" /><category term="cicd" /><category term="terraform" /><category term="iac" /><summary type="html"><![CDATA[“Fail fast” does not imply lack of commitment to a mission or goal, but on the contrary, indicates a willingness to experiment in the process, learn quickly from the results, and make adjustments to better achieve an enhanced customer experience.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-06-05-back-to-terraform/header.png" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-06-05-back-to-terraform/header.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">KubeCon + CloudNativeCon 2023</title><link href="https://ordina-jworks.github.io/cloud/2023/05/17/kubecon-2023.html" rel="alternate" type="text/html" title="KubeCon + CloudNativeCon 2023" /><published>2023-05-17T00:00:00+00:00</published><updated>2023-05-17T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/cloud/2023/05/17/kubecon-2023</id><content type="html" xml:base="https://ordina-jworks.github.io/cloud/2023/05/17/kubecon-2023.html"><![CDATA[<h1 id="introduction">Introduction</h1>

<p style="text-align: center;"><img src="/img/2023-05-17-kubecon-2023/group-picture.jpg" alt="JWorks group picture" class="image" style="margin:0px auto; max-width:100%" />
<em>JWorks at Kubecon 2023</em></p>

<p>Another year, another KubeCon | CloudNativeCon EU edition. 
And, of course, JWorks was also present at this year’s edition in Amsterdam in the <a href="https://www.rai.nl/" target="_blank" rel="noopener noreferrer">RAI</a>.
We wanted to hear more about all the new features, frameworks, tools, ideas and concepts that the Kubernetes and cloud world have to offer.
And we got what we wanted.
We attended many interesting talks, talked to very intriguing people at the event, and had a lot of fun while doing so.
Next to the talks, there were also many booths from companies from all over the world (AWS, Azure, Canonical, …) to engage with other people and to talk and promote their newest products for the developer market.
You can look at some of the talks we attended, which were very interesting.
You can click on the talk title to go to the recorded version on YouTube, which CNCF provides.</p>

<h1 id="metrics-at-full-throttle-intro-and-deep-dive-into-thanos---saswata-mukherjee--filip-petkovski-shopify"><a href="https://www.youtube.com/watch?v=2GokLB5_VfY" target="_blank" rel="noopener noreferrer">Metrics at Full Throttle: Intro and Deep Dive Into Thanos - Saswata Mukherjee &amp; Filip Petkovski, Shopify</a></h1>

<p><a href="https://thanos.io/" target="_blank" rel="noopener noreferrer">Thanos</a> enables a highly available Prometheus setup.
It replaces parts of the Prometheus deployment model that are hard to scale with regular Prometheus.</p>

<p>This talk is a great introduction to Thanos.
Saswata and Filip start by explaining what difficulties you can have with scaling a regular Prometheus setup. 
They explain what components of Thanos solve the different issues with scaling Prometheus. 
The following components of Thanos are introduced: Sidecar, Ruler, Receive, Query, Compactor, and Store gateway.</p>

<p>Where the first half of the talk is mainly targeted towards people who don’t know Thanos or don’t know all capabilities of Thanos yet, the second half was surely targeted towards experienced Thanos users.
In the second half of the talk, Saswata and Filip highlight recent, since the KubeCon Detroit, improvements to Thanos.</p>

<p>They discuss 5 new features in Thanos. 
The first boils down to optimizations to the store gateway to remove the high IO requirements to run the store gateway.
The new store gateway implementation doesn’t require the high IO disk anymore as the information is stored in memory instead of on disk.</p>

<p>Three new features were announced to optimize the querying throughout the different components in Thanos.</p>

<p>First, quality of service limits was added as Thanos configuration options. 
This allows teams managing Thanos to define the limits to tune the Thanos performance and prevent a single query from overloading the system.</p>

<p>The second improvement related to query performance was a newly implemented, Thanos-specific, PromQL engine. 
With the new query engine, the query will be analyzed up front and an optimal (parallel!) execution will be determined. 
Since not all operators are supported yet, the new engine will be used when possible with a fallback to the standard Prometheus PromQL engine when needed.</p>

<p>The third improvement is the distributed execution of queries.
This mechanism allows queries to be executed by the nodes that have direct access to the needed data, preventing costly data transfers between components in the Thanos architecture. 
If you want more information on this amazing feature, definitely check out this talk when it’s available on YouTube!</p>

<p>A new hash ring mechanism was implemented for the Receiver to prevent overloading single Receiver instances.</p>

<p>They end by showing real-world performance graphs from a Thanos deployment and how different versions of Thanos caused visible improvements.</p>

<h1 id="playstation-and-kubernetes-how-to-solve-a-problem-like-real-time---joseph-irving-playstation"><a href="https://www.youtube.com/watch?v=pklRTQoRrNY" target="_blank" rel="noopener noreferrer">PlayStation and Kubernetes: How to Solve a Problem Like Real-Time - Joseph Irving, PlayStation</a></h1>

<p>Since its birth in 1994, (Sony) PlayStation has been a global pioneer in the gaming industry.
As PlayStation grew, its infrastructure needed to grow as well.
Over time, PlayStation had to scale its game servers so that it could handle their demand.
This had to be done in a way where game servers could be scaled in a compatible way with the game clients.
This means that simply spinning up more instances of the game server might not always work.</p>

<p>This talk, presented by Joseph Irving, went over the kind of problems they faced, different types of real-time game servers, and their advantages and limitations (peer-to-peer, dedicated game servers, etc.).</p>

<p>Their solution was to use a project called <a href="https://agones.dev/site/" target="_blank" rel="noopener noreferrer">Agones</a>, which is created and maintained by Google in collaboration with Ubisoft.
Agones is an open-source platform that provides a native way of running game servers on Kubernetes without worrying about the infrastructure.
It provides compatibility with game server connections by using GameServers and connecting those GameServers with Fleets.
Joseph introduces the Agones framework and talks about how they use it and how it helped them to scale their gaming servers and to provide multi-regional operability.</p>

<p>He concludes the talk by talking about Matchmakers, which implements matchmaking (finding matches for people that are searching for a lobby in a game) through Kubernetes.</p>

<h1 id="automating-configuration-and-permissions-testing-for-gitops-with-opa-conftest---eve-ben-ezra--michael-hume-the-new-york-times"><a href="https://www.youtube.com/watch?v=VCX4UALQjeg" target="_blank" rel="noopener noreferrer">Automating Configuration and Permissions Testing for GitOps with OPA Conftest - Eve Ben Ezra &amp; Michael Hume, The New York Times</a></h1>

<p>Open Policy Agent is a tool to add <a href="https://www.openpolicyagent.org/" target="_blank" rel="noopener noreferrer">policy-based control to cloud-native environments</a>.
OPA is used in many different tools and systems as a system to validate configuration before it’s deployed to cloud environments.</p>

<p>In this talk, Eve and Michel explain how OPA is used in the New York Times internal developer platform to control what their developers deploy.
Eve starts by showing what problems they experienced at the New York Times with allowing developers to use the internal platform. 
As with all validation (aka testing) mechanisms, shifting left is the focus. 
They continue to explain how <a href="https://www.conftest.dev/" target="_blank" rel="noopener noreferrer">Conftest</a> helped them in providing feedback to the developer as soon as they write a single line of code.
Validating the configuration using the same definition along every set in the process, makes it very transparent to developers where issues are introduced and what they can do to fix them.</p>

<p>Next, a great introduction to OPAs rule language Rego is shown by Eve.
They take the audience through a complete example, explaining what is defined and how it’s interpreted, including the quirks, using Rego.</p>

<p>Finally, Michel shows how <a href="https://github.com/yannh/kubeconform" target="_blank" rel="noopener noreferrer">Kubeconform</a> can be used to help with Kubernetes version migrations, including CRDs. 
They show a real example and explain how the output of the tool can be used to quickly identify what’s wrong with a manifest.</p>

<p>If anything, this talk is a brilliant introduction to Rego.
If you have an interest in policy management in your Kubernetes cluster or if you have experienced hard-to-find bugs in Kubernetes manifests, this is a must-watch talk from the <a href="https://www.nytimes.com/" target="_blank" rel="noopener noreferrer">New York Times</a>.</p>

<h1 id="state-of-the-mop-cloud-custodian-in-2023"><a href="https://www.youtube.com/watch?v=Lx5f-0WOFrA" target="_blank" rel="noopener noreferrer">State of the Mop: Cloud Custodian in 2023</a></h1>

<p>During the presentation, Kapil Thangavelu provided a concise update on <a href="https://cloudcustodian.io/" target="_blank" rel="noopener noreferrer">Cloud Custodian</a>, an open-source rules engine designed for account and resource management on AWS, Azure, and GCP, based on the Rego language.
Cloud Custodian can effectively scale from small businesses to large enterprises.</p>

<p>Since Kubecon Detroit in October 2022, Cloud Custodian has added support for two new providers.
Additionally, they’ve included Terraform support to enable users to check their Terraform source code while running inside a pipeline.</p>

<p>Looking ahead, Cloud Custodian’s roadmap for the current year includes adding a new K8s admission controller, support for AWS CloudFormation, preventative support for AWS, and an improved authoring experience through the addition of policy testing, policy tracing, a policy debugger, and more.</p>

<p>Cloud Custodian’s open-source nature, flexibility, and upcoming roadmap make it a tool to watch for organizations managing cloud resources.
It provides a powerful and customizable way to improve security, cost optimization, and compliance across different cloud providers.</p>

<h1 id="scaling-databases-at-activision---greg-smith--vladimir-kovacik-activisionblizzard"><a href="https://www.youtube.com/watch?v=_ba9tbivT28" target="_blank" rel="noopener noreferrer">Scaling Databases at Activision - Greg Smith &amp; Vladimir Kovacik, Activision/Blizzard</a></h1>
<p>This talk goes over how Activision / Blizzard scaled their databases over time. 
They introduce the session by talking about hosting and operating their databases in the past, which included bare metal, many virtual machines &amp; containers, …
As they progressed and more people joined their games (especially on launch days), they quickly discovered that there was a need for better scaling, especially in the number of shards, due to performance issues.</p>

<p>As the whole company was internally moving to Kubernetes, they wondered if there was a way to have their MySQL databases hosted on Kubernetes natively. 
This is where they introduce <a href="https://vitess.io/" target="_blank" rel="noopener noreferrer">Vitess</a>, a native way to run and scale MySQL-compatible databases.
They wanted to use something that supported MySQL since they had plenty of MySQL experts employed and because they used MySQL before adopting Vitess.</p>

<p>They divided the migration to Vitess into three adoption stages: MVP, Load Testing &amp; Production Readiness.</p>

<h3 id="mvp">MVP</h3>
<p>In the MVP stage, they wanted to find out how easy or hard it was to migrate to Vitess. 
They needed to verify that Vitess works for them, which they could quickly find out by running their database tests.
Along the way, they were able to fix all the issues that the migration caused, which included talking with the Vitess community to find solutions.</p>

<h3 id="load-testing">Load Testing</h3>
<p>In the Load Testing stage, they needed to make sure that this new solution was able to scale successfully and handle their workload (again, especially on launch days).
This could be done by checking if shards scaled successfully and that there were no performance bottlenecks during the load testing.
The developers were able to confirm that scaling was done as how they expected it to scale.</p>

<h3 id="production-readiness">Production Readiness</h3>
<p>In the Production Readiness stage, they had to prove that their solution worked and that their previous stage turned out as a success.
However, they wanted more confirmation about how Vitess works as they needed to be sure that it was the right solution.
After some chaos testing and implementing other Vitess features, they confirmed that the scaling solution was stable.</p>

<p>One of their most important changes was the move from application sharding to a single endpoint.
Where there previously was a separate sharding configuration in the application configuration, they now moved to a single database endpoint.</p>

<p>They concluded the talk by stating that Vitess does work, and right now there are approximately 60 Vitess clusters in development and production environments.
They are also building an internal team around the company to support Vitess, and it has become the default database solution for any new services that the company will implement.</p>

<p>It was a very long process, but eventually, they were able to successfully adopt Vitess.</p>

<h1 id="sponsors--vendors--projects-with-stands">Sponsors / Vendors / Projects with stands</h1>
<p>At Kubecon, there were a lot of booths where companies and separate projects were able to promote their product, introduce new features, and offer some fun gadgets to the attendees of Kubecon.
Some major players in the industry were there, and of course, a lot of people were interested in what they had to offer to make their lives more simple.</p>

<h2 id="aws">AWS</h2>

<p>AWS had an impressive presence, offering attendees a wide range of experiences and insights into their latest developments.
The AWS stand provided a sneak peek of upcoming features and integrations planned for their EKS portal.
The visuals on display highlighted their focus on monitoring and improving the overall user experience.</p>

<p>In addition to the exciting demos, AWS provided a fun and challenging mini-golf game for conference-goers to play.
Participants had the opportunity to win AWS-branded pyjama pants, adding a touch of fun and excitement to the event.</p>

<p>AWS didn’t stop at the game, as they also offered an array of other goodies for attendees throughout the conference.
Whether it was swag bags or other branded merchandise, AWS had something for everyone.</p>

<p>Finally, AWS brought a team of experts who were available to answer any questions attendees had about EKS.
Their knowledgeable team provided attendees with valuable insights and guidance on using the platform effectively.</p>

<h1 id="conclusion">Conclusion</h1>
<p>Kubecon was a fun experience, and we got a load of interesting topics and talks.
Amsterdam is beautiful and tourist-friendly, with most of the residents speaking English fluently.
The RAI was a huge venue, which meant to go from talk to talk some (power) walking was needed.
Some talks were too popular to fit into the rooms that were assigned to them.
The live stream was always a backup, but it doesn’t have that same effect, even if you sit around a smartphone screen with ten people to watch a talk.
The food and especially the coffee was great this edition.
KubeCon | CloudNativeCon EU was a great experience, and we can’t wait until next year’s edition.
See you in Paris!</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Yolan&quot;, &quot;last_name&quot;=&gt;&quot;Vloeberghs&quot;, &quot;linkedin&quot;=&gt;&quot;yolan-vloeberghs-23825aa3&quot;, &quot;github&quot;=&gt;&quot;yolanv&quot;, &quot;permalink&quot;=&gt;&quot;/author/yolan-vloeberghs/&quot;, &quot;avatar&quot;=&gt;&quot;yolan-vloeberghs.jpg&quot;, &quot;title&quot;=&gt;&quot;Java Consultant&quot;, &quot;email&quot;=&gt;&quot;yolan.vloeberghs@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Yolan Vloeberghs is a Java and Cloud engineer with a sharpened focus on all things related to cloud, specifically AWS. He loves to play around with various technologies and frameworks and is very passionated and eager to learn about everything related to cloud-native development.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2019-04-02-Kickstarter-Trajectory-2019-Light.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-07-10-Spring-IO-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-08-05-deploy-spring-boot-kubernetes.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-11-29-AWS-Dev-Day-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-05-07-jib.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-08-28-kubernetes-clients-comparison.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-12-10-aws-fargate-serverless-deployments.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-11-03-selenium-e2e-testing.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-06-13-aws-rds-iam-authentication-spring-boot.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-01-13-aws-lambda-snapstart-spring-cloud-function.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-05-17-kubecon-2023.md collection=posts&gt;]}</name><email>yolan.vloeberghs@ordina.be</email></author><category term="Cloud" /><category term="cloud" /><category term="conference" /><category term="cloud-native" /><category term="kubernetes" /><summary type="html"><![CDATA[Introduction]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-05-17-kubecon-2023/banner-resized.jpg" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-05-17-kubecon-2023/banner-resized.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Date and Time Testing</title><link href="https://ordina-jworks.github.io/testing/2023/04/28/date-time-testing.html" rel="alternate" type="text/html" title="Date and Time Testing" /><published>2023-04-28T00:00:00+00:00</published><updated>2023-04-28T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/testing/2023/04/28/date-time-testing</id><content type="html" xml:base="https://ordina-jworks.github.io/testing/2023/04/28/date-time-testing.html"><![CDATA[<p>Working with dates and times can be challenging for developers, especially regarding testing.
When testing code that involves the current date or time, it takes time to ensure that the results are correct and consistent.
Luckily, there are several ways to solve this problem.</p>

<p>One approach is to use a fixed date and time in your tests.
This ensures that your code produces consistent results, regardless of the actual date and time.
You can achieve this by mocking the <code class="language-plaintext highlighter-rouge">now()</code> method of the <code class="language-plaintext highlighter-rouge">LocalDateTime</code> class, which returns the current date and time.</p>

<p>Here’s an example of how to mock the <code class="language-plaintext highlighter-rouge">now()</code> method of the <code class="language-plaintext highlighter-rouge">LocalDateTime</code> class using Mockito Inline and Java:</p>

<pre><code class="language-pom.xml">&lt;dependency&gt;
    &lt;groupId&gt;org.mockito&lt;/groupId&gt;
    &lt;artifactId&gt;mockito-inline&lt;/artifactId&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span> <span class="o">(</span><span class="nc">MockedStatic</span><span class="o">&lt;</span><span class="nc">LocalDateTime</span><span class="o">&gt;</span> <span class="n">mockedStatic</span> <span class="o">=</span> <span class="nc">Mockito</span><span class="o">.</span><span class="na">mockStatic</span><span class="o">(</span><span class="nc">LocalDateTime</span><span class="o">.</span><span class="na">class</span><span class="o">))</span> <span class="o">{</span>
    <span class="n">mockedStatic</span><span class="o">.</span><span class="na">when</span><span class="o">(</span><span class="nl">LocalDateTime:</span><span class="o">:</span><span class="n">now</span><span class="o">).</span><span class="na">thenReturn</span><span class="o">(</span><span class="n">fixedDate</span><span class="o">);</span>

    <span class="c1">// Your code here.</span>
<span class="o">}</span>
</code></pre></div></div>

<p>In this code snippet, <code class="language-plaintext highlighter-rouge">fixedDate</code> is a <code class="language-plaintext highlighter-rouge">LocalDateTime</code> object representing the fixed date and time you want to use in your tests.
The <code class="language-plaintext highlighter-rouge">MockedStatic</code> class is a Mockito class that allows you to mock static methods.</p>

<p>To make this code more reusable, you can create a small method that accepts a <code class="language-plaintext highlighter-rouge">fixedDate</code> and a test in a <code class="language-plaintext highlighter-rouge">Runnable</code>.
This will help to improve your code significantly and make it more readable:</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">private</span> <span class="kt">void</span> <span class="nf">tryOn</span><span class="o">(</span><span class="nc">LocalDateTime</span> <span class="n">fixedDate</span><span class="o">,</span> <span class="nc">Runnable</span> <span class="n">test</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">try</span> <span class="o">(</span><span class="nc">MockedStatic</span><span class="o">&lt;</span><span class="nc">LocalDateTime</span><span class="o">&gt;</span> <span class="n">mockedStatic</span> <span class="o">=</span> <span class="nc">Mockito</span><span class="o">.</span><span class="na">mockStatic</span><span class="o">(</span><span class="nc">LocalDateTime</span><span class="o">.</span><span class="na">class</span><span class="o">))</span> <span class="o">{</span>
        <span class="n">mockedStatic</span><span class="o">.</span><span class="na">when</span><span class="o">(</span><span class="nl">LocalDateTime:</span><span class="o">:</span><span class="n">now</span><span class="o">).</span><span class="na">thenReturn</span><span class="o">(</span><span class="n">fixedDate</span><span class="o">);</span>
        <span class="n">test</span><span class="o">.</span><span class="na">run</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>You can then use this method in your tests to ensure that your code produces consistent results:</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tryOn</span><span class="o">(</span><span class="n">fixedDate</span><span class="o">,</span> <span class="o">()</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="c1">// Your code here.</span>
<span class="o">});</span>
</code></pre></div></div>

<p>Using a fixed date and time in your tests ensures that your code produces consistent results, regardless of the actual date and time.
This can help you to identify and fix bugs more quickly and ensure that your code works as expected in all scenarios.</p>

<p>In summary, when testing code that involves the current date and time, it’s essential to use a fixed date and time to ensure consistent results.
You can achieve this by mocking the <code class="language-plaintext highlighter-rouge">now()</code> method of the <code class="language-plaintext highlighter-rouge">LocalDateTime</code> class using Mockito and Java.
By using a small method like <code class="language-plaintext highlighter-rouge">tryOn</code>, you can make your code more reusable and easier to read.</p>

<p>A full code example</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">static</span> <span class="n">org</span><span class="o">.</span><span class="na">assertj</span><span class="o">.</span><span class="na">core</span><span class="o">.</span><span class="na">api</span><span class="o">.</span><span class="na">Assertions</span><span class="o">.</span><span class="na">assertThat</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.time.LocalDate</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.junit.jupiter.api.Test</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mockito.MockedStatic</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.mockito.Mockito</span><span class="o">;</span>

<span class="kd">class</span> <span class="nc">AgeCalculatorTest</span> <span class="o">{</span>

    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="nc">LocalDate</span> <span class="no">APRIL_27_2023</span> <span class="o">=</span> <span class="nc">LocalDate</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="mi">2023</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">27</span><span class="o">);</span>
    
    <span class="nd">@Test</span>
    <span class="kt">void</span> <span class="nf">testCalculateAgeWorksOnlyIn2023</span><span class="o">()</span> <span class="o">{</span>
        <span class="c1">// Arrange</span>
        <span class="nc">LocalDate</span> <span class="n">birthDate</span> <span class="o">=</span> <span class="nc">LocalDate</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="mi">1993</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">27</span><span class="o">);</span>

        <span class="c1">// Act</span>
        <span class="kt">int</span> <span class="n">actualAge</span> <span class="o">=</span> <span class="nc">AgeCalculator</span><span class="o">.</span><span class="na">calculateAge</span><span class="o">(</span><span class="n">birthDate</span><span class="o">);</span>

        <span class="c1">// Assert</span>
        <span class="kt">int</span> <span class="n">expectedAge</span> <span class="o">=</span> <span class="mi">30</span><span class="o">;</span>
        <span class="n">assertThat</span><span class="o">(</span><span class="n">actualAge</span><span class="o">).</span><span class="na">isEqualTo</span><span class="o">(</span><span class="n">expectedAge</span><span class="o">);</span>
    <span class="o">}</span>

    <span class="nd">@Test</span>
    <span class="kt">void</span> <span class="nf">testCalculateAgeWorksEveryYear</span><span class="o">()</span> <span class="o">{</span>
        <span class="c1">// Arrange</span>
        <span class="kd">final</span> <span class="nc">LocalDate</span> <span class="n">birthDate</span> <span class="o">=</span> <span class="nc">LocalDate</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="mi">1993</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">27</span><span class="o">);</span>

        <span class="k">try</span> <span class="o">(</span><span class="nc">MockedStatic</span><span class="o">&lt;</span><span class="nc">LocalDate</span><span class="o">&gt;</span> <span class="n">mockedStatic</span> <span class="o">=</span> <span class="nc">Mockito</span><span class="o">.</span><span class="na">mockStatic</span><span class="o">(</span><span class="nc">LocalDate</span><span class="o">.</span><span class="na">class</span><span class="o">))</span> <span class="o">{</span>
            <span class="n">mockedStatic</span><span class="o">.</span><span class="na">when</span><span class="o">(</span><span class="nl">LocalDate:</span><span class="o">:</span><span class="n">now</span><span class="o">).</span><span class="na">thenReturn</span><span class="o">(</span><span class="no">APRIL_27_2023</span><span class="o">);</span>

            <span class="c1">// Act</span>
            <span class="kt">int</span> <span class="n">actualAge</span> <span class="o">=</span> <span class="nc">AgeCalculator</span><span class="o">.</span><span class="na">calculateAge</span><span class="o">(</span><span class="n">birthDate</span><span class="o">);</span>

            <span class="c1">// Assert</span>
            <span class="kt">int</span> <span class="n">expectedAge</span> <span class="o">=</span> <span class="mi">30</span><span class="o">;</span>
            <span class="n">assertThat</span><span class="o">(</span><span class="n">actualAge</span><span class="o">).</span><span class="na">isEqualTo</span><span class="o">(</span><span class="n">expectedAge</span><span class="o">);</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="nd">@Test</span>
    <span class="kt">void</span> <span class="nf">testCalculateAgeWorksEveryYearUsingTryOn</span><span class="o">()</span> <span class="o">{</span>
        <span class="c1">// Arrange</span>
        <span class="kd">final</span> <span class="nc">LocalDate</span> <span class="n">birthDate</span> <span class="o">=</span> <span class="nc">LocalDate</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="mi">1993</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">27</span><span class="o">);</span>

        <span class="n">tryOn</span><span class="o">(</span><span class="no">APRIL_27_2023</span><span class="o">,</span> <span class="o">()</span> <span class="o">-&gt;</span> <span class="o">{</span>
            <span class="c1">// Act</span>
            <span class="kt">int</span> <span class="n">actualAge</span> <span class="o">=</span> <span class="nc">AgeCalculator</span><span class="o">.</span><span class="na">calculateAge</span><span class="o">(</span><span class="n">birthDate</span><span class="o">);</span>

            <span class="c1">// Assert</span>
            <span class="kt">int</span> <span class="n">expectedAge</span> <span class="o">=</span> <span class="mi">30</span><span class="o">;</span>
            <span class="n">assertThat</span><span class="o">(</span><span class="n">actualAge</span><span class="o">).</span><span class="na">isEqualTo</span><span class="o">(</span><span class="n">expectedAge</span><span class="o">);</span>
        <span class="o">});</span>
    <span class="o">}</span>

    <span class="kd">private</span> <span class="kt">void</span> <span class="nf">tryOn</span><span class="o">(</span><span class="nc">LocalDate</span> <span class="n">fixedDate</span><span class="o">,</span> <span class="nc">Runnable</span> <span class="n">test</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">try</span> <span class="o">(</span><span class="nc">MockedStatic</span><span class="o">&lt;</span><span class="nc">LocalDate</span><span class="o">&gt;</span> <span class="n">mockedStatic</span> <span class="o">=</span> <span class="nc">Mockito</span><span class="o">.</span><span class="na">mockStatic</span><span class="o">(</span><span class="nc">LocalDate</span><span class="o">.</span><span class="na">class</span><span class="o">))</span> <span class="o">{</span>
            <span class="n">mockedStatic</span><span class="o">.</span><span class="na">when</span><span class="o">(</span><span class="nl">LocalDate:</span><span class="o">:</span><span class="n">now</span><span class="o">).</span><span class="na">thenReturn</span><span class="o">(</span><span class="n">fixedDate</span><span class="o">);</span>

            <span class="n">test</span><span class="o">.</span><span class="na">run</span><span class="o">();</span>
        <span class="o">}</span>
    <span class="o">}</span>

    <span class="kd">static</span> <span class="kd">class</span> <span class="nc">AgeCalculator</span> <span class="o">{</span>

        <span class="kd">public</span> <span class="kd">static</span> <span class="kt">int</span> <span class="nf">calculateAge</span><span class="o">(</span><span class="nc">LocalDate</span> <span class="n">birthDate</span><span class="o">)</span> <span class="o">{</span>
            <span class="nc">LocalDate</span> <span class="n">currentDate</span> <span class="o">=</span> <span class="nc">LocalDate</span><span class="o">.</span><span class="na">now</span><span class="o">();</span>
            <span class="kt">int</span> <span class="n">age</span> <span class="o">=</span> <span class="n">currentDate</span><span class="o">.</span><span class="na">getYear</span><span class="o">()</span> <span class="o">-</span> <span class="n">birthDate</span><span class="o">.</span><span class="na">getYear</span><span class="o">();</span>
            <span class="k">if</span> <span class="o">(</span><span class="n">birthDate</span><span class="o">.</span><span class="na">getDayOfYear</span><span class="o">()</span> <span class="o">&gt;</span> <span class="n">currentDate</span><span class="o">.</span><span class="na">getDayOfYear</span><span class="o">())</span> <span class="o">{</span>
                <span class="n">age</span><span class="o">--;</span>
            <span class="o">}</span>
            <span class="k">return</span> <span class="n">age</span><span class="o">;</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Maarten&quot;, &quot;last_name&quot;=&gt;&quot;Casteels&quot;, &quot;permalink&quot;=&gt;&quot;/author/maarten-casteels/&quot;, &quot;avatar&quot;=&gt;&quot;maarten-casteels.png&quot;, &quot;title&quot;=&gt;&quot;Practice Lead Application Development&quot;, &quot;github&quot;=&gt;&quot;denmette&quot;, &quot;linkedin&quot;=&gt;&quot;maartencasteels&quot;, &quot;email&quot;=&gt;&quot;Maarten.Casteels@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Meet Maarten Casteels, a Practice Lead for Application Development at JWorks. With a passion for testing, he ensures that every code he creates is thoroughly tested before release. Outside of work, Maarten actively loves to bring people together.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2018-12-17-Devoxx-MA.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-04-28-date-time-testing.md collection=posts&gt;]}</name><email>Maarten.Casteels@ordina.be</email></author><category term="Testing" /><category term="testing" /><category term="unit testing" /><summary type="html"><![CDATA[Working with dates and times can be challenging for developers, especially regarding testing. When testing code that involves the current date or time, it takes time to ensure that the results are correct and consistent. Luckily, there are several ways to solve this problem.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-04-28-date-time-testing/blog-post-overlay.webp" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-04-28-date-time-testing/blog-post-overlay.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Intelligent Automation</title><link href="https://ordina-jworks.github.io/architecture/2023/02/20/intelligent-automation.html" rel="alternate" type="text/html" title="Intelligent Automation" /><published>2023-02-20T00:00:00+00:00</published><updated>2023-02-20T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/architecture/2023/02/20/intelligent-automation</id><content type="html" xml:base="https://ordina-jworks.github.io/architecture/2023/02/20/intelligent-automation.html"><![CDATA[<p>Already 4 years ago, <a href="https://evolute.be/reviews/bpmnext2019.html" target="_blank" rel="noopener noreferrer">Nathaniel Palmer’s keynote at bpmNext</a> introduced me to the concept of Intelligent Automation. 
This is the extension of the classical Process Management approach using Intelligent Business Process Management Solutions (iBPMS) to automate processes with the influx of new possibilities on a technological level: AI and machine learning to crunch the data, RPA, and the introduction of bots for automating swivel chair processes, and more pronounced use of decision management automation. 
When the <a href="https://bpm-books.com/products/intelligent-automation" target="_blank" rel="noopener noreferrer">titular book</a> was published by Future Strategies, I picked up my copy and started reading in the hopes of figuring out how to implement this. 
The book is similar in structure to other volumes of Future Strategies in that it is a collection of articles by luminaries in the field, accompanied by several award-winning case studies.</p>

<p>Not that the rise in importance of technologies such as AI should come as a surprise. 
Going back to 2016 the Artificial Intelligence Information Society as well as the World Bank offered up this infographic showing AI to be considered the 4th industrial revolution to shape our society.</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/industrialrevolutions.jpg" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<p>The questions that this new approach tries to answer rise from the underlying mismatch of traditional process automation with the current reality within organizations:</p>
<ol>
  <li>Process Modelling is still programming and thus does not alleviate the workload of the scarce resource that are developers.</li>
  <li>Office workplaces are no longer so static that they can easily be contained in a process model.</li>
  <li>Case Management Modeling Notation (CMMN) cannot fully answer this dynamic workplace either.</li>
  <li>Processes do not lend themselves naturally to a centralized consistent view of the organization.</li>
  <li>Maintaining the agreement on an agreed-upon process over time is difficult.</li>
</ol>

<p>Intelligent Automation is one of the latest attempts to efficiently answer digital disruption. 
This disruption is brought about by new technologies that emerge and upset the balance of any business ecosystem. 
We need to answer it with a digital transformation of the current landscape in organizations, and intelligent automation aims for this lofty goal by enhancing the traditional business process management discipline with process automation technology, guided by business rules and bots (both RPA and AI). 
The way to successfully monitor whether the path taken will lead to success is to oversee this process automation with the proper information governance.</p>

<p>Intelligent Automation thus plays on at least four aspects of the organization:</p>
<ol>
  <li>Adding intelligence to (operational) processes.</li>
  <li>Augmenting decision-making within a process through analytics.</li>
  <li>Monitoring processes for correct operation and adapting them to meet changes in the strategic or tactical direction of the organization.</li>
  <li>Applying intelligent automation and analytics to strategic and tactical decision-making.</li>
</ol>

<p>The introduction of bots on top of the typical iBPMS is rooted in the idea that human interaction in processes is still needed at certain times, and if our RPA bots can use the same interfaces as their human counterparts, there would be an efficiency gain by alleviated work from knowledge workers and a cost gain by eliminating the need to implement specific interfaces for these bots. 
It is however imperative that these bots know exactly what to do, and thus the same rules that guide human participants in the process need to apply to these bots. 
In addition to this, the same level of transparency (such as for audit or privacy purposes) should also apply to the bots. 
This way correct optimization of the process automation can be determined.</p>

<p>These different components are aptly assembled in Nathaniel Palmer’s vision for future process automation solutions:</p>
<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/ibpms.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<p>The three guiding principles are Rules, Robots, and Relationships. 
The Robot part of the equation was already tackled earlier in this post. 
The Relationship part focuses mainly on the data used in the processes. 
There is a shift here from the problems of storing and replication data locally towards the problems of where to get data in the wide world. 
The different pieces of the information puzzle could be spread across dozens of repositories all over the world, and although our complete information metamodel still exists, its parts need to be fetched from each of these repositories to complete any sort of 360° view.</p>

<p>Rules are formed by automating decisions and letting AI solutions approach the available information with machine learning to perform both analytical and predictive analyses. 
This isn’t magic, however, and these AI need to be properly trained using historical data to perform with any level of adequacy. 
These bots were dubbed probabilistic as opposed to the deterministic nature of RPA bots.</p>

<p>With all these different angles to process automation, it might look like a mire of patchwork islands. 
But this is where the traditional iBPMS excels, bridging these islands to arrive at an end-to-end process, managing the sequencing, and overseeing the state of running processes. 
This eventually leads to the promise of intelligent automation: Expanding the efficiency of automation while delivering greater transparency and policy compliance. 
And herein lies the business value of this proposition. 
If traditional BPMS automation proves too rigid for the task, consider splitting up the end-to-end process into process fragments to become more flexible. 
Recreating this end-to-end process from these fragments can be done by structuring them through a case that indicates the major phases in the process. 
As such these fragments become once again linked, but still retain a dynamic and adaptable nature. 
This is the <a href="https://www.youtube.com/watch?v=uSQVtm8O7SA&amp;t=1s&amp;ab_channel=bpmNEXT%3ADefiningtheNextGenerationofProcessInnovation" target="_blank" rel="noopener noreferrer">intentional process</a> as posited by the people from Flowable.</p>

<p>It is easy to have initiatives for intelligent automation derail, but here we can apply the same lessons we learned for process management adoption:</p>
<ol>
  <li>Make everyone, on every level, in the organization aware of the benefits and how it will help them in their jobs, not replace them. It alleviates tedious repetitive work and frees up time for employees to pursue more worthwhile endeavors within their organization.</li>
  <li>Think Big, Start Small. It is okay to have a grandiose goal in mind. But starting with a simple proof of concept to show quick results goes a long way to prepare for more complex processes and garner goodwill from those involved.</li>
  <li>If at all possible, get expert help for those first few initiatives to build up expertise within the organization.</li>
  <li>Get intimately acquainted with the data assets available to your processes, in the organization, and external data. What they are, in which format they exist, and their availability. This understanding will facilitate the analytics involved.</li>
</ol>

<h2 id="robots">Robots</h2>

<p>When it comes to robots, RPA springs to mind. 
But it is not the only game in town. 
Automation can also be handed over to an AI, which can determine things like the shortest path (think of your GPS) and other non-trivial decisions. 
The illustration below showcases the most common attributes of both these types of robots.</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/rpavsai.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Taken from the <a href="https://www.bcg.com/publications/2017/technology-digital-operations-powering-the-service-economy-with-rpa-ai?linkId=39429569" target="_blank" rel="noopener noreferrer">Boston Consulting Group Website</a></p>

<p>Looking at the work automation spectrum, we can easily pinpoint where RPA and AI can lend a hand to the classic way of process automation. 
We can also determine where knowledge workers will have the biggest added value. 
These business experts have accumulated an immeasurable wealth of knowledge and expertise that constantly faces an inevitable expiration date (be it pensions or even turnover). 
Safeguarding this knowledge through proper knowledge management often is a set of processes on its own to which intelligent automation can once again be applied.</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/rpa-ai-balance.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<p>For RPA, the biggest gains will be attained by using the robots as a stepping stone to digital maturity in your process automation. 
They are tools to be used to quickly replace error-prone human tasks and capitalize on the quick gains this provides. 
They can also be utilized to quickly set up proof-of-concept endeavors to prove added value derived from ideation. 
Once these steps in the automated process reach the proper level of maturity, they can then more easily be replaced by technologies that trade in the hyper agility RPA provides for more grip on the governance and maintainability that API solutions offer. 
Camunda specified a three-step approach along those lines in their 2020 White Paper titled <a href="https://camunda.com/blog/2021/01/beyond-rpa-how-to-build-toward-end-to-end-process-automation/" target="_blank" rel="noopener noreferrer">“Beyond RPA: How to Build Toward End-to-End Process Automation”</a>. 
These steps are the following:</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/rpause.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<p>A big part of the added value realized by introducing AI robots into the process automation toolbox is knowing where and when to use them. 
Predictive models can help decisioning in organizations, but there are some points to keep in mind:</p>
<ul>
  <li>At its core, effective decisioning should not only answer the “What is Happening?” question, but also the “Why is it happening?”-question. This understanding of the reasons behind information leads to better opportunity detection and initiatives.</li>
  <li>Risk/Reward considerations should be available with each option of the decisions.</li>
  <li>Even with decisions where there are ambiguous or even unknown factors, or where the risk/reward cannot be determined, there is a need to automate, and this is where deep learning/machine learning models can make the difference.</li>
</ul>

<p>The types of tasks that RPA and AI can alleviate are then:</p>
<ul>
  <li>Tasks that are easy to do when there are few, but become cumbersome in large amounts.</li>
  <li>Tasks that are not easy for humans to perform, such as high-speed or high-complexity algorithms.</li>
  <li>Tasks that require the interpretation and/or parsing of large amounts of knowledge (such as a volume of laws or a medical database).</li>
</ul>

<p>Although the degree of independence with which robots can execute these tasks varies widely from case to case. 
Most of these will still require some human interaction to conclude them. 
But it is clear that robot-assisted work has a great benefit over not using robots to aid you in performing knowledge work.</p>

<p>The components that can assist in tackling these points are the following:</p>
<ul>
  <li>Data Integration Agents: These components integrate disparate data sources into a common data repository environment with a focus on cataloging, constructing ontologies, setting up multi-dimensional discovery, domain model creation, and a self-learning data search interface.</li>
  <li>Analytics Agents: These components perform cognitive analysis of available data, correlating and determining data relationships.</li>
  <li>Visualization Agents: As the name suggests, these components allow for the construction of visualizations and support for derived information gathering such as natural language processing, speech recognition, and customer satisfaction analysis.</li>
</ul>

<h2 id="rules--relationships">Rules &amp; Relationships</h2>

<p>IT is all about the data. 
And with the introduction of machine learning, the need for this data to be structured has dropped significantly. 
Traditional BPM tells us that data/information steers the process. 
It decides which route is taken through its execution. 
But with unstructured data in the mix, this becomes a lot harder. 
As such, interpreting this data correctly has become a science. 
Data science has become a de facto foundation for every digital transformation effort. 
When we speak of intelligent business processes, the role data plays has outgrown the classic summaries and reports delivered to the process manager and has taken on the form of the process able to dynamically digest and process an ever-changing data set. 
This data set needs to combine both the transactional process data and the integration-based business data to leverage results in correct business decisions.</p>

<p>The three data pillars for process data were stipulated in Winkler &amp; Kay’s 2019 article ‘Macro Evolution of BPM Data’, and are the following:</p>
<ol>
  <li>Historical Process Data</li>
  <li>Run-time Patterns</li>
  <li>AI/ML-based Forecasting</li>
</ol>

<h3 id="historical-process-data">Historical Process Data</h3>

<p>The obvious data associated with processes is the statistical representation of process behavior (who did what how many times). 
Combined with the effect of business process results on the ongoing business, it shows a clear route on where to finetune and improve our processes. 
This is the bread and butter of the process analyst. 
An example of what this might entail can be seen below:</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/relevant-data-01.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Business and Process Data analytics example in BPM, Winkler, Kay; 2019</p>

<h3 id="run-time-patterns">Run-time Patterns</h3>

<p>Performance metrics add to the previous category of process data. 
More difficult to determine, these numbers underpin most business cases and justifications for BPM initiatives. 
They can be leveraged to determine the Return-on-Investment (ROI), and help to pinpoint the bottlenecks in known business processes.</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/relevant-data-02.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Task Cycle Time; Winkler, Kay; 2019</p>

<h3 id="aiml-based-forecasting">AI/ML-based Forecasting</h3>

<p>Applying Machine Learning to process data is not undertaken lightly. 
It requires a certain level of maturity in both historical process data and run-time patterns before attempting to extract actionable intelligence this way. 
The payout is worth the while, even with the sometimes overwhelming amount of data and information to sift through. 
Gathering all the information avenues into a dedicated repository gives the option for broader correlational analytics (such as time series and cross-sectional investigations). 
Process Mining will amp up your process optimization game and indicate new avenues to explore for such optimizations.</p>

<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/relevant-data-03.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Stages of Process Analytics; Winkler, Kay</p>

<p>If this still does not give the appropriate support for efficiency and effectiveness, applying some frameworks such as Figure of Merit Matrices (FOMM) to increase effectiveness and Resource Allocation, Leveling and Balancing (RALB) to increase efficiency can assist you to achieve the goals your organization has set out.</p>

<h2 id="putting-it-all-together">Putting it all Together</h2>

<p>When leveraging all the tools of intelligent automation, there are concerns as to which goals contradict each other. 
Just like in corporate strategy, you cannot full-out go for both operational excellence (typically by standardizing to a black Ford Model T) and customer intimacy (with a car that is tailor-made to one unique individual), there are trade-offs and balances to be weighed. 
For automation in general these are:</p>
<ul>
  <li>Digital Innovation Speed: Accelerated development for automation using the latest tools and techniques.</li>
  <li>Digital Competency Best Practices: Going for controlled automation with proven technologies, frameworks, and best practices.</li>
</ul>

<p>And just as with corporate strategy, this balance can shift over time, maybe even with more agility. 
For this, a Center of Excellence (CoE) dedicated to the art of automation is a life boon. 
If you are curious about how to correctly set up such an entity, look at <a href="https://evolute.be/reviews/reimaginemgmt.html" target="_blank" rel="noopener noreferrer">my review of Roger Tregear’s book “Reimagining Management”</a>. 
The 7 Enablers approach in this book will give you a clear path forward on how to achieve this on a corporate level. 
But on a  process level, your considerations are business enablement, continuous review of its performance (for example by applying Lean Six Sigma practices), proper governance, and re-use.</p>

<p>If we map the benefits to the individual tools and best practices we get the following conclusions:</p>
<ul>
  <li>Classic iBPMS combined with RPA will give us the tools to streamline processes, enable straight-through processing, and frees up time from knowledge workers that can be spent on more value-adding and less repetitive activities.</li>
  <li>AI allows for skill-based routing, faster response times to customers in straightforward requests, and support for decisioning (next best step analysis).</li>
  <li>As expected re-use and standardization help with cost reduction while still allowing specialization in those areas that benefit the most from customization and variety in possibilities.</li>
  <li>Cloud adoption will assist in covering security concerns and redundancy needs to ensure business continuity. It also allows for closer matching of IT resources to the consumption of these services, so that financial gains can be achieved in this way (much in the same way as re-use and standardization would).</li>
</ul>

<h2 id="pitfalls-of-intelligent-automation">Pitfalls of Intelligent Automation</h2>

<p>There are also risks associated with any architectural trade-off. 
Here we will list some of the pitfalls the case studies in the book mention. 
These should be detected when present in these types of projects and proper mitigations should be devised.</p>
<ul>
  <li>The obvious one for each of these types of initiatives: Think big, but start small!</li>
  <li>Process Automation should never be an IT story, but should be embraced by business people as well to have it be successful. All parties involved should work towards continuous improvement. This includes upper management. A business sponsor for these initiatives should bridge the gap on this account.</li>
  <li>Connections make up the brunt of the complexity associated with IT initiatives. Make sure your solutions are robust and can deal with connections not always available.</li>
  <li>You should not automate processes without the proper control and measuring tools to follow them up. Working in the dark leaves you blind to problems and opportunities for improvement.</li>
  <li>Don’t try to come up with a better wheel. ‘Search for a Commercial-off-the-Shelf component before you Automate’ should be a mantra in process thinking.</li>
  <li>Don’t try to create complete comprehensive processes from the beginning. You won’t be able to capture all requirements in the first iteration. Build your solution for adaptability to embrace new requirements as they become known.</li>
</ul>

<p>The main realization to make is to determine the maturity of your organization in four distinct areas: data, training, deployment, and management. 
In each of these areas, an increase in maturity will yield additional benefits as shown in the table below:</p>
<p style="text-align: center;"><img src="/img/2023-02-20-intelligent-automation/mlmaturity.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>The book focuses much of its content on the analytics and data sciences part of the intelligent automation ecosystem. 
While this gives the reader insights into how this field plays an important role, it left me with an unsatisfied hunger with regard to all other disciplines that are in play when attempting the initiatives needed to successfully roll out intelligent automation adoption in organizations.</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Peter&quot;, &quot;last_name&quot;=&gt;&quot;De Kinder&quot;, &quot;github&quot;=&gt;&quot;peterdekinder&quot;, &quot;linkedin&quot;=&gt;&quot;peterdekinder&quot;, &quot;permalink&quot;=&gt;&quot;/author/peterdekinder/&quot;, &quot;avatar&quot;=&gt;&quot;peterdekinder.jpg&quot;, &quot;title&quot;=&gt;&quot;Solution Architect&quot;, &quot;email&quot;=&gt;&quot;peter.dekinder@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Peter is a Solution Architect with firm roots in the Java technosphere, but with a wide interest in all things architecture. His areas of specialization include Service Oriented Architectures, Business Process Management and Security.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2020-01-28-Architecture-in-Projects.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-03-24-Charting-non-functionals.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-07-08-Book-Five-Dysfunctions.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-11-25-Quite-The-Story.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-01-04-Designing-REST-services.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-02-17-Out-With-The-Old.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-01-14-what-would-discord-bot-do.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-06-13-connecting-the-pods.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-10-24-ode-to-join.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-02-20-intelligent-automation.md collection=posts&gt;]}</name><email>peter.dekinder@ordina.be</email></author><category term="Architecture" /><category term="architecture" /><category term="software architecture" /><summary type="html"><![CDATA[Already 4 years ago, Nathaniel Palmer’s keynote at bpmNext introduced me to the concept of Intelligent Automation. This is the extension of the classical Process Management approach using Intelligent Business Process Management Solutions (iBPMS) to automate processes with the influx of new possibilities on a technological level: AI and machine learning to crunch the data, RPA, and the introduction of bots for automating swivel chair processes, and more pronounced use of decision management automation. When the titular book was published by Future Strategies, I picked up my copy and started reading in the hopes of figuring out how to implement this. The book is similar in structure to other volumes of Future Strategies in that it is a collection of articles by luminaries in the field, accompanied by several award-winning case studies.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-02-20-intelligent-automation/header.jpg" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-02-20-intelligent-automation/header.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Accelerating your slow Java Lambda with AWS Lambda SnapStart</title><link href="https://ordina-jworks.github.io/cloud/2023/01/13/aws-lambda-snapstart-spring-cloud-function.html" rel="alternate" type="text/html" title="Accelerating your slow Java Lambda with AWS Lambda SnapStart" /><published>2023-01-13T00:00:00+00:00</published><updated>2023-01-13T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/cloud/2023/01/13/aws-lambda-snapstart-spring-cloud-function</id><content type="html" xml:base="https://ordina-jworks.github.io/cloud/2023/01/13/aws-lambda-snapstart-spring-cloud-function.html"><![CDATA[<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#what-is-snapstart">What is SnapStart?</a>
    <ul>
      <li><a href="#versions">Versions</a></li>
      <li><a href="#pricing">Pricing</a></li>
      <li><a href="#limitations">Limitations</a>
        <ul>
          <li><a href="#uniqueness">Uniqueness</a></li>
          <li><a href="#networking">Networking</a></li>
          <li><a href="#ephemeral-data">Ephemeral data</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#using-snapstart">Using SnapStart</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>If you use <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda</a> in combination with Java runtimes, you will notice (or probably have already noticed) that one of the main setbacks is the cold start time.
A cold start refers to the process where a Lambda is invoked for the first time and the Lambda has to be initialized.
AWS needs to create a new function instance and spin it up with every <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html#runtimes-lifecycle-ib" target="_blank" rel="noopener noreferrer">initialization</a>.</p>

<p>Depending on your environment and application size, it can take up to 10 seconds to complete the init phase.
Especially when using frameworks such as Spring Boot where features like dependency injection and component scanning can take a lot of time to initialize.
This is a delay that most want to avoid as it significantly slows down your application flow in some situations.
<strong>Do mind</strong> that this is only during the init phase; once the Lambda instance is running, the cold start process is over until the next time your Lambda needs to be instantiated again.</p>

<p style="text-align: center;"><img src="/img/2022-12-23-aws-lambda-snapstart-spring-cloud-function/lambda-execution-lifecycle.png" alt="Lambda execution lifecycle" class="image fit" style="margin:0px auto; max-width:100%" />
<em>Lambda execution environment lifecycle - without SnapStart - <a href="https://www.youtube.com/watch?v=dnFm6MlPnco" target="_blank" rel="noopener noreferrer">Best practices of advanced serverless developers (AWS re:Invent 2021)</a></em></p>

<p>AWS has always recognized the problem and now comes with a solution called <a href="https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html" target="_blank" rel="noopener noreferrer">Lambda SnapStart</a>.</p>

<h2 id="what-is-snapstart">What is SnapStart?</h2>
<p>Introduced at AWS re:Invent 2022, AWS <a href="https://docs.aws.amazon.com/lambda/latest/dg/snapstart.html" target="_blank" rel="noopener noreferrer">Lambda SnapStart</a> is the newest feature to eliminate the cold start problem by initializing the function when you publish a new version of a Lambda.
It takes a snapshot, through <a href="https://firecracker-microvm.github.io/" target="_blank" rel="noopener noreferrer">Firecracker</a> which AWS uses to run Lambda and Fargate, encrypts and caches it so it can be instantly accessed whenever it is required.
When a Lambda is invoked and needs to set up a new instance, it will simply use the cached snapshot, which greatly improves startup times (officially up to 10x).</p>

<h3 id="versions">Versions</h3>
<p>By default, SnapStart is disabled.
You can enable it, but only for published Lambda versions.
This means that it only works for versions that are published on the AWS account and that it is not implemented on the $LATEST tag. If you want to make use of Lambda SnapStart, be sure to do so on a published version.
The snapshot of your Lambda is created upon the version publishing process.</p>

<p style="text-align: center;"><img src="/img/2022-12-23-aws-lambda-snapstart-spring-cloud-function/snapstart-overview.png" alt="SnapStart overview" class="image fit" style="margin:0px auto; max-width:100%" />
<em>SnapStart overview - snapshot gets created during version publishing - <a href="https://www.youtube.com/watch?v=ZbnAithBNYY" target="_blank" rel="noopener noreferrer">AWS Lambda SnapStart (AWS re:Invent 2022)</a></em></p>

<h3 id="pricing">Pricing</h3>
<p>The SnapStart feature comes with AWS Lambda and has no additional pricing.</p>

<h3 id="limitations">Limitations</h3>
<p>While SnapStart is a great feature and can save time in Lambda cold starts, it also comes with its limitations.
SnapStart currently does not support the following features and services:</p>
<ul>
  <li><a href="https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html" target="_blank" rel="noopener noreferrer">provisioned concurrency</a></li>
  <li>arm64 architecture</li>
  <li>the <a href="https://docs.aws.amazon.com/lambda/latest/dg/runtimes-extensions-api.html" target="_blank" rel="noopener noreferrer">Lambda Extensions API</a></li>
  <li><a href="https://aws.amazon.com/efs/" target="_blank" rel="noopener noreferrer">EFS</a></li>
  <li><a href="https://aws.amazon.com/xray/" target="_blank" rel="noopener noreferrer">X-Ray</a></li>
  <li>Ephemeral storage up to 512 MB</li>
  <li>Limited to Java 11 runtime</li>
</ul>

<h4 id="uniqueness">Uniqueness</h4>
<p>SnapStart always requires your snapshot to be unique. 
This means that if you have initialization code that generates unique content, it might not always be unique in the snapshot once it is restored in other Lambda invocations.
The goal is to generate this content after the initialization process, so it is not part of the snapshot.
Luckily, AWS has provided a <a href="https://docs.aws.amazon.com/lambda/latest/dg/snapstart-uniqueness.html" target="_blank" rel="noopener noreferrer">documentation page</a> in which they provide best practices on how to tackle that problem.
They even came up with a <a href="https://github.com/aws/aws-lambda-snapstart-java-rules" target="_blank" rel="noopener noreferrer">SpotBugs plugin</a> which finds potential issues in your code that could prevent SnapStart from working correctly.</p>

<h4 id="networking">Networking</h4>
<p>Network connections are not being shared across different environments.
Thus, if network connections (for example, to other AWS services such as an RDS or SQS) are instantiated in the initialization phase, they will not be shared and will most likely fail when the snapshot is being used later again.
Although most popular frameworks have automatic database connection retries, it is worth the time to make sure that it works correctly.</p>

<h4 id="ephemeral-data">Ephemeral data</h4>
<p>Data that is fetched or temporary (for example a password or secret) should be fetched after the initialization phase.
Otherwise, it will save the secret in the snapshot, meaning that authentication failures (and security risks) might occur once the initial secret value has expired or has been changed.</p>

<h2 id="using-snapstart">Using SnapStart</h2>
<p>To investigate the improvement in cold start execution time when using AWS Lambda SnapStart, we wrote a simple Lambda function in Java 11 using <a href="https://spring.io/projects/spring-cloud-function" target="_blank" rel="noopener noreferrer">Spring Cloud Function</a>.
This Lambda function, when invoked, will retrieve some JSON data from a <a href="https://dummyjson.com/" target="_blank" rel="noopener noreferrer">dummy REST API</a> and return it to the user.
The code can be found on <a href="https://github.com/ordina-jworks/aws-lambda-snapstart-spring-boot" target="_blank" rel="noopener noreferrer">Github</a>.</p>

<p>We made use of the <a href="https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html" target="_blank" rel="noopener noreferrer">AWS Serverless Application Model (SAM)</a> to build our Lambda function and deploy it to AWS.
Enabling the SnapStart feature can be easily done by adding the following two lines to the <code class="language-plaintext highlighter-rouge">Properties</code> section of the Lambda function resource in the <code class="language-plaintext highlighter-rouge">template.yaml</code> file used by AWS SAM:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SnapStart:
  ApplyOn: PublishedVersions
</code></pre></div></div>

<p>We started by invoking our Lambda function’s unpublished version ($LATEST), in which case SnapStart is not used, and received the following summary from AWS:</p>

<p><img src="/img/2022-12-23-aws-lambda-snapstart-spring-cloud-function/lambda-cold-start.png" alt="Summary lambda without SnapStart" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<p>We can observe an <strong>Init duration</strong> of around 2.7s, i.e. the time that is spent initializing the execution environment for our Lambda function.</p>

<p>Next, we manually published a new version of our Lambda function using the AWS Console.
This can be done by navigating to the <em>Versions</em> tab of our Lamdba function and pressing the <em>Publish new version</em> button.</p>

<p style="text-align: center;"><img src="/img/2022-12-23-aws-lambda-snapstart-spring-cloud-function/lambda-versions.png" alt="Lambda function verions" class="image fit" style="margin:0px auto; max-width:100%" />
<em>Versions tab listing all published versions of a Lambda function.</em></p>

<p>Invoking this newly published version provides us with the following summary:</p>

<p><img src="/img/2022-12-23-aws-lambda-snapstart-spring-cloud-function/lambda-snapstart.png" alt="Summary lambda with SnapStart" class="image fit" style="margin:0px auto; max-width:100%" /></p>

<p>In this case, we can see SnapStart is used.
The initialization of the execution environment, represented by the <strong>Init duration</strong> we saw earlier, now happens when publishing the new version.
Only the restoration of the snapshot, represented by the <strong>Restore duration</strong>, is performed now.</p>

<p>It is quite clear that using Lambda SnapStart is advantageous in most cases.
We managed to decrease the cold start execution time of our Lambda function from almost 5s (<strong>Init duration</strong> + <strong>Duration</strong>) to around 2.6s (<strong>Restore duration</strong> + <strong>Duration</strong>), just by enabling this feature.</p>

<h2 id="conclusion">Conclusion</h2>
<p>SnapStart is a great feature and can save a lot of time in your application flow.
It’s a feature that should have been present already as it comes a bit too late. 
But now that it is here, Java developers should take measures in order to implement this as it can save a lot of time in cold-starting their Java Lambdas.
We would have liked to see it implemented by default when you create a version, but sadly, this is not the case (yet).
It comes only for Java, which is understandable as Java Lambdas face this obstacle the most. 
Still, we certainly won’t be surprised if AWS decides to release this feature for other languages and/or frameworks.</p>

<p>Altogether we can definitely recommend using this new feature for your Java Lambdas.</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Yolan&quot;, &quot;last_name&quot;=&gt;&quot;Vloeberghs&quot;, &quot;linkedin&quot;=&gt;&quot;yolan-vloeberghs-23825aa3&quot;, &quot;github&quot;=&gt;&quot;yolanv&quot;, &quot;permalink&quot;=&gt;&quot;/author/yolan-vloeberghs/&quot;, &quot;avatar&quot;=&gt;&quot;yolan-vloeberghs.jpg&quot;, &quot;title&quot;=&gt;&quot;Java Consultant&quot;, &quot;email&quot;=&gt;&quot;yolan.vloeberghs@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Yolan Vloeberghs is a Java and Cloud engineer with a sharpened focus on all things related to cloud, specifically AWS. He loves to play around with various technologies and frameworks and is very passionated and eager to learn about everything related to cloud-native development.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2019-04-02-Kickstarter-Trajectory-2019-Light.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-07-10-Spring-IO-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-08-05-deploy-spring-boot-kubernetes.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-11-29-AWS-Dev-Day-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-05-07-jib.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-08-28-kubernetes-clients-comparison.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-12-10-aws-fargate-serverless-deployments.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-11-03-selenium-e2e-testing.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-06-13-aws-rds-iam-authentication-spring-boot.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-01-13-aws-lambda-snapstart-spring-cloud-function.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-05-17-kubecon-2023.md collection=posts&gt;]}</name><email>yolan.vloeberghs@ordina.be</email></author><category term="Cloud" /><category term="aws" /><category term="lambda" /><category term="snapstart" /><category term="cloud" /><category term="spring" /><category term="java" /><summary type="html"><![CDATA[Introduction What is SnapStart? Versions Pricing Limitations Uniqueness Networking Ephemeral data Using SnapStart Conclusion]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2023-01-13-aws-lambda-snapstart-spring-cloud-function/header.png" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2023-01-13-aws-lambda-snapstart-spring-cloud-function/header.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Keeping up with dependencies like a boss</title><link href="https://ordina-jworks.github.io/cloud/2022/10/27/renovate.html" rel="alternate" type="text/html" title="Keeping up with dependencies like a boss" /><published>2022-10-27T00:00:00+00:00</published><updated>2022-10-27T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/cloud/2022/10/27/renovate</id><content type="html" xml:base="https://ordina-jworks.github.io/cloud/2022/10/27/renovate.html"><![CDATA[<blockquote>
  <p>Anything worth doing twice is worth automation.</p>
</blockquote>

<ul>
  <li><a href="#whats-the-problem">What’s the problem?</a></li>
  <li><a href="#what-is-mend-renovate">What is Mend Renovate?</a></li>
  <li><a href="#behind-the-curtains">Behind the curtains</a></li>
  <li><a href="#how-to-setup">How to setup</a></li>
  <li><a href="#tips-and-tricks">Tips and tricks</a></li>
  <li><a href="#why-should-i-use-it">Why should I use it?</a></li>
  <li><a href="#links">Links</a></li>
</ul>

<h2 id="whats-the-problem">What’s the problem?</h2>

<h3 id="lack-of-features">Lack of features</h3>

<p>Let’s imagine you need to implement support for a new feature.
Let’s imagine that that feature is super easy to implement due to almost native support for the functionality in a library you’re already using. 
That sounds like a great day, right? 
You add the code to glue together the API and the library, perform some tests and call it a day. 
There is just one problem, you didn’t check which version of the dependency you were using and the feature you need is only supported in versions 8.x.x and beyond. 
You check your <code class="language-plaintext highlighter-rouge">pom.xml</code>, only to figure out that you’re on version <code class="language-plaintext highlighter-rouge">6.8.21</code>.
Now you have two options, refactor 25% of your codebase to be able to use the new library or 5x your effort for implementing the feature without the support of the library.
That doesn’t sound like a great day at the (home-)office, now does it?</p>

<p>Wouldn’t be great to have been on version <code class="language-plaintext highlighter-rouge">8.1.2</code> already?</p>

<h3 id="dependent-systems">Dependent systems</h3>

<p>Let’s imagine another scenario.
You’re using a library to connect to an Elasticsearch cluster that’s provided by another team. 
Your application is running nicely in production and you’re steadily adding features to the codebase.
The end of the year approaches and with that, the pressure to deliver the final set of promised features increases.
All of a sudden, your application starts throwing all kinds of errors in your development environment. 
After some stressful debugging, you figure out that the Elasticsearch cluster has been upgraded to the latest version by the other team. 
Furiously, you open up your mailbox and look for an email about the upgrade. 
Of course, you find the email dating back four weeks where the team announced that they will start upgrading this week for development and do production in two weeks.</p>

<p>Since it’s a shared system, you can’t fault the other team, nor can you halt their upgrade path. 
So the only way forward is to upgrade the library. 
You open up your <code class="language-plaintext highlighter-rouge">pom.xml</code> file only to discover that you’re already 2 major versions behind. 
Upgrading will take ages since your code depends on previously deprecated and by now removed API support.</p>

<p>That sounds like a lot of overtime, stress and/or missed deadlines for the end of the year, doesn’t it…</p>

<h3 id="security-vulnerability">Security Vulnerability</h3>

<p>Now we get to a scenario that maybe 50% of the Java community experienced at the end of 2021.
A severe security vulnerability was discovered in a very popular logging library: Log4J. 
Normally, vulnerabilities aren’t this severe and there aren’t part of the nine o’clock news.
Now imagine that you are using a vulnerable software component and you aren’t informed by the news that you need to urgently update, how would you know about the vulnerability? 
Maybe you have some scanning software that checks for know CVEs?
Maybe you have really good developers that are subscribed to the mailing lists of all dependencies they’re using?
Or more likely than not, you just don’t know you’re vulnerable. 
OWASP identified <code class="language-plaintext highlighter-rouge">Vulnerable and outdated components</code> as number six on their <code class="language-plaintext highlighter-rouge">Top 10 Web Application Security Risks</code> of 2021.</p>

<p>Wouldn’t it have been nice to have a PR on every repository that had the Log4J dependency with the new version updated? 
So that you only had to merge that PR to mitigate the vulnerability?</p>

<h2 id="what-is-mend-renovate">What is Mend Renovate?</h2>

<p><a href="https://www.mend.io/free-developer-tools/renovate/" target="_blank" rel="noopener noreferrer">Mend Renovate</a> (formerly known as WhiteSource Renovate) is a tool that detects dependencies in your code and informs you about new versions of your dependencies.
It’s a free (at the time of writing) tool that can be used as a GitHub App or as a self-hosted tool. 
Renovate scans a repository and detects the used dependencies, relying on dependency managers.
As of time of writing, Renovate supports about <a href="https://docs.renovatebot.com/modules/manager/" target="_blank" rel="noopener noreferrer">80 different dependency management</a> systems out of the box.
Next, it can integrate with your code repositories (e.g. Bitbucket, Github, Gitlab, …) and automatically update the dependency in your code. 
It can create pull requests (PR) for every update it detects and even adds changelog (if available) information to the PR. 
This is especially helpful if there are (manual) changes required to use the new version of the dependency.
So Renovate can <code class="language-plaintext highlighter-rouge">detect</code> your dependencies, <code class="language-plaintext highlighter-rouge">inform</code> you about the updates and even tell you <code class="language-plaintext highlighter-rouge">how</code> to update.</p>

<h2 id="behind-the-curtains">Behind the curtains</h2>

<p>Now, how does this magic work?</p>

<p>Renovate starts by scanning your source code for dependencies.
It does this by looking for dependency files in your repository, let’s take the file structure below as an example.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>repo/ 
├─ src/ 
├─ Dockerfile 
├─ pom.xml
</code></pre></div></div>

<p>Renovate will detect the Dockerfile and the Maven dependency file (<code class="language-plaintext highlighter-rouge">pom.xml</code>).
It will pass these files to the dependency managers internally and detect which dependencies are being used.
Let’s imagine that the Dockerfile has the following <code class="language-plaintext highlighter-rouge">FROM</code> line at the top: <code class="language-plaintext highlighter-rouge">FROM amazoncorretto:18.0.0</code>
The dependency manager for docker will detect that this is the repository <code class="language-plaintext highlighter-rouge">amazoncorretto</code> with tag <code class="language-plaintext highlighter-rouge">18.0.0</code> on Docker Hub.
Next, it will look in the <code class="language-plaintext highlighter-rouge">pom.xml</code>.</p>

<p>Excerpt of the pom.xml:</p>
<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.logging.log4j<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>log4j-core<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>2.12.2<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div></div>
<p><strong>Note: Never use this dependency! This is a vulnerable version of log4j-core.</strong></p>

<p>Renovate will detect the following Maven dependency: <code class="language-plaintext highlighter-rouge">org.apache.logging.log4j:log4j-core:2.12.2</code></p>

<p>Next, Renovate will check for new versions of those dependencies. 
By default, Renovate will use the central Apache Maven Repository to check for newer versions of Maven dependencies. 
For Docker images, Docker Hub is used by default.
Like almost everything in Renovate, you can configure it to use different data sources, like your own Nexus, Artifactory or AWS Elastic Container Registry for example.
This way you can limit what sources are considered and you can also use Renovate to update internal dependencies instead of just public ones.</p>

<p>Now let’s imagine that the most recent version of the <code class="language-plaintext highlighter-rouge">amazoncorretto</code> image is <code class="language-plaintext highlighter-rouge">18.0.2</code> (This is <code class="language-plaintext highlighter-rouge">latest</code> at the time of writing).
Renovate will create a branch from your main branch, update the <code class="language-plaintext highlighter-rouge">FROM</code> entry in your Dockerfile and push the changes into your repository as a new branch. 
Now your regular CI/CD process can be followed to build, test, merge and deploy your component with the updated dependency.</p>

<p>The same scenario, with a second branch, will be performed to update the log4j dependency.</p>

<p>As an additional bonus, if the dependency exposes it, Renovate will add the changelogs in the pull/merge request it creates in your repository.
This makes it visible if any breaking changes were introduced and allows you to easily check, together with your tests, if you need to make changes to your code to update the dependency.</p>

<p>Example of changelogs embedded in PR.</p>
<p style="text-align: center;"><img src="/img/20221009-renovate/pr-changelog.png" alt="PR with changelogs" class="image center" style="margin:0px auto; max-width:100%" /></p>

<h2 id="how-to-setup">How to setup</h2>

<p>How to use Renovate depends a bit on where your code lives.</p>

<h3 id="renovate-github-app">Renovate GitHub App</h3>

<p>If you’re using GitHub, the setup is as easy as enabling the <a href="https://github.com/apps/renovate" target="_blank" rel="noopener noreferrer">GitHub Renovate app</a> for your repository. 
Next, Renovate will create an onboarding pull request where it will suggest a default Renovate configuration, show a preview of what dependencies it detected and an example of what updates it has found. 
The only thing left is to merge the pull request and renovate will automatically start scanning your repository and start creating branches and pull requests with dependency updates.</p>

<h3 id="self-hosted-renovate">Self-hosted Renovate</h3>

<p>The second option is to host Renovate yourself.
This option also comes with a lot more configuration capabilities, especially w.r.t. private data sources. 
You can run Renovate <a href="https://docs.renovatebot.com/examples/self-hosting/#gitlab-cicd-pipeline" target="_blank" rel="noopener noreferrer">as (part of) a pipeline</a>, as a <a href="https://docs.renovatebot.com/examples/self-hosting/#docker" target="_blank" rel="noopener noreferrer">Docker container</a> or even as a <a href="https://docs.renovatebot.com/examples/self-hosting/#kubernetes" target="_blank" rel="noopener noreferrer">CronJob on Kubernetes</a>.
Since you only need NodeJS/NPM to be available, you can run it almost anywhere you want. 
You can find an example of how to run Renovate as a Cronjob on a Kubernetes cluster in <a href="https://github.com/pietervincken/renovate-tekton-argo-talk/tree/main/k8s/renovate" target="_blank" rel="noopener noreferrer">this repository</a>.</p>

<p>With the self-hosted deployment method, you need to either explicitly tell Renovate which repositories it needs to consider or configure it to auto-detect repositories based on the access of its user or the integration with your code repositories.
For Bitbucket Server you can for example configure Renovate to automatically discover all repositories in a specific project.
This prevents you from having to make changes to the Renovate configuration every time a new repository gets added.</p>

<p>In this scenario, you’ll have to provide Renovate an identity to interact with the code repositories as well, as it needs to be able to create branches and push code changes.</p>

<h2 id="tips-and-tricks">Tips and tricks</h2>

<p>Aka things we discovered and/or went wrong while we started using Renovate.</p>

<h3 id="limit-concurrent-branches--prs">Limit concurrent branches / PRs</h3>

<p>An important tip, especially if your repository contains quite some outdated components, is to limit the number of concurrent branches and pull requests that Renovate is allowed to create. 
By default, Renovate is not limited to a specific number which might result in literally 10s if not over 100 pull requests being created. 
To prevent you and/or your developers from becoming overwhelmed by this, <a href="https://docs.renovatebot.com/configuration-options/#prconcurrentlimit" target="_blank" rel="noopener noreferrer">limiting the concurrent pull requests</a> is a must!</p>

<h3 id="setup-auto-merging">Setup auto-merging</h3>

<p>Auto-merging is a very powerful feature in Renovate, especially in combination with very good automated tests and continuous integration practices. 
<strong>Don’t</strong> enable this when starting with Renovate. 
Over time, you’ll be able to identify repositories and data sources for which updates are becoming as easy as just accepting the PRs. 
For these combinations of repositories and data sources, you can enable auto-merging in Renovate. 
This means that Renovate will perform the update to the code repository and if the result of the pipeline for that change is green, it will attempt to merge that PR.
If you have a well-automated CI/CD process, this can allow Renovate to automatically update your dependencies and make sure that your software automatically has the latest dependencies.</p>

<h3 id="use-it-in-deployment-repositories">Use it in deployment repositories</h3>

<p>The term dependency is interpreted quite broadly in Renovate. 
Not only classical libraries and Docker base images can be detected. (aka build phase dependencies)
It can also detect dependencies in your deployment setups. 
This means that it can detect updates in <a href="https://docs.renovatebot.com/modules/manager/ansible/" target="_blank" rel="noopener noreferrer">Ansible playbooks</a>, update <a href="https://docs.renovatebot.com/modules/manager/helmv3/https://docs.renovatebot.com/modules/manager/helmv3/" target="_blank" rel="noopener noreferrer">Helm Chart</a> references, update <a href="https://docs.renovatebot.com/modules/manager/kubernetes/" target="_blank" rel="noopener noreferrer">Kubernetes manifests</a> (including the API versions as shown below!) and even <a href="https://docs.renovatebot.com/modules/manager/kustomize/" target="_blank" rel="noopener noreferrer">Kustomize</a> and <a href="https://docs.renovatebot.com/modules/manager/terraform/" target="_blank" rel="noopener noreferrer">Terraform</a> setups.</p>

<p>Enabling automated dependency management for those deployment repositories can greatly reduce the amount of effort that is required to update a newer external dependency. 
It also helps to make sure that an update is rolled out consistently across many different environments. 
Especially in a larger corporate context, it can be hard to determine what the latest version of a specific service is and on which environments it’s running or not. 
By enabling Renovate to detect the dependencies, it can easily inform the different teams/users of newer versions.</p>

<p>Example of automated Kubernetes API updates.</p>
<p style="text-align: center;"><img src="/img/20221009-renovate/k8s-api-update.jpg" alt="Kubernetes API updates" class="image center" style="margin:0px auto; max-width:100%" /></p>

<h3 id="start-slow">Start slow!</h3>

<p>The last tip might sound a bit contradictory, but starting small with Renovate is key. 
You want to build confidence in the data sources you’re using and prevent your teams from being overwhelmed by Renovate PRs. 
A good way to start with Renovate is to enable it on just one or maybe a handful of low-impact repositories. 
This way, you can see what effect it has on your workflow and how teams react to having these PRs pop up in their change feed.</p>

<h2 id="why-should-i-use-it">Why should I use it?</h2>

<p>At the start of this blog post, we’ve discussed why updating dependencies is important.
As discussed, bulk updating dependencies is an option, but it might be time-consuming and therefore might be bumped to the bottom of the priority list.
Not to mention that you might miss important security updates if you need to “look” for the changes manually. 
Automating the update process and allowing a tool to automatically discover the updates, makes the update process a lot simpler and faster. 
If you already have a good CI/CD process that allows you to build, test, merge and deploy small changes easily, Renovate will save you tons of time and prevent you from running unsafe, outdated software.</p>

<p>And as one of my favorite sayings goes: <strong>anything worth doing twice is worth automating</strong></p>

<h3 id="links">Links</h3>

<ul>
  <li><a href="https://www.mend.io/free-developer-tools/renovate/" target="_blank" rel="noopener noreferrer">Mend Renovate website</a></li>
  <li><a href="https://github.com/renovatebot/renovate" target="_blank" rel="noopener noreferrer">Renovate Github Repository</a></li>
  <li><a href="https://docs.renovatebot.com/" target="_blank" rel="noopener noreferrer">Renovate docs</a></li>
  <li><a href="https://youtu.be/fAEbRmD4-G0" target="_blank" rel="noopener noreferrer">JOIN 2022 Talk YouTube Link</a></li>
  <li><a href="https://github.com/pietervincken/renovate-tekton-argo-talk/tree/main/k8s/renovate" target="_blank" rel="noopener noreferrer">Sample Renovate Setup</a></li>
</ul>

<p>This blog post is a companion to a talk at the JOIN 2022 conference.</p>

<p>Feel free to reach out to <a href="https://www.linkedin.com/in/pieter-vincken-a94b5153/" target="_blank" rel="noopener noreferrer">me</a> if you want to look into this solution.</p>

<p>Special thanks to the Unicorn team for helping with the blog post and automating the dependency updates!</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Pieter&quot;, &quot;last_name&quot;=&gt;&quot;Vincken&quot;, &quot;linkedin&quot;=&gt;&quot;pieter-vincken-a94b5153&quot;, &quot;twitter&quot;=&gt;&quot;PieterVincken&quot;, &quot;github&quot;=&gt;&quot;pietervincken&quot;, &quot;permalink&quot;=&gt;&quot;/author/pieter_vincken/&quot;, &quot;avatar&quot;=&gt;&quot;pieter-vincken.jpeg&quot;, &quot;title&quot;=&gt;&quot;Cloud Automation Engineer&quot;, &quot;email&quot;=&gt;&quot;pieter.vincken@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Pieter rarely sees problems, just too many solutions sometimes. That’s why K’nex was his favorite pastime as a child. Nowadays, he builds cloud platforms so organizations don’t have to worry about their IT infrastructure but can focus on their clients.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2019-05-03-istio-service-mesh-s2s.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-09-18-kustomize.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-11-18-devoxx-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-06-02-terraform.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-08-28-kubernetes-clients-comparison.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-10-15-5-reasons-not-to-go-to-the-cloud.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-02-14-postgres-ai.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-06-10-kubecon.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-10-27-renovate.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-05-17-kubecon-2023.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-06-05-back-to-terraform.md collection=posts&gt;]}</name><email>pieter.vincken@ordina.be</email></author><category term="Cloud" /><category term="cloud" /><category term="automation" /><category term="cicd" /><summary type="html"><![CDATA[Anything worth doing twice is worth automation.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/20221009-renovate/logo.png" /><media:content medium="image" url="https://ordina-jworks.github.io/img/20221009-renovate/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ode to JOIN</title><link href="https://ordina-jworks.github.io/architecture/2022/10/24/ode-to-join.html" rel="alternate" type="text/html" title="Ode to JOIN" /><published>2022-10-24T00:00:00+00:00</published><updated>2022-10-24T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/architecture/2022/10/24/ode-to-join</id><content type="html" xml:base="https://ordina-jworks.github.io/architecture/2022/10/24/ode-to-join.html"><![CDATA[<p>We had JOIN, we had fun, we had seasons in the sun. 
It has already been a month since the 9th edition of the JOIN conference was concluded. 
And even though it had been ten years since the first edition, the skipped year due to the pandemic will cause us to have an additional anniversary edition next year. 
The aluminum edition will be a sight to see, and it would be a shame to miss it.</p>

<p>As for this edition, the stakes were high with a new team taking over the reins of those who had come before them. 
As a member of this new team, I had the pleasure to rise to the occasion and make sure the legacy stayed untarnished. 
The new team did not disappoint, and even though there were minor hiccups during the day, in the end, the conclusion was clear. 
A job well done, and a crowd that had gotten a day’s worth of interesting talks topped off with a barbecue blessed with early autumn sun.</p>

<p style="text-align: center;"><img src="/img/2022-10-24-ode-to-join/JOIN-01.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Pieter Van den Wyngaert Opening the Conference in Style</p>

<p style="text-align: center;"><img src="/img/2022-10-24-ode-to-join/JOIN-02.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Getting the inside scoop from our friends at AWS</p>

<p style="text-align: center;"><img src="/img/2022-10-24-ode-to-join/JOIN-03.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Getting to Know your Fellow Conference Goers</p>

<p style="text-align: center;"><img src="/img/2022-10-24-ode-to-join/JOIN-04.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Getting in Touch with the Serverless World (courtesy of November Five)</p>

<p style="text-align: center;"><img src="/img/2022-10-24-ode-to-join/JOIN-05.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Ending the Conference by Queueing up for the Barbecue Diner</p>

<p>Not only did we have a new team, but the founding member of the conference, the jWorks unit or Ordina was joined by their colleagues from the BOLD unit to make it an even more impressive package deal. 
The talks reflected this, not only in a healthy mix of speakers but also in a healthy mix of topics ranging from the concerns about patching 3rd party software to the accessibility concerns of digital products for disabled people. 
We had people from AWS talking about the various ways to modernize applications, and we had people from Flowable showing process automation at its finest. 
On the subject of Flutter, experts brought various insights into the world of mobile app design, and even the mysteries of the design system were laid bare for all to understand.</p>

<p>Even though the topics were varied, the main theme was clear: Software development for the current times, leveraged by the possibilities that are present in the Cloud and the options we have for multi-channel approaches toward the customer. 
Thinking back to the Nexus of Force that Gartner used to taut: Social, Mobile, Analytics, and Cloud, the talks covered three of these four topics. 
Not a bad score for a conference that only lasts a single day.</p>

<p style="text-align: center;"><img src="/img/2022-10-24-ode-to-join/SMAC.png" alt="Workspace" class="image fit" style="margin:0px auto; max-width:100%" />
Taken from the <a href="https://blogs.gartner.com/mark-mcdonald/2022/08/15/relevance-is-customer-and-situationally-specific/smac/" target="_blank" rel="noopener noreferrer">Gartner Blogs Website</a></p>

<p>But shying away from the content for a moment, the process of getting such a conference to happen isn’t as trivial as it might seem. 
Where the preparations range from arranging a venue and finding a date that doesn’t clash too hard with all other conferences in the IT ecosystem, contacting and convincing interesting speakers to grace us with their presence, and setting up a marketing effort to get the word out there, the fun at this point has only begun. 
Facilities need to be arranged, such as making sure the proper recording material can be used to capture these talks for posterity. 
Catering and goodie bags need to be organized to give the attendees the best possible experience. 
You need to make sure that the people you invited to talk have the necessary arrangements to spend the night if needed and get transported for those who require transporting. 
A multitude of things can still get askew during the actual day, and firefighting mode is sometimes required at a moment’s notice.</p>

<p>All of this logistical work does not take away from the fact that we are proud of the ninth IT symphony we have put before our audience with the help of all of those willing to strip up their sleeves and help us out, not in the least the speakers and the technical staff of Elewijt Center that went well beyond the call of duty. 
And even though Beethoven’s magnum opus carried the same edition number, it is our wish that in contract with the maestro, we shall finish our tenth edition next year, and hope to see you all back again.</p>

<p>One closing Remark: The talks will be available on the <a href="https://www.youtube.com/c/OrdinaBelgiumJWorks" target="_blank" rel="noopener noreferrer">JOIN YouTube channel</a> as soon as they have left the capable hands of the post-production team.</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Peter&quot;, &quot;last_name&quot;=&gt;&quot;De Kinder&quot;, &quot;github&quot;=&gt;&quot;peterdekinder&quot;, &quot;linkedin&quot;=&gt;&quot;peterdekinder&quot;, &quot;permalink&quot;=&gt;&quot;/author/peterdekinder/&quot;, &quot;avatar&quot;=&gt;&quot;peterdekinder.jpg&quot;, &quot;title&quot;=&gt;&quot;Solution Architect&quot;, &quot;email&quot;=&gt;&quot;peter.dekinder@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Peter is a Solution Architect with firm roots in the Java technosphere, but with a wide interest in all things architecture. His areas of specialization include Service Oriented Architectures, Business Process Management and Security.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2020-01-28-Architecture-in-Projects.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-03-24-Charting-non-functionals.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-07-08-Book-Five-Dysfunctions.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-11-25-Quite-The-Story.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-01-04-Designing-REST-services.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2021-02-17-Out-With-The-Old.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-01-14-what-would-discord-bot-do.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-06-13-connecting-the-pods.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-10-24-ode-to-join.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2023-02-20-intelligent-automation.md collection=posts&gt;]}</name><email>peter.dekinder@ordina.be</email></author><category term="Architecture" /><category term="architecture" /><category term="software architecture" /><category term="Cloud" /><category term="technical leadership" /><summary type="html"><![CDATA[We had JOIN, we had fun, we had seasons in the sun. It has already been a month since the 9th edition of the JOIN conference was concluded. And even though it had been ten years since the first edition, the skipped year due to the pandemic will cause us to have an additional anniversary edition next year. The aluminum edition will be a sight to see, and it would be a shame to miss it.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2022-10-24-ode-to-join/header.jpg" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2022-10-24-ode-to-join/header.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Waterfall vs Agile</title><link href="https://ordina-jworks.github.io/agile/2022/10/19/waterfall-vs-agile.html" rel="alternate" type="text/html" title="Waterfall vs Agile" /><published>2022-10-19T00:00:00+00:00</published><updated>2022-10-19T00:00:00+00:00</updated><id>https://ordina-jworks.github.io/agile/2022/10/19/waterfall-vs-agile</id><content type="html" xml:base="https://ordina-jworks.github.io/agile/2022/10/19/waterfall-vs-agile.html"><![CDATA[<blockquote>
  <p>Waterfall or agile for a new project? Benefits or disadvantages?</p>
</blockquote>

<h1 id="table-of-contents">Table of contents</h1>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#software-development-life-cycle">Software Development Life Cycle</a>
    <ul>
      <li><a href="#waterfall">Waterfall</a></li>
      <li><a href="#agile">Agile</a></li>
      <li><a href="#scrumfallwaterscrumagilefallwatergile">Scrumfall/Waterscrum/Agilefall/Watergile</a></li>
    </ul>
  </li>
</ul>

<h1 id="introduction">Introduction</h1>
<p>After my graduation until now, about 10 years, I have worked in many different teams and project, for various clients in various sectors.
During those years, many different styles of project management were used but they always came down to</p>
<ul>
  <li>the more traditional type of project management, also known as waterfall</li>
  <li>a more modern type known as agile</li>
  <li>something in between those 2</li>
</ul>

<p>It is widely known that agile is a big buzzword in more recent years, but what is it exactly?
What is the difference between agile and waterfall?
Which one can be used in which situation?
What are the benefits and disadvantages?</p>

<h1 id="software-development-life-cycle">Software Development Life Cycle</h1>
<p>It is important to know the purpose of a software development life cycle (SDLC). 
The usual explanation for this cycle is</p>
<blockquote>
  <p>A process that produces software with the highest quality and lowest cost in the shortest time possible. 
It typically provides a well-structured flow of phases that help an organization to quickly produce high-quality software 
which is well-tested and ready for production use.</p>
</blockquote>

<p>A mouthful for saying that it provides guidance for delivering software as efficiently as possible, with high quality.</p>

<p>In a SDLC there is a focus on the following 6 phases:</p>
<ul>
  <li>Planning</li>
  <li>Analysis</li>
  <li>Design</li>
  <li>Implementation</li>
  <li>Testing &amp; Integration</li>
  <li>Maintenance</li>
</ul>

<p><img alt="SDLC order" src="/img/2022-10-19-waterfall-vs-agile/sdlc_order.png" class="image fit" style="margin:0px auto; max-width: 750px;" /></p>

<p>There are multiple models of SDLC’s, but the most known are the <em>waterfall</em> and <em>agile</em> model.</p>

<h2 id="waterfall">Waterfall</h2>
<p>The waterfall model is the most traditional type of a SDLC. It has been around since 1970.
It divides the effort into a number of steps and defines that only one step can be active at the same time.
While it usually follows the 6 phases as described above, there’s no limit on it.</p>

<p>From where does it get its name?</p>

<p><img alt="Waterfall order" src="/img/2022-10-19-waterfall-vs-agile/waterfall_order.png" class="image fit" style="margin:0px auto; max-width: 750px;" /></p>

<p>As you can see, it looks like a waterfall. Each step can only start after the previous one is done and thus it cascades down like a waterfall.</p>

<p>Now, when is a step done? Only when the specified artifacts that were defined at the beginning of the step are delivered and accepted.
For the implementation step, this can be a collection of software artifacts that satisfy the design.
For the testing step, it can be a test plan that demonstrates that all the requirements and design are working as intended.</p>

<p>A common mistake is that people think that you can not go back to a previous step. 
It is allowed, but it requires that you stop all work on the current step until the errors in the previous step have been resolved.</p>

<h3 id="pros">Pro’s</h3>
<ul>
  <li>It has detailed documentation and metrics</li>
  <li>The requirements are agreed upon and signed off</li>
  <li>There are less defects as there is rigorous planning and testing</li>
  <li>Defined start and end point which allows for easy measuring</li>
</ul>

<h3 id="cons">Con’s</h3>
<ul>
  <li>It starts slow as the requirements need to be defined in detail</li>
  <li>Changing those requirements takes a lot of effort</li>
  <li>The software is not visible until most of the development work is finished</li>
  <li>Less focus on the client because the requirements are the most important item</li>
</ul>

<h3 id="examples">Examples</h3>
<p>When do we use the waterfall model today?</p>
<ul>
  <li>When requirements can be reliably, quickly and thoroughly defined up front</li>
  <li>For very large teams where common understanding must be put in writing to avoid confusion and miscommunication</li>
  <li>When there is a defined budget and schedule given by the customer</li>
  <li>When there is not much involvement from the customer</li>
</ul>

<p>Building a bridge across a river is a good example of a project that is best done with a waterfall model.
That is a project where a clear schedule is needed and where the requirements need to be defined as soon as possible.
Saying “We will start with the first part of the bridge, evaluate that part and then decide if and how to continue with the remaining parts” is not a possibility here.</p>

<h2 id="agile">Agile</h2>
<p>The agile model has been created in direct response to the waterfall model.
It puts the focus on adaptive, simultaneous workflows which is the opposite from the linear flow of the waterfall model.</p>

<p>Instead of beginning with a complete knowledge of the requirements, the team develops a product in small cycles where small parts are build in a evolutionary way.
Each cycle contains the same steps as defined in the SDLC, but they can all be done at the same time, depending on the experience and skills of the team.
Contrary to the waterfall model, the customer can quickly see and evaluate how the project is advancing at the end of each cycle.
Needed changes to the requirements, based on this evaluation, can be done faster and implemented more easily.
This constant feedback from the client allows the team to adjust to the challenges as they arise and not when it is too late.</p>

<p><img alt="Agile order" src="/img/2022-10-19-waterfall-vs-agile/agile_order.png" class="image fit" style="margin:0px auto; max-width: 750px;" /></p>

<p>The main idea behind the agile model is delivering business value early in the process to lower the risk associated with the development.</p>

<p>The most known implementations of the agile model are Scrum and Kanban, but they all share the same characteristics.</p>

<ul>
  <li>Simultaneous, incremental work</li>
  <li>Adaptability</li>
  <li>Faster and multiple deliverables</li>
</ul>

<h3 id="process">Process</h3>
<p>During agile development, the process usually looks like this:</p>
<ol>
  <li>Define a few initial requirements</li>
  <li>Design</li>
  <li>Develop</li>
  <li>Test</li>
  <li>Deploy</li>
  <li>Evaluate the result of the iteration</li>
  <li>Collect feedback from the various stakeholders</li>
  <li>Start the cycle again with new requirements and the feedback</li>
</ol>

<p><img alt="Agile cycle" src="/img/2022-10-19-waterfall-vs-agile/agile_cycle.jpg" class="image fit" style="margin:0px auto; max-width: 750px;" /></p>

<h3 id="pros-1">Pro’s</h3>
<ul>
  <li>More project visibility at the end of each cycle</li>
  <li>It is a collaborative and practical approach for executing complex software development projects</li>
  <li>The client and stakeholders have frequent and early opportunities to evaluate the product</li>
  <li>Constant communication between team members so issues can be resolved proactively</li>
</ul>

<h3 id="cons-1">Con’s</h3>
<ul>
  <li>There is a risk for scope creep as agile projects generally have no set end date and thus additional features may be requested</li>
  <li>A multi-skilled resource pool is needed to deliver the project as all knowledge needs to be in the team</li>
  <li>Less detailed documentation as it is considered less important</li>
  <li>Fragmented output can be a problem as multiple teams may work on different components that then need to be put together</li>
</ul>

<h3 id="examples-1">Examples</h3>
<p>When do we use the agile model today?</p>
<ul>
  <li>If there are little to no requirements at the start of the project</li>
  <li>If your organization does not have strict processes to follow and the existing processes are lenient</li>
  <li>If the client or product owner can be highly available to follow the process</li>
  <li>When you’re trying to create something innovative that does not exist yet and needs to go to market quickly</li>
  <li>When the timeline is short and flexible</li>
  <li>When the budget has some wiggle room so that features can be prioritized</li>
</ul>

<p>Agile wins when the requirements are unclear from the beginning or still need to be discovered during the initial development.
More features will be produced in a shorter time frame and the team can be more flexible throughout the process.</p>

<p>A good example would be the development of a social media app. While the initial requirements are clear, further development depends heavily on the demands of the user.
So a start can be made with some basic features that can go quickly to market. Other features will then be implemented after the feedback of the business and most importantly the reactions of the users.</p>

<h2 id="scrumfallwaterscrumagilefallwatergile">Scrumfall/Waterscrum/Agilefall/Watergile</h2>
<p>This model might be known under even more names than the ones above. But it’s basically a software delivery lifecycle that tries to combine the best of both worlds in waterfall and agile.
It usually starts with an up-front design phase and ends with a legacy deployment mechanism, with agile development in between.</p>

<p><img alt="Scrumfall diagram" src="/img/2022-10-19-waterfall-vs-agile/scrumfall_diagram.png" class="image fit" style="margin:0px auto; max-width: 750px;" /></p>

<p>But the name doesn’t matter. Everyone that knows a bit about agile knows that it’s a bad idea to do the entire design up-front and only then start developing. While that might work in a full waterfall model, it doesn’t in agile. 
Waterfall and agile are 2 fundamental different models with conflicts in interest as explained in the previous chapters.</p>

<h3 id="why">Why?</h3>
<p>So even though it’s not the best idea to mix these models, how does it happen that organizations use them?
The results may not be ideal, but it might be enough for some organizations. The agile model mentions all the time that there is not one solution that fits everyone. Every organization must find an approach that is effective for them and which enables them to deliver value.</p>

<p>Who doesn’t know an organization that follows Scrum to the letter, only to find out that it’s not the best approach for them?</p>

<p>Agile is for a large part about discovering new ways of working. As a result, the scrumfall model can be a temporary step towards a full agile transformation.
Now why would organizations keep using this hybrid form?</p>
<ul>
  <li>The IT department is agile and uses Scrum, but the other business departments have never been convinced. And so the organization is divided.</li>
  <li>The organization is stuck with an incomplete transformation and doesn’t know how to continue</li>
  <li>Or they’re in the middle of such a transformation and will finish it shortly</li>
  <li>The organization’s structure is made in such a way that deployment and operations cannot be done by the development teams</li>
</ul>

<h3 id="impact">Impact</h3>
<p>Now, how can this hybrid model impact your organization?
Let’s look at the following image:</p>

<p><img alt="Scrumfall time" src="/img/2022-10-19-waterfall-vs-agile/scrumfall_time.png" class="image fit" style="margin:0px auto; max-width: 750px;" /></p>

<p>The first phase starts with the design as in every model. However, it’s waterfall design, so it takes a while to get everything designed in detail.
This means that during this phase, the development team has nothing to do. They’re just idle.
Once that phase is done and the developers can finally start working, they accelerate and can start delivering. But since the deployment process is still traditional waterfall, it takes a while to get it done. Which means that the feedback loop is going to be a lot slower than in modern agile models.</p>

<p>Slow feedback loops and long release cycles have a high negative impact on the value of the project as everything takes longer and results of various experimentations are harder to see.</p>

<h3 id="problems">Problems</h3>
<p>What are the major problems when you use this model?</p>
<ul>
  <li>Risk and waste: when using agile models, you get feedback by interacting with the business and the end-users. When designing up-front, you can’t anticipate changes in the demands. So when you’re not using a tight feedback loop, you might be creating the wrong thing. Which results in waste when you need to restart.</li>
  <li>Delayed feedback: since the development team is not doing the releases, it will take more time. Time that could be used to find out how the market responds to the new features.</li>
  <li>Long-term damage: the waste that is created has construction and maintenance costs, not to mention decrease in the team’s motivation. All these things have a long-term impact on the organization.</li>
</ul>

<h3 id="way-out">Way out?</h3>
<p>The goal of any organization is to meet the goals of the corporate vision and mission. These goals should be achieved with the greatest efficiency and effect.
If that is done with waterfall or agile is less important. Even scrumfall might work for some organization, be it less effective than a “pure” model.</p>

<p>As with any model, experimentation must be done and improvements must be made. As in agile, this depends from organization to organization how it can be done. There is no one answer that fits for everyone.µ
The most important to keep in mind is that every organization needs to shift its approach to one that decreases waste while increasing quality and predictability.</p>]]></content><author><name>{&quot;first_name&quot;=&gt;&quot;Wouter&quot;, &quot;last_name&quot;=&gt;&quot;Nivelle&quot;, &quot;permalink&quot;=&gt;&quot;/author/wouter-nivelle/&quot;, &quot;avatar&quot;=&gt;&quot;wouter-nivelle.jpg&quot;, &quot;title&quot;=&gt;&quot;Project Manager&quot;, &quot;linkedin&quot;=&gt;&quot;wouter-nivelle-34a90b31&quot;, &quot;email&quot;=&gt;&quot;wouter.nivelle@ordina.be&quot;, &quot;bio&quot;=&gt;&quot;Wouter is a Project Manager at Ordina Belgium. Passionate about agile. Eager to share knowledge. Not afraid of challenges. Always interested in learning and discovering new things.&quot;, &quot;posts&quot;=&gt;[#&lt;Jekyll::Document _posts/2018-07-03-Agile-DevOps-Summit-Brussels.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-11-12-experience-agile-2019.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2019-11-25-agile-reporting.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2020-02-12-experience-agile-2019-part-2.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-10-03-experience-agile-2022.md collection=posts&gt;, #&lt;Jekyll::Document _posts/2022-10-19-waterfall-vs-agile.md collection=posts&gt;]}</name><email>wouter.nivelle@ordina.be</email></author><category term="Agile" /><category term="Agile" /><category term="Project Management" /><category term="Waterfall" /><summary type="html"><![CDATA[Waterfall or agile for a new project? Benefits or disadvantages?]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ordina-jworks.github.io/img/2022-10-19-waterfall-vs-agile/header.jpg" /><media:content medium="image" url="https://ordina-jworks.github.io/img/2022-10-19-waterfall-vs-agile/header.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>