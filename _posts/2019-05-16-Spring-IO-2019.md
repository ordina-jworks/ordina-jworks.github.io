---
layout: post
authors: [nick_van_hoof,tom_van_den_bulck,yolan_vloeberghs,jago_staes]
title: 'Spring IO 2019'
image: /img/2019-05-16-Spring-IO-2019/spring-io.png
tags: [Spring IO,Spring,Java,Conference]
category: Conference
comments: true
---

# Spring IO 2019!

Spring I/O has become a yearly tradition for our JWorks consultants. With 21 colleagues we went to the conference in beautiful Barcelona on the 16th and 17th of May.

The conference was held at the same location as last year, the **Palau de Congressos de Barcelona**. As indicated last year, this year there was an overlap at the venue with the [Barcelona 
International Motor Show](http://www.automobilebarcelona.com/es/visitar){:target="_blank" rel="noopener noreferrer"}.
This gave us the opportunity to take a look at same beautiful cars during breaks.

****

<img class="image fit" src="{{ '/img/2019-05-16-Spring-IO-2019/venue.jpg' | prepend: site.baseurl }}" alt="Venue Spring I/O 2019" />

****

<img class="image fit" src="{{ '/img/2019-05-16-Spring-IO-2019/jworks.jpg' | prepend: site.baseurl }}" alt="JWorks at Spring I/O 2019" />

****

We'll talk about some of the presentations this year, but it is not a complete list.
There were so many interesting talks, you can find them all on [Youtube](https://www.youtube.com/playlist?list=PLe6FX2SlkJdTlXfwer8JB-WGm-TEyIB2k){:target="_blank" rel="noopener noreferrer"}.

Let us know if we missed anything by [filing an issue](https://github.com/ordina-jworks/ordina-jworks.github.io/issues/new){:target="_blank" rel="noopener noreferrer"} or contacting us at [our general JWorks email](mailto:jworks@ordina.be){:target="_blank" rel="noopener noreferrer"}.

* [Moving from Imperative to Reactive by Paul Harris](#moving-from-imperative-to-reactive-by-paul-harris)
* [Configuration Management with Kubernetes, a Spring Boot use case by Nicolas Frankel](#configuration-management-with-kubernetes-a-spring-boot-use-case-by-nicolas-frankel)
* [Building better monoliths - Modulithic Applications with Spring Boot by Oliver Drotbohm](#building-better-monoliths---modulithic-applications-with-spring-boot-by-oliver-drotbohm)
* [Cutting-Edge Continuous Delivery: Automated Canary Analysis through Spring based Spinnaker by Andreas Evers](#cutting-edge-continuous-delivery-automated-canary-analysis-through-spring-based-spinnaker-by-andreas-evers)
* [Using Java Modules in Practice with Spring Boot by Jaap Coomans](#using-java-modules-in-practice-with-spring-boot-by-jaap-coomans)
* [Stream Processing with the Spring Framework by Josh Long and Viktor Gamov](#stream-processing-with-the-spring-framework-by-josh-long-and-viktor-gamov)
* [How Fast is Spring by Dave Syer](#how-fast-is-spring-by-dave-syer)
* [Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by Matthias Haeussler](#kubernetes-and/or-cloud-foundry---how-to-run-your-spring-boot-microservices-on-state-of-the-art-cloud-platforms-by-matthias-haeussler)
* [Migrating a modern spring web application to serverless by Jeroen Sterken and Wim Creuwels](#migrating-a-modern-spring-web-application-to-serverless-by-jeroen-sterken-and-wim-creuwels)
* [Testing Spring Boot Applications by Andy Wilkinson](#testing-spring-boot-applications-by-andy-wilkinson)
* [Building better monoliths - Modulithic Applications with Spring Boot by Oliver Drotbohm](#building-better-monoliths---modulithic-applications-with-spring-boot-by-oliver-drotbohm)
* [How to live in a post-Spring-Cloud-Netflix world by Olga Maciaszek and Marcin Grzejszczak](#how-to-live-in-a-post-spring-cloud-netflix-world-by-olga-maciaszek-and-marcin-grzejszczak)
* [Event-Driven Microservices with Axon and Spring Boot: excitingly boring by Allard Buijze](#event-driven-microservices-with-axon-and-spring-boot-excitingly-boring-by-allard-buijze)
* [How to secure your Spring apps with Keycloak by Thomas Darimont](#how-to-secure-your-spring-apps-with-keycloak-by-thomas-darimont)
* [Zero Downtime Migrations with Spring Boot by Alex Soto](#zero-downtime-migrations-with-spring-boot-by-alex-soto)

# Day 1: Talks & Workshops

## Moving from Imperative to Reactive by [Paul Harris](https://twitter.com/twoseat){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Paul Harris" src="/img/2019-05-16-Spring-IO-2019/paul-harris.jpg"></span>

When development started on the Cloudfoundry java client, [Spring Reactor](https://projectreactor.io/) was also rebooted. 
Which means that they became their very first customer.

Paul Harris says that he made all the mistakes you can make with reactive programming.
And now he'll teach us how to avoid many of those.

It al started with the **[Reactive Manifesto](https://www.reactivemanifesto.org/){:target="_blank" rel="noopener noreferrer"}** in 2013, which came up with 4 ideas for reactive applications:
* Responsive: it should feel as if the application is progressing, with for example, some feedback.
* Resilient: if a particular part of you application fails, the remainder should be able to cope with that. 
* Elastic: make the most out of the resources available to the application.
* Message Driven: more message driven then event-driven.

A manifesto is nice, but it does not compile.

The next step was [Reactive Streams](https://www.reactive-streams.org/){:target="_blank" rel="noopener noreferrer"} which defined a set of interfaces for how we might deal with reactive streaming situations.
You can disginguish 4 interfaces:
* [Publisher](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Publisher.html){:target="_blank" rel="noopener noreferrer"} which emits 'things', signals.
* [Subscriber](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Subscriber.html){:target="_blank" rel="noopener noreferrer"} listens to those signals.
* [Subscription](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Subscription.html){:target="_blank" rel="noopener noreferrer"} a subscriber together with a publisher gives you a subscription.
* [Processor](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Processor.html){:target="_blank" rel="noopener noreferrer"} is a a combination of a publisher and a subscriber that allows you to process data.

The intention of Reactive Streams was that more useful real world implementations would follow.
One of these is Spring Reactor.
For a good introduction to Spring Reactor you can read our [blogpost](https://ordina-jworks.github.io/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html) about it.

Spring Reactor contains various reactive frameworks, the three big ones are:
* [Reactor-Core](https://projectreactor.io/docs/core/release/api/){:target="_blank" rel="noopener noreferrer"} - the basic provision.
* [Reactor-Netty](https://projectreactor.io/docs/netty/release/api/){:target="_blank" rel="noopener noreferrer"}  - the reactive implementation of Netty. 
* [Reactor-Test](https://projectreactor.io/docs/test/release/api/){:target="_blank" rel="noopener noreferrer"} - which is a bunch of really good usefull methods for testing reactive streams.

Before Paul dove in the code he first explained [Mono](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Mono.html) and [Flux](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html).

### a Mono
Which is a Reactive Streams `Publisher` which will return no or one element.
<img alt="a Mono" src="{{ '/img/2019-05-16-Spring-IO-2019/mono.svg' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

### a Flux
Also a `Publisher` which will emit zero to N elements and then complete.
<img alt="a Flux" src="{{ '/img/2019-05-16-Spring-IO-2019/flux.svg' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

Paul showed us a demo of how to make a legacy spring mvn application reactive.
In order to do so the following steps were taken:
* Add dependency to [spring boot starter webflux](https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-webflux)
This is the reactive variant of `Web MVC`. 
You shouldn't need to change anything to keep it running, unless you have used specific server features.
* Convert the return of a `List` to a `Flux` (of the simplest call)
* Convert return types for repository methods to `Mono` or `Flux`.
    * Use the static method `.justOrEmpty` of the Mono type in order to deal with an optional.
    * Use `.switchIfEmpty` to return a proper error response.
    * In order to return a `Flux`: use `fromIterable`.

Conclusion is that Reactive starts of complicated, but it will become easier when you have used it more often. 
It doesn't have that many different methods you can use, so all in all it is quite easy to wrap your head around.

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/vSHNBgY7MGA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>


## Configuration Management with Kubernetes, a Spring Boot use case by [Nicolas Frankel](https://twitter.com/nicolas_frankel){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Nicolas Frankel" src="/img/2019-05-16-Spring-IO-2019/nicolas-frankel.jpg"></span>

Nicolas Frankel is a Developer Advocate who works for [Exoscale](https://www.exoscale.com/){:target="_blank" rel="noopener noreferrer"}, a European cloud hosting provider.

In this session, he explained how to correctly configure each environment with its own parameters and settings.

There are traditional configuration management tools such as Chef, Ansible, Puppet, ... . 
But what is the point?

*Docker images are and should always be immutable.*  
They should be configurable depending on the environment where we want to run our image in.
A Docker image should be able to run in different environments without problems, this is where [Kubernetes](https://kubernetes.io/){:target="_blank" rel="noopener noreferrer"} comes in. 
Kubernetes can easily configure and parameterize each Docker image to run in different environments.

One thing to remember is that you should make sure that you are working in the correct environment. 
Nicolas likes to add banners to the page to know in which environment you are currently working. 
For example, if you are working in the development environment, then you might want to show a big blue 'Development' banner, while in production you would like a big, red, blinking one.

This can all be done with the power of Kubernetes and immutable images. 
You can simply declare your environment variables in Kubernetes, then you can inject your environment in your Spring Boot application.

There are 3 ways to access your environment variables in Spring Boot: profiles, `@Value` or `@ConfigurationProperties`.

To get started in Kubernetes, you have to create a few Kubernetes objects:
1. (Optional) A [Namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)
2. A [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
3. A [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) / [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) 

In the deployment, you can give the arguments based on the environment that you want to spin up.
With a [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/), you can combine your environment variables that belong to each other (ex. database settings, AWS keys, ...).
Once done defining the ConfigMap, you can import the ConfigMap in your Deployment declaration.

What's also very interesting is that you can declare your environment variables in a seperate Git repository with the use of an [initContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/). 
Of course, you can already do this with [Spring Cloud Config](https://spring.io/projects/spring-cloud-config). 
This is just an alternative on the Kubernetes side of configuration management.

If you want to read more about Nicolas Frankel's work, you can read his blog [here](https://blog.frankel.ch).

You can watch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/cTWu_DLqDt4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Building better monoliths - Modulithic Applications with Spring Boot by [Oliver Drotbohm](https://twitter.com/odrotbohm){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Oliver Drotbohm" src="/img/2019-05-16-Spring-IO-2019/oliver-drotbohm.jpg"></span>

This talk caught our attention because we’re currently working with a client where we see some of the limitations of a microservice architecture. 
We are currently considering merging multiple of them into a more coarse grained architecture containing multiple more 'monolithic' applications.
 
### Microservice architectures also have disadvantages! 
The talk starts off explaining some of the key differences between microservice and classic monoliths from an architectural point of view.

The key take away here is that although there’s an advantage in terms of architecture degradation, it is harder to accidentally call another microservice than to call a method on another bounded context in a monolith.
This advantage comes at a cost, you lose compile-time safety which makes it harder to refactor than a Monolith and it is harder to test the whole system.
These are especially disadvantageous in the early stages of the project when it is not clear yet what the correct bounded contexts are.

### The Modulith
It might thus be useful to consider starting off with a well structured Monolith before considering evolving towards microservices.
But how do we avoid having our architecture degrade quickly?
Enter the Modulith; a modulith is basically a Monolith with multiple modules with well defined dependencies in it.
To achieve this the speaker demonstrates a 'Moduliths' tool that he’s in process of developing for the Spring framework.

The idea is to use a package structure convention and enforce it with tests using reflection.
In this package structure convention only (public members of) the root package of each module are accessible to other
modules, it's considered the API package of that module, subpackages are considered internal. 
There's more to this tool however; another problem of modularizing your application is that you typically want to do integration testing on the bounds of your modules.
The 'Moduliths' tool allows to bootstrap your module alone or in various configurations with specific module dependencies for integration testing. 
To top it off there's support to generate plantUML diagrams for documentation purposes!
For more details take a look at [https://github.com/odrotbohm/moduliths](https://github.com/odrotbohm/moduliths){:target="_blank" rel="noopener noreferrer"}.

### Alternative approaches
There are of course other ways to divide your application into Modules and maintain the architecture: 

#### Multiple artifacts(gradle/maven modules).
You might get an explosion of artifacts and it can become kind of verbose with all the configuration(`pom.xml` or `build.gradle`) files.
Additionally the artifacts are redundant since we're planning on deploying everything together anyway.
There's also no support to dynamically compose modules for tests since the dependencies are typically statically defined.

This was actually the way we were considering handling it at our client.
The big advantage here in our eyes is that reflection can be avoided and the architecture can be verified at compile-time. 
The good news is it's possible to combine it with the 'Moduliths' approach, 
which might be useful for integration testing.

#### Java(9+) Module System
Could be used but it's certainly not designed for this.
It definitely doesn't have any support for dynamically composing your modules for testing.

#### External tools
JQAssist, Sonargraph, jDepend... . 
These are powerful tools but usually run during the build making the feedback loop bigger.

### Wrapping it up

The Moduliths approach explained in this talk gives us a nice intermediate step towards a better architecture.
It alleviates some of the biggest problems with monoliths without introducing new ones using a more complex architecture like microservices!

You can rewatch his talk here: 

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/bVaiTPYlHFE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Cutting-Edge Continuous Delivery: Automated Canary Analysis through Spring based Spinnaker by [Andreas Evers](https://twitter.com/andreasevers){:target="_blank" rel="noopener noreferrer"}

>The ultimate goal of continuous delivery is to deploy software quickly and automatically.
This can only be achieved if we are able to push new code without fear.

<span class="image left"><img class="p-image" alt="Andreas Evers" src="/img/2019-05-16-Spring-IO-2019/andreas-evers.jpg"></span>

Andreas saw through the years that there are two opposing forces that battling it out.
On the one hand there is the need for speed while on the other hand there is the need for confidence.
Like, updating in production without testing will give you a great speed, but not much confidence.

Microservices using integration tests on an acceptance environment might mean that you test and already obsolete topology because microservices can change that quickly.
Contract testing does not cover all the aspects needed to provide confidence as it does not test behaviour.

A good alternative where Andreas is going to talk about is Canary Analysis, in order to do so lets first introduce Spinnaker:
[Spinnaker](https://www.spinnaker.io/){:target="_blank" rel="noopener noreferrer"} is an open source, multi-cloud continuous delivery platform created at Netlfix.
It supports a lot of cloud environments like: Openstack, AWS, Google Cloud, Microsoft Azure, Cloud Foundry, ... 
Major contributors are Netflix, Google, Microsoft, Pivotal, ...

Under the hood of Spinnaker is composed of a bunch of Spring Boot microservices.
An other important component of Spinnaker is [Halyard](https://github.com/spinnaker/halyard){:target="_blank" rel="noopener noreferrer"}: a bill of materials for the different microservices of spinnaker and it helps you with the deploy of spinnaker.
Spinnaker also integrates well with your CI environments


Cloud Deployments are often complex:
* different regions
* different accounts for your environments (production, acceptance, ... )

Teams want an easy road into the cloud, no complexity to deploy.
On the other hand easy rollbacks are important.
Spinnaker can help you with this!

Various deployment strategies exist:
* Red/Black
* Rolling Red/Black
* [Canary analysis](https://www.spinnaker.io/guides/user/canary/){:target="_blank" rel="noopener noreferrer"} 
<img alt="deployment strategies" src="{{ '/img/2019-05-16-Spring-IO-2019/deployment-strategies.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 700px;">

You can define a pipeline to deploy into production:
<img alt="pipeline tasks" src="{{ '/img/2019-05-16-Spring-IO-2019/pipeline-tasks.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

For every stage you will have a series of steps, per step multiple tasks and every task has some operations which are executed.
A lot of these steps are very specific per cloud, Spinnaker tends to abstract that away.


Spinnakers makes it possible to go fast but still do it safely:
* Automated rollbacks
* Deployment windows
* Cluster locking
* Traffic guards, extra safeguards which can be configured
* Manual judgements: which makes use of the human "gut" feeling, which a computer does not have.
* ...

Andreas had a made Rick & Morty demo of which he has an old, already deployed version and a new version.
When doing canary, it is wise to startup a baseline, the old version, so that you have the solid baseline to measure againssst.
Spinnaker will also look at JVM metrics like memory, cpu, etc. 
But you can also define business metrics like startup time of the app, response times, ... 

When the canary fails, it will just rollback and restore the previous version.
Spinnaker will decide if the canary fails by looking at the statistics it gathered.

Canary testing allows you to test with real users and real production data.
At the same time it reduces the possible impact of your new version on end users.

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/uB35m60GAZw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Using Java Modules in Practice with Spring Boot by [Jaap Coomans](https://twitter.com/JaapCoomans){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Jaap Coomans" src="/img/2019-05-16-Spring-IO-2019/jaap-coomans.jpg"></span>

### Current State
Jaap started with addressing the current state of the module system:
* Most tooling support is good (maven, ide, ...)
* In frameworks the adoption is very low.
* For developers it is even lower.

It can be summarized as, it's like eating vegetables, we know it is healthy, we know its beneficial, but we don't do it.

### Using modules?!

What challenges will you face when you want to migrate to modules.
* Split packages: package with same name exposed by more then 1 module.
* Automatic modules: plain jar on your module path and thus interpreted as a module.
This exports and opens all packages, reads all other modules and it derives it's module name from the filename
The problem with that is that you can only have one module with a certain name on your module path.  
    * In maven central 3500 collisions are possible.
    * You can circumvent this with the Automatic-Module-Name in your manifest file.

For the demo application Jaap is going to use MongoDB.
There is a split package issue in the legacy mongo client, not with the new one, but Spring Data Mongo relies heavily on the legacy mongo client instead of the new one.

In order to get started with modules, the first steps are just to minimize the problems you might encounter.
#### Step 1 + 2 + 3: 
* Upgrade your dependencies as this will minimize conflicts.
* Use JDK11+ 
* Compile to  JDK11+.

These first 3 steps are just to reduce the problems you might encounter.

#### Step 4: 
Prepare the module structure within your code, so you can go from module to module.
Don't start with one big module from the start.

#### Step 5: 
Add module descriptors bottom-up.
It has the least dependencies, so good to get started at that level.

Create a new module-info.java, this supported by IntelliJ.

This first module has no external dependencies whatsoever, so very easy to define, you only need to indicate what your are going to expose.

```java
 module nl.jaapcoomans.boardgame.domain {
    exports nl.jaapcoomans.boardgame.domain;
    exports nl.jaapcoomans.boardgame.domain.command;
 }
```
Note: you might need a newer version of the maven surefire plugin; Jaap used version `3.0.0-M3`.

For a module which needs other modules you will need to define a little bit more within the `module-info`.
```java
 module nl.jaapcoomans.boardgame.bgg {
    requires nl.jaapcoomans.boardgame.domain;
    //requires com.sun.xml.bind;
    requires java.xml.bind;
    requires feign.core;
    requires feign.jaxb;
    
    exports nl.jaapcoomans.boardgame.bgg.factory;
    opens nl.jaapcoomans.boardgame.bgg.xmlapi;
 }
```

* `requires`: Defines the modules that you need.
* `exports`: Will be the only stuff that you expose.
* `opens`: This means that this will make a module available for reflection, that you might need for jaxb.

Spring is not yet modular, but they did define automatic module names in all of their jars.

#### Step 6: 
Add a module descriptor to the main jar.
Only then you get all the benefits of the module system
At this moment you will also encounter all the hurdles as this will also get you into runtime errors.
If you do not execute this step your main application will still be using the classpath and not the module path.

Export the main class and the application module.
```java
 module nl.jaapcoomans.boardgame.application {
    requires nl.jaapcoomans.boardgame.bgg;
    ...
    requires spring.context;
    
    exports nl.jaapcoomans.boardgame;
    exports nl.jaapcoomans.boardgame.application;
 }
```

You can also define `requires transitive`.
```java
 module nl.jaapcoomans.boardgame.persistence{
    requires nl.jaapcoommans.boardgame.domain;
 }
```
This last part means that when you depend on that module, you will also implicitly depend on that transitive module.

### Runtime Errors.

When you encounter runtime errors, you can pretty much copy paste the errors you get about opening the modules.

`ClassNotFoundExceptions`, add these to `requires`.

When you stop getting the same error, this means that you have reached the next phase.. 

Spring does use some of the internals of the JDK, which can be fixed by:`requires jdk.unsupported`.
This does help you out for know, but the module name alone screams that you should not use it.

### Lessons Learned.

As a summary here are the lessons learned by Jaap.
* Move bottom up.
* Test all paths on every step, because you will encounter runtime errors.
* The logs have the answer, the JVM gives you a good indication with errors by the module system.
* It still involves pioneering.

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/UFBH7gHJkb4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Stream Processing with the Spring Framework by [Josh Long](https://twitter.com/starbuxman){:target="_blank" rel="noopener noreferrer"} and [Viktor Gamov](https://twitter.com/gamussa){:target="_blank" rel="noopener noreferrer"}

All the source code of the live demo can be found on [https://github.com/joshlong/spring_io_2019/tree/master/kafka_and_spring](https://github.com/joshlong/spring_io_2019/tree/master/kafka_and_spring){:target="_blank" rel="noopener noreferrer"}.

<span class="image left"><img class="p-image" alt="Josh Long" src="/img/2019-05-16-Spring-IO-2019/josh-long.jpg"></span>

<span class="image left"><img class="p-image" alt="Viktor Gamov" src="/img/2019-05-16-Spring-IO-2019/viktor-gamov.jpg"></span>

Statement: *it is dangerous to think of Kafka as a message queue. A
s it tends to become a vine of data within your organization to move data around, it becomes a database.*

In the demo they made use of [Apache Avro](https://avro.apache.org/){:target="_blank" rel="noopener noreferrer"}.
The avro format will be used as a contract for the messages, it also gives you the option to generate java classes based on the schema, you can use an avro maven plugin for that.

Kafka does not care what you put in there.
But passing along a schema gives your consumers the option to verify that they can process the message or not.

[Spring Kafka](https://spring.io/projects/spring-kafka){:target="_blank" rel="noopener noreferrer"} gives you [KafkaTemplates](https://docs.spring.io/spring-kafka/api/org/springframework/kafka/core/KafkaTemplate.html){:target="_blank" rel="noopener noreferrer"} that you can use.
The `KafkaTemplate` wraps a producer and provides you with some extra handy methods to send data to Kafka topics.
For more information you can check out the reference guide to [use KafkaTemplates](https://docs.spring.io/spring-kafka/reference/html/#kafka-template){:target="_blank" rel="noopener noreferrer"}

It is important that you think about the type of the key and the type of the value, serializer and deserializer.

For this you will need to define a `DefaultKafkaProducerFactory` which will provide you with some default config options like:
* Bootstrap servers: where to find your Kafka.
* Schema registry url: where to find your Avro schema registry.
* Key serializer: the class to be used to serialize your key when writing the message to Kafka.
* Value serializer: the class to be used to serialize your value.

Without those serializers Kafka will not be able to transform your message.

Various other frameworks worth mentioning:
* [Spring Cloud Stream](https://spring.io/projects/spring-cloud-stream){:target="_blank" rel="noopener noreferrer"} allows you to abstract the use of message brokers, it will manage a lot of the bindings for you with Kafka Streams so that it will map a lot of the configuration automatically for you.
* [Kafka Streams](https://kafka.apache.org/documentation/streams/){:target="_blank" rel="noopener noreferrer"}, is a stream processing pipeline, you can use it to built processing pipelines, similar like Spark but less of a hassle to set up, Ktable is the representation of state.

Some final notes about Kafka Streams:
* Kafka streams allows you to visualize your topology in a Directed Acyclic Graph using `TopologyDescroption`, for more info see this [link](https://docs.confluent.io/current/streams/faq.html#visualizing-topologies){:target="_blank" rel="noopener noreferrer"}.

* Kafka streams allows you to do stateful stream processing, so you don't need to care with that. 
Its state store is replicated within Kafka so it can restore this in case of failure.

* Do not forget your `SerDes` when writing out Kafka Streams code.
`Spring Cloud Stream` automatically converts to JSON
But your `Kafka streams` code deals with binary data, so it needs to know how to serialize / deserialize. 
There some predefined by Kafka: `StringSerde`, `LongSerde` and `JsonSerdes`.

It was a very entertaining live coding session which you can rewatch here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/OHjVhTQ3j6g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## How Fast is Spring by [Dave Syer](https://twitter.com/david_syer){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Dave Syer" src="/img/2019-05-16-Spring-IO-2019/dave-syer.jpg"></span>

Beer is coming, it is always nice when a speaker starts his talk with that line ;-).
In this talk Dave is going to talk about performance improvements carried out by the Spring team.

Performance in Java, cold start time of the JVM does take up some time, but once started it is an awesome place.

When starting with measurements an app started up in 1300 milliseconds, that went down to 1200 milliseconds by playing with Spring.
Then to 600 milliseconds with spring functional bean definitions.

The overhead of Spring Boot versus not Spring Boot is currently around 15 milliseconds, a lot of the overhead has been dealt with.

If the classloader has been warmed up, startup is even much much smaller.
With Spring Devtools you have a warm classloader, which reduces your startup time.

Lots of optimizations have happened, heap memory from Spring Boot 1 to 2 went from 10 MB to 6 MB.

[Async profiler](https://github.com/jvm-profiling-tools/async-profiler){:target="_blank" rel="noopener noreferrer"} is a tool you can attach to a running java process.
It has little to no impact on the running performance and shows the calls being executed, the width of the flame is the time it took to run it.
Red and yellow colour means: not in java user memory, ready for garbage collection.
<img alt="Async profiler" src="{{ '/img/2019-05-16-Spring-IO-2019/async-profiler.svg' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

Spring Boot 2.2 has had a big impact.
* Classpath exclusion from spring boot web starters.
* `spring-context-indexer`, but this is marginal, with a lot of beans it will have a bigger impact.
* Spring actuators used to be costly to start up, but has no longer a big impact since spring boot 2.0.
* Use explicit spring.config.location, again no longer that much of an effect.
* Switch of jmx `spring.jmx.enabled = false`, but that is since 2.2 the default setting.
* Make bean definitions lazy by default, in production you might not want this because if lazy it might not fail on startup, but it does make sense to do when developing in order to improve development time.
* Unpack the fat jar and run with explicit classpath `java --jar` is little bit slower than `java --cp`.
* Run the jvm with `-noverify` and consider `-XX:TieredStopAtLevel=1`.
    * all jvm experts will say not to do this.
    * `-noverify` will gain you 40% time with any app- it does not validate byte code, not so wise in production as the JVM will just crash and show you no exception whatsoever.
    * `-XX:TieredStopAtLevel=1` this deals with the JIT compiler, don't optimize that much, will gain you around 10% with any app.
* Import autoconfiguration manually as it is not needed and might give you a small gain.
* Functional bean definitions, but no longer that much needed and they do are a big burden to the developer.

A nice list of tools you can use:
* Benchmarks: [JMH](https://openjdk.java.net/projects/code-tools/jmh/){:target="_blank" rel="noopener noreferrer"}
* Junit and JMH: [microbenchmark-runner](https://github.com/mp911de/microbenchmark-runner){:target="_blank" rel="noopener noreferrer"}
* Profiling: [async-profiler](https://github.com/jvm-profiling-tools/async-profiler){:target="_blank" rel="noopener noreferrer"}
* GC pressure: [JMC aka flight controller](https://www.oracle.com/technetwork/java/javaseproducts/mission-control/java-mission-control-1998576.html){:target="_blank" rel="noopener noreferrer"}
* Quick metrics for any Spring Boot jar: [Benchmark launcher](https://github.com/dsyer/spring-boot-startup-bench){:target="_blank" rel="noopener noreferrer"} from dsyer/spring-boot-startup.
* Classpath manipulation: [Thin Launcher](https://github.com/spring-projects-experimental/spring-boot-thin-launcher){:target="_blank" rel="noopener noreferrer"}
* Profiling with [AspectJ](https://www.eclipse.org/aspectj/){:target="_blank" rel="noopener noreferrer"}

There is a pretty tight correlation between # of classes load vs startup time.

The hibernate team is pretty aware of the GC issues and have done serieus optimizations around it.

Lazy beans: Pay attention to custom beans with expensive `@PostConstruct`, because it tends to be misused like opening files, accessing database, which tends to block up the startup.

You can try `@ImportAutoConfiguration`, but then you need to know which autoconfigurations you need to include, discovering that is the hard part.

Functional Bean Definitions: 
If you use `@Configuration` then you make use of reflection, you can implement an `ApplicationContextInitializer` which makes you reflection free, but is harder to implement.

CPU constrained environments benefit from native images built with [GraalVM](https://www.graalvm.org/docs/reference-manual/aot-compilation/){:target="_blank" rel="noopener noreferrer"}, but you also use a lot like debugging, garbage collection, dynamic classloading which these native images not fully support.
Many issues have been solved with it, some remain but they are getting close, Spring 5.3 might be a target for this.

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/T22i3WAa6dI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by [Matthias Haeussler](https://twitter.com/maeddes){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Matthias Haeussler" src="/img/2019-05-16-Spring-IO-2019/matthias-haeussler.jpg"></span>

Matthias Haeussler is a Cloud Advocate at NovaTec Consulting. He gave a presentation about the differences between Kubernetes and Cloud Foundry
and showed us so with a live Spring Boot application which was deployed on both Kubernetes and Cloud Foundry. 

### Cloud Foundry
To deploy your application on Cloud Foundry, you simply have to run one command: cf push (under the assumption that you have the CLI installed and configured).
This will send your whole codebase to Cloud Foundry, which then builds a container for your application and runs it. Cloud Foundry does not use Docker images, only containers.
The thing with Cloud Foundry is that it uses containers behind-the-scenes, but as a CF user, you don't really notice it. 

### Kubernetes
With Kubernetes, it's a whole different story. You can't just 'run' your application on Kubernetes. You will need a Docker image to run your application, which means
your application must have a Dockerfile inside it. This Docker image must be pushed to a Docker registry, which is then pulled from the registry by Kubernetes and ran with
the specified configuration.

### Conclusion
In Kubernetes, you can configure way more which is a huge benefit, but you also need to know more about the platform, whereas with Cloud Foundry is just one command and your codebase
is pushed, wrapped into a container and ran on the platform, which is way more simple but has less configuration options and thus, can configure less than Kubernetes. 

Kubernetes also requires more dependencies if you want to get more out of it (ex. Helm, Prometheus, Istio, ...), which requires additional maintenance of those dependencies.

The ideal platform is: the simplicity of Cloud Foundry with the functional features of Kubernetes.

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/-WZmhofnfII" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Migrating a modern spring web application to serverless by [Jeroen Sterken](https://twitter.com/jeroensterken){:target="_blank" rel="noopener noreferrer"} and [Wim Creuwels](https://twitter.com/wimcreuwels){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Jeroen Sterken" src="/img/2019-05-16-Spring-IO-2019/jeroen-sterken.jpg"></span>
<span class="image left">
<img class="p-image" alt="Wim Creuwels" src="/img/2019-05-16-Spring-IO-2019/wim-creuwels.jpg"></span>

Is serverless the holy grail? 
These guys explored the possibilities while migrating an existing monolith to serverless at one of their clients. 

Serverless will help your developers focus on the code instead of server management and database setup.
Wim and Jeroen also mention the flip side of the coin.
It's a new technology.
And as is the case with every new technology there is a learning curve.
Developers have to get used to the services that the cloud provider supports.
They need to "think serverless" and model applications as a functions in well defined steps.
Infrastructure has to be modelled using Infrastructure as Code.
A topic on which you can find a great resource on our own blog [here](https://ordina-jworks.github.io/cloud/2019/01/14/Infrastructure-as-code-with-terraform-and-aws-serverless.html){:target="_blank"}

No, serverless is not the holy grail. It is however a great solution for some typical use cases:
* Event driven architectures
* Internet-of-things
* Applications with varying load
* Data analysis
* ..

#### Step functions
<span class="image right"><img class="p-image" alt="Step Function Diagram" src="/img/2019-05-16-Spring-IO-2019/step-function-diagram.png"></span> 
Jeroen and Wim glued there app together using AWS Step Fucntions.
Step Functions is a serverless orchestration service that lets you model your workflow as a series of steps.
Step Functions will keep your Lambda functions free of logic that triggers other Lamba functions.
Instead it will use the output of one Lambda function to trigger the next one, thus progressing towards the next step.  
These steps are made visible by a clear step diagram that shows your workflow.
This diagram allows you to monitor your flow by changing color when something goes wrong.
In case of an error Step Functions will automatically retry.

#### Spring Cloud Functions
We are at SpringIO and we're talking about Serverless Cloud technology so Spring-Cloud-Fuctions cannot be left unmentioned.
Spring Cloud Function is a project by the Spring team that allows you to write cloud platform independent code.
In the process you can keep using familiar spring constructs like beans, autowiring and dependency injection.
You can find great guides on [baeldung.com](https://www.baeldung.com/spring-cloud-function){:target="_blank"} and [spring.io](https://spring.io/projects/spring-cloud-function){:target="_blank"}.
Using Spring Cloud Functions will lower the stepping stone towards Serverless because most Java developers are already familiar with the Spring Framework.

Serverless was already a hot topic.
The fact that Spring now has also jumped on the wagon only makes it hotter.
Definitely keep your eyes open for Serverless in the near future.

You can rewatch their talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/594bKxp0wZI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

# Day 2: Talks & Workshops
## Testing Spring Boot Applications by [Andy Wilkinson](https://twitter.com/ankinson){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Andy Wilkinson" src="/img/2019-05-16-Spring-IO-2019/andy-wilkinson.jpg"></span>

Andy Wilkinson of Pivotal explained us the importance and essence of writing tests in your application to ensure the quality of your services and to reduce the risk 
as much as possible. Of course, having a zero risk functionality is practically impossible, but testing helps you to reduce your risk to a minimum.

But how do you know if a test is 'good'? Almost everyone is basing this on the amount of code coverage in their project, while this does not determine the quality of your tests
or your application logic. When you write tests, you want to think about mistakes that you make or that can be made by the end user.

### Unit testing
When you rewrite your application logic, there's a high chance that you have to rewrite your unit tests as well. 
So make sure that you do not have to spend a lot of time on rewriting your tests when you want to refactor your application
or write extra features.

It's also very important to use descriptive names for your tests. Make sure that your tests are readable by the human eye.
No one wants to read a test that is not clear or creates more confusion (JUnit 5 comes with a display annotation to make a test name
more readable).

When you are familiar with writing unit tests, you've probably also heard of mocking. Unit testing is all round mocking external services
and dependencies. After all, in a unit tests, we are under the assumption that all our external dependencies are working
as it should and functioning how we want it to function. 

### Integration Testing
Andy gave us a detailed explanation of how the various testing annotations work such as @SpringBootTest, which gives us a more Spring Boot way of testing our application
(which means less configuration, hooray!). @SpyBean and @MockBean to create a mock or spy object of a Spring Component, @ActiveProfiles to run your test class with a specific
profile, etc.

#### Testing Against Databases
One of the more appearing problems in Integration Testing is working with a database. Typically, if you want to test against data in a database,
you are going to want to use an in-memory database, which will typically be a HSQLDB or H2 instance.
This is where it gets interesting. You can tell your H2 instance to run in a specific database software mode, such as PostgreSQL.
However, it's not exactly the same as working with a real PostgreSQL server. H2 only interprets the queries that are ran in a PostgreSQL dialect and
tries to convert to its own syntax. This can cause lots of problems, because you are not working with a real Postgres server, and even though H2 is ran with
PostgreSQL compatibility mode, it can still fail with queries that will run perfectly on a real PostgreSQL server. I found this part very interesting, as I had an issue exactly like this a couple of weeks ago.

Andy recommended us to use [TestContainers](https://www.testcontainers.org){:target="_blank" rel="noopener noreferrer"}, which basically have the power to spin up a Docker image of a database of your choice,
so you have the full functionality of a database server. 

### What's next? 
I'm really excited to see what the new Spring Boot versions will have in store to help us write better and clearer tests.
Spring Boot 2.2 will come with full JUnit 5 functionality and thus, will leave JUnit 4 behind. 

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/5sjFn9BsAds" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## How to live in a post-Spring-Cloud-Netflix world by [Olga Maciaszek](https://twitter.com/olga_maciaszek){:target="_blank" rel="noopener noreferrer"} and [Marcin Grzejszczak](https://twitter.com/mgrzejszczak){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Olga Maciaszek" src="/img/2019-05-16-Spring-IO-2019/olga-maciaszek.jpg"></span>

<span class="image left"><img class="p-image" alt="Andy Wilkinson" src="/img/2019-05-16-Spring-IO-2019/marcin-grzejszczak.jpg"></span>

Discovering the new Spring Cloud stack.
That's what this talk was all about.
Very interesting talk if you ask me since we got to see some things in action.
Olga Maciaszek and Marcin Grzejszczak showed us the new solutions for Gateway proxying, circuit breaking and the whole new Spring Cloud stack.

### The world is changing

#### Spring Cloud Load Balancer
Client side load balancing via the `@LoadBalancerClient` annotation.
Use the `@LoadBalanced` annotation as a marker annotation to indicate that a `RibbonLoadBalancingClient` should be used to interact with a service.

#### Spring Cloud Gateway
Via routes your requests are processed to downstream services.
Spring Cloud Gateway is used as a simple way to achieve this routing to your APIs.
You can keep configuring this as code:
```java
return builder.routes()
        .route("users_service_route",
                route -> route.path("/user-service/**")
                        .and()
                        .method(HttpMethod.POST)
                        .filters(filter -> filter.stripPrefix(1)
                        )
                        .uri("lb://user-service")).build();
```

or in your properties file:

```properties
spring:
  application:
    name: proxy
  cloud:
    gateway:
      routes:
      - id: fraud
        uri: lb://fraud-verifier
        predicates:
        - Path=/fraud-verifier/**
        filters:
        - StripPrefix=1
        - name: Retry
          args:
            retries: 3
```

#### Circuit Breaking and Resilience4J
You need a design that is resilient and fault tolerant.

After a number of failed attempts, we can consider a service unavailable.
We will then back off and stop flooding it with requests
we can save system resources for calls which are likely to fail.
And give the other service some time to get back on its feet.

#### Micrometer and Prometheus
Periodically scraping metrics form your services to monitor health.

#### Spring Cloud Config Server 
Externalize your configuration.
You don't have to restart your application to reload your configuration.
Just fetch it from the remote service again.

### Resources
Check out a fully working spring cloud microservices demo here:
[https://github.com/OlgaMaciaszek/spring-cloud-netflix-demo](https://github.com/OlgaMaciaszek/spring-cloud-netflix-demo)
A lot of gratitude to Olga and Marcin for providing a working example that we can play around with to get acquainted with the new services.

You can rewatch their talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/CcL8wrguhhM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Event-Driven Microservices with Axon and Spring Boot: excitingly boring by [Allard Buijze](https://twitter.com/allardbz){:target="_blank" rel="noopener noreferrer"}

<span  class="image left"><img  class="p-image"  alt="Allard Buijze"  src="/img/2019-05-16-Spring-IO-2019/allard-buijze.jpg"></span>

In this presentation, Allard Buijze, Founder and CTO of AxonIQ talks about the advantages of event-driven architectures and shows how easy it is to set up your own event-driven microservices using Axon and Spring Boot.

### What is Axon?

The [Axon framework](https://axoniq.io/){:target="_blank" rel="noopener noreferrer"} is used for building event-driven microservices using Domain-Driven Design, CQRS and Event Sourcing.
It is there to prevent developers from getting lost inside a complex microservice architecture.

##### CQRS
Command and Query Responsibility Segregation is a design pattern where you split the reading and writing of data into seperate models,
use querries for reading the data and commands for updating the data.
While for basic CRUD operations having these models combined might be fine,
once the amount of business logic and amount of querries increases it might become increasingly difficult to manage.


### State Storage vs Event Sourcing
A big part of this presentation is about the advantages of using event sourcing rather than state storage.
Events describe the history of an object rather than just the current state of the object.
It is easy to go through the history and generate the current state while also getting a lot of extra information about the object you would otherwise miss out on when just storing its current state. 
Explicit record that something happened, rather than an implicit record of what happened based on changes that occured.
This also makes testing your application easier because you do not have to rely on state but rather on a series of events to take place or an exception to occur.

<img  class="image fit"  alt="event-sourcing"  src="/img/2019-05-16-Spring-IO-2019/event-sourcing.png" style="margin:0px auto; max-width: 800px;">

### Events
One of the biggest advantages of events is that they remain valuable over time.
They need to be the source of everything in the application and show a true representation of your entities.
Once again, you don't save the state of an aggregate, you can generate the state by replaying the history.

### The power of not now
The power of not now basically means that because you save all the events,
you can generate reports whenever you want based on the captured data.
You don't have to know in advance what data is important to store for later on,
everything is stored.

### Axon Server
[Axon Server](https://docs.axoniq.io/reference-guide/axon-server){:target="_blank" rel="noopener noreferrer"} is a service that distributes your components, manages routing, stores events and provides high availability and observability.
By combining all these otherwise different services into one single easy to configure service,
you make your entire architecture a lot easier to manage than if you were to use for example the Netflix Eureka Discovery Service for communication between microservices,
the MySQL Event Store for storing events and RabbitMQ to handle messaging.
By simply adding the Axon Server dependency and adding some annotations you can use all of these services while keeping the complexity low.

### Tracing
Axon can also manage tracing for you by just adding the Axon tracing and Jaeger to your dependencies.
Where otherwise setting up tracing would be a lot of work having to deal with all kinds of headers,
passing headers along and interpreting them,
Axon tracing takes care of all of this for you.

You can rewatch his talk here:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/iVaD3mdwvx4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## How to secure your Spring apps with Keycloak by [Thomas Darimont](https://twitter.com/thomasdarimont){:target="_blank" rel="noopener noreferrer"}

<span  class="image left"><img  class="p-image"  alt="Thomas Darimont"  src="/img/2019-05-16-Spring-IO-2019/thomas-darimont.jpg"></span>

In this presentation Thomas Darimont talks about what Keycloak is,
what you can do with it and gives a demo of how it works and how you can set it up for your own applications.

### What is Keycloak?
Keycloak is a java based authentication and authorization server. 
It is developed by Red Hat who use it as a base for their enterprise RH-SSO application
on which they provide additional support and documentation.
but is also backed by a large open source community providing additional features,
documentation and bugfixes.

### Keycloak Features

One of the most important features of Keycloak is the Single Sign-On,
and with this the Single logout,
sign into keycloak once to gain access to multiple applications and sign out once to sign out of all applications. Do note though that individual applications can disable this single logout so you might not get logged out of all the applications within a realm.
Another great feature of Keycloak is their multi-factor authentication using one of the known authentication apps like the Google Authenticator
and their support for authentication through social media platforms
such as Facebook, Twitter, Google or even Github. 
Then of course there is the fact that keycloak is completely customizable and extensible,
it comes with a prefered stack on which I will dive into more detail later
but you can get away from this and use your own prefered services albeit with some additional configuration.
Keycloak also comes with an easy to use management console for administrators
and a user management interface where all users can update their user details.
The last feature I am going to discuss are the realms, sets of applications,
users and registered OAuth clients to whom the Keycloak settings will be applied. 
With these realms you can give users specific roles or just authenticate them across multiple applications using the Single Sign-on feature.

### The keycloak prefered stack

By default Keycloak is a WildFly based server with a plain JAX-RS application.
It uses JPA for storing data and Infinispan for the horizontal scaling
of mutliple Keycloack nodes that all distribute information like user sessions between eachother.
Other than that it uses the Freemarker template engine to render for example the login pages
and Jackson 2.x for everything JSON related like the tokens.

### Securing your application

To add keycloak to your applications you have to add a dependency
and you will have to register your application within a Keycloak realm.
Once you have done this,
after doing some configuration within your application and the Keycloak management console,
you will have to authenticate through Keycloak to gain access to your application.

### The authentication process

The following steps describe the Keycloak authentication process:
* Unauthenticated user accesses application
* The application redirects to Keycloak for login
* When login is successful, Keycloak will create an SSO session and will emit cookies
* Keycloak generates random code and redirects user back to application
* Application receives code associated with sign-on session and sends the code back to Keycloak via a seperate channel
* If the code sent back is associated with sign-on session, Keycloak will reply with an access token, a refresh token and an id token
* The application verifies the tokens and associate them with a session
* The user is now logged-in to the application

For more information you can refer to Keycloak's [official documentation](https://www.keycloak.org/documentation.html){:target="_blank" rel="noopener noreferrer"} and you can also watch the original talk itself in the following video:

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/KrOd5wIkqls" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>

## Zero Downtime Migrations with Spring Boot by [Alex Soto](https://twitter.com/alexsotob){:target="_blank" rel="noopener noreferrer"}

<span  class="image left"><img  class="p-image"  alt="Alex Soto"  src="/img/2019-05-16-Spring-IO-2019/alex-soto.jpg"></span>

In this talk, Alex Soto, Software Engineer at Red Hat covers the subject of zero downtime migrations of microservices in Spring Boot.
He covers some of the different deployment techniques, some easy to understand ones and some more advanced techniques for when you are dealing with the persistent states of your applications.

### Dealing with downtime when using microservices

While it is easier and faster to take down a single service rather than a single monolith application,
deployment or redeployment of services will happen a lot more often when using a microservices architecture. 
Another thing to concider is that when you have downtime on a service,
all the services that have a dependency to the service that is offline will no longer work either.
For this reason it is important to minimise downtime of your applications or even have no downtime at all. This is why you need to deploy and release services at different times,
following are some techniques on how to do this.

### Blue/Green deployment

Blue/Green deployment is where you will deploy an updated version of the service you want to replace
and release it by changing the routing from the old one to the new one.
It is important to keep the old service deployed and monitor the new one so that in case of errors
you can easily revert the routing back to keep everything up and running.
The downside of blue/green deployment is that if something goes wrong,
all users are affected if changes happen before reverting the routing.
But of course there is a solution to this problem, Canary releases.

### Canary releases

Canary releases is where you route a small percentage of your traffic through an updated service while the rest keeps using the original service.
This limits the amount of users that might be affected by unwanted changes while you monitor your new application.
As everything goes well you increase the percentage of users untill eventually your entire userbase uses the new service.
All while still having the advantage of Blue/Green deployment to fall back on when things go wrong.

### Mirroring traffic

Mirroring traffic is another deployment technique where you deploy an updated version of a service next to the original one and send your requests to both services.
Only the original service handles requests while the requests to the updated service are just fire and forget requests while you monitor if everything goes according to plan.
Before eventually changing routing to the updated version of the service.

### But what about sticky sessions

When dealing with sticky sessions, which you often see with for example shopping carts on webshops,
your session is linked to a specific service by ip.
Therefore you get rerouted to an updates service you will lose your session.
To counter this problem you can use an in-memory datagrid using for example Redis
and duplicate this across all the services that use these sticky sessions in your cluster.
When you do this your shopping cart will stay even when the service you are accessing changes.

### Dealing with persistent data

While problems with in-memory data are often fixed quite easily,
when dealing with persistent data, zero downtime deployment becomes a little bit more tricky.
Take as an example the changing of a column name,
if you were to use different services acessing the same data but using different column names,
this would cause issues. This exact problem and how to tackle it is shown in the demo,
together with a more in depth explaination of the covered topics in a video of the talk below.

<div class="responsive-video">
    <iframe width="1164" height="655" src="https://www.youtube.com/embed/oziyniXdUe8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<br/>
