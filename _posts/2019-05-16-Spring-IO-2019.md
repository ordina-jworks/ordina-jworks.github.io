---
layout: post
authors: [nick_van_hoof,tom_van_den_bulck]
title: 'Spring IO 2019'
image: /img/2019-05-16-Spring-IO-2019/spring-io.png
tags: [Spring IO,Spring,Java,Conference]
category: Conference
comments: true
---

# Spring IO 2019!

* [Moving from Imperative to Reactive by Paul Harris](#moving-from-imperative-to-reactive-by-paul-harris)
* [Configuration Management with Kubernetes, a Spring Boot use case by Nicholas Frankel](#configuration-management-with-kubernetes-a-spring-boot-use-case-by-nicholas-frankel)
* [Cutting-Edge Continuous Delivery with Spinnaker by Andreas Evers](#cutting-edge-continuous-delivery-with-spinnaker-by-andreas-evers)
* [Using Java Modules in Practice with Spring Boot by Jaap Coomans](#using-java-modules-in-practice-with-spring-boot-by-jaap-coomans)
* [Stream Processing with the Spring Framework by Josh Long and Viktor Gamov](#stream-processing-with-the-spring-framework-by-josh-long-and-viktor-gamov)
* [How Fast is Spring by Dave Syer](#how-fast-is-spring-by-dave-syer)




# Day 1: Talks & Workshops

## Moving from Imperative to Reactive by [Paul Harris](https://twitter.com/twoseat){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Paul Harris" src="/img/TODO.png"></span>

First customer of the new spring boot reboot for the cloudfoundry java client
Made all the mistakes you can make with reactive programming

He is not a java jedi - poohbear /wizard/ninja but one like us: 
one of us

Reactive manifesto -> link
responsive, resilient, elastic, message driven
A manifesto is nice, but it does not compile

Reactive Streams -> link 
publisher, subscriber, subscription and processor

Project reactor -> link to my blogpost
reactor-core, reactor-netty and reactor-test => add links

Key concepts of flux and mono -> add link and images

(All code in 1 class, easy for demo, but if it makes you sad think about it as legacy code we will fix later)

Make it reactive:
*  add dependency to spring boot starter webflux (reactive to webmvc), you should not need to change anything to keep it running, unless you have used specific server features
* convert return list to a flux (of the simplest call)
* Convert return from repos to use mono / flux, like with findById
    * Mono justOrEmpty in order to deal with an optional, return either the item or if not present just empty mono.
    * use .switchIfEmpty to return a proper error response
        * Errors are values for monos and flux, within reactive programming, you just need to pass them along as any other value.
    * In order to return a Flux
        * use fromIterable, then .filter
* Creating a new employee, little bit more trickier
    * create employee
        * a flatmap because you want to do something with the values which get returned.
    *then map, which calls repository.save, That returns a mono 

Reactive starts of complicated, but will become easier when you have used it more often. It doesn't have that much of different methods you can use, so all in all quite easy to wrap your head around.



## Configuration Management with Kubernetes, a Spring Boot use case by [Nicholas Frankel](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Works at exoscale -> link (european cloud provider in Switzerland)
Consequences  of the cloud act.

Adding a banner to an environment is not a bad idea, so you know that your are know on development or staging, ... 

Configuration management: scripts, puppet, chef, ansible, ... 
Custom scripts, does not scale.
Currently servers are no longer pets, but are cattle -> if it is bad, kill it and revive

Inject properties with environment specific config.

Whatever happens in a Kubernetes cluster stays in a kubernetes cluster -> make it a quote


You can use the maven jib-plugin, made by google -> add a link
It will create a docker image based on your code.
By default it will push your message to the dockerHub registry (can be configured).
Just classes and jar get dockerized => so you do not create first a jar of the application.

This has some consequences though => you will need to create a new docker image / version for every other new environment because we are using a profile.

You can circumvent that by using system properties
=> but then you need to work with command line bellow the abstraction of jib, which is not that nice.

Environment variables do make it nicer.
But if you have variables depending on each other you might make mistakes

Kubernetes has a config map which allows you to define to variables .
make use of configMapRef:

But these maps will need to be maintained and configured.

You can use an initcontainer (-> add link)
The initContainer has a single command line which reads in from a git repo and make it availeble
It will load in properties in a property folder so they can be used the container.

Blog: blog.frankel.ch
https://git.io/fhNj9
https://bit.ly/2VKNPIS

Questions:
What makes it better then Spring Cloud Config?
It is another approach, not better or worse.

Is changing values @runtime taken into consideration?
No, it is not picked up automatically, but you can setup a Kubernetes watcher to automatically restart the containers when a config change has been committed.

How do you handle passwords?
Default config map with secrets.



## Cutting-Edge Continuous Delivery with Spinnaker by [Andreas Evers](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}


The battle of speed vs confidence
Updating in production vs manual testing

Integration testing: automatic deploying the production setup to acceptance might mean that you already test and old obsolete topology because microservice can change that quickly.

Contract testing does not cover all the aspects needed to provide confidence.

Spinnaker -> add link and short description
Does support openstack and lot of other cloud environments.

Spring Cloud had something, Spring Cloud Pipelines, which lately got rebranded to Cloud Pipelines -> add link

Where do your store the state of you cloud infrastructure
Cloud Pipelines used tags for that (but that might miss manual deployments), but also different versions live across regions and clouds are difficult to manage.

Spinnaker gives you that overview and provides you with the pipelines needed to manage all those releases.

Under the hood it is all spring boot microservices.
Halyard: bill of materials for the different microservices of spinnaker and helps you with the deploy of spinnaker.

Major contributors are Netflix, Google, Microsoft, Pivotal, ...

Spinnaker integrates well with your CI environments, so just keep using these.

Metrics are required for the monitor and the analysis of the canary testing.

Cloud Deployments are complex:
* different regions
* different accounts for your environments (production, acceptance, ... )

Teams want an easy road into the cloud, no complexity to deploy.

Easy rollbacks are important, which Spinnaker makes easy to do.
(do pay attention to db changes and provide rollback scenario's)

Deploy during the right timeframes => deploy when you traffic is the least.
But Spinnaker can also take into consideration the working hours of the team responsible for the application / service.

Various deployment strategies exist -> add links for this.

You can define a pipeline to deploy into production -> add picture I took
For every step you will have a series to steps, per step multiple tasks and every task has some operations which are executed.
A lot of these steps are very specific per cloud, Spinnaker tends to abstract that away.

Canary analysis -> add link to spinnaker explanation
It is possible to integrate chaos monkeys into your pipelines -> add link to chaos monkey

Spinnakers makes it possible to go fast but still do it safely.
* Automated rollbacks
* Deployment windows
* Cluster locking
* Traffic guards, extra safeguards which can be configured
* Manual judgements: which makes use of the human "gut" feeling, which a computer does not have.
* ...

Rick & Morty demo -> add picture of meeseeks and pickle rick??

Highlander strategy: there can only be one

When doing canary, it is wise to startup a baseline as well (the old version)
so that you have the same baseline (startup, memory, ... ) between the old and the new version.

You will look at jvm metrics like memory, cpu, ... but you can also define business metrics like load in time of the app, ... 

When the canary fails, it will just rollback and restore the previous version.

Canary testing allows you to test with real users and real production data, but at the same time it reduces the possible impact of your new version on end users, since not all users will encounter this new version.

Question:
How do you connect spinnaker with DataDog?
Just provide the api keys to spinnaker.



## Using Java Modules in Practice with Spring Boot by [Jaap Coomans](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Current state of the module system.
Most tooling support is good (maven, ide, ...)
In frameworks the adoption is very low.
For developers it is even lower.

It's like eating vegetables, we know it is healthy.

Legacy system developed on for 15 years by hundreds of developers, with legacy code within legacy.
-> add picture

At runtime via reflection calls from the repository to the services, circular dependencies gallore.

What challenges will you face
* Split packages (package with same name exposed by more then 1 module)
* Automatic modules (plain jar on your module path and thus interpreted as a module) => exports and opens all packages, reads all other modules and it derives it module name from filename => without special characters and other ones 
    * In maven central 3500 collisions are possible
    * you can circumvent this with the Automatic-Module-Name in your manifest file

There is a split package issue in the legacy mongo client, not with the new one.
But Spring Data Mongo relies heavily on the legacy mongo client instead of the new one.

The first steps are just to minimize the problems you might encounter.
Step 1: upgrade your dependencies => will minimize conflicts
Step 2: use JDK11+ 
Step 3: compile JDK11+ 

Step 4: prepare the module structure, so you can go from module to module
(Don't start with 1 module from the start)

Step 5: add module descriptors bottom-up (it has the least dependencies, so good to get started at that level)

Create a module-info.java
module a-module-name {
    exports a.package.name;
    exports a.package.name.but.other;
}

You might need a newer version of the maven surefire plugin.


For Spring => they have defined automatic module names

module {
    requires java.xml.bind
    
    exports ...

    opens ...
}

requires: defines the module that you need
exports will be the only stuff that you expose
opens this means that this will make a module available for reflection (that you might need for jaxb)



Step 6: add descriptor to main jar
Then you get all the benefits of the module system, but also you will encounter all the hurdles
This will also get you into runtime errors.

Export the main class and the application module

You can also define
{
    requires transitive a-module-name
}

You can pretty much copy paste the errors you get about opening the modules.

ClassNotFound Exceptions => add to requires

When you stop getting the same error, that is good, then you have reached the next phase ... 


Spring does use some of the internals of the jdk
which can be fixed by:
requires jdk.unsupported
=> this does help you out for know, but the module name alone screams that you should not use it.

Jackson also uses deep reflection of your domain and such, so that might give you HTTP 415 errors and the likes.

Then we had a section about Kotlin.
Kotlin supports modules and it can use the java module descriptors just fine.

Intellij does the conversion to kotlin
It does tend to mess up the maven configuration, for a mixed project (java and kotlin) you will need another config -> add link / and/or google

Lessons learned
* go bottom up
* test all paths on every step (because you will encounter runtime errors)
* the logs have the answer (the jvm gives you a good indication with errors by the module system)
* it still involves pioneering



## Stream Processing with the Spring Framework by [Josh Long](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"} and [Viktor Gamov](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Dangerous to think Kafka as a message queue.

It tends to become a vine of data within your organization to move data around.
It becomes a database.

Use Spring Cloud Stream Kafka Streams.

They use Avro, need to add support for Avro in the dependencies.
Confluent support for scheme registry.

Confluent cloud platform
Via confluent cli you can start several stuff together (kafka, zookeeper and avro)
First run confluent destroy (makes sure that there is no old data present)
> confluent start schema-registry -> check this command

The Avro format is used to generate the java pojo, avro is used as a contract.
For that you have an avro maven plugin.

Send data to the broker and gather insights.

Kafka templates can be used -> at link
Think about the type of the key and the type of the value, serializer and deserializer.

You kan use a command to list your topics on zookeeper.

Kafka does not care what you put in there.
People also put files in there, not wise and you might to consider your life choices, but you can do it.

The schema will not always stored with the image -> need to check this

Spring Cloud Stream allows you to abstract the use of message brokers -> add link

Kafka Streams, is a stream processing pipeline. -> add link
You can use it to built processing pipelines, similar like Spark but less of a hassle to set up.
Ktable is the representation of state.

Spring Cloud Stream will manage a lot of the bindings for you with Kafka Streams so that it will map a lot of the configuration automatically for you.

Directed Acyclic Graph, kafka streams allows you to define your topology. -> TODO add link and rewrite somewhat
There exists a visualizer tool to show you that topology -> add link, something from confluent

Kafka steams allows you to do stateful stream processing, so you don't need to care with that.
Its state store is replicated within Kafka so it can restore this in case of failure.

Do not forget your SerDes when writing out Kafka Streams code, because Spring Cloud Stream automatically converts to JSON, but your kafka streams code deals with binary data => so it needs to know how to serialize / deserialize.
There are StringSerde, LongSerde and JsonSerdes predefined.


## How Fast is Spring by [Dave Syer](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Beer is coming

Cold start time of the JVM does take up some time, once started it is an awesome place though.

When starting with measurements app started up in 1300 milliseconds, that went down to 1200 milliseconds by playing with spring.
600 milliseconds with spring functional bean definitions -> find link

Difference between Spring Boot and not Spring Boot is currently around 15 milliseconds, a lot of the overhead has been dealt with.

If the classloader has been warmed up, startup is even much much smaller.

With Spring Devtools you have a warm classloader, which reduces your startup time.

Lots of optimizations have happened, heap memory from Spring Boot 1 > 2: 10 MB - 6 MB

Async profiler is a tool you can attack to a running java process -> add link.
It has little to no impact on the running performance.
It shows the calls being executed, width of the flame is the time it took to run it.
Red / yellow colour means - not in java user memory => garbage collection.
Lots of focus has been around this for Spring Boot.

Spring Boot 2.2 has had a big impact.
* classpath exclusion from spring boot web starters
* spring-context-indexer - but this is marginal, with a lot of beans it will have a bigger impact
* spring actuators used to be costly to start up, but has no longer a big impact since spring boot 2.0
* use explicit spring.config.location (again no longer that much of an effect)
* switch of jmx spring.jmx.enabled = false (but that is since 2.2 the default setting)
* make bean definitions lazy by default (in production you might not want this because if lazy it might not fail on startup, but it does make sense to do when developing in order to improve development time)
* unpack the fat jar and run with explicit classpath java --jar is little bit slower than java --cp
* run the jvm with -noverify and consider -XX:TieredStopAtLevel=1
    * all jvm experts will say not to do this
    * -noverify will gain you 40% time with any app- it does not validate byte code (not so wise in production) - the JVM will just crash and show you no exception whatsoever.
    * -XX:TieredStopAtLevel=1: this deals with the JIT compiler, don't optimize that much, will gain you around 10% with any app
* Import autoconfiguration manually (not needed, might give you a small gain )
* functional bean definitions, but no longer that much needed and they do are a big burden to the developer.

Tools => retrieve info from the picture I made
* JMH: benchmarks
* microbenchmark-runner (JMH and junit)
* async-profiler
* JMC aka flight controller, GC pressure
* ...

There is a pretty tight correlation between # of classes load vs startup time.

The hibernate team is pretty aware of the GC issues and have done serieus optimizations around it.

Lazy beans:
Pay attention to custom beans with expensive @PostConstruct, because it tends to be misused like opening files, accessing database => which tends to block up the startup.
Profiling with AspectJ

You can try @ImportAutoConfiguration, but then you need to know which autoconfigurations you need to include - discovering that is the hard part.

Functional Bean Definitions
@Configuration => but it uses reflection
You can implement a ApplicationContextInitializer which makes you reflection free, but is harder to implement.

CPU constrained environments

Native images can be very efficient, Graal VM -> add a link
But you also use a lot like debugging, garbage collection, dynamic classloading, ... 
Many issues have been solved with it, but they are getting close - Spring 5.3 might be a target for this.




# Day 2: Talks & Workshops

