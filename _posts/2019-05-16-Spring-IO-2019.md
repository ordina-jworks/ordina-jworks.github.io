---
layout: post
authors: [nick_van_hoof,tom_van_den_bulck, yolan_vloeberghs]
title: 'Spring IO 2019'
image: /img/2019-05-16-Spring-IO-2019/spring-io.png
tags: [Spring IO,Spring,Java,Conference]
category: Conference
comments: true
---

# Spring IO 2019!

* [Moving from Imperative to Reactive by Paul Harris](#moving-from-imperative-to-reactive-by-paul-harris)
* [Configuration Management with Kubernetes, a Spring Boot use case by Nicholas Frankel](#configuration-management-with-kubernetes-a-spring-boot-use-case-by-nicholas-frankel)
* [Cutting-Edge Continuous Delivery with Spinnaker by Andreas Evers](#cutting-edge-continuous-delivery-with-spinnaker-by-andreas-evers)
* [Using Java Modules in Practice with Spring Boot by Jaap Coomans](#using-java-modules-in-practice-with-spring-boot-by-jaap-coomans)
* [Stream Processing with the Spring Framework by Josh Long and Viktor Gamov](#stream-processing-with-the-spring-framework-by-josh-long-and-viktor-gamov)
* [How Fast is Spring by Dave Syer](#how-fast-is-spring-by-dave-syer)
* [Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by Matthias Haeussler](#kubernetes-and-or-cloud-foundry-how-to-run-your-spring-boot-microservices-on-state-of-the-art-cloud-platforms-by-matthias-haeussler)
* [Moving an enterprise application to serverless by Jeroen Sterken and Wim Creuwels](#moving-an-enterprise-application-to-serverless)
* [Testing Spring Boot Applications by Andy Wilkinson](#testing-spring-boot-applications)

# Day 1: Talks & Workshops

## Moving from Imperative to Reactive by [Paul Harris](https://twitter.com/twoseat){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Paul Harris" src="/img/TODO.png"></span>

First customer of the new spring boot reboot for the cloudfoundry java client
Made all the mistakes you can make with reactive programming

He is not a java jedi - poohbear /wizard/ninja but one like us: 
one of us

Reactive manifesto -> link
responsive, resilient, elastic, message driven
A manifesto is nice, but it does not compile

Reactive Streams -> link 
publisher, subscriber, subscription and processor

Project reactor -> link to my blogpost
reactor-core, reactor-netty and reactor-test => add links

Key concepts of flux and mono -> add link and images

(All code in 1 class, easy for demo, but if it makes you sad think about it as legacy code we will fix later)

Make it reactive:
*  add dependency to spring boot starter webflux (reactive to webmvc), you should not need to change anything to keep it running, unless you have used specific server features
* convert return list to a flux (of the simplest call)
* Convert return from repos to use mono / flux, like with findById
    * Mono justOrEmpty in order to deal with an optional, return either the item or if not present just empty mono.
    * use .switchIfEmpty to return a proper error response
        * Errors are values for monos and flux, within reactive programming, you just need to pass them along as any other value.
    * In order to return a Flux
        * use fromIterable, then .filter
* Creating a new employee, little bit more trickier
    * create employee
        * a flatmap because you want to do something with the values which get returned.
    *then map, which calls repository.save, That returns a mono 

Reactive starts of complicated, but will become easier when you have used it more often. It doesn't have that much of different methods you can use, so all in all quite easy to wrap your head around.



## Configuration Management with Kubernetes, a Spring Boot use case by [Nicholas Frankel](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Works at exoscale -> link (european cloud provider in Switzerland)
Consequences  of the cloud act.

Adding a banner to an environment is not a bad idea, so you know that your are know on development or staging, ... 

Configuration management: scripts, puppet, chef, ansible, ... 
Custom scripts, does not scale.
Currently servers are no longer pets, but are cattle -> if it is bad, kill it and revive

Inject properties with environment specific config.

Whatever happens in a Kubernetes cluster stays in a kubernetes cluster -> make it a quote


You can use the maven jib-plugin, made by google -> add a link
It will create a docker image based on your code.
By default it will push your message to the dockerHub registry (can be configured).
Just classes and jar get dockerized => so you do not create first a jar of the application.

This has some consequences though => you will need to create a new docker image / version for every other new environment because we are using a profile.

You can circumvent that by using system properties
=> but then you need to work with command line bellow the abstraction of jib, which is not that nice.

Environment variables do make it nicer.
But if you have variables depending on each other you might make mistakes

Kubernetes has a config map which allows you to define to variables .
make use of configMapRef:

But these maps will need to be maintained and configured.

You can use an initcontainer (-> add link)
The initContainer has a single command line which reads in from a git repo and make it availeble
It will load in properties in a property folder so they can be used the container.

Blog: blog.frankel.ch
https://git.io/fhNj9
https://bit.ly/2VKNPIS

Questions:
What makes it better then Spring Cloud Config?
It is another approach, not better or worse.

Is changing values @runtime taken into consideration?
No, it is not picked up automatically, but you can setup a Kubernetes watcher to automatically restart the containers when a config change has been committed.

How do you handle passwords?
Default config map with secrets.



## Cutting-Edge Continuous Delivery with Spinnaker by [Andreas Evers](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}


The battle of speed vs confidence
Updating in production vs manual testing

Integration testing: automatic deploying the production setup to acceptance might mean that you already test and old obsolete topology because microservice can change that quickly.

Contract testing does not cover all the aspects needed to provide confidence.

Spinnaker -> add link and short description
Does support openstack and lot of other cloud environments.

Spring Cloud had something, Spring Cloud Pipelines, which lately got rebranded to Cloud Pipelines -> add link

Where do your store the state of you cloud infrastructure
Cloud Pipelines used tags for that (but that might miss manual deployments), but also different versions live across regions and clouds are difficult to manage.

Spinnaker gives you that overview and provides you with the pipelines needed to manage all those releases.

Under the hood it is all spring boot microservices.
Halyard: bill of materials for the different microservices of spinnaker and helps you with the deploy of spinnaker.

Major contributors are Netflix, Google, Microsoft, Pivotal, ...

Spinnaker integrates well with your CI environments, so just keep using these.

Metrics are required for the monitor and the analysis of the canary testing.

Cloud Deployments are complex:
* different regions
* different accounts for your environments (production, acceptance, ... )

Teams want an easy road into the cloud, no complexity to deploy.

Easy rollbacks are important, which Spinnaker makes easy to do.
(do pay attention to db changes and provide rollback scenario's)

Deploy during the right timeframes => deploy when you traffic is the least.
But Spinnaker can also take into consideration the working hours of the team responsible for the application / service.

Various deployment strategies exist -> add links for this.

You can define a pipeline to deploy into production -> add picture I took
For every step you will have a series to steps, per step multiple tasks and every task has some operations which are executed.
A lot of these steps are very specific per cloud, Spinnaker tends to abstract that away.

Canary analysis -> add link to spinnaker explanation
It is possible to integrate chaos monkeys into your pipelines -> add link to chaos monkey

Spinnakers makes it possible to go fast but still do it safely.
* Automated rollbacks
* Deployment windows
* Cluster locking
* Traffic guards, extra safeguards which can be configured
* Manual judgements: which makes use of the human "gut" feeling, which a computer does not have.
* ...

Rick & Morty demo -> add picture of meeseeks and pickle rick??

Highlander strategy: there can only be one

When doing canary, it is wise to startup a baseline as well (the old version)
so that you have the same baseline (startup, memory, ... ) between the old and the new version.

You will look at jvm metrics like memory, cpu, ... but you can also define business metrics like load in time of the app, ... 

When the canary fails, it will just rollback and restore the previous version.

Canary testing allows you to test with real users and real production data, but at the same time it reduces the possible impact of your new version on end users, since not all users will encounter this new version.

Question:
How do you connect spinnaker with DataDog?
Just provide the api keys to spinnaker.



## Using Java Modules in Practice with Spring Boot by [Jaap Coomans](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Current state of the module system.
Most tooling support is good (maven, ide, ...)
In frameworks the adoption is very low.
For developers it is even lower.

It's like eating vegetables, we know it is healthy.

Legacy system developed on for 15 years by hundreds of developers, with legacy code within legacy.
-> add picture

At runtime via reflection calls from the repository to the services, circular dependencies gallore.

What challenges will you face
* Split packages (package with same name exposed by more then 1 module)
* Automatic modules (plain jar on your module path and thus interpreted as a module) => exports and opens all packages, reads all other modules and it derives it module name from filename => without special characters and other ones 
    * In maven central 3500 collisions are possible
    * you can circumvent this with the Automatic-Module-Name in your manifest file

There is a split package issue in the legacy mongo client, not with the new one.
But Spring Data Mongo relies heavily on the legacy mongo client instead of the new one.

The first steps are just to minimize the problems you might encounter.
Step 1: upgrade your dependencies => will minimize conflicts
Step 2: use JDK11+ 
Step 3: compile JDK11+ 

Step 4: prepare the module structure, so you can go from module to module
(Don't start with 1 module from the start)

Step 5: add module descriptors bottom-up (it has the least dependencies, so good to get started at that level)

Create a module-info.java
module a-module-name {
    exports a.package.name;
    exports a.package.name.but.other;
}

You might need a newer version of the maven surefire plugin.


For Spring => they have defined automatic module names

module {
    requires java.xml.bind
    
    exports ...

    opens ...
}

requires: defines the module that you need
exports will be the only stuff that you expose
opens this means that this will make a module available for reflection (that you might need for jaxb)



Step 6: add descriptor to main jar
Then you get all the benefits of the module system, but also you will encounter all the hurdles
This will also get you into runtime errors.

Export the main class and the application module

You can also define
{
    requires transitive a-module-name
}

You can pretty much copy paste the errors you get about opening the modules.

ClassNotFound Exceptions => add to requires

When you stop getting the same error, that is good, then you have reached the next phase ... 


Spring does use some of the internals of the jdk
which can be fixed by:
requires jdk.unsupported
=> this does help you out for know, but the module name alone screams that you should not use it.

Jackson also uses deep reflection of your domain and such, so that might give you HTTP 415 errors and the likes.

Then we had a section about Kotlin.
Kotlin supports modules and it can use the java module descriptors just fine.

Intellij does the conversion to kotlin
It does tend to mess up the maven configuration, for a mixed project (java and kotlin) you will need another config -> add link / and/or google

Lessons learned
* go bottom up
* test all paths on every step (because you will encounter runtime errors)
* the logs have the answer (the jvm gives you a good indication with errors by the module system)
* it still involves pioneering



## Stream Processing with the Spring Framework by [Josh Long](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"} and [Viktor Gamov](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Dangerous to think Kafka as a message queue.

It tends to become a vine of data within your organization to move data around.
It becomes a database.

Use Spring Cloud Stream Kafka Streams.

They use Avro, need to add support for Avro in the dependencies.
Confluent support for scheme registry.

Confluent cloud platform
Via confluent cli you can start several stuff together (kafka, zookeeper and avro)
First run confluent destroy (makes sure that there is no old data present)
> confluent start schema-registry -> check this command

The Avro format is used to generate the java pojo, avro is used as a contract.
For that you have an avro maven plugin.

Send data to the broker and gather insights.

Kafka templates can be used -> at link
Think about the type of the key and the type of the value, serializer and deserializer.

You kan use a command to list your topics on zookeeper.

Kafka does not care what you put in there.
People also put files in there, not wise and you might to consider your life choices, but you can do it.

The schema will not always stored with the image -> need to check this

Spring Cloud Stream allows you to abstract the use of message brokers -> add link

Kafka Streams, is a stream processing pipeline. -> add link
You can use it to built processing pipelines, similar like Spark but less of a hassle to set up.
Ktable is the representation of state.

Spring Cloud Stream will manage a lot of the bindings for you with Kafka Streams so that it will map a lot of the configuration automatically for you.

Directed Acyclic Graph, kafka streams allows you to define your topology. -> TODO add link and rewrite somewhat
There exists a visualizer tool to show you that topology -> add link, something from confluent

Kafka steams allows you to do stateful stream processing, so you don't need to care with that.
Its state store is replicated within Kafka so it can restore this in case of failure.

Do not forget your SerDes when writing out Kafka Streams code, because Spring Cloud Stream automatically converts to JSON, but your kafka streams code deals with binary data => so it needs to know how to serialize / deserialize.
There are StringSerde, LongSerde and JsonSerdes predefined.


## How Fast is Spring by [Dave Syer](https://twitter.com/TODO){:target="_blank" rel="noopener noreferrer"}

Beer is coming

Cold start time of the JVM does take up some time, once started it is an awesome place though.

When starting with measurements app started up in 1300 milliseconds, that went down to 1200 milliseconds by playing with spring.
600 milliseconds with spring functional bean definitions -> find link

Difference between Spring Boot and not Spring Boot is currently around 15 milliseconds, a lot of the overhead has been dealt with.

If the classloader has been warmed up, startup is even much much smaller.

With Spring Devtools you have a warm classloader, which reduces your startup time.

Lots of optimizations have happened, heap memory from Spring Boot 1 > 2: 10 MB - 6 MB

Async profiler is a tool you can attack to a running java process -> add link.
It has little to no impact on the running performance.
It shows the calls being executed, width of the flame is the time it took to run it.
Red / yellow colour means - not in java user memory => garbage collection.
Lots of focus has been around this for Spring Boot.

Spring Boot 2.2 has had a big impact.
* classpath exclusion from spring boot web starters
* spring-context-indexer - but this is marginal, with a lot of beans it will have a bigger impact
* spring actuators used to be costly to start up, but has no longer a big impact since spring boot 2.0
* use explicit spring.config.location (again no longer that much of an effect)
* switch of jmx spring.jmx.enabled = false (but that is since 2.2 the default setting)
* make bean definitions lazy by default (in production you might not want this because if lazy it might not fail on startup, but it does make sense to do when developing in order to improve development time)
* unpack the fat jar and run with explicit classpath java --jar is little bit slower than java --cp
* run the jvm with -noverify and consider -XX:TieredStopAtLevel=1
    * all jvm experts will say not to do this
    * -noverify will gain you 40% time with any app- it does not validate byte code (not so wise in production) - the JVM will just crash and show you no exception whatsoever.
    * -XX:TieredStopAtLevel=1: this deals with the JIT compiler, don't optimize that much, will gain you around 10% with any app
* Import autoconfiguration manually (not needed, might give you a small gain )
* functional bean definitions, but no longer that much needed and they do are a big burden to the developer.

Tools => retrieve info from the picture I made
* JMH: benchmarks
* microbenchmark-runner (JMH and junit)
* async-profiler
* JMC aka flight controller, GC pressure
* ...

There is a pretty tight correlation between # of classes load vs startup time.

The hibernate team is pretty aware of the GC issues and have done serieus optimizations around it.

Lazy beans:
Pay attention to custom beans with expensive @PostConstruct, because it tends to be misused like opening files, accessing database => which tends to block up the startup.
Profiling with AspectJ

You can try @ImportAutoConfiguration, but then you need to know which autoconfigurations you need to include - discovering that is the hard part.

Functional Bean Definitions
@Configuration => but it uses reflection
You can implement a ApplicationContextInitializer which makes you reflection free, but is harder to implement.

CPU constrained environments

Native images can be very efficient, Graal VM -> add a link
But you also use a lot like debugging, garbage collection, dynamic classloading, ... 
Many issues have been solved with it, but they are getting close - Spring 5.3 might be a target for this.

## Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms
<span class="image left"><img class="p-image" alt="Matthias Haeussler" src="/img/2019-05-16-Spring-IO-2019/matthias-haeussler.jpg"></span>
Matthias Haeussler is a Cloud Advocate at NovaTec Consulting. He gave a presentation about the differences between Kubernetes and Cloud Foundry
and showed us so with a live Spring Boot application which was deployed on both Kubernetes and Cloud Foundry. 

### Cloud Foundry
To deploy your application on Cloud Foundry, you simply have to run one command: cf push (under the assumption that you have the CLI installed and configured).
This will send your whole codebase to Cloud Foundry, which then builds a container for your application and runs it. Cloud Foundry does not use Docker images, only containers.
The thing with Cloud Foundry is that it uses containers behind-the-scenes, but as a CF user, you don't really notice it. 

### Kubernetes
With Kubernetes, it's a whole different story. You can't just 'run' your application on Kubernetes. You will need a Docker image to run your application, which means
your application must have a Dockerfile inside it. This Docker image must be pushed to a Docker registry, which is then pulled from the registry by Kubernetes and ran with
the specified configuration.

### Conclusion
In Kubernetes, you can configure way more which is a huge benefit, but you also need to know more about the platform, whereas with Cloud Foundry is just one command and your codebase
is pushed, wrapped into a container and ran on the platform, which is way more simple but has less configuration options and thus, can configure less than Kubernetes. 

Kubernetes also requires more dependencies if you want to get more out of it (ex. Helm, Prometheus, Istio, ...), which requires additional maintenance of those dependencies.

The ideal platform is: the simplicity of Cloud Foundry with the functional features of Kubernetes.

## Moving an enterprise application to serverless
<span class="image left"><img class="p-image" alt="Jeroen Sterken" src="/img/2019-05-16-Spring-IO-2019/jeroen-sterken.jpg"></span>

Is serverless the wholy grail? 
These guys explored the possibilities while migrating an existing monolith to serverless at one of their clients. 

Serverless will help your developers focus on the code instead of server management and database setup.
Wim and Jeroen also mention the flip side of the coin.
It's a new technology.
And as is the case with every new technology there is a learning curve.
Developers have to get used to the services that the cloud provider supports.
They need to "think serverless" and model applications as a functions in well defined steps.
Infrastructure has to be modelled using Infrastructure as Code.
A topic on which you can find a great resource on our own blog [here](https://ordina-jworks.github.io/cloud/2019/01/14/Infrastructure-as-code-with-terraform-and-aws-serverless.html){:target="_blank"}

No, serverless is not the wholy grail. It is however a great solution for some typical use cases:
* Event driven architectures
* Internet-of-things
* Applications with varying load
* Data analysis
* ..

#### Step functions 
<span class="image right"><img class="p-image" alt="Step Function Diagram" src="/img/2019-05-16-Spring-IO-2019/step-function-diagram.png"></span> 
Jeroen and Wim glued there app together using AWS Step Fucntions.
Step Functions is a serverless orchestration service that lets you model your workflow as a series of steps.
Step Functions will keep your Lambda functions free of logic that triggers a other Lamba functions.
Instead it will use the output of one Lambda function to trigger the next one, thus progressing towards the next step.  
These steps are made visible by a clear step diagram that shows your workflow.
This diagram allows you to monitor your flow by changing color when something goes wrong.
In case of an error Step Functions will automatically retry.

#### Spring Cloud Functions
We are on SpringIO and we're talking about Serverless Cloud technology so Spring-Cloud-Fuctions cannot be left unmentioned.
Spring Cloud Function is a project by the Spring team that allows you to write cloud platform independent code.
In the process you can keep using familiar spring constructs like beans, autowiring and dependency injection.
You can find great guides on [baeldung.com](https://www.baeldung.com/spring-cloud-function){:target="_blank"} and [spring.io](https://spring.io/projects/spring-cloud-function){:target="_blank"}.
Using Spring Cloud Functions will lower the stepping stone towards Serverless because most Java developers are already familiar with the Spring Framework.

Serverless was already a hot topic.
The fact that Spring now has also jumped on the wagon only makes it hotter.
Definitely keep your eyes open for Serverless in the near future.


# Day 2: Talks & Workshops
## Testing Spring Boot Applications
<span class="image left"><img class="p-image" alt="Andy Wilkinson" src="/img/2019-05-16-Spring-IO-2019/andy-wilkinson.jpg"></span>

Andy Wilkinson of Pivotal explained us the importance and essence of writing tests in your application to ensure the quality of your services and to reduce the risk 
as much as possible. Of course, having a zero risk functionality is practically impossible, but testing helps you to reduce your risk to a minimum.

But how do you know if a test is 'good'? Almost everyone is based on the amount of code coverage in their project, while this does not determine the quality of your tests
or your application logic. When you write tests, you want to think about mistakes that you make or that can be made by the end user.

### Unit testing
When you rewrite your application logic, there's a high chance that you have to rewrite your unit tests as well. 
So make sure that you do not have to spend a lot of time on rewriting your tests when you want to refactor your application
or write extra features.

It's also very important to use descriptive names for your tests. Make sure that your tests are readable by the human eye.
No one wants to read a test that is not clear or creates more confusion (JUnit 5 comes with a display annotation to make a test name
more readable).

When you are familiar with writing unit tests, you've probably also heard of mocking. Unit testing is all round mocking external services
and dependencies. After all, in a unit tests, we are under the assumption that all our external dependencies are working
as it should and functioning how we want it to function. 

<span class="image right"><img class="p-image" alt="Test" src="/img/2019-05-16-Spring-IO-2019/test.jpeg"></span>

### Integration Testing
Andy gave us a detailed explanation of how the various testing annotations work such as @SpringBootTest, which gives us a more Spring Boot way of testing our application
(which means less configuration, hooray!). @SpyBean and @MockBean to create a mock or spy object of a Spring Component, @ActiveProfiles to run your test class with a specific
profile, etc.

#### Testing Against Databases
One of the more appearing problems in Integration Testing is working with a database. Typically, if you want to test against data in a database,
you are going to want to use an in-memory database, which will typically be a HSQLDB or H2 instance.
This is where it gets interesting. You can tell your H2 instance to run in a specific database software mode, such as PostgreSQL.
However, it's not exactly the same as working with a real PostgreSQL server. H2 only interprets the queries that are ran in a PostgreSQL dialect and
tries to convert to its own syntax. This can cause lots of problems, because you are not working with a real Postgres server, and even though H2 is ran with
PostgreSQL compatibility mode, it can still fail with queries that will run perfectly on a real PostgreSQL server. I found this part very interesting, as I had an issue exactly like this a couple of weeks ago.

Andy recommended us to use [TestContainers](https://www.testcontainers.org), which basically has the power to spin up a Docker image of a database of your choice,
so you have the full functionality of a database server. 

### What's next? 
I'm really excited of what the new Spring Boot versions will have in store to help us write better and clearer tests.
Spring Boot 2.2 will come with full JUnit 5 functionality and thus, will leave JUnit 4 behind. 
