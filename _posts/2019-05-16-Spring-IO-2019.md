---
layout: post
authors: [nick_van_hoof,tom_van_den_bulck, yolan_vloeberghs, jago_staes]
title: 'Spring IO 2019'
image: /img/2019-05-16-Spring-IO-2019/spring-io.png
tags: [Spring IO,Spring,Java,Conference]
category: Conference
comments: true
---

# Spring IO 2019!
* [Moving from Imperative to Reactive by Paul Harris](#moving-from-imperative-to-reactive-by-paul-harris)
* [Configuration Management with Kubernetes, a Spring Boot use case by Nicolas Frankel](#configuration-management-with-kubernetes-a-spring-boot-use-case-by-nicolas-frankel)
* [Building better monoliths - Modulithic Applications with Spring Boot by Oliver Drotbohm](#building-better-monoliths-modulithic-applications-with-spring-boot-by-oliver-drothbohm)
* [Cutting-Edge Continuous Delivery: Automated Canary Analysis through Spring based Spinnaker by Andreas Evers](#cutting-edge-continuous-delivery-automated-canary-analysis-through-spring-based-spinnaker-by-andreas-evers)
* [Using Java Modules in Practice with Spring Boot by Jaap Coomans](#using-java-modules-in-practice-with-spring-boot-by-jaap-coomans)
* [Stream Processing with the Spring Framework by Josh Long and Viktor Gamov](#stream-processing-with-the-spring-framework-by-josh-long-and-viktor-gamov)
* [How Fast is Spring by Dave Syer](#how-fast-is-spring-by-dave-syer)
* [Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by Matthias Haeussler](#kubernetes-and-or-cloud-foundry-how-to-run-your-spring-boot-microservices-on-state-of-the-art-cloud-platforms-by-matthias-haeussler)
* [Moving an enterprise application to serverless by Jeroen Sterken and Wim Creuwels](#moving-an-enterprise-application-to-serverless)
* [Testing Spring Boot Applications by Andy Wilkinson](#testing-spring-boot-applications-by-andy-wilkinson)
* [Building better monoliths by Oliver Drotbohm](#building-better-monoliths)
* [How to live in a post-Spring-Cloud-Netflix world](#how-to-live-in-a-post-spring-cloud-netflix-world)
* [Event-Driven Microservices with Axon and Spring Boot: excitingly boring by Allard Buijze](#event-driven-microservices-with-axon-and-spring-boot-excitingly-boring-by-allard-buijze)
* [How to secure your Spring apps with Keycloak by Thomas Darimont](#how-to-secure-your-spring-apps-with-keycloak-by-thomas-darimont)

# Day 1: Talks & Workshops

## Moving from Imperative to Reactive by [Paul Harris](https://twitter.com/twoseat){:target="_blank" rel="noopener noreferrer"}

<span class="image left"><img class="p-image" alt="Paul Harris" src="/img/2019-05-16-Spring-IO-2019/paul-harris.jpg"></span>

When development was started on the Cloudfoundry java client, [Spring Reactor](https://projectreactor.io/) was rebooted at the same time, which means that they became their very first customer.

He made all the mistakes you can make with reactive programming.

He is not a java jedi, a java wizard, a ninja, ... but one like us: one of us.

It al started with the [Reactive Manifesto](https://www.reactivemanifesto.org/){:target="_blank" rel="noopener noreferrer"} in 2013, which came up with 4 ideas for reactive applications:
* Responsive: it should feel as if the application is progressing, with for example, some feedback.
* Resilient: if a particular part of you application fails, the remainder should be able to cope with that. 
* Elastic: make the most out of the resources available to the application.
* Message Driven: more message driven then event-driven.

A manifesto is nice, but it does not compile.

The next step was [Reactive Streams](https://www.reactive-streams.org/){:target="_blank" rel="noopener noreferrer"} which defined a set of interfaces for how we might deal with reactive streaming situations.
You can disginguish 4 interfaces:
* [Publisher](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Publisher.html){:target="_blank" rel="noopener noreferrer"} which emits 'things', signals.
* [Subscriber](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Subscriber.html){:target="_blank" rel="noopener noreferrer"} listens to those signals.
* [Subscription](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Subscription.html){:target="_blank" rel="noopener noreferrer"} a subscriber together with a publisher gives you a subscription.
* [Processor](https://www.reactive-streams.org/reactive-streams-1.0.2-javadoc/org/reactivestreams/Processor.html){:target="_blank" rel="noopener noreferrer"} is a a combination of a publisher and a subscriber that allows you to process data.

The intention of Reactive Streams was that more usefull real world implementations would follow, on of these is Spring Reactor, for a good introduction to Spring Reactor you can read our [blogpost](https://ordina-jworks.github.io/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html) about it.

Spring Reactor contains various reactive frameworks, the three big ones are:
* [Reactor-Core](https://projectreactor.io/docs/core/release/api/){:target="_blank" rel="noopener noreferrer"} - the basic provision.
* [Reactor-Netty](https://projectreactor.io/docs/netty/release/api/){:target="_blank" rel="noopener noreferrer"}  - the reactive implementation of Netty. 
* [Reactor-Test](https://projectreactor.io/docs/test/release/api/){:target="_blank" rel="noopener noreferrer"} - which is a bunch of really good usefull methods for testing reactive streams.

Before Paul dove in the code he first explained [Mono](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Mono.html) and [Flux](https://projectreactor.io/docs/core/release/api/reactor/core/publisher/Flux.html).

### a Mono
Which is a Reactive Streams `Publisher` which will either element one element or return an error.
<img alt="a Mono" src="{{ '/img/2019-05-16-Spring-IO-2019/mono.svg' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

### a Flux
Is also a `Publisher` which will emit zero to N elements and then complete.
<img alt="a Flux" src="{{ '/img/2019-05-16-Spring-IO-2019/flux.svg' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

For the demo all code will in 1 class, easy for the demo, but if it makes you sad think about it as legacy code we will fix later ;-).

In order to make your the legacy spring mvc application reactive, the following steps were taken:
*  Add dependency to [spring boot starter webflux](https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-webflux), which is the reactive variant of webvmc, you should not need to change anything to keep it running, unless you have used specific server features.
* Convert the return of a `List` to a `Flux` (of the simplest call)
* Convert return from repos to use `Mono` or `Flux`, like with `.findById`.
    * Mono `.justOrEmpty` in order to deal with an optional, return either the item or if not present just empty mono.
    * Use `.switchIfEmpty` to return a proper error response, as errors are values for monos and flux, within reactive programming, you just need to pass them along as any other value.
    * In order to return a `Flux`: use `fromIterable`, then `.filter`
* Creating a new employee is a little bit more trickier
    * Create employee as a `Flatmap` because you want to do something with the values which get returned.
    * Then use `.map`, which calls `repository.save()`, That returns a `Mono`. 

A nice conclusion is that Reactive starts of complicated, but it will become easier when you have used it more often. 
It doesn't have that much of different methods you can use, so all in all it is quite easy to wrap your head around.

You can rewatch his talk on [youtube](https://www.youtube.com/watch?v=vSHNBgY7MGA){:target="_blank" rel="noopener noreferrer"}.

## Configuration Management with Kubernetes, a Spring Boot use case by [Nicolas Frankel](https://twitter.com/nicolas_frankel){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Nicolas Frankel" src="/img/2019-05-16-Spring-IO-2019/nicolas-frankel.jpg"></span>
Nicolas Frankel is a Developer Advocate who works for [Exoscale](https://www.exoscale.com/){:target="_blank" rel="noopener noreferrer"}, a European cloud hosting provider.

In this session, he explained how to correctly configure each environment with its own parameters and settings, how it always should be done.

There are traditional configuration management tools such as Chef, Ansible, Puppet, ... . 
But what is the point?

Docker images are and should always be immutable. 
They should be configurable depending on the environment where we want to run our image in.
A Docker image should be able to run in different environments without problems, this is where [Kubernetes](https://kubernetes.io/){:target="_blank" rel="noopener noreferrer"} comes in. 
Kubernetes can easily configure and parameterize each Docker image to run in different environments.

One thing to remember is that you should make sure that you are working in the correct environment. 
Nicolas likes to add banners to the page to know in which environment you are currently working. 
For example, if you are working in the development environment, then you might want to show a big blue 'Development' banner, while in production you would like a big, red, blinking one.

This can all be done with the power of Kubernetes and immutable images. 
You can simply declare your environment variables in Kubernetes, then you can inject your environment in your Spring Boot application.

There are 3 ways to access your environment variables in Spring Boot: profiles, `@Value` or `@ConfigurationProperties`.

To get started in Kubernetes, you have to create a few Kubernetes objects:
1. (Optional) A [Namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)
2. A [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
3. A [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) / [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) 

<span class="image right"><img class="p-image" alt="Environments" src="/img/2019-05-16-Spring-IO-2019/environments.png"></span>
In the deployment, you can give the arguments based on the environment that you want to spin up.
With a [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/), you can combine your environment variables that belong to each other (ex. database settings, AWS keys, ...).
Once done defining the ConfigMap, you can import the ConfigMap in your Deployment declaration.

What's also very interesting is that you can declare your environment variables in a seperate Git repository with the use of an [initContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/). 
Ofcourse, you can already do this with [Spring Cloud Config](https://spring.io/projects/spring-cloud-config). 
This is just an alternative on the Kubernetes side of configuration management.

If you want to read more about Nicolas Frankel's work, you can read his blog [here](https://blog.frankel.ch).

You can rewatch his talk on [youtube](https://www.youtube.com/watch?v=cTWu_DLqDt4){:target="_blank" rel="noopener noreferrer"}.

## Building better monoliths - Modulithic Applications with Spring Boot by [Oliver Drotbohm](https://twitter.com/odrotbohm){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Oliver Drotbohm" src="/img/2019-05-16-Spring-IO-2019/oliver-drotbohm.jpg"></span>
I was very interested in this talk because we’re currently working at a client where we’re seeing some of the limitations of a microservice architecture. 
We are currently considering merging multiple of them into a more coarse grained architecture containing multiple more 'monolithic' applications.
 
### Microservice architectures also have disadvantages! 
The talk starts off explaining some of the key differences between microservice and classic monoliths from an architectural point of view.

The key take away here is that although there’s an advantage in terms of architecture degradation, it is harder to accidentally call another microservice than to call a method on another bounded context in a monolith.
This advantage comes at a cost, you lose compile-time safety which makes it harder to refactor than a Monolith and it is harder to test the whole system.
These are especially disadvantageous in the early stages of the project when it is not clear yet what the correct bounded contexts are.

### The Modulith
It might thus be useful to consider starting off with a well structured Monolith before considering evolving towards microservices.
But how do we avoid having our architecture degrade quickly?
Enter the Modulith; a modulith is basically a Monolith with multiple modules with well defined dependencies in it.
To achieve this the speaker demonstrates a 'Moduliths' tool that he’s in process of developing for the Spring framework.

The idea is to use a package structure convention and enforce it with tests using reflection.
In this package structure convention only (public members of) the root package of each module are accessible to other
modules, it's considered the API package of that module, subpackages are considered internal. 
There's more to this tool however; another problem of modularizing your application is that you typically want to do integration testing on the bounds of your modules.
The 'Moduliths' tool allows to bootstrap your module alone or in various configurations with specific module dependencies for integration testing. 
To top it off there's support to generate plantUML diagrams for documentation purposes!
For more details take a look at [https://github.com/odrotbohm/moduliths](https://github.com/odrotbohm/moduliths){:target="_blank" rel="noopener noreferrer"}.

### Alternative approaches
There are of course other ways to divide your application into Modules and maintain the architecture: 

#### Multiple artifacts(gradle/maven modules).
The biggest disadvantage here according to Oliver is that you can quickly get an explosion of artifacts 
and it can become kind of verbose with all the configuration(pom/build.gradle) files.
Additionally the artifacts are redundant since we're planning on deploying everything together anyway.
There's also no support to dynamically compose modules for tests since the dependencies are typically statically defined.

This was actually the way we were considering handling it, the big advantage here in our eyes is that reflection can be avoided 
and the architecture can be verified at compile-time. The good news is it's possible to combine it with the 'Moduliths' approach, 
which might be useful for integration testing.

#### Java(9+) Module System
Could be used but it's certainly not designed for this,
it definitely doesn't have any support for dynamically composing your modules for testing.

#### External tools
JQAssist, Sonargraph, jDepend... . 
These are powerful tools but usually run during the build making the feedback loop bigger.

### Wrapping it up

The Moduliths approach explained in this talk gives us a nice intermediate step towards a better architecture.
It alleviates some of the biggest problems with monoliths without introducing new ones using a more complex architecture like microservices!

You can rewatch his talk on [youtube](https://www.youtube.com/watch?v=bVaiTPYlHFE){:target="_blank" rel="noopener noreferrer"}.

## Cutting-Edge Continuous Delivery: Automated Canary Analysis through Spring based Spinnaker by [Andreas Evers](https://twitter.com/andreasevers){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Andreas Evers" src="/img/2019-05-16-Spring-IO-2019/andreas-evers.jpg"></span>

>The ultimate goal of continuous delivery is to deploy software quickly and automatically.
This can only be achieved if we are able to push new code without fear.

Andreas saw through the years to there are two opposing forces that try to battle it out.
On one hand you have speed while in the other hand there is confidence.
Like, updating in production without testing will give you a great speed, but not much confidence.

With microservices using integration tests on an acceptance environment might mean that you test and already old obsolete topology because microservices can change that quickly.
Contract testing does not cover all the aspects needed to provide confidence as it does not test behaviour.

A good alternative where Andreas is going to talk about is Canary Analysis, in order to do so lets first introduce Spinnaker:
[Spinnaker](https://www.spinnaker.io/){:target="_blank" rel="noopener noreferrer"} is an open source, multi-cloud continuous delivery platform created at Netlfix.
It supports a lot of cloud environments like: Openstack, AWS, Google Cloud, Microsoft Azure, Cloud Foundry, ... 
Major contributors are Netflix, Google, Microsoft, Pivotal, ...

Spring had a project, Spring Cloud Pipelines, which lately got rebranded to [Cloud Pipelines](https://github.com/CloudPipelines/){:target="_blank" rel="noopener noreferrer"}.
In that project they try to get an overview of your current microservices landscape, your cloud infrastructure, by tracking Git tags.
But that is quite limited and a manual deploy which bypasses the pipeline might not get a tag at all.
Also different versions cross regions and clouds are hard to manage.

For that Cloud Pipelines is now moving towards Spinnaker, as Spinnaker gives you that overview and provides you with the pipelines needed to manage all those releases.
Under the hood of Spinnaker it is all spring boot microservices.

An important component is [Halyard](https://github.com/spinnaker/halyard){:target="_blank" rel="noopener noreferrer"}: a bill of materials for the different microservices of spinnaker and it helps you with the deploy of spinnaker.
Spinnaker integrates well with your CI environments, so just keep using these.


Cloud Deployments are complex:
* different regions
* different accounts for your environments (production, acceptance, ... )

Teams want an easy road into the cloud, no complexity to deploy.

Easy rollbacks are important, which Spinnaker makes easy to do, do pay attention to database changes and provide rollback scenarios.

With Spinnaker you can make sure that you deploy during the right timeframes, like deploy when your traffic is the least.
But Spinnaker can also take into consideration the working hours of the team responsible for the application / service.

Various deployment strategies exist:
* Red/Black
* Rolling Red/Black
* [Canary analysis](https://www.spinnaker.io/guides/user/canary/){:target="_blank" rel="noopener noreferrer"} 
<img alt="deployment strategies" src="{{ '/img/2019-05-16-Spring-IO-2019/deployment-strategies.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 700px;">

You can define a pipeline to deploy into production:
<img alt="pipeline tasks" src="{{ '/img/2019-05-16-Spring-IO-2019/pipeline-tasks.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

For every stage you will have a series of steps, per step multiple tasks and every task has some operations which are executed.
A lot of these steps are very specific per cloud, Spinnaker tends to abstract that away.


Spinnakers makes it possible to go fast but still do it safely.
* Automated rollbacks
* Deployment windows
* Cluster locking
* Traffic guards, extra safeguards which can be configured
* Manual judgements: which makes use of the human "gut" feeling, which a computer does not have.
* ...

Andreas had a made a Rick & Morty demo, where we will either deploy a blue version or a green version:
<img alt="meeseeks" src="{{ '/img/2019-05-16-Spring-IO-2019/meeseeks.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">
vs 
<img alt="pickle rick" src="{{ '/img/2019-05-16-Spring-IO-2019/picklerick.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 500px;">

He first started with the Highlander strategy: there can only be one.
It is going to make sure that only one deployment will be alive.

When doing canary, it is wise to startup a baseline as well, the old version, so that you have the same baseline between the old and the new version.
You will look at jvm metrics like memory, cpu, etc. 
But you can also define business metrics like startup time of the app, response times, ... 

When the canary fails, it will just rollback and restore the previous version.
Spinnaker will decide if the canary fails by looking at the statistics it gathered.

Canary testing allows you to test with real users and real production data, but at the same time it reduces the possible impact of your new version on end users, since not all users will encounter this new version.

You can rewatch his talk on [youtube](https://www.youtube.com/watch?v=uB35m60GAZw){:target="_blank" rel="noopener noreferrer"}.

## Using Java Modules in Practice with Spring Boot by [Jaap Coomans](https://twitter.com/JaapCoomans){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Jaap Coomans" src="/img/2019-05-16-Spring-IO-2019/jaap-coomans.jpg"></span>

Current state of the module system.
Most tooling support is good (maven, ide, ...)
In frameworks the adoption is very low.
For developers it is even lower.

It's like eating vegetables, we know it is healthy.

Legacy system developed on for 15 years by hundreds of developers, with legacy code within legacy.
-> add picture

At runtime via reflection calls from the repository to the services, circular dependencies gallore.

What challenges will you face
* Split packages (package with same name exposed by more then 1 module)
* Automatic modules (plain jar on your module path and thus interpreted as a module) => exports and opens all packages, reads all other modules and it derives it module name from filename => without special characters and other ones 
    * In maven central 3500 collisions are possible
    * you can circumvent this with the Automatic-Module-Name in your manifest file

There is a split package issue in the legacy mongo client, not with the new one.
But Spring Data Mongo relies heavily on the legacy mongo client instead of the new one.

The first steps are just to minimize the problems you might encounter.
Step 1: upgrade your dependencies => will minimize conflicts
Step 2: use JDK11+ 
Step 3: compile JDK11+ 

Step 4: prepare the module structure, so you can go from module to module
(Don't start with 1 module from the start)

Step 5: add module descriptors bottom-up (it has the least dependencies, so good to get started at that level)

Create a module-info.java
module a-module-name {
    exports a.package.name;
    exports a.package.name.but.other;
}

You might need a newer version of the maven surefire plugin.


For Spring => they have defined automatic module names

module {
    requires java.xml.bind
    
    exports ...

    opens ...
}

requires: defines the module that you need
exports will be the only stuff that you expose
opens this means that this will make a module available for reflection (that you might need for jaxb)



Step 6: add descriptor to main jar
Then you get all the benefits of the module system, but also you will encounter all the hurdles
This will also get you into runtime errors.

Export the main class and the application module

You can also define
{
    requires transitive a-module-name
}

You can pretty much copy paste the errors you get about opening the modules.

ClassNotFound Exceptions => add to requires

When you stop getting the same error, that is good, then you have reached the next phase ... 


Spring does use some of the internals of the jdk
which can be fixed by:
requires jdk.unsupported
=> this does help you out for know, but the module name alone screams that you should not use it.

Jackson also uses deep reflection of your domain and such, so that might give you HTTP 415 errors and the likes.

Then we had a section about Kotlin.
Kotlin supports modules and it can use the java module descriptors just fine.

Intellij does the conversion to kotlin
It does tend to mess up the maven configuration, for a mixed project (java and kotlin) you will need another config -> add link / and/or google

Lessons learned
* go bottom up
* test all paths on every step (because you will encounter runtime errors)
* the logs have the answer (the jvm gives you a good indication with errors by the module system)
* it still involves pioneering

You can rewatch his talk on [youtube](https://www.youtube.com/watch?v=UFBH7gHJkb4){:target="_blank" rel="noopener noreferrer"}.


## Stream Processing with the Spring Framework by [Josh Long](https://twitter.com/starbuxman){:target="_blank" rel="noopener noreferrer"} and [Viktor Gamov](https://twitter.com/gamussa){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Josh Long" src="/img/2019-05-16-Spring-IO-2019/josh-long.jpg"></span>
<span class="image left"><img class="p-image" alt="Viktor Gamov" src="/img/2019-05-16-Spring-IO-2019/viktor-gamov.jpg"></span>

Dangerous to think Kafka as a message queue.

It tends to become a vine of data within your organization to move data around.
It becomes a database.

Use Spring Cloud Stream Kafka Streams.

They use Avro, need to add support for Avro in the dependencies.
Confluent support for scheme registry.

Confluent cloud platform
Via confluent cli you can start several stuff together (kafka, zookeeper and avro)
First run confluent destroy (makes sure that there is no old data present)
> confluent start schema-registry -> check this command

The Avro format is used to generate the java pojo, avro is used as a contract.
For that you have an avro maven plugin.

Send data to the broker and gather insights.

Kafka templates can be used -> at link
Think about the type of the key and the type of the value, serializer and deserializer.

You kan use a command to list your topics on zookeeper.

Kafka does not care what you put in there.
People also put files in there, not wise and you might to consider your life choices, but you can do it.

The schema will not always stored with the image -> need to check this

Spring Cloud Stream allows you to abstract the use of message brokers -> add link

Kafka Streams, is a stream processing pipeline. -> add link
You can use it to built processing pipelines, similar like Spark but less of a hassle to set up.
Ktable is the representation of state.

Spring Cloud Stream will manage a lot of the bindings for you with Kafka Streams so that it will map a lot of the configuration automatically for you.

Directed Acyclic Graph, kafka streams allows you to define your topology. -> TODO add link and rewrite somewhat
There exists a visualizer tool to show you that topology -> add link, something from confluent

Kafka steams allows you to do stateful stream processing, so you don't need to care with that.
Its state store is replicated within Kafka so it can restore this in case of failure.

Do not forget your SerDes when writing out Kafka Streams code, because Spring Cloud Stream automatically converts to JSON, but your kafka streams code deals with binary data => so it needs to know how to serialize / deserialize.
There are StringSerde, LongSerde and JsonSerdes predefined.


## How Fast is Spring by [Dave Syer](https://twitter.com/david_syer){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Dave Syer" src="/img/2019-05-16-Spring-IO-2019/dave-syer.jpg"></span>

Beer is coming

Cold start time of the JVM does take up some time, once started it is an awesome place though.

When starting with measurements app started up in 1300 milliseconds, that went down to 1200 milliseconds by playing with spring.
600 milliseconds with spring functional bean definitions -> find link

Difference between Spring Boot and not Spring Boot is currently around 15 milliseconds, a lot of the overhead has been dealt with.

If the classloader has been warmed up, startup is even much much smaller.

With Spring Devtools you have a warm classloader, which reduces your startup time.

Lots of optimizations have happened, heap memory from Spring Boot 1 > 2: 10 MB - 6 MB

Async profiler is a tool you can attack to a running java process -> add link.
It has little to no impact on the running performance.
It shows the calls being executed, width of the flame is the time it took to run it.
Red / yellow colour means - not in java user memory => garbage collection.
Lots of focus has been around this for Spring Boot.

Spring Boot 2.2 has had a big impact.
* classpath exclusion from spring boot web starters
* spring-context-indexer - but this is marginal, with a lot of beans it will have a bigger impact
* spring actuators used to be costly to start up, but has no longer a big impact since spring boot 2.0
* use explicit spring.config.location (again no longer that much of an effect)
* switch of jmx spring.jmx.enabled = false (but that is since 2.2 the default setting)
* make bean definitions lazy by default (in production you might not want this because if lazy it might not fail on startup, but it does make sense to do when developing in order to improve development time)
* unpack the fat jar and run with explicit classpath java --jar is little bit slower than java --cp
* run the jvm with -noverify and consider -XX:TieredStopAtLevel=1
    * all jvm experts will say not to do this
    * -noverify will gain you 40% time with any app- it does not validate byte code (not so wise in production) - the JVM will just crash and show you no exception whatsoever.
    * -XX:TieredStopAtLevel=1: this deals with the JIT compiler, don't optimize that much, will gain you around 10% with any app
* Import autoconfiguration manually (not needed, might give you a small gain )
* functional bean definitions, but no longer that much needed and they do are a big burden to the developer.

Tools => retrieve info from the picture I made
* JMH: benchmarks
* microbenchmark-runner (JMH and junit)
* async-profiler
* JMC aka flight controller, GC pressure
* ...

There is a pretty tight correlation between # of classes load vs startup time.

The hibernate team is pretty aware of the GC issues and have done serieus optimizations around it.

Lazy beans:
Pay attention to custom beans with expensive @PostConstruct, because it tends to be misused like opening files, accessing database => which tends to block up the startup.
Profiling with AspectJ

You can try @ImportAutoConfiguration, but then you need to know which autoconfigurations you need to include - discovering that is the hard part.

Functional Bean Definitions
@Configuration => but it uses reflection
You can implement a ApplicationContextInitializer which makes you reflection free, but is harder to implement.

CPU constrained environments

Native images can be very efficient, Graal VM -> add a link
But you also use a lot like debugging, garbage collection, dynamic classloading, ... 
Many issues have been solved with it, but they are getting close - Spring 5.3 might be a target for this.

## Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by [Matthias Haeussler](https://twitter.com/maeddes){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Matthias Haeussler" src="/img/2019-05-16-Spring-IO-2019/matthias-haeussler.jpg"></span>
Matthias Haeussler is a Cloud Advocate at NovaTec Consulting. He gave a presentation about the differences between Kubernetes and Cloud Foundry
and showed us so with a live Spring Boot application which was deployed on both Kubernetes and Cloud Foundry. 

### Cloud Foundry
To deploy your application on Cloud Foundry, you simply have to run one command: cf push (under the assumption that you have the CLI installed and configured).
This will send your whole codebase to Cloud Foundry, which then builds a container for your application and runs it. Cloud Foundry does not use Docker images, only containers.
The thing with Cloud Foundry is that it uses containers behind-the-scenes, but as a CF user, you don't really notice it. 

### Kubernetes
With Kubernetes, it's a whole different story. You can't just 'run' your application on Kubernetes. You will need a Docker image to run your application, which means
your application must have a Dockerfile inside it. This Docker image must be pushed to a Docker registry, which is then pulled from the registry by Kubernetes and ran with
the specified configuration.

### Conclusion
In Kubernetes, you can configure way more which is a huge benefit, but you also need to know more about the platform, whereas with Cloud Foundry is just one command and your codebase
is pushed, wrapped into a container and ran on the platform, which is way more simple but has less configuration options and thus, can configure less than Kubernetes. 

Kubernetes also requires more dependencies if you want to get more out of it (ex. Helm, Prometheus, Istio, ...), which requires additional maintenance of those dependencies.

The ideal platform is: the simplicity of Cloud Foundry with the functional features of Kubernetes.

## Moving an enterprise application to serverless
<span class="image left"><img class="p-image" alt="Jeroen Sterken" src="/img/2019-05-16-Spring-IO-2019/jeroen-sterken.jpg"></span>

Is serverless the holy grail? 
These guys explored the possibilities while migrating an existing monolith to serverless at one of their clients. 

Serverless will help your developers focus on the code instead of server management and database setup.
Wim and Jeroen also mention the flip side of the coin.
It's a new technology.
And as is the case with every new technology there is a learning curve.
Developers have to get used to the services that the cloud provider supports.
They need to "think serverless" and model applications as a functions in well defined steps.
Infrastructure has to be modelled using Infrastructure as Code.
A topic on which you can find a great resource on our own blog [here](https://ordina-jworks.github.io/cloud/2019/01/14/Infrastructure-as-code-with-terraform-and-aws-serverless.html){:target="_blank"}

No, serverless is not the holy grail. It is however a great solution for some typical use cases:
* Event driven architectures
* Internet-of-things
* Applications with varying load
* Data analysis
* ..

#### Step functions 
<span class="image right"><img class="p-image" alt="Step Function Diagram" src="/img/2019-05-16-Spring-IO-2019/step-function-diagram.png"></span> 
Jeroen and Wim glued there app together using AWS Step Fucntions.
Step Functions is a serverless orchestration service that lets you model your workflow as a series of steps.
Step Functions will keep your Lambda functions free of logic that triggers other Lamba functions.
Instead it will use the output of one Lambda function to trigger the next one, thus progressing towards the next step.  
These steps are made visible by a clear step diagram that shows your workflow.
This diagram allows you to monitor your flow by changing color when something goes wrong.
In case of an error Step Functions will automatically retry.

#### Spring Cloud Functions
We are at SpringIO and we're talking about Serverless Cloud technology so Spring-Cloud-Fuctions cannot be left unmentioned.
Spring Cloud Function is a project by the Spring team that allows you to write cloud platform independent code.
In the process you can keep using familiar spring constructs like beans, autowiring and dependency injection.
You can find great guides on [baeldung.com](https://www.baeldung.com/spring-cloud-function){:target="_blank"} and [spring.io](https://spring.io/projects/spring-cloud-function){:target="_blank"}.
Using Spring Cloud Functions will lower the stepping stone towards Serverless because most Java developers are already familiar with the Spring Framework.

Serverless was already a hot topic.
The fact that Spring now has also jumped on the wagon only makes it hotter.
Definitely keep your eyes open for Serverless in the near future.


# Day 2: Talks & Workshops
## Testing Spring Boot Applications by [Andy Wilkinson](https://twitter.com/ankinson){:target="_blank" rel="noopener noreferrer"}
<span class="image left"><img class="p-image" alt="Andy Wilkinson" src="/img/2019-05-16-Spring-IO-2019/andy-wilkinson.jpg"></span>

Andy Wilkinson of Pivotal explained us the importance and essence of writing tests in your application to ensure the quality of your services and to reduce the risk 
as much as possible. Of course, having a zero risk functionality is practically impossible, but testing helps you to reduce your risk to a minimum.

But how do you know if a test is 'good'? Almost everyone is basing this on the amount of code coverage in their project, while this does not determine the quality of your tests
or your application logic. When you write tests, you want to think about mistakes that you make or that can be made by the end user.

### Unit testing
When you rewrite your application logic, there's a high chance that you have to rewrite your unit tests as well. 
So make sure that you do not have to spend a lot of time on rewriting your tests when you want to refactor your application
or write extra features.

It's also very important to use descriptive names for your tests. Make sure that your tests are readable by the human eye.
No one wants to read a test that is not clear or creates more confusion (JUnit 5 comes with a display annotation to make a test name
more readable).

When you are familiar with writing unit tests, you've probably also heard of mocking. Unit testing is all round mocking external services
and dependencies. After all, in a unit tests, we are under the assumption that all our external dependencies are working
as it should and functioning how we want it to function. 

<span class="image right"><img class="p-image" alt="Test" src="/img/2019-05-16-Spring-IO-2019/test.jpeg"></span>

### Integration Testing
Andy gave us a detailed explanation of how the various testing annotations work such as @SpringBootTest, which gives us a more Spring Boot way of testing our application
(which means less configuration, hooray!). @SpyBean and @MockBean to create a mock or spy object of a Spring Component, @ActiveProfiles to run your test class with a specific
profile, etc.

#### Testing Against Databases
One of the more appearing problems in Integration Testing is working with a database. Typically, if you want to test against data in a database,
you are going to want to use an in-memory database, which will typically be a HSQLDB or H2 instance.
This is where it gets interesting. You can tell your H2 instance to run in a specific database software mode, such as PostgreSQL.
However, it's not exactly the same as working with a real PostgreSQL server. H2 only interprets the queries that are ran in a PostgreSQL dialect and
tries to convert to its own syntax. This can cause lots of problems, because you are not working with a real Postgres server, and even though H2 is ran with
PostgreSQL compatibility mode, it can still fail with queries that will run perfectly on a real PostgreSQL server. I found this part very interesting, as I had an issue exactly like this a couple of weeks ago.

Andy recommended us to use [TestContainers](https://www.testcontainers.org), which basically have the power to spin up a Docker image of a database of your choice,
so you have the full functionality of a database server. 

### What's next? 
I'm really excited to see what the new Spring Boot versions will have in store to help us write better and clearer tests.
Spring Boot 2.2 will come with full JUnit 5 functionality and thus, will leave JUnit 4 behind. 

## How to live in a post-Spring-Cloud-Netflix world

Discovering the new Spring Cloud stack.
That's what this talk was all about.
Very interesting talk if you ask me since we got to see some things in action.
Olga Maciaszek and Marcin Grzejszczak showed us the new solutions for Gateway proxying, circuit breaking and the whole new Spring Cloud stack.

### The world is changing

#### Spring Cloud Load Balancer
Client side load balancing via the `@LoadBalancerClient` annotation.
Use the `@LoadBalanced` annotation as a marker annotation to indicate that a `RibbonLoadBalancingClient` should be used to interact with a service.

#### Spring Cloud Gateway
Via routes your requests are processed to downstream services.
Spring Cloud Gateway is used as a simple way to achieve this routing to your APIs.
You can keep configuring this as code:
```java
return builder.routes()
        .route("users_service_route",
                route -> route.path("/user-service/**")
                        .and()
                        .method(HttpMethod.POST)
                        .filters(filter -> filter.stripPrefix(1)
                        )
                        .uri("lb://user-service")).build();
```

or in your properties file:

```properties
spring:
  application:
    name: proxy
  cloud:
    gateway:
      routes:
      - id: fraud
        uri: lb://fraud-verifier
        predicates:
        - Path=/fraud-verifier/**
        filters:
        - StripPrefix=1
        - name: Retry
          args:
            retries: 3
```

#### Circuit Breaking and Resilience4J
You need a design that is resilient and fault tolerant.

After a number of failed attempts, we can consider a service unavailable.
We will then back off and stop flooding it with requests
we can save system resources for calls which are likely to fail.
And give the other service some time to get back on its feet.

#### Micrometer and Prometheus
Periodically scraping metrics form your services to monitor health.

#### Spring Cloud Config Server 
Externalize your configuration.
You don't have to restart your application to reload your configuration.
Just fetch it from the remote service again.

### Resources
Check out a fully working spring cloud microservices demo here:
[https://github.com/OlgaMaciaszek/spring-cloud-netflix-demo](https://github.com/OlgaMaciaszek/spring-cloud-netflix-demo)
A lot of gratitude to Olga and Marcin for providing a working example that we can play around with to get acquainted with the new services.

## Event-Driven Microservices with Axon and Spring Boot: excitingly boring by [Allard Buijze](https://twitter.com/allardbz){:target="_blank" rel="noopener noreferrer"}

<span  class="image left"><img  class="p-image"  alt="Allard Buijze"  src="/img/2019-05-16-Spring-IO-2019/allard-buijze.jpg"></span>

In this presentation, Allard Buijze, Founder and CTO of AxonIQ talks about the advantages of event-driven architectures and shows how easy it is to set up your own event-driven microservices using Axon and Spring Boot.

### What is Axon?

The [Axon framework](https://axoniq.io/){:target="_blank" rel="noopener noreferrer"} is used for building event-driven microservices using Domain-Driven Design, CQRS and Event Sourcing.
It is there to prevent developers from getting lost inside a complex microservice architecture.

##### CQRS
Command and Query Responsibility Segregation is a design pattern where you split the reading and writing of data into seperate models,
use querries for reading the data and commands for updating the data.
While for basic CRUD operations having these models combined might be fine,
once the amount of business logic and amount of querries increases it might become increasingly difficult to manage.


### State Storage vs Event Sourcing
A big part of this presentation is about the advantages of using event sourcing rather than state storage.
Events describe the history of an object rather than just the current state of the object.
It is easy to go through the history and generate the current state while also getting a lot of extra information about the object you would otherwise miss out on when just storing its current state. 
Explicit record that something happened, rather than an implicit record of what happened based on changes that occured.
This also makes testing your application easier because you do not have to rely on state but rather on a series of events to take place or an exception to occur.

<span  class="image right"><img  class="image-block"  alt="event-sourcing"  src="/img/2019-05-16-Spring-IO-2019/event-sourcing.png"></span>

### Events
One of the biggest advantages of events is that they remain valuable over time.
They need to be the source of everything in the application and show a true representation of your entities.
Once again, you don't save the state of an aggregate, you can generate the state by replaying the history.

### The power of not now
The power of not now basically means that because you save all the events,
you can generate reports whenever you want based on the captured data.
You don't have to know in advance what data is important to store for later on,
everything is stored.

### Axon Server
[Axon Server](https://docs.axoniq.io/reference-guide/axon-server){:target="_blank" rel="noopener noreferrer"} is a service that distributes your components, manages routing, stores events and provides high availability and observability.
By combining all these otherwise different services into one single easy to configure service,
you make your entire architecture a lot easier to manage than if you were to use for example the Netflix Eureka Discovery Service for communication between microservices,
the MySQL Event Store for storing events and RabbitMQ to handle messaging.
By simply adding the Axon Server dependency and adding some annotations you can use all of these services while keeping the complexity low.

### Tracing
Axon can also manage tracing for you by just adding the Axon tracing and Jaeger to your dependencies.
Where otherwise setting up tracing would be a lot of work having to deal with all kinds of headers,
passing headers along and interpreting them,
Axon tracing takes care of all of this for you.

<iframe width="1164" height="655" src="https://www.youtube.com/embed/iVaD3mdwvx4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

## How to secure your Spring apps with Keycloak by [Thomas Darimont](https://twitter.com/thomasdarimont){:target="_blank" rel="noopener noreferrer"}

<span  class="image left"><img  class="p-image"  alt="Thomas Darimont"  src="/img/2019-05-16-Spring-IO-2019/thomas-darimont.jpg"></span>

In this presentation Thomas Darimont talks about what Keycloak is,
what you can do with it and gives a demo of how it works and how you can set it up for your own applications.

### What is Keycloak?
Keycloak is a java based authentication and authorization server. 
It is developed by Red Hat who use it as a base for their enterprise RH-SSO application
on which they provide additional support and documentation.
but is also backed by a large open source community providing additional features,
documentation and bugfixes.

### Keycloak Features

One of the most important features of Keycloak is the Single Sign-On,
and with this the Single logout,
sign into keycloak once to gain access to multiple applications and sign out once to sign out of all applications. Do note though that individual applications can disable this single logout so you might not get logged out of all the applications within a realm.
Another great feature of Keycloak is their multi-factor authentication using one of the known authentication apps like the Google Authenticator
and their support for authentication through social media platforms
such as Facebook, Twitter, Google or even Github. 
Then of course there is the fact that keycloak is completely customizable and extensible,
it comes with a prefered stack on which I will dive into more detail later
but you can get away from this and use your own prefered services albeit with some additional configuration.
Keycloak also comes with an easy to use management console for administrators
and a user management interface where all users can update their user details.
The last feature I am going to discuss are the realms, sets of applications,
users and registered OAuth clients to whom the Keycloak settings will be applied. 
With these realms you can give users specific roles or just authenticate them across multiple applications using the Single Sign-on feature.

### The keycloak prefered stack

By default Keycloak is a WildFly based server with a plain JAX-RS application.
It uses JPA for storing data and Infinispan for the horizontal scaling
of mutliple Keycloack nodes that all distribute information like user sessions between eachother.
Other than that it uses the Freemarker template engine to render for example the login pages
and Jackson 2.x for everything JSON related like the tokens.

### Securing your application

To add keycloak to your applications you have to add a dependency
and you will have to register your application within a Keycloak realm.
Once you have done this,
after doing some configuration within your application and the Keycloak management console,
you will have to authenticate through Keycloak to gain access to your application.

### The authentication process

The following steps describe the Keycloak authentication process:
* Unauthenticated user accesses application
* The application redirects to Keycloak for login
* When login is successful, Keycloak will create an SSO session and will emit cookies
* Keycloak generates random code and redirects user back to application
* Application receives code associated with sign-on session and sends the code back to Keycloak via a seperate channel
* If the code sent back is associated with sign-on session, Keycloak will reply with an access token, a refresh token and an id token
* The application verifies the tokens and associate them with a session
* The user is now logged-in to the application

For more information you can refer to Keycloak's [official documentation](https://www.keycloak.org/documentation.html){:target="_blank" rel="noopener noreferrer"} and you can also watch the original talk itself in the following video:

<iframe width="1164" height="655" src="https://www.youtube.com/embed/KrOd5wIkqls" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
