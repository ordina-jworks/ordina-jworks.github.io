---
layout: post
authors: [yannick_de_turck]
title: "Lagom 1.2: What's new?"
image: /img/lagom.png
tags: [Lagom, Java, Reactive, Domain-Driven Design, CQRS, Event-Sourcing]
category: Microservices
comments: true
---

## Table of Contents

1. [Looking back at the first version](#looking-back-at-the-first-version)
2. [Lagom 1.2](#lagom-12)
3. [Message Broker API & Kafka implementation](#message-broker-api--kafka-implementation)
4. [Generic read-side support, sharding support and automatic offset handling](#generic-read-side-support-sharding-support-and-automatic-offset-handling)
5. [JDBC support](#jdbc-support)
6. [Migrating from 1.0 to 1.2](#migrating-from-10-to-12)
7. [Looking at the rest of our initial feedback](#looking-at-the-rest-of-our-initial-feedback)
8. [Lagom 1.3 preview](#lagom-13-preview)
9. [Conclusion](#conclusion)
10. [Extra resources](#extra-resources)

## Looking back at the first version
Shortly after the first MVP version of Lagom was released we wrote a [blogpost](/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html) in which we wrote down our first impressions and in which we did an initial comparison to [Spring Cloud](http://projects.spring.io/spring-cloud/) and [Netflix OSS](https://netflix.github.io).
We also did an introduction presentation on Lagom which is available on [YouTube](https://www.youtube.com/watch?v=JOGlZzY6ycI).
Lightbend let us know that they really appreciated our feedback.
They definitely understood some of our remarks and suggestions and they were willing to work on some of them.
One of our majors remarks was that Maven support should be added to Lagom in order to properly target Java developers.

We weren't the only ones with this feedback and Lightbend [took note of it](http://www.lagomframework.com/blog/introducing-maven-support.html).
In September 2016, Lightbend released [Lagom 1.1](http://www.lagomframework.com/blog/lagom-1-1.html). 
In this first new minor version they introduced Maven support which also includes support for running the Lagom development environment in Maven.
More information about setting up a Lagom project in Maven is available in the [documentation](http://www.lagomframework.com/documentation/1.2.x/java/GettingStartedMaven.html).

A few months later they released [Lagom 1.2](http://www.lagomframework.com/blog/lagom-1-2.html) with a couple of new features which is what this blogpost will be about.
We will also revisit our initial comparison against Spring Cloud and Netflix OSS.

## Lagom 1.2
In this new minor version of Lagom, the read-side has been overhauled.
Other notable additions are the following:

* [Message Broker API & Kafka implementation](https://github.com/lagom/lagom/pull/161)
* [JDBC support](https://github.com/lagom/lagom/pull/164)
* [Generic read-side support, sharding support and automatic offset handling](https://github.com/lagom/lagom/pull/151)

Naturally at the same time a couple of existing issues and bugs were also resolved, a full list is available on [Github](https://github.com/lagom/lagom/issues?utf8=✓&q=milestone%3A1.2.0).

## Message Broker API & Kafka implementation
The introduction of [message broker support](see http://www.lagomframework.com/documentation/1.2.x/java/MessageBroker.html) is the most notable feature of Lagom 1.2.
By adding message broker support, Lagom now allows both direct streaming of messages between services but also through a broker.
With version 1.2, Lagom comes with out-of-the-box support for [Apache Kafka](https://kafka.apache.org), a very popular scalable message broker for building real-time data pipelines and streaming application.
The message broker API has been designed to be independent of any backend meaning that support for other brokers may be added in the future.

Compared to the existing [Publish-Subscribe](http://www.lagomframework.com/documentation/1.0.x/java/PubSub.html#Publish-Subscribe) mechanism where messaging is only available intra-service, message broker communication happens between one service to many other services.
Another key difference between the two is that with Publish-Subscribe it is possible that messages might get lost, for example due to network issues or a restart of a service, message broker based communication can provide **at-least-once** and **at-most-once** [delivery semantics](http://doc.akka.io/docs/akka/current/general/message-delivery-reliability.html#Discussion__What_does_“at-most-once”_mean_) even if the subscriber is down.
In Publish-Subscribe messaging a subscriber will only receive a message after its subscription has been accepted by the Publish-Subscribe infrastructure.
The message broker on the other hand will allow the subscriber to consume all the messages since the last message it has consumed, even if the subscriber was offline or down, thanks to its decoupling of services producing events from others consuming them.
The Publish-Subscribe mechanism in Lagom is provided by the Akka Cluster underneath a Lagom service whereas message broker support is provided by a third party product such as Kafka.

Lagom takes care of publishing, partitioning, consuming and failure handling of messaging and when executing the `runAll` command, Lagom automatically runs a Kafka server with you along with Zookeeper for you.
Next to `ServiceCall` for communicating with other services mapping down onto HTTP, there now exists a `Topic` abstraction that represents a topic that one service publishes and that other services can consume after subscribing.

In order to make use of it you need to add the [Lagom Kafka Broker](http://www.lagomframework.com/documentation/1.2.x/java/KafkaClient.html) module to the dependencies for both the service publishing to a topic and the service subscribing to a topic.
Note that Lagom Kafka Broker module requires an implementation of Lagom Persistence so you need to add either [Lagom Persistence Cassandra](http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityCassandra.html) or [Lagom Persistence JDBC](http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityRDBMS.html) to the dependencies.

In our simple demo project [Lagom Shop](https://github.com/yannickdeturck/lagom-shop) we define a new method in the `item-api` project's `ItemService` that will return a `Topic` of item creation events to subscribe on and we also add it to the `descriptor`:

{% highlight java %}
Topic<ItemEvent> createdItemsTopic();

@Override
default Descriptor descriptor() {
    return Service.named("itemservice").withCalls(
            Service.restCall(Method.GET,  "/api/items/:id", this::getItem),
            Service.restCall(Method.GET,  "/api/items", this::getAllItems),
            Service.restCall(Method.POST, "/api/items", this::createItem)
    ).publishing(
            Service.topic("createdItems", this::createdItemsTopic)
    ).withAutoAcl(true);
}
{% endhighlight %}

In the `item-api` project we define a new `ItemEvent` interface for external use in other services.
An `ItemEvent` already exists in the `item-impl` project but you want this one to be solely used within the implementation project.
It is considered a best practice to have a separate definition for external use as it would otherwise cause you to break clients if you would apply internal changes.

{% highlight java %}
@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type", defaultImpl = Void.class)
@JsonSubTypes({
        @JsonSubTypes.Type(ItemEvent.ItemCreated.class)
})
public interface ItemEvent {
    UUID getId();

    @JsonTypeName("item-created")
    final class ItemCreated implements ItemEvent {
        private final UUID id;
        private final String name;
        private final BigDecimal price;
    
        @JsonCreator
        public ItemCreated(UUID id, String name, BigDecimal price) {
            this.id = id;
            this.name = name;
            this.price = price;
        }
    
        @Override
        public UUID getId() {...}
    
        public String getName() {...}
    
        public BigDecimal getPrice() {...}
    
        @Override
        public boolean equals(Object o) {...}
    
        @Override
        public int hashCode() {...}
    
        @Override
        public String toString() {...}
    }
}
{% endhighlight %}

In the `item-impl` project we add the implementation to `ItemServiceImpl` that will publish all `ItemCreated` events to the topic:

{% highlight java %}
@Override
public Topic<be.yannickdeturck.lagomshop.item.api.ItemEvent> createdItemsTopic() {
    return TopicProducer.singleStreamWithOffset(offset -> {
        return persistentEntities
                .eventStream(ItemEventTag.INSTANCE, offset)
                .filter(eventOffSet -> eventOffSet.first() instanceof ItemCreated)
                .map(this::convertItem);
    });
}

private Pair<be.yannickdeturck.lagomshop.item.api.ItemEvent, Offset> convertItem(Pair<ItemEvent, Offset> pair) {
    Item item = ((ItemCreated)pair.first()).getItem();
    logger.info("Converting ItemEvent" + item);
    return new Pair<>(new be.yannickdeturck.lagomshop.item.api.ItemEvent.ItemCreated(item.getId(), item.getName(),
            item.getPrice()), pair.second());
}
{% endhighlight %}

We now want to make use of this in our `order-impl` project.
Using the injected `ItemService` instance we can now subscribe on the topic and act on each message.
In this case we log something whenever we receive a new message.

{% highlight java %}
@Inject
public OrderServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,
                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {
    ...
    itemService.createdItemsTopic()
            .subscribe()
            .atLeastOnce(Flow.fromFunction((be.yannickdeturck.lagomshop.item.api.ItemEvent item) -> {
                logger.info("Subscriber: doing something with the created item " + item);
                return Done.getInstance();
            }));
}
{% endhighlight %}

As mentioned earlier, you have the option to make use of either the `atLeastOnce` or `atMostOnceSource` delivery semantic on the `Subscriber` instance.

If we run the application, create an item and afterwards check the logs, we see that the order service is receiving messages:

```
2016-12-23 22:04:22,337 INFO  b.y.l.i.i.ItemServiceImpl - Creating item: CreateItemRequest{name=newItem, price=15}
2016-12-23 22:04:22,349 INFO  b.y.l.i.i.ItemEntity - Setting up initialBehaviour with snapshotState = Optional.empty
2016-12-23 22:04:22,357 INFO  b.y.l.i.i.ItemEntity - Processed CreateItem command into ItemCreated event ItemCreated{item=Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}, timestamp=2016-12-23T21:04:22.357Z}
2016-12-23 22:04:22,359 INFO  b.y.l.i.i.ItemEntity - Processed ItemCreated event, updated item state
2016-12-23 22:04:22,412 INFO  b.y.l.i.i.ItemEntity - Processed GetItem command, returned item
2016-12-23 22:04:22,413 INFO  b.y.l.i.i.ItemServiceImpl - Looking up item d370993c-dd75-4a88-bf8b-d9dba1820feb
2016-12-23 22:04:25,472 INFO  b.y.l.i.i.ItemEventProcessor - Persisted Item Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}
2016-12-23 22:04:25,776 INFO  b.y.l.i.i.ItemServiceImpl - Converting ItemEventItem{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}
2016-12-23 22:04:25,883 INFO  b.y.l.o.i.OrderServiceImpl - Subscriber: doing something with the created item ItemCreated{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name='newItem', price=15}
```

## Generic read-side support, sharding support and automatic offset handling
Up until now, the read-side processor API was specific to Cassandra and required you to do the necessary offset tracking.
This existing API, while still usable, has been declared depricated and instead a new specific implementation for constructing Cassandra read-sides and a more generic one for JDBC read-sides have been added.

The read-side can now also be sharded by tagging persistent entity events with sharded tags.
Instead of declaring only one tag, read-side processors now declare a list of them.
The processing of these tags across the cluster is handled for you by Lagom.

To compare the two of them, here is a sample of using a single tag:

{% highlight java %}
public interface ItemEvent extends Jsonable, AggregateEvent<ItemEvent> {

    int NUM_SHARDS = 20;

    AggregateEventShards<ItemEvent> TAG = AggregateEventTag.sharded(ItemEvent.class, NUM_SHARDS);

    @Override
    default AggregateEventShards<ItemEvent> aggregateTag() {
        return TAG;
    }
}
{% endhighlight %}

And an example of using sharded tags:

{% highlight java %}
public interface ItemEvent extends Jsonable, AggregateEvent<ItemEvent> {
    
    AggregateEventTag<ItemEvent> TAG = AggregateEventTag.of(ItemEvent.class);
    
    @Override
    default AggregateEventTag<ItemEvent> aggregateTag() {
        return TAG;
    }
}
{% endhighlight %}

In your `ReadSideProcessor` you will have to override the `aggregateTags()` abstract method differently.

When using a single tag:

{% highlight java %}
@Override
public PSequence<AggregateEventTag<ItemEvent>> aggregateTags() {
    return TreePVector.singleton(ItemEvent.TAG);
}
{% endhighlight %}

And when using sharded tags:

{% highlight java %}
@Override
public PSequence<AggregateEventTag<ItemEvent>> aggregateTags() {
  return ItemEvent.TAG.allTags();
}
{% endhighlight %}

Lagom now also provides automatic offset tracking, which until now, you had to do yourself in your read-side processors by explicitly loading and persisting offsets.
This allows us to get rid of quite a bit of code and as less code means less bugs, this is definitely a good thing.
In the section dealing with [migrating to Lagom 1.2](#migrating-from-10-to-12) there is a part that shows the code that we got rid of.

That the existing Cassandra read-side support API still exists but it has been deprecated and might be removed in an upcoming version so it is a good idea to migrate as soon as possible.
It isn't really that much work to migrate to the new API, the exact work required is described in the [Migrating from 1.0 to 1.2](#migrating-from-10-to-12) section.

## JDBC support
JDBC support has been added to ease the introduction of Lagom into the existing organisation of potential users in order to provide support for using their existing relational database infrastructure.
A relational database is less preferred when setting up a non-blocking and reactive architecture but by providing support for it, 
it would allow potential users to start making use of the Persistent Entity API without the necessity of having to switch over to Cassandra.

To make use of JDBC support you need to add the [Persistence JDBC](http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityRDBMS.html) module to your project while also having to add the jar for your JDBC driver.
Lagom makes use of [akka-persistence-jdbc](https://github.com/dnvriend/akka-persistence-jdbc) to persist entities to the database.
At the time of writing only four relational databases are supported:

* PostgreSQL
* MySQL
* Oracle
* H2

`akka-persistence-jdbc` uses [Slick](http://slick.lightbend.com) for mapping tables and managing asynchronous execution of JDBC calls.
Slick requires you to configure it to use the right Slick profile for your database.
An example of a Slick configuration in `application.conf`:

```
db.default {
  driver = "org.postgresql.Driver"
  url = "jdbc:postgresql://database.example.com/playdb"
}

jdbc-defaults.slick.driver = "slick.driver.PostgresDriver$"
```

A table and journal table are required by the `akka-persistence-jdbc` plugin and by default, Lagom is able to create these automatically for you.
This automatic generation functionality can be disabled which you will probably want for any environment higher than development.

```
lagom.persistence.jdbc.create-tables.auto = false
```

The definition of the tables differ for each database, here is an example of the table definition for PostgreSQL:

{% highlight sql %}
DROP TABLE IF EXISTS public.journal;

CREATE TABLE IF NOT EXISTS public.journal (
  ordering BIGSERIAL,
  persistence_id VARCHAR(255) NOT NULL,
  sequence_number BIGINT NOT NULL,
  deleted BOOLEAN DEFAULT FALSE,
  tags VARCHAR(255) DEFAULT NULL,
  message BYTEA NOT NULL,
  PRIMARY KEY(persistence_id, sequence_number)
);

DROP TABLE IF EXISTS public.snapshot;

CREATE TABLE IF NOT EXISTS public.snapshot (
  persistence_id VARCHAR(255) NOT NULL,
  sequence_number BIGINT NOT NULL,
  created BIGINT NOT NULL,
  snapshot BYTEA NOT NULL,
  PRIMARY KEY(persistence_id, sequence_number)
);
{% endhighlight %}

The definition scripts for each database are available [here](https://github.com/dnvriend/akka-persistence-jdbc/tree/v2.6.7/src/main/resources/schema).

If you are unaware of CQRS, Event Sourcing and the [Persistent Read-Side](http://www.lagomframework.com/documentation/1.2.x/java/ReadSide.html) in Lagom you could take a look at the [CQRS and Event Sourcing](/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html#cqrs-and-event-sourcing) section in our previous blogpost on Lagom.
It might be a bit out of date regarding the API in Lagom but it should give you an idea.

The new API for Cassandra read-side support, while very similar compared to the existing API, still differs in a few places.
In the [upcoming section](#migrating-from-10-to-12) we describe the migration process of upgrading to Lagom 1.2 and the changes we had to do to our read-side, so the required code changes can be read in that section.

The API for the JDBC read-side support is rather similar compared to the Cassandra read-side support.
Instead of using a `CassandraSession` for querying, you use a `JdbcSession` to retrieve a connection which in turn you will use for the execution of queries.
The JDBC read-side API however, unlike the Cassandra read-side API, is not fully non-blocking which will lead to a performance difference.
It could be an option to switch to Cassandra later on in the project if you want to get better performance out of it.
But at least by having an API right now for these four databases it might be easier to integrate a new Lagom application within an existing architecture having these kinds of databases.

TODO Maybe add code sample
TODO See http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityRDBMS.html
TODO See http://www.lagomframework.com/documentation/1.2.x/java/ReadSideRDBMS.html

## Migrating from 1.0 to 1.2
A [migration guide](http://www.lagomframework.com/documentation/1.2.x/java/Migration12.html) is available with the steps necessary to upgrade your project to Lagom 1.2.
At Ordina Belgium we streamed and recorded an [introduction video on Lagom 1.0](https://www.youtube.com/watch?v=JOGlZzY6ycI) in which we demoed a [shop application](https://github.com/yannickdeturck/lagom-shop).
For the purpose of this blogpost, let us see how much work it is to upgrade to 1.2.

First of all, we have to upgrade the Lagom version itself. 
Lagom 1.0 only supported sbt, so our demo is still using that.
We change the version to be used in `project/plugins.sbt`:

```
addSbtPlugin("com.lightbend.lagom" % "lagom-sbt-plugin" % "1.2.1")
```

For the Lagom Persistence module, Cassandra support has been pulled into its own module so you need to update the `lagomJavadslPersistence` dependency to `lagomJavadslPersistenceCassandra` in the `build.sbt` file.

We also have to update the Scala version in `build.sbt`:

```
scalaVersion in ThisBuild := "2.11.8"
```

The ConductR version in `project/plugins.sbt`:

```
addSbtPlugin("com.lightbend.conductr" % "sbt-conductr" % "2.1.16")
```

The `descriptor()` in the service interfaces needs to be updated since the `.with(...)` has been replaced by `withCalls(...)`. 

We replace the existing code:

{% highlight java %}
@Override
default Descriptor descriptor() {
    return Service.named("itemservice").with(
            Service.restCall(Method.GET,  "/api/items/:id", this::getItem),
            Service.restCall(Method.GET,  "/api/items", this::getAllItems),
            Service.restCall(Method.POST, "/api/items", this::createItem)
    ).withAutoAcl(true);
}
{% endhighlight %}

With the following: 

{% highlight java %}
@Override
default Descriptor descriptor() {
    return Service.named("itemservice").withCalls(
            Service.restCall(Method.GET,  "/api/items/:id", this::getItem),
            Service.restCall(Method.GET,  "/api/items", this::getAllItems),
            Service.restCall(Method.POST, "/api/items", this::createItem)
    ).withAutoAcl(true);
}
{% endhighlight %}

These are the necessary changes for us to successfully compile our project. 
Subsequently, we need to update our read-sides to make use of the new API.
Starting with replacing the deprecated `CassandraReadSideProcessor`:

{% highlight java %}
public class ItemEventProcessor extends CassandraReadSideProcessor<ItemEvent> {
{% endhighlight %}

With `ReadSideProcessor`:

{% highlight java %}
public class ItemEventProcessor extends ReadSideProcessor<ItemEvent> {
{% endhighlight %}

Next step is to inject an instance of `CassandraSession` and `CassandraReadSide` via the constructor:

{% highlight java %}
private final CassandraSession session;
private final CassandraReadSide readSide;

@Inject
public ItemEventProcessor(CassandraSession session, CassandraReadSide readSide) {
    this.session = session;
    this.readSide = readSide;
}
{% endhighlight %}

All code related to handling offsets can be deleted since Lagom now handles this for us.
We delete the following:

{% highlight java %}
private PreparedStatement writeOffset = null; // initialized in prepare

private void setWriteOffset(PreparedStatement writeOffset) {
    this.writeOffset = writeOffset;
}

private CompletionStage<Done> prepareWriteOffset(CassandraSession session) {
    logger.info("Inserting into read-side table item_offset...");
    return session.prepare("INSERT INTO item_offset (partition, offset) VALUES (1, ?)").thenApply(ps -> {
        setWriteOffset(ps);
        return Done.getInstance();
    });
}

private CompletionStage<Optional<UUID>> selectOffset(CassandraSession session) {
    logger.info("Looking up item_offset");
    return session.selectOne("SELECT offset FROM item_offset")
            .thenApply(
                    optionalRow -> optionalRow.map(r -> r.getUUID("offset")));
}
{% endhighlight %}

After having our `ItemEventProcessor` class extend from the `ReadSideProcessor` abstract we are prompted to implement two methods: `buildHandler()` and `aggregateTags()`.
`aggregateTags()` simply replaces `aggregateTag()` where as `buildHandler()` will contain the setup needed for our `ReadSideHandler`.
The `prepare(CassandraSession session)` and `defineEventHandlers(EventHandlersBuilder builder)` methods that used to be overridden are now both implemented in `buildHandler()`.
The logic from the existing `prepare()` is split up into a `setGlobalPrepare()`, for creating Cassandra tables (note that these tasks should be idempotent), and a `prepare()`, for preparing statements, in `buildHandler()`. 

We start by deleting the old `prepare(CassandraSession session)` and the `defineEventHandlers(EventHandlersBuilder builder)`:

{% highlight java %}
@Override
public CompletionStage<Optional<UUID>> prepare(CassandraSession session) {
    return
            prepareCreateTables(session).thenCompose(a ->
                    prepareWriteOrder(session).thenCompose(b ->
                            prepareWriteOffset(session).thenCompose(c ->
                                    selectOffset(session))));
}

@Override
public EventHandlers defineEventHandlers(EventHandlersBuilder builder) {
    logger.info("Setting up read-side event handlers...");
    builder.setEventHandler(ItemCreated.class, this::processItemCreated);
    return builder.build();
}
{% endhighlight %}

Finally we perform the necessary refactoring to implement both `buildHandler()` and `aggregateTags()`, and we clean up the existing processing logic for the read-side:

{% highlight java %}
private CompletionStage<List<BoundStatement>> processItemCreated(ItemCreated event) {
    BoundStatement bindWriteItem = writeItem.bind();
    bindWriteItem.setUUID("itemId", event.getItem().getId());
    bindWriteItem.setString("name", event.getItem().getName());
    bindWriteItem.setDecimal("price", event.getItem().getPrice());
    logger.info("Persisted Item {}", event.getItem());
    return CassandraReadSide.completedStatements(Arrays.asList(bindWriteItem));
}

@Override
public ReadSideHandler<ItemEvent> buildHandler() {
    CassandraReadSide.ReadSideHandlerBuilder<ItemEvent> builder = readSide.builder("item_offset");
    builder.setGlobalPrepare(() -> prepareCreateTables(session));
    builder.setPrepare(tag -> prepareWriteItem(session));
    logger.info("Setting up read-side event handlers...");
    builder.setEventHandler(ItemCreated.class, this::processItemCreated);
    return builder.build();
}

@Override
public PSequence<AggregateEventTag<ItemEvent>> aggregateTags() {
    return TreePVector.singleton(ItemEventTag.INSTANCE);
}
{% endhighlight %}

This leaves us with some refactoring to be done in our service implementations for registering the read-side.
The sole thing that needs to be done is replacing the injected `CassandraReadSide`, which is now deprecated, with `ReadSide`.

So going from:

{% highlight java %}
@Inject
public ItemServiceImpl(PersistentEntityRegistry persistentEntities, CassandraReadSide readSide,
                        CassandraSession db) {
    this.persistentEntities = persistentEntities;
    this.db = db;

    persistentEntities.register(ItemEntity.class);
    readSide.register(ItemEventProcessor.class);
}
{% endhighlight %}

To:

{% highlight java %}
@Inject
public ItemServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,
                       CassandraSession db) {
    this.persistentEntities = persistentEntities;
    this.db = db;

    persistentEntities.register(ItemEntity.class);
    readSide.register(ItemEventProcessor.class);
}
{% endhighlight %}

This concludes migrating our read-side logic to the new API.

Since Lagom now supports multiple persistence backends, and not only just Cassandra, TestKit no longer starts with Cassandra enabled by default.
This requires us to add a single line `.withCassandra(true)` to our server setup:

{% highlight java %}
@BeforeClass
public static void setUp() {
    server = ServiceTest.startServer(ServiceTest.defaultSetup()
            .withCassandra(true)
            .withConfigureBuilder);
}
{% endhighlight %}

All in all, migrating the code base to Lagom 1.2 took about fifteen minutes.

## Looking at the rest of our initial feedback
In the [previous blogpost](/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html) on Lagom 1.0 we, like many other Java developers, made the point that only offering sbt as the buildtool would repel many potential Java developers.
We are glad that they addressed this quickly in the [first new major version (1.1)](http://www.lagomframework.com/blog/lagom-1-1.html) they released.

Regarding using Lagom in production without using ConductR, the bare minimum for this was to write your own service locator.
Jonas Bonér started a service locator project for [ZooKeeper](https://github.com/jboner/lagom-service-locator-zookeeper) and [Consul](https://github.com/jboner/lagom-service-locator-consul) for this purpose.
Lightbend also plans to offer a free limited use evaluation license which will allow developers to start working with the [Reactive Platform's commercial features](https://www.lightbend.com/platform) from the beginning.
Including not only ConductR but other goodies such as monitoring for Lagom circuit breakers and Akka actors.

In the blogpost we also wrote down several impressions compared to Pivotal's Spring Cloud and Netflix OSS, currently the most popular choice of doing microservice architectures in Java.
A strong point for Lightbend is that they want to distinct themselves from Pivotal by providing commercial support.
This is only the case if you buy the commercial Pivotal Cloud Foundry. 
Many Netflix OSS components are provided out-of-the-box as tiles (backing services).

A key advantage of Lagom remains that it is non-blocking down to the core, starting from the persistence layer up to the endpoints, while Spring isn't just yet.
In the upcoming [Spring Framework 5](https://spring.io/blog/2016/07/28/spring-framework-5-0-m1-released), of which a [milestone version (M3)](http://projects.spring.io/spring-framework/) is already available, Pivotal are integrating their [Spring Reactive](https://spring.io/blog/2016/02/09/reactive-spring) initiative providing core reactive functionality and reactive web endpoint support.
On the topic of Spring Reactor, a colleague of ours, [Tom Van den Bulck](https://twitter.com/tomvdbulck), recently wrote a blogpost on [Reactive Programming with Spring Reactor](/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html) following up a presentation of [Stephane Maldini](https://twitter.com/smaldini) at [Ordina's JOIN 2016](https://ordina-jworks.github.io/conferences/2016/09/27/JOIN-2016.html) event, a small one-day conference hosted by Ordina, on reactive programming which has also been recorded and is available on [YouTube](https://www.youtube.com/watch?v=RU0yQhfybDg). 
So it will be interested to see whether Spring Framework 5 allows them to catch up on being fully non-blocking and reactive, which up until now, remains a stronger point of Lagom.

Lagom's CQRS and Event Sourcing integration remain another advantage of Lagom, the out-of-the-box integration is easy to work with and they continue to improve on it, now with JDBC support.
In a Spring Cloud application a common solution for this is making use of the [AxonFramework](http://www.axonframework.org) although it requires you to do the necessary gluing yourself.
There is also the [Eventuate](https://blog.eventuate.io/2016/10/06/eventuate-local-event-sourcing-and-cqrs-with-spring-boot-apache-kafka-and-mysql) framework written by Chris Richardson

Polyglot support is something that still lacks on the side of Lagom.
Spring has [Sidecar](http://projects.spring.io/spring-cloud/spring-cloud.html#_polyglot_support_with_sidecar) for this purpose.
While Lagom services map down to ordinary, standard HTTP and WebSockets, it currently lacks support for any design driven API development.
Lightbend understands that this is an important feature in any microservice architecture and is planning on implementing a solution for this probably based upon [Akka Streams](http://doc.akka.io/docs/akka/2.4.16/scala/stream/) and [Alpakka](https://github.com/akka/alpakka).
The idea is that you should be able to generate a Lagom service interface from specifications, allowing transparent integration.

Having binary coupling was another remark of ours but up until now nothing has really changed.
Lightbend still has plans to support [Swagger](http://swagger.io) but there is no ETA for the time being.

## Conclusion
While there are still a couple of important things in need of being addressed we believe that we can conclude that Lightbend is carefully listening to the feedback given by the community.
They continue to improve Lagom with new features and to offer better user experience for the developers.

At the same time, Pivotal is working on providing better reactive support with their upcoming Spring Framework 5.

Lightbend and Pivotal make some nice rivals to each other which is nice since this will have a positive impact on both frameworks.
Pivotal clearly still has an advantage on Lightbend due to how mature and well-known Spring is although Lagom seems to develop nicely and continues to improve on its strongest points while also trying to address weak points.

We think that Lagom is worth adopting if you plan on getting a Reactive Platform license.
Now that Maven support is available, a big hurdle for Java developers has disappeared.
Without the Reactive Platform, development should be fine but you will probably miss the useful goodies it has to offer for running your system into production such as monitoring and service orchestration which you get all for free if you go with Spring Cloud and Netflix OSS.
We are sure that using Lagom without the Reactive Platform will become more interesting given enough time for the community to come up with solutions for this.

## Lagom 1.3 preview
The [first issue](https://github.com/lagom/lagom/issues/1), created after Lagom was released, was the need to implement a separate Scala API.
Something many users of Lightbend's technologies were thirsting for.
Until now it was already possible to use Scala with Lagom by using the Java API, see the following [seed](https://github.com/dotta/activator-lagom-scala-chirper) created by [Mirco Dotta](https://github.com/dotta).

Lightbend made work of it and since a couple of days a [preview version](http://www.lagomframework.com/blog/lagom-scala-api-preview.html) was released by Lightbend to try out Lagom with Scala.
Users should be aware however that the API is still considered experimental and that it will likely change before the final release.
Therefor it should not be used for any production service just yet but it is ideal if you want to get a first taste of what it is like to develop a Lagom application in Scala.

## Extra resources

- [Lagom: First Impressions and Initial Comparison to Spring Cloud](https://ordina-jworks.github.io/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html)
- [Lagom in Practice by Yannick De Turck](https://www.youtube.com/watch?v=JOGlZzY6ycI)
- [Lagom documentation](http://www.lagomframework.com/documentation/)
- [Lagom Twitter](https://twitter.com/lagom)
- [Lagom Gitter](https://gitter.im/lagom/lagom)
- [Lagom mailing list](https://groups.google.com/forum/#!forum/lagom-framework)
- A nice Lagom 1.2 example project is the [Online Auction](https://github.com/lagom/online-auction-java) sample
