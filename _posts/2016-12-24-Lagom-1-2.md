---
layout: post
authors: [yannick_de_turck]
title: "Lagom 1.2: What's new?"
image: /img/lagom.png
tags: [Lagom, Java, Reactive, Domain-Driven Design, CQRS, Event-Sourcing]
category: Microservices
comments: true
---

## Table of Contents

1. [Looking back at the first version](#looking-back-at-the-first-version)
2. [Lagom 1.2](#lagom-12)
3. [Message Broker API & Kafka implementation](#message-broker-api--kafka-implementation)
4. [Generic read-side support, sharding support and automatic offset handling](#generic-read-side-support-sharding-support-and-automatic-offset-handling)
5. [JDBC support](#jdbc-support)
6. [Migrating from 1.0 to 1.2](#migrating-from-10-to-12)
7. [Looking at the rest of our initial feedback](#looking-at-the-rest-of-our-initial-feedback)
8. [Lagom 1.3 preview](#lagom-13-preview)
9. [Conclusion](#conclusion)
10. [Extra resources](#extra-resources)
TODO monitoring for circuit breakers?

## Looking back at the first version
Shortly after the first MVP version of Lagom was released we wrote a [blogpost](/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html) in which we wrote down our first impressions and in which we did an initial comparison to Spring Cloud.
Lightbend let us know that they really appreciated our feedback and that they definitely understood some of our remarks and suggestions, and that they are willing to work on them.
One of the remarks we had was that we found that in order to properly target Java developers, Maven support should be added to Lagom.

We weren't the only ones with this feedback and Lightbend [took note of it](http://www.lagomframework.com/blog/introducing-maven-support.html).
In September 2016, Lightbend released [Lagom 1.1](http://www.lagomframework.com/blog/lagom-1-1.html). 
In this first new minor version they introduced Maven support including support for running the Lagom development environment in Maven.
TODO add example of starting a Maven project

A few months later they released [Lagom 1.2](http://www.lagomframework.com/blog/lagom-1-2.html) with a couple of changes and new features.
This is what we will look at in this blogpost.

## Lagom 1.2
In this new minor version of Lagom, the read-side has been overhauled.
Other notable additions are the following:

* [Message Broker API & Kafka implementation](https://github.com/lagom/lagom/pull/161)
* [JDBC support](https://github.com/lagom/lagom/pull/164)
* [Generic read-side support, sharding support and automatic offset handling](https://github.com/lagom/lagom/pull/151)

Naturally at the same time a lot of existing issues and bugs were also resolved, a full list is available on [Github](https://github.com/lagom/lagom/issues?utf8=✓&q=milestone%3A1.2.0).

## Message Broker API & Kafka implementation
The introduction of [message broker support](see http://www.lagomframework.com/documentation/1.2.x/java/MessageBroker.html) is the most notable feature of Lagom 1.2.
By adding message broker support, Lagom now allows both direct streaming of messages between service but also through a broker.
With version 1.2, Lagom comes with out-of-the-box support for [Apache Kafka](https://kafka.apache.org), a very popular scalable message broker and very well-known within the Big Fast Data world.
Note though that message broker API has been designed to be independent of any backend meaning that other brokers support might be added in the future.

Compared to the existing [Publish-Subscribe](http://www.lagomframework.com/documentation/1.0.x/java/PubSub.html#Publish-Subscribe) mechanism where messaging is only available intra-service, message broker communication happens between one service to many other services.
Another key difference between the two is that with Publish-Subscribe it is possible that messages might get lost, for example due to network issues or a restart of a service, message broker based communication can provide **at-least-one** and **at-most-once** delivery semantics even if the subscriber is down.
In Publish-Subscribe messaging a subscriber will also only receive a message after its subscription has been accepted by the Publish-Subscribe infrastructure.
The message broker, thanks to its decoupling of services producing events from others that consume them, on the other hand will allow the subscriber to consume all the messages since the last message it has consumed, even if the subscriber was offline or down.
The Publish-Subscribe mechanism in Lagom is provided by the Akka Cluster underneath a Lagom service whereas message broker support is provided by a 3rd party product such as Kafka.

Lagom takes care of publishing, partitioning, consuming and failure handling of messaging and when executing the `runAll` command, Lagom automatically runs Kafka for you.
Next to `ServiceCall` for communicating with other services mapping down onto HTTP, there now also exists a `Topic` abstraction.
This abstraction represents a topic that one service publishes and that other services can consume.

In order to make use of it you need to add the [Lagom Kafka Broker](http://www.lagomframework.com/documentation/1.2.x/java/KafkaClient.html) module to the dependencies for both the service publishing to a topic and the service subscribing to a topic.
Note that Lagom Kafka Broker module requires an implementation of Lagom Persistence so you need to add either [Lagom Persistence Cassandra](http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityCassandra.html) or [Lagom Persistence JDBC](http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityRDBMS.html) to the dependencies.

In our simple demo project [Lagom Shop](https://github.com/yannickdeturck/lagom-shop) we define a new method in the `item-api` project's `ItemService` that will return a `Topic` of item events to subscribe on and we also add it to the `descriptor`:

{% highlight java %}
Topic<ItemEvent> createdItemsTopic();

@Override
default Descriptor descriptor() {
    return Service.named("itemservice").withCalls(
            Service.restCall(Method.GET,  "/api/items/:id", this::getItem),
            Service.restCall(Method.GET,  "/api/items", this::getAllItems),
            Service.restCall(Method.POST, "/api/items", this::createItem)
    ).publishing(
            Service.topic("createdItems", this::createdItemsTopic)
    ).withAutoAcl(true);
}
{% endhighlight %}

In the `item-api` project we also define the `ItemEvent` for external use in other services:

{% highlight java %}
@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = "type", defaultImpl = Void.class)
@JsonSubTypes({
        @JsonSubTypes.Type(ItemEvent.ItemCreated.class)
})
public interface ItemEvent {
    UUID getId();

    @JsonTypeName("item-created")
    final class ItemCreated implements ItemEvent {
        private final UUID id;
        private final String name;
        private final BigDecimal price;
    
        @JsonCreator
        public ItemCreated(UUID id, String name, BigDecimal price) {
            this.id = id;
            this.name = name;
            this.price = price;
        }
    
        @Override
        public UUID getId() {...}
    
        public String getName() {...}
    
        public BigDecimal getPrice() {...}
    
        @Override
        public boolean equals(Object o) {...}
    
        @Override
        public int hashCode() {...}
    
        @Override
        public String toString() {...}
    }
}
{% endhighlight %}

In the `item-impl` project we add the implementation to `ItemServiceImpl` that will publish all `ItemCreated` events to the topic:

{% highlight java %}
@Override
public Topic<be.yannickdeturck.lagomshop.item.api.ItemEvent> createdItemsTopic() {
    return TopicProducer.singleStreamWithOffset(offset -> {
        return persistentEntities
                .eventStream(ItemEventTag.INSTANCE, offset)
                .filter(eventOffSet -> eventOffSet.first() instanceof ItemCreated)
                .map(this::convertItem);
    });
}

private Pair<be.yannickdeturck.lagomshop.item.api.ItemEvent, Offset> convertItem(Pair<ItemEvent, Offset> pair) {
    Item item = ((ItemCreated)pair.first()).getItem();
    LOGGER.info("Converting ItemEvent" + item);
    return new Pair<>(new be.yannickdeturck.lagomshop.item.api.ItemEvent.ItemCreated(item.getId(), item.getName(),
            item.getPrice()), pair.second());
}
{% endhighlight %}

We now want to make use of this in our `order-impl` project.
Using the injected `ItemService` instance we can now subscribe on the topic and act on each message.
Just as a simple example we just log something.

{% highlight java %}
@Inject
public OrderServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,
                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {
    ...
    itemService.createdItemsTopic()
            .subscribe()
            .atLeastOnce(Flow.fromFunction((be.yannickdeturck.lagomshop.item.api.ItemEvent item) -> {
                LOGGER.info("Subscriber: doing something with the created item " + item);
                return Done.getInstance();
            }));
}
{% endhighlight %}

As mentioned earlier, you have the option to make use of either the `atLeastOnce` or `atMostOnceSource` delivery semantic.

If we run the application, create an item and afterwards check the logs we see that the order service is receiving messages:

```
2016-12-23 22:04:22,337 INFO  b.y.l.i.i.ItemServiceImpl - Creating item: CreateItemRequest{name=newItem, price=15}
2016-12-23 22:04:22,349 INFO  b.y.l.i.i.ItemEntity - Setting up initialBehaviour with snapshotState = Optional.empty
2016-12-23 22:04:22,357 INFO  b.y.l.i.i.ItemEntity - Processed CreateItem command into ItemCreated event ItemCreated{item=Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}, timestamp=2016-12-23T21:04:22.357Z}
2016-12-23 22:04:22,359 INFO  b.y.l.i.i.ItemEntity - Processed ItemCreated event, updated item state
2016-12-23 22:04:22,412 INFO  b.y.l.i.i.ItemEntity - Processed GetItem command, returned item
2016-12-23 22:04:22,413 INFO  b.y.l.i.i.ItemServiceImpl - Looking up item d370993c-dd75-4a88-bf8b-d9dba1820feb
2016-12-23 22:04:25,472 INFO  b.y.l.i.i.ItemEventProcessor - Persisted Item Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}
2016-12-23 22:04:25,776 INFO  b.y.l.i.i.ItemServiceImpl - Converting ItemEventItem{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}
2016-12-23 22:04:25,883 INFO  b.y.l.o.i.OrderServiceImpl - Subscriber: doing something with the created item ItemCreated{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name='newItem', price=15}
```


## Generic read-side support, sharding support and automatic offset handling
Up until now, the read side processor API was specific to Cassandra.
While Lagom still has a new specific utility for constructing Cassandra read side, there now exists a more generic one for JDBC read sides.
The upcoming section takes a deeper look at the JDBC support for Read-Sides.

Read side can now also be sharded by tagging persistent entity events with sharded tags.
Instead of declaring only one tag, read side processors now declare a list of them.
The processing of these tags across the cluster is handled for you by Lagom.

To compare the two of them, here is a sample of using a single tag:

{% highlight java %}
public interface ItemEvent extends Jsonable, AggregateEvent<ItemEvent> {

    int NUM_SHARDS = 20;

    AggregateEventShards<ItemEvent> TAG = AggregateEventTag.sharded(ItemEvent.class, NUM_SHARDS);

    @Override
    default AggregateEventShards<ItemEvent> aggregateTag() {
        return TAG;
    }
}
{% endhighlight %}

And an example of using sharded tags:

{% highlight java %}
public interface ItemEvent extends Jsonable, AggregateEvent<ItemEvent> {
    
    AggregateEventTag<ItemEvent> TAG = AggregateEventTag.of(ItemEvent.class);
    
    @Override
    default AggregateEventTag<ItemEvent> aggregateTag() {
        return TAG;
    }
}
{% endhighlight %}

In your `ReadSideProcessor` you will have to override the `aggregateTags()` abstract method differently.
When using a single tag:

{% highlight java %}
@Override
public PSequence<AggregateEventTag<ItemEvent>> aggregateTags() {
    return TreePVector.singleton(ItemEvent.TAG);
}
{% endhighlight %}

When using sharded tags:

{% highlight java %}
@Override
public PSequence<AggregateEventTag<ItemEvent>> aggregateTags() {
  return ItemEvent.TAG.allTags();
}
{% endhighlight %}

Lagom now also provides automatic offset tracking, which until now, you had to do yourself in your read side processors by explicitly loading and persisting offsets.
This allows us to get rid of quite a bit of code and less code means less bugs so this is definitely a good thing.
In the section dealing with [migrating to Lagom 1.2](#migrating-from-10-to-12) there is a part that shows the code that we got rid of.

Note that the existing Cassandra read side support is still supported but has been deprecated and might be removed in an upcoming version so it is a good idea to migrate as soon as possible.
It isn't that much work to migrate to the new API.
The work required is described in the [Migrating from 1.0 to 1.2](#migrating-from-10-to-12) section.

## JDBC support
JDBC support has been added to ease the introduction of Lagom into the existing organisation of potential users in order to provide support for using their existing relational database infrastructure.
A relation database is less preferred when setting up a non-blocking and reactive architecture but by providing support for it, 
it would allow potential users to make use of the Lagom framework without the necessity of having to switch to Cassandra to make full use of the Persistent Entity API.

To make use of JDBC support you need to add the [Persistence JDBC](http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityRDBMS.html) module to your project while also having to add the jar for your JDBC driver.
Lagom makes use of [akka-persistence-jdbc](https://github.com/dnvriend/akka-persistence-jdbc) to persist entities to the database.
At the time of writing only four relational databases are supported:

* PostgreSQL
* MySQL
* Oracle
* H2

`akka-persistence-jdbc` uses [Slick](http://slick.lightbend.com) for mapping tables and managing asynchronous execution of JDBC calls.
Slick requires you to configure it to use the right Slick profile for your database.
An example of a Slick configuration in `application.conf`:

```
db.default {
  driver = "org.postgresql.Driver"
  url = "jdbc:postgresql://database.example.com/playdb"
}

jdbc-defaults.slick.driver = "slick.driver.PostgresDriver$"
```

A table and journal table are required by the `akka-persistence-jdbc` plugin and by default, Lagom is able to create these automatically for you.
This automatic generation functionality can be disabled which you will probably want for any environment higher than development.

```
lagom.persistence.jdbc.create-tables.auto = false
```

The definition of the tables differ for each database, an example of the table definition for PostgreSQL:

{% highlight sql %}
DROP TABLE IF EXISTS public.journal;

CREATE TABLE IF NOT EXISTS public.journal (
  ordering BIGSERIAL,
  persistence_id VARCHAR(255) NOT NULL,
  sequence_number BIGINT NOT NULL,
  deleted BOOLEAN DEFAULT FALSE,
  tags VARCHAR(255) DEFAULT NULL,
  message BYTEA NOT NULL,
  PRIMARY KEY(persistence_id, sequence_number)
);

DROP TABLE IF EXISTS public.snapshot;

CREATE TABLE IF NOT EXISTS public.snapshot (
  persistence_id VARCHAR(255) NOT NULL,
  sequence_number BIGINT NOT NULL,
  created BIGINT NOT NULL,
  snapshot BYTEA NOT NULL,
  PRIMARY KEY(persistence_id, sequence_number)
);
{% endhighlight %}

The definition scripts for each database are available [here](https://github.com/dnvriend/akka-persistence-jdbc/tree/v2.6.7/src/main/resources/schema).

If you are unaware of CQRS and Event Sourcing and the [Persistent Read-Side](http://www.lagomframework.com/documentation/1.2.x/java/ReadSide.html) in Lagom you could take a look at the [CQRS and Event Sourcing](/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html#cqrs-and-event-sourcing) section in our previous blogpost on Lagom.
It might be a bit out of date regarding the API in Lagom but it should give you the main idea.

With the introduction of JDBC support, the existing API for Cassandra Read-Side support has been altered a bit.
In an [upcoming section](#migrating-from-10-to-12) we describe the migration process of upgrading to Lagom 1.2 and the changes we had to do to our Read-Side, so the required code changes can be read in that section.

The API for the JDBC Read-Side support is rather similar compared to the Cassandra Read-Side support.
Instead of using a `CassandraSession` for querying, you use a `JdbcSession` to retrieve a connection which in turn you will use for the execution of queries.
The JDBC Read-Side API however, unlike the Cassandra Read-Side API, is not fully non-blocking which will lead to a performance difference.
It could be an option to switch to Cassandra later on in the project if you want to get better performance out of it.
But at least by having an API right now for these four databases it might be easier to integrate a new Lagom application within an existing architecture having these kinds of databases.

TODO Add code sample necessary?
See http://www.lagomframework.com/documentation/1.2.x/java/PersistentEntityRDBMS.html
See http://www.lagomframework.com/documentation/1.2.x/java/ReadSideRDBMS.html

## Migrating from 1.0 to 1.2
A [migration guide](http://www.lagomframework.com/documentation/1.2.x/java/Migration12.html) is available and explains you the necessary steps to take to upgrade your project to Lagom 1.2.
At Ordina Belgium we streamed and recorded an [introduction video on Lagom](https://www.youtube.com/watch?v=JOGlZzY6ycI) in which we demoed a [shop application](https://github.com/yannickdeturck/lagom-shop) written in Lagom 1.0.
For the purpose of this blogpost, let us see how much work it was to upgrade to 1.2.

First of all actually upgrading the version of Lagom. At the start there was only sbt, Maven support was only added in 1.1 so our example still has got sbt.
We change the version to be used in `project/plugins.sbt` to 1.2.1:

```
addSbtPlugin("com.lightbend.lagom" % "lagom-sbt-plugin" % "1.2.1")
```

For the Lagom Persistence module, Cassandra support has been pulled into its own module so you need to update the `lagomJavadslPersistence` dependency to `lagomJavadslPersistenceCassandra` in the `build.sbt` file.

I also updated the Scala version in `build.sbt`:

```
scalaVersion in ThisBuild := "2.11.8"
```

The ConductR version in `project/plugins.sbt`:

```
addSbtPlugin("com.lightbend.conductr" % "sbt-conductr" % "2.1.16")
```

And some Play plugin in my frontend project in `project/plugins.sbt`:

```
"com.adrianhurt" %% "play-bootstrap" % "1.1-P25-B3"
```

The `descriptor()` in the service interfaces also needed to be updated since the `.with(...)` has been replaced by `withCalls(...)`. So From:

{% highlight java %}
@Override
default Descriptor descriptor() {
    return Service.named("itemservice").with(
            Service.restCall(Method.GET,  "/api/items/:id", this::getItem),
            Service.restCall(Method.GET,  "/api/items", this::getAllItems),
            Service.restCall(Method.POST, "/api/items", this::createItem)
    ).withAutoAcl(true);
}
{% endhighlight %}

To: 

{% highlight java %}
@Override
default Descriptor descriptor() {
    return Service.named("itemservice").withCalls(
            Service.restCall(Method.GET,  "/api/items/:id", this::getItem),
            Service.restCall(Method.GET,  "/api/items", this::getAllItems),
            Service.restCall(Method.POST, "/api/items", this::createItem)
    ).withAutoAcl(true);
}
{% endhighlight %}

All this made it possible for us to successfully compile our project. The next things we need to change is updating our read-sides to make use of the new API.
Starting with replacing the deprecated `CassandraReadSideProcessor`:

{% highlight java %}
public class OrderEventProcessor extends CassandraReadSideProcessor<OrderEvent> {
{% endhighlight %}

With `ReadSideProcessor`:

{% highlight java %}
public class OrderEventProcessor extends ReadSideProcessor<OrderEvent> {
{% endhighlight %}

Next step is to inject an instance of `CassandraSession` and `CassandraReadSide` via the constructor:

{% highlight java %}
private final CassandraSession session;
private final CassandraReadSide readSide;

@Inject
public OrderEventProcessor(CassandraSession session, CassandraReadSide readSide) {
    this.session = session;
    this.readSide = readSide;
}
{% endhighlight %}

All code related to handling offsets can be deleted since Lagom now handles this for us.
We delete the following:

{% highlight java %}
private PreparedStatement writeOffset = null; // initialized in prepare

private void setWriteOffset(PreparedStatement writeOffset) {
    this.writeOffset = writeOffset;
}

private CompletionStage<Done> prepareWriteOffset(CassandraSession session) {
    LOGGER.info("Inserting into read-side table order_offset...");
    return session.prepare("INSERT INTO item_order_offset (partition, offset) VALUES (1, ?)").thenApply(ps -> {
        setWriteOffset(ps);
        return Done.getInstance();
    });
}

private CompletionStage<Optional<UUID>> selectOffset(CassandraSession session) {
    LOGGER.info("Looking up order_offset");
    return session.selectOne("SELECT offset FROM item_order_offset")
            .thenApply(
                    optionalRow -> optionalRow.map(r -> r.getUUID("offset")));
}
{% endhighlight %}

After having our `OrderEventProcessor` class extend from the `ReadSideProcessor` abstract we are prompted to implement two methods `buildHandler()` and `aggregateTags()`.
`aggregateTags()` simply replaces `aggregateTag()` where as `buildHandler()` will contain the setup needed for our `ReadSideHandler`.
The `prepare(CassandraSession session)` and `defineEventHandlers(EventHandlersBuilder builder)` that used to be overridden are now implemented in `buildHandler()`.
Note that the logic from the existing `prepare()` is split up into a `setGlobalPrepare()`, for creating Cassandra tables (note that these tasks should be idempotent), and a `prepare()`, for preparing statements, in `buildHandler()`. 
We delete the old `prepare(CassandraSession session)` and the `defineEventHandlers(EventHandlersBuilder builder)`:

{% highlight java %}
/**
 * Prepare read-side table and statements
 */
@Override
public CompletionStage<Optional<UUID>> prepare(CassandraSession session) {
    return
            prepareCreateTables(session).thenCompose(a ->
                    prepareWriteOrder(session).thenCompose(b ->
                            prepareWriteOffset(session).thenCompose(c ->
                                    selectOffset(session))));
}

/**
 * Bind the read side persistence to the OrderCreated event.
 */
@Override
public EventHandlers defineEventHandlers(EventHandlersBuilder builder) {
    LOGGER.info("Setting up read-side event handlers...");
    builder.setEventHandler(OrderCreated.class, this::processOrderCreated);
    return builder.build();
}
{% endhighlight %}

Finally we do the necessary refactoring to implement both `buildHandler()` and `aggregateTags()`, and we clean up the existing logic for read-side event handling:

{% highlight java %}
/**
 * Write a persistent event into the read-side optimized database.
 */
private CompletionStage<List<BoundStatement>> processOrderCreated(OrderCreated event) {
    BoundStatement bindWriteOrder = writeOrder.bind();
    bindWriteOrder.setUUID("orderId", event.getOrder().getId());
    bindWriteOrder.setUUID("itemId", event.getOrder().getItemId());
    bindWriteOrder.setInt("amount", event.getOrder().getAmount());
    bindWriteOrder.setString("customer", event.getOrder().getCustomer());
    LOGGER.info("Persisted Order {}", event.getOrder());
    return CassandraReadSide.completedStatements(Collections.singletonList(bindWriteOrder));
}

@Override
public ReadSideHandler<OrderEvent> buildHandler() {
    CassandraReadSide.ReadSideHandlerBuilder<OrderEvent> builder = readSide.builder("item_order_offset");
    builder.setGlobalPrepare(() -> prepareCreateTables(session));
    builder.setPrepare(tag -> prepareWriteOrder(session));
    LOGGER.info("Setting up read-side event handlers...");
    builder.setEventHandler(OrderCreated.class, this::processOrderCreated);
    return builder.build();
}

@Override
public PSequence<AggregateEventTag<OrderEvent>> aggregateTags() {
    return TreePVector.singleton(OrderEventTag.INSTANCE);
}
{% endhighlight %}

This leaves us with some refactoring the be done in our service implementations for registering the read-side.
The sole thing that needs to be done is replacing the injected `CassandraReadSide`, which is now deprecated, with `ReadSide`.
From:

{% highlight java %}
@Inject
public OrderServiceImpl(PersistentEntityRegistry persistentEntities, CassandraReadSide readSide,
                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {
    this.persistentEntities = persistentEntities;
    this.pubSubRegistry = topics;
    this.db = db;
    this.itemService = itemService;

    persistentEntities.register(OrderEntity.class);
    readSide.register(OrderEventProcessor.class);
}
{% endhighlight %}

To:

{% highlight java %}
@Inject
public OrderServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,
                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {
    this.persistentEntities = persistentEntities;
    this.pubSubRegistry = topics;
    this.db = db;
    this.itemService = itemService;

    persistentEntities.register(OrderEntity.class);
    readSide.register(OrderEventProcessor.class);
}
{% endhighlight %}

This concludes migrating our read-side logic to the new API.

Since Lagom now supports multiple persistence backends, and not only just Cassandra, TestKit no longer starts with Cassandra enabled by default.
This requires us to add a single line `.withCassandra(true)` to our server setup:

{% highlight java %}
@BeforeClass
public static void setUp() {
    server = ServiceTest.startServer(ServiceTest.defaultSetup()
            .withCassandra(true)
            .withConfigureBuilder(b -> b.overrides(
                    Bindings.bind(ItemService.class).toInstance(itemService))));
}
{% endhighlight %}

All in all, migrating the code base to Lagom 1.2 took about fifteen minutes.

## Looking at the rest of our initial feedback
In the [previous blogpost](/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html) on Lagom 1.0 we, like many other Java developers, made the point that only offering sbt as the buildtool would repel many potential Java developers.
We are glad that they addressed this quickly in the first major version (1.1) they released.

Regarding using Lagom in production without using ConductR, the bare minimum for this was to write your own service locator.
Jonas Bonér started two service locator projects for [ZooKeeper](https://github.com/jboner/lagom-service-locator-zookeeper) and [Consul](https://github.com/jboner/lagom-service-locator-consul).
Coming up is also a free Limited Use evaluation license for Production Suite 2.0 which will allow developers to start working with the [Reactive Platform's commercial features](https://www.lightbend.com/platform) from the beginning.
Including not only ConductR but other goodies such as monitoring for Lagom circuit breakers and Akka actors.
We also suggested providing support for Eureka and while Lightbend might be able to provide support in some fashion their idea is that with Lagom everything is built-in and they don't want you to require any extra infrastructure which is where the service orchestration of ConductR with HAProxy comes in.

In the blogpost we also made several impressions compared to Spring Cloud and Netflix OSS.
A strong point for Lightbend is that they want to distinct themselves from Pivotal by providing commercial support.
This isn't the case with Netflix OSS and Spring although Pivotal works closely with the Netflix to optimize integration.

A key advantage of Lagom remains that it is non-blocking down to the core, starting from the database up to the endpoints, while Spring isn't.
In the upcoming [Spring Framework 5](https://spring.io/blog/2016/07/28/spring-framework-5-0-m1-released), of which a [milestone version](http://projects.spring.io/spring-framework/) (M3) is already available, Pivotal are integrating their [Spring Reactive](https://spring.io/blog/2016/02/09/reactive-spring) initiative providing core reactive functionality and reactive web endpoint support.
On the topic of Spring Reactor, a colleague of ours, [Tom Van den Bulck](https://twitter.com/tomvdbulck), recently wrote a blogpost on [Reactive Programming with Spring Reactor](/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html) following up a presentation of [Stephane Maldini](https://twitter.com/smaldini) at [Ordina's JOIN 2016](https://ordina-jworks.github.io/conferences/2016/09/27/JOIN-2016.html) event, a small 1-day conference hosted by Ordina, on Reactive Programming which has also been recorded and is available on [YouTube](https://www.youtube.com/watch?v=RU0yQhfybDg). 
So it will be interested to see whether Spring Framework 5 allows them to catch up on reactive programming, which up til now, remained a stronger point of Lagom.

Lagom's CQRS and Event Sourcing integration remains another strong point of Lagom, the out-of-the-box integration is pleasant and easy to work with and they continue to improve on it, now with JDBC support.
In a Spring Cloud application a common solution for this is making use of the [AxonFramework](http://www.axonframework.org) which requires you to do the necessary glueing yourself.

Polyglot support is something that still lacks on the side of Lagom.
Spring has [Sidecar](http://projects.spring.io/spring-cloud/spring-cloud.html#_polyglot_support_with_sidecar) for this purpose.
While Lagom service's map down to ordinary, standard HTTP and WebSockets, it currently lacks support for any design driven API development.
Lightbend understands that this is an important feature in any microservice architecture and is planning on implementing a solution for this probably based upon [Akka Streams](http://doc.akka.io/docs/akka/2.4.16/scala/stream/) and [Alpakka](https://github.com/akka/alpakka).
The idea is that you should be able to generate a Lagom service interface from specifications, allowing transparent integration.

Binary coupling was also a concern of ours, and up until now nothing hasn't really changed.
Lightbend also still has plans to support [Swagger](http://swagger.io) but there is no ETA for the time being.

## Conclusion
While there are still a couple of important things in need of being addressed we believe that we can conclude that Lightbend is carefully listening to feedback given by the communicate which is a very nice thing.
They continue to improve Lagom with new features and better user experience for the developers.

At the same time, Pivotal is working on providing better reactive support with their upcoming Spring Framework 5, a point in which they currently lack.

Both companies see each other as a competitor which is nice for us, developers, as this means that we get to have a better user experience using either framework.
Pivotal clearly still has an advantage on Lightbend with the maturity of Spring but Lagom seems to develop nicely and continues to improve on its strongest points.

## Lagom 1.3 preview
The [first issue](https://github.com/lagom/lagom/issues/1), created after Lagom was released, was the need to implement a Scala API.
Something many users of Lightbend's technologies were thirsting for.
Until now it has been possible to use Scala using the Lagom Java API, see the following [seed](https://github.com/dotta/activator-lagom-scala-chirper) created by [Mirco Dotta](https://github.com/dotta).

Lightbend made work of it and since a couple of days a [preview version](http://www.lagomframework.com/blog/lagom-scala-api-preview.html) was released by Lightbend to try it out.
Users should be aware however that the API is still considered experimental and that it will likely change before the final release.
Therefor it should not be used for any production service just yet.

## Extra resources

- [Lagom documentation](http://www.lagomframework.com/documentation/)
- [Lagom Twitter](https://twitter.com/lagom)
- [Lagom Gitter](https://gitter.im/lagom/lagom)
- [Lagom mailing list](https://groups.google.com/forum/#!forum/lagom-framework)
- Another nice example project is the [Online Auction](https://github.com/lagom/online-auction-java) sample
