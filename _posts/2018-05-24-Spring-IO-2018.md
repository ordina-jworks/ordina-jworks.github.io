---
layout: post
authors: [dieter_hubau, yannick_de_turck, tom_van_den_bulck, johan_silkens, jens_bosman, tim_schmitte]
title: 'Spring IO 2018'
image: /img/spring-io-2018/spring-io.jpg
tags: [Spring IO,Spring,Java,Conference]
category: Conference
comments: true
---

## Spring IO is back!

Marked in red on the calendar of every JWorks consultants: the yearly edition of Spring I/O.
This year, we weren't gonna wait for the explicit approval of our manager and we ordered 27 early bird tickets as soon as we could and booked our flights to sunny Barcelona!
It promised to be a special edition, since everything was gonna be **bigger and better**: the venue, the speaker roster, the food, the atmosphere.

> The **Palau de Congressos de Barcelona** is a much bigger venue than the one we're used to from previous years.
> This is why the organizer [Sergi Almar](https://twitter.com/sergialmar?lang=en) was able to accomodate **1000 attendees** this year, which shows how much interest there is in the Spring community.
> It was a really good location, it has ample space to grow in the coming years and the catering was also of good quality.
> Next year we'll most likely get the same venue, but the event will probably overlap with the nearby [Barcelona International Motor Show](http://www.automobilebarcelona.com/es/visitar), which takes place every two years.
> Free test drives during a conference? Yes, please!

****

<img class="image fit" src="{{ '/img/spring-io-2018/venue1.jpg' | prepend: site.baseurl }}" alt="Spring IO 2018 Photo Collage" />

****

<img class="image fit" src="{{ '/img/spring-io-2018/springio.jpg' | prepend: site.baseurl }}" alt="Spring IO 2018 Photo Collage" />

****

We'll talk about some of the presentations this year, but it is definitely not a complete list.
There were so many interesting talks, we're actually gonna need quite some time to rewatch them all on Youtube!

At the bottom, we have a full list of talks and resources.
Let us know if we missed anything by filing an issue or contacting us at [our general JWorks email](mailto:jworks@ordina.be)!

## Day 1: Talks & Workshops

* [Mark Heckler: Migrating legacy enterprise Java applications to Spring Boot](#mark-heckler-migrating-legacy-enterprise-java-applications-to-spring-boot)
* [Andreas Falker: Spring Security 5 Workshop](#andreas-falker-spring-security-5-workshop)
* [Tommy Ludwig: Observability with Spring based distributed systems](#tommy-ludwig-observability-with-spring-based-distributed-systems)

****

## Mark Heckler: Migrating legacy enterprise Java applications to Spring Boot

You can find him on Twitter at [@mkheckler](https://twitter.com/mkheckler).

Mark explained us how easy it can be to migrate an existing legacy Enterprise Java application to a modern, state-of-the-art Spring Boot app.
Many people think that migrating these kinds of applications is impossible or very hard without rewriting the whole thing, but Mark gave us some very good pointers on how to do it quickly and efficiently:

- Generate a new skeleton project from [start.spring.io](https://start.spring.io)
- Use schema.sql and data.sql data sheets to migrate and test your database
- Use Kotlin to vastly simplify your code
  - use data classes to simplify access to members and constructors
  - move the constructor definition in the same line as the class definition
- Using Spring Data, no more need to use `PersistenceContext` or `EntityManager`
- Using Spring MVC with `@RestController`, no more need to declare `@Produces` or `@Consumes`

### Benefits

- Less code: your code vastly diminishes using Kotlin and data classes
- The Spring Boot + Kotlin combination greatly reduces the amount of boilerplate code
- Business logic and Service Layer of the old application remains the same and is better encapsulated
- Code becomes easier to maintain, easier to test
- Spring Boot offers more and better deployment options

JWorks consultants have done these kinds of migrations at multiple clients, with great success rates.
Non-technical people like functional analysts, product owners and business experts are continuously amazed at the speed with which we are able to do this.
Technical people that have been doing JEE development for years ask us how they can learn Spring Boot.

## Andreas Falker: Spring Security 5 Workshop

I don't want to diminish the excellent Spring Security workshop from Andreas Falker by copying anything from him.
He deserves all the credit for his amazing work so I'm just gonna link to it here:

[https://andifalk.github.io/spring-security-5-workshop/](https://andifalk.github.io/spring-security-5-workshop/)

**Thank you Andreas for this great resource!**

## Jurgen Hoeller: Spring Framework 5 - Hidden Gems

Since almost every feature was backported to 4.3, most of them are already known to the general public.
Though there are 7 areas of refinement within 5.0 that aren’t widely known to the public.

### Commons Logging Bridge

So the Spring team came up with a new dependency called spring-jcl which is actually a reimplementation of a logging bridge.
It is a required dependency and is here to help streamline the logging functionality.
The main difference with this way of working is that you don’t need to go through a dependency hell where you would manually add exclusions to ignore certain logging dependencies.
Just add the logging library to your classpath and everything will switch to the logging implementation of your choice.
It now has first class support for Log4J 2 (version 1 has reached its end of life), SLF4J and JUL.

### Build-Time Components Indexer

The file system traversal for classpath scanning of all packages within the specified base packages using either `<context:component-scan>` or `@ComponentScan` might be slow on startup.
This is especially true if your application is started for a small period of time or where I/O is very expensive.
Think short-running batch processes and functions, or applications being started and stopped on Google App Engine every 2 minutes.
The common solution was to narrow your base packages, or even to fully enumerate your component classes so you would skip scanning all together.
Starting with 5.0 there is a new build-time annotation processor that will generate a META-INF/spring.components file per jar containing all the classes which in turn will be used automatically at runtime for compatible component-scan declarations.

### Nullability

The new version contains comprehensive nullability declarations across the codebase.
Fields, method parameters and method return values are still by default non-null, but now there are individual `@Nullable` declarations for actually nullable return values for example.
For Java this means that we have nullability validation in IntelliJ IDEA and Eclipse.
This allows the Spring Team to find subtle bugs or gaps within the framework's codebase.
It will also allow us as developers to validate our interactions with the Spring APIs.
When you're writing code in Kotlin it will give you straightforward assignments to non-null variables because the Kotlin compiler will only allow assignments for APIs with clear nullability.

### Data Class Binding

Spring Data can now work with immutable classes.
No need for setters anymore since it can work with named constructor arguments!
The property names are matched against the constructor parameter names.
You can do this by explicitly using `@ConstructorProperties`, or they are simply inferred from the class bytecode (if you pass `-parameters` or `-debug` as compilation argument).
This is a perfect match with Kotlin and Lombok data classes where the getter and setters are generated at compile time.

### Programmatic Lookup via ObjectProvider

The `ObjectProvider` is a variant of `ObjectFactory`, which is designed specifically for injection points, allowing for programmatic optionality and lenient not-unique handling.
This class had the following original methods: `@Nullable getIfAvailable()` and `@Nullable getIfUnique()`.
With the new version of Spring these methods have been overloaded with `java.util.function` callbacks which empowers the developer to return a default value instead of returning `null`.

### Refined Resource Interaction

Spring's `Resource` abstraction in core.io has been overhauled to expose the NIO.2 API at application level, eg. `Resource.getReadableChannel()` or `WritableResource.getWritableChannel()`.
They are also using the NIO.2 API internally wherever possible, eg. `FileSystemResource.getInput/OutputStream()` or `FileCopyUtils.copy(File, File)`.

### Asynchronous Execution

Spring 5.0 comes with a couple of interface changes that will help you with asynchrous execution:
- The `ListenableFuture` now has a `completable()` method which exposes the instance as a JDK `CompletableFuture`.
- The `TaskScheduler` interface has new methods as an alternative to `Date` and `long` arguments: `scheduleAtFixedRate(Runnable, Instant, Duration)` and `scheduleWithFixedDelay(Runnable, Instant, Duration)`.
- The new `ScheduledTaskHolder` interface for monitoring the current tasks, eg. `ScheduledTaskRegistrar.getScheduledTasks()` and `ScheduledAnnotationBeanPostProcessor.getScheduledTasks()`.

## Cloud Native with Google Cloud Platform and Spring Boot
### by [Ray Tsang](https://twitter.com/saturnism){:target="_blank"}


- workshop: bit.ly/spring-gcp-lab
- code: https://github.com/saturnism/spring-cloud-gcp-guestbook
- Cloud console: https://console.cloud.google.com/

On the one hand this workshop lowered the entry threshold for newbies.
On the other hand it provided insight about what services Google Cloud has to offer.
Google Spanner, Pub/Sub messaging system, CloudSQL, Runtime Config ... They were all addressed. 

We created a guestbook application which consisted of front and backend microservices.
The workshop builds this up nicely by adding features to the application step by step.
Each step introduces you to another Google Cloud service.
Those of you who want to make the workshop yourself, check out the link above.

#### Google PubSub

What stayed with me is Google's Pub/Sub message-oriented middleware.
A publisher that creates the messages sends them to a topic. 
Consumers can subscribe to this topic to obtain the messages.
Publishers and subscribers are decoupled. Neither of them is required to know the other one.
Subscribers will either pull messages or get messages pushed from the topic.
PubSub messages will be delivered at least once, but can be processed multiple times by different subscribers.
Unprocessed PubSub messages are only kept for 7 days

## Flight of the Flux
### by [Simon Baslé](https://twitter.com/simonbasle){:target="_blank"}

In this talk Simon went deeper into the inner workings of [Spring Reactor](https://projectreactor.io/).
 
The session started off giving a brief recap of reactive programming and reactive streams 
before delving deeper into the machinery behind reactor 3.

### Assembly time vs execution time

When programming with Reactor 3 (and other Functional reactive libraries like RxJava)
the programming model is quite different compared to the classic imperative style.
Basically your whole chain is lazy, you describe a sequence of operations and no actual
processing happens until someone subscribes.

Example: 

assembly: 

{% highlight java %}

    this.myFlux = Flux.just("foo")
        .map("String::length");
        
{% endhighlight %} 

As stated above, all that really happens when this code is called is creating a chain of operators 
(under the hood this phase is also used for things like operator fusion, see below).

execution: 

{% highlight java %}

    this.myFlux
        .subscribe(System.out::println);

{% endhighlight %} 

When the subscribe method is called the actual chain is executed and the length of the string is printed in the console.

For those familiar with Java 8 it's basically the same as java 8 streams, 
nothing really happens until a terminal operation is used(collect/reduce/count/...).

One of the drawbacks of this is error handling. 
Since the error doesn't happen until the subscription, it's harder to see where the error actually happens.
To alleviate this Reactor provides a feature called assembly tracing which can be enabled with the checkpoint() operator.

> Nothing happens until you subscribe

### Cold and hot observables

Previous statement holds for most observables typically encountered in a project; http calls, data lookups etc
no data is actually being produced until someone subscribes. 
Sometimes however an observable can be a constantly emitting event stream.
When multiple subscribers subscribe on a cold observable each of these subscriptions 
will trigger the whole chain from the start.
A hot observable is constantly producing data and will only give the elements to the subscriber emitted after subscription time.

### Scheduling

Reactor is concurrency agnostic, this means it doesn't impose a concurrency model, 
it does give the developer the tools to change reactor's executor behaviour however.

This is done using the scheduler abstraction;
a scheduler defines the execution context, this can be the same thread, another thread, using a threadpool...
The special operators publishOn and subscribeOn allow you to change the execution context of the current chain.

'publishOn()' changes the execution context of the downstream operators.

ex:

{% highlight java %}

flux.op1().op2().publishOn(scheduler1).op3().subscribe((result) -> doSomethingWith(result));

{% endhighlight %} 

op1 and op2 will execute in the original execution context(usually the thread in which the subscribe is called).
op3 and the action defined in the subscribe method itself will execute in scheduler1's execution context.

'subscribeOn()' changes the execution context of the subscription, meaning of the start of the execution of the chain.


ex: 

{% highlight java %}

flux.op1().op2().subscribeOn(scheduler1).op3().subscribe((result) -> doSomethingWith(result));

{% endhighlight %} 

The whole chain will execute in scheduler1's execution context.

### Work stealing

Although previous topics surface easily and are simpler to demonstrate this was perhaps the most abstract one of the session along with operator fusion.
When using schedulers supporting parallel execution, reactor uses so called 'work stealing' algorithms to balance the load on the different threads.
If a thread is idle it can take over execution of tasks that were originally scheduled to be executed by a different thread.
Under the hood this is achieved by using a shared queue for the tasks and a drain loop.

  
### Operator fusion

One of the big advantages of having a chain of tasks defined and split up in multiple steps is that it allows 
the engine to identify possible optimizations in the chain. 
Since each individual operator also has an overhead(queue etc for workstealng)
it's sometimes more efficient to combine some operators and execute them as 1. 

ex. map(a).map(b).map(c) => map(abc)

Reactor tries to achieve this by using a negotiation process between the operators.

### Conclusion

This talk gave us more insight into the more advanced parts of Reactor, 
arming us with knowledge to tackle potential problems in a reactive environment and helping us understand 
Reactor's deeper mechanisms.

For more information: He uploaded his presentation on [speakerdeck](https://speakerdeck.com/simonbasle/flight-of-the-flux?slide=1)


## (Spring)Kafka - One more arsenal in a distributed toolbox
### by [Nakul Mishra](https://twitter.com/nklmish){:target="_blank"}

First he described us [Apache Kafka](https://kafka.apache.org/), a very potent messaging system which allows you very easily to act as a throughput between your applications, just make sure to stay away from [recreating ESB antipatterns with Kafka](https://www.thoughtworks.com/radar/techniques/recreating-esb-antipatterns-with-kafka).
Kafka is more then a messaging queue, combining speed, scaleability and stronger ordering guarantees then traditional messaging keys. 
In order to benefit from this ordering it is important to choose a correct partition key.
Kafka does put more emphasis on smart consumers, thus a more client centric approach vs the broker centric approach used by Message Oriented Middleware. 
This is archieved by being designed for rentention and scale, Kafka gives consumers, clients, the time to process the messages they want to process whenever they want to.

It is also possible to use Kafka as a database, by having it process a stream of data in real-time using [KSQL](https://www.confluent.io/product/ksql/) 
Of which the results can very easily be pushed to external systems (HDFS, S3, JDBC).

To be scaleable, Kafka is kept simple at its core, all data is stored as a partitioned log. 
So that writes are append only and reads a single seek & scan allowing the underlying filesystem thus to very easily handle the storing & caching of messages. 
Also when reading data is copied directly over from the disk buffer into the network buffer bypassing the JVM, ideally for flooding your network.

[Spring Kafka](https://spring.io/projects/spring-kafka) integrates Kafka with Spring giving you all the benefits of the Spring ecosystem and supports [Kafka Streams](https://kafka.apache.org/documentation/streams/).
[Testing](https://docs.spring.io/spring-kafka/docs/2.1.6.RELEASE/reference/html/_reference.html#testing) is made easier by providing an @EmbeddedKafka and TestUtils

{% highlight java %}
@EmbeddedKafka(partitions = 1,
         topics = {
                 KafkaStreamsTests.STREAMING_TOPIC1,
                 KafkaStreamsTests.STREAMING_TOPIC2 })
{% endhighlight %}

Spring Kafka also has a starter available for [Spring Boot](https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-messaging.html#boot-features-kafka) which makes it very easy to get started with Kafka and start playing around. 
Just go to [start.spring.io](http://start.spring.io/) and get the Kafka dependency.

The slides of this presentation can be found at [slideshare](https://www.slideshare.net/nklmish/springkafka-one-more-arsenal-in-a-distributed-toolbox/)

## Breaking down monoliths into system of systems - Oliver Gierke
### by [Oliver Gierke](https://twitter.com/olivergierke){:target="_blank"}

The goal of this workshop is not to provide a clear architecture of the perfect application, but more to make you think.
To let you reflect about your existing application.

This talk can also be found in shorter iterations like [here](https://www.youtube.com/watch?v=VWefNT8Lb74)
But now is was not a mere hour, but 2 full hours.
Giving us the possibility to have a more in depth look of the prepared code and look into potential problems which might be glossed over when the talk is shorter.

It is a summary of observations about monoliths and microservices.
All of which tends to boil down to the correct definition of bounded contexts within applications, how you can divide your application in logical modules and how these can communicate with each other.

First we will observe what happens when a monolith is transformed into a microlith, a distributed monolith.
Subsequently we will improve the design of the monolith with these bounded contexts and come to a modulith.
This modulith is still a single application, a monolith, but with different bounded contexts each having clearly defined borders allowing us to easier divide the work over various teams.

From a modulith one can go to a system of sytems, a true microservice architecture.
In a system of systems there are 2 ways you can implement the communication, either via messaging or via rest.


<span class="image center"><img class="p-image" alt="Refactoring to a system of systems  (from Oliver Gierke workshop)" src="/img/spring-io-2018/sos-workshop.png" class="image fit" style="max-height:'1075px'; max-width='1920px'"></span>


The sample code of this workshop can be found on [https://github.com/olivergierke/sos](https://github.com/olivergierke/sos)

### Monolith
The monolith is reasonably ordered and the bounded contexts have been split in various packages.

When building your java application make optimal use of the package options provided by Java as mentioned in this [blog](http://olivergierke.de/2013/01/whoops-where-did-my-architecture-go/) of 2013: 

Make your code package protected whenever it does not need to be accessed from the outside, a good starting point is to make your repositories no longer public.

Whenever there is leakage over the bounded contexts, for example the Linitems contains Products, try to used ids and not the objects of another bounded context.
Because whenever you update an object used within another bounded context you will also leak into that context.

It is also noted that badly structured applications tend to be built from the bottom up, from db to the top.
This means that your design is going to be way to data centric instead of being focussed on the real business interactions you are supposed to handle.

Also try to prevent to use methods which update 2 bounded contexts simultaneously, as these methods have the reflex of drawing more and more code in, they tend to grow like a cancer.
Eventually these methods will become more like a real cancer killing your application from the inside out.

In short, the following design decisions should be made:
- move bounded contexts into packages.
- inter-context interaction is processed locally and resulting in either success or an exception (in jvm method calls are also very efficient and executed exactly once)
- avoid to domain classes reference each other over bounded contexts, but it tends to be convenient. (-)
- when you leak into other bounded contexts there is also a great risk on circular dependencies. (-)
- whenever you need to add a new feature, you tent to need to touch other parts of the system as well, because there are no clear boundaries. (-)
- a monolith is easy to refactor (+)
- it has strong consistency (+) but this is also a disadvantage as transactions become more brittle when they fail because of related business funcionality (-). 

The monolith example code can be found [here](https://github.com/olivergierke/sos/tree/master/00-monolith)

### Microlith
With a microlith you will split up your systems into various smaller systems.

Whenever you managed to transform your monolith to microservices you tend to think that all your problems have been solved.

But if you did not pay attention to correctly define your bounded contexts and thus minimized your communication the chance is great that you have made a microlith with the following problems:
- No longer able to use local transaction consistency. (-)
- Local method invocation is transformed into RPC-ish HTTP calls. (-)
- You have translated the transactions of your monolith into a distributed system, needing HTTP to update each other. (-)
- Remote calls are executed while serving user requests and this over multiple services. (-)
- Running and testing requires the other services to be available. (-)
- There is a strong focus on API contracts, which tend to be very CRUD-y with a lack of business abstraction and hypermedia. (-)
- Detecting breaking API changes is prioritized over making evolvable api's. (-)
- One tends to add more technology in order to solve issues: bulkheads, retries, circuit breakers, asynchronous calls, more monitoring systems, ... (-)

It tends to minimize the risks of a rollback, but it does not really solve any issue, it just distributes your problems.

> The first rule of distributed systems is don’t distribute your system until you have an observable reason to.

If you did not define your bounded contexts properly it is very difficult for you to observe how to distribute your system.

Example code of the microlith can be found [here](https://github.com/olivergierke/sos/tree/master/10-microlith)

### Modulith
With the modulith we will start to use events within the monolith.

Also we will start to use more domain specific methods, like an add() on an Order, as this makes the whole more abstract and your domain objects become more then glorified getters and setters.

We are not doing CQRS or event sourcing, but we just use eventing as a way to signal events over bounded contexts.

Thanks to these events it becomes relatively easy to split the work and use these events as either input or output for different services within the applications.
Your units of work will have clear boundaries making testing and design easier.

The differences with a monolith are:
- Focus of domain logic has moved to aggregates. (+)
- Integration between bounded contexts is event based. (+)
- The dependency between bounded contexts is inverted. (+)

> Side Step: Application Events with Spring Data

This is a very powerfull mechanism to publish events in a Spring application.

Whenever you need to send data to another bounded context you trigger events.
This has the advantage that your business services no longer need to know about each other anymore, they just need to trigger an event which gets picked up by the services which are interested in this event.

Transactional semantics are still retained because the eventing is synchronous, by default.
This also applies for JEE eventing.

The [@TransactionalEventListener](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/transaction/event/TransactionalEventListener.html) annotation allows you to delay the executiong of events, so for example, you can send out an email when the Order has truly been completed.

> Side Step: Error Scenarios

When a synchronous event listener fails, this will be handled by the transaction, so no worries.

But when an asynchronous event listener fails, the transaction does not get rolled back and you will need to deal with retries.

You can make use of an Event Publication Registry when you use of [TransactionalEventListeners](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/transaction/event/TransactionalEventListener.html) as these event listeners are decorated with a log, before the commit because the system needs to know to where the events need to be sent out.
When the event has been processed, the log will be cleared, if it does not get cleared the system can keep retrying. 
So you do not lose events.

Example code of the modulith can be found [here](https://github.com/olivergierke/sos/tree/master/20-modulith)

### System of Systems

#### Messaging
Whenever you make use of a message broker you do introduce a potentional single point of failure, like with [Apache Kafka](https://kafka.apache.org/) or [RabbitMQ](https://www.rabbitmq.com/).
These brokes know about all messages of all systems and decide how long these messages will be retained.

Coupling does exist, although not explicit, but the message format will decide which version of a service can process these messages, just as with REST.
Especially if you keep your events for a long time, which is possible with Kafka, you might need to think about transforming existing events. 

But these messaging systems tend to be designed for scale.

Pro tip: make use of [JsonPath](https://github.com/json-path/JsonPath) annotations for the message payload in order to make it more robust.

Example code of a system of systems with messaging can be found [here](https://github.com/olivergierke/sos/tree/master/30-messaging-sos)

#### REST Messaging
If you use REST you will have to deal with caching, pagination and conditional requests.

Messages do not tend to be stored for long periods of time and most communication tends to be synchronous.
One does have to pay attention on not to lose events as your application will immediately know if the message was processed correctly, either that or it times out.


#### REST Polling
With polling your producers do not send out messages to your consumers, but the consumers will poll the producers for new events they can process.

This means that:
- You do not need any additional infrastructure like an Apache Kafka or a Message Bus. (+)
- Event publication is part of the local transaction. (+)
- The publishing system, producer, controls the lifecycle of the events and can transform these if necessary. (+)
- The events never leave the publishing system. (+)
- There might be a bigger consistency gap, depending on how frequently the consumers poll. (-)
- It does not scale that well. (-)

The example code with REST can be found [here](https://github.com/olivergierke/sos/tree/master/40-restful-sos)
### Conclusion
This is a great workshop which makes you ponder about the design decisions you have made for your applications. 
If you can ever participate at one of these workshops, do not hesitate to join it as workshops like these are what software conferences are all about.

## Tommy Ludwig: Observability with Spring based distributed systems

<span class="image left"><img class="p-image" alt="Tommy Ludwig" src="/img/spring-io-2018/tommy-ludwig.jpg"></span>

### Introduction

[Tommy](https://twitter.com/tommyludwig){:target="blank"}'s talk introduced three main pillars of observability: logging, metrics, and tracing.

Tommy explained that observability is achieved through a set of tools and practices that aim to turn data points and contexts into insights.
Observability is something you should care about as it provides a great experience for the users of your system and it builds confidence in production where failure **will** happen.
You ought to give yourself the tools you need in order to be a good owner in order to detect this failures as early as possible.
Mean time to recovery is key here.
He also quoted Werner Vogels's, the CEO of Amazon, "You build it, you run it" while also adding to it that you need to monitor it.

Within a Spring Boot project, we have access to Actuator and it is awesome.
It comes with a lot of goodies out of the box.
There is also [Spring Boot Admin](https://github.com/codecentric/spring-boot-admin){:target="_blank"} that makes it easy to access and use each instance's Actuator endpoints.

Distributed systems make observing them hard by design as a request spans multiple processes.
You therefore need to stitch these together in order to fully make sense of it.
There are also more points of failure and adding multiple instances of the same service, for scaling reasons, will only increase the monitoring complexity.

Tommy named three sides to observability:
* Logging
* Metrics
* Tracing

### Logging
Logs are request scoped, arbitrary messages that you want to find back later.
They are formatted to give you context via things such as logging levels and the timestamp.
The issue with logs is that they do not scale, concurrent requests intermingle logs, and searching through them can be cumbersome.
In order to tackle these issues you can make use of centralized logging while also adding a query capability to retrieve a collection of matching logs.
Within Spring Boot we can configure the logging via Spring Environment and via Actuator at runtime.
[Spring Cloud Sleuth](https://cloud.spring.io/spring-cloud-sleuth/){:target="_blank"} is useful to add a trace ID for request correlation.

### Metrics
Metrics aggregate time series data and have a bounded size.
You can slice these based on dimensions, tags and labels.
The main goal of metrics is to visualize and identify trends and deviations, and to raise alerts based on metric queries.
Some examples of metrics are: response time, the response's body size and memory consumed.
In order to properly measure all this, you need to set up a metrics backend to which all applications publish their metrics data.
In Spring Boot 2, [Micrometer](https://github.com/micrometer-metrics/micrometer){:target="_blank"} is introduced as its native metrics library.
Micrometer supports many metrics backends such as Atlas Datadog, Prometheus, SignalFX and Wavefront.
A lot of the instrumentation is auto-configured by Spring Boot and custom metrics are easy to add.
These are configurable via properties and common tags such as the application name, the instance, region, zone, and more.

### Tracing
Local tracing happens via the Actuator `/httptrace` endpoint and displays the latency data.
With distributed tracing you can go across process boundaries which is useful as metrics lack request context and as logs have a local context but limited distributed info.
You define the sample of how many request to trace yourself as you don't want to trace everything especially if you have a high load.
[Zipkin](https://zipkin.io){:target="_blank"} with its UI helps you to see the timing information visually and is a good tracing backend for Spring applications.
Using Spring Cloud Sleuth, it auto-configured the tracing instrumentation via [Zipkin's Brave](https://github.com/openzipkin/brave){:target="_blank"}.
Via properties you can configure things such as the sampling probability and whether certain endpoints should to be skipped.

### Correlation everywhere
Having set up all of these, you now have correlated logging, metrics and tracing across your system, and you can find the data from each based on identifiers.

### Observability cycle
If an issue produces itself we can take the following steps to troubleshoot and bandage the situation:
* The issue should've been reported via an alert or report
* We check the metrics of our system
* If needed, we check the tracing data
* If needed, we check the logs
* Based on the gathered information we can triage the issue and make adjustments to prevent a recurrence

### Key takeaways
System wide observability is crucial in distributed architectures.
The tools to help you with this exist and Spring makes it easy to integrate them in your system as the most common cases are covered out-of-the-box or easily configurable.
Use the right tool for the job and synergise across the different tools.

## Day 2: Talks & Workshops

- [James Weaver: Machine Learning exposed: The fundamentals](#james-weaver-machine-learning-exposed)
- [Joe Grandja: Next generation OAuth support with Spring Security 5  ](#joe-grandja-next-generation-oauth-springsec5)
- [Jeroen Sterken & Kristof Van Sever: Testing every level of spring microservices application (Workshop)  ](#joe-grandja-next-generation-oauth-springsec5)

## Machine learning exposed: The fundamentals
**By [James Weaver](https://twitter.com/JavaFXpert) (Pivotal)**

Machine Learning is a hot topic in tech land nowadays with all kinds of applications like predicting property prices, forecasting weather , self driving cars , plants classification and so on. James gave a brief overview about the fundamentals of Machine Learning and it's applications. 

But how can we define Machine Learning? [Andrew Ng](https://twitter.com/AndrewYNg), Co-founder of Coursera and Adjunct Professor of Stanford University defined Machine learning in his introduction course "Welcome To Machine Learning"  <sup>1</sup> as "*Machine Learning is the science of getting computers to learn, without being explicitly programmed*". An example that Andrew gave was a cleaning robot that can tidy your house. Instead you program the algorithm explicitly on how it should clean. You can for instance let the robot watch you while you demonstrate the tasks on how it should clean and learn from it.

Later on he gave examples of different categories of machine learning.

### Categories of Machine Learning

#### Supervised learning
 
This was the category where James gave the most examples of during his talk . Supervised learning is where you train your model with a data set wich contains the inital data and it's correct answers. The more training data you have, the more accurate your predictions will be.

##### Regression example

An example he showed us was the prediction of house prices using regression.
<span class="image center"><img class="p-image" alt="House price prediction (From Andrew Ng Learning course)" src="/img/spring-io-2018/houseprice.png" class="image fit" style="max-height:'476px'; max-width='876px'">
<small class="center">(From Andrew NG's Machine learning course)</small></span>

In this example, the data set consists of instances with a square footage (input) and price (output). With a regression, we can predict a continuous valued price.
##### Classification example

<div class="row"><span class="image left"><img alt="Iris flower classification using machine learning" src="/img/spring-io-2018/iris-dataset.png" class="image fit" style="max-width='374.5px'; max-height='374.5px;'">
<small>Source:<a href="https://en.wikipedia.org/wiki/User:Nicoguaro/Gallery">Nicoguaro's Wikipedia media gallery</a> (CC BY 4.0) An example of Supervised Learning using classification </small></span>
<span class="image right"><img alt="Iris flower classification using machine learning" src="/img/spring-io-2018/test-data.png" cslass="image fit">
<small></small></span></div>

Another example of Supervised Learning is to determine a certain species of an Iris flower. Your algorithm must try to determine the species with the Sepal and Petal size of a flower as input.  

 
	 
#### Unsupervised Learning 

For Unsupervised learning on the other hand, you don't give the right answers with your dataset. Your learning algorithm will try to find a structure in the given data.

A method to try to find a structure, is to do it by clustering. So your data is 'grouped' is clusters together with data that more or less belongs to each other. Market segment discovery and social media analysis are examples of Unsupervised Learning
#### Reinforcement Learning

By Reinforcement Learning, you give your algorithm rewards when it did something well. This type of learning is very popular in game playing. AlphaGo for example from Google Deepmind was thaught by Reinforcement Learning


### Neural networks

<span class="image left"><img alt="Neural network example" src="/img/spring-io-2018/neural-network.png" class="image">An example neural network</span>
<br/>
<br/>  
The second part of his talk was about Neural Networks. (Artificial) Neural Networks are computing systems that are inspired of biological neural networks. It's made up of highly interconnected processing elements or 'nodes' that can process information. An Neural Network consists of different layers. An input layer, one or more hidden layers and an output layer.

We can visually demonstrate on how Neural Networks work with the help of deeplearning4j. You can clone and try out his example on[ https://github.com/JavaFXpert/visual-neural-net-server](https://github.com/JavaFXpert/visual-neural-net-server) 

Let's use the flower classification example with our neural network.
<span class="image center"><img alt="Iris flower classification Neural Network Example" src="/img/spring-io-2018/iris-flower-neural-network.png" class="image fit"></span>

 


<small> [1: Welcome to Machine Learning (Andrew Ng) ](https://www.coursera.org/learn/machine-learning/lecture/zcAuT/welcome-to-machine-learning) </small>   




## Next generation OAuth support with Spring Security 5
**By [Joe Grandja](https://www.linkedin.com/in/joegrandja/) (Pivotal)**

TODO

## Testing every level of your Spring Microservices application (Workshop)
**By [Jeroen Sterken](https://twitter.com/jeroensterken) & [Kristof Van Sever](https://twitter.com/vanseverk) (Faros NV)**

### Introduction

This workshop focused on testing the different levels of a microservices application. 
It was split up into two parts:
- Testing within a single microservice
- Testing the relationships between microservices with Spring Cloud contract

### Testing within a single microservice with Cucumber and JUnit

The presentation started off with some of the new features that JUnit 5 has to offer. 
Since JUnit 5 supports Java 8, it allows you to use lambda's in assertions, as well as using group assertions with the `assertAll()` method.
It's also possible to run tests multiple times with different parameters, by annotating them with the `@ParameterizedTest` and `@ValueSource` for the arguments.

#### Behaviour driven testing with Cucumber

Unit tests alone are not enough of course, you also need to test how different components work together.
Usually it's the developer who writes such tests, but it's also possible for non-technicals to write such tests with Cucumber.

How does this work exactly?

Cucumber achieves this by using Gherkin, an English plain text language.
It has .feature files where the different scenario's for a certain feature are described.

<pre>
<code>
<b>Feature:</b> A new empty basket can be created and filled with Tapas

<b>Scenario:</b> Client creates a new Basket, and verifies it's empty
    <b>When</b> the user creates a new Basket
    <b>Then</b> the total number of items in the Basket with id 1 equals 0
</code>
</pre>

The above is a simple example of how to describe a feature and scenario.
The words in bold represent [Cucumber keywords](https://github.com/cucumber/cucumber/wiki/Feature-Introduction), called step definitions.
It's also possible to substitute `Scenario` with `Scenario outline` in case you need to test the same scenario with different values.
You can put parameters inside angle brackets (`<>`), which are substituted with values that you define in an [Examples data-table](https://github.com/cucumber/cucumber/wiki/Scenario-Outlines).

The next step is to annotate your methods with the Cucumber step definitions (e.g. `@Given` `@When`)..
The text that you provide the annotations with, should match the text in your .feature file so that Cucumber can glue the two together.
In this case, the <code><b>When</b> the user creates a new Basket</code> of the example above matches with:

```
@When("^the user creates a new Basket$")
```

The annotated methods should execute what you described in the feature files, so in this case the method looks something like this:

```
public void theUserCreatesANewBasket() {
    userBasketManagement.createNewBasket();
  }
```

To try it out for yourself, go to the [Workshop repo.](https://github.com/faros/bdd-cucumber).
There's a solution branch in case you're stuck or wish to compare your code.


### Spring Cloud Contract

One of the challenges of testing chained microservices is making sure that a microservice stub reflects the actual service at all times.  

One way this can be achieved is by using Spring Cloud Contract. 
Spring Cloud Contract enables Consumer Driven Contract development, where one service (consumer) defines its expectations of another service (producer) through a contract.

The first step is for the consumer to write the test for the new feature, following the TDD approach.

Next, add the Spring Cloud Starter Contract Verifier dependency and maven plugin to your producer.
Create a base test class to the test package that loads the Spring Context. 
Make sure to annotate it with `@AutoConfigureMessageVerifier`.

We should also add the contract to our resources on the producer-side:

<pre>
<code>
Contract.make{
    description "should return a list of all tapas"

    request{
        method GET()
        url "/tapas"
    }

    response{
        status 200
        headers {
            contentType applicationJson()
        }
        body (
            [
                [
                        id: 0,
                        name: "All i oli",
                        price: 1.5
                ],
                [
                        id: 1,
                        name: "Banderillas",
                        price: 3
                ]
             ]
        )
    }
}
</code>
</pre>

The above is an example of how a contract is defined, written in Groovy (although YAML is a possibility as well).
It simply specifies that a GET request to `/tapas` should return the provided body as application/json.

Now it's time to create the stub.
Since you've already added the dependency and plugin to your producer, simply run your build for the plugin to generate the stubs.
The built stub artifact will be stored in your local maven repository.
The plugin will also create a test class that extends the base test class we created earlier containing the necessary setup to run your tests.

The next step is to add the Spring Cloud Contract Stub Runner dependency to the consumer and annotate your test class with `@AutoConfigureStubRunner`.
By annotating your class with `@AutoConfigureStubRunner` and providing the groupId, artifactId and port on which the stub will run to it, your test class is configured to use the producer's generated stub.

Interested to try out this Spring Cloud Contract workshop?
You can find the Github repo [here](https://github.com/faros/spring-cloud-contract/)!  

## Got triggered?

All talks were recorded by the Spring IO team. You can view them [here](https://www.youtube.com/channel/UCLMPXsvSrhNPN3i9h-u8PYg).

****
