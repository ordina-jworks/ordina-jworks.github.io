<!DOCTYPE html>
<!--suppress ALL -->
<html>
<head>
        <meta charset="utf-8">
    <meta name="description" content="We build innovative solutions with Java and JavaScript. To support this mission, we have several Competence Centers. From within those Competence Centers, we provide coaching to the employee and expert advice towards our customer. In order to keep in sync with the latest technologies and the latest trends, we frequently visit conferences around the globe.
">
    <meta name="keywords" content="Ordina,ORAJ,JWorks,Blog,Java,JavaScript,TypeScript,Angular,DevOps">
    <meta name="author" content="Ordina Belgium">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    

    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Search">
    <meta name="twitter:description" content="Ordina JWorks Tech Blog">
    <meta name="twitter:image" content="http://ordina-jworks.github.io/img/jworks/jworks-400x400.png"/>

    <!-- Facebook Open Graph -->
    <meta property="og:url" content="http://ordina-jworks.github.io/search/"/>
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Search"/>
    <meta property="og:description" content="Ordina JWorks Tech Blog"/>
    <meta property="og:image" content="http://ordina-jworks.github.io/img/jworks/jworks-400x400.png"/>

    
        
        <title>Search &mdash; Ordina JWorks Tech Blog</title>
        
    

        <!-- Styles -->
    <!-- Font awesome CSS -->
    <link href="/css/font-awesome.min.css" rel="stylesheet">
    <!-- Magnific Popup -->
    <link href="/css/magnific-popup.css" rel="stylesheet">
    <!-- Owl carousel -->
    <link href="/css/owl.carousel.css" rel="stylesheet">

    <!-- CSS for this page -->

    <!-- Syntax highlighting -->
    <link href="/css/syntax.css" rel="stylesheet">


    <!--[if lte IE 8]>
    <script src="/js/ie/html5shiv.js"></script><![endif]-->
    <link rel="stylesheet" href="/css/main.css"/>
    <!--[if lte IE 9]>
    <link rel="stylesheet" href="/css/ie9.css"/><![endif]-->
    <!--[if lte IE 8]>
    <link rel="stylesheet" href="/css/ie8.css"/><![endif]-->

    <!-- Custom CSS. Type your CSS code in custom.css file -->
    <link href="/css/custom.css" rel="stylesheet">

    <!-- Vertical timeline -->
	<link rel="stylesheet" href="/css/vertical-timeline/style.css"> <!-- Resource style -->

    <!-- Favicon -->
    <link rel="shortcut icon" href="#">

    <script src="https://use.typekit.net/gzh0mbm.js"></script>
    <script>
        try {
            Typekit.load({async: true});
        } catch (e) {
        }
    </script>
</head>

<body>
<div id="header-image"></div>


<!-- Page Wrapper -->
<div id="page-wrapper">

        <!-- Header -->
    <header id="header">
        <h1>
            <a href="/">
                JWorks Tech Blog
            </a>
        </h1>

        <nav>
            <a href="#menu">Menu</a>
            <form action="/search/" method="get" class="header-search search-form">
                <input type="text" id="search-box-header" name="query" placeholder="Search &hellip;">
            </form>
        </nav>
    </header>

    <!-- Menu -->
    <nav id="menu">
        <div class="inner">
            <h2>Menu</h2>
            <ul class="links">
                <li class="menu-item"><a href="/">Home</a></li>
                <li class="menu-item"><a href="/about">About Us</a></li>
                <li class="menu-item"><a href="/jobs">Jobs</a></li>
                <li class="menu-item"><a href="/launch-your-career">Career</a></li>
                <li class="menu-item"><a href="/search">Search</a></li>
                <li class="menu-item"><a href="/contact">Contact Us</a></li>
            </ul>
            <a href="#" class="close">Close</a>
        </div>
    </nav>

    <section id="banner">
        <header>
            <div class="inner">
                

                <h2>Search</h2>
                

                

                

            </div>
        </header>
    </section>

    <section id="wrapper">
        <div class="inner">
            <section class="wrapper spotlight style1 search-page">
    <div class="inner">
      <div class="content">
        <h2 class="major">
          Search for
          <form action="/search/" method="get" class="search-form">
            <input type="text" id="search-box" name="query">
          </form>
        </h2>
        <section id="search-results" class="features search-results">
        </section>
      </div>
    </div>
</section>

<script>
  window.store = {
    
      "microservices-2017-09-17-monitoring-your-microservices-with-micrometer-html": {
        "title": "Monitoring your microservices with Micrometer.io",
        "url": "/microservices/2017/09/17/monitoring-your-microservices-with-micrometer.html",
        "image": "/img/2017-09-17-monitoring-your-microservices-with-micrometer/post-image.jpg",
        "date": "17 Sep 2017",
        "category": "post, blog post, blog",
        "content": "When we want to instrument our application, we don‚Äôt want to worry about which monitoring system we want to use, now or in the future.Nor do we want to change a lot of code throughout our microservice because we need to change from system X to system Y.Meet Micrometer!So what is Micrometer you ask?Basically, it comes down to this:  Think SLF4J, but for metrics.Micrometer provides a simple facade over the instrumentation clients for the most popular monitoring systems.It allows you to instrument your code with dimensional metrics with a vendor-neutral interface and decide on the monitoring system as a last step.Using this interface, we can support multiple monitoring systems and switch easily to an other system with little to no hassle.It already contains built-in support for Prometheus, Netflix Atlas, and Datadog, while InfluxDB, statsd, and Graphite are on their way!Using Micrometer in your applicationStarting with Spring Boot 2, more specifically since milestone M4, Micrometer becomes the defacto instrumentation library that will be powering the delivery of application metrics from Spring.Luckily for us, they also backported this functionality to Spring Boot 1.x through an additional library dependency!Just add the micrometer-spring-legacy module together with the additional monitoring system module, and you‚Äôre good to go!In Gradle:compile 'io.micrometer:micrometer-spring-legacy:latest.release'Or in Maven:&lt;dependency&gt;  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;  &lt;artifactId&gt;micrometer-spring-legacy&lt;/artifactId&gt;  &lt;version&gt;${micrometer.version}&lt;/version&gt;&lt;/dependency&gt;Creating metricsThere are a couple of ways to create meters.We will cover all different types, when to use them, and furthermore how to implement them.Dimensions/TagsA meter is uniquely identified by its name and dimensions (also called tags).Dimensions are a way of adding dimensions to metrics, so they can be sliced, diced, aggregated and compared.For example, we have a meter named http.requests with a tag uri.With this meter we could see the overall amount of HTTP requests, but also have the option to drill down and see the amount of HTTP requests for a specific URI.CountersCounters are a cumulative metric that represents a single numerical value that only ever goes up.They are typically used to count requests served, tasks completed, errors occurred, etc.Counters should not be used to expose current counts of items whose number can also go down, gauges are a better fit for this use case.        MeterRegistry registry = ...Counter counter = registry.counter(\"received.messages\");    counter.increment();GaugesA gauge is a metric that represents a single numerical value that can arbitrarily go up and down.Gauges are typically used for measured values like current memory usage, but also ‚Äúcounts‚Äù that can go up and down, like the number of messages in a queue.        MeterRegistry registry = ...AtomicInteger currentHttpRequests = registry.gauge(\"current.http.requests\", new AtomicInteger(0));Queue&lt;Message&gt; receivedMessages = registry.gauge(\"unprocessed.messages\", new ConcurrentLinkedQueue&lt;&gt;(), ConcurrentLinkedQueue::size);Instead of returning a gauge, the gauge method will rather return the thing that is being observed.This allows us to have quick one liners that both create the object to be observed and set up metrics around it.TimersTimers measure both the rate that a particular piece of code is called and the distribution of its duration.They do not record the duration until the task is complete.These are useful for measuring short-duration latencies and the frequency of such events.long startTime = System.nanoTime();MeterRegistry registry = ...Timer timer = registry.timer(\"timer\");    // this will record how long it took us to get a registry and create a new timertimer.record(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);Or we could just annotate a method with @Timed and let Micrometer do the rest for us@Timedpublic void doSomethingWhichShouldBeFastButIsActuallyReallySlow() {}Long task timersThe long task timer is a special type of timer that lets you measure time while an event being measured is still running.To time a long running task we use the same @Timed annotation, but we set the property longTask to true.@Timed(longTask = true)@Scheduledpublic void doSomethingWhichCanTakeALoooooongTime() {}It is up to the application framework to make something happen with @Timed.In case it isn‚Äôt able to do that, you can still use the long task timer.MeterRegistry registry = ...LongTaskTimer looooongTimer = registry.more().longTaskTimer(\"sync\");private void doSomethingWhichCanTakeALoooooongTime() {    looooongTimer.record(() =&gt; {        // actually do some synchronization which takes a loooooong time    });}Distribution summariesA distribution summary is used to track the distribution of events.It is similar to a timer but more general in that the size does not have to be a period of time.Usually it is used to sample observations of things like response sizes.MeterRegistry registry = ...DistributionSummary summary = registry.summary(\"response.size\");Summary statisticsMicrometer provides quantile statistics computed at instrumentation time and histograms for use in calculating quantile statistics at query time for monitoring systems that support this.QuantilesQuantiles are cutpoints dividing the range of a probability distribution into contiguous intervals with equal probabilities, or dividing the observations in a sample in the same way.Timers and distribution summaries can be enriched with quantiles computed in your app prior to shipping to a monitoring backend.Depending on the size of your deployments, computing quantiles at instrumentation time may or may not be useful.It is not possible to aggregate quantiles across a cluster.Four quantile algorithms are provided out of the box with different tradeoffs:  WindowSketchQuantiles - The importance of an observation is decayed as it ages.This is the most computationally costly algorithm.  Frugal2UQuantiles - Successive approximation algorithm that converges towards the true quantile with enough observations.This is the least costly algorithm, but exhibits a higher error ratio in early observations.  CKMSQuantiles - Lets you trade computational complexity for error ratio on a per-quantile basis.Often, it is desirable for higher quantiles to have a lower error ratio (e.g. 0.99 at 1% error vs. 0.5 at 5% error).This algorithm is still more computationally expensive than Frugal.  GKQuantiles - Lets you trade computational complexity for error ratio across all quantiles.This is used inside of WindowSketchQuantiles.HistogramsA histogram measures the statistical distribution of values in a stream of data.It samples observations, like HTTP request durations or database transaction durations, and counts them in buckets.They can be used to compute quantiles or other summary statistics like min, max, average or median.Because histograms buckets are exposed as individual counters to the monitoring backend, it is possible to aggregate observations across a distributed system and compute summary statistics like quantiles for an entire cluster.Naturally, the error rate of the computed summary statistic will be higher because of the lossy nature of putting data in buckets.BindersBinders define a collection of meters and are used to encapsulate best practices for monitoring certain types of objects or a part of the application‚Äôs environment.For example, the JvmThreadMetrics binder which gauges thread peak, number of daemon threads, and live threads.Micrometer ships with a basic set of binders:  JVM and system monitoring  Cache monitoring  Executor and ExecutorService monitoring  Logback monitoring"
      },
    
      "conference-2017-06-21-devoxx-pl-html": {
        "title": "Devoxx Poland 2017",
        "url": "/conference/2017/06/21/Devoxx-pl.html",
        "image": "/img/2017-devoxx-pl/devoxx-poland.jpg",
        "date": "12 Jul 2017",
        "category": "post, blog post, blog",
        "content": "Devoxx Poland 2017Krakow in the ICE Krakow Congress Centre.We started off day 1 with the keynote in the absolutely, phenomenal main room:Table Of Contents  Keynote: Speed without Discipline: a Recipe for Disaster  Feature Branches And Toggles In A Post-GitHub World  A reasonable overview of Java 9 and how you could think of it  The Language of ActorsKeynote: Speed without Discipline: a Recipe for Disaster (Venkat Subramaniam)Venkat kicked off the keynote, talking about a paradigm shift, that is happening right now in software development:In the nineties, everybody was doing imperative programming, using objects to implement functionality.Nowadays, this style of software development is shifting towards a more declarative approach.In imperative programming, developers focus on both what they want to do and how they want to do it. In declarative programming on the other hand, developers focus on what they want to do and use tools and libraries to facilitate their goal.Venkat went on to state that programming in a functional style is declarative, but that not all declarative code is functional.Functional style = declarative style + higher order functionslet names = [\"Dieter\", \"Tom\", \"Andreas\", \"Ken\", \"Yannick\", \"Tim\", \"Bart\"];let count = 0;for(const name of names){  if(name.length === 4)    count++;}console.log(count);console.log(names.filter(name =&gt; name.length === 4).size);Declarative vs ImperativeVenkat told the audience that he doesn‚Äôt like driving cars.He compared driving a stick shift to imperative programming.His goal is going from point A to point B and he does not want to be involved in changing the gears (Manipulating the DOM).A car with an automatic drive train, is a step in the right direction, but still requires too much focus on how he wants to reach his destination (Using a library like JQuery).Using the auto pilot functionality in certain modern cars is another step in the correct direction, but what he really wants is a car with a dedicated driver, like Uber or Lyft offer (Abstracting the DOM and using frameworks like Angular).In this comparison the ride-sharing service is the declarative approach.Testing  I automate my tests, not because I have a lot of time, but because I don‚Äôt.After an introduction to declarative programming, Venkat switched to the topic of testing.To really be agile, we need to be confident that implementing new features won‚Äôt cause failure.We can achieve this confidence by automating our tests and making sure they are repeatable.If we are really confident, we might even be able to ship software, without running the application.Writing software without writing tests is described as JDD: Jesus Driven Development. Pray that it works.Obviously, TDD (Test Driven Development) makes a lot more sense.Software development: a profession where people get paid to write poor quality code and get paid more later to cleanup the mess.&mdash; Venkat Subramaniam (@venkat_s) 27 september 2015Testing vs verificationTesting and verification are two different things.Verification is the process that checks if the code (still) works.This is not something anyone should do manually, verification is exactly what should be automated.Testing is the process that checks if a feature is correctly implemented.Code represents what you have typed, not what you might have wanted the system to do.It is the act of gaining insight in the application and the business.This could well be a manual task.Unfortunately, most of our industry has neglected this important difference.The maturity of software verification can be categorized in three maturity stages. Projects without verification automation are in denial, they are building up an increasing technical debt.The second stage describes projects that have some automated verification on the UI level. Venkat describes tools using WebDriver for UI level verification as a pathway to hell automation.This test method can be represented in the ice-cream cone anti-pattern. For projects with the right level of automation, the pyramid pattern is a good representation.The last maturity stage contains these projects with the right measure of automated verification.DisciplineVenkat drew a comparison with 1820, where patients died regularly within three weeks after being operating.Doctors (Joseph Lister, Louis Pasteur) started cleaning their tools after surgery and noticed a positive trend in survival.Analogous to the doctors back then, we need to discipline ourselves in software engineering.This discipline is needed to keep up to speed and to stay agile, so that teams can react rapidly to customer requests. To build up this discipline, automated verification can be seen as the software equivalent of exercising.  We‚Äôre practicing a beautiful craft, let‚Äôs go turn it into a wonderful profession. Focusing on quality and creative things.Feature Branches And Toggles In A Post-GitHub World (Sam Newman)Sam told us about his experience at a project where the team was having trouble merging branches.The release branch for the next release was called R3, but for a large refactoring, branch R4 was created.Afterwards, he described merging the branches as a car crash.They even needed to introduce a dedicated R3-R4 merge bug fix team.Later on, they set up Continuous Integration in order to prevent the merging issues.The code, pushed by the developers, would get automatically validated by the CI setup.The problem with the R3-R4 release was that validation was done only for a branch and not on the integrated branches.  The integration should be validated every day and when the build breaks, fix it!For unfinished work, we can wait until it is ready before checking in.This exposes us to the risk of losing work when it‚Äôs only on the developer‚Äôs computer.Feature BranchAn alternative would be to create a feature branch, which brings us back to the problem of merging branches.  Pain of merge = fn(size_of_merge, duration_since_last_merge)Merging branches can be a difficult task and might lead to a commit race, offloading the effort to a colleague.Trunk-based developmentA third option would be to ‚Äòcheck in anyway‚Äô, called trunk-based development.Every commit integrates to the trunk and developers should integrate their local changes daily.Small changes and integrating often makes it easier to merge new code.New half-finished features can be hidden with feature toggles.These toggles can be managed using flags or configurations (eg, in Zookeeper, Consul, ‚Ä¶).  A flag should be set and evaluated in as few places as possible, preferably only once each.Flags should be removed when the new implementation is done.More info: Trunk-based developmentChanges to an existing functionality can be done by providing an abstraction above the existing functionality.The new functionality can then be developed for the abstraction and when it is done, changed to the new implementation.Branch by abstraction has the side-benefit that it can be used for A/B and canary releasing.The Continuous Delivery book tells us to treat every check-in as a possible release candidate.Developers start with the assumption that it is worthy, the CI tool decides whether it truly is.Deploy frequently with small changes, making it easier to rollback and lowering the risk of running into problems.GitAnd then there was Git, developed by Linus Torvalds with the goal to merge a patch in less than three seconds.In Git, branches are much more lightweight and every local repository contains the full source history.In 2008, GitHub was founded and introduced pull requests.If you wanted to contribute to open source projects before pull requests you had to:  Develop it locally  Generate a patch file  Mail it over to the project ownersThis feature contributed to GitHub‚Äôs success as three years later in 2011, they passed SourceForge and Google Code in popularity.Sam made the remark that pull requests use branches, which might bring problems. On top of that GitFlow was introduced.Because GitFlow introduces even more branches, it is in controversy with fast deployment and small changes cycle.With tools like Split and LaunchDarkly, GitFlow is not needed, if merged frequently.The conclusion was that experimental and release branches, that might even never get merged, still have their uses.The pull request mechanism works well in open source projects.Except for experiments, releases and pull requests, Sam recommends to prevent branches and to keep batch sizes small, integrate often and ship often.A reasonable overview of Java 9 and how you could think of it (Oleg ≈†elajev - Slides)Since Java 9 does not seem to have a codename and Java 10 is called Project Valhalla, Oleg proposed codename Java 9 the Fury Road, a Mad Max reference.  Java 9 Release date: September 21st 2017JShellJShell is the new REPL (Read-Eval-Print Loop) for Java.It can be used to run commands and get results immediately.For user-friendliness, the semicolons can be omitted after the instructions in JShell.Example command:jshell&gt; List.of(1).getClass()$1 ==&gt; class java.util.ImmutableCollections$List1OptionalsSeveral improvements will be added to the Optional class.Optionals can be turned into streams and have filter, flatMap and map methods.For eager evaluation these functional methods can be applied directly to the Optional.jshell&gt; Optional.of(1).map(x-&gt;x*3)$2 ==&gt; Optional[3]When using stream() in front of the functional methods a ReferencePipeline is returned.This can be used for lazy evaluation.jshell&gt; Optional.of(1).stream().map(x-&gt;x*3)$3 ==&gt; java.util.stream.ReferencePipelineAn or() method will be added to chain a supplier to empty Optionals.jshell&gt; Optional.empty().or(()-&gt;Optional.of(\"Devoxx rocks!\"))$4 ==&gt; Optional[Devoxx rocks!]StreamsTwo new methods will be added to the Stream interface, dropWhile and takeWhile.For ordered streams, these methods drop or take elements while the predicate is true.In unordered streams, dropWhile returns a subset of elements starting from the first predicate match, takeWhile returns a subset of elements matching the predicate.Stream&lt;T&gt; dropWhile‚Äã(Predicate&lt;? super T&gt; predicate)Stream&lt;T&gt; takeWhile‚Äã(Predicate&lt;? super T&gt; predicate)jshell&gt; IntStream.range(1,10).takeWhile(x-&gt; x&lt;5).boxed().collect(Collectors.toList())$5 ==&gt; [1, 2, 3, 4]jshell&gt; IntStream.range(1,10).dropWhile(x-&gt; x&lt;5).boxed().collect(Collectors.toList())$6 ==&gt; [5, 6, 7, 8, 9]ConcurrencyCompletableFuture will be extended with a copy.The copied CompletableFuture is a defensive copy and completing it doesn‚Äôt complete the original CompletableFuture.jshell&gt; CompletableFuture.runAsync(()-&gt;{while(true){}})$7 ==&gt; java.util.concurrent.CompletableFuture[Not completed]jshell&gt; $7.copy()$8 ==&gt; java.util.concurrent.CompletableFuture[Not completed]jshell&gt; $8.cancel(true)$9 ==&gt; truejshell&gt; $8$8 ==&gt; java.util.concurrent.CompletableFuture[Completed exceptionally]jshell&gt; $7$7 ==&gt; java.util.concurrent.CompletableFuture[Not completed, 1 dependents]A new ProcessHandle interface will be added, it can be used to get information and control processes.Bits and piecesThe underscore will become a keyword, so assigning a value to _ does not work.This is probably a feature for the future where _ will be used for matching arguments of any type.Assigning a value to __ will keep on working.In Java 8, default methods were added to interfaces, in 9 they can be private.Property files will support UTF-8 and there is already Java 9 support in several IDEs.There will be several changes to improve String performance, for example using a more space-efficient internal representation for Strings.Javadoc will get an improved search, HTML5 compliance and more info on the module where the class or interface comes from.The use of agents will be more flexible, a process can attach an agent to itself and a JAR can contain multiple agents.ModulesThe Java Platform Modules System (JPMS) allows modularization for Java applications.A module can define dependent modules with requires, to provide an API the exports key word is used.To give access to everyone the opens keyword can be used.There is a method getAccessable().You should be aware though about using it for determining if a module is usable or not since it actually just returns the value of setAccessable(), a toggle that you have to set yourself.Actually it just returns the value of setAccessable(), a toggle that you have to set yourself.To make a smooth transition to the JPMS, any JAR on the classpath will become an automatic module.By default, --illegal-access=permit is the default mode for JDK 9, allowing modules access to all automatic modules.As a migration strategy, Oleg proposes to wait for dependencies to modularize before modularizing yourself.Otherwise you might need to modularize twice to align with the dependencies.Java 9 with Maven is complicated, many plugins need to be upgraded and a lot of functionality is not yet fully integrated with JPMS.Gradle releases fixes more often and currently supports more features.A multi-release JAR containing multiple versions for the same file, in the same JAR, is a new feature that should be used with caution.Garbage collectionThe G1 Garbage Collector (G1GC) will become the default in Java 9.Previous Garbage Collectors were not as scalable nor predictable.The G1GC promises a more scalable and more predictable system with few modification options.By default, a quarter of the physical ram will be allocated to the heap, unless the size is specified with the -mx flag.Due to its new heap division system, it might run into problems with large chunks of data.It is recommended to feed streams of data directly to parsers without first capturing it in a byte array, this also applies to JSON parsing and database operations.Another improvement is using immutable objects wherever possible.Using a StringBuilder instead of concatenating Strings will reduce heap usage. For more info, Oleg referred to a talk on Moving to G1GC by Kirk PepperdineHTTP/2 ClientThe JDK 9 will contain an incubator package with a HTTP/2 client with a fluent API.The modules in the incubator package are non-final APIs that can be finalized or removed in future releases.HttpClient.newHttpClient().send(        newBuilder(URI.create(\"https://google.com\"))            .GET()            .build(),        HttpResponse.BodyHandler.asString())    .body();)Oleg concluded by recommending the audience not to touch multi-release JAR, jlink and Unsafe, unless you are 100% sure what you are doing.For now, he recommends to upgrade your IDE and tools and upgrade Spring to version 5.0. Then add the --illegal-access=warn startup option and fix the easy fixable warnings and then wait a year or more until the classpath and the libraries, you depend on, are upgraded.Yannick, a colleague at Ordina did a nice presentation on Java 9: A first look at Java 9 by Yannick De Turck.Presentation and sources are available at GitHub.The Language of Actors (Vaughn Vernon - Slides)Vaughn started his talk by introducing Rear Admiral Grace Hopper to the audience.In the American Navy, she was a Computer Scientist and wrote software for a long time.She was really into not wasting cycles and emphasised on not wasting nanoseconds.Then Vaughn introduced Donald Knuth, another legend in Computer Science.Knuth is known from the quote Premature optimization is the root of all evil.But that is not exactly what he said, the full quote says:  We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.Another quote was shared of Donald Knuth:  People who are more than casually interested in computers should have at least some idea of what the underlying hardware is like. Otherwise the programs they write will be pretty weird.To further build his point, Vaughn told about a project that was written in Cobol.The code was across 5 diskettes thus user interaction was needed to run the application.To improve the usability of the application, it was rewritten in C, allowing the software to fit on just one diskette.With this introduction, Vaughn wanted to emphasize how hard it is to optimize software for resource usage.Threading is hardIn 1973, academics discovered the Actor Model. 13 year later in 1986 Joe Armstrong rediscovered the approach.Armstrong designed and implemented a programming language on this model, ErlangIn 2008, Jonas Bon√©r came up with Akka for the Java Virtual Machine and in 2011 Jos√© Valim came up with another Actor based language called Elixir.Because the Actor Model is Message Driven, it inherently is Reactive.Now is the time for the Actor Model, with the decreasing expense of memory, network and chips.Processors are having a lot of cores these days, Intel Xeon units go up to 88 cores, Intel Xeon Phi can have more then 200 coprocessors.The actor model allows us to embrace latency. If we design for latency, it will not have a blocking impact on the design.We are not at Google scale, why use actors?With the Actor Model you can do more with less. The total number of nodes can be reduced to just a few, several million actors per machine is not a problem.The actor model uses the essence of Domain Driven Design (DDD), the bounded context and ubiquitous language.DDD is excellent way to make complexity surrender, by knowledge crunching.Actors help us reason better by having less moving parts.This allows us to focus on business aspects, instead of the architecture around it.How to do DDD in projects:  Talk with customer (iterate)  Write some scenarios (iterate)  Strategic Event Storming (iterate)  Tactical Event Storming (iterate)  Implement acceptance tests and model (iterate)This concludes our recap of this amazing edition of Devoxx Poland."
      },
    
      "spring-2017-07-11-springio17-summary-html": {
        "title": "Spring I/O 2017 Recap",
        "url": "/spring/2017/07/11/SpringIO17-Summary.html",
        "image": "/img/springio2017.jpg",
        "date": "11 Jul 2017",
        "category": "post, blog post, blog",
        "content": "  On the 18th and 19th of May, we had another great edition of Spring I/O, brought to us by organizer Sergi Almar.In this blog post, we will go over some of our favourite sessions of the conference.  DISCLAIMER: we could not include ALL the talks from Spring IO in this blogpost. We provide an extensive summary of our favorite talks and created a curated list of all the talks and resources at the bottom of this post.  Table Of Contents      Keynote: The Only Constant Is Change    Bootiful Database-centric Applications with jOOQ    Google Spanner    Easily secure and add Identity Management to your Spring(Boot) applications with Keycloak    Spring Cloud Streams (Data Services)    The Road To Serverless: Spring Cloud Function    Reactive Spring Data    The Future Of Event Driven Microservices With Spring Cloud Stream    New in Spring 5: Functional Web Framework    Spring Auto REST Docs    References  1. Keynote: The Only Constant Is Changeby Andy Wilkinson and St√©phane NicollObviously, the keynote presentation was filled with announcements, interesting tidbits and two great presenters.  The biggest topic revolved around how Spring has always tried to enable developers to adapt their applications rapidly.This capacity of adapting to change increased dramatically when Spring Boot was released, which explains part of its success.They also reported the release of Spring Boot 2.0.0 M1.This release was announced very shortly after Spring Framework 5.0 went RC1 on the 8th of May.The keynote can be found here and the demo code here.2. Bootiful Database-centric applications with jOOQby Michael SimonsAs a personal fan of @rotnroll666, I went to see his talk, fully expecting to be impressed by the level of quality and code.As always, Michael delivered: he made me realize there are still plenty of PL/SQL developers out there in the enterprise landscape, who hold on to their code like kids to their favorite stuffed animal before bedtime.Luckily for us, someone created a library called jOOQ:  jOOQ generates Java code from your database and lets you build type safe SQL queries through its fluent APIThere are many use cases where jOOQ can prove useful, but the example that Michael used described the enterprise environment at his current employer:He was working for a big utility company in Germany, who developed applications on power and gas usage consisting of lots of time-series data.Almost all of the data was stored in SQL databases with a big layer of PL/SQL on top.They also developed some desktop GIS applications using Oracle Spatial to visualize the data.So the question they asked themselves at a certain moment:  Should we approach all of our data using plain PL/SQL? Should we use an ORM tool like Hibernate? Or can we use something in between?Of course, as with any good question, the answer is:  It depends. There is no silver bulletThere are many options in the Java space to approach this problem:  Plain JDBC  Using a JDBCTemplate  JPA with JPQL and / or Criteria APIMichael‚Äôs team solved most of their problems using Hibernate, and while that comes with several advantages, it doesn‚Äôt mean you have to use it for everything.One of the improvements they made was using Hibernate combined with jOOQ:  Use Hibernate for the regular database queries and day-to-day manipulations of your database  Use jOOQ for your complex queries and database migrationsTo briefly summarize what jOOQ can do for you:  jOOQ is SQL-centric which means jOOQ infers information from the actual database, not from the ORM model  It exposes a typesafe meta-model generated from your SQL  The Query Builder framework uses a very legible DSL (in a much more concise way than the Criteria API)  It generates a Java-based schema (using Maven or Gradle)  It can reverse-engineer an existing DB and generate SQL  Spring provides integration with Spring Boot through the spring-boot-starter-jooq dependencyAn ideal scenario to use jOOQ would be to:  Run your database migration with Flyway or Liquibase first  Run the code generator to generate the Java DSL context (this happens in the Maven generate-sources lifecycle phase)  Use the DSL context to write your typesafe queries, for example:BookRecord book = create.selectFrom(BOOK).where(BOOK.ID.eq(1)).fetchOne();For more information about jOOQ, you can check out their website.  Question: What‚Äôs the difference between the jOOQ Query API and using JPA Criteria API with the Hibernate ModelGen, which is also typesafe?  It resembles the native SQL much better  jOOQ provides standardization since it performs SQL transformations that work for any SQL dialect  It should make it easier to migrate existing PL/SQL applications  There is a much more extensive collection of SQL functions and possibilities  jOOQ provides POJO mappers which are also generated from the code generator  As much as you hate them, it supports calling Stored Procedures!The code from Michael‚Äôs talk can be found on Github.3. Google Spannerby Robert KubisGoogle Spanner is a globally distributed relational database service that provides ACID transactions and SQL semantics, without giving up horizontal scaling and high availability.When building cloud applications, you are no longer forced to choose between traditional databases that guarantee transactional consistency, or NoSQL databases that offer simple, horizontal scaling and data distribution.Cloud Spanner offers both of these critical capabilities in a single, fully managed service.With Spanner, your database can scale up and down as needed, and you only pay for the amount you use.Spanner keeps application development simple by supporting standard tools and languages in a familiar relational database environment.It supports distributed transactions, schemas and DDL statements, SQL queries and JDBC drivers and offers client libraries for the most popular languages, including Java, Go, Python and Node.js.As a managed service, Cloud Spanner provides key benefits to DBAs:  Focus on your application logic instead of spending valuable time managing hardware and software.  Scale out your RDBMS solutions without complex sharding or clustering.  Gain horizontal scaling without migration from relational to NoSQL databases  Maintain high availability and protect against disaster without needing to engineer a complex replication and failover infrastructure.  Gain integrated security with data-layer encryption, identity and access management and audit logging  4. Easily secure and add Identity Management to your Spring(Boot) applications with Keycloakby S√©bastien BlancI must say, this was one of the funniest talks of the conference.S√©bastien knows how to entertain the crowd and he kicked off with a great quote which, of course, I immediately stole and tweeted:Similar to a quite from @sebi2706 last week at #springio17 : forget about companies, it&#39;s all about community and code! üôå&mdash; Dieter Hubau (@dhubau) May 26, 2017First of all, let‚Äôs forget that Keycloak was created by Redhat and that it is written in Java EE.The following aspects of Keycloak are more important:  It‚Äôs Open Source)  Redhat provides support through their commercial fork called Redhat SSO)  Great Spring Boot Integration through the use of a Spring Boot Starter)  Seamless Spring Security Integration  Supports OAuth 2.0, SAML 2.0, OpenID Connect  Integration with Active Directory, LDAP and even Kerberos (start drooling enterprise users!)It‚Äôs actually quite easy to setup Keycloak:  Download the Keycloak standalone server  Extract and run it  Start the server and create an admin user  Create a new realm  Create a new application  Add roles to your application  Create a user to authenticate with  Create a Spring Boot application at The Happiest Place On Earth) and include the Keycloak starter  Add the Keycloak properties to your application.yml:          server URL      realm      resource name of your application      security constraints for your users        Run!  There are many additional features for power users:  Automatic registration of applications should be possible using a one-time token (coming soon?)  Centralized User Management  CORS support for Single Page Applications  Social Login Integration  Registration and Forgot Password functionality, all out-of-the-box, configurable at runtime  UI Customization of all pages is possible through theming (start drooling designers!)All in all, the setup and demo went very smooth and I genuinely feel this product is about to become very popular, partly because of the Spring Boot integration, but also because it just seems very solid and user-friendly.There might be a dedicated blogpost coming soon about Keycloak, so stay tuned and check our blog regularly or subscribe to our RSS feed!5. Spring Cloud Streams (Data Services)by Michael MinellaMichael gave a summary about all the new projects in the Spring ecosystem that process data and / or messages very well.He explained that there are lots of big data frameworks out there (Hadoop, Spark, ‚Ä¶), which can handle BIG amounts of data very well.However, they are usually too bulky / difficult / inappropriate for handling smaller volumes of data.Also, for quickly setting up something like Hadoop or Spark, the learning curve is too high and the effort doesn‚Äôt justify the benefits.Solution: data microservices  Developed and tested in isolation, also easier to test  Independently scalable depending on data processing load  Familiar development model, just like regular cloud-native Spring microservices  Easier to govern for Ops  So the need for data / app integration / composition arises  Which means the need for orchestration and operational coverage arises (lots of plumbing required)Spring Cloud Stream  Streams are thin wrappers around Spring integration  Supported binder for integration between services: Kafka, RabbitMQ, ‚Ä¶  Source, Processor, Sink model is easy to comprehendSpring Cloud Task  Tasks are finite microservices, built around Spring Batch  ‚ÄúMicroservices that end‚Äù  Contain Task repository which tracks run/start/end of the tasks  Has Spring Batch integration (partition steps using workers)  Has Spring Cloud Stream integration (eg. launch Tasks via Streams)  Simple annotation @EnableTask  Use cases: batch jobs, scheduled one-off processes, ETL processing, data scienceSpring Cloud Data Flow  AKA the new and Improved Spring XD  Data flow orchestrator  Use a shell or the UI which goes over REST endpoints  Has custom DSL  All the components are regular Spring Boot apps (Data Flow server, Shell, ‚Ä¶)  Data Flow server has datastore for task repository, batch repository, configuration, ‚Ä¶  Data Flow server does not do any of the actual work  We will be publishing a fun blogpost about Spring Cloud Streams soon, so stay tuned or subscribe to the RSS feed!6. The Road to Serverless: Spring Cloud Functionby Dr. Dave SyerFaaSIn recent years we‚Äôve seen the emergence and evolution of following cloud abstraction layers in order of abstraction level:  Virtual Machines (IaaS)  Containers (CaaS)  Applications (PaaS)  Functions (FaaS)The Goal of each of these is raising the value line; in other words, the purpose of each of these is to abstract away various concerns that are of no business value(e.g. setting up and maintaining infrastructure, the environment the code has to run in‚Ä¶).The latest and most extreme level of these is FaaS (or ‚Äòserverless‚Äô).Basically all the programmer should do in a FaaS environment is write a Function and hand it over to the platform.The platform takes care of:  Making sure the function is executed on demand  Deploying and undeploying (often on demand!)  Scaling up the amount of instances quickly and in parallel if the need arises (is easier with functions since they are simpler in nature than applications)  Managing integrations with other systems  The naming scheme of ‚Äúserverless‚Äù is unfortunate; of course you‚Äôre gonna have servers, you just don‚Äôt care about themProblem  By now there are a lot of FaaS solutions out there; AWS Lambda, Google Cloud Function, Azure Function, IBM Openwhisk, Fission, Kubeless, ‚Ä¶  Deploying and programming functions is different for each platform because you have to use their native APIs and they have their own platform to deploy on.  Running and testing these functions locallyEnter Spring Cloud FunctionThe purpose of the new project called Spring Cloud Function is to solve these problems by:  Keeping all advantages of serverless/functions, but with all the possibilities that Spring offers (DI, integration, autoconfig, build tools)  Providing a low entry level for Spring devs to jump on the FaaS model  Providing a low entry level for FaaS people without having knowledge of Spring  Making it possible to run the same business logic as web endpoint, stream processor or a task  Introduce an uniform programming model across providers and able to run standalone (not on a IAAS or PAAS).  Support a reactive programming model (Flux, Mono) as wellThe project will try to achieve this by:  Supporting the familiar Java 8 Function types:        @SpringBootApplication    public class Application {        @Bean        public Function&lt;String, String&gt; uppercase(){           return (value) -&gt; value.toUpperCase();        }        public static void main(String[] args) {            SpringApplication.run(Application.class, args);        }    }          As well as the Reactive Types Flux and Mono :        ...    public Function&lt;Flux&lt;String&gt;, Flux&lt;String&gt;&gt; uppercase() {        return flux -&gt; flux.map(String::toUpperCase);    }    ...          Building/deploying the Function as a web endpoint, a task or a stream  can be done by merely altering dependencies, for example:          Deploying the function as a web endpoint can be done by adding the dependency spring-cloud-function-web        In a similar fashion, it will also possible to build for a platform like AWSConclusionIn a way the goal of FaaS is similar to the Spring framework; allowing the developer (or the IT department) to focus on writing code that has real value.The purpose of FaaS is to help us with infrastructure, scalability etc for the functions we write while Spring cloud function will allow us to write and deploy these functions in an (almost) platform agnostic fashion.At the same time, it will enable the programmer to leverage the Spring Framework with it‚Äôs various features that helps the programmer to focus even more on his main purpose; programming things that deliver real value: business code!PS: Kenny Bastani has just published a VERY detailed blogpost about Spring Cloud Function on AWS Lambda.It‚Äôs a follow-up of his earlier blogpost about Event-driven Microservices using CQRS and Serverless.I would highly recommend his blog!Spring Break  7. Reactive Spring Databy Christophe StroblBiggest changes of Spring Data Kay M3  Java 8 baseline  ID classes don‚Äôt need to be Serializable anymore  breaking change: No more null in the repositories (arguments or return values)  breaking change: More consistent naming (eg. findOne -&gt; findOneById)  Composable interfaces (separate Readable / Insertable and make custom repositories as composable interface as well)  Builder style Template APIs  Kotlin extensions are coming in M4Data Store specifics  MongoDB:          breaking change: MongoDB driver baseline to 3.x      Introduction of ReactiveMongoTemplate      Enhanced aggregation support      Collation support        Cassandra:          breaking change: Update to Cassandra 3.2      No reactive native driver ‚Äì&gt; mimicking reactive driver with thread pool (and blocking) underneath (with ReactiveCassandraTemplate)        Redis:          JRedis discontinued      Upgraded to Lettuce 5 (not GA yet though) supports native reactive driver        Gemfire:          Lucene index support      Off-heap, Redis Adapter, Security annotation config        Solr:          Upgrade to Solr 6        Spring Data REST:          CORS config mechanism      Improved headers      Improved media type support      8. The future of event driven microservices with Spring Cloud Streamby Kenny BastaniEvolutionMonolith applicationThere are some cultural problems with monoliths.One big application slows down the velocity of getting into production.Everyone has to use a shared deployment pipeline.For large code-bases it is harder for new engineers to get up to speed.The engineers that were there from the beginning, who designed the application, are busy explaining the history of the application to new engineers joining the project. These developers are creating change but might get blocked by DBA and Ops teams.Monolith organizationCentralized authority for operations, database and change management slows progress.These coordinated releases batch many changes together from different teams.Usually operations drives the runtime environment of applications because they take all operation responsibility including upgrades of virtual machines.The key problem is that everything is deployed at once or nothing at all.Move towards SOAWith Service-oriented Architecture the application is split up in components which can be deployed individually but now the key problem are the shared libraries.Releasing a change in an object that is not shared can be done separate, but still a problem for the shared ones.Now we arrived at MicroservicesThere are a lot of improvements but Microservices also adds the complexity of running a distributed system.Small, two pizza (5-8 members), teams organized around business capabilities with responsibility of running their own services.We gain independent deployability because each team produces and consumes rest APIs to communicate.Team also have more freedom to chose the best tool for the job they are facing.Microservices brings the challenge of eventual consistency, in a monolith you could rollback a transaction at the database level if something went wrong. Now eventual consistency is not guaranteed, inconsistency happens all the time. Rolling back transactions across multiple services is not easy.Is it a Monolith or Microservice?  If it takes longer than a day for an engineer to ramp up its probably a monolithIn a typical architecture the front-end teams integrate directly with the microservices.This is an anti-pattern in distributed systems. Consumers should not have to worry which instance of the replicated services they have to go to.You could use Spring Cloud, it allows you to centralize authentication with OAuth tokens and routing through an  API Gateway. The front-end does not need to be concerned with all these services, for them it looks like consuming a monolithic API.Splitting the monolithThe popular route from monolith to a microservice architecture is slicing off bits of functionality.This is hard in practice, splitting up a schema is usually the complex part.Refactoring out functionality and tables to new services can be hard because of foreign key constraints for example.Why we need event-driven microservicesThe problem with microservices is that there are no foreign key constraints between services.Furthermore, distributed transactions are brittle and distributed systems are hard.  You will drown in problems you didn‚Äôt know existed!Without event-driven microservices and an audit trail you will never know why something went wrong.This audit trail allows you to reason about what went wrong and roll back state.Rules for Event-driven microservicesA lot of these rules are from reference applications and Kenny‚Äôs work with Chris Richardson.Domain events are a first class citizen, every time you change some piece of data, domain events should be exchanged.These events can be used as an audit trail to determine why state changed in a system.Each domain event contains a subject with the project aggregate and a payload with immutable data.@Entity@EntityListeners(AuditionEntityListener.class)public class ProjectEvent {    @Id    @GeneratedValue(strategy = GenerationType.AUTO)    private Long eventId;        @Enumerated(EnumType.STRING)    private ProjectEventType type;        private Project entity;        private Map&lt;String, Object&gt; payload;To make this way of working accessible to the developers, hypermedia APIs need to expose links on the aggregates.A traversal list of command and a log of events that happened should be accessible.Command handlers trigger commands on aggregate and then the command is going to generate events.Every domain event applies a state transition to an aggregate.Event handlers are going to subscribe to an event and apply changes to an aggregate to change the state.In a graph representation, event handlers would be the nodes and events the edges.CQRS is used to create materialized views from streams of events.With CQRS you will have a command side and a query side.For example the commands might be written to an Apache Kafka event store.Then an event processor could be using Spring Cloud Stream to retrieve these events and create a data model.The data model is then written to a Data Store like MySQL, where the query side reads the data.An API gateway, like Spring Cloud Netflix Zuul, can be put in front so it looks for the consumer like a regular microservice.For deploying this application you can combine these components together or scale them independently.ServerlessChanges the pricing model for the execution on a cloud provider.With Serverless you are going to have a function in the cloud and you are going to pay for each execution.It is an event driven model, so if data is fed to for instance a AWS Lambda function this can invoke other functions in Python for example.Kenny concluded with a demo and recommended Dave Syer‚Äôs Talk on Spring Cloud Functions for more info about serverless.Spring break  9. New in Spring 5: Functional Web Frameworkby Arjen PoutsmaIn the keynote Andy Wilkinson and St√©phane Nicoll mentioned that the Spring framework and especially Spring Boot is all about providing choices to developers.The framework provides us with tools to tackle problems in multiple ways. In the light of this, starting from Spring 5 there will be a new functional alternative to handle incoming web requests.This new functional web library is an alternative to annotation driven approach that is broadly applied in current applications.  Arjen Poutsma states that some people are not happy with magic that happens behind the scenes when you use annotations like @RequestMapping or the newer @GetMappingThis was one of the reasons that made Spring develop this new library.In the next sections we show a quick introduction to what was shown at Spring IO about what this new framework has to offer.Handler function exampleThe following UserHandler class is the replacement of the Controller class that we would have annotated in the regular web framework. In this new functional style the way we handle requests is a bit different.We define functions that have a ServerRequest as parameter and we return a Mono with a ServerResponse.The request contains all the information we need.It contains the body of the request, pathvariables, request headers, ‚Ä¶So no more injecting pathvariables and body objects, we have everything we need in this ServerRequest.What we return is the ServerResponse in which we can easily put all the information we want to give back to the client.And Spring provides us with an easy builder to create such a response as it already did with the ResponseEntity builder.You can see that these new objects and builders provide us with an easy and declarative way to handle requests and create responses, without the ‚Äúmagic‚Äù that we used previously with the annotations.public class UserHandler {    public UserHandler(UserRepository repository) {        this.repository = repository;    }        public Mono&lt;ServerResponse&gt; getUser(ServerRequest request) {        int userId = Integer.valueOf(request.pathVariable(\"id\"));        Mono&lt;ServerResponse&gt; notFound = ServerResponse.notFound().build();        Mono&lt;User&gt; userMono = this.repository.getUser(personId);                return userMono                .flatMap(user -&gt; ServerResponse.ok().contentType(APPLICATION_JSON).body(fromObject(user)))                .switchIfEmpty(notFound);    }        public Mono&lt;ServerResponse&gt; createUser(ServerRequest request) {        Mono&lt;User&gt; user = request.bodyToMono(User.class);        return ServerResponse.ok().build(this.repository.saveUser(user));    }        public Mono&lt;ServerResponse&gt; listUsers(ServerRequest request) {        Flux&lt;User&gt; people = this.repository.allUsers();        return ServerResponse.ok().contentType(APPLICATION_JSON).body(users, User.class);    }}We have defined how we want to handle requests and how we translate it to a response.What we need next, is a way to say which requests will be handled by which handler function.In Spring MVC, this was done by adding an annotation that declared some parameters, for example, to couple a request path to a controller method.The functional web framework does this by creating RouterFunctions.This RouterFunction is a function that takes a ServerRequest and returns a Mono&lt;HandlerFunction&gt;. To choose which requests get handled by which HandlerFunction, Spring again provides us with some builder functions.That way we can easily bind the handlers we just created with a path as shown in the next code example.Router examplepublic RouterFunction&lt;ServerResponse&gt; routingFunction() {    PersonHandler handler = new PersonHandler(userRepository);        return nest(path(\"/person\"),            nest(accept(APPLICATION_JSON),                    route(GET(\"/{id}\"), handler::getPerson)                    .andRoute(method(HttpMethod.GET), handler::listPeople)            ).andRoute(POST(\"/\").and(contentType(APPLICATION_JSON)), handler::createPerson));}Creating a Tomcat serverNow that we have declared which routes are handled by which functions we have to let our server know this.In the next code example we show how to create a Tomcat server and how to bind the RouterFunction to our server.public void startTomcatServer() throws LifecycleException {    RouterFunction&lt;?&gt; route = routingFunction();    HttpHandler httpHandler = toHttpHandler(route);        Tomcat tomcatServer = new Tomcat();    tomcatServer.setHostname(HOST);    tomcatServer.setPort(PORT);    Context rootContext = tomcatServer.addContext(\"\", System.getProperty(\"java.io.tmpdir\"));    ServletHttpHandlerAdapter servlet = new ServletHttpHandlerAdapter(httpHandler);    Tomcat.addServlet(rootContext, \"httpHandlerServlet\", servlet);    rootContext.addServletMapping(\"/\", \"httpHandlerServlet\");    tomcatServer.start();}And then in the main method we only have to start our Tomcat server and we‚Äôre up and running:public static void main(String[] args) throws Exception {    Server server = new Server();    server.startTomcatServer();}ConclusionThe new functional web framework gives us a more declarative and functional way to create a server and handle web requests. In my opinion this code is a lot clearer because you have a direct link between routing and handling requests.This code may also be easer to test than the annotation driven web request handling because we don‚Äôt necessarily need to fire up our spring context to test the routing.We can just create a unit test for our RouterFunction and verify our routes are correct.What I do still wonder about is how this integrates with Spring security.How can we define which users can access which handler.Do we still do this with annotations or will we get a new way to do this as well?The Spring functional web framework is an interesting new development and we will be following it closely to see how we can use it in our new projects.Spring break  10. Spring Auto REST Docsby Florian BenzSpring Auto REST Docs is an extension on Spring REST Docs (our post on Spring REST Docs can be found here).This extension helps you to write even less code by including your Javadoc into the Spring REST Docs.For a more detailed overview on what is possible and how to start using this extension, please visit the official documentation here.Imagine you have the following method in your controller:@RequestMapping(\"users\")public Page&lt;ItemResponse&gt; searchItem(@RequestParam(\"page\") Integer page, @RequestParam(\"per_page\") Integer per_page) { ... }With the following POJO:public class User {    private String username;    private String firstName;    private String lastName;        ...}And the test that generates Spring REST Docs:this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk())\t.andDo(document(\"users\",     requestParameters(         parameterWithName(\"page\").description(\"The page to retrieve\"),         parameterWithName(\"per_page\").description(\"Entries per page\")     ),    responseFields(            fieldWithPath(\"username\").description(\"The user's unique database identifier.\"),            fieldWithPath(\"firstName\").description(\"The user's first name.\"),            fieldWithPath(\"lastName\").description(\"The user's last name.\"),    )));When using Spring Auto REST Docs, this could be replaced by adding Javadoc to the method in the controller:/** * @param page The page to retrieve * @param per_page Entries per page */@RequestMapping(\"users\")public Page&lt;ItemResponse&gt; searchItem(@RequestParam(\"page\") Integer page, @RequestParam(\"per_page\") Integer per_page) { ... }And adding Javadoc to the POJO fields:public class User {    /**    * The user's unique database identifier.    */    @NotBlank    private String username;        /**    * The user's first name.    */    @Size(max = 20)    private String firstName;        /**    * The user's last name.    */    @Size(max = 50)    private String lastName;        ...}And then removing the requestParameters and responseFields from the test:this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk());You notice that I added the annotations @NotBlank and @Size in the POJO, these annotations will also be reflected in the resulting documentation.You could also create your own annotations.Result:            Path      Type      Optional      Description                  username      String      false      The user‚Äôs unique database identifier.              firstName      String      true      The user‚Äôs first name. Size must be between 0 and 20 inclusive.              lastName      String      true      The user‚Äôs last name. Size must be between 0 and 50 inclusive.      Because the description of the POJO is now added on field level, it is guaranteed that this description will be the same everywhere this field is used, meaning less maintenance is needed.11. ReferencesYoutube PlaylistAll the talks of Spring IO 2017 are available on Youtube.Talks: Day One            Topic      Presenter(s)      Resource(s)                  KEYNOTE - The Only Constant Is Change      St√©phane Nicoll, Andy Wilkinson                    Reactor 3, the reactive foundation for Java 8 (and Spring 5)      Simon Basl√©                    Architecture Deep Dive in Spring Security      Joe Grandja      ¬†              The Spring ecosystem in 50 minutes      Jeroen Sterken                    Bootiful Development with Spring Boot and Angular [WORKSHOP]      Matt Raible                    Spring Boot at AliExpress      Juven Xu                    Database centric applications with Spring Boot and jOOQ      Michael Simons                    Testing for Unicorns      Alex Soto                    Front Ends for Back End Developers      Matt Raible                    The Beginner‚Äôs Guide To Spring Cloud      Ryan Baxter                    Microservices, but what about the UI      Marten Deinum      ¬†              Making the most of Spring boot: adapt to your environment! [WORKSHOP]      Arjan Jorritsma, Erwin Hoeckx      ¬†              New in Spring 5: Functional Web Framework      Arjen Poutsma                    Deep Learning with DeepLearning4J and Spring Boot      Artur Garcia, Dimas Cabr√©      ¬†              Easily secure and add Identity Management to your Spring(Boot) applications      S√©bastien Blanc      ¬†              The Future of Event-driven Microservices with Spring Cloud Stream      Kenny Bastani                    Container orchestration on Apache Mesos - DC/OS for Spring Boot devs      Johannes Unterstein      ¬†              Building Spring boot + Angular4 apps in minutes with JHipster      Deepu K Sasidharan      ¬†              Hands-on reactive applications with Spring Framework 5 [WORKSHOP]      Brian Clozel, Violeta Georgieva                    DDD Strategic Design with Spring Boot      Michael Pl√∂d                    Awesome Tools to Level Up Your Spring Cloud Architecture      Andreas Evers                    Surviving in a Microservices Team      Steve Pember            Talks: Day Two            Topic      Presenter(s)      Resource(s)                  Reactive Spring      Mark Heckler, Josh Long                    Spanner - a fully managed horizontally scalable relational database with ACID transactions that speaks SQL      Robert Kubis      ¬†              Reactive Spring UI‚Äôs for business      Risto Yrj√§n√§      ¬†              Hands-on reactive applications with Spring Framework 5 [WORKSHOP]      Brian Clozel, Violeta Georgieva                    Data Processing With Microservices      Michael T Minella                    Protection and Verification of Security Design Flaws      Marcus Pinto, Roberto Velasco      ¬†              Experiences from using discovery services in a microservice landscape      Magnus Larsson      ¬†              Harnessing the Power of Spark &amp; Cassandra within your Spring App      Steve Pember                    It‚Äôs a kind of magic: under the covers of Spring Boot      Andy Wilkinson, St√©phane Nicoll      ¬†              Introducing Spring Auto REST Docs      Florian Benz                    Leveraging Domain Events in your Spring Boot Microservices [WORKSHOP]      Michael Pl√∂d                    Functional web applications with Spring and Kotlin      S√©bastien Deleuze                    Setting up a scalable CI platform with jenkins, docker and rancher in 50 minutes      Wolfgang Brauneis, Rainer Burgstaller                      The Road to Serverless: Functions as Applications      Dave Syer                     TDD with Spring Boot - Testing the Harder Stuff      Sannidhi Jalukar      ¬†              Splitting component containers to simplify dependencies      Eugene Petrenko                    Build complex Spring Boot microservices architecture using JHipster [WORKSHOP]      Deepu K Sasidharan                    Caching Made Bootiful      Neil Stevenson                    Getting Thymeleaf ready for Spring 5 and Reactive      Daniel Fern√°ndez                     Developing a Spring Boot Starter for distributed logging      Carlos Barragan      ¬†              Reactive Meets Data Access      Christoph Strobl                    Building on spring boot lastminute.com microservices way      Luca Viola, Michele Orsi      ¬†              Growing Spring-based commons, lessons learned      Piotr Betkier      ¬†              CQRS with Spring Cloud Stream [WORKSHOP]      Jakub Pilimon      ¬†              Develop and Run your Spring Boot application on Google App Engine Flexible      Rafael S√°nchez                    Manage distributed configuration and secrets with Spring Cloud and Vault      Andreas Falk                    From Zero to Open Source Hero: Contributing to Spring projects      Vedran Pavic            "
      },
    
      "paas-2017-06-29-openshift-an-introduction-html": {
        "title": "OpenShift: An introduction",
        "url": "/paas/2017/06/29/Openshift-an-introduction.html",
        "image": "/img/Openshift.png",
        "date": "29 Jun 2017",
        "category": "post, blog post, blog",
        "content": "        Why my first blog about OpenShiftWhen I started as a developer, the cloud ecosystem started expanding and became the next big thing.So obviously I wanted to see what all the fuzz was about and started taking a deeper look at it.Soon, I got introduced with so many new technologies I was not familiar with: microservices, containers, pods, Kubernetes, load balancing, Docker, PaaS,‚Ä¶To be honest, for me it was really overwhelming.I told myself, I would never have the time to become a guru in all these technologies to start with cloud native development.So I just sat in a corner crying about why I became a dev in a time where things never looked more complicated and changed faster than ever before.But actually it‚Äôs not all that complicated.To be honest, deploying your containers in the cloud and managing things are easier than ever before with PaaS and OpenShift.A couple of months ago, I got introduced to OpenShift and got really excited about it!Recently, Ordina gave me the chance to visit the Red Hat‚Äôs partner conference and my excitement for OpenShift reached new heights.With my body being unable to contain all that excitement for OpenShift, I had to funnel it into a blog post or otherwise I would spontaneously combust.I do have to mention that if you want to work with OpenShift, you still need to have a basic understanding about containers, PaaS and Kubernetes if you want to understand some of its magic.If you have no idea what Docker containers are or what Kubernetes is, don‚Äôt panic! There are some great blogposts on the JWorks blog explaining more about them.What is OpenShiftOpenShift is a PaaS. For those who don‚Äôt know what a PAAS is, stop reading now, take a timecab to the year 2011 and check it out because PaaS is awesome.Gartner calls OpenShift a Cloud Enabled Application Platform (CEAP).For those who are not sure anymore, here is a quick reminder.  Platform as a service (PaaS) or application platform as a service (aPaaS) is a category of cloud computing services that provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an appThere are multiple PaaS providers available.For example, you have OpenShift, Cloudfoundry, Heroku, Google App Engine and more.Most of these platforms offer a lot of the same solutions, each with their own pros and cons, but today we are going talk about OpenShift specifically.When I look up OpenShift in Google (since that‚Äôs the first thing we do these days), it gives me the following explanation:  OpenShift Container Platform (formerly known as OpenShift Enterprise) is Red Hat‚Äôs on-premise private platform as a service product, built around a core of application containers powered by Docker, with orchestration and management provided by Kubernetes, on a foundation of Red Hat Enterprise Linux.Well that explains it!I never thought writing my first blog post would be that easy!Obviously you wouldn‚Äôt be reading this blog post if this was my only explanation since my pull request would never be accepted.If I would try to explain it with my own words to someone who never heard of OpenShift, I would define it like this.  OpenShift container Platform is a platform as a service you can deploy on a public, private or hybrid cloud that helps you deploy your applications with the use of Docker containers.It is build on top of Kubernetes and gives you tools like a webconsole and CLI to manage features like load balancing and horizontal scaling. It simplifies operations and development for cloud native applications.Okay, I know this is still pretty vague and it can do so much, so why don‚Äôt we simply start with seeing where OpenShift fits in.  As you can see in the image, the IT landscape has evolved a lot in recent years.We now have DevOps, Microservices, Containers, Cloud and Kubernetes.OpenShift combines all of those things in one platform you can easily manage.So it actually fits right on top of all of that.Overview  SELF-SERVICEDevelopers can quickly and easily create applications and deploy them.With S2I (Source-to-Image), a developer can even deploy his code without needing to create a container first.Operators can leverage placement and policy to orchestrate environments that meet their best practices.It makes your development and operations work fluently together when combining them in a single platform.POLYGLOT, MULTI-LANGUAGESince it deploys Docker containers, it gives you the ability to run multiple languages, frameworks and databases on the same platform.You can easily deploy microservices written in Java, Python or other languages.AUTOMATIONBuild automation:OpenShift automates the process of building new container images for all of your users.It can run standard Docker builds based on the Dockerfiles you provide and it also provides a ‚ÄúSource-to-Image‚Äù feature which allows you to specify the source from which to generate your images.This allows administrators to control a set of base or ‚Äúbuilder images‚Äù and then users can layer on top of these.The build source could be a Git location, it could also be a binary like a WAR/JAR file.Users can also customize the build process and create their own S2I images.Deployment automation:OpenShift automates the deployment of application containers. It supports rolling deployments for multi-containers apps and allows you to roll back to an older version.Continuous integration:It provides built-in continuous integration capabilities with Jenkins and can also tie into your existing CI solutions.The OpenShift Jenkins image can also be used to run your Jenkins masters and slaves on OpenShift.ScaleWhen you want to start scaling your application, whether it‚Äôs from one replica to two or scale it to 2000 replicas, a lot of complexity is added.OpenShift leverages the power of containers and an incredibly powerful orchestration engine to make that happen. Containers make sure that applications are packed up in their own space and are independent from the OS, this makes applications incredibly portable and hyper scalable. OpenShift‚Äôs orchestration layer, Google‚Äôs Kubernetes, automates the scheduling and replication of these containers meaning that they‚Äôre highly available and able to accommodate whatever your users can throw at it.This means that your team spends less time in the weeds and keeping the lights on, and more time being innovative and productive.OpensourceThere are multiple versions of OpenShift (spoiler: it‚Äôs going to be the next topic in this blog post) but they are all based on OpenShift Origin.Origin provides an open source application container platform. All source code for the Origin project is available under the Apache License (Version 2.0) on GitHubOpenShift landscapeThere are a few different OpenShift releases depending on what you need.As of this writing, the OpenShift landscape looks like this:  OpenShift OriginIt‚Äôs the upstream community project used in OpenShift Online, OpenShift dedicated and OpenShift container Platform.It‚Äôs build around Docker and Kubernetes cluster management.Origin is augmented by application lifecycle management functionality and DevOps tooling.Origin updates as often as open source developers contribute via Git.Sometimes as often as several times per week.Here you get the new feature the quickest but at the cost of stability.OpenShift container platformFormerly known as OpenShift Enterprise.It‚Äôs the platform software to deploy and manage OpenShift on your own infrastructure of choice.It integrates with Red Hat Enterprise Linux 6 and is tested via Red Hat‚Äôs QA process in order to offer a stable, supportable product with may be important for enterprises.OpenShift dedicatedOpenShift dedicated is the latest offering of OpenShift.It‚Äôs OpenShift 3 hosted on AWS and maintained by Red Hat but it is dedicated to youOpenShift onlineOpenShift Online is managed by Red Hat‚Äôs OpenShift operations team, and quickstart templates enable developers to push code with one click, helping to avoid the intricacies of application provisioning.You can view it as OpenShift delivered as a SaaS (Software as a Service)Benefits for developersBefore I show you how easy OpenShift is for a developer, let me quickly explain Source-to-Image (S2I).Let‚Äôs see how easy your life can be with the following image:  Source-to-Image (S2I) is a toolkit and workflow that creates a deployable Docker image based on your source code and add it to the image registry. You don‚Äôt even need a Docker file anymore.It combines source code with a corresponding builder image from the integrated Docker registrySo now that you know S2I, let‚Äôs take a look at the next picture        Code:If you‚Äôre a developer I assume you know how to code and push it to Git, so nothing new here‚Ä¶        Build:The developer can push code to be built and run on OpenShift through their software version control solution or OpenShift can be integrated with a developer‚Äôs own automated build and continuous integration/continuous deployment system. Here is were S2I can get useful.        Deploy:OpenShift orchestrates where application containers will run and manages the application to ensure it‚Äôs available for end users.        Manage:With your app running in the cloud you can monitor, debug, and tune on the fly.Scale your application automatically or allocate capacity ahead of time.  A deeper lookTime to get a little bit more technical and take a deeper look at how it works.I already talked about the developer part of the picture below, so let‚Äôs focus on the rest!  InfrastructureOpenShift runs on your choice of infrastructure (Physical, Virtual, Private, Public).OpenShift uses a Software-Defined Networking (SDN) approach to provide a unified cluster network that enables communication between pods across the OpenShift cluster.This pod network is established and maintained by the OpenShift SDN, which configures an overlay network using Open vSwitch (OVS).  The OVS-subnet plug-in is the original plug-in which provides a ‚Äúflat‚Äù pod network where every pod can communicate with every other pod and service.  The OVS-multitenant plug-in provides OpenShift Enterprise project level isolation for pods and services. Each project receives a unique Virtual Network ID (VNID) that identifies traffic from pods assigned to the project. Pods from different projects cannot send packets to or receive packets from pods and services of a different project.However, projects which receive VNID 0 are more privileged in that they are allowed to communicate with all other pods, and all other pods can communicate with them.In OpenShift Enterprise clusters, the default project has VNID 0.This facilitates certain services to communicate with all other pods in the cluster and vice versa.NodesA node provides the runtime environment for containers.Each node in a Kubernetes cluster has the required services to be managed by the master. OpenShift creates nodes from a cloud provider, physical systems, or virtual systems.Kubernetes interacts with node objects that are a representation of those nodes.A node is ignored until it passes the health checks, and the master continues checking nodes until they are valid.In OpenShift nodes are instances of RHEL (Redhat Enterprise Linux).PodsOpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason.OpenShift treats pods as largely immutable, changes cannot be made to a pod definition while it is running.It implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated.RegistryIntegrated OpenShift Container Registry:OpenShift Origin provides an integrated container registry called OpenShift Container Registry (OCR) that adds the ability to automatically provision new image repositories on demand.This provides users with a built-in location for their application builds to push the resulting images.Whenever a new image is pushed to OCR, the registry notifies OpenShift about the new image, passing along all the information about it, such as the namespace, name, and image metadata.Different pieces of OpenShift react to new images, creating new builds and deployments.Third Party Registries:OpenShift Origin can create containers using images from third party registries, but it is unlikely that these registries offer the same image notification support as the integrated OpenShift Origin registry.In this situation OpenShift Origin will fetch tags from the remote registry upon imagestream creation.MasterManaging data storage is a distinct problem from managing compute resources.OpenShift leverages the Kubernetes PersistentVolume subsystem, which provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed.The Kubernetes pod scheduler is responsible for determining placement of new pods onto nodes within the cluster.It reads data from the pod and tries to find a node that is a good fit based on configured policies.The Management/Replication controller manages the lifecycle of pods.For instance when you deploy a new version of your application and create a new pod, OpenShift can wait until the new pod is fully functional before downscaling the old pod leading to no downtime.But what if the master node goes down? That‚Äôs no high availability ‚Ä¶ You can optionally configure your masters for high availability to ensure that the cluster has no single point of failure.Service layerOn top of the domain and persistence layer sits the service layer of the application.A Kubernetes service can serve as an internal load balancer.It identifies a set of replicated pods in order to proxy the connections it receives to them.Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address.Persistant storageManaging storage is a distinct problem from managing compute resources. OpenShift Origin leverages the Kubernetes Persistent Volume (PV) framework to allow administrators to provision persistent storage for a cluster.Using Persistent Volume Claims (PVCs), developers can request PV resources without having specific knowledge of the underlying storage infrastructure.PVCs are specific to a project and are created and used by developers as a means to use a PV.PV resources on their own are not scoped to any single project; they can be shared across the entire OpenShift Origin cluster and claimed from any project.After a PV has been bound to a PVC, however, that PV cannot then be bound to additional PVCs. This has the effect of scoping a bound PV to a single namespace (that of the binding project).OpenShift.ioSo before ending this blog post, I have to quickly mention OpenShift.io.As of this moment, it‚Äôs not yet available but you can try to register for the preview.I haven‚Äôt had the chance to play with it, as I haven‚Äôt received my access just yet.Basically it‚Äôs an online development environment for planning, creating and deploying hybrid cloud services.It provides the following features:  Hosted, integrated toolchain  Planning tools for managing and prioritizing work  Code editing and debugging tools built on Eclipse Che  Integrated and automated CI/CD pipelines  Dashboards and reporting toolsConclusionOf course there is so much more to tell you and show about PaaS and OpenShift.I hope that with this post you got a nice introduction to OpenShift itself and some of the benefits it offers.If you enjoyed the post, I intend to write another post later this year about OpenShift, so make sure to regularly check our JWorks blog!May the PaaS be with you."
      },
    
      "architecture-2017-06-21-pragmatic-architecture-today-html": {
        "title": "Pragmatic Architecture, Today",
        "url": "/architecture/2017/06/21/pragmatic-architecture-today.html",
        "image": "/img/prag-arch/arch.png",
        "date": "21 Jun 2017",
        "category": "post, blog post, blog",
        "content": "Software development has evolved. Agile is now the de facto standard. The role of an architect in an agile project is very different from the typical role in a more classic waterfall approach. This article presents an updated interpretation of viewpoints and perspectives and will demonstrate how to make rapid, agile delivery sustainable in a constantly changing world. These viewpoints and perspectives can be linked to easy-to-produce models that can be used immediately. A good agile architect needs to strive for consensus and buy-in.Content  What?  Why?  How?What?  Architecture exists, because we want to create a system. A system is the combination of all the different components that together define an application.These components can be loosely coupled, eg. using Microservices; it can be a monolithic application or any other combination of runtime components that fulfill certain business needs.This is a different scope than a system of systems.That would be the goal of Enterprise Architecture where the focus is on the strategic vision of an enterprise.A system is built for its stakeholders. And stakeholders are diverse: the customer (who is paying for the system), the users, the developers, ‚Ä¶ I believe, sharing a crystal-clear vision with these stakeholders and getting buy-in from them, is necessary to create a successful system.Every system has an architecture, even when it is not formally defined. The architecture of a system is typically described in an Architectural Description.The architectural description documents the system for the stakeholders and needs to make architectural decisions explicit.The goal of the architectural description is to help in understanding how the system will behave.  Following the approach in the book Software Systems Architecture by Nick Rozanski and Eoin Woods, an architectural description is composed of a number views.These views describe what it is architecturally significant: info that is worth writing down because the system can not be successful without it or because stakeholders say it is significant.Deciding what to put in these views, means making decisions.Woods and Rozanski identified the following viewpoints:  Context View  Functional View  Information View  Concurrency View  Development View  Deployment View  Operational ViewThese viewpoints will assist in the writing of an architectural description.The website of the book contains a nice summary of these viewpoints.The views are shaped by perspectives. These are the cross-cutting concerns that have an impact on the views. Sometimes perspectives are also called quality properties or non-functional requirements:  Accessibility  Availability and Resilience  Development resource  Evolution  Internationalisation  Location  Performance and Scalability  Regulation  Security  UsabilityAgain, summaries are available on the website of the book.If you want a more in-depth explanation, I really recommend reading the book.In today‚Äôs agile world, I believe the Evolution perspective is a key differentiator in any architectural description.Generally, perspectives shape the architecture and deserve the necessary attention.Example  This is the 2017 Mercedes F1 W08 Hybrid. It weights 702kg and has between 750 and 850 horsepower. It is made out of approximately 80.000 different components. The price of the car is an estimated 10 million Euro. That is just for the car, not for the R&amp;D that made the car possible.Looking back at the viewpoints from above, it is easy to identify how these relate to the construction of the car:  A Formula One car needs a very specific factory (Development View).It is not built in the same factory Mercedes builds its trucks.  The cars need to be transported all around the world (all the Formula One cars travel over 100.000 miles in the air).This can be documented in the Deployment view.  Maintaining a Formula One car during a race has a huge operational cost and requires a lot of coordination (Operational View).Just count the number of engineers during a pitstop.  ‚Ä¶In the 2015 and 2016 season, the predecessors of this car won the Formula One World Championship.At the moment of writing, the 2017 car is also leading the championship.This pattern is quite common in Formula One.The older cars however, are currently up for display in a museum.They are rarely used anymore.This throw-away approach can also be noticed when comparing to other industries like smartphones or smartwatches.A lot of the success of the car, must be its architecture then.More specifically, its ability to change: to adapt to new rules, competitors and market change.If the architecture of a system, has the ability to change, it immediately has a competitive advantage.This is especially true in agile projects.  Grady Booch  Architecture represents the significant design decisions that shape a system, where significant is measured by cost of change.Often, it is very difficult to get a system right from the beginning.That is why creating a system, that has the ability to evolve, is important.Things are changing all the time: known change and unknown change.Within this evolving system, it is the responsibility of the software architect to make sure the system remains consistent.Multiple architectural patterns exist to support this:In the past, many systems were built with a configurable metamodel. Nowadays, loosely coupled, replaceable services are favoured.  When creating a 10 million Euro car, many teams (with different responsibilities) are involved.The people who design the engine are different from the people who design the brakes.Creating the best engine, creating the best brakes, ‚Ä¶ does not imply you will have the best car.Everything needs to work together.The integrity of the system is very important.This point is again proven by Formule One: other teams can buy the current Mercedes engine.They might win some races, but they haven‚Äôt won the world championship  Russell L. Ackoff  A system is more than the sum of its parts; it is an indivisible whole. It loses its essential properties when it is taken apart.To ensure system integrity, the software architect needs to be part of the project team.He must make sure that he enables the right people to collaborate on the system architecture.Being part of the team does not mean not taking responsibility.It is great to give ownership to members of the team, but in the end, the architect needs to stay accountable.When collaborating, an architect should not enforce all technical decisions.Part of working as a team, is accepting to be challenged and embracing this challenge.When team members have spirited discussions, it shows they are passionate enough about their craft to care.Disagreeing and discussing alternatives is a great way to come to a better solution and to learn from each other.Being part of the team, as an architect, will lead to a system with a consistent vision, where the implementation matches the architectural description.This also implies that an architect should be involved in the codebase of the system: writing code, performing code-reviews, doing proof-of-concepts, supporting others, ‚Ä¶By being involved in the codebase, you can make sure that the architectual description is understood by the developers.Visual?  While code is a very important asset of a working system, code alone is not enough to have an accurate and lasting description of a system.  Grady Booch  One cannot see the ocean‚Äôs currents by studying drops of water.The goal of visually representing a system, through the architectural description, is to make sure the architecture of the system is in the stakeholder‚Äôs heads.The visual representation can be used to check for consistency, reporting, validation and sharing information.Some ground rulesWhile UML has its merits, often it is not necessary to create an extensive UML model for the architecture.It will be time-consuming and, unfortunately, it is often the case that UML is not correctly understood by stakeholders.An alternative to UML is to use plain boxes and lines.However, when using boxes and lines:  Be consistent (especially when collaborating on the architecture).Try to be consistent over multiple projects. Templates offer a good start, but not every architecture needs the same viewpoints.  Avoid mixed responsibilities.  Avoid fluffy diagrams. Documents should not be vague. They should be about one abstraction.  Always provide a legend.Explain what a certain line or box means. Don‚Äôt make stakeholders guess.  Don‚Äôt be afraid to add text to a diagram.  Don‚Äôt model what nobody needs. Eg. if you are not using a data store, do not create an Information View.  Make sure your stakeholders understand what you are documenting.Whatever your preferred visualisation approach is, keep a decision log.Document your decisions, the considered alternatives and the timing a decision was made.Since the system will (very likely) evolve, a decision log will keep track of the reasoning behind a certain decisions.Decisions might need to change, so keeping track of the rationale behind a decision is valuable.Why?  Up-front designSome up-front design is necessary to start efficiently and to prevent too much rework.This means thinking about the big picture:  Used technology  Automation  Architectural patterns  Layering  Evolution  ‚Ä¶  Simon Brown  Just enough up-front design to create firm foundations for the software product and its delivery.But what does just enough mean?Just enough depends on a lot of variables like budget, scope, team, ‚Ä¶ The approach will also be different for greenfield projects or for existing projects.When you are working on a greenfield project, it is important to start with a high-level view of all components in the application.These components are all the pieces necessary for a system to operate.Other components and details can be added later.Working with existing systems benefits from a slightly different approach, where you can start with an accurate high-level diagram of the current architecture of the existing application.Once this diagram is available, identify the domain-of-change of the architecture: the reason people are working on the system.On top of that, adding extension points will enable evolvability.Communication  In the inception phase of a project, you will need to talk to all the different stakeholders and make sure that their desired product will be build.Aligning requirements from different stakeholders, will often be a challenge.  In the implementation phase, it is important for the team to share a technical vision.All team members need to collaborate to the same end-goal, which requires strong communication skills.Including team members in defining the technical vision is useful to make sure everybody knows how they, individually, are contributing to the technical vision on a day-to-day basis.PoliticsThe architecture of a system will have a large impact on the implementation, delivery and usage of the system.Systems generally consist of multiple parts and it is the responsibility of the architect to focus on system integrity, creating a system that has a built-in ability to respond to change.When the system lacks integrity, it will rapidly become a system nobody wants to touch.Unfortunately many enterprises have this fear of change embedded in their culture and it will take strategy and sound people skills to prevent this from happening.Influence Maps present an interesting way to map relationships between people and to visualise who influences who, in an enterprise.Being aware of these relationships might be a game-changer.How?  One way of creating an architectural description is OODA: Observe, Orient, Decide, Act.OODA can be compared with PDCA, also known as the Deming Cycle or with Discovery Activities.  Any architectural model introduces abstraction and removes noise.This model should be well-understood and feedback loops can help with this.As an example, comparing a written down version with bullet points of a certain idea, will help in verifying that the message hasn‚Äôt changed.This insight should be mapped on the model.  Observe: Observing both external and internal circumstances or dependencies of your systems.          Collect up-to-date information from different sources: stakeholders, competitors, similar systems, other viewpoints,‚Ä¶        Orient: Using your past experience to make sense of these observations.          Analyse the observed information and use it to update your current reality. View events, filtered through your own experiences and perceptions.        Decide: Deciding on a response, because there might be multiple alternative solutions.          Determine a course of actions.        Act: Execute the selected decision.          Follow through on your decision.      This is not a linear process. This process benefits from continuous feedback loops.Feedback loops imply that certain decisions may lead to new observations etc.The OODA process can be used as a means of creating an architectural description.Consequently, significant decisions will become part of it.Since the creation of (significant parts of the) the architectural description, starts with (runtime) observations, capturing data and measuring stakeholder value will help to achieve better observations of the system."
      },
    
      "nodejs-2017-06-20-rest-api-nodejs-koa-html": {
        "title": "Creating a REST API with NodeJS, TypeScript and Koa.",
        "url": "/nodejs/2017/06/20/REST-api-NodeJS-koa.html",
        "image": "/img/nodejs-typescript-koa/koa-logo.png",
        "date": "20 Jun 2017",
        "category": "post, blog post, blog",
        "content": "  This article assumes you already have some knowledge of npm and JavaScript development in general. It will not be a detailed tutorial about how to write a REST API, it‚Äôs more of an extra explanation for the application I made, the libraries I used and my experience with them.Why?I started this little project because I wanted to be able to quickly write backends for small personal projects with little overhead. Coming from the Java backend world, I have been writing almost only JavaScript for close to 2 years now, but only frontend. I had tried NodeJS in the past for a small project with plain old JavaScript and had a very bad time. Now however, with my new experience in JavaScript, the arrival of ES6 and TypeScript, I wanted to give it another shot.What exactly did I make?The idea was to write a backend for an application called MovieListr. It‚Äôs a simple application to track movies you have watched or want to watch. The API allows you to create, delete, update and see movies and directors. A movie also has a one-to-one relation with a director.You can find the code on Github.SetupSetting up a node project with TypeScript doesn‚Äôt require a lot of effort, the following commands are enough to get started.mkdir &lt;project-name&gt;cd &lt;project-name&gt;mkdir srcnpm init //follow the setupnpm install --save-dev typescript tsc //install TypeScript and the TypeScript compilertsc --init //generates a `tsconfig.json`, a config for the TypeScript compilerThis is it, you still have to tinker with the tsconfig.json to get it to your liking, but after that you can just start writing code.Using async / awaitI want to start with talking about the async / await features. They were what really made this code so fast to write and easy to read. The async keyword marks a function that will always return a promise. The await keyword will automatically unwrap the value from the promise and continue the code when the promise has been resolved. A small example:      const promiseFn = (): Promise&lt;string&gt; =&gt; {\t    return Promise.resolve(\"Hello World\");    }     // Old way:     const asyncFn = () =&gt; {\t    promiseFn()\t\t    .then(value =&gt; {\t\t\t    console.log(value);            });    }     // With async / await     const asyncFn = async () =&gt; {\t    const text = await promiseFn();\t    console.log(text);    }You can see how readable it is with the async / await syntax. You can write asynchronous code in a synchronous way and I used it heavily everywhere in my code. I think this is one of the things that will really make writing JavaScript fun. No more callbacks, no more boilerplate code, just the important bits. For error handling you can rely on try catch statements to catch errors and act on them.To use the async / await syntax, you can have to add esnext.asynciterable to the lib array in the tsconfig.json file.The libraries I usedKoaKoa is a small node library to create REST APIs. It was made by the guys who created Express. It takes advantage of the new ES6 feature of generator functions and it allows you to write very readable code by using the async / await features (that are based on the generator functions). For a full understanding of koa and generator functions, I suggest the Koa course on Pluralsight from Hammarberg.Koa relies heavily on middleware, so for every ‚Äústep‚Äù of the process we need middleware. For instance koa-bodyparser middleware will parse the request body to json, the koa-logger middleware will log all the incoming requests and the koa-router middleware will make it easy for us to configure the url mapping to certain actions. These middlewares are installed apart from the Koa framework or you can write them yourself.typescript-iocTo make testing easy, I started looking for a dependency injection framework for TypeScript. I wanted to more or less copy the way I wrote unit tests in Java, which is using dependency injection in your actual code and just creating an instance in your unit test while passing mocks instead of the dependencies. The first dependency injection framework I found, was Awilix. I got Awilix to work, and it worked quite well, but there was still a lot of boilerplate code to write to actually register the services to the container and to get it working. You can also pass folder names so it will register all the services in that folder, but I didn‚Äôt find this optimal. I was also using Webpack in the beginning (which I write about later in the article) to build my application and bundle my code, by bundling the code, the paths of the folders obviously didn‚Äôt work out anymore in the compiled code, so Awilix was no good for me. I kept searching and I found the library typescript-ioc. This library was based on annotations, so there is barely any configuration overhead and it worked much more like I was used to in Java. typescript-ioc requires you to set experimentalDecorators and emitDecoratorMetadata to true in the tsconfig.json file. You can then just write code like      import { Container } from \"typescript-ioc\";        class Foo {        doSomething(): string {        \treturn \"Hello World\";        }\t    }        class Bar {        constructor(@Inject private foo: Foo) {        }\t        \tdoAnotherThing(): string {    \t\tthis.foo.doSomething();        }    }        const bar:Bar = Container.get(Bar);    bar.doAnotherThing();typeormAt first I just saved the movies and directors in the services as an in-memory array for testing purposes, but in a real application you will want persistence of some sort, so I needed a database. I decided on a regular old MySQL database and an ORM library to do the mapping between the database records and my TypeScript model classes. For ORM I used typeorm. It‚Äôs pretty easy to use. It also uses the annotations like typescript-ioc, which makes code very readable. The experience with this library was more or less pain free, so I really recommend it. To check a real example from my repository, check the Movie model.TestingUnit testingFor unit testing I used the classic combination of Mocha, Sinon and Chai. Since I was using dependency injection, I also needed a good way of mocking my dependencies, for this I found ts-mockito. Ts-mockito is more or less a clone of the Mockito library in Java. It allows you to create mocks of classes, make functions return certain values and verify that calls have been made. This made it super easy to write tests. For examples check the tests folder in my repository. To execute the TypeScript tests, I used ts-node. Ts-node compiles the TypeScript and keeps the compiled JavaScript in memory while it executes it. This way you don‚Äôt have to create an additional folder to compile the tests to and execute them. You can then easily create an npm script like this:mocha -r ts-node/register test/**/*.spec.tsThis tells Mocha  to require the ts-node/register module (this is what the -r ts-node/register) means and then it just passes the path of the test files to it. This also worked pretty much painlessly.end-to-end testingI wanted to be able to do some real end to end testing. So I wanted to be able to spin up my application, pass some HTTP requests to it and then verify the output of the requests. The first question was how to pass the requests to my application. For this I found the library SuperTest. You can just start you Koa app and pass the HTTP server (the return value of the app.listen function) to the agent and it will make sure the app is started and you can do some requests and check the results. This worked pretty well.The second problem was a test database. I needed a database that was as close to the real one as possible. I ran the real database in a Docker container with a volume that mapped the /var/lib/mysql (the configuration / data folder for MySQL) to a host directory, so I could recreate the container without losing data. I figured I could more or less copy the Docker configuration for the database for a test database, only without the volume. Without the volume, the data would just be saved to the container itself, so it would be lost every time the container was recreated, which is perfect for end-to-end tests, because we want to start the tests with the exact same dataset, so we can make sure our assertions keep working.So I created an npm script to start the Docker and to do healthchecks to the Docker container until it told me that the entire container was up and running and MySQL was ready to take connections. Then I wrote a script to start the actual end to end tests, which was simply the same mocha call I wrote earlier, only pointing to the e2e folder instead of the test folder. At last I wrote an npm script to stop the Docker container and remove it. You can check these scripts here. I made heavy use of the shelljs package. This npm package allows you to execute shell commands, which I used to start Docker containers from JavaScript.NOTE: this setup works well, but the starting of the Docker container takes ~30 seconds, which is quite long, considering that the tests take maybe a few seconds. In a continuous integration build, this doesn‚Äôt matter as much, but when you are trying to fix tests, it does take a lot of time if you have to wait about a minute for each test run.Task runnerWebpackWhen I started this project, I was looking up some best practices for node. I came across an article that suggested you should use Webpack for backend too. I already have some experience with Webpack from frontend development, so at first it seemed logical to use it for backend too. When I was trying to get the dependency injection to work with Awilix, I realized that I could not pass any paths to libraries, because when my code was bundled, the paths would be invalid. Then I started to actually wonder why I was bundling my code. In frontend you bundle your code to make it as small as possible so you don‚Äôt waste the user‚Äôs bandwidth and make you website load faster, but in backend, that does not matter, since the code does not have to be sent anywhere. At this point I decided I didn‚Äôt need Webpack at all and I could just use npm scripts‚Äô functionality to create tasks.npmNpm is actually the only build tool you need. If what you want to do is more than a single line command, you can just write scripts in either TypeScript (you can execute them with ts-node), bash, JavaScript, ‚Ä¶ whatever you like. I wrote my scripts in TypeScript, because to me it makes more sense to use TypeScript for everything, but I could just as well have written them using bash. Npm also gives you pre and post task hooks. So if you write a task with the name ‚Äúe2e‚Äù as I did, you can also add a task with the ‚Äúpre‚Äù prefix or the ‚Äúpost‚Äù prefix that will automatically be executed before and after the task is executed. This way I could easily separate the starting of the Docker container, the executing of the tests and the stopping of the Docker container into different scripts. I could then just execute npm run pree2e to check if my script to start the Docker worked. I really like this approach and the fact that I don‚Äôt need another tool to learn like gulp or Webpack.DebuggingApplication codeI had some trouble at the beginning with debugging my TypeScript. For some reason in the Chrome Devtools I could not get my sourcemaps working (even though they were inline sourcemaps). Then I tried the Visual Studio Code debugger and that worked much better. To get this to work, I did the following:tsconfig.json      {        \"compilerOptions\": {            \"inlineSourceMap\": true,            \"inlineSources\": true,        }    }package.json      \"scripts\": {        \t    \"start:debug\": \"ts-node --inspect=5858 --debug-brk --ignore false src/index.ts\",            }.vscode/launch.json      {\t    \"configurations\": [\t        {                \"type\": \"node\",                \"request\": \"launch\",                \"name\": \"Debug Application\",                \"runtimeExecutable\": \"npm\",                \"windows\": {                    \"runtimeExecutable\": \"npm.cmd\"                },                \"runtimeArgs\": [                    \"run-script\",                    \"start:debug\"                ],                \"outFiles\": [],                \"protocol\": \"inspector\",                \"sourceMaps\": true,                \"port\": 5858            }        ]    }The npm script will start the execution of the index.ts with ts-node in debug mode on port 5858 and the --debug-brk tells it to break on the first line of code. The launch configuration will just execute this npm script and attach it to the debugger.Test codeDebugging the test code is more or less the same as the application code, there is just a small caveat. When you create breakpoints in Visual Studio Code, they will appear gray as if they cannot be reached. But when you execute the code, it will break on the breakpoints and then they will become red like a normal breakpoint.package.json      \"scripts\": {\t    \"test:debug\": \"mocha --inspect --debug-brk --not-timeouts --compilers ts:ts-node/register test/**/*.spec.ts\",    }.vscode/launch.json  {    \"configurations\": [        {            \"type\": \"node\",            \"request\": \"launch\",            \"name\": \"Debug Tests\",            \"runtimeExecutable\": \"npm\",            \"windows\": {                \"runtimeExecutable\": \"npm.cmd\"            },            \"runtimeArgs\": [                \"run-script\",                \"test:debug\"            ],            \"outFiles\": [],            \"protocol\": \"inspector\",            \"sourceMaps\": true,            \"port\": 9229        }    ]}ConclusionI really had a good time making this project. I really love readable and compact code and with TypeScript and the async / await syntax, I really got what I asked for. My previous experience with node.js and regular old JavaScript was really bad, mostly because of the loose typing, which forces you to constantly write a lot of tedious checks on parameters. With TypeScript that is all in the past. Apart from that, the enormous amount of npm packages available, makes it very easy to find some package that does what you need. If for some reason you can‚Äôt find something, you can easily write it yourself and publish it to npm.I always used to use Java for my backends, but the setup is always a bit of work and you have to write more boilerplate code than with TypeScript. If I make more small projects in the future, I will probably use TypeScript and Node, but for me at this point, it‚Äôs hard to tell if NodeJS will hold up in bigger projects. I would assume so, since the structure for me at this point, is very similar to Java, just a more concise syntax."
      },
    
      "spring-2017-06-07-spring-io-2017-the-spring-ecosystem-html": {
        "title": "Spring IO 2017: The Spring Ecosystem",
        "url": "/spring/2017/06/07/Spring-IO-2017-The-Spring-ecosystem.html",
        "image": "/img/spring.png",
        "date": "07 Jun 2017",
        "category": "post, blog post, blog",
        "content": "  When I was at Spring IO back in May, I was intrigued by a presentation given by Jeroen Sterken.There he talked about the Spring Ecosystem in 50 minutes.Since he only had 50 minutes, he could not focus on all the projects Spring boasts.I wanted to get a feel of what the Spring team has to offer in all its glory, by getting to know all of the main projects.Jeroen Sterken (@jeroensterken) is a Java and Spring consultant from Belgium. He‚Äôs a certified Spring instructor and currently employed at Faros Belgium. His slides of his talk The Spring Ecosystem in 50 minutes can be found here.The Spring EcosystemThere are many ways to divide the Spring portfolio.One way could be based on architecture, another way could be based on popularity. Jeroen divided the Spring Ecosystem in three categories: classic, popular and other.Before we dive into the Spring ecosystem, let‚Äôs take a look at which projects our own JWorks unit have been using the most over the past two years. Here‚Äôs the JWorks top 10, beside the Spring Framework.        Spring Boot is currently at the top. Other notable mentions are Spring Session, Spring Social and Spring Cloud Data Flow.But what‚Äôs even more interesting are the Spring projects that aren‚Äôt that widely used: Spring Mobile, Spring for Android, Spring Kafka, Spring Statemachine, Spring Shell, Spring Flo and Spring XD.ClassicThe classic projects are showing a range of the many beloved portfolio projects, where for instance Spring Security and its LDAP module will help you build your secure applications at ease.Or where the Spring IO platform will show you the insights in its development.        Spring FrameworkThe core of Spring, currently at its fifth revision.It provides key components for dependency injection, web apps, transaction management, testing, messaging, model-view-controller, remote access, data access and more.Just add the modules you need and start programming.In the fifth version, the focus lays on reactive programming with reactive streams, as well as other features and revisions like support for JUnit 5.Spring 5 will require at least JDK 8 but is already being built continuously on JDK 9.The release is planned for the end of the year, regardless whether Java 9 is released or not.Spring IO PlatformThe Spring IO Platform is built on Spring Boot and is mainly used in combination with a dependency management system.It provides dependencies that work well together.It‚Äôs basically a library on the classpath of your application which gives developers production-ready features.It does this by providing a bill-of-material Maven artifact.The libraries used in the BOM file are all curated and normalized, so they work greatly together.But if that is not to your liking, you can easily just use your own versions.The platform supports JDK 7 and 8 and is still being updated frequently.Spring SecurityNowadays you can‚Äôt ignore problems of security failures and the importance of privacy.Spring Security provides your application with authentication and authorization.It will also protect your application against a handful of possible attacks.Spring Security supports many popular authentication protocols and services like OpenID, LDAP, HTTP, ‚Ä¶ and support is extended through the available third party modules.The fifth version of Spring Security will add OAuth 2.0 support.Spring LDAPSpring LDAP hides a lot of the boilerplate code for LDAP interactions.It makes sure all the connections are created and correctly closed.This library helps out with the looping through the results and filtering those.It‚Äôs also possible to manage your transactions with a client-side transaction manager.If you‚Äôre working with this Lightweight Data Access Protocol, this might definitely be worth your while.Spring IntegrationWhen an architecture revolves around events or messages, you can get the help of Spring Integration.This project focuses on the implementation of Enterprise Integration patterns.When you want to send something from point A to point B, there could be a lot of different network protocols or restrictions in between.Spring Integration minimizes the boilerplate code needed by implementing those patterns.It just makes it easy to send events and messages throughout different endpoints.Spring BatchWith Spring Batch it is possible to write an offline batch application using Java and Spring.It makes it very convenient when you‚Äôre used to the Spring Framework to execute a bunch of jobs.It features a possibility to read and write your resource and a way of dividing data for processing and much more.There is also support for a transaction manager, job processing statistics, job status changes and much more.Spring Web FlowThe Spring Web Flow was created to help users navigate through the different views of a stateful web application.A common example could be when shopping online.The process has a clear starting and finishing view, but in between, it can change state or views dynamically.Through guided navigations, the user makes changes and it should register those changes as well as the possibility to finalize those changes through a confirmation.All this is possible with Spring Web Flow.Although this project is listed with the main projects, there hasn‚Äôt been any progress over the last years, and will be removed when Spring 5 hits the shelves.Spring Web ServicesThere are several ways to develop a web service, one of which is used in combination with SOAP.Spring Web Services helps with creating contract-first SOAP web services which are flexible by manipulating the XML contents.But due to the popularity of the architectural style of REST, the interest in SOAP has diminished.This is noticeable in the maintenance of this Spring project which hasn‚Äôt had any significant version updates.Version 2.4.0 was released on August 26th 2016 and only brought some CI jobs that are built for every commit for Spring 4.2, 4.3, and 5.0.PopularWhen you look at modern applications and their infrastructure, you‚Äôll see the power of the Spring portfolio coming to its use.With the easy of use of Spring Boot, you can quickly start the development of a secure application and use Spring Cloud to help you with the deployment and integration for your online service provider.        A modern application might look like this:        Spring BootBeing built onto the Spring Framework, the popular Spring Boot project provides an easy to use way for creating stand-alone Spring applications without code generation and configuration of XML files.If you want to get started quickly without too much hassle, Spring Boot is the way to go by adding the dependencies you need.Spring Beans don‚Äôt need to be defined in XML or Java, as they are mostly configured automatically by Spring Boot.This way, there is no need to find and configure libraries for your specific Spring version, Spring Boot tries to do that for you.However, if you wish, you can fine-tune the auto-configuration to your own needs by adding the library to the classpath of the application, setting some properties, or adding some annotations.When you want to deploy your Spring Boot application, there‚Äôs no need to build a WAR file, since you can build self-contained JAR files with an embedded servlet container such as Jetty, Tomcat or Undertow.Spring Boot also features a command line tool for quick prototyping with Spring.The easiest way to get started with Spring Boot is to go to the Spring Initializr and add the dependencies to the project.The Spring team is maintaining the Spring Boot project regularly as it‚Äôs becoming the de facto way of using Spring.Spring CloudSpring Cloud is an umbrella project which lets you build distributed systems by implementing many best practice patterns.It consists out of many sub-projects.With the use of Spring Cloud Config Server you can setup a server with a repository, like Git, as its data store and view the changes made in the configuration.Spring Cloud Contract allows you to write Consumer Driven Contract Tests with ease.Many of the Netflix OSS components are wrapped into Spring Cloud, which makes it a lot easier to deal with the complexity of microservice architectures.And of course with a cloud service there‚Äôs often a lot of security involved which is provided by the Spring Cloud Security.You can easily integrate this with Amazon Web Services or Cloud Foundry, through their related subprojects.Spring Cloud Security is build on OAuth2 and Spring Boot which provides single sign-on, token relay and token exchange.One of the latest projects in the Spring Cloud umbrella is Spring Cloud Function.It offers an extreme convention-over-configuration approach which can leverage all of Spring Boot‚Äôs capabilities while writing only a single function.The full list of sub-projects are available here.Spring Cloud Data FlowSpring Cloud Data Flow used to be know as Spring XD and is part of Spring Cloud.It‚Äôs an updated and revised toolkit for cloud-native message-driven microservices.The change was made by the Spring team after their experience with Spring Boot.Spring Cloud Data Flow is suitable for processing, analyzing and predicting data.Through streaming it can consume data from an HTTP endpoint and writes the payloads to a database of your choice.It also manages to scale the data pipelines to your liking without any interruptions.After development, an application can be easily executed in Cloud Foundry, Apache YARN, Kubernetes or Apache Mesos, but with the Service Provider Interface you can deploy your application to other runtimes.Spring DataWhether you‚Äôre working with relational or non-relational databases, Spring Data will soothe your needs.As an umbrella project it will ease your way into data access.It abstracts the complexity of data access layers by allowing the developer to simply extend an interface.Some of the related sub-projects will help you develop quicker for your favorite database, like Spring Data Mongodb, Spring Data JPA, Spring Data for Apache Cassandra or Spring Data for Apache Solr.And through the help of some community modules this is extended to several others.With Spring Data REST you can expose your Spring Data repository automatically as a REST resource.As usual with Spring projects, they provide an excellent base but can be customised to your own needs.A full list of sub-projects and community projects are available here.Spring HATEOASHATEOAS stands for Hypermedia As The Engine Of Application State.It enables the server to update its functionality by decoupling the server and client.With Spring HATEOAS it‚Äôs easy to create a REST resource implementation using the HATEOAS as an underlying principle.It helps the client by returning a response in combination with more information on what to do next.If the state of the resource changes, the information on the next steps will also vary throughout the application.As this is a subconstraint one of the core principles of REST, the uniform interface, using Spring HATEOAS you can achieve ‚Äòthe glory of REST‚Äô.Spring REST DocsWhen you develop a RESTful service, you‚Äôll probably want to document it so it‚Äôs easy for other developers to implement your API.Spring REST Docs helps you with the documentation process to make it more accurate and readable.It does this by running integration tests, which generate guaranteed up-to-date request and response snippets when those tests succeed.Those snippets can be included in Asciidoctor templates, which are then converted to HTML output.Alternatively it can be configured to use Markdown.The advantage here is that the documentation is always up-to-date with your code, since the integration tests will fail otherwise.There are also options for you to customize the layout of the documentation.A more in-depth look at Spring REST Docs was presented at Spring IO 2016 by JWorks colleague Andreas Evers: Writing Comprehensive and Guaranteed Up-to-date REST API Documentation.Spring SocialSpring Social lets you connect your application with Facebook, Twitter and LinkedIn.But through its many community projects it‚Äôs possible to connect to dozens other like Google, Instagram, Pinterest, ‚Ä¶The full list is of supported third-party APIs is available hereSpring SessionWhen someone uses your web application, they will be using an HTTP session underneath.Spring Session allows you to manage those sessions separately, outside of the servlet container.It supports multiple sessions at once and can even send the sessions in the header.Spring sessions isn‚Äôt specifically tied to any container.Although the project is quite popular and has very interesting features, the project hasn‚Äôt had any major changes over the past year.OtherThese projects are mainly focused on one specific (niche) part of an application.Some wil help you with the development of specific front-end applications, while others will help you implement specific patterns.        Spring AMQPAMQP is an abbreviation for Advanced Messaging Query Protocol which Spring AMQP implements.It helps you with routing, queuing, exchanging and bindings.Additionally, there‚Äôs a listener available when sending messages asynchronously.Spring AMQP also provides a template service for sending and receiving messages.In the upcoming second version of Spring AMQP it uses version 4.0.x of the library which has been developed by RabbitMQ.Spring MobileSpring Mobile is the Spring team‚Äôs attempt at making it easier to develop mobile web applications with the use of Spring MVC.Spring Mobile implements a way of detecting the type of the device used to view the url and tries to adjust its view accordingly.Unfortunately the project isn‚Äôt that well maintained as significant updates are several years ago.Spring for AndroidAnother project without any recent updates is Spring for AndroidSpring for Android brings some of the key benefits of using Spring to Google‚Äôs mobile operating system, Android.It has a REST API client for Android with authentication support.For your social media authentication, you can use Spring Social in conjunction with Spring for Android.But there‚Äôs no use of Spring‚Äôs dependency injection, transaction manager or some other useful Spring features.Spring ShellThe Spring team provided a way for building command-line applications.Through the use of Spring you could build a full-featured shell application with your very own commands or just use the default commands that are already implemented.Or you could get access to an exposed REST API.The Spring Shell hasn‚Äôt been updated with new functionality in more than 3 years.Spring XDSpring XD is the predecessor of Spring Cloud Data Flow and therefore hasn‚Äôt been maintained.End of support will be in July 2017.Spring FloThis JavaScript library was a foundation for the stream builder in Spring Cloud Data Flow.It provides a basic embeddable HTML5 visual builder.Spring Flo is especially focused on pipelines and simple graphs.It‚Äôs built using Grunt where the commands can be ran directly or indirectly through Maven.With the use of a drag and drop interface it‚Äôs easy to create real-time streaming and batch pipelines.Additionally you can also choose to use the shell instead of the GUI interface.Spring KafkaThis is Spring for Apache Kafka, an open-source streaming processing platform.Spring Kafka provides an interface for sending messages for Kafka-based applications.It also supports a listener container and a way of sending message-driven POJOs.Spring StatemachineSome applications may require state machine concepts being implemented.Spring Statemachine provides a framework that helps with that.It provides a lot of useful things for making complex configuration easy, but also provides listener states and much more.Spring RooSpring Roo gives you the possibility to easily build full Java applications.This is a tool for rapid development of Java applications that are fully written in Java.It is focused on using the new Spring projects, like Spring Boot and Spring Data, as well as other common Java technologies.However, since the introduction of Spring Boot, Spring Roo has become less of a necessity, as Spring Boot hides a lot of the boilerplate code Spring Roo was designed to generate.Spring ScalaWhen developing applications in Scala, you can make use of Spring through Spring Scala, a community project.This brings a lot of Spring technologies to the Scala programming language.This is one of the two presented community projects by the Spring team on their main project page, the other one being Spring Roo."
      },
    
      "blockchain-2017-05-10-blockchain-introduction-html": {
        "title": "Blockchain introduction",
        "url": "/blockchain/2017/05/10/Blockchain-Introduction.html",
        "image": "/img/blockchain/blockchainHeaderImagePNG.png",
        "date": "10 May 2017",
        "category": "post, blog post, blog",
        "content": "  A lot of people are talking about blockchain these days.They‚Äôre talking about blockchain as the next big thing after mainframes, computers, the internet and social networking.This introduction is the first part in a series of blockchain posts.TopicsIn this first article about the innovative blockchain technology, we‚Äôll cover the following topics:  Blockchain and its relation to Bitcoin  What is blockchain  Types of blockchain networks  The consensus process  Smart contracts  Valid blockchain business cases  Existing platforms  Thinking decentralized  Conclusion  Recommended readingBlockchain and its relation to BitcoinFirst of all, Bitcoin and blockchain are two different things.People tend to use both words by each other in three different contexts:              1. Digital cryptocurrency                    2. Protocol and client for executing transactions        \t            3. The blockchain which stores all Bitcoin transactions    So when talking about Bitcoin or blockchain with people, it‚Äôs important to mind this terminology.Here‚Äôs a funny quote I read in the book Blockchain: A Blueprint for a New Economy,which describes this ambiguity very well:It's as if PayPal called the internet PayPal on which the PayPal protocol was run to transfer PayPal currency.In January 2009, the Bitcoin network came into existence.Bitcoin isn‚Äôt the first attempt to digital currency, but it‚Äôs the first one that uses a peer-to-peer network to create a platform for executing transactions without depending on central authorities who validates them.You should see Bitcoin as the first platform that implemented blockchain technology.What is blockchain?So forget about Bitcoin now.That‚Äôs not what this post is about.People say blockchain is as important as the introduction of the internet. The internet is a worldwide network to share information with one another, but it is far less suitable for transferring value.If you send someone a file, it is always a copy of your file, which means you and the receiver are both in possession of the file.As we already stated, that is ideal for sharing information, but not applicable for money, certificate of ownership, and so forth.And the latter is exactly what blockchain enables: digitalizing and transferring such values.Let‚Äôs take a look at the underlying decentralized ledger technology.We believe blockchain‚Äôs definition is a good starting point:  ‚ÄúBlockchain is a type of distributed database that stores a permanent and tamper-proof ledger of transaction data.‚ÄùTL;DR versionBlockchain is a decentralized immutable data structure.In short the blockchain is a network of computers, called nodes. Every node has the same copy of the database which they manage together. A transaction is encrypted and signed on a mathematical way. When a transaction is saved in the blockchain, it is duplicated across all nodes in the network.That‚Äôs why we talk of blockchain as distributed ledger technology, a ledger of transactions, distributed across a computer network.Transactions are bundled in one block before they are validated by other nodes.Once the network reached consensus about the validity of these transactions, the block is appended to the existing chain of blocks.The block stores the validated transactions together with a hash and a reference to the previous block.Stored transactions cannot be undone, as this would invalidate all hashes in the chain.Now a little more in detail‚Ä¶Transactions are broadcasted to the network for miners to mine. They assess the non-validated transactions on the memory pool by solving a mathematical puzzle. A miner builds a block containing all transactions, a proof of work that the puzzle was solved (also known as the block root hash, which is also the ID of the block) and a hash to the previous block.A block also contains the following items:  A timestamp  a nonce  and a merkle root hashA merkle root does not verify transactions, but verifies a set of transactions.Transaction IDs are hashes of the transaction, and the merkle tree is constructed from these hashes.It means that if a single detail in any of the transactions changes, so does the merkle root. It also means that if the exact same transactions are listed in a different order, the merkle root will also change.So the merkle root is cryptographic proof of the transactions in the block together with the order they are in.The nonce number is a field whose value is set so that the hash of the block will comply with the predefined network rules (eg: a run of leading zeros in Bitcoin). Miners increase the nonce until the hash is valid. Sha-256 is used to hash.The miner appends the block to the blockchain. And the majority of the other nodes, 50% + 1, double-check by verifying the proof of work in the block.It sometimes occurs that miners will validate two blocks at the same time and they will be appended to the chain. When this occurs, which doesn‚Äôt happen often, the principle of Longest Chain Wins will be implemented.The longest chain remains and the conflicting chain will be discarded.The transactions of the discarded chains will be put back in the memory pool to be mined another time.You now have a basic understanding of why we call it the blockchain.  \t        Types of blockchain networksPublic blockchains (aka. permissionless)This is a blockchain that everyone in the world can view, write transactions to, expect that these transactions will be validated and added to the blockchain.In this type of blockchain network, any connected node can contribute the consensus process.This process is used to determine if a block is valid or not.You can read more about the consensus process further in this blogpost.The public blockchain is generally a complete peer-to-peer network. Its characteristics are:  The users from the chain get protected from the creators of the chain, because there are actions to the network that even they cannot perform.Developers are not the owners of the network and don‚Äôt have more or less privileges than normal users.  These chains are transparent because everyone can see what is happening inside the chain.\"In some cases, public is clearly better; in others, some degree of private control is simply necessary. As is often the case in the real world, it depends.\" - Vitalik Buterin of EthereumConsortium blockchainsIn this type of blockchain network, the consensus process is executed by a predetermined group of nodes in the network.Let‚Äôs take a consortium of fifteen financial institutions as an example, each with a node.From this group of fifteen, there are ten nodes that need to sign each block before it is valid.You could say that these ten take ownership of the data in the blockchain.They decide which transactions are valid and which ones are not.Read rights can be public or restricted to the members of the network, eg. we can limit public view to a set number of times.Public and consortium blockchain networks are decentralized, with the difference that the consortium network is not completely peer-to-peer, because not everyone is equal.Private blockchains (aka. permissioned)There is only a small difference between consortium and private blockchain networks: write rights are with one organization instead of multiple.The read rights can be the same as with a consortium blockchain.The following characteristics apply for a private blockchain network:  The company that controls the private chain can alter the rules of the chain. In some cases this can be necessary.  The nodes that confirm a block are known, so there can‚Äôt be a majority by a mining farm with a 51% attack.  Transactions are cheaper than with public chains. This is because they need to be verified by less nodes.  Nodes are well connected and errors can be fixed quickly with manual interaction.This means that these networks give a faster confirmation and they will always be faster than public networks.  Private blockchains are just better at privacy because the access to the blockchain can be limited.From a legal point-of-view, this characteristic can have significant impact on the type of blockchain network you‚Äôll pick.The consensus processAs we mentioned before, the network must reach a consensus of 50%+1 for a transaction to be written to the blockchain.There are a few ways a blockchain network will do this.We will be discussing the two most used.Ronald Chan wrote a nice article about consensus mechanisms in Consensus Mechanisms used in blockchain.Proof-of-WorkThis is used to deter people from tampering with the blocks and launching (d)dos attacks. We let them do a feasible but not insignificant amount of work to get a consensus. For example in the blockchain they need to find the correct nonce number that is part of the block to create a hash that fits the predetermined rules. A rule can be that the hash must start with six zeros.Proof-of-StakeIn this case you don‚Äôt need to find a nonce number but you just need to proof that you have a certain stake in the network.The bigger your stake, the more you can mine from the network.Smart ContractsThe term smart contract has no clear and settled definition.So what is it?Smart contracts are traditional contracts and official documents, but written in code.As such, the contract is understandable for everyone across the globe, irrespective of the jurisdiction it is related to. Smart contracts are like If This Then That statements, only they tend to be a lot more complex.The different definitions usually fall into one of the following two categories:  Sometimes the term is used to identify a specific technology.Pieces of code that are stored, verified and executed on a blockchain.For example, a hello world program.  The term can also be used to refer to a specific application of that technology: as a complement, or substitute, for legal contracts.  \t        Valid blockchain business casesIt‚Äôs important to understand that blockchain isn‚Äôt a solution to all of your business problems.Like in any other project, you shouldn‚Äôt make critical technology decisions on hyped buzzwords.Instead you should focus on the business value it delivers.When we translate the blockchain characteristics to business values, it can potentially solve business problems in the following five key elements:  Transparency  Operation harmonization  Business continuity  Permanence  Security  DecentralizedWe‚Äôll discuss each element in detail and explain why blockchain technology can be an answer to that business problem.TransparencyIn a public blockchain network, by default every member of the ecosystem can access all transactions stored in the chain.They can even access smart contracts.An example of improved transparency is in the supply chain. Documenting a product‚Äôs journey across the supply chain reveals its true origin and touchpoints, which increases trust and helps eliminate the bias found in today‚Äôs opaque supply chains.Manufacturers can also reduce recalls by sharing logs with OEMs and regulators.Another potential use involves the recording of patents and intellectual property. Due to blockchain‚Äôs 100% transparency and its unforgeable nature, the information cannot be altered.Because transactions are easily trackable, it‚Äôs the perfect solution for recording ownership of patents and properties.  You can only achieve 100% transparency if you setup a public, permissionless blockchain network.In a consortium- or private blockchain network, you can define access rules to say which members can query certain information, which reduces its transparent nature.Operation harmonizationBecause business logic is implemented as smart contracts, and smart contracts are replicated over the different nodes that execute them, you have decentralized business logic.This allows you to use the same open source technology in all departments of your business.As a result, business processes are joint together, in contrast to Enterprise BPM, where business logic reuse is limited due to single enterprise data silos.Business continuityBy using blockchain technology, you have less dependency on a central infrastructure.That is because all nodes can execute transactions.When one node goes down, other nodes take over the processing.You can say that in a blockchain network, you have automatic failover.PermanenceWe already talked about the fact that activities in a blockchain cannot be undone.They are immutable.Because of this characteristic, there‚Äôs an audit trail of what happened in the system.You could say that this audit trail has a lot of similarities with the architectural pattern Event Sourcing.With Event Sourcing, all changes to application state are stored as a sequence of events.This is comparable to how transactions are stored in the blockchain.It could be interesting to combine both blockchain technology and Event Sourcing principles in a project.If you want to learn more about Event Sourcing, make sure to visit the following pages:  There‚Äôs an excellent article on Martin Fowler‚Äôs blog  Our colleague Yannick De Turck also has a chapter on Event Sourcing in his blogpost about Lagom  Ken Coenen has written about CQRS and Event Sourcing too after Ordina JWorks was present at DDD Europe back in 2016  Please note that you can only achieve full immutability if you setup a public, permissionless blockchain network.In a consortium or private blockchain network, transactions can be altered because you know the nodes that validate them.SecurityBlocks are timestamped and protected with cryptographic technology that is considered unbreakable.If a block is added it can‚Äôt be removed or altered.If you change a single bit of a transaction, the hash of this transaction will be completely different. So the merkle root hash (Merkle trees are explained in the section What is blockchain) won‚Äôt be the same, the nonce number will then be wrong and the block will be considered invalid.In this way transactions are secure once chained to the blockchain.The cryptographic technology works with the principal of public and private keys, but hashing is also a part of this technology.The private key is linked to the public key, but you cannot find out the private key if you have the public key. The private key allows you to verify that you are the owner of the public key. To make transactions, you‚Äôll need a unique key (private key) to make a digital signature to prove that you are the owner.The private key is stored in your wallet.  Your wallet doesn‚Äôt always need to contain money, it can also hold your identity.The network is also protected from (d)dos attacks because of the distributed nature of blockchain. If a hacker wants to take down the blockchain they would need to take down every node in the network. The proof-of-work can also help deter these attacks and spam because of the high costs of mining. Even if a hacker is able to penetrate one network and attempts to steal funds, there are multiple redundant copies of the same ledger stored around the world. If one is tampered with, the others could be used as a backup to prove what funds actually belong in each account.Existing platformsWe will now discuss a few platforms that can be used to set up a blockchain and also compare Bluemix and Azure.The first one is Ethereum, a public blockchain. Ethereum looks like the Bitcoin blockchain, but it uses Ether as the currency.It is faster than Bitcoin with a transaction taking seven seconds instead of ten minutes. We can also put smart contracts on the chain, with bitcoin you can only put transactions on there.Another big one we have is Hyperledger. This is a open source collaborative effort created by The Linux Foundation. Another big partner in Hyperledger is IBM because they helped them with development and donated some patents.Hyperledger is also more focused on private networks.The fun part is that you can run Hyperledger locally on your computer and try out the technology.That brings us to IBM Blockchain.IBM‚Äôs Bluemix platform focusses on private blockchains.It empowers businesses to digitize their transaction workflow through a highly secured, shared and replicated ledger.The current technology possibilities weren‚Äôt cutting it in terms of privacy so they added their code and patents to the Hyperledger project.The next one is Multichain. Multichain is an open source private blockchain, which is Bitcoin compatible.Next up is Openchain. Openchain is a little bit special because it doesn‚Äôt use the concept of blocks but the transactions are directly chained with one another, which makes it a lot faster.Openchain is an open source private blockchain. It also doesn‚Äôt use proof-of-work but proof-of-authority.BigChainDB is not really a complete blockchain but it is more a database with blockchain features like: decentralization, immutability, public/private and consensus.BigChainDB is also open source.Last but not least, we have Microsoft Azure blockchain left to discuss.As you may have guessed, Azure is the complete opposite of IBM‚Äôs Bluemix.Azure focuses on being public, although this does not mean they don‚Äôt believe in the private model.Microsoft has said that private networks will still be important for the commercial adaptation of the blockchain technology.Microsoft also don‚Äôt dedicate their platform to one type of technology like Hyperledger for Bluemix but they support many different technologies like Ethereum, Hyperledger and more. They do have a preference though for Ethereum because they joined the Enterprise Ethereum Alliance.  \t            Table: IBM is part of Hyperledger and Microsoft is part of Enterprise Ethereum Alliance.    Thinking decentralizedLast year, Ken Coenen gave a presentation about the popularity of APIs, and how companies team up to create innovative solutions.Data is freed from their silos and made available through APIs.It‚Äôs consumable for other departments and even other companies.However, when you think about it, all of this data is centralized and we need extra effort to expose it to other parties.When working with blockchain technology, your data is decentralized by nature.It‚Äôs funny when you think about it‚Ä¶Why do we want to store the data somewhere centralized in a silo and then make an extra effort to expose it?Isn‚Äôt is easier to start decentralized from the beginning and give access to the people who need it?What have we been doing all these years?We‚Äôll give an example.All applications implement their own user profile functionality.All of this user data - your profile information - is duplicated across many companies.It‚Äôs already a big improvement that applications allow you to use another platform‚Äôs credentials.Logging in with your Facebook or Google account is becoming a habit.This gives the end user a way to minimize his/her digital footprint.Don Tapscott explains this really well in summer 2016‚Äôs TED Talk How the blockchain is changing money and business.Of course, blockchain technology is still in its early stages.It‚Äôs not even sure whether the technology will last.Although these statements are purely hypothetical, we find much food for thought in them.  \t        ConclusionWhen talking with people about the possibilities of blockchain, it quickly becomes clear that we still have a long way to go.People aren‚Äôt waiting for yet another technological revolution.Instead, we need to start small.Blockchain and distributed ledger technology in general will have to evolve naturally.Blockchain solutions like IBM Blockchain or Microsoft Azure Blockchain-as-a-Service make the technology very accessible to companies in an early stage.We believe that a private blockchain network is the best way to start for a company because of the following reasons:  Throwing all your data at the world is still a very scary idea  You have to take all legal aspects into account (think of the EU‚Äôs new General Data Protection Regulation)  You can start small and expose some transactions by defining permissionsCompanies are starting to develop applications on their proprietary Bluemix- or Azure platform, without exposing everything to the outside world.Get inspired by visiting State of the Dapps.Recommended readingYou can read the following books if you like to get a grasp on possible use cases which can be implemented using blockchain technology.Please note that neither of these books will deep dive into the technical aspects.  Blockchain: Blueprint for a New Economy by Melanie Swan  Blockchain Revolution: How the Technology Behind Bitcoin Is Changing Money, Business, and the World by Don Tapscott, Alex Tapscott"
      },
    
      "conference-2017-05-04-saturn-html": {
        "title": "SATURN 2017",
        "url": "/conference/2017/05/04/saturn.html",
        "image": "/img/saturn/denver.jpg",
        "date": "04 May 2017",
        "category": "post, blog post, blog",
        "content": "SATURN is the leading international conference for software architecture practitioners who look beyond the details of today‚Äôs technologies to the underlying trends, techniques and principles that underpin lasting success in our fast-moving field. SATURN offers a unique mix to learn, exchange ideas, and find collaborators at the leading edge of modern software architecture practice.2017 marks the 13th edition of the SATURN conference, organised by the Software Engineering Institute of Carnegie Mellon University.This edition was held at the Hilton Denver Inverness in Englewood, Colorado, set against the backdrop of the majestic Rocky Mountains.Pragmatic Architecture, Today  I was honoured to talk about Pragmatic Archtecture at SATURN 2017. I had done this talk a couple of times at developer conferences, so I was very anxious to compare the feedback to certain statements at an architecture conference.As it turns out, feedback was (surprisingly) similar:  Architects nor developers are very fond of UML. Both emphasize the importance of communicating the architecture of a system to stakeholders, in a clear and understable manner.  Virtually all architects have a developer background and want to be actively involved in code.  Architects understand the necessity to play the political game. Developers might also understand this, but prefer not to participate in these, sometimes frustrating, discussions.In general, I am very happy with this feedback and it shows that the two communities can (and have to) really work together nicely.Software is details (Kevlin Henney)On the first day of SATURN 2017, Kevlin Henney presented a very interesting keynote on details in software stating that everything is a detail, depending on the level of abstraction of a certain problem.  Marissa Mayer  Geeks are people who love something so much that all the details matter.And these details can be very important.In Architecture, abstractions are necessary to focus on specific parts of a system.Modern software systems are often too complex to grasp all at once so we restrict our attenton to a small number of the software system‚Äôs structures.To avoid the obvious discrepancy between abstraction and detail, Kevlin quoted Tom Gilb stating that Architecture is a hypothesis, that needs to be proven by implementation and measurement.This illustrates the importance of collaborating as a team.We will need to make decisions, given our current understanding of a certain problem.Implementing these decisions, might lead to new beliefs and insights for the architecture. If a plot works out exactly as first planned, we might not be working loosely enough to give room to imagination and instincts.This encourages looking at things from more than one point of view.An indepth look at event sourcing with CQRS (Sebastian von Conrad)While Event Sourcing is not new, it never gained wide adoption and often is a source of confusion.However, now that microservices really have become mainstream, Event Sourcing is back on everyone‚Äôs radar.Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states.So event sourcing offers a different way of storing information, by expressing data as events.Sebastian went to great lengths to explain that this is how the real world works.  Sebastian von Conrad  Delete sucks.Event sourcing is commonly combined with the CQRS (Command and Query Responsibility Segregation) pattern by performing the data management tasks in response to events, and by materializing views from the stored events.Zooming in on CQRS, we can use the following definitions and best practices:Reading with Queries  Clients never query the events directly.  Clients query denormalised projections that are optimised for querying.  Projections are build with projectors that process the event stream.  Projectors are decoupled from each other and don‚Äôt share any state.  Projections are cheap and easy to build and rebuild.Writing with Commands  Clients never write the events directly.  Clients express an intent to do something via commands.  If replaying gets slow, performance can be improved by snapshots.  Commands are validated by Aggreggates, which is a concept borrowed from DDD.          Aggregates fetch events from the Event Store, and replay them to reconstitute their current state.      If the Aggregate accepts the Command, it results in an event.      Event Sourcing with CQRS  Event Sourcing makes you store business facts as the source of truth.  Event Sourcing makes the system deterministic.  CQRS and the Circular Architecture work well with Event Sourcing.  Asynchronous reactors process the event stream and react to events according to business logic, outputting more events.ConclusionEvent Sourcing potentionally brings a lot of value, but it‚Äôs important not to impose Event Sourcing on a team that:  Lacks buy-in to try it  Lacks stakeholder support  Lacks intestinal fortitude  A related talk at SATURN, by Paul Rayner, provided a way to visualise large scale complexity using EventStorming. In EventStorming, developers and business experts use sticky notes to map out an event-based story of how a software system behaves.He recommended Alberto Brandolini‚Äôs book on EventStorming, to learn more on the concept.Paul‚Äôs slides are available here.How to Gain Influence as a Software Architect (Adi Levin)As a software architect, you need to deal with people: it‚Äôs important to encourage collaboration.As such, a software architect will need leadership skills and will need to know where he or she is on the leadership journey.John C. Maxwell talks about this journey to great length in his book ‚ÄúThe five levels of leadership‚Äù.A nice summary is also available on his website.Adi shared a couple of really great tips:  Express your trust in people  Show your commitment          Never say I don‚Äôt care      Share responsiblity        Admit your mistakes  Let people know you understand them          Seek first to understand, then to be understood      Acknowledge people‚Äôs position      Encourage others to contribute to the design      Listen to people who disagree      "
      },
    
      "conference-2017-04-19-dockercon-linuxkit-and-moby-html": {
        "title": "DockerCon 2017: LinuxKit and Moby",
        "url": "/conference/2017/04/19/DockerCon-LinuxKit-And-Moby.html",
        "image": "/img/dockercon2017/thumbnail.jpg",
        "date": "19 Apr 2017",
        "category": "post, blog post, blog",
        "content": "Batteries included, but swappable.That has always been the philosophy of Docker.Since the incubation of Docker four years ago,the project has undergone many evolutions.Over the years, it has split up parts of Docker into smaller reusable components,which moved to their own projects.Docker, instead of being just one project,can now be considered a composition of multiple projects.We got runC, VPNKit, containerd, SwarmKit, InfraKit and so on.These projects are now used by many other projects other than Docker.Docker Inc. now open-sourced two new projects,called LinuxKit and The Moby ProjectLinuxKitLinuxKit is a toolkit to create small, lean Linux subsystems.The difference with other Linux distributions is the fact that you can create a distribution that only contains exactly what is needed.All system services are containers and can be removed or replaced at will.Docker partnered with multiple companies like Intel, HPE, ARM, IBM and Microsoft and the Linux Foundation,to create this new component.The minimal image size is only 35MB!These portable distributions can be used to run Linux on platforms that do not support Linux out-of-the-box.For example,LinuxKit is now being used to run Linux containers on Windows Server,using Hyper-V isolation techniques.This means you can run both Windows and Linux containers side-by-side,and create Linux/Windows hybrid clusters!The Moby ProjectUsers have been asking for the Docker-native experience on their favorite platform.These requests have not gone unheard.We received Docker for Mac, Docker for Windows, Docker for AWS and Docker for Azure.All these tools are built by composing the same open components that are used for Docker.Docker now, in total, has a library of over 80 containerized components.The problem here is,a lot of work is duplicated,to compose all these components together.Each project has its own assembly system.To fix this problem,Docker Inc. looked at the automotive industry,and copied the idea of common assemblies.Just like cars can be completely different,they can share the same chassis.And this is how the Moby project was born.It attempts to bring a set of standards and best practices together.Instead of spending months of work tying all these loose components together,you can now build a tool with the components you need within a few hours.Docker stays true to its battery philosophy.You can choose which version of the kernel you want,you can choose to use which components you want.It is all up to you!An example shown at DockerCon was ‚ÄúRedisOS‚Äù.They composed LinuxKit, containerd and Redis,and exported it to different formats that can run on Mac, Windows and GCP.Since these distributions are so small and portable,it is possible for companies to use this for IoT, cloud, desktop and many more platforms.You can find some examples in the LinuxKit examples repository.Trying it outI wanted to try it out myself,so I went to the LinuxKit Github repository and cloned it.$ git clone git@github.com:linuxkit/linuxkit.gitThen I ran make to build the moby tool:$ makeThis created the moby tool in the bin directory.I added this to my PATH:$ export PATH=$PATH:$(pwd)/binNow that I have the Moby tool available and on my PATH,I can build LinuxKit.Let‚Äôs take a look first at the YAML file that is used to build it.$ cat linuxkit.ymlkernel:  image: \"mobylinux/kernel:4.9.x\"  cmdline: \"console=ttyS0 console=tty0 page_poison=1\"init:  - linuxkit/init:42fe8cb1508b3afed39eb89821906e3cc7a70551  - mobylinux/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9  - linuxkit/containerd:60e2486a74c665ba4df57e561729aec20758daed  - mobylinux/ca-certificates:eabc5a6e59f05aa91529d80e9a595b85b046f935onboot:  - name: sysctl    image: \"mobylinux/sysctl:2cf2f9d5b4d314ba1bfc22b2fe931924af666d8c\"    net: host    pid: host    ipc: host    capabilities:     - CAP_SYS_ADMIN    readonly: true  - name: binfmt    image: \"linuxkit/binfmt:8881283ac627be1542811bd25c85e7782aebc692\"    binds:     - /proc/sys/fs/binfmt_misc:/binfmt_misc    readonly: true  - name: dhcpcd    image: \"linuxkit/dhcpcd:48e249ebef6a521eed886b3bce032db69fbb4afa\"    binds:     - /var:/var     - /tmp/etc:/etc    capabilities:     - CAP_NET_ADMIN     - CAP_NET_BIND_SERVICE     - CAP_NET_RAW    net: host    command: [\"/sbin/dhcpcd\", \"--nobackground\", \"-f\", \"/dhcpcd.conf\", \"-1\"]services:  - name: rngd    image: \"mobylinux/rngd:3dad6dd43270fa632ac031e99d1947f20b22eec9\"    capabilities:     - CAP_SYS_ADMIN    oomScoreAdj: -800    readonly: true  - name: nginx    image: \"nginx:alpine\"    capabilities:     - CAP_NET_BIND_SERVICE     - CAP_CHOWN     - CAP_SETUID     - CAP_SETGID     - CAP_DAC_OVERRIDE    net: hostfiles:  - path: etc/docker/daemon.json    contents: '{\"debug\": true}'trust:  image:    - mobylinux/kerneloutputs:  - format: kernel+initrd  - format: iso-bios  - format: iso-efiThis YAML file contains all the necessary information to create a distribution,that will be available in multiple formats.In this case,a random number generated and nginx are added as services.Let‚Äôs build it!$ moby build linuxkit.ymlExtract kernel image: mobylinux/kernel:4.9.xAdd init containers:Process init image: linuxkit/init:42fe8cb1508b3afed39eb89821906e3cc7a70551Process init image: mobylinux/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9Process init image: linuxkit/containerd:60e2486a74c665ba4df57e561729aec20758daedProcess init image: mobylinux/ca-certificates:eabc5a6e59f05aa91529d80e9a595b85b046f935Add onboot containers:  Create OCI config for mobylinux/sysctl:2cf2f9d5b4d314ba1bfc22b2fe931924af666d8c  Create OCI config for linuxkit/binfmt:8881283ac627be1542811bd25c85e7782aebc692  Create OCI config for linuxkit/dhcpcd:48e249ebef6a521eed886b3bce032db69fbb4afaAdd service containers:  Create OCI config for mobylinux/rngd:3dad6dd43270fa632ac031e99d1947f20b22eec9  Create OCI config for nginx:alpineAdd files:  etc/docker/daemon.jsonCreate outputs:  linuxkit-bzImage linuxkit-initrd.img linuxkit-cmdline  linuxkit.iso  linuxkit-efi.isoYou can see that a few init, onboot and service containers were added,and a configuration file was added for Docker.Finally,you can see the tool was outputted in multiple formats.Let‚Äôs try to run it:$ moby run linuxkit# ...# Lots of boot information# ...Welcome to LinuxKit                        ##         .                  ## ## ##        ==               ## ## ## ## ##    ===           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~           \\______ o           __/             \\    \\         __/              \\____\\_______// # [    2.464063] IPVS: Creating netns size=2104 id=1[    2.464434] IPVS: ftp: loaded support on port[0] = 21[    2.490221] tsc: Refined TSC clocksource calibration: 1993.943 MHz[    2.490613] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x397ba967053, max_idle_ns: 881590807276 ns[    2.713076] IPVS: Creating netns size=2104 id=2[    2.713560] IPVS: ftp: loaded support on port[0] = 21[    3.503395] clocksource: Switched to clocksource tsc/ #This image booted in just a few seconds!Now let‚Äôs see which service containers are running using runC:/ # runc listID          PID         STATUS      BUNDLE                        CREATED                        OWNERnginx       542         running     /run/containerd/linux/nginx   2017-04-19T12:34:42.1852841Z   rootrngd        601         running     /run/containerd/linux/rngd    2017-04-19T12:34:42.3200486Z   rootAs you can see,all the service containers are up and running.Within just a few minutes,I created a Linux distribution and got it up and running,with everything running in a container.If you would like to learn more about LinuxKit and the Moby Project,you can check out the following resources:  The Moby Project  Docker Blog: Anouncing LinuxKit  Docker Blog: Introducing Moby Project  GitHub: LinuxKit  GitHub: Moby"
      },
    
      "conference-2017-04-18-dockercon-multi-stage-builds-and-more-html": {
        "title": "DockerCon 2017: Multi-Stage Builds and More",
        "url": "/conference/2017/04/18/DockerCon-Multi-Stage-Builds-And-More.html",
        "image": "/img/dockercon2017/thumbnail.jpg",
        "date": "18 Apr 2017",
        "category": "post, blog post, blog",
        "content": "DockerCon 2017 has kicked off, and Docker has changed a great deal since last year‚Äôs edition.The authors of Docker have constantly stated that they wish to make the Docker experience as simple as possible.Nothing is less true if you look at some of the new features they released in the last few months,which are being presented now at DockerCon.I have compiled a list of the most useful changes and features.These will certainly help you when building your own Docker images!Multi-Stage BuildsBuilding an image is often done in multiple stages.First you compile your application.Then you run your tests.When the tests succeed, you package it into an artifact.Finally, you add this artifact to an image.You could put all these steps within one Dockerfile,but that would result into an image that is bloated with stuff that is not required for the final product, like the compilation and build frameworks.The Docker images would also be huge!A solution to this problem is to build the application outside of Docker,or to use multiple Dockerfiles.You can build the artifact with one build,extract the artifact,and use that artifact for a final build.However,this whole build process is often tied together with a script that has been hacked together,and does not truly feel like the Docker way of doing things.Docker has often been sceptical about adding new features or making changes to the Dockerfile syntax,but finally decided to tackle this build problem with a simple and elegant solution.Introducing multi-stage builds,it is now possible to define multiple stages by using several FROM statements.# First stage to build the applicationFROM maven:3.5.0-jdk-8-alpine AS build-envADD ./pom.xml pom.xmlADD ./src src/RUN mvn clean package# Final stage to define our minimal runtimeFROM FROM openjdk:8-jreCOPY --from=build-env target/app.jar app.jarRUN java -jar app.jarEach time FROM is used,you define which image is used for that stage,and in subsequent stages you can use the COPY --from=&lt;stage&gt; to copy artifacts from a previous stage.The final stage results in the image,which can contain the minimal runtime environment and the final artifact.Perfect!Using arguments in FROMUsing arguments isn‚Äôt a new thing with Dockerfiles.You could already use ARG statements to pass on arguments to the build process.These arguments are not persisted in the Dockerfile,and are frequently used to pass on versions,or secrets like SSH keys.Now it is also possible to use arguments in the version of the base image.ARG GO_VERSION=1.8FROM golang:${GO_VERSION}ADD . /srcWORKDIR /srcRUN go buildCMD [\"/bin/app\"]For above Dockerfile,I could build an image with another Go version!$ docker build --arg=GO_VERSION=1.7 .Cleaning up DockerA comment I often hear is that Docker takes a lot of space.This can be true,if you never clean up!Docker has added the docker system subcommand a while ago,You can use this subcommand to check the disk usage,and to free up space!The following command outputs the disk usage:$ docker system dfTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLEImages              7                   5                   1.247GB             769MB (61%)Containers          7                   2                   115.9MB             99.23MB (85%)Local Volumes       1                   1                   85.59MB             0B (0%)You can then use prune to clean up all resources that are no longer needed:$ docker system pruneWARNING! This will remove:\t- all stopped containers\t- all volumes not used by at least one container\t- all networks not used by at least one container\t- all dangling imagesAre you sure you want to continue? [y/N] yIt is also possible to prune certain subsystems:$ docker image/container/volume/network pruneWhich Ports?People often have trouble understanding or defining the published ports of a container,since the syntax can be confusing.Here‚Äôs a list of all possible formats that you can use to define which ports are published on a container:ports: - 3000 - 3000-3005 - 49100:22 - 9090-9091:8080-8081 - 127.0.0.1:8001:8080-8081 - 6060:7060/udpThis syntax is okay when using the CLI,but when you have to define a lot of them in a Compose file,it is no longer readable.To counter this issue,you can now use a more verbose format to define ports:ports:  - target: 6060    published: 7060    protocol: udpThis Volume Is Mounted Where?Just like the ports,volumes have a similar syntax.volumes:  - /var/lib/mysql  - /opt/data:/var/lib/mysql  - ./cache:/tmp/cached  - datavolume:/var/lib/mysql  - ~/configs/etc/configs/:roA verbose syntax has been added as well for volumes:volumes:  - type: bind    source: ~/configs    target: /etc/configs    read_only: trueTo be continuedThese few changes,especially the multi-stage builds,will certainly make your life as developer easier!I am curious what DockerCon has to offer more today and tomorrow!"
      },
    
      "architecture-2017-04-14-bredemeyer-html": {
        "title": "Bredemeyer: Architects Architecting Architecture",
        "url": "/architecture/2017/04/14/bredemeyer.html",
        "image": "/img/bredemeyer.jpg",
        "date": "14 Apr 2017",
        "category": "post, blog post, blog",
        "content": "Software architecture is getting a lot of attention.It looks beyond the details of today‚Äôs technologies to the underlying trends, techniques, and principles that underpin lasting success in our fast-moving field.It is critical to today‚Äôs business success; yet it requires technical, business and organizational talents and skills that warrant their own path of career development, education, and research.Earlier this month, I was lucky enough to participate in a four-day workshop organized by Bredemeyer Consulting in the Netherlands.The goal of the workshop: helping good architects become great architects.Bredemeyer Consulting intends to inspire and encourage architects to greatness, by helping them to visualize what is possible, and see how to get there.They aim to achieve this by providing the tools and techniques to help architects be successful.The training covers the essential technical tasks of architecting (architecture modeling and specification, architecture tradeoff analysis, component design and specification, etc.) as well as architectural leadership skills.Topics  Architecture  Strategy  Conceptual Architecture  Logical Architecture  Leadership1. ArchitectureWhatDefining architecture is never easy, as many different definitions are used by different organizations.Dana Bredemeyer defines architecture as a set of decisions that have multiple uses.These uses can span time, projects and places.Of course, this makes it easy to have too much or too little architecture.Finding the right balance, is difficult.To illustrate this difficulty, the following properties should be kept in mind when validating architectural decisions.An architectural description can be:  Good          Technically sound (eg. having well-defined interfaces)      Well-documented      Elegant        Right          A solution to the problem        Successful          When the system realizes value      It is up to the architect to make sure value is realized.HowDuring the architectural process, it is of extreme importance to look both at strategy and at implementation, as an architect generally moves between these two worlds.This process is comparable to the elevator approach from Gregor Hohpe.His book ‚Äú37 Things One Architect Knows About IT Transformation‚Äù is  approachable to read and contains a large number of useful tips for aspiring and seasoned architects.Most certainly a recommended read.To facilitate decision-making, Dana Bredemeyer suggests using a re-reentrant discovery activity.In this method, decisions are taken early (and written down) because at an early stage it is still cheap to change them.However, when decisions become expensive (eg. impacting implementation), they should be taken at the last possible, responsible moment.The re-entrant nature of this model, facilitates change, dialogue and consensus.It is very similar to OODA (Observe, Orient, Decide, Act) and PDCA (Plan, Decide, Check, Act), also known as the Deming cycle.PrinciplesTo kickstart an architectural description, it is useful to define a strong foundation with a set of principles.These principles must be clear, unambiguous and actionable.They can not conflict with each other and must be followed.The reason to define these principles early, is to provide confidence when solving hard problems and to constrain decisions that get made numerous times.A couple of examples:  Ebay  Bredemeyer2. StrategyFor an architecture to be successful, it must support the business strategy of the system and the organization.One might define architecture as the translation of strategy into technology.An approach to achieve this is:  Clarify the business concept(s)  Brainstorm (with business stakeholders to look at possible and alternative business concepts)  Define high-level requirements  Define high-level architecture (From Conceptual Architecture to Logical Architecture)  ValidateAn architect must understand what business is trying to achieve and understand what an organization needs to be good at to realize success (or what the system needs to be good at).From idea to strategyWhen business concepts are clear, but the strategy isn‚Äôt (fully) established, shaping strategy from business concepts can be achieved by:  Writing down business concepts as a bulleted list  Translating this bulleted list into plain text  Looking at the difference between the list and the textThis will, more often than not, refine the business concepts and identify what is strategically most important.Another way to refine business concepts, is using the Business Model Canvas.3. Conceptual ArchitectureThe goal of the Conceptual Architecture, is to define components (subsystems) and the interaction between these components.The Conceptual Architecture must remain high-level, because in this phase, the architect wants to explore alternatives.Adding too much detail to the Conceptual Architecture will become expensive.White board sketching can be a useful method to determine to Conceptual Architecture: talking with the business users next to a white board and drawing the system together.Dana shared a couple of tips and tricks to increase participation from business users:  Make the diagram a bit sloppy. This will prevent participants from thinking it‚Äôs finished.  Have fun. Let the ideas flow.  Use colors and icons. Make it visual.Another tool to formalize components are CRC-R Templates. These are typically half a page narratives that define a component, its responsibility, its collaboration with other components and the rationale behind its responsibilities.The Conceptual Architecture can be used to validate the feasibility of alternatives.For example by going over use-cases or by identifying if the system can be in a state that renders the architecture invalid.4. Logical ArchitectureThe Logical Architecture details out full responsibilities per component and all the interfaces per component.It adds precision, providing a detailed blueprint from which component developers and component users can work in relative independence.These components ought to be derived from the Conceptual Architecture.By selecting the core behavior of the components and defining the data that moves between components (eg. in a sequence diagram), the architecture becomes actionable and ready for implementation.In larger systems, the sequence diagram can aid a component owner in understanding how his component lives in the bigger system.When the state of the data (moving between components) evolves, it might also be useful to draw a state diagram.A very interesting attention point to creating a logical architecture is that the value of a system is not the sum of its parts, but the sum of the interaction between the parts.This is beautifully explained by Russell Ackoff in this video on YouTube.5. LeadershipBeing successful as a leader often depends on the ability to influence others: getting from a lot of (possibly good) ideas to a shared vision: a philosophical harmony of values.Getting this buy-in from stakeholders only strengthens the importance of interaction and collaboration.The activities of a leader range from inspiring, mentoring, listening and setting directions.Settings directions means formally defining what a system must do or what a strategy wants to achieve.When a leader empowers his (or hers) team, trust will be established and more value will be realized.Passion and Discipline: Don Quixote‚Äôs Lessons for LeadershipWhy Don Quixote?What lessons can we learn from the fictional 16th-century gentleman who careered around the Spanish countryside tilting at windmills and challenging sheep to battle?  James G. March  We live in a world that emphasizes realistic expectations and clear successes.Quixote had neither.But through failure after failure, he persists in his vision and his commitment.He persists because he knows who he is. The critical concerns of leadership are not technical questions of management or power, they are fundamental issues of life.  ‚Äì Source: Insights by Stanford BusinessBelieving in something can make others believe in it."
      },
    
      "angular-2017-04-04-optimising-performance-of-your-enterprise-angular-application-html": {
        "title": "Optimising performance of your Enterprise Angular Application",
        "url": "/angular/2017/04/04/optimising-performance-of-your-enterprise-angular-application.html",
        "image": "/img/optimising-performance-of-your-enterprise-angular-application.png",
        "date": "04 Apr 2017",
        "category": "post, blog post, blog",
        "content": "This blog post contains best practices that helped us optimise performance of our Enterprise Angular (v2+) Application we created for one of our clients.The project has been created in under 6 months with a dedicated team of 7 people of which 4 people are from the JWorks unit (2 front-end, 2 backend) and consists of two Angular Applications that use modules and components from a shared library.The use of Angular Universal does not apply (yet) for this project.Topics  Lazy loading  Code splitting and commons chunk plugin (webpack)  ChangeDetectionStrategy: OnPush  Reusable CSS with BEM and Sass  GZIP  AOT1. Lazy loadingLazy loading your project modules can greatly enhance performance.After each successful navigation, the router looks in its configuration for an unloaded module that it can preload.Whether it preloads a module, and which modules it preloads, depends upon the preload strategy.The Router offers two preloading strategies out of the box:  No preloading at all which is the default. Lazy loaded feature areas are still loaded on demand.  Preloading of all lazy loaded feature areas.We implemented the PreloadAllModules strategy in its default configuration, but know that it is possible to create your own custom preloading strategy.To do so, include the preloadingStrategy in your @NgModule like so:@NgModule({    ...    imports: [        RouterModule.forRoot(ROUTES, { preloadingStrategy: PreloadAllModules })    ]});And define your routes like this:{    path: 'performance',    loadChildren: 'performance.module#PerformanceModule',    canLoad: [AuthGuard] // Optional}Note that when using guards, the CanLoad guard blocks loading of feature module assets until authorised to do so.If you want to both preload a module and guard against unauthorised access, use the CanActivate guard instead.Want to get started with lazy loading?Maybe create a custom preloading strategy?Check out the talk Manfred Steyer gave at NG-BE 2016 about improving start-up performance with lazy loading or view the Angular docs.2. Code splitting and commons chunk plugin (webpack)Code splitting is one of the most compelling features of webpack.It allows you to split your code into various bundles which you can then load on demand ‚Äî like when a user navigates to a matching route, or on an event from the user.This allows for smaller bundles, and allows you to control resource load prioritization, which, if used correctly, can have a major impact on your application load time.There are mainly two kinds of code splitting that can be accomplished with webpack: ‚ÄúVendor code splitting‚Äù and ‚ÄúOn demand code-splitting‚Äù (used for lazy loading).The CommonsChunkPlugin is an opt-in feature that creates a separate file (known as a chunk), consisting of common modules shared between multiple entry points.By separating common modules from bundles, the resulting chunked file can be loaded once initially, and stored in cache for later use.This results in pagespeed optimisations as the browser can quickly serve the shared code from cache, rather than being forced to load a larger bundle whenever a new page is visited.Among other optimisations the extra async commons chunk allows us to drastically improve performance by moving common modules out of the parent so that a new async-loaded additional commons chunk is used, which decreases initial load time.This is automatically downloaded in parallel when the additional chunk is downloaded.new webpack.optimize.CommonsChunkPlugin({  children: true,  // (use all children of the chunk)  async: true,  // (create an async commons chunk)});3. ChangeDetectionStrategy: OnPush3.1 The problemUnlike using a Virtual DOM, like ReactJS, Angular uses change detection to update the actual DOM presented to the user.Each component in an Angular application has its own change detector and in order to guarantee the latest data is always presented to the user, the default change detection strategy on an Angular component is set to always update.This means that any time JavaScript finishes executing, Angular will check for changes in all components.This usually works fast in small applications.However, when a component has a large subset of components (e.g.: a list with several items in which every row is presented by a component), performance may take a hit, even when (almost) nothing changes.The reason is that, due to the default change detection, Angular will also check for updates on a component when a change occurs on its siblings or ancestors or child components, while this is not needed in most cases.3.2 The solutionNext to trying to have less DOM, the solution is to use the OnPush strategy for change detection.The OnPush strategy will let the change detector run only in the following situations:  when an event handler is fired in the component  when one of its input properties changes  when you manually request the change detector to look for changes (using ChangeDetectorRef‚Äôs function markForCheck())  when a child‚Äôs change detector runs3.3 Setting up OnPushThere are two ways to set up the OnPush strategy3.3.1. Immutable input objectsThe simplest way is to use only immutable objects.   import { Component, Input, ChangeDetectionStrategy } from '@angular/core';   @Component({       selector: 'my-sub-component',       template: `{{ item.name }}`,       changeDetection: ChangeDetectionStrategy.OnPush   })   export class MySubComponent implements OnInit {       @Input() item: {name: string};       constructor() {}   }   The change detector will only run when the input property ‚Äòitem‚Äô changes.   The key here is to update the reference to the object.   The change detector won‚Äôt run when something inside the object (e.g.: property ‚Äòname‚Äô) changes.3.3.2. Observable input objectsAnother way is to use observables as inputs.   import { Component, Input, ChangeDetectionStrategy, ChangeDetectorRef } from '@angular/core';   @Component({       selector: 'my-sub-component',       template: `{{ myItemName }}`,       changeDetection: ChangeDetectionStrategy.OnPush   })   export class MySubComponent implements OnInit {       @Input() itemStream:Observable&lt;any&gt;;       myItemName: string;       constructor(private changeDetectorRef: ChangeDetectorRef) {}       ngOnInit() {           this.itemStream.subscribe(i =&gt; {               this.myItemName = item.name;               this.changeDetectorRef.markForCheck();           });       }   }   The change detector will run when the itemStream emits a new item.As change detection is run from top to bottom components, start by setting OnPush on the leaf components and work your way up.This allows to skip change detection in entire subtrees.4. Reusable CSS with BEM and SassSass has been an all-time favorite for writing structured and maintainable CSS for large projects.We combined this with the BEM methodology which helps to create extendable and reusable interface components.We used this approach on our project to create a style guide in the shared module that includes all working components used in the application.Once most of the components were available we could simply start including them in the modules that needed to be built.This greatly decreased the time needed to build the functional module.Things like colors, typography, utilities, etc. are bundled in separate files that can be included where needed.This prevents writing the same CSS over and over again and keeps the code base small(er).5. GZIPGzip is a file format and also a method of compressing files (making them smaller) for faster network transfers.It allows your web server to provide files with a smaller size that will be loaded faster by your browser.Compression of your files with gzip typically saves around fifty to seventy percent of the file size.You can easily enable gzip compression on your server by editing your .htaccess file:#Set to gzip all outputSetOutputFilter DEFLATE#exclude the following file typesSetEnvIfNoCase Request_URI \\.(?:exe|t?gz|zip|iso|tar|bz2|sit|rar|png|jpg|gif|jpeg|flv|swf|mp3)$ no-gzip dont-vary#include the following file typesAddType x-font/otf .otfAddType x-font/ttf .ttfAddType x-font/eot .eotAddType image/x-icon .icoAddType image/png .pngAddType image/svg+xml .svgAddOutputFilterByType DEFLATE text/plainAddOutputFilterByType DEFLATE text/htmlAddOutputFilterByType DEFLATE text/xmlAddOutputFilterByType DEFLATE text/cssAddOutputFilterByType DEFLATE application/xmlAddOutputFilterByType DEFLATE application/xhtml+xmlAddOutputFilterByType DEFLATE application/rss+xmlAddOutputFilterByType DEFLATE application/javascriptAddOutputFilterByType DEFLATE application/x-javascriptAddOutputFilterByType DEFLATE image/svg+xml#set compression levelDeflateCompressionLevel 9#Handle browser specific compression requirementsBrowserMatch ^Mozilla/4 gzip-only-text/htmlBrowserMatch ^Mozilla/4.0[678] no-gzipBrowserMatch bMSIE !no-gzip !gzip-only-text/html# Make sure proxies don't deliver the wrong contentHeader append Vary User-Agent env=!dont-vary6. AOTAt the time of writing, the application still runs using the just-in-time (JIT) compiler.But we are looking into how we can integrate AOT.JIT compilation incurs a runtime performance penalty.Views take longer to render because of the in-browser compilation step.The application is bigger because it includes the Angular compiler and a lot of library code that the application won‚Äôt actually need.Bigger apps take longer to transmit and are slower to load.Compilation can uncover many component-template binding errors.JIT compilation discovers them at runtime, which is late in the process.The ahead-of-time (AOT) compiler can catch template errors early and improve performance by compiling at build time.AOT ensures  Faster rendering  Fewer asynchronous requests  Smaller Angular framework download size  Earlier detection of template errors  Better securityConclusionWhile Angular states it‚Äôs performance driven out of the box, it is very important to optimise, where possible, especially when building a large Enterprise Angular Application.As you can see it‚Äôs not that difficult to integrate, so why wouldn‚Äôt you?Every bit of data that doesn‚Äôt end up downloading to the device of your users is a bless.Hat tip: Try to integrate these changes when setting up your project.It would be a shame to end up refactoring your code when halfway into development."
      },
    
      "ionic-2017-02-02-ordina-becomes-ionic-trusted-partner-html": {
        "title": "Ordina becomes Ionic Trusted Partner",
        "url": "/ionic/2017/02/02/Ordina-becomes-Ionic-Trusted-Partner.html",
        "image": "/img/ordina-becomes-ionic-trusted-partner.png",
        "date": "02 Feb 2017",
        "category": "post, blog post, blog",
        "content": "Ordina becomes Ionic Trusted PartnerWithin the JWorks unit of Ordina, Jan De Wilde has been following Ionic framework since 2014.Jan goes way back in the world of frontend development and Ionic allowed him to use his knowledge of JavaScript, HTML and CSS to create hybrid mobile applications.Along the way of experimenting, evangelizing Ionic within the unit and promoting Ionic at our clients we have had the opportunity to build some amazing Ionic (1 &amp; 2) applications for our clients.We strongly believe that Hybrid and Progressive Web Apps are the future and keep investing time in getting better in it and giving training to our employees.Some time ago Ionic opened up the Trusted Partner Program and after applying with a motivational letter and a description of projects we did for our clients, we have been selected as a Trusted Partner.  What does Ionic say about Trusted Partners?  Ionic Trusted Partners are certified consulting agencies that we connect with businesses looking to jumpstart their Ionic app development.  ‚Äì Source: https://ionic.io/trusted-partners  Currently we work with both Ionic 1 and Ionic 2.We use technologies such as:  Firebase for realtime communication and data synchronization,  TypeScript and Webpack to organize our code,  and Karma, Jasmine and Protractor for testing.As mobile devices are increasingly used in almost all organisations, it is important for businesses to better protect these systems through Enterprise Mobility Management.By pushing Custom Device Policies to mobile devices via the EMM platform, administrators can control how these devices should behave within the organization, taking into account the already existing security rules.This can reduce the risk of data loss, unauthorized access and unapproved software installs on mobile devices with access to the company network.Mobile security is not just something for very large enterprises, but is relevant to all types of businesses.So it is important to take this into account from the very beginning.Want to work with us for your next mobile project? Or train your people to use Ionic?Contact Jan De Wilde.A selection* of our projectsProximus MyThings LoRa IoT PlatformFor Proximus, a big telecom operator in Belgium we have created a portal to manage LoRa connected IoT sensors and an Ionic 1 application for the field engineers to onboard and configure sensors. First part of the application allows the field engineer to identify and install/onboard a sensor using a QR code scanner or via manual input of the MAC address.Sensor type, company, location with lat/lng detection is provided in the application.Second part of the application is the ability to adjust or replace an existing sensor. Third part is a view where active/inactive sensors are presented together with their properties and last sent data. All data is exposed using web services created in Spring Boot and data is stored in a Mongo DB cluster and MySQL instance.This application is publicly available in the app store, but only accessible for field engineers.  Arcelor Mittal slab quality applicationFor Arcelor Mittal a steel processing company in Belgium we have created an application in Ionic 2 for quality assurance of the steel slabs that need to be processed. The application allows the quality assurer to look up a slab using a unique slab ID and check different properties such as: measurement, removal of edges, scarfing and cutting.The application integrates with web services provided by Arcelor Mittal and is privalely hosted by Arcelor Mittal.  4411 Parking Application for the city of CharleroiFor the city of Charleroi we created an Ionic 1 application that allows the parking guard to identify which vehicles are not parked according to their neighborhood subscription.The parking guard needs to identify himself in order to look up license plate numbers in the zone they are active.As a additional POC we integrated with a service that enabled the parking guard to take pictures of a parked car to identify the license plate number, car type, color, etc.This application is privately hosted by 4411.*Due to copyright, we are not able to display all Ionic projects."
      },
    
      "microservices-2017-02-01-lagom-1-2-html": {
        "title": "Lagom 1.2: What's new?",
        "url": "/microservices/2017/02/01/Lagom-1-2.html",
        "image": "/img/lagom.png",
        "date": "01 Feb 2017",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Looking back at the first version  Lagom 1.2  Message Broker API &amp; Kafka implementation  Generic read-side support, sharding support and automatic offset handling  JDBC support  Migrating from 1.0 to 1.2  Looking at the rest of our initial feedback  Lagom 1.3 preview  Conclusion  Extra resourcesLooking back at the first versionShortly after the first MVP version of Lagom was released we wrote a blogpost in which we wrote down our first impressions and in which we did an initial comparison to Spring Cloud and Netflix OSS.We also did an introduction presentation on Lagom which is available on YouTube.Lightbend let us know that they really appreciated our feedback.They definitely understood some of our remarks and suggestions and they were willing to work on some of them.One of our majors remarks was that Maven support should be added to Lagom in order to properly target Java developers.We weren‚Äôt the only ones with this feedback and Lightbend took note of it.In September 2016, Lightbend released Lagom 1.1. In this first new minor version they introduced Maven support which also includes support for running the Lagom development environment in Maven.More information about setting up a Lagom project in Maven is available in the documentation.A few months later they released Lagom 1.2 with a couple of new features which is what this blogpost will be about.We will also revisit our initial comparison against Spring Cloud and Netflix OSS.Lagom 1.2In this new minor version of Lagom, the read-side has been overhauled.Other notable additions are the following:  Message Broker API &amp; Kafka implementation  JDBC support  Generic read-side support, sharding support and automatic offset handlingNaturally, at the same time, a couple of existing issues and bugs were also resolved. A full list is available on Github.Message Broker API &amp; Kafka implementationThe introduction of message broker support is the most notable feature of Lagom 1.2.By adding message broker support, Lagom now allows both direct streaming of messages between services but also through a broker.With version 1.2, Lagom comes with out-of-the-box support for Apache Kafka, a very popular scalable message broker for building real-time data pipelines and streaming application.The message broker API has been designed to be independent of any backend meaning that support for other brokers may be added in the future.Compared to the existing Publish-Subscribe mechanism where messaging is only available intra-service, message broker communication happens between one service to many other services.Another key difference between the two is that with Publish-Subscribe it is possible that messages might get lost, for example due to network issues or a restart of a service, message broker based communication can provide at-least-once and at-most-once delivery semantics even if the subscriber is down.In Publish-Subscribe messaging, a subscriber will only receive a message after its subscription has been accepted by the Publish-Subscribe infrastructure.The message broker on the other hand will allow the subscriber to consume all the messages since the last message it has consumed, even if the subscriber was offline or down, thanks to its decoupling of services producing events from others consuming them.The Publish-Subscribe mechanism in Lagom is provided by the Akka Cluster underneath a Lagom service whereas message broker support is provided by a third party product such as Kafka.Lagom takes care of publishing, partitioning, consuming and failure handling of messaging and when executing the runAll command, Lagom automatically runs a Kafka server with you along with Zookeeper for you.Next to ServiceCall for communicating with other services mapping down onto HTTP, there now exists a Topic abstraction that represents a topic that one service publishes and that other services can consume after subscribing.In order to make use of it you need to add the Lagom Kafka Broker module to the dependencies for both the service publishing to a topic and the service subscribing to a topic.Note that Lagom Kafka Broker module requires an implementation of Lagom Persistence so you need to add either Lagom Persistence Cassandra or Lagom Persistence JDBC to the dependencies.In our simple demo project Lagom Shop we define a new method in the item-api project‚Äôs ItemService that will return a Topic of item creation events to subscribe on and we also add it to the descriptor:Topic&lt;ItemEvent&gt; createdItemsTopic();@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").withCalls(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).publishing(            Service.topic(\"createdItems\", this::createdItemsTopic)    ).withAutoAcl(true);}In the item-api project we define a new ItemEvent interface for external use in other services.An ItemEvent already exists in the item-impl project but you want this one to be solely used within the implementation project.It is considered a best practice to have a separate definition for external use as it would otherwise cause you to break clients if you would apply internal changes.@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\", defaultImpl = Void.class)@JsonSubTypes({        @JsonSubTypes.Type(ItemEvent.ItemCreated.class)})public interface ItemEvent {    UUID getId();    @JsonTypeName(\"item-created\")    final class ItemCreated implements ItemEvent {        private final UUID id;        private final String name;        private final BigDecimal price;            @JsonCreator        public ItemCreated(UUID id, String name, BigDecimal price) {            this.id = id;            this.name = name;            this.price = price;        }            @Override        public UUID getId() {...}            public String getName() {...}            public BigDecimal getPrice() {...}            @Override        public boolean equals(Object o) {...}            @Override        public int hashCode() {...}            @Override        public String toString() {...}    }}In the item-impl project we add the implementation to ItemServiceImpl that will publish all ItemCreated events to the topic:@Overridepublic Topic&lt;be.yannickdeturck.lagomshop.item.api.ItemEvent&gt; createdItemsTopic() {    return TopicProducer.singleStreamWithOffset(offset -&gt; {        return persistentEntities                .eventStream(ItemEventTag.INSTANCE, offset)                .filter(eventOffSet -&gt; eventOffSet.first() instanceof ItemCreated)                .map(this::convertItem);    });}private Pair&lt;be.yannickdeturck.lagomshop.item.api.ItemEvent, Offset&gt; convertItem(Pair&lt;ItemEvent, Offset&gt; pair) {    Item item = ((ItemCreated)pair.first()).getItem();    logger.info(\"Converting ItemEvent\" + item);    return new Pair&lt;&gt;(new be.yannickdeturck.lagomshop.item.api.ItemEvent.ItemCreated(item.getId(), item.getName(),            item.getPrice()), pair.second());}We now want to make use of this in our order-impl project.Using the injected ItemService instance we can now subscribe on the topic and act on each message.In this case we log something whenever we receive a new message.@Injectpublic OrderServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {    ...    itemService.createdItemsTopic()            .subscribe()            .atLeastOnce(Flow.fromFunction((be.yannickdeturck.lagomshop.item.api.ItemEvent item) -&gt; {                logger.info(\"Subscriber: doing something with the created item \" + item);                return Done.getInstance();            }));}As mentioned earlier, you have the option to make use of either the atLeastOnce or atMostOnceSource delivery semantic on the Subscriber instance.If we run the application, create an item and afterwards check the logs, we see that the order service is receiving messages:2016-12-23 22:04:22,337 INFO  b.y.l.i.i.ItemServiceImpl - Creating item: CreateItemRequest{name=newItem, price=15}2016-12-23 22:04:22,349 INFO  b.y.l.i.i.ItemEntity - Setting up initialBehaviour with snapshotState = Optional.empty2016-12-23 22:04:22,357 INFO  b.y.l.i.i.ItemEntity - Processed CreateItem command into ItemCreated event ItemCreated{item=Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}, timestamp=2016-12-23T21:04:22.357Z}2016-12-23 22:04:22,359 INFO  b.y.l.i.i.ItemEntity - Processed ItemCreated event, updated item state2016-12-23 22:04:22,412 INFO  b.y.l.i.i.ItemEntity - Processed GetItem command, returned item2016-12-23 22:04:22,413 INFO  b.y.l.i.i.ItemServiceImpl - Looking up item d370993c-dd75-4a88-bf8b-d9dba1820feb2016-12-23 22:04:25,472 INFO  b.y.l.i.i.ItemEventProcessor - Persisted Item Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}2016-12-23 22:04:25,776 INFO  b.y.l.i.i.ItemServiceImpl - Converting ItemEventItem{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}2016-12-23 22:04:25,883 INFO  b.y.l.o.i.OrderServiceImpl - Subscriber: doing something with the created item ItemCreated{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name='newItem', price=15}Generic read-side support, sharding support and automatic offset handlingUp until now, the read-side processor API was specific to Cassandra and required you to do the necessary offset tracking.This existing API, while still usable, has been declared depricated and instead a new specific implementation for constructing Cassandra read-sides and a more generic one for JDBC read-sides have been added.The read-side can now also be sharded by tagging persistent entity events with sharded tags.Instead of declaring only one tag, read-side processors now declare a list of them.The processing of these tags across the cluster is handled for you by Lagom.To compare the two of them, here is a sample of using a single tag:public interface ItemEvent extends Jsonable, AggregateEvent&lt;ItemEvent&gt; {        AggregateEventTag&lt;ItemEvent&gt; TAG = AggregateEventTag.of(ItemEvent.class);        @Override    default AggregateEventTag&lt;ItemEvent&gt; aggregateTag() {        return TAG;    }}And an example of using sharded tags:public interface ItemEvent extends Jsonable, AggregateEvent&lt;ItemEvent&gt; {    int NUM_SHARDS = 20;    AggregateEventShards&lt;ItemEvent&gt; TAG = AggregateEventTag.sharded(ItemEvent.class, NUM_SHARDS);    @Override    default AggregateEventShards&lt;ItemEvent&gt; aggregateTag() {        return TAG;    }}In your ReadSideProcessor you will have to override the aggregateTags() abstract method differently.When using a single tag:@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {    return TreePVector.singleton(ItemEvent.TAG);}And when using sharded tags:@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {  return ItemEvent.TAG.allTags();}Lagom now also provides automatic offset tracking, which until now, you had to do yourself in your read-side processors by explicitly loading and persisting offsets.This allows us to get rid of quite a bit of code and as less code means less bugs, this is definitely a good thing.In the section dealing with migrating to Lagom 1.2 there is a part that shows the code that we got rid of.That the existing Cassandra read-side support API still exists but it has been deprecated and might be removed in an upcoming version so it is a good idea to migrate as soon as possible.It isn‚Äôt really that much work to migrate to the new API, the exact work required is described in the Migrating from 1.0 to 1.2 section.JDBC supportJDBC support has been added to ease the introduction of Lagom into the existing organisation of potential users in order to provide support for using their existing relational database infrastructure.In Lagom 1.3.0, support for JPA will also be added which should become the preferred choice.A relational database is less preferred when setting up a non-blocking and reactive architecture but by providing support for it, it would allow potential users to start making use of the Persistent Entity API without the necessity of having to switch over to Cassandra.To make use of JDBC support you need to add the Persistence JDBC module to your project while also having to add the jar for your JDBC driver.Lagom makes use of akka-persistence-jdbc to persist entities to the database.At the time of writing only four relational databases are supported:  PostgreSQL  MySQL  Oracle  H2akka-persistence-jdbc uses Slick for mapping tables and managing asynchronous execution of JDBC calls.Slick requires you to configure it to use the right Slick profile for your database.An example of a Slick configuration in application.conf:db.default {  driver = \"org.postgresql.Driver\"  url = \"jdbc:postgresql://database.example.com/playdb\"}jdbc-defaults.slick.driver = \"slick.driver.PostgresDriver$\"A table and journal table are required by the akka-persistence-jdbc plugin and by default, Lagom is able to create these automatically for you.This automatic generation functionality can be disabled which you will probably want for any environment higher than development.lagom.persistence.jdbc.create-tables.auto = falseThe definition of the tables differ for each database, here is an example of the table definition for PostgreSQL:DROP TABLE IF EXISTS public.journal;CREATE TABLE IF NOT EXISTS public.journal (  ordering BIGSERIAL,  persistence_id VARCHAR(255) NOT NULL,  sequence_number BIGINT NOT NULL,  deleted BOOLEAN DEFAULT FALSE,  tags VARCHAR(255) DEFAULT NULL,  message BYTEA NOT NULL,  PRIMARY KEY(persistence_id, sequence_number));DROP TABLE IF EXISTS public.snapshot;CREATE TABLE IF NOT EXISTS public.snapshot (  persistence_id VARCHAR(255) NOT NULL,  sequence_number BIGINT NOT NULL,  created BIGINT NOT NULL,  snapshot BYTEA NOT NULL,  PRIMARY KEY(persistence_id, sequence_number));The definition scripts for each database are available here.If you are unaware of CQRS, Event Sourcing and the Persistent Read-Side in Lagom you could take a look at the CQRS and Event Sourcing section in our previous blogpost on Lagom.It might be a bit out of date regarding the API in Lagom but it should give you an idea.The new API for Cassandra read-side support, while very similar compared to the existing API, still differs in a few places.In the upcoming section we describe the migration process of upgrading to Lagom 1.2 and the changes we had to do to our read-side, so the required code changes can be read in that section.The API for the JDBC read-side support is rather similar compared to the Cassandra read-side support.Instead of using a CassandraSession for querying, you use a JdbcSession to retrieve a connection which in turn you will use for the execution of queries.The JDBC read-side API however, unlike the Cassandra read-side API, is not fully non-blocking which will lead to a performance difference.It could be an option to switch to Cassandra later on in the project if you want to get better performance out of it.But at least by having an API right now for these four databases it might be easier to integrate a new Lagom application within an existing architecture having these kinds of databases.Migrating from 1.0 to 1.2A migration guide is available with the steps necessary to upgrade your project to Lagom 1.2.At Ordina Belgium we streamed and recorded an introduction video on Lagom 1.0 in which we demoed a shop application.For the purpose of this blogpost, let us see how much work it is to upgrade to 1.2.First of all, we have to upgrade the Lagom version itself. Lagom 1.0 only supported sbt, so our demo is still using that.We change the version to be used in project/plugins.sbt:addSbtPlugin(\"com.lightbend.lagom\" % \"lagom-sbt-plugin\" % \"1.2.2\")For the Lagom Persistence module, Cassandra support has been pulled into its own module so you need to update the lagomJavadslPersistence dependency to lagomJavadslPersistenceCassandra in the build.sbt file.We also have to update the Scala version in build.sbt:scalaVersion in ThisBuild := \"2.11.8\"The ConductR version in project/plugins.sbt:addSbtPlugin(\"com.lightbend.conductr\" % \"sbt-conductr\" % \"2.1.16\")The descriptor() in the service interfaces needs to be updated since the .with(...) has been replaced by withCalls(...).We replace the existing code:@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").with(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).withAutoAcl(true);}With the following:@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").withCalls(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).withAutoAcl(true);}These are the necessary changes for us to successfully compile our project. Subsequently, we need to update our read-sides to make use of the new API.Starting with replacing the deprecated CassandraReadSideProcessor:public class ItemEventProcessor extends CassandraReadSideProcessor&lt;ItemEvent&gt; {With ReadSideProcessor:public class ItemEventProcessor extends ReadSideProcessor&lt;ItemEvent&gt; {Next step is to inject an instance of CassandraSession and CassandraReadSide via the constructor:private final CassandraSession session;private final CassandraReadSide readSide;@Injectpublic ItemEventProcessor(CassandraSession session, CassandraReadSide readSide) {    this.session = session;    this.readSide = readSide;}All code related to handling offsets can be deleted since Lagom now handles this for us.We delete the following:private PreparedStatement writeOffset = null; // initialized in prepareprivate void setWriteOffset(PreparedStatement writeOffset) {    this.writeOffset = writeOffset;}private CompletionStage&lt;Done&gt; prepareWriteOffset(CassandraSession session) {    logger.info(\"Inserting into read-side table item_offset...\");    return session.prepare(\"INSERT INTO item_offset (partition, offset) VALUES (1, ?)\").thenApply(ps -&gt; {        setWriteOffset(ps);        return Done.getInstance();    });}private CompletionStage&lt;Optional&lt;UUID&gt;&gt; selectOffset(CassandraSession session) {    logger.info(\"Looking up item_offset\");    return session.selectOne(\"SELECT offset FROM item_offset\")            .thenApply(                    optionalRow -&gt; optionalRow.map(r -&gt; r.getUUID(\"offset\")));}After having our ItemEventProcessor class extend from the ReadSideProcessor abstract we are prompted to implement two methods: buildHandler() and aggregateTags().aggregateTags() simply replaces aggregateTag() where as buildHandler() will contain the setup needed for our ReadSideHandler.The prepare(CassandraSession session) and defineEventHandlers(EventHandlersBuilder builder) methods that used to be overridden are now both implemented in buildHandler().The logic from the existing prepare() is split up into a setGlobalPrepare(), for creating Cassandra tables (note that these tasks should be idempotent), and a prepare(), for preparing statements, in buildHandler().We start by deleting the old prepare(CassandraSession session) and the defineEventHandlers(EventHandlersBuilder builder):@Overridepublic CompletionStage&lt;Optional&lt;UUID&gt;&gt; prepare(CassandraSession session) {    return            prepareCreateTables(session).thenCompose(a -&gt;                    prepareWriteOrder(session).thenCompose(b -&gt;                            prepareWriteOffset(session).thenCompose(c -&gt;                                    selectOffset(session))));}@Overridepublic EventHandlers defineEventHandlers(EventHandlersBuilder builder) {    logger.info(\"Setting up read-side event handlers...\");    builder.setEventHandler(ItemCreated.class, this::processItemCreated);    return builder.build();}Finally we perform the necessary refactoring to implement both buildHandler() and aggregateTags(), and we clean up the existing processing logic for the read-side:private CompletionStage&lt;List&lt;BoundStatement&gt;&gt; processItemCreated(ItemCreated event) {    BoundStatement bindWriteItem = writeItem.bind();    bindWriteItem.setUUID(\"itemId\", event.getItem().getId());    bindWriteItem.setString(\"name\", event.getItem().getName());    bindWriteItem.setDecimal(\"price\", event.getItem().getPrice());    logger.info(\"Persisted Item {}\", event.getItem());    return CassandraReadSide.completedStatements(Arrays.asList(bindWriteItem));}@Overridepublic ReadSideHandler&lt;ItemEvent&gt; buildHandler() {    CassandraReadSide.ReadSideHandlerBuilder&lt;ItemEvent&gt; builder = readSide.builder(\"item_offset\");    builder.setGlobalPrepare(() -&gt; prepareCreateTables(session));    builder.setPrepare(tag -&gt; prepareWriteItem(session));    logger.info(\"Setting up read-side event handlers...\");    builder.setEventHandler(ItemCreated.class, this::processItemCreated);    return builder.build();}@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {    return TreePVector.singleton(ItemEventTag.INSTANCE);}This leaves us with some refactoring to be done in our service implementations for registering the read-side.The sole thing that needs to be done is replacing the injected CassandraReadSide, which is now deprecated, with ReadSide.So going from:@Injectpublic ItemServiceImpl(PersistentEntityRegistry persistentEntities, CassandraReadSide readSide,                        CassandraSession db) {    this.persistentEntities = persistentEntities;    this.db = db;    persistentEntities.register(ItemEntity.class);    readSide.register(ItemEventProcessor.class);}To:@Injectpublic ItemServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,                       CassandraSession db) {    this.persistentEntities = persistentEntities;    this.db = db;    persistentEntities.register(ItemEntity.class);    readSide.register(ItemEventProcessor.class);}This concludes migrating our read-side logic to the new API.Since Lagom now supports multiple persistence backends, and not only just Cassandra, TestKit no longer starts with Cassandra enabled by default.This requires us to add a single line .withCassandra(true) to our server setup:@BeforeClasspublic static void setUp() {    server = ServiceTest.startServer(ServiceTest.defaultSetup()            .withCassandra(true)            .withConfigureBuilder);}All in all, migrating the code base to Lagom 1.2 took about fifteen minutes.Looking at the rest of our initial feedbackIn the previous blogpost on Lagom 1.0 we, like many other Java developers, made the point that only offering sbt as the build tool would repel many potential Java developers.We are glad that they addressed this quickly in the first new major version (1.1) they released.Regarding using Lagom in production without using ConductR, the bare minimum for this was to write your own service locator.Jonas Bon√©r started a service locator project for ZooKeeper and Consul for this purpose.Lightbend also plans to offer a free limited use evaluation license which will allow developers to start working with the Reactive Platform‚Äôs commercial features from the beginning.Including not only ConductR but other goodies such as monitoring for Lagom circuit breakers and Akka actors.In the blogpost we also wrote down several impressions compared to Pivotal‚Äôs Spring Cloud and Netflix OSS, currently the most popular choice of doing microservice architectures in Java.A strong point for Lightbend is that they want to distinct themselves from Pivotal by providing extensive commercial support if you get the Reactive Platform license.Pivotal also offers commercial support but only if you buy the commercial Pivotal Cloud Foundry.A key advantage of Lagom remains that it is non-blocking down to the core, starting from the persistence layer up to the endpoints, while Spring isn‚Äôt just yet.In the upcoming Spring Framework 5, of which a milestone version (M3) is already available, Pivotal are integrating their Spring Reactive initiative providing core reactive functionality and reactive web endpoint support.On the topic of Spring Reactor, a colleague of ours, Tom Van den Bulck, recently wrote a blogpost on Reactive Programming with Spring Reactor.In the blogpost, Tom writes about the presentation of Stephane Maldini at Ordina‚Äôs JOIN 2016 event, a small one-day conference hosted by Ordina, on reactive programming which has also been recorded and is available on YouTube. So it will be interested to see whether Spring Framework 5 allows them to catch up on being fully non-blocking and reactive, which up until now, remains a stronger point of Lagom.Lagom‚Äôs CQRS and Event Sourcing integration remain another advantage of Lagom, the out-of-the-box integration is easy to work with and they continue to improve on it, now with JDBC support and soon JPA support.In a Spring Cloud application a common solution for this is making use of the Axon Framework although it requires you to do the necessary gluing yourself.There is also the Eventuate framework written by Chris Richardson.Polyglot support is something that still lacks a bit on the side of Lagom.Spring has Sidecar for this purpose.Lagom services map down to ordinary, standard HTTP and WebSockets and as for other frameworks calling Lagom services, an integration client exists for JVM frameworks while others will have to invoke the Lagom services directly via REST.As for Lagom consuming other REST services, you define an API Service and implement the descriptor() to describe the external API. Documentation for this is currently lacking but an example with Httpbin and Slack‚Äôs Messages API exists.In order to further address the polyglot support, Lightbend is planning on implementing a solution for this probably based upon Akka Streams and Alpakka.The idea is that you should be able to generate a Lagom service interface from specifications, allowing transparent integration.Having binary coupling was another remark of ours and to address this, support for Swagger will be added in version 1.4.ConclusionWhile there are still a couple of important things in need of being addressed we believe that we can conclude that Lightbend is carefully listening to the feedback given by the community.They continue to improve Lagom with new features and to offer better user experience for the developers.At the same time, Pivotal is working on providing better reactive support with their upcoming Spring Framework 5.Lightbend and Pivotal make some nice rivals to each other which is nice since this will have a positive impact on both frameworks.Pivotal clearly still has an advantage over Lightbend due to how mature and well-known Spring is although Lagom seems to develop nicely and continues to improve on its strongest points while also trying to address weak points.We think that Lagom is worth adopting if you plan on getting a Reactive Platform license.Now that Maven support is available, a big hurdle for Java developers has disappeared.Without the Reactive Platform, development should be fine but you will probably miss the useful goodies it has to offer for running your system into production such as monitoring and service orchestration which you get all for free if you go with Spring Cloud and Netflix OSS.We are sure that using Lagom without the Reactive Platform will become more interesting given enough time for the community to come up with solutions for this.Scala developers will also be eagerly awaiting the release of 1.3 after which they can finally set their teeth into Lagom with the Scala API that it introduces.Lagom 1.3 previewThe first issue, created after Lagom was released, was the need to implement a separate Scala API.Something many users of Lightbend‚Äôs technologies were craving for.Until now it was already possible to use Scala with Lagom by using the Java API, see the following seed created by Mirco Dotta.Lightbend made work of it and at the time of writing, a release candidate for 1.3.0 has been made available which finally includes the Scala API for Lagom!Other features to be expected in 1.3.0 include JPA support and new test APIs for testing message broker integration.Extra resources  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom in Practice by Yannick De Turck  Lagom documentation  Lagom Twitter  Lagom Gitter  Lagom mailing list  Online Auction: Lagom 1.2 example"
      },
    
      "iot-2017-01-21-node-with-typescript-html": {
        "title": "Node with TypeScript",
        "url": "/iot/2017/01/21/Node-with-TypeScript.html",
        "image": "/img/node-with-typescript/node-ts.jpg",
        "date": "21 Jan 2017",
        "category": "post, blog post, blog",
        "content": "  NodeJS is a fantastic runtime to quickly and easily make projects.However as these projects tend to grow larger and larger, the shortcomings of JavaScript become more and more visible.This blog post will take a look at using TypeScript to write your Node application making it much more readable, introducing more OO like concepts whilst also making your code less error prone.NodeJS and its use cases  NodeJS has many use cases.It is an easy to pickup and use runtime.It uses Google‚Äôs V8 JavaScript engine to interpret and run JavaScript code.The user does not have to worry about threading.This is taken care off by the runtime.You write your code and make use of the many asynchronous operations provided by Node.This will take care of any multithreading for you.However, as you will read later in this blog post, making use of multiple Node instances to divide work is still possible!More on that later!Node can be used for a variety of tasks:  Small yet efficient web server  Code playground, test something quickly  Automation and tooling, instead of using ruby/python/‚Ä¶  IoT, Raspberry pi‚Äôs and other devices that can run Node!However, you should not use node for computationally heavy tasks!While the V8 engine is highly performant, there are other much more performant options available for computationally heavy operations!This blog post is not meant for people who have no NodeJS experience!Below are some resources for those that are new to the platform:  The main NodeJS website  The Node Package Manager  Code school intro to NodeJSThe old way, using plain JavaScript  Since NodeJS uses Google‚Äôs V8 JavaScript engine, it speaks for itself that node interprets and runs regular JavaScript code.This has some pros and cons.While it is an easy language to pick up, it can be hard to master.Javascript has always had some quirks and getting to know and how to avoid these can be tricky!It also does not require any compilation, which makes running your code very easy.However, this also removes any help from the compiler as no compile time checks are performed.No type checking, no checking for illogical structures or things that will just not work.Code for Node can be run by simple opening a command prompt or terminal window and typing    node    This will start a Node instance and present you with an interpreter.You can now type commands and press return to execute them.This can be handy to test something quickly.It is also possible to run a JavaScript file directly.This can be done via:    node path/to/javascript-file.js    However, most of the time you will not be using this way of running code.Most of the time you will use npm to install your dependencies and start the node instance:    npm install    npm start    This reads the package.json file and executes the scripts contained inside it.  Extensive documentation about the package.json file can be found on the NPM websiteTypeScript you say!?  TypeScript has been around for some years now.TypeScript is a superset of JavaScript.It uses the same syntax but adds among other things compile time type checking.It also adds a more Object Oriented model.A detailed explanation of the differences of the prototype based JavaScript and a more Object Oriented language can be found on theMozilla Developer websiteTypeScript developed mainly by Microsoft and is completely open source!This means developers can make suggestions and report bugs (and even fix these bugs if they want).  TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. Any browser. Any host. Any OS. Open source.TypeScript is very well documented and getting started with the language is fairly easy.A lot of common development tools have support for TypeScript syntax checking.These include, but are not limited to:  Intellij  Webstorm  Atom  Visual Studio Code  ‚Ä¶As Node applications regularly use other NPM dependencies it is required for the TypeScript compiler to know about these dependencies and what types they use.You could make or generate these typings yourself.However, you can easily find these typings on TypeSearch website.The most commonly used dependencies have their typings available here!You can add the typings to the dependencies in the package.json file.    \"dependencies\": {        \"typescript\": \"2.0.8\",        \"@types/node\": \"0.0.2\",        \"@types/mime\": \"0.0.29\",        \"@types/johnny-five\": \"0.0.30\",        \"@types/serialport\": \"4.0.6\",        \"mime\": \"1.3.4\",        \"johnny-five\": \"0.10.6\",        \"serialport\": \"4.0.7\"    }    Making it all work: An exampleA few years back I started working on my own server application to host some web content and provide REST services.The code was written in JavaScript and ran on a Raspberry Pi 2 (by now a pi 3).For those of you that are interested the old code can be found on the following Github repositories:  WeatherGenie This was the initial implementation, a simple weather web application for checking the weather conditions for any city in Belgium  LoRa-IoT-Demo The second, extended iteration, based on the code from the WeatherGenie application.Because with the advent of IoT we needed a simple to extend/run/maintain solution to create IoT demos for clients.  NodeSimpleServer The third and current iteration. Written from the ground up in TypeScript and completely reworked to work better and be more maintainable.This is the application that will be detailed below!Node Simple Server: High level architecture  The Application starts in app.ts under the main src folder.This is the entry point for the application.This file contains the actual master instance code.The master instance is in charge of forking the workers and reviving them if they die.The master is also used to pass messages between the workers For this a specialized MessageHandler singleton is used.This MessageHandler instance (one per worker) is used to relay messages.The master instance itself will not execute any application logic.Its purpose is to manage the other workers and be the message bridge.    /**     * Forks the workers, there will always be one DataBroker and one IntervalWorker.     * HTTPWorker will be created based on the number of cpu cores. If less than two cores are available     * two http workers will be created.     */    private forkWorkers = (): void =&gt;{        //Fork data broker.        this.databroker = cluster.fork({name: 'broker', debug: this.isDebug});        //Fork interval worker.        this.intervalWorker = cluster.fork({name: 'interval', debug: this.isDebug});        //Fork normal server worker instances. These will handle all HTTP requests.        let cores:number                = os.cpus().length;        let numberOfHttpWorkers:number  = cores - 2 &gt; 0 ? cores - 2 : 1;        console.log('There are ' + cores + ' cores available, starting ' + numberOfHttpWorkers + ' HTTP workers...');        for (let i:number = 0; i &lt; numberOfHttpWorkers; i++) {            let worker = cluster.fork({name: 'http', debug: this.isDebug});            this.httpWorkers.push(worker);        }        //Revive workers if they die!        if(!this.isDebug) {            cluster.on('exit', this.reviveWorker);        }    };    The master will create a number of workers:  HttpWorker: Each HttpWorker is an endpoint for requests to be received.There will always be a minimum of two HttpWorkers created.If more CPU cores are available, more HttpWorkers are created.  DataBroker: For the application there is one DataBroker worker instance.This worker handles CRUD operations for data (for now in memory only).  IntervalWorker: For the application there is one IntervalWorker instance.This worker can run code periodically and is used to connect to other devices such as Arduino‚Äôs and the Raspberry Pi I/O pins.These workers are created by a WorkerFactory, as the master forks new Node instances, a process variable is set, the factory uses this to see which type the node instance should become.Each type of worker instance implements the basic NodeWorker interface.Each implementation will be detailed below.Handling HTTP requests: The HttpWorkerEach HttpWorker instance will create a Server instance.This instance will be used to receive HTTP requests.Node will automatically load balance requests between all instances that register a server on the same port.Simply put all HttpWorkers compete for the next request, the least burdened process (depending on OS/CPU process affinity) will be given the next Http request to handle.The Server class will also register the endpoints that are known to the application and can be handled.The EndpointManager is used to register endpoints.An EndPoint has a path, a method to execute and optional parameters.A Parameter is provided with a Generic type for compile time type checking, a name which should be used in the url, a description that provides information what the parameter should contain and an optional ParameterValidator.A ParameterValidator is used to validate the Parameter at runtime.If the check fails an error is shown to the user.    /**     * Maps the default endpoints.     * Endpoints can always be added at any other location and point in time.     * This can be done by getting the instance of the EndPointManager and calling the registerEndpoint method.     */    private mapRestEndpoints = (): void =&gt; {        this.endpointManager.registerEndpoint(            new EndPoint(                '/',                GenericEndpoints.index,                null            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/endpoints',                GenericEndpoints.listEndpoints,                null            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/helloworld',                GenericEndpoints.helloworld,                [new Parameter&lt;string, null, null&gt;('name', 'string field containing the name', new HelloWorldValidatorImpl())]            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/arduino/setArduinoMethod',                ArduinoEndpoint.setArduinoMethod,                [new Parameter&lt;string, null, null&gt;('method', 'string field that contains the method used for adruino implementations', new ArduinoMethodValidatorImpl())]            )        );    };    The Server instance forwards all requests to the Router instance.As the name suggests this will perform the routing.It will see if a resource is requested or and endpoint has been called.If a resource is requested it will be served if found.If an endpoint has been called, that endpoint will be executed and passed the parameters that were entered, but only after the correct amount of parameters has been passed and they are all valid.Handling data: The DataBrokerThe DataBroker is the Node instance in the application that will save and retrieve data.For the time being it is sufficient to only have in memory ‚Äòcaches‚Äô on which basic CRUD operations can be performed.All methods on the DataBroker are called by sending an IPCRequest with the data that needs to be saved of the instruction for what data should be retrieved.The DataBroker will reply to the original worker by sending an IPCReply with the result of the operation.The DataBroker for now only has a concept of caches.A cache has a name, type and values (of said type).Values can be retrieved, added, updated and deleted from the caches.Caches can be retrieved, added and deleted at runtime.Handling asynchronous tasks: The IntervalWorkerThe IntervalWorker as its name suggest performs tasks at a certain interval.It is also used for other asynchronous workloads, such as connecting to an Arduino and running Arduino/Raspberry pi Johhny-Five scenarios.The IntervalWorker is handy when you need for example to update the content of a cache every so often.It can also run Arduino scenarios.These are Implementations that contain logic to perform actions on the Arduino or in response to something that happens on the Arduino.The IntervalWorker picks up what type of Arduino Scenario you want to run and starts the logic.    /**     * Sets up the connection to the Arduino and starts the desired Arduino Scenario.     */    private setupArduino = (): void =&gt; {        if(this.config.arduino.enableArduino) {            if(this.config.arduino.useSerialOverJohnnyFive) {                this.arduino = new ArduinoSerial(                    this.config.arduino.serialPortName,                    this.config.arduino.serialPortBaudRate,                    new PingScenario()                );            } else {                this.arduino = new ArduinoJohnny(new BlinkScenario());            }            this.arduino.init();        } else {            console.log('Skipping arduino setup, disabled in settings!');        }    };    There are two Arduino implementations available.Both can execute a Scenario.The first and simplest implementation is the Johnny-Five Arduino implementation.This allows you to make use of the Johnny-Five framework to write dynamic code for the Arduino that can change at runtime.This is possible because it uses the StandardFirmata firmware.Johnny-Five supports a lot of components and peripherals.Their website has extensive documentation and very clear examples.Johnny-Five also supports the Raspberry PI I/O pins.This allows it to be used on a Raspberry pi also.The second Arduino implementation uses no framework and communication is done via regular serial.In the type of scenarios you have to handle all the serial communication yourself.You also have to write Arduino firmware and thus it cannot be dynamically updated at runtime.Use this Arduino implementation if some component is incompatible or not supported by Johnny-Five.Inter Process Messaging: Communicating between different Node instancesHaving all these different worker instances is quite handy.However they are of not much use if there cannot be any communication between them.Each Node instance has its own allocated memory and cannot access variables or call methods on other instances.The Node cluster and process framework provide the option to send messages between Node instances.The IPCMessage instances that are sent exist in two forms.  IPCRequest: This is the initial message that is sent to a target.  IPCReply: This is the response (if any) from the target back to the original caller.This allows for easy two way communication and identification whether the message was a reply to an earlier message.Messages can be sent with or without a callback.The callback is executed when a reply to the original message is received.Because only basic data types can be sent across Node instances the MessageManager instance of the caller stores the callback reference and generates an unique id for said callback.This allows the application to send the callback ID across Node instances and execute it when it arrives back at the caller.    /**         * MessageManager singleton class.         * This class has an array of tuples of string and Function.         * The string field is the callbackId and the Function is the actual callback.         * The message manager is a per worker instance that can only execute callbacks on the same worker.         * The integration with the IPC framework allows messages to be sent to other workers and replies to be sent back to the original worker.         * It is important that the original worker is called to execute the callback since a function cannot cross a node instance!         *         * This singleton can be used to manage IPC messages.         */        export class MessageManager {            private static instance: MessageManager         = null;            private callbacks: Array&lt;[string, Function]&gt;    = null;            private workerId: string                        = null;            /**             * Private constructor for the singleton.             */            private constructor() {                this.callbacks = [];                this.workerId = cluster.worker.id;            }            /**             * Use this method to get the instance of this singleton class.             *             * @returns {MessageManager} The instance of this singleton class.             */            public static getInstance(): MessageManager {                if(!MessageManager.instance) {                    MessageManager.instance = new MessageManager();                }                return MessageManager.instance;            }            /**             * Sends an IPCMessage of the subtype IPCRequest to the given MessageTarget (one of the three worker types).             * A target function is also given and contains the name of the function that will be executed on the target.             * The target should implement a specific handler or switch statement to handle these different target function names.             * This message is sent without a callback. This means that when the target function has finished no reply will be sent to inform the caller.             *             * @param payload The payload for the target, can be of any kind.             * @param messageTarget The MessageTarget, being one of the three types of workers.             * @param targetFunctionName The name of the function to be executed on the target. This value is NOT evaluated by eval for security reasons.             */            public sendMessage(payload: any, messageTarget: MessageTarget, targetFunctionName: string): void {                let message: IPCMessage = new IPCRequest(this.workerId, null, payload, messageTarget, targetFunctionName);                process.send(message);            }            /**             * Sends an IPCMessage of the subtype IPCRequest to the given MessageTarget (one of the three worker types).             * A target function is also given and contains the name of the function that will be executed on the target.             * The target should implement a specific handler or switch statement to handle these different target function names.             * This message is sent with a callback. The callee sends a new IPCMessage of the subtype IPCReply to inform the caller and provide it with new information if needed.             * A reply can be sent by using the sendReply method on this class.             *             * @param payload The payload for the target, can be of any kind.             * @param callback The function that should be called when a reply has been received.             * @param messageTarget The MessageTarget, being one of the three types of workers.             * @param targetFunctionName The name of the function to be executed on the target. This value is NOT evaluated by eval for security reasons.             */            public sendMessageWithCallback(payload: any, callback: Function, messageTarget: MessageTarget, targetFunctionName: string): void {                let callbackId: string = process.hrtime()  + \"--\" + (Math.random() * 6);                this.callbacks.push([callbackId, callback]);                let message: IPCMessage = new IPCRequest(this.workerId, callbackId, payload, messageTarget, targetFunctionName);                process.send(message);            }            /**             * Sends and IPCMessage of the subtype IPCReply to the sender of the original message.             *             * @param payload A new payload to provide to the original sender.             * @param originalMessage The message the sender originally sent.             */            public sendReply(payload: any, originalMessage: IPCRequest): void {                let reply: IPCMessage = new IPCReply(this.workerId, payload, originalMessage);                process.send(reply);            }            /**             * For a given callbackId execute the callback function.             *             * @param callbackId The callbackId for which to execute the callback function.             */            public executeCallbackForId(callbackId: string) :void {                for (let callbackEntry of this.callbacks) {                    if(callbackEntry[0] == callbackId) {                        callbackEntry[1]();                        return;                    }                }            }        }    &lt;br/&gt; &lt;br/&gt;    /**     * MessageHandler singleton class.     *     * This singleton can be used to handle IPC messages.     */    export class MessageHandler {        private static instance: MessageHandler         = null;        private dataBroker : cluster.Worker             = null;        private intervalWorker : cluster.Worker         = null;        private httpWorkers : Array&lt;cluster.Worker&gt;     = null;        public emitter: EventEmitter                    = null;        /**         * Private constructor for the singleton.         */        private constructor() {        }        /**         * Use this method to get the instance of this singleton class.         *         * @returns {MessageHandler} The instance of this singleton class.         */        public static getInstance(): MessageHandler {            if(!MessageHandler.instance) {                MessageHandler.instance = new MessageHandler();            }            return MessageHandler.instance;        }        /**         * Initialises the MessageHandler for being a handler for the master NodeJS process.         *         * @param dataBroker The DataBroker worker instance.         * @param intervalWorker The IntervalWorker worker instance.         * @param httpWorkers The HTTPWorker worker instance.         */        public initForMaster = (dataBroker: cluster.Worker, intervalWorker: cluster.Worker, httpWorkers: Array&lt;cluster.Worker&gt;): void =&gt; {            this.dataBroker     = dataBroker;            this.intervalWorker = intervalWorker;            this.httpWorkers    = httpWorkers;            this.emitter        = new EventEmitter();        };        /**         * Initialises the MessageHandler for being a handler for a slave (worker) NodeJS process.         */        public initForSlave = (): void =&gt; {            this.emitter        = new EventEmitter();        };        /*-----------------------------------------------------------------------------         ------------------------------------------------------------------------------         --                         MASTER MESSAGE HANDLING                          --         ------------------------------------------------------------------------------         ----------------------------------------------------------------------------*/        //TODO: Separate master and slave message handling?        /**         * Handler function for messages sent by HTTPWorkers.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by an HTTPWorker.         */        public onServerWorkerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from server worker');            this.targetHandler(msg);        };        /**         * Handler function for the messages sent by the IntervalWorker.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by the IntervalWorker.         */        public onIntervalWorkerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from interval worker');            this.targetHandler(msg);        };        /**         * Handler function for the messages sent by the DataBroker.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by the DataBroker.         */        public onDataBrokerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from data broker');            cluster.workers[msg.workerId].send(msg);        };        /**         * This method is used to direct the IPCMessage to the correct target as specified in the message.         * This handler makes a distinction between messages of the types IPCRequest and IPCReply.         *         * @param msg The IPCMessage that is to be forwarded to the correct target.         */        private targetHandler = (msg: IPCMessage) =&gt; {            if(msg.type == IPCMessage.TYPE_REQUEST) {                let m: IPCRequest = &lt;IPCRequest&gt; msg;                console.log('Master received request');                switch (m.target){                    case MessageTarget.DATA_BROKER:                        this.dataBroker.send(msg);                        break;                    case MessageTarget.INTERVAL_WORKER:                        this.intervalWorker.send(msg);                        break;                    case MessageTarget.HTTP_WORKER:                        let index: number = Math.round(Math.random() * this.httpWorkers.length) - 1;                        index = index === -1 ? 0 : index;                        this.httpWorkers[index].send(msg);                        break;                    default:                        console.error('Cannot find message target: ' + m.target);                }            } else if(msg.type == IPCMessage.TYPE_REPLY) {                let m: IPCReply = &lt;IPCReply&gt;msg;                console.log('Master received reply!');                cluster.workers[m.originalMessage.workerId].send(msg);            }        };        /*-----------------------------------------------------------------------------         ------------------------------------------------------------------------------         --                          SLAVE MESSAGE HANDLING                          --         ------------------------------------------------------------------------------         ----------------------------------------------------------------------------*/        /**         * Handler function for the messages sent by the Master NodeJS process.         * This handler makes a distinction between messages of the types IPCRequest and IPCReply.         *         * @param msg The IPCMessage as passed on by the master process.         */        public onMessageFromMasterReceived = (msg: IPCMessage): void =&gt; {            if(msg.type == IPCMessage.TYPE_REQUEST) {                let m: IPCRequest = &lt;IPCRequest&gt;msg;                console.log('[id:' + cluster.worker.id  + '] Received request from master: routing to: ' + MessageTarget[m.target] + '.' + m.targetFunction);                this.emitter.emit(MessageTarget[m.target] + '', m);            } else if(msg.type == IPCMessage.TYPE_REPLY) {                let m: IPCReply = &lt;IPCReply&gt;msg;                console.log('Slave received reply!');                MessageManager.getInstance().executeCallbackForId(m.originalMessage.callbackId);            }        };    }    Every worker has an instance of the MessageHandler, it in its turn has an event emitter on which events from the messages are broadcast.The actual worker implementations register themselves on the emitter to receive said events.In a future version the message handling should be split up, because now a single file (with an instance on each Node instance) handles both master and slave messages.Final wordsIn conclusion; It is perfectly possible to make a more complex application for NodeJS with TypeScript.By using TypeScript you gain compile time type checking and a more robust and better readable codebase.Fewer errors and strange bugs are encountered because TypeScript ‚Äòforces‚Äô you to write better code.The Node Simple Server application was a great way to learn the ‚Äònew‚Äô TypeScript language.The project is not finished, as some parts could use some more work, but it should stand as a solid starting point.Feel free to fork the codebase, submit issues or start some discussion."
      },
    
      "conference-2017-01-17-oredev2016-html": {
        "title": "√òredev 2016",
        "url": "/conference/2017/01/17/Oredev2016.html",
        "image": "/img/oredev/oredev-logo.png",
        "date": "17 Jan 2017",
        "category": "post, blog post, blog",
        "content": "When arrived in Malm√∂ city, we were welcomed with an exploration tour of the third biggest city in Sweden.The next day √òredev 2016 officially started!It was noticeable that there was a healthy combination for everyone: Microservices, Docker, UX, Management, etc‚Ä¶All these buzzwords were applied in various talks.This blog post will be about the talks I favoured and the extra knowledge I gained which I now get to apply in my nowadays projects.The Overview Wizard  Dockerizing Microservices  Secure your Docker  Kubernetes  Best Practices &amp; Traps to avoid in UX  Linked data API: The future of HTTP API‚Äôs?Dockerizing MicroservicesThe Slideless Microservices session - Adam BienAdam Bien is an Expert Group member for the Java EE 6 and 7, EJB 3.X, JAX-RS, and JPA 2.X JSRs. He has worked with Java technology since JDK 1.0 and with Servlets/EJB 1.0 and is now an architect and developer for Java SE and Java EE projects. He has authored several books about JavaFX, J2EE, and Java EE, and he is the author of Real World Java EE Patterns‚ÄîRethinking Best Practices and Real World Java EE Night Hacks‚ÄîDissecting the Business Tier. Adam is also a Java Champion, Top Java Ambassador 2012, and JavaOne 2009, 2011, 2012, 2013 and 2014 Rock Star.In this session Adam guided us in how to dockerize and communicate between microservices.He says that involving the container technology to the microservices ecosystem is considered the future and inevitable.When looking into the communication between dockerized microservices, there are several ways in doing so.Before communicating, we first make the service dockerized with the help of a Dockerfile.The Dockerfile will be the configuration for your image:FROM openjdk:8-jre                     // Which base image am I using?VOLUME /tmp                            // Is where you will send data toADD service.jar app.jar                // Copies the existing jar in to the imageRUN sh -c 'touch /app.jar'             // This will execute any commands in a new layer on top of the current imageEXPOSE 42001                           // Which ports should I open at runtime?ENTRYPOINT [\"java\",\"-jar\", \"/app.jar\"] // This will allow you to configure a container that will run as an executableAfter we created the Dockerfile we will have to setup a Docker network for connecting these containers.Since Adam is doing this manually, I will explain you how to do it easier with Docker Compose.Docker Compose will run all the containers you setup in the configuration.Since we work with the microservices ecosystem, we will need a lot of containers.To build our image using the Dockerfile, we will add a Docker dependency in our pom.xml.First add a property for naming your repository:&lt;docker.image.prefix&gt;testApp&lt;/docker.image.prefix&gt;And add support for docker:&lt;plugin&gt;  &lt;groupId&gt;com.spotify&lt;/groupId&gt;  &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;  &lt;version&gt;0.4.13&lt;/version&gt;  &lt;executions&gt;    &lt;execution&gt;      &lt;phase&gt;package&lt;/phase&gt;        &lt;goals&gt;          &lt;goal&gt;build&lt;/goal&gt;        &lt;/goals&gt;    &lt;/execution&gt;  &lt;/executions&gt;  &lt;configuration&gt;    &lt;imageName&gt;${docker.image.prefix}/${project.artifactId}&lt;/imageName&gt;    &lt;dockerDirectory&gt;${project.basedir}/src/main/docker&lt;/dockerDirectory&gt;    &lt;resources&gt;      &lt;resource&gt;        &lt;targetPath&gt;/&lt;/targetPath&gt;        &lt;directory&gt;${project.build.directory}&lt;/directory&gt;        &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;      &lt;/resource&gt;    &lt;/resources&gt;  &lt;/configuration&gt;&lt;/plugin&gt;First we make images of our services using the Maven Docker plugin from Spotify.Every time Mavens builds the project, a Docker image is generated.Docker ComposeTo tell the containers that they have to be under the same network, we will use Docker Compose.First we make a docker-compose.yml file and configure our microservices:version: '2'services:  cars:    image: testApp/cars-service    ports:      - \"8081\"    networks:      - backend  gateway:      image: testApp/zuul-service      ports:        - \"9900:9900\"      networks:        - backendnetworks:  backend:Variables explanation:  image          The image you will be using to create the container, we will be using our microservice image        ports          The port the service will listen to. When you want to expose your port use: ‚Äú5432:5432‚Äù.        build          This will build the Dockerfile inside the same directory        networks as property          This be the network to which you want to be assigned to        networks          This will create one or multiple networks      When you want to add an Eureka service, follow the above steps and configure it inside the docker-compose.yml file.It is important that every container, you want to communicate with, resides under the same network.Secure systems with Docker - Emil KvarnhammerEmil Kvarnhammer is a Security Software Engineer at TrueSec,a leading edge company in IT security and development.He‚Äôs been involved in several security-critical projects,developing applications and components that are used by millions of users on a regular basis.Emil has found severe security vulnerabilities in Apple OS X. His recent focus has been securing systems using Docker and Amazon EC2.When entering the world of cloud and distributed systems, security can be challenging for developers and DevOps.Docker is a part of this world and Emil talks about Docker from a security perspective.When looking at it in a secure way, what are the input sources?Where does the base image come from?If we look closer we can see that it retrieves its images from Docker Hub.In addition to that, more complexity comes in when we add new packages on top of the base image that will be retrieved from the package manager server.Finally we have the application package that can come from a CI server and, if it is a fat jar, it can consist of different dependencies through Maven or some kind.The base image is retrieved from Docker Hub every time a Docker image is being updated.A hacker could hijack the image and squeeze in malicious software without anyone noticing.Same goes for the package manager server.What could an attacker do if he gets control of the channel between the Docker image andthe host operating system?The place where the image gets instantiated or the configuration related to the instantiated image.An attacker could infect the image with his own configuration or modify the orchestration tool.The most important issue is when the attacker gets control of code execution within our application.Emil says that it happens quite often and if it isn‚Äôt in your code, it is in the third party libraries or the package manager server you‚Äôre using.Now you might think that, seeing as the attacker is in a docker container, that he is unable to break out.Think again, he can break out and gain control of the whole operating system.Building secure images  Choose base image carefully  Strategy for security patches - Regularly update your Docker container so it gets the necessary updates  Continuously rebuild and test - Security patches in your build environment  In a secure build environment  Digitally sign Docker images - # docker build -t test --disabled-content-trust=falseInternal private registry - Push and pull the containers internallyThis private registry is insecure by design.If an attacker gains control of the network where the registry is located, he can easily read and modify each image.An attacker pushes a modified image and the orchestration will pull this in.If the attacker gains control of your CI/CD server he can again push modified images to the registry.To avoid this attack, be sure to have these activated inside your registry.  Secure transfer using TLS  User authentication  Limited access rights  Docker Content Trust which will digitally sign your images  Secured development environmentHow does the attacker inject code?  You use whatever Docker image available in the registry and add your backdoor binary in a Dockerfile,then you build the image and point it to the very same image in the registry.Security measurements inside the container  User namespaces          An extra security measure are user namespaces, this will make the root in the container differentiate from the root in the host.        Remote API          If you have no good reason for using the remote API then you should disable this option.      If you have to use it, be sure to use secure transport over TLS and that you have client authentication so you don‚Äôt let the world in.        Try to enable SELinux (enforcing) on the host  Drop unused capabilities that are installed by default and install the capabilities that you actually need.          --cap-drop=ALL      --cap-add=needed_cap         Only pull from private registriesThings you should avoid in Docker  --privileged: breakout is trivial and will give you access to different machines          Be sure to check that Kubernetes is not doing this        --insecure-registry : disable TLS requirement  --net=host: use host network stackKubernetes automation in production - Paul BakkerPaul Bakker is working as a software architect for Luminis Technologies, where he‚Äôs currently leading the team that develops the CloudRTI,a Kubernetes/Docker based service that takes care of DevOps tasks like blue/green deployment, failover, centralised logging and monitoring.He is the author of ‚ÄúBuilding Modular Cloud Apps With OSGi‚Äù published by O‚ÄôReilly and an advocate for modularity in software.He is currently writing his second book for O‚ÄôReilly ‚ÄúJava 9 Modularity‚Äù which is expected early 2017.He‚Äôs a regular speaker at conferences speaking both about modularity/OSGi related topics, and topics related to Kubernetes and cloud architecture in general.Kubernetes is momentarily the best tool to use for orchestrating containers in a clustered production environment.In this technical deep dive you will learn the architecture and production deployment of Kubernetes.Why Kubernetes?  Run Docker in clusters          Scheduling containers on machines - Load balance and failover containers      Networking - How to communicate between containers      Storage - Use a network attached database      Automation - A commit will result in a new deployment      The ArchitectureMasterThe master component is the most important component of Kubernetes, everything that happens in the cluster is handled by the master.Generally there is one master but you can have a failover setup and work with multiple masters.For now we simply see this as a single component with an API.When a master goes down, the rest keeps running, but during that moment you won‚Äôt get any scheduling or things like that.Replication ControllerThe controller is there to configure where and how many nodes the container has to run in.It will manage scheduling and monitoring inside the cluster,so if one container crashes for some reason, the replication controller will look at how many replicas you configured and if one is down, it will schedule a new instance into the cluster.It is important that we don‚Äôt start our containers ourselves but configure the amount of replicas that are necessary.After our containers are set up, the cluster will take care of it and monitor it.Worker nodesThis is where your docker containers will run in.PodsAn abstraction on top of containers inside the worker node:  May contain multiple containers  Lifecycle of these containers bound together          Don‚Äôt place microservices into the same pod        Containers in a pod see each other on localhost  Env vars for servicesEtcdIs a key-value store and contains the configuration of the cluster.You don‚Äôt want to lose your etcd otherwise your cluster will go down.If you want fault tolerance, you will need quite a lot of machines.At least one master and three worker nodes to meet the requirements.NetworkThese pods will have to talk to each other so you will have to open ports.  We run many pods on a single machine  Pods may expose the same portsHow to avoid conflicts?  Each pod gets a virtual IP  Ports are not shared with other pods  This can cause some troubles when your process crashes and obtains a new IP address.So how do they keep communicating? Are they tracking each other? Do we have to use something like a service discovery?It would be complex if you want to go there and at the end, you don‚Äôt want to depend on those virtual IP addresses.If you want the pods to communicate, we will be using services as a proxy on top of them.ServicesService are basically a proxy on top of the pod and if you look from a technical perspective, we see configured IP tables.When we create a service, it gets a fixed IP address and a DNS name so we can communicate through our services.Finally it will round-robin traffic to the destined paths and keep track of which paths are running.Multi component deployments (microservices):  Each component deployed as a pod  Individually update and scale pods  Use services for component communication  The Frontend talks to the service of the backend pod and this will round robin the call to the right instances.NamespacesIsolated world in Kubernetes where you can have different environments inside the same cluster.If you want to run a TEST, DEV and PRD environment it‚Äôs possible with Kubernetes to keep them isolated from each other.Kubernetes in ProductionHTTP load balancingKubernetes does not have support for the following points in production so be sure to have a look at the following points when deploying to PRD.  Expose Kubernetes services to the outside world  SSL offloading  Gzip  Redirects - redirect every HTTP call to HTTPSNodePort  Exposing service ports as ports on physical nodes  Beware of port conflicts!kubectl expose deployment my-nginx --type=NodePortKubernetes Ingress  Level 7 load balancer configuration  SSL offloading  Support for GCE load balancers  Future support for extensions (not quite there yet)  At the moment useful on Google Cloud EngineUsing a custom load balancer  Use HAProxy in front of Kubernetes  Configure HAProxy dynamically  The same works for NGINX and Apache  HAProxy does SSL offloadingHow does ha-proxy know about our services?  HAProxy uses a static config file  Auto-generate it based on data in etcd  ConfdBlue-green deployment  Deployment without downtime  Only one version is active at a time  Rolls back on failed deployment  When updating our cluster we have two options:We want to update our cluster with a rolling update,which updates one pod at a time, rather than taking downthe entire service at the same time or you can do a blue-green deployment where youmake an entirely new cluster of instances and let the load balancer go from blue to green.A developer‚Äôs guide to the UX galaxy - Tess FerrandezTess Ferrandez is a full stack developer on the Microsoft Platform.She is equally happy debugging nasty backend issues as she is developing apps or working on UI/UX design.She has been blogging at her blogsite for the better part of her career, and has spoken at lots and lots of conferences around the world.Software is built into two parts, it is what we see and how we do it.In this session Tess talks about what these features should look like.If you design ‚Äòan order‚Äô incorrectly and the user can‚Äôt figure out to place that order, you have a bad UX design and it could cost your company a lot of money.Now, two UX researches at Microsoft deep dived into hundred years of research of signs and symbols and listened to what people define as a good interface.Finally they categorized every piece into ‚ÄòTenets‚Äô (attributes of good user interfaces) and ‚ÄòTraps‚Äô (bloopers and things to avoid).Tenets: attributes of good useWe will be using a scanner as example that you use in the supermarket.Understandable  Ask yourself, what can I do? Is it understandable?If you look at the scanner in a supermarket, do you know which button you have to press to register a product?It is important to let your users understand what they are interacting with because if they have to read the manual first or ask someone for help, you‚Äôre doing something wrong.Physically effortless  Is what I‚Äôm holding quick and comfortable to use?When scanning a product you want your users to feel that they can scan the product easily and without difficulty.Responsive  You get immediate feedback, when scanning a product you get an immediate response (ex: Hey! you scanned something)If you don‚Äôt get an immediate response, the user will keep trying to scan the product and at the end he will have ten milk boxes.Efficient  What is the most efficient way in a process?The best way to buy ten milk boxes is to scan the milk ten times than to write it down ten times with your other hand.Forgiving  I can undo my actions.If I scan too many items, I have a minus button to unregister the last item.It‚Äôs a bad practice if there is no button and you have to ask an employee to help you.Discrete  Don‚Äôt over share, don‚Äôt overreact.Having a siren on the scanner to point out that you did something wrong is too much.Protective  Always deliver a qualified product with no failure or data loss.A bad practice for example is that after you just filled in a form, clicked through to the next page and tried to go back to the form to add some missing information only to find out that your form has been completely reset.Habituating  Muscle memory, make it a habit to use.When users go to your website and know where to go to buy a specific product automatically after visiting it a couple of times.Try to avoid changing your structure over and over and if you do so, make sure that you remind your users of the change in advance.Beautiful  I find it attractive compelling.Make the user think that you put a lot of effort in it.Use the same buttons, nicely aligned and use the same colors.Traps: attributes to avoidEvery ‚ÄòTenet‚Äô has its own set of traps, each trap is there to teach us to avoid it as much as possible.Understandable TrapsPerceptibleInvisible ElementThe user has learned about a critical hidden capability but cannot see it.NoticeableEffectively Invisible ElementA critical cue goes unseen because it is outside of the user‚Äôs vision or unexpected.DistractionSomething is drawing the user‚Äôs attention from what they need to see.Comprehensible TrapsUncomprehended ElementIt looks wrong but it is right (ex. If they have to put a sticker on it, you are doing something wrong)Inviting Dead EndIt looks right but it is wrong (ex. Having a music icon on your IPhone and you expect iTunes, but it is not iTunes)Poor GroupingA link between two or more elements is not recognized (ex. Right icon with the wrong button)Memory ChallengeAn unreasonable demand is made on the user‚Äôs memory.Confirmatory TrapsAmbiguous FeedbackThe feedback or response to a user action is unclear or absentPhysically effortless Traps - Targeting and readability  Buttons should be big enough to pressAccidental activationTrying to click one button but clicking another.Physical challengeAlways use your design on the end product.When designing a mobile form, be sure you do it on a mobile phone.Invisible element on phoneAccidental activation if shutdownResponsive Traps  Be sure your response takes half a second at most  If the response takes longer you should display a loading circle or show the progression of the processEfficiency TrapsInformation overloadHaving a page that shows you too much information, be sure to keep it as simple as possible.System amnesiaWhen they show you information that you don‚Äôt needAutocorrectThe name says it all, when autocorrect changed your word in something unintendedHabituating Traps  Doing things so many times it gets automatic and then it changes.Non redundant gratuitous redundancyToo much of the same action (ex. if you have one button that does multiple different actions on other systems)InconsistencyMessage button on an older version is on another place or it is replaced with textBeautiful TrapsUnattractive designNot appealing design where the user will not stay on the website.Best Practices  Identify common scenarios using the site  Walk through all the different ways the user can complete a certain task  Identify and log any traps you observe  If you can‚Äôt find a trap, identify the tenet  Document and discussFeed the links, tuples‚Äô a bag ‚Äì An introduction to Linked Data APIs - Sebastien LamblaSebastien Lambla is a keen open-source advocate and contributor, a long-time REST proponentand distributed systems aficionado With a career spanning over 20 years on many platforms,he‚Äôs a keen speaker and trainer, and has been known to talk a lot about technical things and unicorns.Started with a calm song of tuples‚Äô a bag from Mary Poppins, Sebastien guides us through a possible future of HTTP API‚Äôs.When talking about creating our own protocol you know that it is recommended not to not to reinvent the wheel and use HTTP instead.Someone else did the hard work, so why should you?This is called the network effect.How is HTTP designed?  Model things - Resources (Person, Animal, Fruit)  Interact with things - Operations (POST, GET, PATCH, DELETE)  Understand the result of the interaction - Common Responses (200 - OK , 201 - Created, 400 - Bad Request)  Reuse existing libraries  One interface to be taught for everythingCommon Media Types  Expose the structure of data in a common format  Reuse existing libraries (JSON)Hypermedia Media Types  Have structure and links understood and defined commonly  Typed links - A link that knows where to go next  Reuse existing libraries (HAL)Common LanguageJSON-LD will try to use a common language for everyone to understand.It is a lightweight Linked Data format, easy for humans to read and write.It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale.JSON-LD is an ideal data format for programming environments, REST Web servicesand unstructured databases such as CouchDB and MongoDB.Example:{    \"@context\":    {        \"author\": {            \"@id\": \"http://schema.org/url\",            \"@type\": \"@id\"         }    },    \"author\": \"https://serialsev.com/author/\"}In the above example, author is an uri which is the identifier of a resource.If you follow the uri, you will find a resource that is defined as a data type that everybody understands and everyone has agreed upon.Again, here we use the network effect because the last thing we want is to rebuild stuff.If you are interested in playing with the format, there is a playground where you can try out examples:PlaygroundWhen we open the playground, you can see a couple of examples you can look at.{  \"@context\": \"http://schema.org/\",  \"@type\": \"Person\",  \"name\": \"Jane Doe\",  \"jobTitle\": \"Professor\",  \"telephone\": \"(425) 123-4567\",  \"url\": \"http://www.janedoe.com\"}The context defines the uri where to go and the type that tells us which resource we are retrieving.Finally the exact uri link will be http://schema.org/Person.If you visit the uri, you‚Äôll get this:When finding the Person resource we see that there are a lot of properties already defined and documented by several people so you don‚Äôt have to.When looking back at our example you can use the property already defined in the resource and use them to initialise.If you reuse the property defined, you will get the proper documentation for it.Some nice tools  Google Structured Data Testing Tool  Hydra console  PlaygroundConclusion√òredev is one of the best conferences I attended, not only did it innovate in different talks, it managed to organize so many topics to attend.Not only did it improve my technical skills, it also taught me management skills and a realistic view of what the world is today.Very well organized, interesting talks, great local dishes from Sweden and beautiful sightseeing in Malm√∂."
      },
    
      "reactive-2016-12-12-reactive-programming-spring-reactor-html": {
        "title": "Reactive Programming with Spring Reactor",
        "url": "/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html",
        "image": "/img/reactive/reactor_logo.png",
        "date": "12 Dec 2016",
        "category": "post, blog post, blog",
        "content": "Overview  Stephane Maldini @ JOIN  The new normal that is not new  The Reactive Manifesto  Latency &amp; Blocking  The Contract  Reactive Types  Testing &amp; Debuging  Other Changes  RxJava  Spring Framework 5  Conclusion &amp; Do It YourselfStephane Maldini @ JOIN 2016On 5 October 2016, we had the pleasure to welcome Stephane Maldini at our JOIN event.A multi-tasker eating tech 24/7, Stephane is interested in cloud computing, data science and messaging.Leading the Reactor Project, Stephane Maldini is on a mission to help developers create reactive and efficient architectures on the JVM and beyond.He is also one of the main contributors for Reactive support in the upcoming Spring 5 framework, which can be seen as the new standard for reactive applications in the Java world.  You can rewatch his talk on on our Channel on Youtube.The new normal that is not newIt has been around for 30-40 years and boils down to Event-Driven ProgrammingWhat is new is ‚Äúreactive motion bound to specification‚Äù, this means that reactive programming is based on something solid, a specification and no longer some functional concepts.Namely the Reactive Manifesto.Because of this specification, Spring found it the right time to start with Reactor as they could now build something, which would be able to work and where it was clear what people could expect.The Reactive Manifesto  According to the manifesto, reactive systems are  Responsive: respond in a timely manner if at all possible, responsiveness means that problems can be detected quickly and dealt with accordingly.  Resilient: remain responsive in the event of failure, failures are contained with each component isolating components from each other.  Elastic: stay responsive under varying workload, reactive systems can react to changes in the input rate by increasing or decreasing the resources allocated to services.  Message Driven: rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency.This boundary also provides the means to delegate failures as messages.Systems built as reactive systems are thus more flexible, loosely-coupled and scalable. This makes them easier to develop and to allow changes.They are significantly more tolerant of failure and when failure does occur they meet it with elegance rather than disaster.LatencyLatency is also a real issue, the real physical distance of various components and services becomes more important with cloud based systems.This is also a very random number which is difficult to predict because it can depend on network congestion.With Zipkin, you can measure this latency.The same latency can also exist within an application - between the different threads - although the impact will be less severe than between various components.Something needs to be done when latency becomes too big of an issue, especially if the receiver can not process enough.Too much data will fill up the buffer and can result, with an unbounded queue, to the infamous OutOfMemoryException().While you won‚Äôt run out of memory with a circular buffer, you risk losing messages as the oldest ones get overwritten.BlockingOne way to prevent out of memory exceptions is to use blocking.But this can be a real poison pill: when a queue is full, it will block a thread and as more and more queues get blocked your server will die a slow death.Blocking is faster and has better performance, than reactive, but reactive will allow for more concurrency.Concurrency is important if you have a microservice based architecture, as there you typically need to be more careful and more exact when allocating resources between services.As in, by being more concurrent you can save a lot of money when using cloud and microservices.ContractReactive is non-blocking and messages will never overflow the queue, see for the standard definition http://www.reactive-streams.org/.  Created by Pivotal, Typesafe, Netflix, Oracle, Red Hat and others.The scope of Reactive Streams is to find a minimal set of interfaces, methods and protocols that will describe the necessary operations and entities to achieve the goal‚Äîasynchronous streams of data with non-blocking back-pressure.With back-pressure, a consumer which can not handle the load of events sends towards it, can communicate this towards the upstream components so these can reduce the load.Without back-pressure the consumer would either fail catastrophically or drop events.  This contract defines to send data 0 .. N.Publisher is an interface with a subscribe() method.Subscriber has 4 callback methods:onSubscribe(), onNext() (which can be called 0 to N times), onComplete() and onError().The last two signals (complete and error) are terminal states, no further signals may occur and the subscriber‚Äôs subscription is considered cancelled.What is important is the reverse flow and the back-pressure.After subscribing, the subscriber gets a subscription which is a kind of 1 on 1 relationship between the subscriber and the publisher with 2 methods: request and cancel.  Request: this is the more important one, with this method the subscriber will ask the publisher to send x messages (and not more), a so called pull.  Cancel: the subscription is being cancelled.Spring Reactor focuses on the publisher side of the reactive streaming, as this is the hardest to implement and to get right.It provides you with the tools to implement publishers in a back-pressure way.The publisher is a provider of a potentially unbounded number of sequenced elements, publishing them according to the demand received from its Subscriber(s).The Reactive Streams specification has been adopted for java 9.DIY Reactive StreamsImplementing a Reactive Stream framework yourself is very hard to do, for Stephane Maldini this is the 4th or 5th attempt. For Davik Karnok, the tech lead of RxJava, it is attempt 7 or 8.The main difficulty is to make it side effect free.For example:Publisher&lt;User&gt; rick = userRepository.findUser(\"rick\");Note that a publisher is returned instead of directly returning the entity.By doing so it does not block the subscribers when querying for the user and the publisher will produce the user when ready.But by using the specification as is, your publisher might produce 0, 1 or N users, returning an Iterable as result.This is not really practical to work with, as most of the time we are only interested in a single user and not a stream of multiple results.When you would be building the method findOneUser() you also would not want to return an Iterable but just a single User.Also you will have to implement a subscriber to define the action to perform when the result is available.rick.subscribe(new Subscriber&lt;User&gt;(){...});Implementing this subscriber would not be that hard, because the specification has been made so that all complexity lies at the publishers side.Another issue is that you can only subscribe on the publisher, there are no other methods available like map, flatmap, ‚Ä¶The other point is that when designing your own API you will also have to deal with the following issues:  Should work with RS TCK (otherwise it might not work with other libraries as well)  Address reentrance  Address thread safety  Address efficiency  Address state  For Many-To-One flows, implement your own merging operation  For One-To-Many flows, implement your own broadcasting operation  ‚Ä¶  This is all very hard to do yourself.3 Years to MatureIt took Spring Reactor 3 years to mature.2.0 was not side effect free - also existential questions were raised around the project. At the same time Spring evolved and microservices became the norm.Spring needs to work nicely with these microservices, concurrency is important, can Reactor not be used for that?With 3.0 the team wanted to focus on microservices, take some ideas from Netflix OSS and implement these in a pragmatic way.Actually Reactor 3 was started as 2.5, but so many new features were added that the version had to be changed as well in order to reflect this.Since 3.0 Spring Reactor has been made more modular and consists of several components:    Core is the main library.Providing a non-blocking Reactive Streams foundation for the JVM both implementing a Reactive Extensions inspired API and efficient message-passing support.  IPC: back-pressure-ready components to encode, decode, send (unicast, multicast or request/response) and serve connections.Here you will find support for Kafka and Netty.  Addons: Bridge to RxJava 1 or 2 Observable, Completable, Flowable, Single, Maybe, Scheduler, and also Swing/SWT Scheduler, Akka Scheduler.  Reactive Streams Commons is the research project between Spring Reactor and RxJava as both teams had a lot of ideas they wanted to implement.Lots of effort was put in order to create real working, side-effect free operations.Map and Filtering for example are easy, but mergings, like Flatmap are hard to implement side-effect free.Having a proper implementation in the research project for these operations allowed the team to experiment and make it quite robust.This project contains Reactive-Streams compliant operators, which in turn are implemented by Spring Reactor and RxJava.Both the Spring and RxJava teams are very happy with this collaboration and this is still continuing.When a bug gets fixed in Spring Reactor it will also be fixed in RxJava and vice versa.Everything in Reactor is just reactive streams implementation - which is used for the reactive story of spring 5.There also exists an implementation for .NET, Reactor Core .NET and one for javascript Reactor Core TypeScript.Reactive TypesFlux vs Observable  Observable is not implementing Reactive Streams Publisher which means that if you would like to use the Spring 5 save(Publisher&lt;T&gt;) you first have to convert the Observable to a Flowable as you can see in Observable and Flowable.This was too much noise for the Spring team, they are less dependant on Android developers so they could go all in with Java 8.Flux is a Reactive Streams Publisher with basic flow operations, where you start from a static method which will describe how the data will be generated, just() is the simplest wayAfter that you have other operators like Flatmap(), Map(), ‚Ä¶ to work with that dataSome of the method names will be different to RxJava2, but the logic behind these methods has been aligned among RxJava and Spring .Flux.just(\"red\", \"white\", \"blue\")       .flatMap(carRepository::findByColor)       .collect(Result:: new, Result::add)       .doOnNext(Result::stop)       .subscribe(doWithResult);Interface CarRepository {    Flux&lt;Car&gt; findByColor(String color);}This Flux will retrieve all cars which match the color ‚Äúred‚Äù then those with the color ‚Äúwhite‚Äù and finally ‚Äúblue‚Äù.So instead of just three elements, after this Flatmap we are going to have a lot more elements.This is all handled with back-pressure in mind, for example when the flatmap is busy merging data we will not ask for extra recordsIf the Repository implements Flux as a method signature, it will be picked up automatically as a reactive repository.This support for Flux will be part of the whole of Spring 5.Spring Data, Spring Security, Spring MVC, ‚Ä¶ are all good candidates who will have this kind of support.Mono  None is like a flux, but will return at most 1 result, so it does have less methods.Mono.delayMillis(3000)    .map(d -&gt; \"Spring 4\")    .or(Mono.delayMillis(2000).map(d -&gt; \"Spring 5\"))    .then(t -&gt; Mono.just(t + \" world\"))    .elapsed()    .subscribe()This Mono will wait for 3 seconds on the ‚Äúcall‚Äù to Spring 4 or 2 seconds on that of Spring 5.The fastest result will be the one which will be outputted.The Mono has as advantage over an Observable Future of Java 8 that a Mono will only be triggered if you subscribe to it.While with an Observable the call to send() will execute the operation.TestingBlock() exists for very specific use cases and for testing.Never, ever use this in production, as is it blocks your call, which does infer with the Reactive non-blocking statements. ;-)Mono.delayMillis(3000)    .map(d -&gt; \"Spring 4\")    .or(Mono.delayMillis(2000).map(d -&gt; \"Spring 5\"))    .then(t -&gt; Mono.just(t + \" world\"))    .elapsed()    .block()You can also make use of Stepverifier to test Flux, Mono and any other kind of Reactive Streams Publisher.@Testpublic void expectElementsWithThenComplete() {    expectSkylerJesseComplete(Flux.just(new User(\"swhite\", null, null), new User(\"jpinkman\", null, null)));}Use StepVerifier to check that the flux parameter emits a User with ‚Äúswhite‚Äù username and another one with ‚Äújpinkman‚Äù then completes successfully.void expectSkylerJesseComplete(Flux&lt;User&gt; flux) {    StepVerifier.create(flux)            .expectNextMatches(user -&gt; user.getUsername().equals(\"swhite\"))            .expectNextMatches(user -&gt; user.getUsername().equals(\"jpinkman\"))            .expectComplete();}DebugWhen you use reactive libraries you will quickly realize that step debugging is hard especially when you try to read your stacktraces, there are a lot of recursive calls taking place.Before you invoke your operations you can enable an, expensive, debug mode.Hooks.onOperator(op -&gt; op.operatorStacktrace());try {    Mono.just(\"a\")        .map(d -&gt; d)        .timestamp()        . ...                } catch (Exception e) {    e.printStacktrace()}When an exception is returned it will contain the exact operation that failed and the backtrace to that operation.You must enable this Hooks.onOperator before the operations you want to track.More cool stuffParallelFluxIf you want to stress test your CPU you can use ParallelFlux which will spread the workload in concurrent tasks when possible.Mono.fromCallable( () -&gt; System.currentTimeMillis() )    .repeat()    .parallel(8) //parallelism    .runOn(Schedulers.parallel())    .doOnNext( d -&gt; System.out.println(\"I'm on thread \"+Thread.currentThread()) ).    .sequential()    .subscribe()    This basically avoids that you have to write flatMap(), where after the parallel(x) you will have exactly x number of Rails or Flux.Afterwards you can merge these back into a Flux with sequential().A nice feature is that it keeps the code more readable with everything on a single indentation level.But the cool part is that it is also very performant, with parallel, Reactor is very close to the bare metal of what the JVM can do as you can see in the below comparisation:        https://twitter.com/akarnokd/status/780135681897197568Bridge Existing Async codeTo bridge a Subscriber or Processor into an outside context that is taking care of producing non concurrently, use Flux.create(), Mono.create(), or FluxProcessor.connectSink().Mono&lt;String&gt; response = Mono.create( sink -&gt; {    HttpListener listener = event -&gt; {        if (event.getResponseCode() &gt;= 400) {            sink.error(new RunTimeException(\"Error\"));        } else {            String result = event.getBody();            if (body.isEmpty()) {                sink.succes();            } else {                sink.success(body);            }        }    };    client.addListener(listener);        emitter.setCancellation(() -&gt; client.removeListener(listener));});This create() allows you to bridge 1 result, which will be returned somewhere in the future, to a Mono.If you add a Kafka call, for example, where they have this callback so one can return onSuccess and onError you can use Mono.create(): see Reactor Kafka where this is used a lot.Also exists for Flux of N items but it‚Äôs tougher and more dangerous as you must explicitly indicate what to do in the case of overflow; keep the latest and risk losing some data or keep everything with the risk of unbounded memory use. ¬Ø\\(„ÉÑ)/¬ØCreate Gateways to Flux and MonoThere also exist some options to bridge the synchronous world with the Flux and the Mono.Like for example the EmitterProcessor which is a signal processor.  EmitterProcessor&lt;Integer&gt; emitter = EmitterProcessor.create();BlockingSink&lt;Integer&gt; sink = emitter.connectSink();sink.next(1);sink.next(2);emitter.subscribe(System.out::println);sink.next(3); //output : 3sink.finish();But you also have:  ReplayProcessor, a caching broadcaster.  TopicProcessor, an asynchronous signal broadcaster  WorkQueueProcessor, which is similar to the TopicProcessor but distributes the input data signal to the next available Subscriber.These are all an implementation of a RingBuffer backed message-passing Processor implementing publish-subscribe with synchronous drain loops.OptimizationsOperation fusion: Reactor has a mission to limit the overhead in stack and message passing.They distinguish 2 types of optimization:  Macro Fusion: Merge operators in one during assembly time, for example, if the user does .merge() - .merge() - .merge() spring reactor is smart enough to put this in a single .merge()  Micro Fusion: Because of the Reactive specification and the asynchronous nature of the response, queues are heavily used, but creating a queue for every request/response is very costly.Spring Reactor will avoid to create queues whenever possible and short circuit during the lifecycle of the request. They are going to merge the queue from downstream with the one from upstream - hence the name fusion.If the parent is something we can pull (an Iterable or a queue) then Reactor is going to use the parent as a queue, thus avoiding to create a new queue.This is very smart to do - but also very complicated to do yourself, because Spring Reactor has this in place you do not have to deal with this hassle..A Simpler APIReactor: a Simpler API, the entire framework just fits in 1 jar: reactor-core jar.Flux and Mono live in the reactor.com.publisher package, reactor.core.scheduler contains the FIFO task executor.By default the Publisher and Subscriber will use the same thread.With publishOn() the publisher can force the subscriber to use a different thread, while the subscriber can do the same with subscribeOn().For Reactor 3.x there will be more focus on the javadoc, as this has been lagging behind compared to the new features which have been developed.RxJavaWhy Reactor when there‚Äôs already RxJava2?RxJava2 is java 6 while for Reactor the Spring team decided to go all in and focus only on Java 8.This means that you can make use of all the new and fancy Java 8 features.If you are going to use Spring 5, Reactor might be the better option.But if you are happy with your RxJava2, there is no direct need to migrate to Reactor.Spring Framework 5It will still be backwards compatible. You can just take your Spring 4 application, put Spring 5 behind it and you will be good to go.But with Spring 5 you will be able to make use of the following new components/ Spring Web Reactive and Reactive HTTP.Which under the hood support Servlet 3.1, Netty and Undertow.The annotations are still very similar but you just return a Mono, so the User can now be retrieved in a non-blocking way.@GetMapping(\"/users/{login}\")public Mono&lt;User&gt; getUser(@PathVariable String login) {    return this.repository.getUser(login);}ConclusionSpring Reactor is a very interesting framework, after 3 iterations it has matured and gives you a good base to get started with Reactive Streams.With the upcoming support in Spring 5 it will also start to become more mainstream.Therefore I can see no better way then to get your hands dirty and learn more about Spring Reactor yourself.  reactive-programming-part-I:Provides you with a clear description of what reactive programming is about and its use cases.But also the different ways about how people have implemented reactive programming (actor model, futures, ‚Ä¶ ) and more specifially the different frameworks which implement reactive programming in java.          Frameworks like: Spring Reactor, Spring Framework 5, RxJava , Akka, Reactive Streams and Ratpack.        reactive-programming-part-II:You will learn the API by writing some code, how to control the flow of data and its processing.      reactive-programming-part-III:Here you will focus on more concrete use case and write something useful, but also on some low level features which you should learn to treat with respect.        reactor-api-hands-on:This hands-on will help you learn easily the lite Rx API provider by Spring Reactor. You just have to make the unit tests green.    On spring.io you can find more interesting blog posts which will give you more background around Spring Reactor and provide you with the resources to start coding."
      },
    
      "jax-20london-2016-11-18-jax-london-2016-html": {
        "title": "JAX London 2016",
        "url": "/jax%20london/2016/11/18/JAX-London-2016.html",
        "image": "/img/jax-london-2016/jax-london-logo.png",
        "date": "18 Nov 2016",
        "category": "post, blog post, blog",
        "content": "  JAX London is a four-day conference for cutting-edge software engineers and enterprise-level professionals. JAX brings together the world‚Äôs leading innovators in the fields of Java, microservices, Continuous Delivery and DevOps.For this year‚Äôs slogan they decided on: ‚ÄúCreate, Innovate, Code‚Äù.Ordina was present at JAX London on the 11th and 12th of October 2016 where one of our colleagues, Bart Blommaerts, also presented a talk on The Serverless Cloud.In this blogpost we want to give the highlights of some of the talks we followed.Day 1Introduction to JAX London - Sebastian MeyenTo start off, Sebastian Meyen, program chair of the JAX conferences, gave an introduction to JAX London.He mentioned that only half of the attendants of JAX originate from the UK, making it an international conference.JAX is all about ‚Äúopenness‚Äù, we should be celebrating open source and embrace open source thinking as it makes our code smarter.Java is one of the most powerful ecosystems out there according to him and it comes with a unique culture.Sebastian explained that there are different cultures within a company:  Pioneers: They go into the wild and experiment and although they might fail often, they‚Äôre looking at what the next big thing might be  Settlers: They make a valid business model based on the results of the research of the pioneers and make stable technology for it  Town builders: They look at the portfolio of the settlers and decide what to industrialise, they want to create volumeHe stresses that innovation happens on all three levels and not just only at the pioneers or settlers.Furthermore he went briefly over the different genres of the talk and the conference app before introducing James Governor.Opening keynote: Java for Cloud Natives - James GovernorJames Governor opens up his talk by showing how Java is still high on the Programming Language rankings despite the ‚ÄúJava is dead‚Äù-doomspeak every now and then.Regarding Java frameworks, Spring and Spring Boot are really crushing it.He also mentions that most startups usually start with a new and fancy language but as they mature, a lot of them actually turn into Java shops.Examples of this being: Uber, LinkedIn, Netflix, Twitter, Amazon, Etsy, Facebook, Yahoo and Google.Twitter for example started in Ruby and during the US presidential election in 2012 they migrated to Scala and Java on the JVM for scalability and performance reasons.After the migration they managed to sustain peaks of TPS (Tweets Per Second) for hours, at one point even reaching 15,107 TPS.James went through a couple of companies and the transformations they underwent for staying competitive.Amazon for example started with a messy code base but they did manage to refactor it.Being a top down company, they also managed to create small teams.Netflix is a similar case, at a certain point they had a messy codebase but they really put a lot of effort into refactoring it.They also invested a lot in their software engineers and continue to do so.Open source is the new normal, there are a lot of cool open source frameworks around that you can contribute to such as Zookeeper, Spark, Kafka, Hadoop, Giraph, Jenkins, Cassandra,‚Ä¶It‚Äôs awesome when enterprises contribute to open source.  Bosch: They are doing interesting work with Eclipse Foundation, they know they need to do open source so they make open source contributions  Comcast: Everybody hates Comcast in the US, but they are making open source contributionsJames mentions a couple of things he finds important:  Microservices and container based deployment such as Docker and Kubernetes are very hot topics  Break down the technical model and teams  Continuous integration is very important, there are still people not using it!  Make people responsible for their own Quality Assurance  Embrace failure and graceful degradationFinally, he mentioned that there is always the need to deal with the politics and that governance is still important and needed.Oracle needs to give their commitment to Java and the Cloud and we as a community need to encourage Oracle to step up and not only complain about them.Links:  PresentationDeveloping Microservices with aggregates - Chris RichardsonThe goal of Chris Richardson‚Äôs presentation was to show how Domain-Driven Design aggregates and microservices are a perfect match.A microservice based architecture tackles complexity through modularisation.A microservice should be seen as a business capability for example a catalog service, review service or order service.By having service boundaries you enforce modularity.Also important to mention is that each microservice needs to have its own database, microservices do not share a database!Finally there should always be an API gateway in front of the microservices, which is the entrypoint for the frontend user interface and mobiles devices.They should never access your microservice directly!Chris strongly suggest reading Domain Driven Design by Evan Evans.The core building blocks of Domain-Driven Design are the following:  Entities  Value objects  Services  Repositories  AggregatesAbout Domain-Driven Design aggregates:  Cluster of objects that can be treated as a unit  Graph consisting of a root entity and one or more other entities and value objects  Typically business entities are aggregates e.g., Customer, Account, Order, ProductChris talked about the problems you have to handle when dealing with microservices and that practicing Domain-Driven Design can help you a lot to address these.He mentioned that you would probably ask yourself how you can enforce invariants if the microservices reside in different JVMs.You would be reliant on ACID transactions to maintain consistency.Transactions violate encapsulation and require 2-Phase Commits (2PC) which is not an option because of the following reasons:  It guarantees consistency but 2-Phase Commit is a single point of failure  It is a chatty protocol: at least O(4n) messages, with retries O(n^2)  Reduced throughput due to locks  Not supported by many NoSQL databases (or message brokers)  Doesn‚Äôt fit in a NoSQL mindsetAggregates are a solution to these ACID transactions since they allow you to use eventual consistency.You reference other aggregate roots via an identity, this being the primary key.If an update must be atomic then it must be handled by a single aggregate therefore aggregate granularity is important.You should have your aggregates as fine grained as possible.In an Event-driven architecture you can work with steps where each step publishes an event that trigger the next step in the sequence.You will need to write custom logic to compensate the well known ACID transaction rollback so careful design is required.Using Event Sourcing with Aggregates:  There is no notion of updating the database and then publishing the event but you rather just publish the event  For each Domain-Driven Design aggregate:          Identify state changing domain events (eg with Event Storming)      Define Event classes (for example Order events: OrderCreated, OrderCancelled, OrderApproved, OrderRejected, OrderShipped, ‚Ä¶)        Persist events and NOT current state          Store the events themselves in a database      Replay events to recreate state        All Aggregates are storing their events in the Event Store, each aggregate subscribes to events of the other aggregates          When using CQRS, update the view after processing an event      Benefits of Event Sourcing:  Solves data consistency issues in a microservice/NoSQL based architecture  Reliable event publishing needed by predictive analytics, user notifications, etc  Eliminates object relationship mapping problems  Rectifies state changes:          Built in, reliable audit log      Temporal queries      Preserved history      Drawbacks of Event Sourcing:  Requires application rewrite (mechanical transformation)  Learning curve: weird and unfamiliar style of programming  Events can be a historical record of your bad design decisions          You will probably implement a mechanism to deal with that such as versioning        Must detect and ignore duplicate events:          Write idempotent event handlers      Track most recent event and ignore older ones        Querying the event store can be challenging:          Event store might only support lookup of events by identity id      Must use CQRS to handle queries meaning application must handle eventually consistent data      Example application: https://github.com/cer/event-sourcing-examples  Orders and Customers example with Kafka in between as backchannel  Both of them connected with the event store  Written in SpringSummary  Aggregates are the building blocks of microservices  Use events to maintain consistency between aggregates  Event Sourcing is a good way to implement an event-driven architectureSecure by design - Eoin WoodsEoin Woods went over security and that we should care about it for things like protection against malice, mistakes and mischance, theft, fraud, destruction, disruption,‚Ä¶He mentioned OWASP top 10 which he approves and should definitely be checked out if you haven‚Äôt seen it.There are four aspects of security practice:  Secure Application Design  Secure Infrastructure Design  Secure Application Implementation  Secure infrastructure deploymentHe explains that there are many sets of security design principles but he notes that there are many similarities between them at a fundamental level:  Viege and McGraw (10)  OWASP (10)  NIST (33)  NCSC (europe)(44)  Cliff Berg‚Äôs set (185)Out of all these, Eoin has distilled 10 key principles himself:  Least privilege          Why: Broad privilege allows malicious or accidental access to protected resources      Principle: Limit privileges to the minimum for the context      Tradeoff: It is less convenient, less efficient and adds more complexity      Example: Run a server process with the minimum privileges        Separate responsibilities          Why: Achieve control and accountability, limit the impact of successful attacks, make attacks less attractive      Principle: Separate and compartmentalise responsibilities and privileges      Tradeoff: Development and testing costs, operation complexity, troubleshooting can be more cumbersome      Example: Admins of a submodule should have no access to other module features        Trust cautiously          Why: Many security problems are caused by inserting malicious intermediaries in communication paths      Principle: Assume unknown entities are untrusted, have a clear process to establish trust, validate who is connecting      Tradeoff: Operational complexity, reliability, some development overhead      Example: Don‚Äôt accept untrusted RMI connections, use client certificates, credentials or network controls        Simplest solution possible          Why: Security requires understanding of the design, complex design is rarely understood, simplicity allows analysis      Principle: Actively design for simplicity - avoid complex failure modes, implicit behaviour, unnecessary features,‚Ä¶      Tradeoff: Hard decisions on features provided and needs serious design effort in order to be simple      Example: Does the system needs a dynamic runtime config via a custom DSL?        Audit Sensitive Events          Why: Provide record of activity, deter wrongdoing, provide a log to reconstruct the past, provide a monitoring point      Principle: Record all security significant events in a tamper-resistant store      Tradeoff: Performance, operational complexity, development cost      Example: Record all changes to ‚Äúcore‚Äù business entities in an append-only store        Secure defaults &amp; fail securely          Why: Default passwords, ports and rules are ‚Äúopen doors‚Äù. Failure and restart states often default to ‚Äúinsecure‚Äù      Principle: Force changes to security sensitive parameters. Think through failures - Must be secure but recoverable      Tradeoff: Convenience      Example: Don‚Äôt allow ‚ÄúSYSTEM/MANAGER‚Äù after installation. On failure don‚Äôt disable or reset security controls        Never rely on obscurity          Why: Hiding things is difficult - someone is going to find them, accidentally if not on purpose      Principle: Assume that the attacker has perfect knowledge, this forces secure system design      Tradeoff: Designing a truly secure system takes time and effort      Example: Assume that an attacker will guess a ‚Äúport knock‚Äù network request sequence or a password encoding        Defence in-depth          Why: Systems do get attacked, breaches do happen, mistakes are made - need to minimise impact      Principle: Don‚Äôt rely on single point of security, secure every level, stop failures at one level from propagating      Tradeoff: Redundancy of policy, complex permissioning and troubleshooting, can make recovery harder      Example: Access control in UI, services, database, OS        Never invent security tech          Why: Security technology is difficulty to create, it‚Äôs a specialist‚Äôs job, avoiding vulnerabilities is difficult      Principle: Don‚Äôt create your own security technology, always use a proven component      Tradeoff: Time to assess security technology, effort to learning it, complexity      Example: Don‚Äôt invent your own OSS mechanism, secret storage, crypto libs, use existing proven ones        Secure the weakest link          Why: ‚Äúpaper wall‚Äù-problem, common when focus is on technologies and not threats      Principle: Find the weakest link in the security chain and strengthen it, repeat (threat modelling)      Tradeoff: Significant effort required, often reveals problems at the least convenient moment      Example: Data privacy threat met with encrypted communication but with unencrypted db storage and backups      Links:  Presentation  Book: Software Systems Architecture  UK Government NCSC  NIST Engineering Principles for IT Security  OWASP Security by Design PrinciplesEvent-driven Microservices - Jeremy DeaneJeremy Deane starts off telling that within Event-driven architectures (EDA) you have events representing a snapshot in time of an occurrence within a system.We should distinguish the following Enterprise Integration Patterns (EIP):  Event Message  Command Event  Event SourcingAs for possible Middleware solutions:  ActiveMQ (JSM), RabbitMQ (AMQP)  Kafka, ZeroMQ  AkkaThere are a couple of EDA principles to take in mind:  Events are emitted by a Producer and received async, and optionally acted upon, by a stateless Consumer  Streams are sets of related Events  Intermediate Processors can enrich the raw Event  Ideally, Producer and Consumer should be decoupled so they can evolve independently over time  Producer should be a magnanimous writer and consumer should be a tolerant reader  Consumers can listen to Event Queues or subscribe to Event TopicsTo give some EDA examples:  Fraud prevention  Medical Alerting (ER check-in)  Financial Portfolio Management  Supply Chain ManagementMicroservices Architectural Style:  Application as a suite of small services  Each running in its own process  Communicating with lightweight mechanisms  Built around business capabilities and independently deployable by fully automated deployment machinery  Bare minimum of centralized managementJeremy really likes Apache ActiveMQ for several reasons:  Not the fastest but the easiest to implement and maintain  Easy to learn  Great and active community  High Availability via a master/slave approachFinally he did a demo of an application built with ActiveMQ and Apache Camel to show how well and easy they integrate together.The source of the demo is available at the links below.Links:  Presentation  https://github.com/jtdeane/event-driven-microservices  https://github.com/jtdeane/camel-standalone-routerOperating the Spotify backend - Niklas GustavssonNiklas Gustavsson immediately starts off with the culture at Spotify.Spotify made two very nicely animated videos that explain their culture:  Spotify Engineering Culture Part 1  Spotify Engineering Culture Part 2Basically it comes down to having autonomous teams called squads containing of 7-12 team members:  A squad should be fully staffed for their ‚Äúmission‚Äù  A squad can decide how they want to tackle their issue  Each squad is on call for their own microservicesAt Spotify they have hundreds of fairly small services that only do one thing well.Each service is owned by a squad that implements and deploys it into production.Later on, they started putting ops guys in the squads in order to be able to tackle issues a lot faster.Jeremy also stresses that it‚Äôs very important to automate as much as possible.At Spotify they have a simple incident process to avoid the same issues from reappearing:  Something explodes, an incident gets created and it gets fixed  (Blameless) post-mortem meeting  Remediations to make sure it never happens again  The incident is closedSystem-Z is the service registry in which all services in production get registered in and the following information is available for each microservice:  Who‚Äôs on call for each service  When a service is down  What the dependencies are for each service  The hardware in use  The amount of instancesAlso, in order to encourage squads to register their microservices, if you‚Äôre not registering your services in there you‚Äôre not getting any hardware and thus you can‚Äôt get your service into production.In order to maintain their hardware they utilise the Cortana Pool Manager where they have the following information available:  See hardware available  Specify how much you need like how many instancesThey currently do a lot of self-hosting but are planning to fully migrate to Google Cloud Platform.In order to manage their Docker containers they use Helios which is their own Docker orchestration platform.Links:  PresentationJAX Innovation AwardsThe JAX Innovations Awards reward the most innovative contributions in the shape of technology, idea, documentation, industry impact or community engagement.Most innovative contribution to the Java Ecosystem:  1st: Spring Boot  2nd: Camunda BPM Platform  3rd: SparkInnovation in Software delivery &amp; DevOps:  1st: Docker  2nd: Prometheus  3rd: habitatSpecial Jury Award:  Let‚Äôs EncryptExtra: Special Honour Award:  Pieter Hintjes (1962-2016) who passed away last week          ‚ÄúLook at the internet, that‚Äôs how software should be, so resilient‚Äù      Worth a read: Confessions of a Necromancer      Day 2Opening keynote: Decision-making in the face of disruption - Duncan PearsonDuncan is a Chief Architect at Anaplan.Anaplan is the leading planning and performance management platform for smart business.It provides a mechanism that helps the business understand the consequences of what they intent to do.Duncan first sums up the forms of disruption:  Market replacement          Film -&gt; Digital camera      Digital Camera -&gt; smartphone      GPS -&gt; smartphone        Market change          Free online ad-funded services      Comparison websites        New market          Personal gene technology      Recorded music        Environmental change          Natural disaster      War and civil unrest      Time is of the essence and it is important to act quickly to a window of opportunity when trying to tackle a changing situation over time.There are two perspectives on disruption:  The disruptors          High organisational growth leading to complexity transitions      High sensitivity to: growth rate mismatch and delay effects      For example: DataStax, Facebook, Salesforce, Groupon, Box, OpenDNS        The disrupted          New business models required      Disconnect between business functions and supporting systems      Entrenched organisational systems      For example: Yahoo, Toyota, PWC, Verizon, Vodafone, Cisco      Duncan mentions that there are two kinds of systems: modelling systems and execution systems.A good modelling system has the following characteristics:  Personal  Flexible  General purpose  It has to feel like a spreadsheetWhereas a good execution system tends to be:  Specialist, use-case-specific  Embed the ‚Äúmodel‚Äù in the software  Secure / multi-user  User-task focusedAn execution system tends to be a system that has already been analysed in-depth.Influence on design:  Grids with pivots, recalculations, access to formulae everywhere, inline changes to names/formulae/structure          It has to support the task-focused application developer                  Easy screen and navigation design          Persistent navigational context          Security &amp; access control                    Completely flexible                  Anything can happen at any time          Remodelling the business should be easy          Fast delivery time                    Scale and perform                  If it looks like a spreadsheet then it had better behave like one even if you are changing billions of numbers                    A business management tool                  No programming          No DB design required                    Influence on the company:  We were the disruptor, on a tight deadline          Success through simplicity      Something that works today is better than something that is right tomorrow      Led by our customers, driven by the features that they need        We are undergoing inevitable complexity transitions          New HR systems, management processes, multiple dev locations      Standard Compliance (security, development process)      The Serverless Cloud - Bart BlommaertsBart Blommaerts starts off explaining that serverless, although as suggested by the name, does not actually mean that you‚Äôre not using any server.What it does mean is that the existence of a server is hidden for you.The name might be a bit misplaced and it would be more correct to simply call it Functions as a Service (FaaS).Serverless or FaaS might actually be the next step in the evolution of cloud computing.Going serverless will lead to less worries seeing as you no longer have the server management to take care of.Security updates, scalability and availability is all taken care off for you, by the provider.On the other hand, more trust must be put into the provider.The need for an ops team will not fade since you still need to take care of things such as monitoring, debugging support, memory management, application configuration and more.All these things could be handled by a specialised, outsourced team.Seeing as serverless is rather new, there is also the opportunity for tooling to be built!We can distinguish the following characteristics:  Event-driven  Pay per execution instead of server uptime  The actually invocation cost depends on how long it took to execute the callAn example of the pricing of AWS:  Pay per 100ms minimum each call  3.2 million free tier seconds per month  Outside of the free tier the price per 100ms is $0.000000208Serverless comes with seamless scaling:  No risk of under- or over-provisioning  Short-lived ‚Äúcompute containers‚Äù that:          are isolated from other functions      have their resources provided from the function config      may be reused      Serverless is stateless, there is no state persisted in between invocations.In order to preserve state you can still use a db, file system or cross-application cache.Bart then listed a couple of the current providers:  AWS Lambda          Integrates well with all other AWS services        IBM Bluemix OpenWhisk          OpenWhisk is open source and available on Github      Better UI than AWS and also has a dashboard for monitoring        MS Azure Cloud Functions          Mature system      Different payment model compared to AWS that could be more expensive in the long run        Google Cloud Functions          A new initiative by Google, but it benefits from services Google offered over 5 years ago      Currently reimplementing it      Currently in private alpha      Very active community        Auth0 WebTask          CLI      Really simple to use, you can be up and running in 30 seconds      Well documented with examples      Serverless enables experimentation due to how easy it is to get something up and running and the low running costs.It could lead to a more collaborative economy seeing as a lot of companies are sitting on a ton of data currently not being used.All this data could be made public by publishing an API and others could consume the data and combine it with their own data, enriching it.In Belgium there is already a company, Realo, that only combines data and they seem really popular.Bart did a demo using an Arduino measuring the temperature.The Arduino then sends the data off to multiple providers, enriching the data, after which it gets logged in the final step.    During the demo, Bart mentioned that he used the Serverless Framework for development.The Serverless Framework is a CLI, soon to be supplier independent (at the time of writing), with the following features:  Scaffolding  Best practices (grouping of functions)  Lifecycle support (create, deploy, invoke,‚Ä¶)The demo and code samples are available at:  https://github.com/bart-blommaerts/serverless-demo  https://github.com/ordina-jworks/lora-iot-demoBart notes that the size of the function is important as it has an impact on execution time and will affect the cost.For example a service written in Java could be 34MB whereas in NodeJS the exact same function may only be 100KB.Best practices:  Compare the following regarding supplier choice:          Integration      Offering      Tooling available      Execution time is limited so check if it fulfils your needs      SLA available?        Code          Initialise services outside of the function such as making a database connection      Limit function size      Use an external editor and a VCS      Bart also recently wrote a blogpost on The Serverless Cloud, you should definitely check it out if you‚Äôre interested on the subject.Links:  PresentationFour Distributed Systems Reference Architectures - Tim BerglundAccording to Tim Berglund, a distributed system is a collection of independent computers that appear to its user(s) as one computer.Independent computers should operate concurrently, fail independently and should not share a global clock.The point of Tim‚Äôs talk is to compare the following four systems:  Modern 3-tier Architecture  Sharded Architecture  Lambda Architecture  Serverless ArchitectureEach system has its strengths and weaknesses compared and finally receives a rating for the scalability, hipness and difficulty of use.Modern 3-tier ArchitectureThe classic Presentation, Business and Data tier.  Before: JSP, EJB/Servlet, Oracle DB  Now: React.JS, Node.JS, CassandraStrengths  Rich front-end framework (scale/UX)  Hip, scalable middle tier (such as Spring)  Basically infinitely scalable data tierWeaknesses  State in the middle tierRating  Scalability: 4/5  Hipness: 2/5  Difficulty: 3/5Sharded ArchitectureWith a Sharded Architecture the Business and Data tier is usually sharded.In between the clients and the business tier there‚Äôs usually a router that performs loadbalancing and routes the request to the service of the application.The Data tier contains multiple databases.Strengths  Client isolation is easy (data and deployment)  Known, simple technologiesWeaknesses  Complexity  No comprehensive view of data  Oversize shardsRating  Scalability: 3/5  Hipness: 1/5  Difficulty: 4/5Lambda ArchitectureStreaming data versus handling data in batch or even better, unbounded data vs bounded data.Event-driven, events are either stored in a database or processed via an Event Processing framework such as Apache Storm.Storing them in a database includes:  Long-term storage  Bounded analysis  High latency.On the other hand, processing them via an Event Processing framework includes:  Temporary queueing  Unbounded analysis  Low latencyStrengths  Optimises subsystems based on operational requirements  Good at unbounded dataWeaknesses  Complex to operate and maintain  Bad at mutable/update-in-place dataRating  Scalability: 3/5  Hipness: 3/5  Difficulty: 5/5ServerlessDoes not mean that there isn‚Äôt any server, just that you no longer maintain it.Also called Functions as a Service (FaaS), these can be seen as ‚Äúextreme‚Äù microservices.Strengths  Potentially low resource cost  Rigorous microservices disciplineWeaknesses  Can be very complex  Requires a different way of thinkingRating  Scalability: 4/5  Hipness: 5/5  Difficulty: 5/5Links:  Presentation"
      },
    
      "cloud-2016-11-12-theserverlesscloud-html": {
        "title": "The Serverless Cloud",
        "url": "/cloud/2016/11/12/TheServerlessCloud.html",
        "image": "/img/serverless.jpg",
        "date": "12 Nov 2016",
        "category": "post, blog post, blog",
        "content": "  In recent years, the uprise of the cloud has brought us a lot of new and disruptive technologies. Everybody is talking about SaaS, PaaS, IaaS and other sorts of aaS. In 2014, Amazon launched AWS Lambda as the pinnacle of the cloud computing. It allows developers to focus on code, without spending time on managing servers.Part 1What?While Microservices have been reigning the Cloud landscape for a couple of years, today the Serverless movement is one of the hottest trends in the industry. Historically, software developers have been pretty bad at naming things and Serverless is no exception. Disregarding what the name suggests, Serverless does not imply the complete absence of servers. It implies that developers who are using the Serverless architectural style, are not responsible for managing or provisioning the servers themselves, but use a vendor-supplied Cloud solution. Serverless means less worrying about servers. Although in the future, it might be possible to install this kind of service on-premise, for example with the open-source IBM OpenWhisk implementation.In regard to this, the definition FaaS: Functions as a Service makes a lot more sense. Functions are short-lived pieces of runtime functionality that don‚Äôt need a server that‚Äôs always running. Strictly speaking a function can have a longer execution time, but most FaaS providers will currently limit the allowed computation time. When an application calls a function (eg. a calculation algorithm), this function gets instantiated on request. When it‚Äôs finished, it gets destroyed. This leads to a shorter ‚Äúrunning‚Äù time and thus a significant financial advantage. As an example, you can find the AWS Lambda pricing here. FaaS functions are also a great match for event-driven behaviour: when an event is dispatched, the function can be started instantly and ran only for the needed time. A Serverless application is a composition of event chaining. This makes the Serverless style a natural match for API Economy.As a result of being runtime components, FaaS functions are stateless and need to rely on a database (or file system) to store state. Being stateless and short-lived naturally lead to extreme horizontal scaling opportunities and all major FaaS providers support these.NoOpsNoOps (No Operations) is the concept that an IT environment can become so automated and abstracted from the underlying infrastructure that there is no need for a dedicated team to manage software in-house. NoOps isn‚Äôt a new concept as this article from 2011 proves. When Serverless started gaining popularity, some people claimed there was no longer a need for Operations. Since we already established that Serverless doesn‚Äôt mean no servers, it‚Äôs obvious it also doesn‚Äôt mean No Operations.It might mean that Operations gets outsourced to a team with specialised skills, but we are still going to need: monitoring, security, remote debugging, ‚Ä¶ I am curious to see the impact on current DevOps teams though. A very interesting article on the NoOps topic, can be found over here.AWSAWS LambdaAWS Lambda was the first major platform to support FaaS functions, running on the AWS infrastructure. Currently AWS Lambda supports three languages: Node.js, Java, and Python. AWS Lambda can be used both for synchronous and asynchronous services.Currently the tooling for AWS Lambda is still relatively immature, but this is changing rapidly. At the time of writing, the AWS Lambda console offers the possibility to create a Lambda using blueprints. This is already easier than setting up a lambda by hand (using a ZIP-file). Blueprints are sample configurations of event sources and Lambda functions. Currently 45 blueprints are available. To give a short introduction, we‚Äôll select the hello-world blueprint. This blueprint generates a very simple NodeJS function:'use strict';console.log('Loading function');  exports.handler = (event, context, callback) =&gt; {   console.log('value1 =', event.key1);   console.log('value2 =', event.key2);   console.log('value3 =', event.key3);   callback(null, event.key1); };After creating this function, it can be immediately be tested from the console, using a test event. If we want to call this function synchronously, we need to create an API endpoint with the AWS API Gateway. The API Gateway creates API‚Äôs that acts as a ‚Äúfront door‚Äù to your functions. To make this work with the events in our hello-world example, we need to select the resources of our API:In Integration Request, we add a body mapping template of type application/json with the following template:{ \"key3\": \"$input.params('key3')\",\"key2\": \"$input.params('key2')\",\"key1\": \"$input.params('key1')\"}In ‚ÄòMethod request‚Äô we add 3 URL String query parameters: key1, key2 and key3. If we then redeploy our API, hitting the Test button gives us an input form to add the 3 query parameters and the function is executed successfully:If you want to test this directly from a browser, you will need to change the Auth to NONE in the ‚ÄòMethod request‚Äô and do a new deploy of the API. The URL itself can be found in the ‚Äòstage‚Äô-menu.This example obviously is not very interesting, so let‚Äôs try another blueprint: microservice-http-endpoint. This will generate a CRUD backend, using DynamoDB with a RESTful API endpoint. The code generated, covers all common use-cases:'use strict';letdoc = require('dynamodb-doc');letdynamo = newdoc.DynamoDB();exports.handler = (event, context, callback) =&gt; {   const operation = event.operation;   if(event.tableName) {      event.payload.TableName = event.tableName;   }   switch(operation) {   case'create':      dynamo.putItem(event.payload, callback);      break;   case'read':      dynamo.getItem(event.payload, callback);      break;   case'update':      dynamo.updateItem(event.payload, callback);      break;   case'delete':      dynamo.deleteItem(event.payload, callback);      break;   case'list':      dynamo.scan(event.payload, callback);      break;   case'echo':      callback(null, event.payload);      break;   case'ping':      callback(null, 'pong');      break;   default:      callback(newError(`Unrecognized operation \"${operation}\"`));   }};Obviously you will need a DynamoDB instance with some data in it:You can reference your new table, from your lambda, using the following event:{\"tableName\": \"garage-car-dev\",\"operation\": \"list\",\"payload\": { }} The only difficult part remaining, is finding out the required payload for the different operations :) This is a good start for creating new records:{\"operation\": \"create\",\"tableName\": \"garage-car-dev\",\"payload\": {   \"Item\": {      \"id\": \"1980b61a-f5d7-46e8-b62a-0bbb91e20706\",      \"body\": \"Lamborghini\",      \"updatedAt\": \"1467559284484\"      }   }}The blueprint also generates an API in the API Gateway that we can invoke with the above events as body mapping template in integration request of the method execution, just like the first example.Serverless FrameworkWhile the above approach works as expected, it‚Äôs quite cumbersome to get your first function working. Especially since we didn‚Äôt write any actual code in the previous examples. Luckily the Serverless Framework (formerly JAWS) is here to make our lives easier. Currently the Serverless Framework only supports AWS Lambda, but support for other IaaS providers is coming. A pull-request for Microsoft Azure already exists and other providers are also working on an implementation. Vendor-neutral FaaS would be a true game-changer!One problem with FaaS, is the (deliberate) mismatch between runtime unit and deploy unit. This is also true for other architectural patterns. It should be possible to deploy one specific function, but often functions will hang out in groups. I‚Äôd prefer to deploy a group of functions in one go, when it makes sense, eg. different CRUD operations on the same resource. This way, we benefit from the advantages of functions (scalability, cost, service independence, ‚Ä¶) but also ease deployment. This is a key feature of the Serverless Framework.On June 29th, Serverless V1.0-alpha1 was announced. New Alphas and Betas will be released on a regular basis. Currently the documentation can only be found in their v1.0 branch on GitHub. Serverless V1.0 introduces the ‚ÄúServerless Service‚Äù concept, which is a group of functions with their specific resource requirements. In essence Serverless V1.0 is a powerful and easy to use CLI to create, deploy and invoke functions. Serverless V1.0 uses AWS CloudFormation to create AWS resources. It uses the default AWS profile for access to your AWS account. Creating, deploying and invoking a ‚ÄúHello World‚Äù NodeJS function with Serverless is as easy as:serverless create --name cars --provider aws serverless deploy serverless invoke --function hello --path data.jsonThis generates the following lambda:'use strict'; module.exports.hello = (event, context, cb) =&gt; cb(null,    { message: 'Go Serverless v1.0! Your function executed successfully!', event } ); The current version of the Serverless Framework (unfortunately) doesn‚Äôt use the region from the AWS config, so you might need to look for your function in a different region.Adding an API Gateway endpoint, is also very easy and can be done by adding this http-event:events:  - http:       path: greet       method: getThe actual URL can be found in the API Gateway in the stages section, as we saw before.Part 2In the first part of these article, I introduced the Serverless architectural style and focused on ‚Äúmarket maker‚Äù AWS Lambda and on the Serverless Framework. In this part, I want to focus on other Faas providers.Auth0 WebtaskCompared to giants such as Amazon, Google, Microsoft and IBM, Auth0 is a rather small player. However acknowledging their experience with BaaS (Backend as a Service), FaaS is a logical choice for them. Currently Webtask only supports NodeJS.The recommended way of using webtask is through the wt command line interface. Auth0 has put the focus on easy of use. This is really visible by looking at their 30 second example. The wt create command wil generate a function (a webtask) and will automatically return an HTTP endpoint, supporting URL query parameters. Every query parameter is available in your webtask in the form of context.data JavaScript object. With AWS Lambda you need to configure these in the AWS API Gateway, which is both tedious and time-consuming.A very interesting feature of Webtask is the availability of built-in storage.Webtask code can store a single JSON document up to 500KB in size. This data can be stored with ctx.storage.set and retrieved with ctx.storage.get. While I don‚Äôt believe your function will often need this, it‚Äôs a very nice option.This small example (using Lodash), shows a webtask using a query parameter and built-in storage.module.exports = function (ctx, cb) {    var name = ctx.query.name;     if(name) {        ctx.storage.get(function(err, data){            if(err) cb(err);             data = data || [];             if(_.indexOf(data, name) === -1 ){                data.push(name);                 ctx.storage.set(data, function(err){                    if(err){                        cb(err);                    } else {                        cb(null, data);                    }                })            } else {                cb(null, data);            }        })    } else {        cb(null, \"422\");    }}Deploying this webtask, using the CLI:Webtask created You can access your webtask at the following url: https://webtask.it.auth0.com/api/run/wt-&amp;lt;your username&amp;gt;-0/query_store?webtask_no_cache=1Another way to access your webtask is as a CRON job, using the wt cron command or as a web hook.Contrary to AWS Lambda, you don‚Äôt need to bundle the NodeJS modules you want to use. The list of supported modules is available here. An option to bundle other modules is also available. Another difference is the use of query parameters.Not surprisingly, Webtask can be integrated with Auth0 for authentication and authorization.Google Cloud FunctionsGoogle Cloud Functions (GCF) was released early 2016 and is currently in private alpha. Being in private alpha not only means that you specifically need to request access to use the GCF API, but also that you‚Äôre limited in sharing information. While this is obviously very unfortunate, it also means that Google is very serious about releasing a complete product. The activity in their (again private) Google Group proves this.Like its competitors, Cloud Functions can be triggered asynchronously by events (from Cloud Pub/Sub and Cloud Storage) or invoked synchronously via HTTPS. Currently GCF only supports NodeJS. Tutorials on common use-cases are available in their documentation. To build functions with GCF, you will first need to download and install the Google Cloud SDK. With the SDK installed, you can create your initial function (replace datastore_gcf with your own staging bucket name):$ gsutil mb gs://datastore_gcfFrom the (very useful) (unofficial) GCF recipes by Jason Polites (Product Manager, GCP), we cloned the datastore example that will persist data to a Google Coud Datastore.From this repository, we deployed 2 functions ‚Äòds-get‚Äô and ‚Äòds-set‚Äô by executing:$ gcloud alpha functions deploy ds-set --bucket datastore_gcf --trigger-http --entry-point setThe names of the deployed functions, need to be exported in the Node.js module. These functions can be called with:$ gcloud alpha functions call ds-get --data '{\"kind\": \"test\", \"key\": \"kid\"}'or via the Cloud Functions Console.Your newly added data is also available in the Datastore Entities after selecting a project on the top. After executing a couple of functions, you can also find some metrics of your function (number of calls, execution time, ‚Ä¶)Other arguments for the deploy command are listed in the reference documentation. These steps are also available in the Cloud Platform Console.After deployment, your webtrigger URL will be displayed similar to Webtask.Although much information on Google Cloud Functions is not (publicly) available yet, Google is well on its way to become a serious FaaS provider.Azure FunctionsSimilar to Google Cloud Functions, Microsoft Azure Functions is currently in preview stage, meaning it‚Äôs not (yet) meant to be used in a production environment. Azure Cloud Functions (ACF) support a variety of languages such as NodeJS, C#, Python, and PHP.Today, it can be used for these common cases:  Events triggered by other Azure services  Events triggered by SaaS services (not limited to Microsoft)  Synchronous requests  WebHooks  Timer based processing (CRON)creating quite a large number of possibilities.Azure Functions are grouped in App Services. This is quite different from AWS Lambda, where the functions are organised independently. Hardware resources are allocated to an App Service and not directly to an Azure Function. It‚Äôs important to select a dynamic App Service if you‚Äôre aiming for ‚Äúpay-per-execution‚Äù.When creating a new function, you can start from different templates. This can be compared to the blueprints from AWS Lambda. Currently 44 templates are available (but some are very similar). When selecting HttpTrigger for example, Azure Functions will generate a function that is able to use all query parameters passed to the function, similar to Webtask. This short video demonstrates this use case.In the example below, an Azure Cloud Function will store entities in a Storage Table when it receives an HTTP request:function.json: \"bindings\": [    {      \"type\": \"httpTrigger\",      \"direction\": \"in\",      \"name\": \"req\",      \"methods\": [        \"post\"      ],      \"authLevel\": \"function\"    },    {      \"type\": \"http\",      \"direction\": \"out\",      \"name\": \"res\"    },    {      \"type\": \"table\",      \"name\": \"outTable\",      \"tableName\": \"entities\",      \"partitionKey\": \"functions\",      \"rowKey\": \"%rand-guid%\",      \"connection\": \"YOUR_STORAGE\",      \"direction\": \"out\"    }  ],  \"disabled\": false}index.js:    var statusCode = 400;    var responseBody = \"Invalid request object\";     if (typeof req.body != 'undefined' &amp;amp;&amp;amp; typeof req.body == 'object') {        statusCode = 201;        context.bindings.outTable = req.body;        responseBody = \"Table Storage Created\";    }     context.res = {        status: statusCode,        body: responseBody    };     context.done();};To retrieve the added entities:functions.json:  \"bindings\": [    {      \"type\": \"httpTrigger\",      \"direction\": \"in\",      \"name\": \"req\",      \"methods\": [        \"get\"      ],      \"authLevel\": \"function\"    },    {      \"type\": \"http\",      \"direction\": \"out\",      \"name\": \"res\"    },    {      \"type\": \"table\",      \"name\": \"inTable\",      \"tableName\": \"entities\",      \"connection\": \"YOUR_STORAGE\",      \"direction\": \"in\"    }  ],  \"disabled\": falseindex.js:    context.log(\"Retrieved records:\", intable);    context.res = {        status: 200,        body: intable    };    context.done();};What immediately struck me was the quality of their documentation (videos, tours, quickstarts, templates, ‚Ä¶) and the user experience from the Azure Portal. The portal can be a little slow sometimes, but the experience is miles ahead of what Amazon and Google are offering. Azure Functions is open source and available on GitHub.Azure Functions will soon be supported by the Serverless Framework, which is a big step towards vendor-neutral FaaS.IBM Bluemix OpenWhiskBluemix OpenWhisk is also an open source service and currently supports NodeJS and Swift. Contrary to other FaaS providers, IBM emphasises on container integration. When an event or an API call invokes an action, OpenWhisk creates a container to run the action in a runtime appropriate to the programming language used. You can even create Docker functions (called actions in OpenWhisk) allowing you to build in any language. OpenWhisk can also run locally on your own hardware, which no other provider currently offers. IBM is very open about this and even provides guidelines on how this can be achieved.As expected, the documentation has a getting started guide to build and run a Hello World action. While working with the CLI works as advertised, it quickly becomes quite cumbersome, especially when integrating with other Bluemix services. After executing your first OpenWhisk function, you can see some metrics in the (pretty) OpenWhisk dashboard. The OpenWhisk dashboard will show all invoked actions, also from actions you didn‚Äôt implement yourself. For example when using existing packages.What‚Äôs even more impressive is the Openwhisk Editor. This editor only lists the actions you created yourself.As you can see from the screenshot, you immediately get links to the REST Endpoint.ConclusionCurrently it‚Äôs too soon to draw any conclusions. These services are constantly changing. What is obvious, is that all major cloud providers want to make sure that they don‚Äôt miss the FaaS opportunity. Cloud providers create value by integrating FaaS services with their other offerings. This confirms the value of a Serverless Cloud. The current FaaS solutions have a lot of similar characteristics and choosing one, will likely depend on what other services you already use (or want to use) from a certain provider. It‚Äôs important to know the environment your FaaS code lives in and the services available to it. In this phase available documentation also is crucial.Obviously, this high-level introduction doesn‚Äôt list all the differences or similarities, but it offers a nice starting point to experience the FaaS (r)evolution first-hand.Part 3In the first part of this article, I introduced the Serverless architectural style. In the second part, I compared all major serverless providers. In this third and last part, I would like to look at serverless as an enabler of collaborative economy.Collaborative EconomyWhat is collaborative ecomomy?  Benita Matofska: The Sharing Economy is a socio-economic ecosystem built around the sharing of human, physical and intellectual resources.It includes the shared creation, production, distribution, trade and consumption of goods and services by different people and organisations.The last part of Benita‚Äôs quote: shared creation, production .. of services by different people and organisations makes a very nice use-case for the serverless style of building applications.Your dataIn this day and age, all companies have become IT companies, meaning a lot of data is gathered and stored somewhere. Often the usage of the available data changes over time. If data is not used for the benefit of the enterprise or its employees, does it still hold value? Wouldn‚Äôt it be great if we could turn cost into profit?Thanks to its cost model (pay per execution), its focus on scalability (no risk of overprovisioning) and resilience, serverless enables companies to experiment with exposing their data:  Offering an API for others to consume  Enriching existing API‚Äôs with their data  ‚Ä¶Your ideasServerless also makes a lot of sense for companies that don‚Äôt want to expose their data, but have great or new ideas on how to use others data:  Combining data from multiple providers  Filtering and transforming data  New business cases beyond the scope of the original API  ‚Ä¶ExampleI implemented a small and simple application that will consume data from different serverless cloud providers. Every ‚Äúhop‚Äù in the system will parse its input and add some new data.Component diagramDescriptionAny client can post a JSON to the first function, made with Auth0 webtask. The body of the post request is simple:{\"temp\":\"42\"}The WebTask will parse that input, add some input of its own and POST request to an IBM OpenWhisk action. The body of this POST request:{  \"hops\": [    {      \"provider\": \"Auth 0 Webtask\",      \"start\": \"2016-08-24T20:32:03.629Z\",      \"temperature\": \"42\",      \"stop\": \"2016-08-24T20:32:03.629Z\"    }  ]}To continue the chain, IBM OpenWhisk will POST the parsed JSON to a function on the AWS Lambda platform after adding a new ‚Äúhop‚Äù:{  \"hops\": [    {      \"provider\": \"Auth 0 Webtask\",      \"start\": \"2016-08-26T18:38:25.021Z\",      \"temperature\": \"44\",      \"stop\": \"2016-08-26T18:38:25.021Z\"    },    {      \"provider\": \"IBM OpenWhisk\",      \"start\": \"2016-08-26T18:38:35.024Z\",      \"temperature\": \"42\",      \"stop\": \"2016-08-26T18:38:35.024Z\"    }  ]}The Lambda, created with Serverless V1.0 Beta 2 will parse the input again and create items in an AWS DynamoDB:The AWS DynamoDB table will stream events to another AWS Lambda that will log the content of the event to the logs of AWS CloudWatch:The source code of all these components is available on GitHub.Best practiceObviously I wouldn‚Äôt recommend anyone to use a different cloud provider for every function. Choosing the right one will depend on your specific needs, goals and current cloud landscape. In previous parts of this article, you may find some tips on how to make a reasoned choice.Final noteThis article was originally posted in three parts on the JAX London blog and is also available in German."
      },
    
      "kickstarters-2016-10-31-kickstarters-project-html": {
        "title": "Kickstarter project 2016",
        "url": "/kickstarters/2016/10/31/Kickstarters-Project.html",
        "image": "/img/kicks.png",
        "date": "31 Oct 2016",
        "category": "post, blog post, blog",
        "content": "  On August 1‚Äòst it was D-day for all the kickstarters that had recently joined Ordina. A batch of talented new people were ready to embark on a new adventure. This year around fifty people joined Ordina and participated in the Kickstarter Project. The JWorks kickstarter group consisted of seven people, all of which were eager to get started. Six people joined the JWorks unit and one joined the Security unit. The purpose of the two month long kickstarter project is to broaden the knowledge of and prepare the kickstarters for their first project.Kickstarter project 2016First impressionsYou never get a second chance to make a first impression.‚Äì Harlan HoganAnd boy Ordina did a pretty good job!The reception on the first day was really great and pretty informal.First off the kickstarters received a tour of the company.They introduced themselves and got to know eachother in a pretty playful way.FINALLY the moment had arrived that everybody was waiting for !The kickstarters received their highly anticipated company car and laptop.The overal atmosphere is pretty loose, you can ask anyone anything and you can talk to everybody.After a few days the kickstarters also got the chance to go on a teambuilding day in Mechelen.During the course of this day, they had to work together as a team to complete some questions and games.The winners were rewarded with a cup. This enabled them to learn how to communicate in a team and under stress, because some of the tests had to be completed within an certain amount of time.Most of the kickstarters already had the chance to go to one of the Ordina events like JOIN or CC meetings, where they networked with lots of interesting people.August - LearningThe focus was primarily on learning during the first month of the Kickstarters project.The first few days the kickstarters had to improve their softskills by learning how to be more assertive towards the client when necessary.They also learned to introduce themselves properly with the emphasis on their qualities and strengths, to ensure they make a good first impression of themselves when going to the client.The kickstarters were brought up to date with the preferred technologies, editors and best practices used by JWorks.During the first month they received different courses in which they could improve their technical skills about:  backend          Java (7 &amp; 8) + JavaEE      Spring      JPA      Webservices (REST &amp; SOAP)      MongoDB        frontend          HTML &amp; CSS      Javascript &amp; Typescript      Angular      Ionic      These courses were given by the JWorks unit who tried to teach the kickstarters a much as possible with theoretical material and some exercises afterwards.Unfortunately, these technical skills aren‚Äôt enough to survive in the forever changing IT world.This is why some extra help was provided in the form of books and courses about how to write clean code, how to work agile and learning how to work in a team while understanding and using the SCRUM principles.By paying attention to technical development with the necessary certification processes and also focusing on the development of soft skills like communication, advising and collaboration,Ordina commits to the personal development of these kickstarters.September - Dev-caseThe focus during the second month was on the implementation of this year‚Äôs dev-case.Although they still had to follow a few courses along the way, like the basic priciples of security, GIT and learning how to use MongoDB.SensyMcSenseFace was born‚Äì Chosen by popular vote, who would have guessed it‚Ä¶The kickstarters already had learned how to write clean code and how to do this in the best possible way.The purpose of the SensyMcSenseFace project was to give the kickstarters a use case where they could develop an end-to-end IoT solution, in which they could test and use their newly acquired skills.What did the client requestThe kickstarters were given the task to build an application that accepts incoming data, while being able to process this data and output it in a more user friendly way.The data would be sent by three different sensors:  Temperature sensor  Humidity sensor  Motion sensorThese sensors send some data every few seconds to the backend, the backend then processes this data and sends it back to the frontend.Here the frontend developers made sure that all the data has been received and outputted in the correct way.The following picture depicts the two meeting rooms that are equipped with three different sensors, which send their data back to the application‚Äôs backend.Each meeting room equipped with the sensors, which have their values read by an Arduino that then sends these across the Proximus LoRa network to the backend. For the initial stages and testing the LoRa part was omitted and a simple node server instance was used to relay the sensor values to the actual backend.This way, the client (Ordina) could figure out when they are using excessive power.For example people leaving the TV on for too long inside of the meeting rooms.Used technologies - sofware  backend          Maven      Spring      Spring Boot      MongoDB      Mockito        frontend          Angular 2      Angular Material 2        extra          Waffle      Cloudfoundry      GitHub      Process of the projectThe first week went pretty well, they divided themselves up into two groups.One group for the frontend and the other one for the backend.At first they started to create different user stories for their SCRUM board.Using the newly acquired scrum techniques during the first month.The kickstarters had to work in four short sprints of one week. During the first sprint they also decided to change their real life SCRUM board into an online version using Waffle.This software would track the pull requests and merges automatically from Github and change the board accordingly.Continuous Integration was pretty important during the course of the project.This way, whenever they made changes to the code and made a pull request to Github that failed to build, they had to fix their code before they could continue.Once the build on codeship succeeded and the pull request was merged. The main development branch and master branch would have their changes (if any) deployed to their Cloudfoundry instance.The process for the backend was pretty simple.They started out with around five people, so some of them started to pair program while others started to program on their own.But, with paying attention to the SCRUM principles and how to write clean code.Their first job was to start with the basic implementations of the sensor, room and notifications classes and writing the JUnit and Mockito tests.While the backend was pretty straight forward and relatively easy to begin with, the frontend team were confronted with some problems.The team consisted of only two people who had to tackle a lot of problems with the use of Angular 2 which was still in Beta at the time.Also the combination with the other frameworks wasn‚Äôt quite that easy to work with.Every morning the team did a stand up meeting where they would discuss their changes in eachothers code and what they were going to do next.During these four weeks they tried to work in these short sprints but after a week or two it became clear this wouldn‚Äôt be an easy task.A few team members already had left the group because they were assigned to projects, which messed up their sprints completely.During the first two weeks the backend team started with using Spring Data JPA, but soon figured out Spring Data MongoDB was the better alternative because of the large amount of data being pumped into the DB.A lot of time of went into providing REST documentation that covers all the different calls handled by the application.This documentation was created with MockMvc tests, which create code snippets that were easy to use.On the frontend side they didn‚Äôt have test cases yet, but nobody in the entire team had ever written frontend tests before.Which caused a little bit of a delay, also the webpages didn‚Äôt seem to be responsive at all.Luckily there were some online tutorials available on mocking and writing frontend tests.The responsiveness issue was sovled by using their own components and CSS code instead of using material design.When starting their two final weeks there were only three people left who were able to fully commit to the dev-case.During the last week the kickstarters also had to prepare and give a proper introduction to the management of Ordina.Showing off their newly learned presentation and introduction techniques.At the very end the core of the application was finished.  You are able to watch an overview of the rooms  You are able to check if a room is occupied or available  You are able to check which sensors are in a room and check their last history  You are able to check the sensor history between two timestamps  You are able to get notifications on your cellphone when certain values are exceededpossible future changesBecause there wasn‚Äôt enough time to completely finish the project, this project can still evolve in a lot of ways.  Later on it could be possible to add roles or users.  Make adding sensors or rooms more user friendly.  Add predictions to the applications for every room.  Add user management with users and rolesLessons learned during the Kickstarters project  How to be more confident  How to introduce yourself in a professional way  How to be more assertive  You have to keep an eye out for possible changes in your code that encourage clean code  How to work better and agile in a team and how to use SCRUM principles  Pitfalls and difficulties when using and combining new technologies  How to write proper tests (JUnit, Mockito, MvcTests)  How to write proper REST documentationThe new JWorks colleagues"
      },
    
      "conference-2016-10-10-percona-live-amsterdam-2016-html": {
        "title": "Percona Live Amsterdam 2016",
        "url": "/conference/2016/10/10/Percona-Live-Amsterdam-2016.html",
        "image": "/img/2016-10-16-Percona-Live/PLAM-16-01.png",
        "date": "10 Oct 2016",
        "category": "post, blog post, blog",
        "content": "Percona Live Open Source Database Conference 2016, Amsterdam  It was only three weeks before the conference, that I coincidentally discovered (and by Googling) that Percona was organizing one of the biggest Open Source Databases conferences in Amsterdam, Percona Live Europe.Until then I had never heard of Percona.Shame on me! But, Percona is mostly known in the US and according to db-engine.com it‚Äôs takes the 47th  place on the popularity list of Relational Databases, and the 97th spot if you consider all database systems.Yet, Percona is celebrating its 10th anniversary this year and among it‚Äôs more than 3000 customers worldwide it can count well-known brands like Cisco Systems, Time Warner Cable, Alcatent-Lucent, Groupon, BBC, Flickr, ‚Ä¶ among  it‚Äôs customers.It was at the conference that I uncovered that Percona Server is in fact a fork of the MySQL open source database, just like the more popular fork MariaDB.Percona presents itself on their website as the only company that delivers enterprise-class solutions for both MySQL and MongoDB across traditional and cloud-based platforms.So it was obvious that the focus of the three-day conference was on MySQL and MongoDB.With 16 tutorials on Monday, 48 sessions on Tuesday and even 64 sessions on Wednesday the line-up was impressive.Most of the sessions covered MySQL, MongoDB and PostgreSQL topics but other open source databases like ElasticSearch, Redis, RethinkDB, Clickhouse,‚Ä¶ were also discussed.Even with two people of JWorks attending the Conference, picking the right sessions was difficult and the FoMO syndrome was clearly around the corner.While the conference mainly had a technical focus (sometimes in-depth), with subjects as analytics, architecture and design, security, operations, scalability and performance, I was pleased that these sessions were alternated with customer stories.Eventually it is always interesting to see real life uses cases and big names like Facebook, Uber, Dropbox and Booking.com talking about the open source databases they use, and especially how they use them.Facebook was represented by 10 of its employees involved in 7 talks.To pick some : Shared MySQL hosting at Facebook, Massive Schema changes in Facebook, MyRocks Deep Dive: Flash Optimized LSM Database for MySQL, and its Use Case at Facebook, Online Shard Migration at Facebook,‚Ä¶The conference made it also clear that interest in Open Source Databases continues to grow.More and more companies are looking to replace their proprietary databases to open source alternatives.The reasons for that are clear, open source databases maturity have risen to the level of the proprietary databases and some of them have even gone beyond that.And al of that, without the hefty price tag.  Top Internet applications have embraced open source databases a long time ago, and now traditional enterprises are catching up to.So me and my colleague Chris De Bruyne both returned with a backpack full of very useful information that we like to share with you in the coming months.We will definitely try out a lot of stuff, like the different open source monitoring tools for MySQL and MongoDB.The JWorks DBA / NoSQL competence center also advocates the use of open source alternatives when appropriate, and this conference perfectly matches with our ambitions.So we already reserved 25th - 27th of September 2017 in our agendas for the next Percona Live Europe in Dublin."
      },
    
      "conference-2016-09-27-join-2016-html": {
        "title": "JOIN 2016",
        "url": "/conference/2016/09/27/JOIN-2016.html",
        "image": "/img/join16.jpg",
        "date": "27 Sep 2016",
        "category": "post, blog post, blog",
        "content": "  Next week, on the 5th of October 2016, the JWorks Business Unit of Ordina will organize its yearly JOIN event. The purpose of this event is to share knowledge between colleagues and fellow Java, JVM, JavaScript, Cloud and DevOps enthusiasts. Last year, a total of 83 attendees visited Ordina Belgium‚Äôs headquarters in Mechelen to learn and talk about the hottest technology trends and developments.This year we expect to have more than 100 attendees, and what‚Äôs most exciting is that the event is completely free of charge and everyone is invited. Food and drinks are provided (including a barbecue).JOIN 2016 Schedule  Here are some of the highlights of the day:10AM - 12AM - Docker for Java Developers - Arun GuptaArun has been an avid Docker user for many years and is also one of the Docker Captains, among being a Java Champion and JUG leader.He will bring us a very informative talk about the current state of Docker and how Java Developers can get started with Docker in no time.We‚Äôre very much looking forward to this one!  During another talk he will talk about Couchbase, the product company he‚Äôs working for at the moment.More information about Arun in his Docker Community Spotlight.5PM - 6PM - The Google Cloud Platform - Koen MaesKoen Maes provides expert advice and development services for Google Cloud Platform and related products.He is a Google Cloud Platform Authorized Trainer &amp; partner and has been working in the software industry since the early nineties and with web/Internet technology since its inception.He designed key applications for several large corporations as well as running a handful of startups of his own, some more successful than others.Since his first encounters with AppEngine in 2009, he never looked back and has been specializing in Google Cloud Platform ever since.  We are currently working for one of our customers on a large scale greenfield microservices system in the Google Cloud Platform, so this will be especially interesting for us.6PM - 7PM - Reactive Programming - Stephane MaldiniA multi-tasker eating tech 24/7, Stephane is interested in cloud computing, data science and messaging.Leading the Reactor Project, Stephane Maldini is on a mission to help developers create reactive and efficient architectures on the JVM and beyond.He is also one of the main contributors for Reactive support in the upcoming Spring 5 framework.  David Karnok, RxJava project lead, identifies the Reactor project as the new standard for reactive applications in the Java world.Most of the developers in our JWorks unit are using Spring (as opposed to JEE), so this talk is going to be very interesting.7PM - 8PM - Typescript: enjoying large scale browser development - Joost De VriesJoost De Vries is one of our Dutch colleagues at Codestar. He will talk about Typescript and how it enables development of large scale front end applications.You can find him on Twitter and Github.FoodWe have foreseen food and drinks during the conference and we will end the night with something special.We hope and believe it will make everyone very happy! Another reason to JOIN us @Ordina, the 5th of October:  Morning reception:          Coffee or Tea      Mini biscuits and chocolates        Lunch:          Luxury sandwiches &amp; subs with salads, French cheese, grey shrimp, prawns or Parma ham      Viking bread with smoked salmon      Grilled chicken wraps      Vegetarian options also available        Afternoon coffee break:          Coffee or Soda      Candy bars or fruit salad        Dinner:          BBQ      Marinated prawn (scampi)      Greenway balls (vegetarian)      Spicy chipolatas      Marinated chicken sat√©s      Steak chimichurri cut and grilled √† la minute      Coleslaw, tomatoes, cucumbers, carrots, potato salad, mix of salads, red onions and olives      Bread, potatoes and sauces      SubscribingAnyone who still wants to attend the free 2016 JOIN event at Ordina Mechelen, can subscribe through the link below:https://www.ordina.be/en/evenementen/2016/join-2016/"
      },
    
      "monitoring-2016-09-23-monitoring-with-prometheus-html": {
        "title": "Monitoring with Prometheus",
        "url": "/monitoring/2016/09/23/Monitoring-with-Prometheus.html",
        "image": "/img/prometheus.jpg",
        "date": "23 Sep 2016",
        "category": "post, blog post, blog",
        "content": "It is needless to say the world is shifting towards DevOps and microservices.This holy grail we aim for adds a great deal of complexity.Monitoring included.Rather than having to monitor one system,we are suddenly faced with the challenge to oversee our manifold services.There are numerous monitoring systems available,but not all of them are fit for monitoring large, distributed systems.Black box monitoring systems like Nagios allow you to check if an application is alive and healthy.This is done by e.g. pinging the service,checking if there is enough disk space,or monitoring the CPU usage.In a world of distributed architectures where high availability and fast response times are key,it is not sufficient to be only aware if a service is alive.It is crucial to know how a service is working internally as well.How many HTTP requests is it receiving?Are they handled correctly?How fast are requests handled for different endpoints?Are there many errors being logged?How many disk IO operations is the service performing?These are all important questions that need to be monitored to keep a service functional.Prometheus is a white box monitoring and alerting system that is designed for large, scalable environments.With Prometheus,we can answer all these questions,by exposing the internal state of your applications.By monitoring this internal state,we can throw alerts and act upon certain events.For example,if the average request rate per second of a service goes up,or the fifty percent quantile response time of a service suddenly passes a certain threshold,we could act upon this by upscaling the service.Overview  The Rise of Prometheus  Architecture  Data Model  Slice &amp; Dice with the Query Language  Instrumenting Your Services  Exporters  Scraping the Targets  Visualization and Analytics  Alert! Alert!  Monitoring Time!  Final WordsThe Rise of PrometheusAs with most great technologies,there is usually a great story hiding behind them.Nothing is different with Prometheus.Incubated at SoundCloud,the social platform for sharing sounds and music,Prometheus has come a long way.When SoundCloud was just a start-up,they originally developed their application as a single application.Many features later,this resulted in one big, monolithic application called the Mothership.With only a few thousand artists and users sharing music,the application performed sufficiently.However,nowadays,about 12 hours of music is uploaded every minute to SoundCloud.The platform is used by hundreds of millions of users every day.To be able to handle this size of volume,SoundCloud adapted a more scalable approach.Deciding against a complete rewrite of their whole technology stack,they stopped adding new features to the Mothership.Instead, new features were written as microservices,living next to the Mothership.If you want to know more about how SoundCloud moved from one monolithic application to a microservices architecture,you can find a three-partblog postseries on their developer blog (which is an excellent read, by the way).Moving towards a microservices architecture paved the way for many possibilities for SoundCloud,but it also introduced a lot of complexity.Monitoring a single application is easy.Monitoring hundreds of different services with thousands of instances is an entirely different story.SoundCloud‚Äôs original monitoring set-up consisted of Graphite and StatsD.This setup did not suffice for the new, scalable microservices architecture.The amount of generated events could not be handled in a reliable way.SoundCloud started looking for a new monitoring tool,while keeping the following requirements in mind:      A multi-dimensional data model,where data can be sliced and diced along multiple dimensions like host, service, endpoint and method.        Operational simplicity,so that you can setup monitoring anywhere you want,whenever you want,without having to have a Ph.D. in configuration management.        Scalable and decentralized,for independent and reliable monitoring.        A powerful query language that utilizes the data model for meaningful alerting and visualisation.  Since no existing system combined all of these features,Prometheus was born from a pet project at SoundCloud.Although the project has been open source from the beginning,SoundCloud did not make any noise about it until the project was mature enough.In January 2015,after 2 years of development and internal usage,the project was publicly announcedand a website was put online.The amount of attention it received was totally unexpected for the team at SoundCloud.After a post on Hacker News,which made it all the way to the top,things got serious.There was a sharp rise in contributions, questions, GitHub issues, conference invites, and all that stuff.The following image depicts the amount of stars the project received on GitHub since its inception.ArchitecturePrometheus‚Äô architecture is pretty straightforward.Prometheus servers scrape (pull) metrics from instrumented jobs.If a service is unable to be instrumented,the server can scrape metrics from an intermediary push gateway.There is no distributed storage.Prometheus servers store all metrics locally.They can run rules over this data and generate new time series,or trigger alerts. Servers also provide an API to query the data.Grafana utilizes this functionality and can be used to build dashboards.Finally,Prometheus servers know which targets to scrape from due to service discovery,or static configuration.Service discovery is more common and also recommended,as it allows you to dynamically discover targets.Data ModelAt its core,Prometheus stores all data as time series.A time series is a stream of timestamped values that belong to the same metric and the same labels.The labels cause the metrics to be multi-dimensional.For example,if we wish to monitor the total amount of HTTP requests on our API,we could create a metric named api_http_requests_total.Now,to make this metric multi-dimensional,we can add labels.Labels are simple key value pairs.For HTTP requests,we can attach a label named method that takes the HTTP method as value.Other possible labels include the endpoint that is called on our API,and the HTTP status returned by the server for that request.The notation for a metric like that could be the following:api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}Now,if we start sampling values for this metric,we could end up with the following time series:            Metrics      Timestamp      Value                  api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}      @1464623917237      68856              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"500\"}      @1464623917237      5567              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}      @1464624516508      76909              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"500\"}      @1464624516508      6789      One of the great aspects of time seriesis the fact that the amount of generated time series is independent of the amount of events.Even though your server might suddenly get a spike in traffic,the amount of time series generated stays the same.Only the outputted value of the time series is different.This is wonderful for scalability.Prometheus offers four metric types which can be used to generate one or multiple time series.A counter is a metric which is a numerical value that is only incremented,never decremented.Examples include the total amount of requests served,how many exceptions that occur, etc.A gauge is a metric similar to the counter. It is a numerical value that can go either up or down.Think of memory usage, cpu usage, amount of threads, or perhaps a temperature.A histogram is a metric that samples observations.These observations are counted and placed into configurable buckets.Upon being scraped,a histogram provides multiple time series,including one for each bucket,one for the sum of all values,and one for the count of the events that have been observed.A typical use case for a histogram is the measuring of response times.A summary is similar to a histogram,but it also calculates configurable quantiles.Depending on your requirements,you either use a histogram or a summary.Slice &amp; Dice with the Query LanguageA powerful data model needs a powerful query language.Prometheus offers one,and it is also one of Prometheus‚Äô key features.The Prometheus query language,or promql,is an expressive, functional language.One which apparently,by the way,is Turing complete.The language is easy to use.Monitoring things like CPU usage,memory usage, amount of HTTP request served, etc. are pretty straightforward,and the language makes it effortless.Using an instant vector selector,you can select time series from a metric.For example,Continuing with our API example,we can select all the time series of the metric api_http_requests_total:api_http_requests_totalWe can dive a little bit deeper by filtering these time series on their labels using curly braces ({}).Let‚Äôs say we want to monitor requests that failed due to an internal server error.We can achieve this by selecting the time series of the metric api_http_requests_totalwhere the label status is set to 500.api_http_requests_total{status=\"500\"}We can also define a time window if we only want to have time series of a certain period.This is done by using a range vector selector.The following example selects time series of the last hour:api_http_requests_total[1h]The time duration is specified as a number followed by a character depicting the time unit:  s - seconds  m - minutes  h - hours  d - days  w - weeks  y - yearsYou can go further back in time by using an offset.This example selects time series that happened at least an hour ago:api_http_requests_total offset 1hWe can use functions in our queries to create more useful results.The rate() function calculates the per-second average rate of time series in a range vector.Combining all the above tools,we can get the rates of HTTP requests of a specific timeframe.The query below will calculate the per-second rates of all HTTP requeststhat occurred in the last 5 minutes an hour ago:rate(api_http_requests_total{status=500}[5m] offset 1h)A slightly more complex example selects the top 3 endpoints which have the most HTTP requestsnot being served correctly in the last hour:topk(  3, sum(    rate(api_http_requests_total{status=500}[1h])  ) by (endpoint))As you can see,Prometheus can provide a lot of useful information with several simple queries that only have a few basic functions and operators.There is also support for sorting, aggregation, interpolation and other mathematical wizardry that you can find in other query languages.Instrumenting Your ServicesOne of the requirements to be able to query data and get results,obviously,is the fact that there must be data that can be queried.Generating data can be done by instrumenting your services.Prometheus offers client libraries forGo,Java/Scala,Python andRuby.There is also a lengthy list of unofficial third-party clients for other languages,including clients for Bash and Node.js.These clients enable you to expose metrics endpoints through HTTP.This is totally different compared to other,more traditional,monitoring tools.Normally,the application is unaware that it is being monitored.With Prometheus,you must instrument your codeand explicitly define the metrics you want to expose.This allows you to generate highly granular data which you can query.However,this technique is not much different than logging.Logging statements are,most of the time,also explicitly defined in the code,so why not for monitoring as well?For short-lived jobs,like cronjobs,scraping may be too slow to gather the metrics.For these use cases,Prometheus offers an alternative,called the Pushgateway.Before a job disappears,it can push metrics to this gateway,and Prometheus can scrape the metrics from this gateway later on.ExportersNot everything can be instrumented.Third-party tools that do not support Prometheus metrics natively,can be monitored with exporters.Exporters can collect statistics and existing metrics,and convert them to Prometheus metrics.An exporter,just like an instrumented service,exposes these metrics through an endpoint,and can be scraped by Prometheus.A large variety of exporters is already available.If you want to monitor third-party software that does not have an exporter publicly available,you can write your own custom exporterScraping the TargetsPulling metrics from instances is called scraping.Scraping is done at configurable intervals by the Prometheus server.Prometheus allows you to configure jobs that fetch time series from instances.global:  scrape_interval: 15s # Scrape targets every 15 seconds  scrape_timeout: 15s # Timeout after 15 seconds  # Attach the label monitor=dev-monitor to all scraped time series scraped by this server  labels:    monitor: 'dev-monitor'scrape_configs:  - job_name: \"job-name\"    scrape_interval: 10s # Override the default global interval for this job    scrape_timeout: 10s # Override the default global timeout for this job    target_groups:    # First group of scrape targets    - targets: ['localhost:9100', 'localhost:9101']      labels:        group: 'first-group'    # Second group of scrape targets    - targets: ['localhost:9200', 'localhost:9201']      labels:        group: 'second-group'This configuration file is pretty self-explanatory.You can define defaults for all jobs in the global root element.These defaults can then be overridden by each job,if necessary.A job itself has a name and a list of target groups.In most cases,a job has one list of targets (one target group),but Prometheus allows you to split these between different groups,so you can add different labels to each scraped metric of that group.Next to your own custom labels,Prometheus will additionally append the job and instance labels to the sampled metrics automatically.Visualization and AnalyticsPrometheus has its own dashboard,called PromDash,but it has been deprecated in favor of Grafana.Grafana supports Prometheus metrics out-of-the-boxand makes setting up metrics visualization effortless.After adding a Prometheus data source,you can immediately start creating dashboards using PromQL:                  Step 1: Create datasource                        Step 2: Profit      Alert! Alert!Prometheus provides an Alert Manager.This Alert Manager is highly configurable and supports many notification methods natively.You can define routes and receivers,so you have fine-grained control over every alert and how it is treated.It is possible to suppress alerts and define inhibition rules,so you can prevent getting thousands of the same alert if a many-node cluster goes down.Alerts can be generated by defining alerting rules.This is done in Prometheus and not in the Alert Manager.Here are a few simple alerting rule examples:# Alert for any instance that have a median request latency &gt;1s.ALERT APIHighRequestLatencyIF api_http_request_latencies_second{quantile=\"0.5\"} &gt; 1FOR 1mLABELS { severity=\"critical\"}ANNOTATIONS {  summary = \"High request latency on \",  description = \" has a median request latency above 1s (current value: s)\",}ALERT CpuUsageIF cpu_usage_total &gt; 95FOR 1mLABELS { severity=\"critical\"}ANNOTATIONS {  summary = \"YOU MUST CONSTRUCT ADDITIONAL PYLONS\"  description = \"CPU usage is above 95%\"}After an alert is generated and sent to the Alert Manager,it can be routed using routes.There is one root route on which each incoming alert enters,and you can define child routes to route alerts to the correct receiver.These routes can be configured using a YAML configuration file:# The root route on which each incoming alert enters.route:  # The default receiver  receiver: 'team-X'  # The child route trees.  routes:  # This is a regular expressiong based route  - match_re:      service: ^(foo|bar)$    receiver: team-foobar    # Another child route    routes:    - match:        severity: critical      receiver: team-criticalThere are multiple types of receivers to which you can push notifications to.You can push alert notifications to SMTP,HipChat,PagerDuty,PushOver,Slack and OpsGenie.Additionally,you can use a web hook to send HTTP POST requests to a certain endpoint with the alert as JSON,if you wish to push notifications to somewhere else.Check out this guy‚Äôs audio alarm,which alerts him when his internet goes down!The receivers are configured in the same YAML configuration file:receivers:# Email receiver- name: 'team-X'  email_configs:  - to: 'alerts@team-x.com'# Slack receiver that sends alerts to the #general channel.- name: 'team-foobar'  slack_configs:    api_url: 'https://foobar.slack.com/services/hooks/incoming-webhook?token=&lt;token&gt;'    channel: 'general'# Webhook receiver with a custom endpoint- name: 'team-critical'  webhook_configs:    url: 'team.critical.com'Monitoring Time!Do you wish to get your hands dirty quickly with Prometheus?Perfect!I have prepared a project for demonstration purposes,which can be found on the Ordina JWorks GitHub repository.The project can be set up using only one command,leveraging Docker and Make.It covers most of the features discussed in this blog post.First clone the project with Git:$ git clone git@github.com:ordina-jworks/prometheus-demo.gitAfter the project is cloned,run make in the project directory:$ makeThis will compile all applications,build or pull all necessary Docker images,and start the complete project using Docker Compose.The following containers are started:$ docker psCONTAINER ID        IMAGE                              COMMAND                  PORTS                     NAMESc620b49edf4c        prom/alertmanager                  \"/bin/alertmanager -c\"   0.0.0.0:32902-&gt;9093/tcp   prometheusdemo_alertmanager_167b461b6a44b        grafana/grafana                    \"/run.sh\"                0.0.0.0:32903-&gt;3000/tcp   prometheusdemo_grafana_1920792d123bd        google/cadvisor                    \"/usr/bin/cadvisor -l\"   0.0.0.0:32900-&gt;8080/tcp   prometheusdemo_cadvisor_1215c20eb849b        ordina-jworks/prometheus-prommer   \"/bin/sh -c /entrypoi\"   0.0.0.0:32901-&gt;9090/tcp   prometheusdemo_prometheus_1f3cfc2f63f00        tomverelst/prommer                 \"/bin/prommer -target\"                             prometheusdemo_prommer_1574f14998424        ordina-jworks/voting-app           \"/main\"                  0.0.0.0:32899-&gt;8080/tcp   prometheusdemo_voting-app_166f2a00fcbcb        ordina-jworks/alert-console        \"/main\"                  0.0.0.0:32898-&gt;8080/tcp   prometheusdemo_alert-console_14fd707d4e80c        ordina-jworks/voting-generator     \"/main -vote=cat -max\"   8080/tcp                  prometheusdemo_vote-cats_15b876a131ad0        ordina-jworks/voting-generator     \"/main -vote=dog -max\"   8080/tcp                  prometheusdemo_vote-dogs_1As you can see,a lot of containers are started!You can view the public ports of the containers in this list,which you need to access the applications.The project consists of the following components:  Prometheus which scrapes the metrics and throws alerts  Grafana to visualize metrics and show fancy graphs  Alert Manager to collect all alerts and route them with a rule based system  cAdvisor which exposes container and host metrics  Prommer, a custom Prometheus target discovery tool  An alert console which displays the alerts in the console  A voting application which registers and counts votes  A voting generator which generates votesThe voting application exposes a custom metric called voting_amount_total.This metric holds the total amount of votes and is labeled by the type of vote,e.g. voting_amount_total{name=dog}.An alerting rule is configured in Prometheus that checks for the amount of votes.Once it passes a certain threshold,the alert is fired.This alert is sent to the Alert Manager,which in turn routes it to the custom alert console through a webhook.Inactive alertThe alert is firedThe alert console logs the JSON body of the POST request from the Alert Manager.We can check the output of these logs using Docker Compose:$ docker-compose logs -f --tail=\"all\" alert-consoleAttaching to prometheusdemo_alert-console_1alert-console_1  | {\"receiver\":\"alert_console\",\"status\":\"firing\",\"alerts\":[{\"status\":\"firing\",\"labels\":{\"alertname\":\"TooManyCatVotes\",\"instance\":\"172.19.0.5:8080\",\"job\":\"voting-app\",\"name\":\"cat\",\"severity\":\"critical\"},\"annotations\":{\"summary\":\"Too many votes for cats!\"},\"startsAt\":\"2016-09-22T17:09:22.807Z\",\"endsAt\":\"0001-01-01T00:00:00Z\",\"generatorURL\":\"http://215c20eb849b:9090/graph#%5B%7B%22expr%22%3A%22votes_amount_total%7Bname%3D%5C%22cat%5C%22%7D%20%3E%20100%22%2C%22tab%22%3A0%7D%5D\"}],\"groupLabels\":{\"alertname\":\"TooManyCatVotes\"},\"commonLabels\":{\"alertname\":\"TooManyCatVotes\",\"instance\":\"172.19.0.5:8080\",\"job\":\"voting-app\",\"name\":\"cat\",\"severity\":\"critical\"},\"commonAnnotations\":{\"summary\":\"Too many votes for cats!\"},\"externalURL\":\"http://c620b49edf4c:9093\",\"version\":\"3\",\"groupKey\":1012006562800295578}GrafanaThe default credentials for Grafana are admin:admin.After logging in,you must first configure a Prometheus data source.Prometheus is available at http://prometheus:9090 (from within the container).                  Configuring the data source                        Visualizing metrics      cAdvisorcAdvisor also has a simple dashboard which displays most important host and container metrics.Since Prometheus scrapes cAdvisor,these metrics are also available from Grafana.                  Network Throughput                        CPU Usage per Core      Final WordsJust a few months ago,the Prometheus team joined the Cloud Native Computing Foundation.  Today, we are excited to announce that the CNCF‚Äôs Technical Oversight Committee voted unanimously to accept Prometheus as a second hosted project after Kubernetes!You can find more information about these plans in the official press release by the CNCF.  By joining the CNCF, we hope to establish a clear and sustainable project governance model, as well as benefit from the resources, infrastructure, and advice that the independent foundation provides to its members.Cloud Native Computing Foundation (CNCF) is a nonprofit, open standardization organisation which commits itself to advance the development of cloud native technologies,formed under the Linux Foundation.It is a shared effort by the industry to create innovation for container packaged, microservices based, dynamically scheduled applications and operations.Prometheus has proven itself to be worthy to be an industry standard in alerting and monitoring.It offers a wide-range of features,from instrumenting to alerting,and is supported by many other tools.If you are looking for a monitoring tool,definitely give it a shot!"
      },
    
      "testing-2016-09-16-automation-testing-with-postman-html": {
        "title": "API Testing with Postman and Newman",
        "url": "/testing/2016/09/16/Automation-testing-with-postman.html",
        "image": "/img/postman.png",
        "date": "16 Sep 2016",
        "category": "post, blog post, blog",
        "content": "PrerequisitesFor the purpose of this tutorial it is mandatory to have Postman installed which is available has native apps for Windows, OS X and Linux.It is also mandatory to create an account at Algorithmia.Creating and selecting an environmentPostman‚Äôs environment functionality makes it very easy to switch between different environments.A set of variables can be configured per environment and when switching from one environment to another one these will be replaced accordingly.For example let‚Äôs create an environment called ‚Äúproduction‚Äù.Click the ‚ÄúNo environment‚Äù dropdown in the header and select ‚ÄúManage environments‚Äù.Select the ‚ÄúAdd‚Äù button on the popup that is presented to you.Add url https://api.algorithmia.com/v1/algo/ and key simNz9pf7hfAQNifdA224K1GFhs1.Don‚Äôt forget to replace the secret by your own key.Finally select the ‚ÄúProduction‚Äù environment in the environment dropdown and let‚Äôs create our first request.Creating a POST requestEnter {{url}}/WayneS/Calculator/0.1.0 in the request field and change the method from GET to POST.We need to add some additional headers as well so add Content-Type application/json, Authorization Simple {{key}}.As you can see,we are using the environment variables {{url}} and {{key}} so when switching environments,those variables will get replaced.The {{...}} format can only be used in the request URL/URL params/Header values/form-data/url-encoded values/Raw body content/Helper fields.Postman also has a few dynamic variables which you can use. For example, {{$guid}} is generating a random v4 style guid,{{$timestamp}} is the current timestamp,{{$randomInt}} a random integer between 0 and 1000.More of those will be added in future releases.But for now,let us just simply enter \"x=log(2)\" as the raw content of our request.Finally let‚Äôs hit the ‚ÄúSend‚Äù button and if everything goes as expected,we should receive the following response.Next we are going to write our test, but first let us save our request into a collection. By clicking on the create collection button on the collections tab, the following popup will be displayed.  Simply enter ‚ÄúCalculator‚Äù as the name of the collection and hit the create button.Now hit the ‚ÄúSave‚Äù button next to the request field. Enter ‚ÄúLog‚Äù as the name of the request and select ‚ÄúCalculator‚Äù from the dropdown menu.Writing a testA Postman test is essentially JavaScript code which sets values for the special ‚Äòtests‚Äô object. To know which other objects and libraries are available while writing your test cases, make sure you check the following link. Let‚Äôs copy following code snippet in the Tests sandbox.tests[\"Status code is 200\"] = responseCode.code === 200;var jsonData = JSON.parse(responseBody);tests[\"Verify result\"] = jsonData.result.x === \"0.69314718056\";The test will run each time you hit the ‚ÄúSend‚Äù button. Let‚Äôs say we need a custom function to set some variables,this can easily be achieved in the pre-request sandbox as shown below:Here we are using the ‚Äòpostman‚Äô object and are calling the setEnvironmentVariable function on it,this allows us to assign the result of our function to a variable on the environment scope for later use.Collection RunnerLet‚Äôs assume we want to run several tests at once.Postman has a Collection Runner utility that allows us to just do that,even thousands of tests if we want.To access the runner click on ‚ÄúRunner‚Äù in the top header then select ‚ÄúCalculator‚Äù as the collection and ‚ÄúProduction‚Äù as the environment.We want the runner to do that 2 times so enter 2 in the iteration inputfield like shown in the screenshot below.Scroll down and hit the blue ‚ÄúStart Test‚Äù button. Following test report will be presented to you.Writing a request and tests for each different permutation of data could get tiresome and tedious.On the test runner screen we are given the option to choose a data file.This data file can be either a CSV or a JSON file,but will allow us to set up data in bulk to be run through the test runner.Create a new csv file and copy following snippet into it.input,expected_result2,\"0.69314718056\"224,\"5.41164605186\"3000,\"8.00636756765\"388949,\"12.8712035086\"We need to rewrite the body of our request so it will use the variable of our csv as follows.We also need to rewrite our test.Like you can see we use the ‚Äòdata‚Äô object to call our expected_result variable.Back to the runner window.Select the ‚ÄúCalculator‚Äù collection and the ‚ÄúProduction‚Äù environment.Click the ‚ÄúChoose Files‚Äù button and select the csv file you just created,click the ‚ÄúPreview‚Äù button to check for any inconsistenties.As there are 4 entries in our csv we want to use to feed our test enter 4 in the iteration inputfield.Hit the ‚ÄúStart Test‚Äù button and you will now see 12 green tests.Pretty neat, isn‚Äôt it?NewmanIntegrating Postman tests with build systems can easily be accomplished with Newman. Newman is the command line tool companion for Postman. It can be installed through the Node.js package manager, npm. You‚Äôll find more information on how the install Newman here.After Newman is installed we can export our previously created collection and environment.Select the ‚ÄòCalculator‚Äô collection and hit export and save as my_collection.json.To export the ‚ÄòProduction‚Äô environment select ‚ÄòManage Environment‚Äô and on the next popup hit export and save as ‚Äòprod_environment.json‚Äô.Now run you test with Newman using following command where my_collection.json is the exported collection,my_data.csv the csv, prod_environment.json the environment and -n the number of lines from our csv.newman run my_collection.json -n 4 -d my_data.csv -e prod_environment.jsonSummaryIn this tutorial we saw how to create a request and a test.We saw how to create a collection and how to run it with the collection runnner and Newman.I hope you enjoyed this tutorial and if you have any question feel free to add these as a comment or to email me at gregory.rinaldi@ordina.be.Useful links  Importing Swagger files  Postman Slack invite  Importing cURL commands  Creating cURL commands  Making SOAP requests  Running Newman in Docker  Authentication helpers  Publish Documentation for your Collections  Conditional Workflows in Postman (work in progress)  Newman  Integrating Newman with Jenkins "
      },
    
      "microservices-2016-09-12-microservices-dashboard-1-0-1-html": {
        "title": "Microservices Dashboard",
        "url": "/microservices/2016/09/12/Microservices-Dashboard-1.0.1.html",
        "image": "/img/microservices-dashboard.png",
        "date": "12 Sep 2016",
        "category": "post, blog post, blog",
        "content": "So you‚Äôve jumped on the hype train, built a bunch of microservices, and got your first releases under your belt. Now what?Our experiences taught us this is the easy part.With the newly obtained microservices freedom, teams easily plunge into a world of cowboys and unicorns.The big ball of mud is just around the corner.Panic, mayhem and chaos loom over the organisation, waiting for everything to spin out of control.Especially for any enterprise not residing in Silicon Valley, maintaining some sort of governance and compliancy is essential.What does a microservice architecture mean not just for the developers, but also for analysts and managers?What can we as developers do to offer them peace of mind?Managers like to have a grip on thingsThey want to get a sense of compliancy and maturity of the components part of the ecosystem.In theory a microservices architecture gives developers complete freedom to use whatever tools and frameworks they want inside their microservice.In practice, managers often want to slightly restrict that freedom to avoid complete chaos.It‚Äôs not uncommon for managers and architects to impose a set of choices developers can choose from, and goals the teams have to achieve.In order to facilitate recruitment and knowledge transfer, developers could be forced to choose between for instance Java or Javascript.Similarly, architects might enforce every microservice to have a quality gate in place and to have a technical debt less than five days.Aside from the technical aspects inside a microservice, compliancy is even more crucial at the contract level.They should be defined according to an architectural vision and comply to standards across the organisation.Having the ability to track these compliancy regulations and quality assurances is a key enabler for management to push for technical excellence.Too often managers are left clueless on how much effort is required to mature the architecture and which teams they have to chase.Having a dashboard at their disposal indicating where a lack of compliancy and maturity needs their attention can help to ensure budget and priorities are in line with the architectural goals.Aside from compliancy and maturity, managers want some level of change management in place.Oftentimes this is achieved through ticketing systems and cumbersome processes.A microservice architecture goes hand in hand with devops, including full automation and decoupling.In that respect, teams ought to be able to define their own release schedule as there is no need for a waterfall manual testing effort of months on end, and the impact on the ecosystem is contained and managed due to the decoupled nature of microservices.Change management in a devops organisation is much more a read-model instead of a process-heavy model.Managers want to know what is currently out there and what will be out there in the future.This doesn‚Äôt require a manual ticketing system, simply a smart dashboard with a timeline.Analysts need to know what functionality is out thereIn order to reuse functionality and avoid duplication, functional analysts have a strong need for an overview of the current functional landscape.Knowing which resources are exposed by what microservices, and which events and messages are being sent back and forth between microservices and queues, can go a long way in helping analysts understand the state of the architecture.Furthermore, impact analysis can significantly improve when an overview of components and how they are linked together is available to the analysts.Not only does it encourage analysts to identify and inform consumers of a changing service, it can help to avoid introducing breaking changes due to negligence or ignorance.During troubleshooting, testers and analysts should be able to find out what services and backends are involved in a certain functional flow.Just like managers, functional analysts are interested in upcoming features and releases.On top of that, analysts can benefit from being able to define the future state of the ecosystem.Especially when multiple teams are working on similar functionality, it can be notoriously difficult to avoid duplication and breaches of bounded contexts.Using a dashboard to define what is coming up, can help to give them an unambiguous view of the current and future landscape.Developers can benefit from a broader view as wellIn a devops organisation, developers have the responsibility to not only build but also run their services.Knowing which versions are deployed where, can assist developers in verifying whether their deployments are successful, but also to determine the versions of their dependencies.A graphical dashboard can go a long way in providing clarity to developers.But most of all, it can act as a hub for other tools and documentation available.Integrations can be made with for instance API documentation, performance tooling, service registries, in-depth instance-specific dashboards and perhaps even reactive insights.The introduction of the Microservices DashboardVisualising the state of the architecture and dependencies in the system can be a huge benefit to all stakeholders in the IT organisation.The Microservices Dashboard is a brand new open source project, which officially launched its first major release at Spring One Platform.Building on top of Spring Boot and Spring Cloud, it visualises your microservice architecture and integrates with tools every microservice architecture benefits from.This ranges from consumer-driven-contract testing over service discovery to hypermedia traversal and more.Microservices Dashboard is a simple application to visualize links between microservices and the encompassing ecosystem.This AngularJS application consumes endpoints exposed by its server component.It displays four columns: UI, Resources, Microservices and Backends.Each of these columns show nodes and links between them.The information for these links come from Spring Boot Actuator health endpoints, Pact consumer-driven-contract tests and hypermedia indexes, which are aggregated in the microservices-dashboard-server project.The architectureThe dashboard currently consists out of an AngularJS 1.x application which communicates over HTTP to a Spring Boot application.The frontend uses D3.js to visualise the nodes in the four columns.We are currently in the process of completely rebuilding the frontend stack.Next version will be running on Angular 2, Typescript and EcmaScript 6.Most of D3.js will be taken care of by Angular 2 itself.Thanks to this refactor we‚Äôll see the introduction of RxJS, making the frontend much more reactive in nature.This aligns our frontend and backend components goals, since the backend is already running RxJava.Our efforts currently focus on replicating all functionalities currently available in the dashboard, albeit with much more attention to quality and testing.Subsequently new features and enhancements will be built on top of a much more mature and extendible frontend application.Our server component is powered by Spring Boot‚Äôs auto configuration.It‚Äôs a library which, once on the classpath of a regular Spring Boot application, will automatically transform the Spring Boot application into a JSON graph-serving engine.It does so by using aforementioned RxJava.The idea of the server application is that it will fetch information from the microservices ecosystem, with for instance Spring Cloud‚Äôs integration of service registries, and collect details of components and their relation within said ecosystem.Needless to say collecting this information requires a lot of outbound calls, and can pose a serious performance burden in case the landscape gets bigger.Making intelligent use of the system‚Äôs resources is absolutely necessary, and RxJava does just that.In the future we might migrate to Spring‚Äôs Reactor which has a more formal integration of the ReactiveX specification and better integration with Spring itself.Once the frontend‚Äôs revamp is completed, the last step towards an end-to-end reactive flow is the HTTP connection between both components.Currently the server still converts the Observable to blocking, undoing a lot of the performance gains we could achieve.Yet even while eventually blocking, we‚Äôve benchmarked a thirty percent performance gain in switching from CompletableFutures to Observables thanks to the more sustained async handling.Aside from its reactive nature, the server component of the dashboard is also built in a very pluggable way.Information is retrieved from the ecosystem through so-called aggregators.Currently four aggregators are provided: the health-indicators aggregator, the index aggregator, the mappings aggregator and the Pact aggregator.We‚Äôre looking into supporting Spring Cloud‚Äôs recent addition, Spring Cloud Contract, as a source for aggregation.New aggregators can be easily added, and all existing aggregators can be overridden, extended, turned on and off.In the next section we‚Äôll go through these aggregators and their purpose.Collecting information from the ecosystemThe dashboard on its own doesn‚Äôt really make a lot of sense when it‚Äôs not connected to the architecture it‚Äôs supposed to visualise.Aggregators pull in information which eventually gets translated into nodes and links on the dashboard.Health-indicators aggregatorSpring Boot exposes production-ready endpoints through its Actuator module.The health endpoint returns information regarding the current health status of the application.The source of this information is a bunch of health indicators, describing various components and dependencies of the application.For instance, an application can have a dependency on a database, for which a health indicator will usually provide health information to the health endpoint, indicating whether the database is up and the connection pool hasn‚Äôt been depleted.Hence, health indicators describe an up-to-date relationship between the application they run on and its dependencies.Spring Cloud ensures health indicators are automatically enabled when you are using service discovery, circuit breakers, a config server or other Spring Cloud services. However, health indicators don‚Äôt automatically describe a relationship between an application and another application it calls.Luckily Spring Boot has a very easy way of adding custom health indicators.As such, developers can add a health indicator the moment a remote service call is added to the application.  What about real-time?  Using health indicators we are certain the application calls another application programmatically.This provides clarity in terms of the calls in the code and therefore the dependencies that exist across the applications.However, using this method we aren‚Äôt sure whether this remote call is actually being executed at runtime.These concerns are currently provided by other tools such as Twitter‚Äôs Zipkin.In the future we will integrate the dashboard with real-time traffic information from Zipkin or similar tooling.Index aggregatorREST over HTTP is arguably the most popular communicational style in microservices architectures.Therefore, gathering information on where and how REST is used can be quite useful.Index and mappings aggregators perform this specific task, albeit each in a different way.The index aggregator relies on a subconstraint of REST called HATEOAS.It stands for Hypermedia As The Engine Of Application State, and describes the idea of adding links in the payload of responses to other resources.This enables discovery of resources, much like we are using the world wide web from its inception.It prevents the need to bookmark URIs to resources, decoupling implementations and enabling independent evolution of the service.Similarly to a regular website, REST APIs using HATEOAS require a homepage or index from which the resource discovery starts.Simply creating an index resource with links to the other resources the service provides, and exposing this index resource at the root of the application takes care of this.Spring HATEOAS provides useful tools to add links to resources.Once every microservice has an index resource, we can use service discovery to discover all the services, and fetch all the index resources to map out the landscape of resources.This is an excellent source for the dashboard, as it shows the relation between microservices and the RESTful resources they expose.Mappings aggregatorOftentimes, RESTful resources are exposed in a more traditional way (using REST level 2) without the added complexity of HATEOAS.While this is not fully REST compliant, it is most common among APIs using JSON over HTTP.Spring Boot offers a handy endpoint in their Actuator module, called the mappings endpoint.It describes all the resources exposed by the application when Spring MVC REST is used.While also describing Spring‚Äôs own resources, a simple filter allows us to deduct node and link information from these endpoints to visualise in the dashboard.Pact aggregatorIn a microservices architecture, testing is absolutely crucial.As the primary benefit of microservices is faster time-to-market, changes happen all the time.Not only unit and integration testing is required, but also more advanced contract testing to act as a safety net.Consumer-driven-contract testing allows the consumer (the client) to define what he expects from the producer (the service), and ensure the producer validates that definition every time a change is made to the service.This allows the consumer to rest at ease, knowing the producer will remain backwards compatible or version accordingly, and gives the producer knowledge of who uses exactly which parts of its service.The latter gives the producer the chance to request consumers to update their service in case they are causing too much complexity on the producer‚Äôs side due to backwards compatibility.Tests like these document with guaranteed certainty relations between clients and services or services and services.Querying the contracts that define these relations offer a great source of information for the dashboard‚Äôs nodes and links between them.When working with the consumer-driven-contract testing framework Pact, a repository called the Pact-broker holds all the available contracts and exposes them through a REST interface.Our Pact aggregator makes use of this interface to pull the information into the dashboard.Spring Cloud recently added the Spring Cloud Contract module to their portfolio, based on Accurest.We‚Äôre planning to integrate the Microservices Dashboard with Spring Cloud Contract in the near future as well.ConclusionThe Microservices Dashboard gives managers, analysts and developers peace of mind when working in a microservices architecture.Not only does it map relations between components in a visually attractive manner, it can also be a great tool for compliancy, change management, functional analysis and troubleshooting.The dashboard is currently at version 1.0.1, and can be downloaded through maven central.To quickly get up and running, make sure to check out the reference documentation.Since the project is still fairly new, any feedback is greatly appreciated.You can reach us through Gitter or GitHub."
      },
    
      "conference-2016-08-09-s1p-html": {
        "title": "SpringOne Platform",
        "url": "/conference/2016/08/09/s1p.html",
        "image": "/img/s1p.jpg",
        "date": "09 Aug 2016",
        "category": "post, blog post, blog",
        "content": "  SpringOne Platform is the successor of SpringOne 2GX, with a focus mainly on Spring and Cloud Foundry. Next to these technical topics, SpringOne Platform also offered many sessions on cultural transformation and DevOps. Cultural transformation and DevOps are key to deliver meaningful solutions more quickly. This can be achieved by creating empowered teams, able to make independent decisions. To implement these collaborative teams, leadership buy-in is hugely important. Getting away from legacy thinking will allow enterprises to obtain short feedback cycles and thus continuously improve.From Imperative To Reactive Web Apps - Rossen StoyanchevThe marquee feature of Spring 5, will be first class support for Reactive Web applications. Reactive programming is about non-blocking, event-driven applications with back-pressure. Back-pressure helps to ensure a good collaboration between producers and consumers. The Reactive Manifesto is an interesting read on this topic.To support reactive, Spring 5 will use Project Reactor (led by St√©phane Maldini) through the Spring Web Reactive project. This blogpost, by Rossen Stoyanchev is a nice starting point to learn about Flux, Mono and the Spring Reactive world.Managing Secrets at Scale - Mark PaluchIn a world, where we run large amounts of microservices in orchestrated containers, we can never forget about security, encrypting passwords, storing keys, rotating secrets, etc. Today, applications consume both first and third party APIs and need authentication and authorization to do this in a safe way. Traditional patterns cannot keep the security bar high with dynamic deployment scenarios.As a Secure-By-Design company, this talk immediately caught my attention. In a Spring world, we can use Spring Cloud Vault Config, wrapping Vault. An interesting tutorial on this Spring library is available on spring.io.Slides from the talk are available online.It‚Äôs not you, it‚Äôs us: Winning over people for yourself and the team - Neha BatraThis was one of the non-technical talks, but aimed to help with the daily management of technical projects. Neha‚Äôs session was the most interactive one I attended at SpringOne Platform: about 10 minutes in the session, she wanted us to pair to do a personal SWOT analysis with a stranger in the room and see how we can learn from each other. Everyone participated and I believe this might actually be useful in the context of a project. Something to try out!She ended her session with a tool chest to prevent and mitigate issues as they come up:  SWOT analysis  Personal goals  Inception  Set schedule / cadence  Provide feedback  Provide a ‚Äúsafe haven‚Äù  Collect and discuss concerns  Talk in person  Write down useful conversations  Find a way to align first (eg. TDD + pair programming)  Daily retrosThe slidedeck of her talk is available on Slideshare.The five stages of Cloud Native - Casey WestAnother non-technical talk, from the talented and funny Casey West, on how companies are adopting the Cloud Native approach to software development.The talk was very entertaining and resonated with the audience to such an extent, that there was constantly someone laughing. The slides itself don‚Äôt say much without explanation so I‚Äôll try to clarify them a bit here.Analogous to the K√ºbler-Ross model, there are five stages when adopting Cloud Native development:Denial@caseywest immediately grasps the crowd‚Äôs attention with these very familiar quotes:  ‚ÄúContainers are just tiny virtual machines‚ÄùNo they‚Äôre not. Stop treating them as such. Moving a huge application or database from a virtual machine to a container doesn‚Äôt really solve anything.  ‚ÄúWe don‚Äôt need to automate Continuous Delivery because we already automate our infrastructure with Puppet‚ÄùThe problem is that these measures are not enough and they don‚Äôt solve enough of the problem. Managing infrastructure and deploying applications using Puppet scripts already is a great improvement by treating Infrastructure as Code but it still requires too much manual labour.AngerAgain, the goal of these funny quotes is prove a very valid point:  ‚ÄúIt works on my machine‚ÄùThe following quote isn‚Äôt in the slides but might also sound familiar:  ‚ÄúLet me do a hotfix, I can figure it out‚Äùand my favourite:  ‚ÄúDEV is just YOLO-ing sh#t to production‚ÄùThese illustrate the problems, you are likely to get when trying to develop Cloud Native applications without the proper culture in place.This is a clear breakdown in communication and is more a people problem than an IT problem.It just doesn‚Äôt work, especially when also considering the compliance or legal aspect. There is a lack of acknowledgement that we need roles and responsibilities.Bargaining  ‚ÄúWhat if we create microservices that all talk to the same datasource?‚ÄùSingle data model and data ownership are not possible this way.  ‚ÄúWe crammed this monolith in a container and called it a microservice‚ÄùApplications need to adhere to some restrictions to run and scale in the cloud, otherwise you cannot take advantage of the benefits of a platform.Often, there also is the notion of something called bi-modal IT.This is basically dividing your company up into sad mode vs awesome mode.A lot of organizations believe they don‚Äôt need to change and prefer to stay in sad mode, they use bi-modal IT as an excuse. Honestly, nobody really wants to work in sad mode.DepressionOnce people actually start creating Cloud Native applications, the depression kicks in:  ‚ÄúWe created 200 microservices and forgot to setup Jenkins‚ÄùA common mistake is not to go for a fully automated CI/CD pipeline from the start.This should be your first action when you start a new project. It is necessary to automate your path to production.  ‚ÄúWe have an automated build pipeline but release twice a year‚ÄùWhen business is not on board with rapid, iterative delivery, you will never get the desired fast feedback loops.AcceptanceFinally, everyone start realizing the painful, but obvious truth:  All software sucksby which he means that creating software is not easy and will never become easy. But we can try to make it as easy as possible for ourselves.Casey also advises us to respect the CAP theorem, respect Conway‚Äôs Law and automate everything.Also, don‚Äôt expect to get all of these things right from the start. Taking baby steps and improving gradually over time is certainly possible. An example is to put a monolith inside a container and start breaking it up into more manageable pieces.&amp;TLDR;The (very pretty) slides can be found on Slideshare.OrdinaOrdina was represented at SpringOne Platform with 2 speakers and 3 talks:Writing your own Spring Boot Starter - Dieter HubauDieter Hubau gave a very nice introduction on how to write your own Spring Boot Starter. A Spring Boot Starter is the de-facto standard tool for starting with a greenfield Spring project. He started by explaining the magic behind Spring Boot Starters (and @AutoConfiguration) and ended with a cool game of Josh Long Pokemon, deployed on Cloud Foundry.His slides are available here.Writing Comprehensive and Guaranteed Up-to-date REST API Documentation - Andreas EversAndreas Evers talked about Spring REST Docs to generate documentation that is always up to date. To achieve this, a test-driven approach can be used: generate snippets from integration tests. Combine these snippets with manually written templates and finally generate HTML. Personally, I have always been a huge fan of ‚Äúdocumentation-as-code‚Äù and Spring REST Docs is a great tool to achieve this goal.His slides are available here. This blogpost by Kevin Van Houtte provides more insight and examples on Spring REST Docs.Ignite: Microservices Dashboard - Andreas EversOn Monday evening, Andreas pitched the Ordina Microservices Dashboard that was released a couple of hours earlier. The Ordina Microservices Dashboard left a big impression:Definitely worth checking out. Expect an in-depth blogpost here soon!Simplifying the Future - Adrian CockroftThe closing keynote at SpringOne Platform was reserved for one of the most influential people in our industry: Adrian Cockcroft. Always at the edge of technology, Adrian often is credited with making Microservices a mature and useful architectural pattern. His talk focussed on:  Simplifying work  Simplify the organization  Simplify things we buildI really recommend watching his entire presentation on YouTube."
      },
    
      "security-2016-07-25-web-of-trusted-commits-html": {
        "title": "A web of trusted commits",
        "url": "/security/2016/07/25/Web-of-trusted-commits.html",
        "image": "/img/digitally-signing-your-json-documents.png",
        "date": "25 Jul 2016",
        "category": "post, blog post, blog",
        "content": "Who Do You Trust?When you‚Äôre building software with people from around the world, it‚Äôs important to validate that commits and tags are coming from an identified source. By using a distributed revision control system like Git, anyone can have an offline copy of your project‚Äôs code repository. In theory having a central repository is not necessary, but it can be used to provide an ‚Äúofficial‚Äù source from which other developers can clone from and work on. These other floating repositories may contain malicious code because, unfortunately, it is remarkably easy to fake your identity when committing code using Git.The following command allows any individual with bad intentions to commit (malicious) code under your name, meaning that you will get the blame for the backdoor or exploit ‚Äúyou‚Äù committed:  # Individual commit.  $ git commit -a -m \"a message\" --author \"Sherlock H. &lt;sherlock.h@bakerstreet.org&gt;\"  # Global settings.  $ git config --global user.name 'Sherlock H.'  $ git config --global user.email sherlock.h@bakerstreet.org  Ensuring TrustThis blog post tells the story of Sherlock H. Sherlock is a witty developer who holds any security-related topic very close to his heart. After a fair amount of pondering about how he could solve the problem of black-hearted developers impersonating his personality, he decided to add a Digital Signature to his commits. By adding a signature Sherlock can finally sleep soundly at night because the signature indicates that he really issued the commit and that it has not been tampered with since he sent it. Moreover it can be used to trace the origin of malicious code that has made its way into a repository. The signature also assures non-repudiation, meaning that it becomes difficult for the signer to deny having signed something because the Digital Signature is unique to both the commit and the signer, and binds them together. Sherlock can now wholeheartedly vouch for the commit.Consider the following scenario:  Sherlock wants to send an urgent message to his fellow developer John W. telling that their application has been compromised by Jim M, a criminal mastermind who only has unkind intentions. John wants the guarantee that the message he received is sent by Sherlock and has not been tampered with by Jim.In order to securely exchange messages, both Sherlock and John will make use of their Key Pairs. A Key Pair consists of a Public and Private Key which are two unique mathematically related cryptographic keys. As its name suggests, the Public Key is made available to everyone by handing out copies or sharing them through a publicly accessible repository. The Private Key however must be kept confidential to its respective owner.Sherlock and John can do the following with the use of their Key Pair:  Signing          The message is still readable to everyone.      Guarantee of the sender‚Äôs identity (aka Sherlock).      Guarantee that the message has not been tampered with since it has been signed by the sender (aka Sherlock).        Encryption          The message is only readable by the designated recipient (aka John).      No guarantee of the sender‚Äôs identity (aka Sherlock).      Encryption can be done symmetrically by using a Shared Secret Key, a single key is then used for both encryption and decryption. Asymmetrical encryption (aka Public Key encryption) with a Public/Private Keypair uses one key for encryption and another for decryption. Note that the advantages and challenges of using either encryption type is beyond the scope of this blog post.      Enforcing TrustSherlock will combine a digital signature with encryption to convince John that his message is trustworthy.      Sherlock wants to send the following message to John: Data! Data! Data! I can‚Äôt make bricks without clay.. He calculates the Hash of this message by applying a publicly known hashing algorithm to the message. The calculated hash by using the SHA-256 hashing algorithm is d6ba26816599a75310c4c263126d4b44979c7026f90e1db8e9b317d6658f3811. The hash value is unique to the hashed data.        Sherlock encrypts the Hash with his Private Key. This encrypted Hash together with a certificate containing additional information about the sender forms the Digital Signature. The reason why the Hash is encrypted and not the entire message, is that a hash function can convert an arbitrary input into a fixed length value which is usually much shorter than the original message. This saves time since hashing is much faster than signing.        Sherlock sends the original message and its Digital Signature to John.        John receives the message and Digital Signature.        Whatever is encrypted with a Public Key can only be decrypted by using its corresponding Private Key and vice versa. Therefore John uses Sherlock‚Äôs Public Key to decrypt the Signature.        John also re-calculates the Hash of the original message by applying the same hashing algorithm as Sherlock.        John compares the Hash he calculated himself and the decrypted Hash received with Sherlock‚Äôs message.If they‚Äôre identical he knows the message has not been tampered with during transit.Should the message been compromised by Jim, then John would have calculated a different Hash than the encrypted Hash that Sherlock has sent along with his message.  Creating An IdentityIn order to sign his commits, Sherlock decided to use Gnu Privacy Guard (GPG) as his weapon of choice. GPG is a complete and free implementation of the OpenPGP standard. It allows to encrypt and sign data and communication, features a versatile key management system as well as access modules for all kinds of public key directories.      Download and install GPG from the official website        Open a command prompt        # Generate a new Key Pair.      $ gpg --gen-key            Sherlock accepted the default RSA and RSA key. RSA is a widely-used asymmetric encryption algorithm and is named after Ron Rivest, Adi Shamir and Len Adleman who invented it in 1977. Should you be interested in more mathematical details how this algorithm works, I can highly recommend watching ‚ÄúPublic Key Cryptography: RSA Encryption Algorithm‚Äù on YouTube.        Enter the desired key size. I recommend the maximum key size of 4096 bits because they provide far better long-term security. While the default of 2048 bits is secure now, it won‚Äôt be in the future. 1024 bit keys are already considered within the range of being breakable and while technology advances 2048 bit keys will also become breakable. Eventually 4096 bit keys will be broken too, but that will be so far in the future that better encryption algorithms will also likely have been developed by then.        Sherlock accepted the default expiration for his key.        He entered his real name and email address. Sherlock provided the verified email address for his GitHub account. This will make it very easy to link his account with his Public Key.        Provide a secure passphrase. Choose wisely and be sure to remember it because else the key cannot be used and any data encrypted using that key will be lost.        Congratulations, a newly fresh Key Pair should be generated now.        # List all keys.      $ gpg --list-keys        pub   4096R/90C3C3DE 2016-07-24        uid     Sherlock H &lt;sherlock.h@bakerstreet.org&gt;        sub   4096R/586B3A7B 2016-07-24        Like many other developers, Sherlock is very active on GitHub and would like to link his Public Key with his account. He therefore will need to create a textual version of his Public Key. After having executed the command below, the content of the generated ‚Äòpubkey.txt‚Äô needs to be added to his account as described in the GitHub Help pages. More details about distributing and registering your Public Key to a key server can be found in the chapter ‚ÄòDistributing keys‚Äô of the GPG Users Guide. For other usages like encryption and decryption, please refer to GPG‚Äôs Mini HowTo.    # Export the Public Key to a text file.    $ gpg --armor --output pubkey.txt --export 'Sherlock H'    Signing Your WorkOnce Sherlock generated his Key Pair, he can configure Git to use it for signing commits and tags. Following tools can be used to store a GPG key passphrase in a keychain so he doesn‚Äôt have to provide it every time he signs a commit: GPG Suite (Mac) or Gpg4win (Windows).  # Set the signing key by taking your Public Key id as parameter.  $ git config --global user.signingkey 90C3C3DE  # Automatically signs every commit.  $ git config --global commit.gpgsign true  # Manually sign a commit.  $ git commit -S -m \"some commit message\"  # Verify whether your commit has been signed.  $ git log --show-signature    commit 81314da640320c65896a4348842d303a754f37d2    gpg: Signature made Sun Jul 24 15:02:25 2016 CEST using RSA key ID 90C3C3DE    gpg: Good signature from \"Sherlock H &lt;sherlock.h@bakerstreet.org&gt;\"    Author: Sherlock H &lt;sherlock.h@bakerstreet.org&gt;    Date:   Sun Jul 24 15:01:52 2016 +0200  # Verify all signatures during merge. If the signatures can not be verified then merge will be aborted.  $ git merge --verify-signatures other_branch  Earlier this year GitHub announced that they now will show when commits and tags are signed and verified using any of the contributor‚Äôs GPG keys upload to GitHub. Keep your eyes open for commits and tags labeled with those green verified badges.Secure-By-DesignOrdina‚Äôs Secure-By-Design programme encourages to consider and take account of possible security risks as early as possible in a business process.So follow Sherlock‚Äôs example by embedding and safeguarding security in your daily work as a developer and Sign Your Work!Resources  GitHub‚Äôs Help on GPG  The GNU Privacy Handbook  GPG‚Äôs Mini HowTo  ‚ÄúA Git Horror Story‚Äù by Mike Gerwitz  ‚ÄúPublic Key Cryptography: RSA Encryption Algorithm‚Äù"
      },
    
      "conference-2016-07-10-springio16-ddd-rest-html": {
        "title": "Spring I/O 16: Bridging the worlds of DDD &amp; REST",
        "url": "/conference/2016/07/10/SpringIO16-DDD-Rest.html",
        "image": "/img/springio.jpg",
        "date": "10 Jul 2016",
        "category": "post, blog post, blog",
        "content": "SpringIO 2016 in Barcelona was loaded with tons of interesting talks and workshops about Spring Cloud, Spring Boot, Spring Data, Microservices, REST &amp; HATEOAS, Reactive programming, and many many more.In this blogpost I will highlight Oliver Gierke‚Äôs 2 hour presentation about bridging the world of Domain Driven Design (DDD) and the world of Representational State Transfer (REST).Oliver Gierke (@olivergierke) is the lead of the Spring Data project at Pivotal and member of the JPA 2.1 expert group. He has been into developing enterprise applications and open source projects for over 10 years. His working focus is centered around software architecture, DDD, REST, and persistence technologies.Domain Driven DesignDDD is an approach to developing software that meets core business objectives by providing on the one hand tactical modeling tools which include well founded patterns and concepts such as entities, repositories and factories. On the other hand DDD also facilitates strategic principles and methodologies for analyzing and modeling domains such as Bounded Contexts and Context Maps.For an in depth understanding of DDD I highly recommend reading ‚ÄúDomain Driven Design - Tackling Complexity in the Heart of Software‚Äù by Eric Evans (@ericevans0). There‚Äôs also a short, quick-readable summary and introduction to the fundamentals of DDD made available by InfoQ.Oliver‚Äôs talk at SpringIO 2016 highlighted a few basic DDD concepts like Entities, Value Objects, Repositories, Aggregates and Bounded Contexts.Value Objects  Avoid Stringly typed codeValue Objects are vital building blocks of DDD. They are small immutable objects that encapsulate value, validation and behaviour. You can use them to group related values together and provide functionality related to what they represent, making implicit concepts explicit.Some common use cases for VOs are: EmailAddress, Money, ZIPCode, Status, ‚Ä¶ avoid writing these as just plain Strings!Writing VOs can be a cumbersome task but there are some source code generator frameworks out there like Project Lombok and Google‚Äôs AutoValue which can handle all the boilerplate code.Entities &amp; RepositoriesIn contrast to Value Objects which are identified by the attributes they carry, Entities are distinguished by their identity. Entity objects have a life cycle because their identity defines their responsibilities and associations. It is this unique identity and their mutability that sets Entities apart from Value Objects. This means that two Value Objects with the same properties should be considered the same whereas two Entities differ even if their properties match.  Aggregates form nice representation boundaries and become the key things to refer to.An Aggregate is a cluster of closely related entities that can be treated as a single unit. The common parent of that cluster is called an Aggregate Root. An example can be an Order and its Line Items, these will be separate objects but it is useful to treat the Order (the Aggregate Root) together with its Line Items as a single Aggregate.When trying to discover Aggregates, we should understand the model‚Äôs invariants. An invariant is a business rule that must always be consistent and usually refers to transactional consistency. When a transaction commits then everything inside the Aggregate should be consistent and any subsequent access by any client should return the updated value. In most cases it is a best practice to modify only one Aggregate in a single transaction. For updating multiple aggregates eventual consistency can be used. There will be an inconsistency window during which an access may return either the old or the new value but eventually all accesses will return the last updated value. The duration of the inconsistency window can be calculated based on factors like network delays, number of copies of the object, and the system load.A Repository is an abstraction over a persistence store for Aggregates. It acts like a collection by exposing methods to add and remove objects which encapsulate the actual interaction with the underlying data store. It also has elaborate query capabilities which return fully instantiated Aggregates whose attributes values meet the criteria.Bounded ContextDDD aims to create software models based on the underlying domain. A Bounded Context is the boundary that surrounds a part of a particular domain. This boundary isolates the model and language from other models and therefore helps reducing ambiguity and clarifying the meaning. When the boundaries are chosen well, greater decoupling between systems can be achieved which allows to easily change or replace the internals of a BC. Avoid having transactions across multiple BCs.The language that is structured around the domain model is called the Ubiquitous Language. It is important that this language is used by all team members (developers, analysts, business stakeholders, ‚Ä¶) to connect all the activities of the team with the software. The vocabulary on its own does not have any relevance, it only has meaning inside a certain context. For example, an Item has a different meaning in the Orders BC than in the Products BC.Domain EventsA Domain Event is an extremely powerful tool in DDD. It is a type of message that describes something that has happened in the past and that is of interest to the business. (e.g. OrderShipped, CustomerBecamePreferred, ‚Ä¶). It is important to model Event names and its properties according to the Ubiquitous Language of the BC where they originated. When Events need to be delivered to interested parties in either a local BC or broadcasted across BCs eventually consistency is generally used.Maturity LevelThe maturity level of the use of Domain Events can be categorized into 4 levels:  Level 0: no events at all          procedural code with just getters and setters      data just goes in and out        Level 1: explicit operations  Level 2: some operations as events          domain events are used as state transition      important domain events are exposed to interested parties via feeds        Level 3: event sourcing - all changes to application state are stored as a sequence of events          only event logs and snapshots are kept (Event Store)      separation of read and write operations (CQRS)      REST  REST ‚â† CRUD via HTTP. Representation design matters.ResourcesJust like an Aggregate, a well designed Resource should be identifiable, referable and should have a clear scope of consistency.Exposing the core domain model directly via RESTful HTTP can lead to brittle REST interfaces because each change in the domain model will be reflected in the interface. Decoupling the core domain from the REST interface has the advantage that we can make changes to the domain and then decide in each individual case whether a change is needed in the REST interface and how to map it.Also avoid using HTTP PATCH or PUT for (complex) state transitions of your business domain because you are missing out on a lot of information regarding the real business domain event that triggered this update. For example, changing a customer‚Äôs mailing address is a POST to a new ‚ÄúChangeOfAddress‚Äù resource, not a PATCH or PUT of a ‚ÄúCustomer‚Äù resource with a different mailing address field value.This goes hand in hand with DDD‚Äôs concept of Event Sourcing because those state transitions are domain relevant events, not just some changes to the state of some object.HATEOASA RESTful HTTP client can navigate from resource to resource in two different ways. Firstly by being redirected as a result of sending data for processing to the server, and secondly by following links contained in the response of the server. The latter technique is called Hypermedia as the Engine of Application State or HATEOAS.The goal of Hypermedia is to serve not only data but also navigation information at the same time. This has a great impact on the client architecture because now we‚Äôre trading domain knowledge with protocol complexity. The client becomes dumber because it no longer needs to know business rules in a sense that its decisions are reduced to checking whether a link is present or not, e.g. whenever there‚Äôs a cancel link in the HTTP response, then display the Cancel button. This will make the client‚Äôs behavior more dynamic.On the other hand, the client becomes smarter because it needs to handle a smarter and more comprehensive protocol.Maturity levelIn analogy to the maturity level of Aggregates described earlier, Leonard Richardson‚Äôs model can be used to determine the maturity or our REST services.  Level 0: Swamp of POX          the HTTP protocol is used to make RPC calls without indication of the application state        Level 1: Resources          exposure of multiple URIs and each one is an entry point to a specific resource, e.g. http://example.org/orders, http://example.org/order/1, http://example.org/order/2      use of only one single method like POST.        Level 2: HTTP verbs          use of HTTP protocol properties (POST, GET, DELETE, ‚Ä¶)      use of HTTP response codes, e.g. HTTP 200 (OK)        Level 3: Hypermedia controls          refer to description earlier in this blog post.      Translating domain concepts into web appropriate ones            DDD      REST                  Aggregate Root / Repository      Collection / Item Resource              Relations      Links              IDs      URIs              @Version      ETags              Last Modified Property      Last Modified Header      Sample implementationOliver also prepared a small sample implementation using Spring Boot, Spring Data and Lombok. The project is called Spring RESTBucks and is definitely worth checking out!Resources  ‚ÄúDDD &amp; REST‚Äù (slide deck used at SpringIO 2016) by Oliver Gierke  ‚ÄúSpring RESTBucks‚Äù (sample project used at SpringIO 2016) by Oliver Gierke  ‚ÄúBenefits of hypermedia‚Äù by Oliver Gierke  ‚ÄúDomain Driven Design - Tackling Complexity in the Heart of Software‚Äù by Eric Evans  ‚ÄúImplementing Domain Driven Design‚Äù by Vaughn Vernon  ‚ÄúDomain Driven Design Quickly‚Äù by InfoQ"
      },
    
      "conference-2016-06-30-springio16-spring-rest-docs-html": {
        "title": "Spring I/O 16: Test-driven documentation with Spring REST Docs",
        "url": "/conference/2016/06/30/SpringIO16-Spring-Rest-Docs.html",
        "image": "/img/springio.jpg",
        "date": "30 Jun 2016",
        "category": "post, blog post, blog",
        "content": "Spring IO 2016The main focus this year was definitely about cloud, reactive and microservices.But it is important not to forget other topics, like documentation! Keep calm, you don‚Äôt have to do it manually! Spring made it easy for us with Spring REST Docs! This year at Spring IO, Andy Wilkinson himself talked about why, how and when Spring REST Docs are being used. Last but not least, he talked about the new features that came out in version 1.1.Since I implemented Spring REST Docs in a project, I‚Äôll use examples from my experiences.Andy WilkinsonAndy is a Spring Boot, REST docs committer and Spring IO platform lead at Pivotal. You can find him on Twitter using the handle @ankinson.Writing documentation is critical in the world of development. It is used to make an accurate and straight declaration and intent of what the service has to offer. Frontend developers will be able to know which endpoints they have to call and receive the right data. Now, we all know it's tedious for developers to write documentation...It's your lucky day! Spring REST Docs will make your life easier.While you are writing tests, Spring will generate a fully HTML api guide for you and your team. This blog post will take you through the best practices, how to and new features in 1.1.Why Test driven approach  It‚Äôs an accurate definition of your application (no side effects)  It describes the specific HTTP request and response  It‚Äôs straight forward without repetition  It‚Äôs easier to write (no annotations like Swagger)Markup languagesAsciidoctorAsciidoctor is a markup language that processes plain text and produces HTML, completely styled to suit your needs.If you are interested in writing in Asciidoctor be sure to check out the manual.Markdown (New in 1.1)With the newest version of REST Docs, the developer now has more options in terms of markup languages.The Markdown support is not as feature-rich as Asciidoctor, but Markdown can work very well when combined with existing documentation toolchains such as Slate.Here is a good sample working with slate.Andy‚Äôs pickAsciidoctor!Since Asciidoctor boasts more features than Markdown, it gives Asciidoctor the edge.Test ToolsWhen we want to use Spring REST Docs, we‚Äôll have to use one of the test tools. Here are the different tools of choice. To use these tools we‚Äôll have to initialise which document, Mockmvc and ObjectWriter we‚Äôll be using.MockMvcA lightweight server-less documentation generation by the Spring Framework that has been the default use in Spring REST Docs.private MockMvc mockMvc;@Autowiredprivate WebApplicationContext context; @Before    public void setup() throws Exception {        this.document = document(\"{method-name}\");        mockMvc = MockMvcBuilders.webAppContextSetup(wac)                .apply(documentationConfiguration(this.restDocumentation).uris().withScheme(\"https\")).alwaysDo(this.document)                .addFilter(new JwtFilter(),\"/*\")                .build();        objectWriter = objectMapper.writer();        authToken = TestUtil.getAuthToken();        TestUtil.setAuthorities();    }RestAssured (New 1.1)As an alternative, you can use RestAssured to test and document your RESTful services. Available in V1.1, RestAssured will be more expandable than MockMvc.private RequestSpecification spec;@Beforepublic void setUp() {    this.spec = new RequestSpecBuilder().addFilter(            documentationConfiguration(this.restDocumentation))             .build();}Andy‚Äôs pickThis time he didn‚Äôt favor one but he mentioned that RestAssured gives you more functionality and extends your possibilities with HTTP.Snip, snip, snippets everywhere!Default SnippetSnippets are generated by the documented test method.Once you run the test method, you can add these snippets in your Markdown/Asciidoctor file. Be aware, these type of snippet will fail if you don‚Äôt have the correct response/request syntax.this.document.snippets(                links(                        halLinks(), linkWithRel(\"self\").description(\"The employee's resource\"),                        linkWithRel(\"employee\").optional().description(\"The employee's projection\")),                        responseFields(                                fieldWithPath(\"username\").description(\"The employee unique database identifier\"),                                fieldWithPath(\"firstName\").description(\"The employee's first name\"),                                fieldWithPath(\"lastName\").description(\"The employee's last name\"),                                fieldWithPath(\"linkedin\").description(\"The employee's linkedin\"),                                fieldWithPath(\"unit\").description(\"The employee's unit\").type(Unit.class),                                fieldWithPath(\"_links\").description(\"links to other resources\")                        )); mockMvc.perform(                get(\"/employees/1\").accept(MediaType.APPLICATION_JSON));Relaxed Snippet (New in 1.1)In contrast to default snippets, relaxed snippets don‚Äôt complain when something was neglected in the document.This is an advantage when you only need to focus on a certain scenario or specific part of the response/request.Reusable Snippet (New in 1.1)With the newly introduced reusable snippet, you can define a snippet at the beginning of your test class and reuse it every time you need it. When added to your test method, you can extend it with extra variables.// First we define a snippet for reuseprotected final LinksSnippet pagingLinks = links(        linkWithRel(\"first\").optional().description(\"The first page of results\"),        linkWithRel(\"last\").optional().description(\"The last page of results\"),        linkWithRel(\"next\").optional().description(\"The next page of results\"),        linkWithRel(\"prev\").optional().description(\"The previous page of results\"));// Then you perform the mockMvc and add the snippet to the document.// As you can see, it is expendable.this.mockMvc.perform(get(\"/\").accept(MediaType.APPLICATION_JSON))    .andExpect(status().isOk())    .andDo(document(\"example\", this.pagingLinks.and(             linkWithRel(\"alpha\").description(\"Link to the alpha resource\"),            linkWithRel(\"bravo\").description(\"Link to the bravo resource\"))));Type of Snippets:A snippet can be one of the following:Hypermedia linksWhen documenting your hypermedia application, you‚Äôll have to define your links and where they go to. If you have dynamic links that can disappear at one time, you can use relaxed snippets or optional so it won‚Äôt complain. this.document.snippets(                links(                        halLinks(), linkWithRel(\"self\").description(\"The employee's resource\"),                        linkWithRel(\"employee\").optional().description(\"The employee's projection\")),                responseFields(                        fieldWithPath(\"username\").description(\"The employee unique database identifier\").type(String.class),                        fieldWithPath(\"_links\").description(\"links to other resources\")                ));Request fieldsThis defines the fields you request from the client.Normally Spring REST Docs will complain when you neglect a field but with v1.1 we now have support for Relaxed Snippets.Because I use constraints, I made my own method `withPath, this will add an extra column constraint to the documentation.  private static class ConstrainedFields {         private final ConstraintDescriptions constraintDescriptions;         ConstrainedFields(Class&lt;?&gt; input) {             this.constraintDescriptions = new ConstraintDescriptions(input);         }         private FieldDescriptor withPath(String path) {             return fieldWithPath(path).attributes(key(\"constraints\").value(StringUtils                     .collectionToDelimitedString(this.constraintDescriptions                             .descriptionsForProperty(path), \". \")));         }     } @Test public void postEmployee() throws Exception{         Employee employee = employeeRepository.findByUsernameIgnoreCase(\"Nivek\");         employee.setId(null);         employee.setUsername(\"Keloggs\");         String string = objectWriter.writeValueAsString(employee);          ConstrainedFields fields = new ConstrainedFields(Employee.class);          this.document.snippets(                 requestFields(                         fields.withPath(\"username\").description(\"The employee unique database identifier\"),                         fields.withPath(\"firstName\").description(\"The employee's first name\"),                         fields.withPath(\"lastName\").description(\"The employee's last name\"),                         ));          mockMvc.perform(post(\"/employees\").content(string).contentType(MediaTypes.HAL_JSON).header(\"Authorization\", authToken)).andExpect(status().isCreated()).andReturn().getResponse().getHeader(\"Location\");     }                              Response fieldsThis defines the result after consultation of a resource.  this.document.snippets(              responseFields(                       fieldWithPath(\"username\").description(\"The employee unique database identifier\"),                       fieldWithPath(\"firstName\").description(\"The employee's first name\"),                       fieldWithPath(\"lastName\").description(\"The employee's last name\"),                           ));Request/response headersDefines your request/response headers in your API. This is useful when there are extra headers to set. When the request has to involve an authorization header for security reasons, you can add this header to your document.mockMvc.perform(                get(\"/employees/1\").accept(MediaType.APPLICATION_JSON)                .header(\"Authorization\", authToken)                .andDo(document(\"headers\",                \t\t\t\trequestHeaders(                 \t\t\t\t\t\theaderWithName(\"Authorization\").description(                \t\t\t\t\t\t\t\t\"Basic auth credentials\")),                 \t\t\t\tresponseHeaders(                 \t\t\t\t\t\theaderWithName(\"X-RateLimit-Limit\").description(                \t\t\t\t\t\t\t\t\"The total number of requests permitted per period\"),                \t\t\t\t\t\theaderWithName(\"X-RateLimit-Remaining\").description(                \t\t\t\t\t\t\t\t\"Remaining requests permitted in current period\"),                \t\t\t\t\t\theaderWithName(\"X-RateLimit-Reset\").description(                \t\t\t\t\t\t\t\t\"Time at which the rate limit period will reset\")))));Request parametersThe parameters passed by in the uri as a query string are documented with the requestParameters.this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk())\t.andDo(document(\"users\", requestParameters( \t\t\tparameterWithName(\"page\").description(\"The page to retrieve\"), \t\t\tparameterWithName(\"per_page\").description(\"Entries per page\") \t)));\t\tRequest parts (new in 1.1)The parts of a multipart request can be documenting using requestPartsExampleRestAssured.given(this.spec)\t.filter(document(\"users\", requestParts( \t\t\tpartWithName(\"file\").description(\"The file to upload\")))) \t.multiPart(\"file\", \"example\") \t.when().post(\"/upload\") \t.then().statusCode(is(200));What makes good documentation?Andy‚Äôs pickHe told us that the GitHub API is one of the most complete and correct documentation there is. So if you want some guidelines, inspire yourself with this API.Structure and accuracyWhen documenting your application, your accuracy has to be 100% correct and understandable. The structure of your API is the representation of your application, so it better be good.Cross-cutting concernsAndy put forward to document cross-cutting concerns on a general documentation level, avoiding repeating yourself in every single documented API call.Concerns who made it to the top are:  Rate limiting  Authentication and authorisationAnd HTTP verbs/codes (PATCH VS PUT)To be RESTfull, you‚Äôll have to follow the guidelines in having a correct API design. This picture shows you how and when to use the correct verbs and HTTP codes3 main questions if you are working with resources  What do they represent?  What kind of input do they accept?  What output do they produce?Last but not least: do not document uri‚Äôs!QuestionsWill constraints be officially added in future releases?The constraints snippets won‚Äôt be added in the future.This is because Andy wants to give the developers the choice of what they want to implement.ConclusionSince Spring REST Docs is so effective in bringing documentation to the fun part of development I highly recommend to use this in your future Spring applications. Not only you will be smiling when the API guide is being generated but the Frontend developers will get a more understandable view of the backend.Sources  @ankinson  Spring REST Docs  GitHub API  Verbs &amp; HTTP codes  Asciidoctor manual  Slate example"
      },
    
      "conference-2016-06-20-whats-new-in-docker-112-html": {
        "title": "DockerCon 2016 - What is new in Docker 1.12",
        "url": "/conference/2016/06/20/whats-new-in-docker-112.html",
        "image": "/img/dockercon/dockercon.png",
        "date": "20 Jun 2016",
        "category": "post, blog post, blog",
        "content": "Orchestration Made EasyLast week,I tried out the new orchestration tools that were made available on GitHub.My first impressions were very positive.The setup is easy and it works like a charm.Today,at DockerCon 2016,these new orchestration tools were officially announced during the opening session.There is also an official blog post.Before we start talking about orchestration,let‚Äôs take a step back and look at how easy it has become to set up a Swarm cluster.Creating a Swarm manager can be done with one simple command:$ docker swarm initYou can run this command on any Docker 1.12 host.After we created the Swarm manager,we can add additional nodes to the swarm by running the following command on other Docker hosts:$ docker swarm join &lt;IP of manager&gt;:2377That‚Äôs it.No messing around with key-value stores or certificates.Docker will automatically configure everything you need out-of-the-box.Under the hood,Docker uses a Raft consensus.There are two types of nodes: manager and worker.The first initial node is a manager.When adding more nodes to the Swarm,these nodes will be worker nodes by default.Manager nodes are responsible for managing the cluster‚Äôs desired state.They do health checks and schedule tasks to keep this desired state.Worker nodes are responsible for executing tasks that are scheduled by the managers.A worker node cannot change the desired state.It can only take work and report back on the status.The role of a node is dynamic.We can increment or reduce the amount of managers by promoting or demoting nodes.$ docker node promote &lt;node-id&gt;$ docker node demote &lt;node-id&gt;ServicesDocker 1.12 introduces a new service command.A service is a set of tasks that can be easily replicated.A task represents a workload and is executed by a container.A task does not necessarily have to be a container,but currently that is the only option.In the future,tasks can also be different types of workloads,for example Unikernels.The service command is very similar to the run commandand utilizes a lot of similar flags which we are used to work with.$ docker service create --replicate 3 --name frontend --network mynet --publish 80:80/tcp frontend_image:latestThe above command will create a service named frontend,add it to the mynet network,publish it to port 80,and use the frontend_image for this service.This does not only create the service,but it defines the desired state.The cluster constantly reconciles its state.Upon a node failure,the cluster will automatically self healand converge back to the desired state by scheduling new tasks on other nodes.You can also define a Swarm mode.For example,if you wish to create a service that runs on every node,you can easily do this using the global mode.This will schedule all the tasks of a service on each node.This is great for general services like monitoring.$ docker service create --mode=global --name prometheus prom/prometheusJust like we can put constraints on containers,we can put constraints on services:$ docker daemon --label com.example.storage=\"ssd\"$ docker service ... --constraint com.example.storage=\"ssd\" ...If we want more instances of our service,we can scale our services up and down:$ docker service scale frontend=10 backend=20This will change the desired state of the service(s),and the managers will schedule new tasks (or remove existing tasks) to attain this desired state.We can also apply rolling updates to our services.For example,if we wish to upgrade our service to a newer version without any downtime,we can use the service update command:$ docker service update myservice --image myimage:2.0 --update-parallellism 2 --update-delay 10sThis will update our service by replacing 2 tasks at the time,every 10 seconds.We can also use this command to change environment variables,ports,etc.As you can see,the new service subcommand is very powerful and easy to use.BundlesA Distributed Application Bundle (DAB) file declares a stack of services,including the versioning and how the networking is setup.It is a deployment artifact that can be used in your continuous integration tools,all the way from your laptop to production.Currently,one way to generate a .dab file is by creating the bundle using Docker Compose:$ docker-compose bundleThis command will generate a .dab or .dsb file,which is just a JSON text file.Here‚Äôs a partial example:{  \"services\": {    \"db\": {      \"Env\": [        \"constraint:type==backend\",        \"constraint:storage==ssd\"      ],      \"Image\": \"postgres@sha256:f76245b04ddbcebab5bb6c28e76947f49222c99fec4aadb0bb1c24821a9e83ef\",      \"Networks\": [        \"back-tier\"      ]    }  }}This feature is still experimental in Docker 1.12and the specification is still being updated.Docker invites everyone to provide feedback and hopes it will become the de facto standard for deploying applications.Routing Mesh NetworksA problem with load balancers is the fact they are not container-aware,but host-aware.Load balancing containers has been hard up until now,because you have to update the configuration of the load balancers as containers are started or stopped.This is done by overriding the configuration file of the load balancer and restarting it,or by updating the configuration in a distributed key-value store like etcd.Docker now has built in load balancing in the Engine using a container-aware routing mesh.This mesh network can transparantly reroute traffic from any host to a container.For example,publishing a service on port 80 will reserve a Swarm wide ingress port,so that each node will listen to port 80.Each node will then reroute traffic to the container using DNS based service discovery.This is compatible with existing infrastructure.External load balancer no longer need to know where the containers are running.They can just point towards any node and the routing mesh will automatically redirect traffic.Even though this introduces an extra hop,it is still very efficient since it uses IPVS.Security Out Of The BoxDocker now comes with out-of-the-box, zero-configuration security.Docker sets up automatic certificate rotation,TLS mutual authenticationand TLS encryption between nodes.There is no way to turn off security.One of the core principles of Docker is simplicity.Therefor,security must be so simple to use,that you don‚Äôt want to turn it off!Container Health Check in DockerfileA new HEALTHCHECK keyword is available for Dockerfiles.This keyword can be used to define the health check of a container.HEALTHCHECK --interval=5m --timeout=3s --retries 3 CMD curl -f http://localhost || exit 1In the above example,health checking is done every 5 minutes.A container becomes unhealthy if the curl command fails 3 times in a row with a 3 second timeout.New Plugin Subcommands (experimental)A new plugin subcommand has been added which allows you to easily manager Docker plugins.$ docker plugin install &lt;plugin-name&gt;$ docker plugin enable &lt;plugin-name&gt;$ docker plugin disable &lt;plugin-name&gt;Plugins also have a manifest file which describes the resources it needs.You can compare it to how a new app on your smart phone asks for access to different resources,like your photos or contacts.Try It Out!As of today,the Docker for Mac/Windows beta,which is already at Docker 1.12,is open for everyone!You can download it at docker.com/getdocker."
      },
    
      "conference-2016-05-13-js-conf-budapest-day-2-html": {
        "title": "JS Conf Budapest Day 2",
        "url": "/conference/2016/05/13/JS-Conf-Budapest-day-2.html",
        "image": "/img/js-conf-budapest-2016.jpg",
        "date": "13 May 2016",
        "category": "post, blog post, blog",
        "content": "From JS Conf Budapest with loveThis year‚Äôs edition of JS Conf Budapest was hosted at Akv√°rium Klub.Located right in the center of the city, below an actual pool, filled with water!  Akv√°rium Klub is more than a simple bar: it is a culture center with a wide musical repertoire from mainstream to underground.There is always a good concert and a smashing exhibition, performance, or other event happening here, in a friendly scene, situated right in the city center.JS Conf Budapest is hosted by the one and only Jake Archibald from Google.Day 2 started at 9 o‚Äôclock.Enough time to drink great coffee and enjoy the breakfast.Day 2: Talks  Suz Hinton: The Formulartic Spectrum  Oliver Joseph Ash: Building an Offline Page for theguardian.com  NicolaÃÅs Bevacqua: High Performance in the Critical Rendering Path  Anand Vemuri: Offensive and Defensive Strategies for Client-Side JavaScript  Sam Bellen: Changing live audio with the web-audio-api  Rob Kerr: Science in the Browser: Orchestrating and Visualising Neural Simulations  Stefan Baumgartner: HTTP/2 is coming! Unbundle all the things?!?  Claudia HernaÃÅndez: Down the Rabbit Hole: JS in Wonderland  Lena Reinhard: Works On My Machine, or the Problem is between Keyboard and ChairDay 2: MorningSuz Hinton: The Formulartic SpectrumSuz is front-developer at Kickstarter. Member of the NodeJS hardware working group. Member of the Ember-A11y Project team.You can find her on Twitter using the handle @noopkat. She blogs on meow.noopkat.com.The physical world is just another binary machine.Data creation, analysis, and corruption combined with JavaScript can make new and unexpected things.Can you programmatically extract joy from the subjectivity it exists in?Can it be translated into intentional forms to hook others in?This session will gently take you along on a personal journey of how you can use code to expose new expressions of the mundane secrets we hold dear.Why are we here  Data &amp; Art  Make a messFeelingsWarning, a lot of feelingsPersonal history1994  Commodore 64 graphics book  Wants to make art on computer  The littlest artist  Accidental programmer (Suz didn‚Äôt really want to become a programmer)  Semicolon wars;; It doesn‚Äôt matter how you place your semicolon!This story is inspired by the movie Contact by Carl Sagan and makes Suz wonder: what does sound look like?Formulartic spectrum (made up word: art)  Analysing PCM data (Pulse Code Modulation -&gt; raw uncompressed data)  Resulted in only 13-ish lines of codeaudioContext.decodeAudioData(audioData)    .then(function(decoded) {    // just get the left ear, it's fine ;)    let data = decoded.getChannelData(0);    for (let i = 0; i &lt; data.length; i += 1) {        // convert raw sample to within 0-255 range        let hue = Math.ceil((data[i] + 1) * 255 / 2);        // convert HSL to an RGB array        let rgb = hslToRgb(hue, 200, 150);        // create the pixel        imgData.data[i*4] = rgb[0];        imgData.data[i*4+1] = rgb[1];        imgData.data[i*4+2] = rgb[2];        imgData.data[i*4+3] = rgb[3];    }    // put the pixels on a canvas element    canvas.putImageData(imgData, 0, 0);});Suz talked about programming and art.She spent a lot of time on the subway and was wondering if it would be possible to use the sounds of the subway to create art.So she started by taking the sound of the subway doors closing and analysing that part.  Sampling the audio to pixels resulted in 300k pixels  Make it smaller by converting to 16-beat songCheck out the visualisation!  The top section: Stand clear of the closing doors, please.  The mid section: white noise  The bottom section: ding dong!Suz created a visualisation of the sampled audio that resulted in cats sitting on an subway.  Cats can sit on 16 seats in subway car, each seat representing a beat  In total there were 308.728 samples which divided by 16 beats result in 19.295 samples per beat. Suz took the average of the sample values of each ‚Äòbeat‚Äô  The seats have different colors that represent the drum beat and oscillator note  When a cat is sitting on a chair, we get a guitar strum and noteThe subway example is made using:  SVG images  divs  CSS animationsCheck out the working example!But I‚Äôm better at hardwareSo Suz created a subway card with built in speaker!Recap  Creative coding gets you out of your comfort zone and teaches you to use tools you use everyday in another context  Art doesn‚Äôt care about your semicolons          Code can be messy      No one cares about semicolons, etc.        Art doesn‚Äôt care about perfection          Again, your code doesn‚Äôt really matter      Art is about what you learned  Write messy code  Make lots of mistakes  You deserve a break from being judged  Code like no one‚Äôs watching  Don‚Äôt ‚Äògit rebase -i‚Äô          Show the history behind good code      Code evolves from a first idea to a final solution.      At first, code might not be perfect      Don‚Äôt rebase to hide this fact      View the slides of Suz‚Äôs talk here!Oliver Joseph Ash: Building an Offline Page for theguardian.comOliver is a software engineer working on the team behind theguardian.com.Being passionate about the open web, he aims to work on software that exploits the decentralised nature of the web to solve non-trivial, critical problems.With a strong background in arts as well as engineering, he approaches web development in its entirety: UX, performance, and functional programming are some of the things he enjoys most.You can find him on Twitter using the handle @OliverJAsh.You‚Äôre on a train to work and you open up the Guardian app on your phone.A tunnel surrounds you, but the app still works in very much the same way as it usually would, despite your lack of internet connection, you still get the full experience, only the content shown will be stale.If you tried the same for the Guardian website, however, it wouldn‚Äôt load at all.Native apps have long had the tools to deal with these situations, in order to deliver rich user experiences whatever the user‚Äôs situation may be.With service workers, the web is catching up.This talk will explain how Oliver used service workers to build an offline page for theguardian.com.Oliver talked about the functionality they created with service workers on The Guardian.When offline on The Guardian, you‚Äôll get a crossword puzzle (always the most recent) that you can play.We summarized the key parts of the talk for you.Website vs nativeNative  Content is cached  Experience:          offline: stale content remains      server down: stale content remains      poor connection: stale while revalidate      good connection: stale while revalidate      Website  Experience          offline: nothing      server down: nothing      poor connection: white screen of death      good connection: new content      How it worksService workers  Prototype built in &lt; 1 dayWhat is a service worker?  A script that runs in the background  Useful for features that don‚Äôt need user interaction, e.g.:          Listen to push events, useful for pushing notifications      Intercept and handle network requests      Future                  Background sync          Alarms (e.g. reminders)          Geofencing                      A progressive enhancement  Trusted origins only (HTTPS only or localhost)  Chrome, Opera and Firefox stableFor now The Guardian is not yet fully on HTTPS, but they are switching at this time of writing.Some pages have service workers already enabled such as:  theguardian.com/info  theguardian.com/science  theguardian.com/technology  theguardian.com/businessHow did they do it?1. Create and register the service worker&lt;script&gt;if (navigator.serviceWorker) {    navigator.serviceWorker.register('/service-worker.js');}&lt;/script&gt;You can debug service workers in Chrome by selecting Service Workers under the Resources tab.2. Prime the cache  install event: get ready!  Cache the assets needed later  Version the cache. To check if a user has an old version so you can update with newer versions&lt;script&gt;var version = 1;var staticCacheName = 'static' + version;var updateCache = function () {    return caches.open(staticCacheName)        .then(function (cache) {            return cache.addAll([                '/offline-page',                '/assets/css/main.css',                '/assets/js/main.js'            ]);        });};self.addEventListener('install', function (event) {    event.waitUntil(updateCache());});&lt;/script&gt;3. Handle requests with fetch  fetch events          Default: just fetch      Override default      Intercept network requests to:                  Fetch from the network          Read from the cache          Construct your own response                    &lt;script&gt;self.addEventListener('fetch', function (event) {    event.respondWith(fetch(event.request));});&lt;/script&gt;It is possible to use custom responses when using Service Workers. E.g. Use templating to construct a HTML respose from JSON.&lt;script&gt;self.addEventListener('fetch', function (event) {    var responseBody = '&lt;h1&gt;Hello, world!&lt;/h1&gt;';    var responseOptions = {        headers: {            'Content-Type': 'text/html'        }    };    var response = new Response(        responseBody,        responseOptions    );    event.respondWith(response);});&lt;/script&gt;(Im)mutable  Mutable (HTML)          Network first, then cache      Page -&gt; service worker -&gt; server or cache -&gt; Page        Immutable (assets: CSS, JS)          Cache first, then network      Page -&gt; service worker -&gt; cache or server -&gt; Page      4. Updating the crosswordCheck if the cache has been updated and if it‚Äôs not up to date, update it and delete old cache.isCacheUpdated().then(function (isUpdated) {    if (!isUpdated) {        updateCache().then(deleteOldCaches);    }});Offline-firstWhy should we be building with offline first?  Instantly respond with a ‚Äúshell‚Äù of the page straight from cache when navigating a website  It improves the experience for users with poor connections  No more white screen of death  Show stale content whilst fetching new contentProblems and caveats  Browser bugs in both Chrome and Firefox  Interleaving of versions in CDN cacheThis can be fixed with a cache manifest.// /offline-page.json{    \"html\": \"&lt;html&gt;&lt;!-- v1 --&gt;&lt;/html&gt;\",    \"assets\": [\"/v1.css\"]}Why? Is this valuable  Fun  Insignificant usage due to HTTPS/browser support          ‚Ä¶ but plant the seed and see what happens        Iron out browser bugs, pushes the web forward  ‚ÄúIf we only use features that work in IE8, we‚Äôre condemning ourselves to live in an IE8 world.‚Äù ‚Äî Nolan LawsonConclusion  Service workers allow us to progressively enhance the experience for          Offline users      Users with poor connections        It‚Äôs easy to build an offline page  A simple offline page is a good place to startThe slides of Oliver‚Äôs talk can be viewed on Speaker Deck.NicolaÃÅs Bevacqua: High Performance in the Critical Rendering PathNicolaÃÅs loves the web. He is a consultant, a conference speaker, the author of JavaScript Application Design, an opinionated blogger, and an open-source evangelist.He participates actively in the online JavaScript community ‚Äî as well as offline in beautiful Buenos Aires.You can find him on Twitter using the handle @nzgb and on the web under the name ponyfoo.com.This talk covers the past, present and future of web application performance when it comes to delivery optimization.I'll start by glancing over what you're already doing -- minifying your static assets, bundling them together, and using progressive enhancement techniques.Then I'll move on to what you should be doing -- optimizing TCP network delivery, inlining critical CSS, deferring font loading and CSS so that you don't block the rendering path, and of course deferring JavaScript.Afterwards we'll look at the future, and what HTTP 2.0 has in store for us, going full circle and letting us forego hacks of the past like bundling and minification.Getting startedMeasure what is going on and see what is going on!Use the Chrome DevTools Audits.  Per-resource advice  Caching best practicesPageSpeed Insights (Google)developers.google.com/speed/pagespeed/insights/  Insights for mobile  Insights for desktop  Get a rough 1-100 score  Best practices  Practical adviceWebPageTest (webpagetest.org)webpagetest.org  Gives analytics and metrics where you can act on  A lot of statistics  PageSpeed Score  Waterfall View: figure out how to parallelize your download to speed up loading  Makes it easy to spot FOIT  Calculates SpeedIndex: SpeedIndex takes the visual progress of the visible page loading and computes an overall score for how quickly the content painted  Inspect every request  Analyze TCP traffic  Identify bottlenecks  Visualize progressAutomate!But we can automate a lot!  Measure early. Measure often.PageSpeed Insights is available as npm module.npm install psi -gWebpagetest is also available as npm module but is a bit slower.npm install webpagetest-api underscore-cliYSlow is available for different platforms.npm install grunt-yslow --save-devBudgets  Enforce a performance budget  Track impact of every commit  What should I track? More info about this on timkadlec.com/2014/11/performance-budget-metrics          Milestone Timings: Load time, time to interact, ‚Äútime to first tweet‚Äù      SpeedIndex: Average time at which parts of a page are displayed      Quantity based metrics: Request count, page weight, image weight ‚Ä¶      Rule based metrics: YSlow grade, PageSpeed score, etc.      Budgeting can also be automated using the grunt-perfbudget plugin.npm install grunt-perfbudget --save-devWhat can we do beyond minification?Minification is usually the first thing developers think of when talking about optimizing your code for speed.But what are the things we can do beyond minification?A lot of best practices on optimizing performance in your app are described in the High Performance Browser Networking book written by Ilya Grigorik.For all the detailed tips and tricks we suggest to view the slides for NicolaÃÅs‚Äôs talk on ponyfoo.com.Anand Vemuri: Offensive and Defensive Strategies for Client-Side JavaScriptAnand is Senior Application Security Consultant at nVisiumYou can find him on Twitter using the handle @brownhat57.This talk will specifically focus on the other less common client-side vulnerabilities that are not as frequently discussed.Intentionally vulnerable applications developed with client-side JavaScript frameworks will be attacked and exploited live.Remediation strategies will also be discussed so that developers have tools to prevent these vulnerabilities.Through strengthening the security posture of JavaScript applications, we can take strides towards creating a more secure Internet.Break the web together!  They say the best offense is good defense.No. The best offense is offense.Hands-on vulnerability exploitation of medcellarAnand‚Äôs talk started by explaining the most common web application vulnerabilities that currently exist.We‚Äôre talking about SQL Injection, Cross Site Scripting (XSS) and Cross Site Request Forgery (CSRF).During the talk, Anand used an open source application that contains all of these vulnerabilities and which is available for you as a developer to fool around with.The application is called ‚ÄòMedCellar‚Äô and you can find it on github.XSS &amp; CSRFWe saw how to perform XSS attacks and CSRF attacks on the MedCellar Application.These attacks weren‚Äôt extremely harmful at first but showed just how they could be exploited.Using the Burp Suite‚Äôs proxy, we were able to inspect all requests/responses the application was performing to get more insights in how the app actually worked.Burp Suite is an integrated platform for performing security testing of web applications.XSS  attacks users  JS Injection  Exploits can be bad, really badXSS is a serious vulnerability. It may not seem so for some people or clients but it really is!  How do we exploit apps where users have direct control  How do we attack web apps on a private networkCSRF Attacks!!  ‚ÄúSession Riding‚Äù  Attacker sends malicious URL to submit a form to a third party domain  Victim is tricked into interacting with the malicious link and performs undesirable actionsUsing a third party domain you can create a form (you won 1 million dollars) to perform an action like this.BeEFDuring the talk, Anand demonstrated how to perform XSS and CSRF attacks.However, it seemed like you were only able to hack yourself.Things got serious though, when Anand demonstrated how you could exploit these vulnerabilities way more by using a special Linux distro called Kali Linux and BeEF (Browser Exploitation Framework).Kali Linux is a linux distro designed specifically for Penetration Testing and Ethical Hacking.BeEF is a Penetration Testing tool that focusses on the browser and possible vulnerabilities in it and the applications running in it.Combining these two, Anand was able to do basically anything in the users browser and he demonstrated this by running some random audio in the users browser.Playing audio isn‚Äôt that harmful, but you could have installed a keyLogger instead and start tracking anything the user types on his computer.That seems to be a little bit worse than playing some audio!  When you enter a coffee shop and see someone using this, disconnect from the internet and run away as fast as you possibly can.‚Äù - Quote from AnandMitigate against these attacksImplementation of a CSRF mitigation is Tough!  Method Interchange  Beware of CSRF Token replay  Token must be tied to the user‚Äôs session on the server  CSRF Token exposed as GET Param: Could potentially have logs or some other network traffic see the CSRF token and intercept it that way.But, luckily for us, CSRF middleware which implements these mitigations has already been developed for us! You can find these libraries on github:  koajs  crumb  csurfKey takeaways  App Sec vulnerabilities can be used in combination  No state changing operations should be GET requests  Make sure the CSRF token is cryptographically secure          Random !== Cryptographically secure        CSRF Middleware Saves Lives!!Oh‚Ä¶ And  Cross Origin Resource sharing (CORS)          Access-control-Allow-Origin: * IS BAD!      Day 2: afternoonSam Bellen: Changing live audio with the web-audio-apiSam is developer at Made with love.You can find him on Twitter using the handle @sambego.As a guitar player, I usually use some effects pedals to change the sound of my guitar.I started wondering: ‚ÄúWhat if, it would be possible to recreate these pedals using the web-audio-api?‚Äù.Well, it turns out, it is entirely possible to do so.This talk takes you through the basics of the web-audio-api and explains some of the audio-nodes I‚Äôve used to change the live sound of my guitar.Presentation can be found here: https://github.com/Sambego/pedalboard-presentationGet the sound in the browser  Create new audio context.  Get the audio input of your computer: navigator.getUserMedia()  Create inputNode from the media stream we just fetched  Connect the inputNode to the audiocontext.destinationAdd effects to the soundVolume pedal  Create a gainNode = audioContext.createGain();  Value of gain is 0 tot 1  So for now we have input -&gt; gain -&gt; output.Distortion pedal  Make the audio sound rough.  Create a waveShaperNode = audioContext.createWaveShaper();  Set a value  So for now we have input -&gt; Waveshaper -&gt; output.Delay pedal  delayNode = audioContext.createDelay();  Set a value delayNode.delayTime.value = 1 (1 second)Reverb pedal  Some kind of echo on your sound  convolverNode = audioContext.createConvolver()  Load impulse-response-file and do some crazy stuffHow to create an oscilator  oscilatorNode = audioContext.createOscilator()  Set Hz valueweb-midi-api  Request access and start doing things with itRob Kerr: Science in the Browser: Orchestrating and Visualising Neural SimulationsRob works at IBM Research Australia.You can find him on Twitter using the handle @robrkerr.My talk will show how the old-school, computationally-heavy software used in science can be set free using the centralized power of cloud resources and the ubiquity of the browser.We'll see real-time, publicly-broadcast, simulations of the electrical activity in brain cells, visualised in 3D using Javascript.Neuroscience introductionThe topic for this talk was quite some heavy material.However, Rob managed to give us a quick, super high-level, introduction to neuroscience and more specifically an introduction to how neurons actually work.Very High level, there are 3 parts in a neuron:  Dendrites  Neuron body (Soma)  Axons.Neurons receive electrical signals through their dendrites, and transmit those to the neuron body, called the Soma.From the neuron body, new electrical signals travel to other neurons.Sending electrical current from one neuron to another is being done through its axons.So the axons actually send electrical signals to other neurons and those other neurons receive these signals trough their dendrites.A better, more thorough explanation of neurons is being described on Wikipedia, but we needed a super simplified explanation of neurons and their main components to further explain what Rob showed us.Science in the browserNeurons and their main components can be ‚Äòencoded‚Äô in special files .swc files.These files contain multiple records with an ID, X, Y, Z, Radius and Parent-link.Using all the records and their properties allows you to visually represent the neurons.There‚Äôs already an online repository containing these encoded neurons which you can find here.Now, what does all of this have to do with the browser or JS or anything you would expect at JSConf?Well, while he was working on his Ph.D. thesis, he started playing around with JS and its related technologies.And he continued to do so since then, all in function of the neuroscience domain.As we saw earlier, there‚Äôs already a webpage where you can upload swc files with neuron data to visually represent these, but these are rather static images.Instead, Rob decided to create a platform which can also simulate the behaviour of such a neuron when you trigger it with electrical current on its dendrites.Technology stackRob used a combination of tools and technologies to build the platform.Together with his colleagues at IBM research Australia, they built an entire Cloud platform that could perform these complex simulations.On their IBM Bluemix cloud they run Docker Containers running the algorithm that performs the neuron simulations.The algorithm is written in C and is based on mathematic formula which is shown in the below image.Hodgkin-Huxley Model of the Squid Giant AxonThe web application used to render the neurons used a combination of tools, most importantly:  Webgl: Web Graphics API. Javascript API for rendering interactive 3D graphics.  three.js: A Javascript 3D library that uses WebGL.  D3.js: Javascript library for visualizing data using HTML, SVG and CSSThe tool in actionIn the below video you can see what the tool looks and animations look like:\tThe tool enables researchers to replay a scenario where a certain spike is triggered in a branch of the neuron.This gives scientists a lot of knowledge and insights about how neurons behave.Rob gave a really entertaining talk with some really cool visuals of neurons in action.He introduced us to just the right amount of neuroscience to be able to follow what he was actually doing and showing!Stefan Baumgartner: HTTP/2 is coming! Unbundle all the things?!?Stefan is a web developer/web lover based in Linz, Austria.Currently working at Ruxit, making the web a faster place.He is also a co-host at the German Workingdraft podcast.You can find him on Twitter using the handle @ddprrt.In this session, we will explore the major features of the new HTTP version and its implications for todays JavaScript developers.We will critically analyze recommendations for deployment strategies and find out which impact they have on our current applications, as well as on the applications to come.Unbundle all the things?Everybody is saying to not bundle things, minify things, concatenate things, ‚Ä¶ when moving to HTTP/2.Tools like Browserify, Webpack, etc. would all become obsolete.But why? We need to question this and see if this is actually the truth.The best request is a request not being madeIn HTTP version 1.1 we need to do as few requests possible. Pages like Giphy have 40 TCP connection at a single time!HTTP/2 was made to prevent the bad parts of HTTP/1.1HTTP/2 allows a connection to stay open and transfer multiple things over the same connection.No need for handshakes for each file that needs to be transferred from the server to the client.Rule of thumbA slow website on HTTP/1.1 will still be a slow website on HTTP/2.You need to perform optimisations no matter what.Most important part: do not block the render path.Only serve what you really need.Again, the best request is a request not being made.So, unbundle all the things?So in some way, yes unbundle all the things.Because you don‚Äôt want to transfer bytes you don‚Äôt need, but there is something more to it.This article about packaging will get you on the way: engineering.khanacademy.org/posts/js-packaging-http2.htm.Create a lot of modules to update as flexible as possible and as small as possible.When using ES6 we can also use Treeshaking.  Create independent, exchangeable components  Create small, detachable bundles  Think about long-lasting applications and frequently of changeUse tools, not rules!Claudia HernaÃÅndez: Down the Rabbit Hole: JS in WonderlandClaudia is Mexican front-end developer.You can find her on Twitter using the handle @koste4.What even makes sense in Javascript?For a language originally created in 10 days it surely has a lot of quirks and perks many JS developers are unaware of.Sometimes, it might even seem like we fell down the rabbit hole only to find that NaN is actually a Number, undefined can be defined, +!![] equals 1, Array.sort() may not work as you suspected and so much other nonsense that can trip any JS developer‚Äôs mind.This talk is a collection of Javascript‚Äôs oddities and unexpected behaviors that hopefully will prevent some future headaches and help understand the language that we all love in a more deeper and meaningful way.This talk by Claudia was so much fun! We didn‚Äôt write down anything because it was virtually impossible to do. You need to see this with your own eyes!You can view the slides on Speaker Deck.Be sure to check out jsfuck.com for some fun times and jQuery Screwed to get an idea of what you can actually do with JavaScript quirks.Lena Reinhard: Works On My Machine, or the Problem is between Keyboard and ChairLena is teamleader, consultant and photographer.You can find her on Twitter using the handle @lrnrd.In this talk we will look at the many facets that affect our decision making and interactions, and work out how we can change for the better.Together, we will take a look at the effects that our software has on the daily lives of the thousands of people who are using it.You‚Äôll learn what you can do as an individual to support change into a positive direction, and how you can help debug this system and make a difference in the tech industry.You‚Äôll leave knowing about practical things you can do in your daily life to make the tech industry a better, more inclusive and diverse environment that is a better place for everyone.Code debuggingDebugging can be hard and it becomes harder when working with complex software.Spaghetti code is difficult to read and maintain.It can be code that is not organised, has lots of dependencies and is difficult to debug.The Tech Industry is buggedA lot of people already contributed to the tech industry.It has grown very fast and has many flaws.That‚Äôs why we need to have a look at it and try to fix the defects.Understanding ourselvesTo be able to fix this we need to understand ourselves. Our flaws, limitations, ‚Ä¶  We are privileged and need to understand that.Privilege: The human version of ‚Äúworks on my machine‚Äù.Privilege is sitting in your comfy home and not knowing a big thunderstorm is coming that could harm people.Privilege is being able to stand up when attending a standup and not having to sit because you are disabled.We are biasedWe need to understand we are biased.More often we are being objective and often that is not OK.We all have biases and we need to realise and understand.EmpathyWe need to understand that we need to be empathetic.Empathy is the right direction.CreativityCreativity is necessary to design and build good software.DiversityAnd so is diversity and understanding each other.InclusionInclusion means all people in the group are respected for who they are.The lack of inclusion and diversity is a real problem in our industry.The Tech IndustryLet‚Äôs look at some key points within our industry.Company  Lack of diversity  Lack of inclusion  HarassmentSociety  Racism  Patriarchy  CapitalismTech industry  Lack of diversity  Lack of inclusion  Harassment  Racism  Patriarchy  CapitalismSoftware can help peopleOur software can help people. A screenreader, accessibility, ‚Ä¶But can also ruin livesOur software is racist.Our software (tools like Siri or Cortana or Snapchat) does not correctly recognise skin color, alters skin color and does not recognise harassment or racism.Animations in software can trigger panic attacks or epileptic attacks.  We have a collective responsibility and need to take that very seriously.Technology and our code is not neutral. Our work is political and has consequences on lives.Debugging the systemChange starts with you, starts with all of us.What can we do to debug the system?  Educate yourself, about systemic issues and oppression  Practice empathy, because we need it to be good designers and developers  Work on Humility, because none of us are Unicorns  Understanding Privileges, and use them for good  Address biases, and establish policies to address them  Listen, and actively look for voices outside of your networks  Amplify others‚Äô voices, and speak less  Work on diversity, because it‚Äôs our moral obligation  Work on inclusion, to make spaces welcoming and safe  Give, our knowledge, time, technical skills, money  Work on being allies, constantlyQuite a talk on some serious matter to close the second day of JS Conf Budapest.Have you experienced these things yourself in the tech industry?Have you contributed to debugging the tech industry?Day 2: ConclusionJust like day 1, day 2 was one hell of a nice day packed full of great speakers and a superb atmosphere!The talks by Rob Kerr and the one of Lena Reinhart surely got the most attention.Rob‚Äôs talk because it was impressive to see what they achieved over a course of 2 years to visualise neurons in the browser.Lena‚Äôs talk because we got slammed in the face about how faulty the tech industry is at the moment.This year‚Äôs edition was, just like the one we attended last year a very good one!It is nice to see such a diverse community that cares about technology and people.This is something we should be very proud of.A big thank you to the organisers and volunteers to make JS Conf Budapest what it is!Find us on the family photo!Next yearIn 2017, JS Conf Budapest will be held on the 14th and 15th of September.We will surely be present for what will be another great edition! See you next year!JS Conf Budapest 2016, day 1Read our full report on day 1 of JS Conf Budapest here!."
      },
    
      "conference-2016-05-12-js-conf-budapest-day-1-html": {
        "title": "JS Conf Budapest Day 1",
        "url": "/conference/2016/05/12/JS-Conf-Budapest-day-1.html",
        "image": "/img/js-conf-budapest-2016.jpg",
        "date": "12 May 2016",
        "category": "post, blog post, blog",
        "content": "From JS Conf Budapest with loveThis year‚Äôs edition of JS Conf Budapest was hosted at Akv√°rium Klub.Located right in the center of the city, below an actual pool, filled with water!  Akv√°rium Klub is more than a simple bar: it is a culture center with a wide musical repertoire from mainstream to underground.There is always a good concert and a smashing exhibition, performance, or other event happening here, in a friendly scene, situated right in the city center.JS Conf Budapest is hosted by the one and only Jake Archibald from Google.After waiting in line at 8 o‚Äôclock in the morning to get our badges, we were welcomed at the main hall where some companies hosted stands.In another space after the main hall, tables were nicely dressed and people could have breakfast.When going downstairs to the right of the main hall, we entered the room where the talks would be given.For the coffee lovers, professional baristas served the best coffee possible.With a nice heart drawn on top if it.At 9 o‚Äôclock the conference would officially start so we went downstairs.After taking our seat, we played the waiting game and all of a sudden, we got this nice intro made with blender and three.js! Check it out for yourself!Day 1: Talks  Laurie Voss: What everybody should know about npm  Safia Abdalla: The Hitchhiker‚Äôs Guide to All Things Memory in Javascript  Yan Zhu: Encrypt the web for $0  Denys Mishunov: Why performance matters  Princiya Sequeira: Natural user interfaces using JavaScript  Maurice de Beijer: Event-sourcing your React-Redux applications  Rachel Watson: The Internet of Cats  Nick Hehr: The other side of empathyDay 1: MorningLaurie Voss: What everybody should know about npmLaurie is CTO at npm Inc.You can find him on Twitter using the handle @seldo.The presentation he gave can be found a slides.com/seldo/jsconf-budapest.npm is six years old, but 80% of npm users turned up in the last year.That's a lot of new people! Because of that, a lot of older, core features aren't known about by the majority of npm users.This talk is about how npm expects you to use npm, and the commands and workflows that can make you into a power user.There will be lots of stuff for beginners, and definitely some tricks that even most pros don't know.How does npm look up packages?In contrast to what most people think, npm does not download its modules from GitHub or other version control systems.They would not like it that such an amount of data is transferred on a daily basis.In short npm does this: You -&gt; CLI -&gt; Registry.Let‚Äôs dive in.  First, npm will take a look at your local cache and see if the package your are looking for is present.  Next, it will resort to the CDN network and use the server which is the closest as possible to your position.  Finally, if npm can‚Äôt find the package in local cache or the CDN network, it will look it up in the registry. The registry is a set of servers all around the world and it will try to match the best version that you are looking for.EACCESS errorA lot of people have issues with EACCESS errors because they used sudo to install things.The easy solution is to always keep on using sudo, BUT we can easily fix npm permission issues.package.jsonDon‚Äôt write your package.json yourself. Let NPM do it!It will always do it better. Use npm init, which will ask you some basic questions and generate package.json for you.ScopesA new feature in npm is scopes.These are modules that are ‚Äúscoped‚Äù under an organization name that begins with @.Scopes can be public and private.Here is how to use scopes:npm init --scope=usernamenpm install @myusername/mypackagerequire('@myusername/mypackage')npm-init.jsTo extend the npm init command, it is possible to create an npm-init.js file.This file is a module that will be loaded by the npm init command and will provide basic configurations for the setup.By default the file is placed in the root of your project: ~/.npm-init.js.You can use PromZard to ask questions to the user and perform logic based on the answers.Remember that npm init can always be re-run.Why add stuff in devDependencies.Simple: because production will install faster! A lot of people don‚Äôt tend to do this, so please do this!When using this you can simple run the command below on production and be done with it.npm install --productionBundled dependenciesOne of the biggest problems right now with Node.js is how fast it is changing.This means that production systems can be very fragile and an npm update can easily break things.Using bundledDependencies is a way to get round this issue by ensuring that you will always deliver the correct dependencies no matter what else may change.You can also use this to bundle up your own, private bundles and deliver them with the install.npm install --save --save-bundleOffline installsA way to prevent npm to look up the registry, and ensure local installs, is by adding the variable --cache-min and to set it to a high value such as 999999.npm install --cache-min 999999Run scriptsIn package.json it is possible to define default run scripts as shown below.npm startnpm stopnpm restartnpm testOf course it is also possible to define your own run scripts.You can run these scripts like this:npm run &lt;anything&gt;Run scripts get devDependencies in pathDon‚Äôt force users to install global tools. That is just not cool.This way you can prevent to get conflicts over global tools, because different projects can use different versions.SemVer for packagesnpm uses Semantic Versioning, which is a standard a lot of projects use to communicate what kind of changes are in a release.It‚Äôs important to communicate what kinds of changes are in a release because sometimes those changes will break the code that depends on the package.Let‚Äôs take a look at an example.1.5.6Breaking Major . Feature Minor . Fix PatchThis is quite obvious, right?npm allows you to change the version (and to add a comment) by using the commands below.npm version minornpm version majornpm version patchnpm version major -m \"Bump to version %s\"Microservices architectureWhen working with a microservices architecture, it is possible to work with multiple packages for your services.This can be done by using the link function within npm.npm link &lt;dependency&gt;Let‚Äôs say we have a package named Alice and we have other packages that depend on this package.We can run npm link.In packages that depend on Alice, say Bob, we simply run npm link alice.All changes made in Alice will be immediately available in Bob without performing any npm update commands.Unpublish a packageBefore the recent events where a package called left-pad got pulled from npm and broke the internet, it was possible to unpublish a package just like that by using npm unpublish.Now this is restricted after the package has been online for 24 hours.To really unpublish the package you will need to contact support.A more friendly way is the use of npm deprecated that will tell users the package has been deprecated.Keeping projects up to dateBefore running npm update, it‚Äôs preferred to run npm outdated.This command will check the registry to see if any (specific) installed packages are currently outdated.npm outdatednpm updateBy doing so, you can prevent yourself from breaking the project if certain packages would not be compatible.Stuff everybody should know about npmA lot of things are available for npm that will make your life as a developer easier.  Babel: Transpile all the things! JavaScript, TypeScript, JSX, ‚Ä¶  Webpack and Browserify  Greenkeeper (greenkeeper.io) is npm outdated as a service!  Node Security Project: Install by using npm install nsp -g. Use by running nsp check. You can use this to check if your project contains vulnerable modules.Why should I use npm?npm reduces friction.It takes things you have to do all the time and makes things simpler and faster.Safia Abdalla: The Hitchhiker‚Äôs Guide to All Things Memory in JavascriptSafia is a lover of data science and open source software.You can find her on Twitter using the handle @captainsafia or on her webpage safia.rocks.Slides and interactive tutorialThe slides of this talk can be found here http://slides.com/captainsafia/memory-in-javascript.Safia also created an interactive tutorial on how to use the Chrome DevTools for memory management.This talk will take beginners through an exploration of Javascript's garbage collector and memory allocation implementations and their implications on how performant code should be written.Attendees will leave this talk having gained insights into the under-the-hood operations of Javascript and how they can leverage them to produce performant code.Why should I care about memory?  It forces us to be (better) more inventive programmers, adds restrictions and forces us to use the best tools to create the best possible experience.  Memory is scarce. A lot of people still use devices that are not packed with a lot of memory.Not everyone has high performant development machines.  It helps us exercise our empathy muscles.What does it mean to manage memory?The Good, The Bad, The UglyHow does JS manage memory?Safia focuses on the V8 JS Engine.We have basic types in JavaScript:  booleans  numbers  stringsMemory is allocated in a heap structure and uses a root node which has references to other ones: booleans, string, etc.So basically: root node -&gt; references -&gt; variables.V8 allocates objects in memory in 6 contiguous chunks, or spaces:  New space: Memory gets allocated here when an object is created immediately.It is small and is designed to be garbage collected very quickly, independent of other spaces.  Old pointer space: Contains most objects which may have pointers to other objects.Most objects are moved here after surviving in new space for a while.  Old data space: Objects that just contain raw data (no reference or pointer) will end up here after surviving in new space for a while.  Large object space: Used to store large object tables.They get stored here so it doesn‚Äôt conflict with the store space of the above mentioned spaces.  Code space: Code objects are allocated here. This is the only space with executable memory.  Map space: Contains objects which are all the same size and has some constraints on what kind of objects they point to, which simplifies collection.How does V8 collect garbage memory?V8 uses a ‚Äòstop the world‚Äô technique that enables it to run a short garbage collection cycle.This means it will literally halt the program.V8 has different approaches on how it collects garbage in the new and old space.  New space: Garbage collection by using a scavenging technique.Each scavenging cycle will go through the entire heap starting from the root and will create copies.It will clear out what is currently in new space.Everything that is not reachable will be cleared out of the space.You need double the size of the memory that is available for the new space to use for the copy.  Old space: Mark and sweep technique.Remove unmarked objects on a regular basis.How do I write memory performant applications?Asking yourself the following two question will get you started!  How much memory is my application using?  How often do garbage collection cycles occur in my application?Of course you need to have the tools to work with.The Chrome DevTools HEAP allocation profiler will be our weapon of choice.It allows you to check the retain size and shallow size of objects.  Shallow size of an object is the amount of memory it holds of itself.  Retain size is all of its size and its dependents.Heap dumpHeap dump takes a snapshot of your heap at a specific moment.It will provide a file with .heap extension which enables you to load it in the Chrome DevTools for further inspection.npm install heapdumpLet‚Äôs practice!Follow this interactive tutorial on how to use the Chrome DevTools for memory management.Yan Zhu: Encrypt the web for $0Yan is engineer @brave and likes information freedom, breaking shit, cryptography, theoretical physics, free software, infosec, stunt h4cking, and an Internet that respects humans.You can find her on Twitter using the handle @bcrypt.Everyone in 2016 knows that websites should use HTTPS.However, there is a common misconception that TLS and other security measures are expensive and slow down both web developers and page load times.This talk will show you some easy tricks to make your site more secure without sacrificing performance or requiring a lot of effort.Is the web fast yet?Yes. Size of pages is rising. Amount of HTTPS requests is also rising!Is TLS fast yet?Yes. Netflix is going to secure streams this year over HTTPS.  2015: Netflix and chill  2016: Netflix and HTTPS and chillThe numbers aren‚Äôt entirely clear, so here they are:  Without encrypted Netflix streams, 65% of internet traffic is unencrypted. Only 29% of internet traffic is encrypted.  With encrypted Netflix streams, unencrypted internet traffic will drop to 26,9% and encrypted traffic will increase to 67,1%.Source: https://www.sandvine.com/downloads/general/global-internet-phenomena/2015/encrypted-internet-traffic.pdf  TLS has exactly one performance problem: it is not used widely enough.Everything else can be optimized.  Data delivered over an unencrypted channel is insecure, untrustworthy, and trivially intercepted.We owe it to our users to protect the security, privacy, and integrity of their data ‚Äî all data must be encrypted while in flight and at rest.Historically, concerns over performance have been the common excuse to avoid these obligations, but today that is a false dichotomy. Let‚Äôs dispel some myths.Keep reading about this matter on istlsfastyet.com.HTTP/2Another technology that can help the adoption of TLS is HTTP/2.HTTP/2 offers:  Binary encoding instead of text encoding  header compression  Server push  multiple requests on single TCP connection!!HTTP/2 allows for requests to be sent in parallel rather than sequentially.Does HTTP/2 require encryption? No. However, Chrome and Firefox will only support HTTP/2 with encryption.Let‚Äôs EncryptLet‚Äôs Encrypt (a non-profit certificate authority) has left beta stage on the 12th of April and is a new Certificate Authority: It‚Äôs free, automated, and open.It is backed by some major sponsors such as Mozilla, Akamai, Cisco Chrome, and so much more.  The objective of Let‚Äôs Encrypt and the ACME protocol is to make it possible to set up an HTTPS server and have it automatically obtain a browser-trusted certificate, without any human intervention.This is accomplished by running a certificate management agent on the web server.Interested in Let‚Äôs Encrypt? Keep reading on letsencrypt.org.Get HTTPS for free!Manually setting up your free HTTPS certificates from Let‚Äôs Encrypt is also an option. You can do that on gethttpsforfree.com.Denys Mishunov: Why performance mattersDenys is frontend developer, speaker. Science aficionado. And writes for @smashingmag.You can find him on Twitter using the handle @mishunov or on his personal website mishunov.me.Performance is not about Mathematics.Performance is about Perception.Perception is what makes a site with very few requests nevertheless feel slow, while a site that delivers search results during tens of seconds can feel fast enough for your user.User‚Äôs perception of your website‚Äôs speed is the only true performance measure.This talk is about perception, neuroscience and psychology. The time is ripe to understand performance from the user‚Äôs perspective.In this talk Denys showed us that performance is not always in the numbers, but that it is most of the time perception.So next time you decide to invest a bunch of money in getting that request 100ms faster, make sure it will have impact!  Performance is about perception! Not mathematics.Houston Airport was used as an example to illustrate this quote.At Houston Airport, there were a lot of complaints about long waiting times at the baggage claim.They decided to optimize the baggage handling process.They managed to get luggage to the baggage claim in about 8 minutes (which is nice!).However, complaints weren‚Äôt dropping at all.It turned out that passengers needed only 1 minute to get from the plane to the baggage claim, which meant they needed to wait 7 minutes for their luggage.Eventually they decided to literally taxi and park the airplanes further so passengers now needed to walk 6 minutes from the plane to the baggage claim which reduced waiting times for luggage to 2 minutes.This caused complaints to drastically reduce!Speed!1 second gain will increase revenue by 1% for Company X. 1 second slower will decrease conversions by approximately 5%.The 20% rule.This rule defines that you should make a page load at least 20% faster, otherwise users will not notice.We‚Äôre talking about noticeable difference.A big difference with meaningful difference.Noticeable !== MeaningfulWe did a live test on the conference where the crowd needed to decide which of the two pages displayed loaded faster.The first page loaded in 1.6 seconds whereas the second one loaded in 2 seconds.Most of the people thought the second page, with 2 seconds load time was faster. This is all about perception!Another fun fact is when delaying audio on a video, our mind will trick us by syncing the audio with what is visible on the screen. Again perception!Key takeawayDon‚Äôt spend to much time optimizing the nitty gritty details of your code, instead try moving the active phase forward.As soon as there is activity being shown (pages being loaded), the brain enters the active phase.The user no longer feels as if he‚Äôs waiting (Remember the perception?).You can move the active phase forward by making use of:  async  Service workers  ‚ÄúThe perception of performance is just as effective as actual performance in many cases‚Äù - Apple quoteDay 1 afternoonPrinciya Sequeira: Natural user interfaces using JavaScriptPrinciya works at Zalando Tech where she uses React and Redux. She‚Äôs also a startup enthusiast, teacher, speaker, DataViz Diva and has a love for food and JavaScriptYou can find her on Twitter using the handle @princi_ya and Zalando Tech using the handle @ZalandoTech.The way we interacted with computers on a large scale was stuck in place for roughly 20 years.From mouse to keyboard to joystick, it is game over.Today it is the era of gestures.Today‚Äôs gamers can do everything from slice and dice produce in Fruit Ninja to quest for a dragon in Skyrim.We‚Äôve been captivated by these powerful, natural, and intuitive interactions; imagining what it would be like to have that power at our own fingertips.In this recent decade, we‚Äôve seen some staggering advances in technologies bring us closer making these magical experiences a reality.In this talk I will present how we can create new, intuitive, interactions for these novel input devices using JavaScript.This talk takes on a different approach in user interaction, in way that different ways of input can result in the same output.At this moment we know the evolution of Typed, Clicked and Touched but currently we are evolving to Typed, Clicked, Touched, Guestures/Speech/‚Ä¶ etc.Evolution of user interfaces.  CLI: Codified, Strict  GUI: Metaphor, Exploratory  NUI (Natural User Interfaces): Direct Intuitive. More natural and more intuitive.NUI + JS = NUIJSAt first, Princiya was trying to build a simulator for motion controlled 3D camera‚Äôs.A tool that is not dependent on any platform without using a physical device.The simulator is purely based on JavaScript and easily integrates with the device‚Äôs SDKs.Once the simulator was made, she tried to build some apps with it (using leap motion for example) to move a slideshow or any other purpose.The tool can be used for many purposes an a lot of devices are already available (VR, motion, ‚Ä¶)  Augmented Reality.  Virtual Reality.  Perceptual Computing: bringing human like behaviour to devicesWhat next?Architecture  Step 1: USB controller reads sensor data  Step 2: Data is stored in local memory  Step 3: Data is streamed via USB to SDKLive demoPrinciya demonstrated a drawing board with a brush, both with mousepointer and LEAP motion.NUIJS will translate the input data from the mouse pointer to the Node.js Web Socket server and this one will process the data and send it back to the LEAP motion SDK.The same code can be used with the LEAP motion itself since it integrates nicely with the device‚Äôs SDKs.Other open source tools Princiya mentioned were Webcam Swiper and js-objectdetect.Viola-Jones AlgorithmMost of the tools will use or depend on the Voila-Jones Algorithm which can be used for object detection.Combined with other tools this can be very powerful.  HAAR feature selection  Creating an integral image  Adaboost training  Cascading classifiersMaurice de Beijer: Event-sourcing your React-Redux applicationsMaurice is a freelance developer/trainer and Microsoft Azure MVP.You can find him on Twitter using the handle @mauricedb.With Event-Sourcing every action leading up to the current state is stored as a separate domain event.This collection of domain events is then used as the write model in the CQRS model.These same events are projected out to a secondary database to build the read model for the application.In this session Maurice de Beijer will explain why you might want to use Event-Sourcing and how to get started with this design in your React and Flux applications.What is Event-sourcingEvent-sourcing is a way of capturing changes in the state of an application.The traditional way of doing this would be to just update the existing state of your application to whatever state it should be in.This way you always have the latest state of your data at your disposal.In Event-sourcing, you‚Äôll capture all changes as events.These events will be stored in the sequence they were applied.You now have a complete log of events that happened in your application.This allows for features such as:  Complete Rebuild: Possibility to rebuild the entire application state by re-running all events.  Temporal Query: Determining the state of the application at a given point in time.  Event Replay: Replay incorrect events by reversing it and all subsequent events, then replaying the correct event and re-applying all later events.Common example of systems that use Event Sourcing are Version Control Systems.When to use Event-sourcing?Event-Sourcing is particularly useful in situations where you need to keep an audit trail of all changes that occurred to your data.Accountancy for example is a domain in which Event-Sourcing is very useful, because you need to be able to provide that trail for audit purposes.REPHRASE! -&gt; The immutability of events allows for more scalability in your apps also.CQRS and Event SourcingWhere Event-Sourcing describes the practice of storing all application state changes in individual events, CQRS describes the practice of separating the command from the read side.This means you‚Äôll have a service exposing all write functionality in your application and a separate service exposing all read operations.This model works well with Event-Sourcing as you can use the Events occurring on your system as Commands in the CQRS model.During the talk, Maurice showed some samples of code that were the pieces of the puzzle in setting up Event-Sourcing and CQRS in your React-Redux application.Check out the slides for his talk here to find out more!Rachel White: The Internet of CatsRachel is a front end developer at IBMWatson. A lover of retro graphics &amp; horror &amp; coding &amp; games, but above all, of Cats.You can find her on Twitter using the handle @ohhoe.Find out more about her and her projects on rachelisaweso.me and imcool.onlineEver lose out on a good night's rest because your pesky cats keep waking you up at 4am for food?Rachel has.Many times.For her first project using node, socket.io, microcontrollers, and johnny-five, Rachel built a web-based feeder that delivers tasty cat chow on a configurable schedule or when triggered remotely.She'll walk you through her learning process and get you excited about trying new things in your own projects.Finally, she'll show you how to take the first steps to release your work to the open source community.One thing is for sure, Rachel really, really, really likes cats!Where a lot of people try to create things that improve others peoples lives, Rachel tries to do the same, instead, she does this for cats‚Ä¶One Question is constantly on her mind:  ‚ÄúHow can we incorporate cats in technology?‚ÄùEventually, she decided to create a feeder bot for her cats and immediately thought of open sourcing ‚Äúthe thing‚Äù.The talk was mainly a tour of what she‚Äôs learned and encountered along the way.Trying new things is scaryRachel wasn‚Äôt exactly familiar with robotics or backend development, so she would be entering a whole new world.She‚Äôd have to try out new things and start a project without any idea of whether all of this would actually work out.  Embarking on a new project: will it succeed, will it suck?  Using new technologies for the first time: what will happen, will it work for me?  Contributing to Open Source: putting yourself out there is terrifying!Why so scary?Why is this so scary? It turns out the Open Source developer community can sometimes be quite a harsh environment‚Ä¶  Fear of rejection  Imposter Syndrome  Inclusiveness of Communities  Bad behaviour in General: e.g. Oh you didn‚Äôt know about THIS?, e.g. completely ignoring contributions  Your GitHub green timeline is not a representation of what you‚Äôre worth. Just opening a PR just for the sake of it sucks.  Don‚Äôt insult the contributor. Why on earth ‚Ä¶  Vulgar and brutal harassment of the community, seriously, get a life!  PR‚Äôs that get ignored (for over a year) and then the maintainer writes the same fixes and says: Oops!Eventually, Rachel set up a Twitter poll asking people about what bad experiences in Open Source Software development they‚Äôd already encountered, showing off an entire list of Twitter responses. Which weren‚Äôt that positive (euphemism!)One of her Twitter contacts actually created (and open sourced) a tool called echochamber.js, which allows you to include a commenting form in your site that stores the comments only in the local storage.That way, you can be an a**hole and post really offensive comments without actually insulting people.Echochamber.jsProposals for new contributorsKnowing all of these things now, you might wonder if it‚Äôs even worth it putting yourself out there.The answer of course is YES, but consider the following tips when doing so!  Find something you are passionate about  Something new you want to try  Make something cool and open source it yourself  First point of contact is your peers  Constructive criticism!Building a cat feeder botNow, let‚Äôs talk about the actual Cat Feeder bot, which was most suitably named RoboKitty. Check it out at here! and hereIt‚Äôs a node based cat feeder that works over the web.You can use it to instantly feed your cat, or you can feed periodically using cron triggers.After some trial and error on choosing the right combination of hardware, the final list of technologies involved in creating the Cat Feeder Bot looks something like this:  Node.js  Johnny-five: Javascript Robotics &amp; IoT platform.  Particle Photon kit (with breadboard)  4xAA battery pack with on/off switch  Misc hardware accessoriesOther things learned along the way were:  A servo needs external power, so yeah, plugging it in the microcontroller is not enough! :D  No idea how to solder‚Ä¶? Worked out! -&gt; Youtube -&gt; Learn how to solder.Lessons learned  Don‚Äôt be afraid of the unfamiliar  Don‚Äôt be afraid to ask for help  People really like cat stuff  Don‚Äôt downplay your abilities: I mean, it‚Äôs a super cool kitty food dispenser!  I like nodebots a lotNick Hehr: The other side of empathyNick is an Empathetic Community Member, Front-End Dev @NamelyHR ,@hoodiehq Contributor, @tesselproject Contributor and @manhattan_js OrganizerYou can find him on Twitter using the handle @HipsterBrown.In an industry that is so focused frameworks &amp; tooling, we tend to lose sight of the people behind the products and how we work with them.I‚Äôve found empathy to be a powerful resource while collaborating with teams inside companies and across the open source community.By breaking down The Other Side of Empathy, I will demonstrate how applying its principles to your development process will benefit the community and the products they create.EmpathyNick Hehr shares Rachels‚Äô point of view on the sometimes rude Open Source communication and communication on Social media in general.In his talk, he addressed the way you should behave when volunteering to contribute or when giving feedback to contributors in Open Source Software (OSS) projects.And Empathy turns out to be key in this process.RantingIt‚Äôs all too easy to judge or express prejudice these days, through these social media channels and not think about the people who are actually behind the idea or concept you‚Äôre judging.People that decide to Open Source the work on which they‚Äôve spend tons of effort (usually because it‚Äôs their passion, but still‚Ä¶) aren‚Äôt exactly waiting for trolls or rants from people who like this easy judging.Empathy also plays a huge role in the other way around.It happens all too often that people trying to contribute to OSS for the first time are being ignored (by literally ignoring their pull requests for example), being treated like idiots (instead of being given constructive feedback when there is room for improvement), etc‚Ä¶Saying nice things  ‚ÄúIf you don‚Äôt have anything nice* to say, don‚Äôt say anything at all!‚ÄùNice in this context means constructive. Comment on something you think could use improvement and offer a solution.Compliment on certain aspects that really improve the tool.Due to the relative anonymity of social media and other communication channels, we tend to forget these principles.Key take-awaysKey points to take away from this session are:  Give constructive feedback!!!  Always keep in mind the language your using when commenting on Open Source initiatives          Don‚Äôt be to blunt or direct in your reactions.        Use the right channels for your communication          meaning, don‚Äôt ask for feedback on twitter      Instead turn to platforms such as Slack, IRC, Gitter‚Ä¶      Get (constructive) feedback from people you trust        People that open source their tools don‚Äôt owe you anything.          They‚Äôre not entitled to give up all their time for you.      They‚Äôre not here to start fulfilling all requests from a demanding user base. It‚Äôs open source, submit a pull request      Living by these rules will make the (web-)world a little bit of a better place, but won‚Äôt prevent other people from still continuing these bad habits.Don‚Äôt let these people get to you! Continue doing what you‚Äôre passionate about and seek those that will give you that constructive feedback.Afterparty with Beatman and LudmillaAfter a long day, it was time for some party time and since JS Conf Budapest was hosted at a club, this could only be good!We were presented a live set by Breakspoll 2015 winner Beatman and Ludmilla.Day 1: ConclusionDay 1 was packed full of great speakers and the atmosphere was superb! A lot of inspiring talks that gave us a lot of topics to cover for the months to come within the JWorks unit at Ordina Belgium.The after party with Beatman and Ludmilla was a perfect closing of the day. On our walk to the hotel we could only imagine what day 2 would bring.Read our full report on day 2 of JS Conf Budapest here!."
      },
    
      "microservices-2016-05-01-using-jwt-tokens-for-state-transfer-html": {
        "title": "Using JWT for state transfer",
        "url": "/microservices/2016/05/01/Using-JWT-Tokens-for-State-Transfer.html",
        "image": "/img/JWT/jwt-logo.png",
        "date": "01 May 2016",
        "category": "post, blog post, blog",
        "content": "At one of our clients, we have been using Json Web Tokens quite extensively.We even use it to persist state on the client.Why persist state on the client?When building microservices, we need to build so-called ‚Äúcloud native‚Äù applications.One of the key tenets of cloud native application design is keeping your services stateless.The benefit of having stateless applications is foremost the ability to respond to events by adding or removing instances without needing to significantly reconfigure or change the application.More stateless services can easily be added when load suddenly increases, or if an existing stateless service fails, it can simply be replaced with another.Hence, resilience and agility, are easier to achieve with stateless services.Keeping your services stateless means we need to persist our state somewhere else.Since we are transferring state in a REST architectural style, we can use the client to retain our state.For scaling purposes this is a great solution, as the client will only ever have to store its own state, and the server will be relieved of the state of all its clients.At our client we have chosen to use JWT for this state transfer to the client.While JWT is primarily intended for authentication and authorization purposes, the specification allows us to add any data we‚Äôd like to verify later on.Looking goodImagine the following scenario:A list of products is fetched from the ‚Äúproducts microservice‚Äù.The user isn‚Äôt allowed to view all products, so only those products the user has access to are returned.When the user wants to order a product, he sends an order request to the ‚Äúorders microservice‚Äù with the id of the product he wants to order.At that moment the ‚Äúorders microservice‚Äù needs to know whether or not the user is allowed to access this product, let alone order it.Since the rights to access and order are the same, we‚Äôd like to reuse the information returned from the first call to the ‚Äúproducts microservice‚Äù.This flow is illustrated below.We could call the ‚Äúproducts microservice‚Äù from the ‚Äúorders microservice‚Äù and rely on caching, but that would still be an extra network hop and the cache could potentially be invalidated by the time the user orders the product.Using the JWT approach, state is given to the client (the list of product ids the user is allowed to access), and being passed to the server again the moment an order is placed.The signature of the token guarantees us that the state has not been tampered with, while residing on the client.Too good to be trueThis solution prevents the server from having to care about state.It allows the client to store its own state and send it to the server whenever the server requires it - while being guaranteed the data isn‚Äôt tampered with.While this might seem like a good idea, it can backfire quickly.CouplingIn distributed systems such as microservices, it‚Äôs very important to manage the way we talk between components over the network.Using protocols such as HTTP and especially with the REST architectural style, great care needs to go in defining the contracts between these components.While we can use content-negotiation to version our resources, and JSON for instance as content type, we can build our clients as tolerant readers.Headers don‚Äôt have any of these benefits.A header is basically just a key and a value, and in case of JWT, the value is encoded.Therefore it‚Äôs hard to do versioning or any kind of content management on the data transferred inside these tokens.In the aforementioned example, the token couples the ‚Äúproducts microservice‚Äù with the ‚Äúorders microservice‚Äù.If the ‚Äúproducts microservice‚Äù changes the structure of the token, the ‚Äúorders microservice‚Äù will no longer be able to read it.While this coupling would exist as well when the ‚Äúorders microservice‚Äù would call the ‚Äúproducts microservice‚Äù directly, we would manage that coupling as part of the contract between these two microservices.In our case we don‚Äôt know there is a link between the two microservices since they don‚Äôt call each other directly.Yet by transferring the token from one microservice over the client to the other microservice, we are creating a hidden dependency.It‚Äôs also hard to have versioning on headers unless we put the version inside the name of the header.ScalingAdding versions to the headernames, documenting which microservices expect which versions of tokens of other microservices, and making sure we implement the tolerant-reader principle when reading the tokens might be a step in the right direction to avoid mass hysteria when tokens have to be adjusted.But what is simply impossible to get around, is the size restriction of headers in HTTP requests and responses.The HTTP specification doesn‚Äôt put any restriction on header size (singular or combined).But web servers, reverse proxies, CDNs and other network components do.Why they do this is not entirely clear as the spec allows any size, but the fact of the matter is that these restrictions exist.Putting a list of ids in a header like in our products example, will eventually break as the list could get too long.It‚Äôs not even clear how long is too long.AlternativesWe see three possible alternatives to this failed approach to manage state.Instead of passing the state from one microservice over a client to another microservice, we could pass the state as part of the body of the request and response.The downside of this approach is that we can no longer use GET methods for the calls where we need to pass the previously fetched state.The second alternative is to persist the state in a key value datastore on the server.We could asynchronously fetch products data and store it inside a datastore owned by the ‚Äúorders microservice‚Äù.This could get stale, but so could a cache on the ‚Äúproducts microservice‚Äù.This approach seems most common in the industry and could be well be the most preferable.And when all else fails, we can still simply make a call from the ‚Äúorders microservice‚Äù to the ‚Äúproducts microservice‚Äù and count on caching.ConclusionUsing Json Web Tokens as a means to transfer state to and from microservices via the client seemed like a good idea, but in the end turned out to be quite an anti-pattern.It introduces hidden coupling which is hard to manage, and can outright fail completely when headers become too big.Transferring state through the body of requests and responses could be a better approach.Using key value datastores to cache data of other microservices on your own microservice feels like the best way to go."
      },
    
      "angularjs-typescript-2016-04-25-component-based-application-architecture-with-angularjs-and-typescript-html": {
        "title": "Component-based application architecture with AngularJS and Typescript",
        "url": "/angularjs-typescript/2016/04/25/component-based-application-architecture-with-angularjs-and-typescript.html",
        "image": "/img/components-angularjs.jpg",
        "date": "25 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Introduction  Ideally, the whole application should be a tree of components that implement clearly defined inputs and outputs, and minimize two-way data binding. That way, it‚Äôs easier to predict when data changes and what the state of a component is.  ‚Äì AngularJS documentationIn this article I will offer some basic guidelines on how to create a scalable AngularJS application with reusable, well encapsulated components that are easy to maintain and refactor.AngularJS (version 1.5.5 at the time of writing) and its latest features offers us the ability to structure our apps as a tree of components.  Each component contains its own controller and templateIt can even have its own (relative) routing configured if you take advantage of the new Component Router.If you are on a team with multiple front-end developers you can easily divide the work by letting each developer focus on a separate component.It also helps in migrating to Angular 2, though I cannot promise it will be an easy task.Another bonus point is you are getting into the mindset of modern front-end development: web components.My preferred toolchain when developing AngularJS applications consists of Typescript, NPM and Webpack.The sample code in this article and the sample application are created together with these tools.You can find the sample application on Github:https://github.com/ryandegruyter/angularjs-componentsWhat is a component?  In AngularJS a component is a directive.More specifically we call it a component directive or template directive.It is an approach to writing your own custom HTML elements which browsers are able to read and render.HTML comes with a set of pre-defined elements, for example the &lt;div&gt;&lt;/div&gt; element or the &lt;span&gt;&lt;/span&gt; element.By combing and nesting these standard HTML tags we can build complex UI widgets.We can change their appearance and behavior dynamically with JavaScript and CSS.True web components can isolate their structure, appearance and behavior.They make use of a technology called the Shadow DOM, which isolates the component in a separate DOM tree.This element will have its styles and scripts encapsulated, they will not conflict with the styles and scripts inside the parent DOM.Angular 2 takes full advantage of this technology, but unfortunately AngularJS, the framework I will be talking about in this article, does not.  Components you write and register inside AngularJS do not get isolated into a separate DOM tree.Lucky for us we are able to mimic the effect of Web components by using directives.We can write a reusable UI element, declare it with a custom tag and configure it by supplying attributes on the element.My advice is to be sure to use correct naming conventions and a module system so styles and scripts will not conflict with each other.Directives and componentsTo create and register a custom element in AngularJS, we can use either methods:  .directive (name, factoryFunction)  .component (name, object)While components are restricted to custom elements, directives can be used to create both elements as well as custom attributes.There are 3 types of directives:  Component directive  Attribute directive  Structural directiveA Component directive is a directive with a template.The .component() method is a helper method which creates a directive set with default properties.An Attribute directive is declared as an element attribute and they can change the appearance or behavior of an element (ng-change, ng-click, ‚Ä¶).A Structural directive is an attribute or element that manipulates the DOM by adding or removing DOM elements (ng-if, ng-repeat, ‚Ä¶).When do we use .directive(), and when do we use .component()?Custom UI elements should be created with the .component() helper method because it:  enforces best practices and provides optimizations (isolate scope, bindings)  has handy defaults making it easy to create components  makes migration to Angular 2 easier  can take advantage of the new component router which will be the default router in Angular 2  Use the .directive() method when you want to manipulate the DOM by adding or removing elements (Structural directive) or when you want to change the appearance or behavior of an element (Attribute directive).Creating a component-based AngularJS applicationBeginning with a component-based application architecture we need to have a root component.Before creating a component you have to decide if it will be a  Presentational component or a Container component.Presentational componentAlso known as a dumb component.They are used to visualize data and can easily be reused.They don‚Äôt manipulate application state nor do they fetch any data.Instead they define a public API in which they can receive inputs (&lt; and @ bindings), and communicate any outputs (&amp; binding) with their direct parent.  Designers can easily work on Presentational components because they don‚Äôt interfere with application logic.These components are unaware of any application state, and they only get data passed down to them.A simple presentational root component.\texport class RootComponent implements IComponentOptions {\t\tstatic NAME:string = 'app';\t\tbindings:any = {\t\t    title: '@',\t\t};  \t\ttemplate:string = `\t    \t&lt;h1&gt;&lt;/h1&gt;\t    \t&lt;currency-converter&gt;&lt;/currency-converter&gt;\t  \t`;\t}        angular                .module('currencyConverterApp, []')                .component(Rootcomponent.NAME, new RootComponent());This component has a very simple API with one input -  title - and zero outputs.It doesn‚Äôt call a service or fetch any data.It doesn‚Äôt update any outside resources or make any requests to manipulate application state.Also notice how easy it was to register this component directive.Let‚Äôs create the same component directive but register it with the .directive() method.    export class RootComponent implements IDirective {        static NAME:string = 'app';        restrict:string = 'E',        bindToController:any = {            title: '@',        },        scope:IScope = {},        controller:Function = ()=&gt;{},        controllerAs:string = '$ctrl',        template:string = `            &lt;h1&gt;&lt;/h1&gt;            &lt;currency-converter&gt;&lt;/currency-converter&gt;         `;        static instance():IDirective {            return new RootComponent();        }    }    angular    .module('currencyConverterApp, []')    .directive(Rootcomponent.NAME, RootComponent.instance());As you can see, this has a lot more configuration compared to using the .component() helper method.Although it offers more power and flexibility, its more practical to have the .component() method when creating custom UI elements.  bindings are automatically bound to the controller  controllerAs defaults to $ctrl  always creates an isolate scopeContainer componentsAlso known as smart components.This type of component is more tightly coupled to the application and not intended for reusability.It fetches data, manages part of the application state and provides the data to its child components.The child component communicates any update on the data through its output bindings (&amp;).The container component eventually decides what action to take with the data, not the child component.Let‚Äôs look at an example of a container component, I will leave out the complete template for brevity‚Äôs sake, you can view the complete code in the companion repository.First we start with our component definition:    export class CurrencyConverter implements IComponentOptions{        static NAME:string = 'currencyConverter';        template:string = `            ...            &lt;currencies-select                title=\"From\"                on-selected=\"$ctrl.fromSelected(selectedCurrency)\"                show-values-as-rates=\"true\"                currencies=\"$ctrl.fromCurrencies\"            &gt;&lt;/currencies-select&gt;            &lt;currencies-select                title=\"To\"                on-selected=\"$ctrl.toSelected(selectedCurrency)\"                show-values-as-rates=\"true\"                currencies=\"$ctrl.toCurrencies\"            &gt;&lt;/currencies-select&gt;            ...\t\t        `;        controller:Function = CurrencyConverterComponentController;        }The template contains two declarations of a presentational component &lt;currencies-select&gt;.When we look at the attributes of the currencies-select element, the component API consists of three inputs (title, show-values-as-rates and currencies) and one output (on-selected).Our container component can bind a callback method on the on-selected attribute which offers an opportunity for the currencies-select component to communicate with its parent component.Below we define our components controller, here we can set and manipulate our template‚Äôs view model.export class CurrencyConverterComponentController {    selectedFromCurrency:Currency;    selectedToCurrency:Currency;    amount:number;    result:number;    fromCurrencies:Currency[];    toCurrencies:Currency[];    static $inject = [CurrenciesDataService.NAME];        constructor(private currencyDataService:CurrenciesDataService) {    }    $onInit():void {        this.fromCurrencies = this.toCurrencies = this.currencyDataService.getCurrenciesByYear(2016);    }    convert(from:number, to:number):void {        this.result = (this.amount / from) * to;    }    fromSelected(currency:Currency):void {        if(this.selectedToCurrency){            this.convert(currency.rate, this.selectedToCurrency.rate);        }        this.selectedFromCurrency = currency;    }    toSelected(currency:Currency):void {        if(this.selectedFromCurrency){            this.convert(this.selectedFromCurrency.rate, currency.rate);        }        this.selectedToCurrency = currency;    }}This component injects a data service to fetch a list of currencies.We pass this list to each &lt;currencies-select&gt; element in the $onInit method.The $onInit is a component lifecycle method that gets called by the framework each time the component gets instantiated.In this method we set our view model properties _fromCurrencies_ and _toCurrencies_ equal to a list of currencies fetched from the data service.The fromSelected and toSelected methods are passed down as callbacks for the &lt;currencies-select on-selected&gt; output.So how does our presentational component definition look like?export class CurrencySelectComponent implements IComponentOptions {    static NAME:string = 'currenciesSelect';    bindings:any = {        title: '@',        currencies: '&lt;',        onSelected: '&amp;',        showSelected: '&lt;',        showValuesAsRates: '&lt;'    };    controller:Function = CurrencySelectComponentController;    template:string = `...`;}export class CurrencySelectComponentController {    public title:string;    public currencies:Currency[];    public onSelected:Function;    public showSelected:boolean;    public showValuesAsRates:boolean;    public selected:Currency;    constructor() {    }    onCurrencyClick(currency:Currency) {        this.selected = currency;        this.onSelected({selectedCurrency: currency});    }}Bindings define the components API, in the above case there are four bindings.Our previous example declared this component but we only noticed three inputs and one output.Apparently there is a fourth input called _showSelected_.We can guess that it‚Äôs a flag for showing the selected currency.But as a new developer, we cannot be sure.  This is one of the reasons why it is important to document your components API.It will save new developers and designers a lot of time figuring out how to correctly use your component.Your component will become more transparent and not just an abstract definition.As you can see this component does not inject any data services or manage any outside state.It only receives data through its input bindings:  @ stands for one way string binding  &lt; stands for one way any other primitive/type bindingThe output binding public onSelected:Function; gets called each time the onCurrencyClick method gets called, it passes the selected currency which gets communicated back to the parent component.Make sure the parameter object key matches the parameter name in the parent component‚Äôs viewmodel, or the component will not be able to communicate any data.in this case selectedCurrency:onCurrencyClick(currency:Currency) {  this.selected = currency;  this.onSelected({selectedCurrency: currency});}And inside the parent component‚Äôs template:&lt;currencies-select on-selected=\"$ctrl.toSelected(selectedCurrency)\" ...Another way of accessing selectedCurrency, is to use $locals.This is useful when you want to send multiple types of data back.The advantage is you don‚Äôt have to specify each parameter separately in the component‚Äôs template.The disadvantage is $locals is not descriptive.&lt;currencies-select\ton-selected=\"$ctrl.toSelected($locals)\"...To access the selectedCurrency you would use the property on the $locals object with the same name:toSelected($locals:any):void{\tvar selected:Currency = $locals.selectedCurrency ...}Component communicationOutput bindingIn our previous example we saw an example of child to parent communication by mapping an output binding:binding:any = {\tonSelected: '&amp;'}The parent component can pass a method to this binding which the child component can call back and optionally send back any data to.Mapping the require propertyA child component can also require its parent components controller by mapping it in the require property:require:any = {\tparentCtrl: '^parentComponentName'}The ^ symbol is important here.You should replace parentComponentName with the correct component name, you are free to choose a different name for the key, in this case parentCtrl.The parent controller will get bound on to the property parentCtrl.Be aware that this creates a tight coupling between the child and parent component.Using a serviceWe should access and manipulate application state in our container components, but only through services, a component‚Äôs controller primary responsibility is to manage the template‚Äôs view model.You can implement a custom observer pattern inside the service, or use the rootscope as an eventbus.export class SampleService{    static SERVICE_NAME:string = \"mysampleservice\";    static EVENT_NAME:string = \"sampleEvent\";    static $inject = ['$rootScope'];    constructor(private $rootScope:IRootscopeService){}    subscribe(scope:IScope, callback:Function):void {        var handler = this.$rootScope.$on(SampleService.EVENT_NAME, callback);        scope.$on('$destroy', handler);    },    notify():void {        this.$rootScope.$emit(SampleService.EVENT_NAME);    }}A component controller can get notified by any changes by subscribing to the service:    export class MyComponentController{\t\t        static $inject = ['$scope', SampleService.SERVICE_NAME];        constructor(private isolatescope:IScope, private sampleService:SampleService){}        $onInit():void{            this.sampleService.subscribe(this.isolateScope, ()=&gt;{                ...            });         }    }SummaryStart with a root component and work your way down building components that are composed of either presentational and container components.Data should flow down in one direction (&lt; and @ input bindings), and events should propagate back up (&amp; output binding).  Services manage application state  Controllers manage a templates‚Äô view model  Application state is accessed only by container components through services.  Use .component() when writing custom HTML elements in AngularJS  Use .directive() when you need to manipulate the DOM or need to change the appearance or behavior of a DOM element  Minimize 2 way binding (ngModel and = binding)  Presentational components can contain both container components and presentational components and vice versa  Use the component router, which makes it easy to bind URL paths to components. A component can contain its own relative routes too  Document your component‚Äôs API so new developers and designers know how to use it correctly  Keep your controllers clean, their main purpose is to set and manipulate the templates‚Äô view model. Delegate business logic to services"
      },
    
      "microservices-2016-04-22-lagom-first-impressions-and-initial-comparison-to-spring-cloud-html": {
        "title": "Lagom: First Impressions and Initial Comparison to Spring Cloud",
        "url": "/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html",
        "image": "/img/lagom.png",
        "date": "22 Apr 2016",
        "category": "post, blog post, blog",
        "content": "  ‚ÄúIt‚Äôs open source. It‚Äôs highly opinionated.   Build greenfield microservices and decompose your Java EE monolith like a boss.‚Äù - LightbendTable of Contents  Just the right amount  Design philosophy  Building blocks  Getting started with Lagom  Anatomy of a Lagom project  Example of a microservice  CQRS and Event Sourcing  Lightbend Q&amp;A at the CodeStar launch event  Comparison with Spring  Our advice  Conclusion  Useful linksJust the right amountMeet Lagom, Lightbend‚Äôs (formerly Typesafe) new open source framework for architecting microservices in Java.On the 10th of March, Lightbend released the first MVP version of Lagom which is the current version at the time of writing.Although there is currently only a Java API, Scala enthusiasts should not fret because a Scala API is a main priority and well on its way.Lagom is a Swedish word meaning ‚Äújust the right amount‚Äù.Microservices have often been categorised as small services.However, Lightbend wants to emphasize that finding the right boundaries between services, aligning them with bounded contexts, business capabilities, and isolation requirements are the most important aspects when architecting a microservice-based system.Therefore, it fits very well in a Domain-Driven Design focused mindset.Following this will help in building a scalable and resilient system that is easy to deploy and manage.According to Lightbend the focus should not be on how small the services are, but instead they should be just the right size, ‚ÄúLagom‚Äù size services.Lagom, being an opinionated framework, provides a ‚Äúgolden path‚Äù from which the developer can deviate if necessary.Being based on the reactive principles as defined in the Reactive Manifesto, Lagom provides the developer a guard-railed approach with good defaults while also allowing to deviate if necessary.This blogpost will cover our initial impression on the framework together with our opinion on the choices made while architecting the framework.Note that we won‚Äôt go too deep into detail in all the different aspects of the framework, for more details refer to Lagom‚Äôs extensive documentation.As Lightbend is entering the microservices market with Lagom, we feel obliged to make a fair comparison with existing frameworks out there.In the Java world this is predominantly the Spring stack with Spring Boot and Spring Cloud, standing on the shoulders of giants such as the Netflix OSS.In this current stage, it would be a bit too early to make an in-depth comparison between the two, seeing as you would be comparing a mature project to an MVP.What we can share though, are our initial observations.Design philosophyLagom‚Äôs design rests on the following principles:  Message-Driven and Asynchronous: Built upon Akka Stream for asynchronous streaming and the JDK8 CompletionStage API.Streaming is a first-class concept.  Distributed persistence: Lagom favours distributed persistence patterns using Event Sourcing with Command Query Responsibility Segregation (CQRS).  Developer productivity: Starting all microservices with a single command, code hot reloading and expressive service interface declarations are some examples of Lagom‚Äôs high emphasis on developer productivity.Building blocksThe Lagom framework acts as an abstraction layer upon several Lightbend frameworks and consists of the following core technologies and frameworks:  Scala  Java  Play Framework  Akka and Akka Persistence  sbt  Cassandra  Guice  ConductRSeeing as it acts as an abstraction layer the developer doesn‚Äôt need to hold any knowledge of Play Framework and Akka in order to successfully use Lagom.Sbt has been chosen as the build tool because it also acts as a development environment.Lagom relies heavily on the following sbt features:  Fine-grained tasks  Each task may return a value  The value returned by a task may be consumed by other tasksAccording to Lightbend, Scala‚Äôs build tool ‚Äòsbt‚Äô offers many handy features to Lagom such as fast incremental recompilation, hot code reloading, starting and stopping services in parallel and automatic injection of configuration defaults.Sbt might be seen as a hurdle by most Java developers since it is Maven and Gradle (and to a lesser extent Ant) that rule most Java projects.Moving towards a microservices framework such as Lagom would already constitute quite a transition so we think that this might hold back Java developers from adopting the framework.Lightbend‚Äôs rebranding could be interpreted as a move away from a Scala-oriented company towards a more Java-minded company.In that regard it would make sense to lower the initial learning curve especially for a rather trivial component such as a building tool.After all, the most important thing to achieve adoption is allowing people to easily get started with the new technology. We think that providing integration for Maven or Gradle would have a positive effect on the adoption rate and although it may not be trivial to implement, it should help convince Java developers to give Lagom a go.Google‚Äôs Guice has been chosen for dependency injection since it is a lightweight framework.What is remarkable is that Guice is used as well for intermicroservices calls.Lagom acts as a communication abstraction layer and it does so by adding a dependency on the interfaces of remote microservices.Just like a shared domain model and shared datastores being antipatterns for microservices, having code dependencies from one service in another is as well.Changing the code of one microservice should not have an immediate cascading effect on other microservices.This is the very essence of the microservices architecture.In a monolith, having code changes in one component can result in immediate breaking changes in other components of the system.While this may be desired in order to keep technical debt low, this is an inherent characteristic of monolithic systems.One of the reasons microservices were introduced, is to decouple components on all levels, especially binary coupling.Using protocols between components instead of actual binary dependencies allows us to implement the tolerant reader principle and versioning through for instance content negotiation.Lightbend argues that sharing interfaces as code will increase productivity and performance, but we fear the result of this is a distributed monolith instead of an actual decoupled microservices architecture.While we question the default way of communicating between microservices in Lagom, we are enthusiastic that more ways of making intermicroservices calls are becoming available.Using HTTP is possible as well, and one of the upcoming features is a Lagom Service Client.The Guice approach might also be quite favorable for people migrating from monolithic applications to microservices.In the end it is a trade-off, but one that shouldn‚Äôt be taken lightly.As a default persistence solution, Apache Cassandra is used due to how well it integrates with CQRS and Event Sourcing.Lagom has support for Cassandra as datastore, both for the reading and writing data.It is possible to use other datastore solutions but this comes at the cost of not being able to take advantage of the persistence module in Lagom.ConductR is an orchestration tool for managing Lightbend Reactive Platform applications across a cluster of machines and is Lightbend‚Äôs solution for running Lagom systems in production.Note that ConductR comes with a license fee and is majorly targeted at enterprises.The other option we currently have in order to run our Lagom system in production is to write our own service locator compatible with Lagom.At the time of writing someone already started working on Kubernetes support and we are sure that, given more time, more options will become available.For now though, Lagom is still in an early stage where we either have to pay for the ConductR license, build our own service locator, or wait until someone does the work for us.Getting started with LagomIn order to start using Lagom, Activator must be correctly set up.Currently two Lagom templates exist that can be used for creating a new Lagom application.The Lagom Java Seed template should be the template of choice, the Lagom Java Chirper template is an example of a Twitter-like app created in Lagom.Creating a new Lagom application is as simple as using the following command:$ activator new my-first-system lagom-javaAfterwards the project can be imported in any of the prominent IDEs as an sbt project.In order to boot the system, we first need to navigate to the root of the project and start the Activator console:$ activatorAfter which we can start all our services using a single simple command:$ runAll&gt; runAll[info] Starting embedded Cassandra server.......[info] Cassandra server running at 127.0.0.1:4000[info] Service locator is running at http://localhost:8000[info] Service gateway is running at http://localhost:9000[info] application - Signalled start to ConductR[info] application - Signalled start to ConductR[info] application - Signalled start to ConductR[info] Service helloworld-impl listening for HTTP on 0:0:0:0:0:0:0:0:24266[info] Service hellostream-impl listening for HTTP on 0:0:0:0:0:0:0:0:26230[info] (Services started, use Ctrl+D to stop and go back to the console...)This command starts a Cassandra server, service locator and service gateway.Each of our microservices is started in parallel while also registering them in the service locator.Additionally, a run command to individually start services is available as well.Note that the ports are assigned to each microservice by an algorithm and are consistent even on different machines.The possibility to assign a specific port is available though.Similar to Play Framework, Lagom also supports code hot reloading allowing you to make changes in the code and immediately seeing these changes live without having to restart anything.A feature we‚Äôre very fond of.In general, a restart is only required when adding a new microservice API and implementation module in the project.Anatomy of a Lagom projecthelloworld-api           ‚Üí Microservice API submodule ‚îî src/main/java         ‚Üí Java source code interfaces with model objectshelloworld-impl          ‚Üí Microservice implementation submodule ‚îî logs                  ‚Üí Logs of the microservice ‚îî src/main/java         ‚Üí Java source code implementation of the API submodule ‚îî src/main/resources    ‚Üí Contains the microservice application config ‚îî src/test/java         ‚Üí Java source code unit testslogs                     ‚Üí Logs of the Lagom systemproject                  ‚Üí Sbt configuration files ‚îî build.properties      ‚Üí Marker for sbt project ‚îî plugins.sbt           ‚Üí Sbt plugins including the declaration for Lagom itself.gitignore               ‚Üí Git ignore filebuild.sbt                ‚Üí Application build scriptExample of a microserviceIn order to write a new microservice you create a new API and implementation project.In the API project you define the interface of your microservice:HelloService.javapublic interface HelloService extends Service {  ServiceCall&lt;String, NotUsed, String&gt; hello();    ServiceCall&lt;String, GreetingMessage, String&gt; useGreeting();  @Override  default Descriptor descriptor() {    return named(\"helloservice\").with(        restCall(Method.GET,  \"/api/hello/:id\",       hello()),        restCall(Method.POST, \"/api/hello/:id\",       useGreeting())      ).withAutoAcl(true);  }}A Descriptor defines the service name and the endpoints offered by a service. In our case we define two REST endpoints, a GET and a POST.GreetingMessage is basically an immutable class with a single String message instance variable.On the subject of immutability the Lagom documentation mentions Immutables, a Java library that helps you create immutable objects via annotation processing.Definitely worth a look seeing as it helps you get rid of boilerplate code.In the implementation submodule we implement our API‚Äôs interface.HelloServiceImpl.javapublic class HelloServiceImpl implements HelloService {  @Override  public ServiceCall&lt;String, NotUsed, String&gt; hello() {    return (id, request) -&gt; {      CompletableFuture.completedFuture(\"Hello, \" + id);    };  }  @Override  public ServiceCall&lt;String, GreetingMessage, String&gt; useGreeting() {    return (id, request) -&gt; {      CompletableFuture.completedFuture(request.message + id);    };  }}You‚Äôll immediately notice that the service calls are non-blocking by default using CompletableFutures introduced in JDK8.Interesting to know is that Lagom also provides support for the Publish-subscribe pattern out of the box.We also need to implement the module that binds the HelloService so that it can be served.HelloServiceModule.javapublic class HelloServiceModule extends AbstractModule implements ServiceGuiceSupport {  @Override  protected void configure() {    bindServices(serviceBinding(HelloService.class, HelloServiceImpl.class));  }}We define our module in the application.config:play.modules.enabled += sample.helloworld.impl.HelloServiceModuleAnd finally register our microservice in build.sbt with its dependencies and settings:lazy val helloworldApi = project(\"helloworld-api\")  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies += lagomJavadslApi  )lazy val helloworldImpl = project(\"helloworld-impl\")  .enablePlugins(LagomJava)  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies ++= Seq(      lagomJavadslPersistence,      lagomJavadslTestKit    )  )  .settings(lagomForkedTestSettings: _*)  .dependsOn(helloworldApi)We can then test our endpoint:$ curl localhost:24266/api/hello/WorldHello, World!$ curl -H \"Content-Type: application/json\" -X POST -d '{\"message\": \"Hello \"}' http://localhost:24266/api/hello/WorldHello WorldSeeing as any good developer writes unit tests for his/her code so should we!public class HelloServiceTest {  private static ServiceTest.TestServer server;  @BeforeClass  public static void setUp() {    server = ServiceTest.startServer(ServiceTest.defaultSetup());  }  @AfterClass  public static void tearDown() {    if (server != null) {      server.stop();      server = null;    }  }  @Test  public void shouldRespondHello() throws Exception {    // given    HelloService service = server.client(HelloService.class);    // when    String hello = service.hello().invoke(\"Yannick\", NotUsed.getInstance()).toCompletableFuture().get(5, SECONDS);    // then    assertEquals(\"Hello, Yannick\", hello);  }  @Test  public void shouldRespondGreeting() throws Exception {    // given    HelloService service = server.client(HelloService.class);    // when    String greeting = service.useGreeting().invoke(\"Yannick\", new GreetingMessage(\"Hi there, \")).toCompletableFuture().get(5, SECONDS);    // then    assertEquals(\"Hi there, Yannick\", greeting);  }}Tests can be executed in Activator via the following command:$ test&gt; test[info] Test run started[info] Test sample.helloworld.impl.HelloServiceTest.testHello started[info] Test sample.helloworld.impl.HelloServiceTest.testGreeting started[info] Test run finished: 0 failed, 0 ignored, 2 total, 16.759s[info] Passed: Total 2, Failed 0, Errors 0, Passed 2[success] Total time: 21 s, completed Apr 14, 2016 10:06:41 AMCQRS and Event SourcingBeing an opinionated framework Lagom suggests to use CQRS and Event Sourcing seeing as it fits well within the reactive paradigm.In this blogpost we are not going to explain CQRS and Event Sourcing in detail seeing as it is very well documented in the documentation of Lagom.The gist of it is that each service should own its own data and only the service itself should have direct access to the database.Other services need to use the service‚Äôs API in order to interact with its data.Sharing the database across different services would result in tight coupling.Ideally we want to work with Bounded Contexts following the core principles of Domain-Driven Design where each service defines a Bounded Context.Using Event Sourcing gives us many advantages such as not only storing the current state of data but having an entire journal of events that tell us how the data achieved its current state.With event sourcing we only perform reads and writes, there are no updates nor deletes.All this makes it easy to test and debug and allows us to easily reproduce scenarios that happened in production by replaying the event log from that environment.Note that just because Lagom encourages us to use CQRS and Event Sourcing it isn‚Äôt forcing us to use it as it is not always applicable to every use case.It is perfectly possible to, for example, plug in a PostgreSQL database for our persistence layer.Someone has already set up PostgreSQL integration using Revenj persistence.However, Lightbend suggests that for best scalability preference must be given to asynchronous APIs because using blocking APIs like JDBC and JPA will have an impact on that.By default, when launching our development environment, a Cassandra server will be booted without having to do any setup ourselves besides adding the lagomJavadslPersistence dependency to our implementation in our build.sbt.Regarding the code, a persistent entity needs to be defined, combined with a related command, event and state.Note that the following code samples are mainly here to give an idea of the work needed for implementing all this.For more information and a detailed explanation, consult the excellent documentation on the subject.In the persistent entity we define the behaviour of our entity.In order to interact with event sourced entities, commands need to be sent.We therefore need to specify a command handler for each command class that the entity can receive.Commands are then translated into events which will get persisted by the entity.Each event has its own event handler registered.Example of a PersistentEntity:HelloWorld.javapublic class HelloWorld extends PersistentEntity&lt;HelloCommand, HelloEvent, WorldState&gt; {  @Override  public Behavior initialBehavior(Optional&lt;WorldState&gt; snapshotState) {    BehaviorBuilder b = newBehaviorBuilder(        snapshotState.orElse(new WorldState(\"Hello\", LocalDateTime.now().toString())));    b.setCommandHandler(UseGreetingMessage.class, (cmd, ctx) -&gt;      ctx.thenPersist(new GreetingMessageChanged(cmd.message),        evt -&gt; ctx.reply(Done.getInstance())));    b.setEventHandler(GreetingMessageChanged.class,        evt -&gt; new WorldState(evt.message, LocalDateTime.now().toString()));    b.setReadOnlyCommandHandler(Hello.class,        (cmd, ctx) -&gt; ctx.reply(state().message + \", \" + cmd.name + \"!\"));    return b.build();  }}Our PersistentEntity requires a state to be defined:WorldState.java@Immutable@JsonDeserializepublic final class WorldState implements CompressedJsonable {  public final String message;  public final String timestamp;  @JsonCreator  public WorldState(String message, String timestamp) {    this.message = Preconditions.checkNotNull(message, \"message\");    this.timestamp = Preconditions.checkNotNull(timestamp, \"timestamp\");  }  @Override  public boolean equals(@Nullable Object another) {    if (this == another)      return true;    return another instanceof WorldState &amp;&amp; equalTo((WorldState) another);  }  private boolean equalTo(WorldState another) {    return message.equals(another.message) &amp;&amp; timestamp.equals(another.timestamp);  }  @Override  public int hashCode() {    int h = 31;    h = h * 17 + message.hashCode();    h = h * 17 + timestamp.hashCode();    return h;  }  @Override  public String toString() {    return MoreObjects.toStringHelper(\"WorldState\").add(\"message\", message).add(\"timestamp\", timestamp).toString();  }In our command interface we define all the commands that our entity supports.In order to get a complete picture of the commands an entity supports, it is the convention to specify all supported commands as inner classes of the interface.HelloCommand.javapublic interface HelloCommand extends Jsonable {  @Immutable  @JsonDeserialize  public final class UseGreetingMessage implements HelloCommand, CompressedJsonable, PersistentEntity.ReplyType&lt;Done&gt; {    public final String message;    @JsonCreator    public UseGreetingMessage(String message) {      this.message = Preconditions.checkNotNull(message, \"message\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof UseGreetingMessage &amp;&amp; equalTo((UseGreetingMessage) another);    }    private boolean equalTo(UseGreetingMessage another) {      return message.equals(another.message);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + message.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"UseGreetingMessage\").add(\"message\", message).toString();    }  }  @Immutable  @JsonDeserialize  public final class Hello implements HelloCommand, PersistentEntity.ReplyType&lt;String&gt; {    public final String name;    public final Optional&lt;String&gt; organization;    @JsonCreator    public Hello(String name, Optional&lt;String&gt; organization) {      this.name = Preconditions.checkNotNull(name, \"name\");      this.organization = Preconditions.checkNotNull(organization, \"organization\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof Hello &amp;&amp; equalTo((Hello) another);    }    private boolean equalTo(Hello another) {      return name.equals(another.name) &amp;&amp; organization.equals(another.organization);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + name.hashCode();      h = h * 17 + organization.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"Hello\").add(\"name\", name).add(\"organization\", organization).toString();    }  }}And finally we want to define all events that the entity supports in an event interface.It follows the same convention as with commands, specifying all events as inner classes of the interface.HelloEvent.javapublic interface HelloEvent extends Jsonable {  @Immutable  @JsonDeserialize  public final class GreetingMessageChanged implements HelloEvent {    public final String message;    @JsonCreator    public GreetingMessageChanged(String message) {      this.message = Preconditions.checkNotNull(message, \"message\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof GreetingMessageChanged &amp;&amp; equalTo((GreetingMessageChanged) another);    }    private boolean equalTo(GreetingMessageChanged another) {      return message.equals(another.message);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + message.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"GreetingMessageChanged\").add(\"message\", message).toString();    }  }}The HelloServiceImpl.java class will look like the following:public class HelloServiceImpl implements HelloService {  private final PersistentEntityRegistry persistentEntityRegistry;  @Inject  public HelloServiceImpl(PersistentEntityRegistry persistentEntityRegistry) {    this.persistentEntityRegistry = persistentEntityRegistry;    persistentEntityRegistry.register(HelloWorld.class);  }  @Override  public ServiceCall&lt;String, NotUsed, String&gt; hello() {    return (id, request) -&gt; {      PersistentEntityRef&lt;HelloCommand&gt; ref = persistentEntityRegistry.refFor(HelloWorld.class, id);      return ref.ask(new Hello(id, Optional.empty()));    };  }  @Override  public ServiceCall&lt;String, GreetingMessage, Done&gt; useGreeting() {    return (id, request) -&gt; {       PersistentEntityRef&lt;HelloCommand&gt; ref = persistentEntityRegistry.refFor(HelloWorld.class, id);       return ref.ask(new UseGreetingMessage(request.message));    };  }}Lightbend Q&amp;A at the CodeStar launch eventOn the 24th of March we attended the launch event of CodeStar, the new unit from our Dutch Ordina colleagues focused on Full Stack Scala and Big Data solutions.CodeStar also hold a Lightbend partnership.One of the presentations was an introduction to Lagom by Markus Eisele, Developer Advocate at Lightbend.After his talk we had the opportunity to ask Markus and his colleague, Lutz H√ºhnken, Solutions Architect at Lightbend, several questions regarding Lagom.      What do you guys consider to be the major competitor for Lagom?Spring Cloud and Netflix OSS?          Yes, we would consider that stack to be Lagom‚Äôs main competitor. But we believe that with Lagom we have a number of unique features that makes us shine (because otherwise we wouldn‚Äôt have built it):1) Lagom‚Äôs development environment, in my humble opinion a major productivity boost2) Fostering good practices for building reactive services seeing as Lagom is opinionated, e.g. async communication by default, ES/CQRS, ‚Ä¶3) Batteries-included, from development to production4) Streaming is first-class            Lagom suggests that Event Sourcing and CQRS should be used as the default solution for persistence but is it really applicable in the majority of the scenarios?          Lagom is an opinionated framework and will try to suggest using ES &amp; CQRS as the primary solution to use since it fits very well with the reactive mindset.Of course it also depends on the use case.            Don‚Äôt you think you encourage code coupling by having microservices depend on the interface of another microservice?          It is true that the default way to do service calls between Lagom services is to use binary dependencies, though of course it is not enforced. We have taken great care to ensure that service calls map down to idiomatic REST and/or websockets. We do have plans in the future to allow simple removal of the binary coupling. To make service interfaces go through a non-binary specification such as Swagger, where Swagger specs will be generated and interfaces will be generated from the Swagger specs.            Does Lagom support REST level 3? Is there support for hypermedia?          Currently not supported but we are open to it. Feel free to create a suggestions ticket at the GitHub project.            Don‚Äôt you think it is a bad idea to only support ConductR for production deployments?What about pet projects of single developers? This makes it less appealing to motivate people to pick up Lagom compared to for example Spring Cloud and Netflix OSS.          It is in the strategic planning of Lightbend to push ConductR forward as the main solution for your production environment.Do note that it‚Äôs perfectly possible to deploy your Lagom services elsewhere as long as you implement your own service locator (as an example, the integration needed to support Lagom in ConductR is available on GitHub).Looking at our Open Source Position Statement you will notice that one of the differentiators we see between our open source offerings and the commercial products is Time. Open source users tend to invest their time rather than their money. ConductR integration into Lagom could be seen as an example for this. If you would rather spend the money than invest time, buy ConductR. If you would rather invest time instead of money, build your own ServiceLocator implementation and use a different infrastructure.An example of this is the GitHub issue for implementing Kubernetes support.            How do you integrate with other non-Lagom microservices?          Currently you would call them via REST URLs. In the near future the Lagom Service Client could also be used to consume them. Additionally it should also be possible to integrate Eureka in Lagom.            What is the deployment procedure exactly? How do I prepare my Lagom application for deployment into production?          The deployment unit in ConductR is a bundle which is an abstract term that can mean a Docker image or a zip file with a certain structure.By default, when you have multiple services in one project, it will create multiple bundles. You call bundle:dist once on the top level and it will create a separate bundle for each service which can then be deployed to ConductR.You can put multiple components in one bundle so you could have multiple services in one bundle, but we think that it is unusual.Ideally, each service needs to be its own bundle managed in isolation by ConductR, for it to be able to be able to be developed, rolled out, upgraded and failed in isolation.            What about API versioning?          Currently there is no versioning for your services besides the ‚Äúdefault‚Äù way to do it, e.g. via the header or by versioning your urls.            What do you think about the so-called serverless architectures like AWS Lambda or Google Cloud Functions?          We think that those architectures are part of the future. Lagom can be seen as a step in that direction since it decouples the stateless part of the service (the behavior) from the stateful (persistent entity), allowing the stateless part to be scaled out independently, and automatically by the runtime, in a similar fashion to AWS Lambda. A hosted version of Lagom could give a very similar experience.            About sbt, will you also support a more widely adopted tool such as Maven or Gradle?          Lagom relies on some sbt features, so supporting other build tools is not trivial. While it is probably doable to support Maven, we‚Äôd need to do build a proof-of-concept to verify this. This is currently not prioritized. We‚Äôll be watching the community‚Äôs feedback on this.            Does the Lagom circuit breaker have a dashboard such as the Hystrix dashboard? Does Lagom in general have operational dashboards?          You could integrate the circuit breaker data with monitoring tools such as Graphite and Grafana.In addition, with Lightbend Monitoring you do get a suite of tools for monitoring your microservices. Lightbend Monitoring is included in the ConductR license.            Is it true that Typesafe rebranded to Lightbend to get a broader adoption than what was possible with a more Scala-orientated reputation attached to Typesafe?          That is correct.This doesn‚Äôt mean that we are giving up on Scala, it is still core to all of our technologies.      Comparison with SpringSpring has been out there for more than 10 years and with Spring Boot and Spring Cloud a trend has been set to move to self-contained applications as a basis for microservices development.Spring reaps the fruits of the Netflix OSS while offering Spring‚Äôs own components such as Spring Cloud Config and Spring Security as well.The Netflix and Spring stack comes with all the necessary tools to build and run microservices in production.Externalized configuration, out-of-the-box free dashboards for service registries, circuit breaker monitoring and distributed tracing, integration with service registries such as Eureka, Consul and Zookeeper, production-ready monitoring and metrics features with Actuator endpoints, integration with build tools such as Maven and Gradle and extensive security features including upcoming integration with Vault are only a subset of the features Spring has to offer.Seeing as Lagom is still in its early days, it wouldn‚Äôt be fair to Lightbend to make an in-depth comparison with the Spring stack.We hope that Lagom will continue to grow towards a more mature framework and a true alternative to Spring on all levels.The first steps we currently see look promising and we hope that they will consider our remarks for how they want to further evolve the framework.It is great to see more microservices frameworks become available and we applaud Lightbend for taking up the competition with Spring.Our adviceOur advice is to keep track of Lagom‚Äôs progress closely.If you are currently looking for a mature framework with integration capabilities for just about anything, go with Spring.If you want to use Event Sourcing, Lagom should be a great fit. Additionally, Lagom‚Äôs focus on CQRS and its reactive core are truly differentiators with other frameworks.Lagom has great potential and is eager to get community involvement. If you are willing to join forces with Lightbend, Lagom might already be a viable candidate for you.ConclusionWe think that Lagom looks very promising and we will definitely follow it up.Due to Lagom being an opinionated framework everything glues together well.Lagom is just a thin layer on top of Akka and Play, which is very mature and hardened over the years.It might be a bit too early to do an in-depth comparison between Lagom and Spring Cloud since we would be comparing an MVP against a mature technology.We do think that using sbt might be a hurdle for Java developers and it would ease adoption if there would be other ways to use Lagom in production besides ConductR.As it stands right now you would need to write a custom service locator yourself.It would close the gap with Spring if support would already be available for service discovery via for example Eureka or Consul.It is clear that Lagom puts a lot of focus on reactiveness and gaining the best performance. This could come at the cost of binary coupling, seeing as the default way to do service calls between Lagom services is to use binary dependencies. We are looking forward to Lightbend‚Äôs plans to go through non-binary specifications in order to reduce coupling on a binary level as well.Given that it is currently an MVP version we are interested in seeing how Lagom matures. Since it is all new and shiny, you will be able to give back to the community by helping to develop parts of this new and exciting framework yourself.Contributing to the framework is easy via pull requests and are actively reviewed by Lightbend developers.The developers are very active on their Gitter channel and they are quick to answer questions. We are also very excited to the release of the Scala API.Our colleague Andreas Evers, who has extensive knowledge on Spring Cloud and Netflix OSS, will soon be participating in a podcast with Markus Eisele hosted by Lightbend to discuss Lagom and microservices trends.The date should be announced soon.Be sure to follow Andreas and Lightbend to catch it!Useful links  Lagom  Lagom documentation  Lagom Twitter  Lagom GitHub  Lagom Gitter  Lightbend  Lightbend Twitter  Markus Eisele Twitter  Lutz H√ºhnken Twitter"
      },
    
      "microservices-2016-04-09-ordina-becomes-netflix-oss-contributor-html": {
        "title": "Ordina becomes Netflix OSS contributor",
        "url": "/microservices/2016/04/09/Ordina-becomes-Netflix-OSS-contributor.html",
        "image": "/img/netflix.jpg",
        "date": "09 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Netflix has officially added Ordina as active user and contributor to their open source cloud and microservices tools and frameworks.Ordina continues to be a leading force in cloud and microservice architectures in the BeNeLux.Since 2011, Netflix has been releasing more and more components of their cloud platform and utilities as free and open source software. These projects are available to the public through Netflix OSS.Starting around 2009, Netflix completely redefined its application development and operations models. They were driven completely by APIs and riding the initial wave of what we would come to know as microservices. Industry onlookers derided the company with disbelief and uncertainty. While this may work with Netflix, no one else can possibly do this. Fast-forward to 2016, when most of those sentiments changed to commitments of active migration to microservices. The concept is definitely both valid and powerful.Pivotal has embraced these technologies and made them approachable for the masses through the Java-based Spring ecosystem. In the Spring Cloud OSS program, an abstraction layer is added on top of Netflix‚Äôs components to ease adoption outside of Netflix. This allows local companies without silicon valley-grade scientists to embrace microservices and their benefits.Ordina has successfully architected and implemented the Netflix and Spring Cloud stack at clients in Belgium. We continue to do so and are proud we can call ourselves Netflix OSS and Spring Cloud contributors. Netflix has added our logo to their contributors page for our continued adoption and contributions to the Netflix and Spring Cloud ecosystem.Our expertise and experience using and contributing to the Netflix and Spring Cloud stack is second to none in the BeNeLux. We are always looking to help new or existing clients to migrate to microservices and make the step to Cloud Native architectures.If this sounds interesting to your company, or these architectures personally excite you, make sure to contact us directly or take a look at jobs.ordina.be."
      },
    
      "ionic-2016-04-07-ionic-protractor-html": {
        "title": "Protractor testing in Ionic app",
        "url": "/ionic/2016/04/07/Ionic-Protractor.html",
        "image": "/img/ionic-protractor.png",
        "date": "07 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Using Protractor in an ionic appSince a few days I‚Äôve been playing around with Protractorand I am also involved on an internal project in which an Ionic app has to be created.So I thought:  Why not use Protractor in my Ionic app?So here we are.It‚Äôs not hard to get started and I will explain how I got it working.For the full example please refer to the ionic documentation1. Getting startedFirst of all you need Node.js.When Node.js is installed you should install Ionic and Cordova using npm (Node Package Manager):npm install -g cordova ionicI personally needed an app with tabs, but you can also start a blank app or an app with a side menu:ionic start &lt;your app name&gt; tabsYou can also choose blank or sidemenu instead of tabs2. Running the app in the browserOnce the Ionic app has been generated, you can run t it in the browser using the following command:cd &lt;your app name&gt;ionic serve  It‚Äôs worth noting that Ionic has live reload built in by default.So any changes will be immediatelyreflected in the browser.To view the application using the iOS and Android styling applied you can use the following command: ionic serve -lab There are many more awesome things you can do with the Ionic CLI.If you want to know more about the CLI you can find it in the Ionic documentation.3. Structuring the applicationAt this moment you are set up with an Ionic starter app.The first thing I did was refactor the code from technical to functional modules.                    --&gt;                  I strongly advise to use functional modules, it‚Äôs easier to work with.Related code should be in one folder and when testing you can use the same structure to test each module separately while coding.  You‚Äôll find yourself navigating less trough your open tabs or a tree-view.Additionally, it makes your code more comprehensible to other developers.The style guide from John Papa on how to structure AngularJS applications is a very good resource.4. Sign-in pageThe one we‚Äôll be testingAfter refactoring, I implemented a sign-in page, which has no access to the tabs.The code can be seen below.If you work in functional modules like I do, it is as easy as referring to the controller and the service from index.html, then pass starter.sign-in as a module to your application.sign-in/sign-in-controller.js:(function () {    angular        .module('starter.sign-in',[])        .controller('SignInCtrl',SignInCtrl);    SignInCtrl.$inject = ['$scope','$state','SignInService'];    function SignInCtrl ($scope,$state,SignInService) {        $scope.user = {};        $scope.signIn = function() {            SignInService.signIn($scope.user)                .then(function(data){                    if(data){                        $state.go('tab.rooms');                    }else{                        $scope.incorrect = true;                    }                })        };    }})();sign-in/signin.html:&lt;ion-view view-title=\"Sign in\"&gt;    &lt;ion-content&gt;        &lt;form name=\"frmLogin\" novalidate ng-cloak&gt;            &lt;ion-list&gt;                &lt;ion-item class=\"item item-input item-floating-label\"&gt;                    &lt;label&gt;                        &lt;span class=\"input-label\"&gt;Username&lt;/span&gt;                        &lt;input type=\"email\" ng-model=\"user.username\" ng-required=\"true\" placeholder=\"Username\" id=\"username\" name=\"username\"&gt;                    &lt;/label&gt;                    &lt;div class=\"assertive\" ng-if=\"frmLogin.$submitted || frmLogin.username.$touched\"&gt;                        &lt;div ng-if=\"frmLogin.username.$error.required\"&gt;Username is required&lt;/div&gt;                        &lt;div ng-if=\"frmLogin.username.$error.email\"&gt;Username is not valid&lt;/div&gt;                    &lt;/div&gt;                &lt;/ion-item&gt;                &lt;ion-item class=\"item item-input item-floating-label\"&gt;                    &lt;label&gt;                        &lt;span class=\"input-label\"&gt;Password&lt;/span&gt;                        &lt;input type=\"password\" ng-model=\"user.password\" ng-minlength=\"4\" ng-required=\"true\" placeholder=\"Password\" id=\"password\" name=\"password\"&gt;                    &lt;/label&gt;                    &lt;div class=\"assertive\" ng-if=\"frmLogin.$submitted || frmLogin.password.$touched\"&gt;                        &lt;div ng-show=\"frmLogin.password.$error.required\"&gt;Password is required&lt;/div&gt;                    &lt;/div&gt;                &lt;/ion-item&gt;                &lt;div class=\"padding\"&gt;                    &lt;div class=\"assertive\" ng-if=\"incorrect\"&gt;                        &lt;div&gt;Username or password is incorrect.&lt;/div&gt;                    &lt;/div&gt;                    &lt;button id=\"btnSignIn\" ng-disabled=\"frmLogin.$invalid\" class=\"button button-full button-positive\" ng-click=\"signIn()\"&gt;                        Sign in                    &lt;/button&gt;                &lt;/div&gt;            &lt;/ion-list&gt;        &lt;/form&gt;    &lt;/ion-content&gt;&lt;/ion-view&gt;sign-in/sign-in.service.js:(function() {    angular        .module('starter.sign-in')        .factory('SignInService', SignInService);    SignInService.$inject = ['$timeout'];    function SignInService ($timeout) {        var _user = {            email: 'yannick@gmail.com',            pass: '1234'        };        function signIn (user) {            return $timeout(function() {                return true;                //return !!(user.username === _user.email &amp;&amp; user.password == _user.pass);            },2000);        }        return {            signIn : signIn        };    }})();Next you need to provide a state, so add the following in app.js:state('signin',{    url:'/signin',    templateUrl: 'sign-in/sign-in.html',    controller: 'SignCtrl')}and change redirect to /signin by default:$urlRouterProvider.otherwise('/signin');For complete authentication you should check the authenticated state when changing pages,but that‚Äôs not in the scope of this blog5. Preparing protractorThe sign-in part is the one I am going to test with Protractor.First thing to do, is to install Protractor on your system:npm install -g protractorThe webdriver manager is a helper tool to easily get a Selenium server running.Run the following commands in order to start it:webdriver-manager updatewebdriver-manager startTo keep your code clean, you could put tests in a dedicated folder, but many argue against it.  Since I work in functional modules, tests of these modules should live in the module itself.Next I created a Protractor configuration file in the root of my project called protractor.config.js:touch protractor.config.jsprotractor.config.js:exports.config = {    capabilities: {        'browserName': 'chrome'    },    specs: [        'www/sign-in/sign-in.spec.js',    ],    jasmineNodeOpts: {        showColors: true,        defaultTimeoutInterval: 30000,        isVerbose: true    },    allScriptsTimeout: 20000,    onPrepare: function(){        browser.driver.get('http://localhost:8100');    }};  Don‚Äôt forget to set the correct URL to your running app.If not, you‚Äôll see many errors, except that you might be referring to a wrong URL6. Preparing the testsAs you can see, there is already a spec file defined in the protractor config file, so let‚Äôs create it:cd www/sign-in/touch sign-in.spec.jsIn the newly created file, you can start writing your tests.If everything went well, you can simply add another test and it should validate to true.It only tests if the first page you see, is the login page:describe('Signing in', function(){    it('should start on sign-in view', function(){        expect(browser.getTitle()).toEqual('Sign in');    });});Basically, we define a describe function which will describe the whole scope of our specs.Every ‚Äòit‚Äô function is called a spec.We only created one for now.As you can see this is a very readable way of testing.We expect the browsers title to be equal to ‚ÄòSign in‚Äô.If the expect statement evaluates to true, the spec has passed without failures, otherwise it will have a failure.  Feel free to change Sign in to something else to fail the test.To run the tests, we can execute the following command in the folder of our protractor.config.js file:protractor protractor.config.jsRunning this command will read the config file and run all the spec-files defined.You will get some output in the command line.At the end you‚Äôll get a summary like 1 specs, 0 failures Finished in x.xxx seconds.This is a simple test but it doesn‚Äôt show the full potential of Protractor at all.Lets add a new spec as part of the describe.it('should be unable to click Sign-in button when fields are empty', function(){    var button = element(by.id('btnSignIn'));    expect(button.getAttribute('disabled')).toEqual('true');});So here we test the availability of the sign-in button when the fields are empty.Next is to test if the button becomes available if the fields are filled in with valid data.So lets add another test:it('should be possible to click Sign-in button when fields are filled in', function(){    var button = element(by.id('btnSignIn'));    var txtUsername = element(by.id('username'));    var txtPassword = element(by.id('password'));    txtUsername.sendKeys('yannick@gmail.com');    txtPassword.sendKeys('1234');    expect(button.getAttribute('disabled')).toBe(null);});All these tests should pass correctly in protractor.7. Page Object PatternYou might have noticed that your tests run synchronous after each other.In this scenario this might be useful, but sometimes you need to start with a ‚Äòclean page‚Äô which would mean you need to duplicate a lot of code (for finding the button and text-fields).  When you are working in an agile team, it is quite common that requirements or user stories change.This can implicate you‚Äôll have to change a lot of duplicated code.How can we work around that.The solution is called the page object pattern.The general idea is to put your page in a JavaScript object.Lets dive into sign-in.page.js.This file should also be put into the module folder:var SignInPage = function () {    browser.get('http://localhost:8100/signin');};SignInPage.prototype = Object.create({}, {    txtUsername: { get: function () { return element(by.id('username')); }},    txtPassword: { get: function () { return element(by.id('password')); }},    btnSignIn: { get: function () { return element(by.id('btnSignIn')); }},    typeUsername: {value: function (keys) { return this.txtUsername.sendKeys(keys); }},    typePassword: {value: function (keys) { return this.txtPassword.sendKeys(keys); }},    clickSignIn: {value: function (keys) { return this.btnSignIn.click(); }}});module.exports = SignInPage;In the constructor we make sure our browser opens the signin page by passing the correct URL.Then we use the prototype method to link our HTML elements with the object.Finally, it is wise to create helper methods for basic functionality, such as filling in a username, in case you ever would want to change that behaviour.Then you only need to change that line and all your tests will still pass.  Using logical method names keeps your tests readable which is what you‚Äôll want when you look back in a few months.We can now change our sign-in.spec.js to this:var SignInPage = require('./sign-in.page.js');describe('Signing in', function(){    var page;    beforeEach(function () {        page = new SignInPage();    });    it('should be unable to click Sign-in button when fields are empty', function(){        expect(page.txtUsername.getText()).toEqual('');        expect(page.txtPassword.getText()).toEqual('');        expect(page.btnSignIn.getAttribute('disabled')).toEqual('true');    });    it('should be possible to click Sign-in button when fields are filled in', function(){        page.typeUsername('yannick@gmail.com');        page.typePassword('1234');        expect(page.btnSignIn.getAttribute('disabled')).toBe(null);        page.clickSignIn();        expect(browser.getTitle()).toEqual('Rooms');    });});What changed?We created a page variable and before each it we assigned a new SignInPage object to the page variable.This way, your page gets loaded again before running every spec.This means it always returns in the same state.Now you can create your specs as user stories.8. ConclusionProtractor is an awesome way to test your app‚Äôs functionality.Using a descriptive syntax you can emulate almost every user action and run trough the whole app in no time, again and again.Using Protractor, you won‚Äôt have to spend a lot of time testing your application manually,and you can focus on feature development without having to worry about accidentally breaking some functionality.Protractor will ensure that your user gets a working app without frustrations!"
      },
    
      "ionic-2016-03-31-adding-typescript-to-ionic-framework-html": {
        "title": "Adding TypeScript to Ionic Framework",
        "url": "/ionic/2016/03/31/Adding-TypeScript-to-Ionic-Framework.html",
        "image": "/img/ionic-and-typescript.jpg",
        "date": "31 Mar 2016",
        "category": "post, blog post, blog",
        "content": "Ionic and TypeScript sitting in a treeSo, TypeScript is the all new thing that allows you to use features from ES6 (or ES2015), ES7 and beyond.Say goodbye to loosely typed variables and say hello to modules, classes, interfaces and so much more.In order to use TypeScript in an Ionic Framework project there are a few small things you need to do to get things running.1. Install and configure the gulp package  Install the gulp-tsc package and save it to the development dependencies in package.jsonnpm install gulp-tsc --save-dev  Next, require the package in your gulpfile.js like sovar typescript = require('gulp-tsc');      Add the following line to the paths object: ts: ['./src/*.ts', './src/**/*.ts'].You may have noticed two things here: All my TypeScript files are in a src folder which means I‚Äôm not using the www folder that Ionic provides by default.This way I can keep the TypeScript files and JavaScript files separated.Next to that I‚Äôm also targeting subfolders in that folder because I‚Äôm bundling my logic based on AngularJS modules.You can read more about structuring an AngularJS project in the John Papa AngularJS Style Guide.        Add our compile task  gulp.task('compile', function(){    gulp.src(paths.ts)        .pipe(typescript({ emitError: false }))        .pipe(gulp.dest('www/'));});  Add our task to watchgulp.task('watch', function () {    gulp.watch(paths.sass, ['sass']);    gulp.watch(paths.ts, ['compile']);});  Now change the ionic.project file and add the compile task to the gulpStartupTasks. If the gulpStartupTasks section is not present at all, just add it anyway.\"gulpStartupTasks\": [    \"sass\",    \"compile\",    \"watch\"]2. Add TSDTSD is a TypeScript Definition manager for DefinitelyTyped.TypeScript used TypeScript Definition files so it knows how to handle the TypeScript you are writing and gives you intellisense.Let‚Äôs install TSD so we can continue.$ sudo npm install -g tsd$ tsd install ionic cordova --saveThis will create a typings folder which contains a tsd.d.ts file with references to the typings needed for ionic and cordova.In the root of your project a tsd.json file will be created with all the installed definitions.All you need to do to use the typings in your TypeScript file is include it at the top like so:/// &lt;reference path=\"../typings/tsd.d.ts\" /&gt;Note: TSD has been deprecated in favour of Typings to manage and install TypeScript definitions.More info on how to switch from TSD to Typings can be found here.3. Prevent editor from compiling on saveNow to prevent your editor to auto compile TypeScript we add a tsconfig.json file to the src folder with this in it:{    \"compileOnSave\": false}4. Add TypeScript files in src folderNow that we have everything set up it‚Äôs time to start refactoring your application.It‚Äôs important to know that every JavaScript file is essentially TypeScript because TypeScript is a superset of the current JavaScript implementation.This basically means that you can take your JavaScript files from your www folder, paste them to the src folder and rename them from file.js to file.ts.Of course don‚Äôt forget to add the reference on top of your files to the tsd.d.ts file in the typings folder.If you now run ionic serve, you should see a message that looks like this one. ‚ÄúCompiling TypeScript files using tsc version x.x.x‚Äù.TypeScript will process these files and write ES5 files to the www folder.ConclusionAs you can see it is fairly simple - just 4 steps - to add TypeScript support to your Ionic project by changing the default gulp setup used by Ionic.It‚Äôs nice to know that Ionic 2 will have support for TypeScript built in so you won‚Äôt have to do it yourself.By adding a flag --ts to your Ionic 2 project setup it will be enabled.Personally I love using TypeScript and will use it whenever I can.It makes my life as a developer a lot easier by spotting errors before I even hit the browser.What are your thoughts about TypeScript? Feel free to add them in the comments section."
      },
    
      "angularjs-typescript-2016-03-16-angularts-html": {
        "title": "AngularTS: A new look @ Angular",
        "url": "/angularjs-typescript/2016/03/16/AngularTS.html",
        "image": "/img/angularts.jpg",
        "date": "16 Mar 2016",
        "category": "post, blog post, blog",
        "content": "Combining the best of two worlds.Since my introduction to the heroic AngularJS framework at Devoxx around 4 years ago, I was intrigued and set for an adventure.With the upcoming release of Angular 2 we have to prepare ourselves with the migrating road map coming up.One of the core changes in Angular 2 is the focus on using TypeScript.This post will cover the use of Angular components in TypeScript.But what is it?TypeScript is a superset of JavaScript that focuses on strong typing and new ES6 features: classes, interfaces and modules.Like in common object-oriented languages such as Java and C# these features aren‚Äôt new.These features give the developer the opportunity to build an object-oriented architecture in JavaScript.With that in mind, let‚Äôs see what the advantages are:TranspilingThe DOM can only recognize JavaScript.With this said they had to come up with a way to compile TS (TypeScript).Because TS is a superset of JS it can transpile to plain JavaScript before including it into HTML.Transpilers are integrated in the latest IDE‚Äôs. Any valid JavaScript is valid TypeScript.Strongly typedWhen you‚Äôre used to plain JavaScript, you notice that every time you need a variable, it is loosely typed.TypeScript gives you the opportunity for each of your variables to have its own type.This comes with great benefits like better refactoring, less bugs and better type checking at compile time.OO architectureTypeScript offers an object-oriented architecture experience, which means all code is defined in classes, interfaces and most of those classes can be instantiated into objects. It also supports encapsulation, which protects the data from unintended access and modification.Learning path of AngularTSIf you‚Äôre no stranger to AngularJS you will notice that the structure remains the same. Two way data binding, controllers, services, ‚Ä¶ But be aware that it has a different syntax in TypeScript. I will show you the different best practices to implement such components.TypeScript Definition FilesWhen using TS we will refer to TSD files.These files describe the types defined in external libraries such as Angular. To install the Angular TSD files we use typings.To use the typings manager we install it with:npm install typings --globalAfterwards install Angular with:typings install angular --ambient --save--ambient --save enables the flag and persists the selection in ‚Äòtypings.json‚ÄôAll the installed TSD files are gathered in the typings folder.In the main.d.ts file you will see the references the application will use for Angular.Since Angular has multiple libraries, you can use the search command to find the required definition.typings search AngularIt is possible that you have to declare the reference on top of your file./// &lt;reference path=\"../../typings/main.d.ts\" /&gt;ModulesAngular ModulesModules are here to help us modularize our code.It is a best practice to use one main module as the root of your application. Multiple modules are being used for third-party libraries or common code.To let the module know the existence of every component, they have to register themselves.Below every component declaration you will see a registration to the module. When registering the module you have to add all the libraries you want to depend upon.In this example we inject the routing service for navigation.module JWorks {    \"use strict\";    angular       .module(\"jworks360\", ['ngRoute'])}Internal TypeScript ModulesThese modules are similar to namespaces.You can define an unique namespace around your code.This will encapsulate variables, interfaces and classes. TypeScript supports sub namespaces for further encapsulation.module JWorks {    \"use strict\";}Transpiled JavaScriptvar JWorks;(function(JWorks){})(JWorks || (JWorks = {}));To encapsulate our code, the module will transpile to an IIFE (Immediately-Invoked Function Expression) around our components.This will avoid global code which helps prevent variables and function declarations from living longer than expected in the global scope, which also helps avoid variable collisions.Entity ClassNow that TypeScript supports object-oriented programming, we can analyse our business problem and define the business objects into entity classes.When you analyse and define these entities you can define which properties and methods each entity needs.If you have a couple entities, you can even establish a relationship.This will provide a clear view on what you want to achieve and have the possibility to create multiple instances of these classes. When building an entity class you can optionally define an interface to show what the intention of the class is.module JWorks {    export interface IEmployee {         username:string;         name:string;         eat(food:string):void    }    export class Employee implements IEmployee {           constructor(public username:string, public name:string){}          eat(food:string):void{             //implementation          }    }}To use your entity class in a controller you have to define the export key.This will expose the class to other classes.When exporting the interface you will use it as a data type.Class as propertyEmployee:IEmployee;Instance of the classemployee = new Employee();Access propertyemployee.nameCall methodsemployee.eat(eggs);ControllersAs you know the controller defines the model to the view of your application, methods for every action you require and the scope where you hold a two way binding.Because TS offers an object-oriented architecture, we can use classes and interfaces instead of functions.Interfaces, like in all object-oriented languages, are a contract that must be implemented by classes that use it.When implemented, all methods and properties have to be used.Classes declare and implement the properties and methods exposed to the view.Every class has his own constructor function, in this function we can declare default property values and other initialisation code.Controller Interfacemodule Jworks {    interface IEmployeeController{        person:IPerson;        save(person:IPerson):void;    }}The interface will show you the intent of our controller and declare the properties and methods that will be used.When you look at the syntax, you see that the properties are strongly typed and the type is declared after the colon.If you aren‚Äôt certain what type a property should have, you can fall back to the general type ‚Äòany‚Äô.When you declare methods in an interface you have to specify the necessary parameters and return types.The parameters have the same syntax as the properties.Controller Classmodule JWorks {    class EmployeeController implements IEmployeeController{        static $inject = [\"EmployeeService\"];        constructor(private employeeService:IEmployeeService,public employee:IEmployee){             this.employee = new Employee(\"Nivek\",\"Kevin\");        }        save():void{             this.employeeService.addEmployee(this.employee).then(function(result){             console.log(\"added successfully!\");        });   }}angular.module(\"jworks360\").controller(\"EmployeeController\", EmployeeController);Dependency Injection in classesWhen a service is needed in your controller, it must to be injected before it can be used.In the above example it is important that you declare the static $injection on top of your constructor.By doing this the constructor will recognize and initialise the injected services.If you inject a custom service you have to reference to the related service./// &lt;reference path=\"../services/employee.service.ts\"/&gt;ConstructorTypeScript supports initialisation of your properties and injections in a constructor.When declaring properties in your class, you can declare them directly into your constructor.Although these two examples are correct you can have issues in your tests with the second example.So this:class Controller {    name:string;    constructor(public name:string){        this.name=name;    }}Becomes:class Controller {    constructor(public name:string){    }}Be sure to notice that we are using access modifiers to tell the controller which properties we want to expose to the view.The best practice is that you put your injections and Angular services private and all your properties you want to use on your view public.When initialising strings, TypeScript makes no distinction between double or single quotes.ControllerAsController classes use the controllerAs feature by default.So it‚Äôs important to declare this into your routes and views.In your HTML you will have to prefix your methods and properties with the ControllerAs syntax.module JWorks {   \"use strict\";   function routes($routeProvider) {       $routeProvider           .when('/', {               redirectTo: '/login'           })           .when('/profile', {               templateUrl: 'app/persons/profile.html',               controller: 'EmployeeController',               controllerAs: '$profile',           })           .otherwise({               redirectTo: '/'           });   }   routes.$inject = [\"$routeProvider\"];   angular.module(\"jworks360\")       .config(routes);}ServicesWhen you make a custom service, the code you implement is reusable and can be called in any other Angular component, including controllers and other services.It is important to know that services are singletons, so there will be only one instance for each service.With this in mind we can use the custom service to share data across all components in Angular.Communicating with an HTTP service to collect and share data with any other component by injecting the service.RestangularFor my project I used an Angular service that simplifies common verb requests with a minimum of client code.In my custom services you‚Äôll see examples of Restangular in TypeScript.If you like to checkout what the difference is with $resource, you can check this listmodule JWorks {   export interface IEmployeeService {       username:string;       employee:IEmployee;       getEmployee(): Employee;       setEmployee(employee:IEmployee);       getEmployeeByUsername(username:string):ng.IPromise&lt;{}&gt;;       getEmployeeByLink(href:string):ng.IPromise&lt;{}&gt;;   }   export class EmployeeService implements IEmployeeService {       static $inject = [\"EmployeeRestangular\", \"$location\"];       constructor(private employeeRestService:restAngular.IService, private $location:ng.ILocationService, private employees:restAngular.IElement,public employee:IEmployee,public username:string) {            username =window.sessionStorage.getItem(\"username\");            this.employees = employeeRestService.all(\"employees\");            employees.one(username).get().then((data:any)=&gt; {                   this.setEmployee(data);               });           }       }       getEmployee():Employee {           return this.employee;       }              setEmployee(employee:IEmployee) {           this.employee = employee;       }                    getEmployeeByUsername(username:string):ng.IPromise&lt;{}&gt;            return this.employees.one(username).get()       }       getEmployeeByLink(href:string):ng.IPromise&lt;{}&gt;{           return this.employees.oneUrl(href).get();       }          }          angular.module(\"jworks360\")              .service(\"EmployeeService\", EmployeeService);       }In the above example, to use the Restangular service you have to install the proper typings.For services it is a best practice to declare an interface for data typing and getting a clear view of the intent.The service class will implement all methods related to the data communication with the backend and returns a promise to the controllers or services that will inject this custom service.Restangular has its own configuration you can modify in the .config component to point to the right api call.After the config you can inject the Restangular service and use its services to build up a request to the backend.DirectivesCustom directives allow you to create highly semantic and reusable components.A directive allows Angular to manipulate the DOM and add its own behaviour. These can either be a set of instructions or a JSON representation.To define a directive in TypeScript we use the directive service that Angular provides.module JWorks {   export interface IAnimate extends ng.IAttributes {       jwAnimate:string;   }   class Animate implements ng.IDirective {       restrict = \"A\";       static instance():ng.IDirective {           return new Animate();       }       link($scope, elm:ng.IRootElementService, attr:IAnimate,ngModel:ng.INgModelController):void {           $scope.right = function(){               $(this).animate({                   left: '+=150'               });                elm.fadeOut(\"slow\");           };           var direction = attr['jwAnimate'];           elm.on('click',$scope[direction]);       }   }   angular.module(\"jworks360\").directive(\"jwAnimate\", Animate.instance);}In the interface above we have to tell Angular what name will be used for our directive.The attribute service will be called to add the name to its attributes.Secondly the class has to implement the directive interface to be recognized by the compiler as a directive.Inside the class you have to declare the prefixed properties and override the methods you will be using.The static instance() method has to be declared to let your module know that there is a new directive.At the end you register the directive to your module with the instance as value.Final noteBest practices can change over time. With webpack for example the registry to the module is contained in one file.TypeScript keeps on growing, and in my opinion will be the default language for many future front-end projects.When it comes to testing our code, TypeScript will provide better support because of encapsulation. Finally, this is a nice learning path to take if you want to migrate to Angular 2."
      },
    
      "security-2016-03-12-digitally-signing-your-json-documents-html": {
        "title": "Digitally signing your JSON documents",
        "url": "/security/2016/03/12/Digitally-signing-your-JSON-documents.html",
        "image": "/img/digitally-signing-your-json-documents.png",
        "date": "12 Mar 2016",
        "category": "post, blog post, blog",
        "content": "What is a digital signature?A digital signature is a mathematical scheme for demonstrating the authenticity of a digital message or documents.A valid digital signature gives a recipient reason to believe that the message was created by a known sender, that the sender cannot deny having sent the message (authentication and non-repudiation), and that the message was not altered in transit (integrity).Digital signatures are a standard element of most cryptographic protocol suites.They are commonly used for software distribution, financial transactions, and in other cases where it is important to detect forgery or tampering.Non-repudiation refers to a state of affairs where the author of a statement will not be able to successfully challenge the authorship of the statement or validity of an associated contract.The term is often seen in a legal setting wherein the authenticity of a signature is being challenged.In such an instance, the authenticity is being ‚Äúrepudiated‚Äù.Meet JOSEJOSE is a framework intended to provide a method to securely transfer claims (such as authorisation information) between parties.The JOSE framework consists of several specifications to serve this purpose:  JWK ‚Äì JSON Web Key, describes format and handling of cryptographic keys in JOSE  JWS ‚Äì JSON Web Signature, describes producing and handling signed messages  JWE ‚Äì JSON Web Encryption, describes producing and handling encrypted messages  JWA ‚Äì JSON Web Algorithms, describes cryptographic algorithms used in JOSE  JWT ‚Äì JSON Web Token, describes representation of claims encoded in JSON and protected by JWS or JWEJWKA JSON Web Key (RFC7517) is a JavaScript Object Notation (JSON) data structure that represents a cryptographic key.    {         \"kty\": \"EC\",        \"crv\": \"P-256\",        \"x\": \"f83OJ3D2xF1Bg8vub9tLe1gHMzV76e8Tus9uPHvRVEU\",        \"y\": \"x_FEzRu9m36HLN_tue659LNpXW6pCyStikYjKIWI5a0\",        \"use\": \"sig\",        \"kid\": \"Public key used to sign our messages\"    }In this example you can see a couple of parameters.The first of them ‚Äúkty‚Äù defines the key type, which is a mandatory field.Depending on the type you‚Äôve chosen other parameters can be set, like you see above.As our type is EC, or Elliptic Curve, we want to specify the type of curve and our point.Next to these parameters we also have the optional ‚Äúuse‚Äù to denote intended usage of the key and ‚Äúkid‚Äù as key ID.At the time of writing there are three supported key types: ‚ÄúEC‚Äù, ‚ÄúRSA‚Äù and ‚Äúoct‚Äù.While ‚ÄúEC‚Äù and ‚ÄúRSA‚Äù are used for asymmetric encryption, ‚Äúoct‚Äù is used for symmetric encryptionJWSThe JSON Web Signature (RFC7515) standard describes the process of creation and validation of a data structure representing a signed payload.Assume someone wants to transfer an amount of money to his savings account.This action could be represented like the following JSON:    {         \"from\": {            \"name\": \"Tim Ysewyn\",            \"account\": \"Checking account\"        },        \"to\": {            \"name\": \"Tim Ysewyn\",            \"account\": \"Savings account\"        },        \"amount\": 250,        \"currency\": \"EUR\"    }In this example we are using a JSON document, but this is not relevant for the signing procedure.Before we can sign this we need to convert this to base64url encoding, which will be our payload.So actually we might be using any type of data!The result of the base64url encoding of above transaction is:eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9Additional parameters are associated with each payload.One of those is the required ‚Äúalg‚Äù parameter, which indicates what algorithm needs to be used to generate a signature.Here we can also specify ‚Äúnone‚Äù to send unprotected messages.All parameters are included in the final JWS.These can either be sent as a protected or unprotected header.The data in the unprotected header is human readable associated data, whereas data in the protected header is integrity protected and base64url encoded.Assume we want to sign our payload using a key like we generated in the previous section.Our data structure would look like this:    {         \"alg\": \"ES256\"    }and base64url encoded this would be:eyAKICAgICAgICAiYWxnIjogIlJTMjU2IgogICAgfQ==The base64url encoded payload and protected header are concatenated with a ‚Äò.‚Äô to form raw data, which is fed to the signature algorithm to produce the final signature.Finally all of this output will be serialized using one the JSON or Compact serialisations.Compact serialisation is simple concatenation of dot separated base64url encoded protected header, payload and signature.JSON serialisation is a human readable JSON object, which for the example in this section would look like this:    {        \"payload\": \"eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9\",        \"protected\": \"eyAKICAgICAgICAiYWxnIjogIlJTMjU2IgogICAgfQ==\",        \"header\": {            \"signature\": \"DtEhU3ljbEg8L38VWAfUAqOyKAM6-Xx-F4GawxaepmXFCgfTjDxw5djxLa8ISlSApmWQxfKTUJqPP3-Kg6NU01Q\"        }    }Before we conclude this section, there is one more thing I would like to share with you.Because we want to sign and protect our messages, we always want to use asymmetric encryption.But, once our private key has been captured, anyone who has this can forge transactions.One way that COULD counter this is to generate a new key pair every session, or even per transaction.Including the public key in the protected header would not only give the server the ability the validate the signature, we will also be sure that it is the correct one since the protected header is integrity protected!JWEJSON Web Encryption (RFC7516) follows the same logic as JWS with a few differences:  by default, for each message a new content encryption key (CEK) should be generated.This key is used to encrypt the plaintext and is attached to the final message.Public key of recipient or a shared key is used only to encrypt the CEK (unless direct encryption is used).  only AEAD (Authenticated Encryption with Associated Data) algorithms are defined in the standard, so users do not have to think about how to combine JWE with JWS.To keep it short: While JWS can be read by everyone because of the simple base64url encoding, we could use JWE to encrypt some or all of our fields.JWAJSON Web Algorithms (RFC7518) defines algorithms and their identifiers to be used in JWS and JWE.The three parameters that specify algorithms are ‚Äúalg‚Äù for JWS, ‚Äúalg‚Äù and ‚Äúenc‚Äù for JWE.Visit following links to view the list of supported algorithms for JWS and JWEJWTJSON Web Token (RFC7519) is used for passing claims between parties in a web application environment.Because the tokens are designed to be compact and URL-safe they are especially usable in a web browser single sign-on (SSO) context.JWT claims can be typically used to pass the identity of authenticated users between an identity provider and a service provider.JWT relies on all previously mentioned JSON standards.The JWT standard defines claims - key/value pairs asserting information about a subject.The claims include  ‚Äúiss‚Äù identifies the principal that issued the token  ‚Äúsub‚Äù identifies the principal that is the subject of the token  ‚Äúaud‚Äù (audience) identifies the intended recipients  ‚Äúexp‚Äù identifies the expiration time on or after which the token MUST NOT be accepted for processing  ‚Äúnbf‚Äù (not before) identifies the time before which the token MUST NOT be accepted for processing  ‚Äúiat‚Äù (issued at) identifies the time at which the token was issued  ‚Äújti‚Äù (JWT ID) provides a unique identifier for the tokenThese claims are not mandatory to be used or implement in all cases, but they rather provide a starting point for a set of useful, interoperable claims.So, how do we sign this JSON document in code?Ranging from Java and .NET to Node.js, there are already a lot of libraries available on the internet.And even JavaScript has its own implementation of the standard!Because of its fluent API, we are using the Java JWT implementation in this post.Since not all algorithms are implemented in Java, we are also going to use Bouncy Castle as our JCA provider.In our maven configuration we just add following two dependencies:    &lt;dependency&gt;        &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt;        &lt;artifactId&gt;jjwt&lt;/artifactId&gt;        &lt;version&gt;0.6.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt;        &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt;        &lt;version&gt;1.54&lt;/version&gt;    &lt;/dependency&gt;If you are working with a gradle project it would be:    runtime 'io.jsonwebtoken:jjwt:0.6.0',            'org.bouncycastle:bcprov-jdk15on:1.54'If we were to implement the examples from the previous sections, we would start of with generating a new public-private key pair.    KeyPair keyPair = EllipticCurveProvider.generateKeyPair(SignatureAlgorithm.ES256);It‚Äôs as easy as that!We want to have a key of type ‚ÄúEC‚Äù so we use the EllipticCurveProvider, and by specifying SignatureAlgorithm.ES256 we use the P-256 bit curve with SHA-256 hashing.Next we want to sign our base64url encoded payload    Jwts.builder()                .setPayload(\"eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9\")                .signWith(SignatureAlgorithm.ES256, keyPair.getPrivate())                .compact();Since we already encoded our original message in the JWS section, I‚Äôm not getting here into detail again.signWith(SignatureAlgorithm.ES256, keyPair.getPrivate()) does a couple of things.First it is going the create a header if not already present and it will add the ‚Äúalg‚Äù key with the value of ‚ÄúES256‚Äù.After that it will base64url encode that header and will append this with a ‚Äò.‚Äô and the encoded payload.This whole blob of data will then be signed using the private key of the previously generated key pair.Last, but not least, is the compact method.This will just output the base64url encoded header and payload with the generated signature, and all parts are separated with a dot.An outcome would be something like:eyJhbGciOiJFUzI1NiJ9.ZXlBS0lDQWdJQ0FnSUNBaVpuSnZiU0k2ZXdvZ0lDQWdJQ0FnSUNBZ0lDQWlibUZ0WlNJNklDSlVhVzBnV1hObGQzbHVJaXdLSUNBZ0lDQWdJQ0FnSUNBZ0ltRmpZMjkxYm5RaU9pQWlRMmhsWTJ0cGJtY2dZV05qYjNWdWRDSUtJQ0FnSUNBZ0lDQjlMQW9nSUNBZ0lDQWdJQ0owYnlJNmV3b2dJQ0FnSUNBZ0lDQWdJQ0FpYm1GdFpTSTZJQ0pVYVcwZ1dYTmxkM2x1SWl3S0lDQWdJQ0FnSUNBZ0lDQWdJbUZqWTI5MWJuUWlPaUFpVTJGMmFXNW5jeUJoWTJOdmRXNTBJZ29nSUNBZ0lDQWdJSDBzQ2lBZ0lDQWdJQ0FnSW1GdGIzVnVkQ0k2SURJMU1Bb2dJQ0FnSUNBZ0lDSmpkWEp5Wlc1amVTSTZJQ0pGVlZJaUNpQWdJQ0I5.MEYCIQCcwunLBiuHu2z_SlDVJyZuQv0NU8X4VYoOFN1EuIvObQIhAJeZuTeZw9k5uhpBc60iT13s3yb01ItSB2MhEd5pUSqCWe split the three parts for better visualisation, the JWS would be one large StringValidating the signatureFirst we will check if the JWS was actually signed.This can be accomplished by executing following line of code.    Jwts.parser().isSigned(jws);To parse the JWS, we use the parse() method.    Jwts.parser()        .setSigningKey(publicKey)        .parse(jws);Depending wether it is signed or not we might need to set the key for validation.In our case we need to specify the public key of our asymmetric key pair.If we would try to parse the JWS without a key an IllegalArgumentException will be thrown.Should a wrong public key have been provided a SignatureException would be thrown, telling us to not trust this JWS.If we were to pass our public key in the protected header like we said in the JWS section, we should use the setSigningKeyResolver() method.This custom resolver would read out the ‚Äújwk‚Äù field from the protected header and return a public key based on the data that was provided.Our own SigningKeyResolver implementation could look like this:    public class ECPublicSigningKeyResolver implements SigningKeyResolver {        public Key resolveSigningKey(JwsHeader header, Claims claims) {            return getPublicKey(header);        }        public Key resolveSigningKey(JwsHeader header, String plaintext) {            return getPublicKey(header);        }        private Key getPublicKey(JwsHeader header) {            try {                HashMap&lt;String, String&gt; jwk = new ObjectMapper().readValue(header.get(\"jwk\").toString(), HashMap.class);                            String curve = jwk.get(\"crv\");                BigInteger x = new BigInteger(jwk.get(\"x\"), 16);                BigInteger y = new BigInteger(jwk.get(\"y\"), 16);                String keyType = jwk.get(\"kty\");                ECNamedCurveParameterSpec ecNamedCurveParameterSpec = ECNamedCurveTable.getParameterSpec(crv);                            ECCurve curve = ecNamedCurveParameterSpec.getCurve();                ECPoint g = ecNamedCurveParameterSpec.getG();                BigInteger n = ecNamedCurveParameterSpec.getN();                BigInteger h = ecNamedCurveParameterSpec.getH();                ECParameterSpec ecParameterSpec = new ECParameterSpec(curve, g, n, h);                ECPoint ecPoint = curve.createPoint(x, y);                ECPublicKeySpec ecPublicKeySpec = new ECPublicKeySpec(ecPoint, ecParameterSpec);                KeyFactory keyFactory = KeyFactory.getInstance(kty);                return keyFactory.generatePublic(ecPublicKeySpec);            } catch (IOException e) {                e.printStackTrace();            } catch (NoSuchAlgorithmException e) {                e.printStackTrace();            } catch (InvalidKeySpecException e) {                e.printStackTrace();            }            return null;        }    }First we read all our data from the ‚Äújwk‚Äù field.Next we retrieve the ECNamedCurveParameterSpec based on the ‚Äúcrv‚Äù field and assemble a new ECParameterSpec.After that we create a new ECPublicKeySpec with the ECParameterSpec and an ECPoint out of the x and y coordinates.Finally we get a KeyFactory instance for our key type ‚Äúkty‚Äù and generate the public key with our ECPublicKeySpec.ConclusionJOSE is a simple, compact and lightweight framework to sign and encrypt your payload messages.Because of the combination of base64url encoded messages and JSON data structures it is web friendly.With the wide range of libraries this can be used across platforms with native and hybrid applications, even web applications can use this!One particular disadvantage with the use of the compact dot notation is that you can‚Äôt send unprotected header data anymore.Final noteAbove examples should only be used as reference. In a production environment we need to use both JWS and JWE.One could embed a public key of an asymmetric key pair in the application.During login a new symmetric key will be generated, encrypted with that public key and sent to the server.This symmetric key can only be decrypted by the server with the private key, and should then be stored in the session.Every time we need to sign a JSON document, we would use the symmetric key to encrypt the JWS using JWE.It doesn‚Äôt matter how you encrypt your messages, and which algorithms you use.Once your application has been hacked, the whole system is vulnerable."
      },
    
      "conference-2016-03-10-javaland2016-html": {
        "title": "The 5 key trends of JavaLand 2016",
        "url": "/conference/2016/03/10/JavaLand2016.html",
        "image": "/img/javaland.png",
        "date": "10 Mar 2016",
        "category": "post, blog post, blog",
        "content": "  JavaLand is a software conference, held annually, in Phantasialand, Br√ºhl (Germany). JavaLand focuses on Java enthusiasts, developers, architects, strategists, administrators and project managers. With more than 100 lectures, JavaLand caters to the interest of both beginners and experts. These are, what I believe, the 5 key trends of the conference.1. Microservices stay hot and are maturingJavaLand dedicated an entire track to containers and microservices. This resulted in a large variety of talks on the subject. I attended a couple, but the talk by Ordina‚Äôs very own Andreas Evers hit the sweet spot between introducing the microservice concepts and making them tangible. Microservices transfer a lot of the application‚Äôs complexity to the interactions between the services. Applying patterns such as circuit breakers and bulkheads are quintessential to building successful distributed systems. Andreas presented all of this in a clear and concise manner to the delight of the audience.2. TypeScript / Angular 2Frontend developers are in for a treat. Currently, nobody exactly knows when Angular 2 is going to be released, but everybody is eagerly looking forward to it. Rumor has it .. release will be very soon. Angular 2 promises to be a faster, more powerful, cleaner, and easier to use tool. The Angular team provides an upgrade path to migrate your old Angular 1 applications. What‚Äôs also really interesting is the Angular 2 Style Guide, that contains best practices on how to organize your project, name your components, etc.Angular 2 was migrated to TypeScript, because of the great tooling support. TypeScript isn‚Äôt the first language to compile to JavaScript, but supported by Angular 2, it just might be a game changer.3. Cloud-Native JavaJosh Long did a stunning job, giving a whirlwind talk on a large number of Spring tools to support the building of Cloud Native Java applications. He started with Spring Data REST to build a hypermedia-driven REST web service with a Spring Data Repository. Then he introduced Spring Cloud Config to externalize configuration files of the different microservices he was building. When building distributed applications, a config server is essential, in my opinion. Next up was Spring Cloud Netflix, which is a ‚ÄúSpringified‚Äù collection of tools open sourced by Netflix. Josh demoed:  Eureka for service registry and discovery  Zuul as an API gateway  Hystrix for circuit breakersFinally he used Spring Cloud Sleuth with Zipkin to show us a nice dashboard of the different requests going through his freshly deployed microservices.A talk by Josh Long is always an event and we‚Äôre very proud to announce that he will be doing a presentation at Ordina Belgium later this month!4. Reactive‚ÄúReactive‚Äù is used broadly to define event-driven systems. Reactive Systems are responsive, resilient, elastic and message driven. Details on these key characteristics can be found in the Reactive Manifesto. The most popular Java library to compose asynchronous and event-based programs is RxJava. To build reactive applications, RxJava uses Observable sequences that make it easy to wrap synchronous methods in asynchronous calls.An interesting presentation to learn more about this topic is available on Youtube.While Reactive programming isn‚Äôt new, it has been gaining a lot of traction recently. For example thanks to the recently released Lagom framework from Lightbend. Last year the Spring team announced that Spring 5 will also focus on Reactive.5. KubernetesKubernetes is an orchestration system for Docker and Rocket containers, initiated by Google in 2014. In Kubernetes, containers run in Pods. These pods are managed by Replication Controllers (create, destroy, start / stop, failure, scaling, ‚Ä¶). Since Replication Controllers can create and destroy Pods dynamically, we can‚Äôt rely on their IP addresses to communicate with each other. This can be solved by using Kubernetes Services.Kubernetes can schedule and run containers on clusters of both physical and virtual machines.An interesting discussion, after one of the Kubernetes talks, was about the differences between Kubernetes and a regular PaaS. This post on Stackoverflow provides a lot of input for that discussion, stating that Kubernetes is PaaS-like: Cloud Foundry can be considered an ‚ÄúApplication PaaS‚Äù and Kubernetes a ‚ÄúContainer PaaS‚Äù, but the distinction is fairly subtle and fluid, given that both projects change over time to compete in the same markets. The subtlety of the difference is demonstrated by the Kubernetes documentation, explicitly stating Kubernetes is not a PaaS.The Community ActivitiesIt‚Äôs impossible to talk about JavaLand, without mentioning The Community Activities. These focus on  Innovation discovery: Humanoid Robots, Virtual Reality (VR), Neural Networks, ‚Ä¶  Gamification: Hacking sessions and contests, Dojos, ‚Ä¶  Networking: User Groups, Jogging, Tours, ‚Ä¶  Phantasialand: The theme park opened its door exclusively for JavaLand on Tuesday evening. What better way to bond with colleagues and leaders in the Java community than in a roller coaster :)"
      },
    
      "spring-2016-03-05-http-public-key-pinning-with-spring-security-html": {
        "title": "HTTP Public Key Pinning with Spring Security",
        "url": "/spring/2016/03/05/HTTP-Public-Key-Pinning-with-Spring-Security.html",
        "image": "/img/spring-security-logo.png",
        "date": "05 Mar 2016",
        "category": "post, blog post, blog",
        "content": "What kind of sorcery is this?HTTP Public Key Pinning, or short HPKP, is a security mechanism which allows HTTPS websites to resist impersonation by attackers using mis-issued or otherwise fraudulent certificates.This was standardized in RFC 7469 and creates a new opportunity for server validation. Instead of using static certificate pinning, where public key hashes are hardcoded within an application, we can now use a more dynamic way of providing this public key hashes.One caveat to remember is that HPKP uses a Trust On First Use (TOFU) technique.How does this work?A list of public key hashes will be served to the client via a special HTTP header by the web server, so clients can store this information for a given period of time.On subsequent connections within previous given period of time, the client expects a certificate containing a public key whose fingerprint is already known via HPKP.I strongly encourage you to read this article by Tim Taubert, where he explains what keys you should pin and what the different tradeoffs are.Imagine you want to terminate the connection between the client and a malicious server for your main domain and all of your subdomains, but also want to be notified when such events happen.In the next paragraph you can find the implementation details.The web server needs to send following header to the connecting client with the first response    Public-Key-Pins:        max-age=5184000;        pin-sha256=\"d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=\";        pin-sha256=\"E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=\";        report-uri=\"https://example.net/hpkp-report\";        includeSubdomainsBy specifying the Public-Key-Pins header the client MUST terminate the connection without allowing the user to proceed anyway. In this example, pin-sha256=‚Äùd6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=‚Äù pins the server‚Äôs public key used in production. The second pin declaration pin-sha256=‚ÄùE9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=‚Äù also pins the backup key. max-age=5184000 tells the client to store this information for two month, which is a reasonable time limit according to the IETF RFC. This key pinning is also valid for all subdomains, which is told by the includeSubdomains declaration. Finally, report-uri=‚Äùhttps://www.example.net/hpkp-report‚Äù explains where to report pin validation failures.So how can we implement this with Spring Security?Retrieving  the list of public key hashesWe first need to get a list of public key hashes. Currently the standard only supports the SHA256 hashing algorithm. The following commands will help you extract the Base64 encoded information:From a key file\topenssl rsa -in my-key-file.key -outform der -pubout | openssl dgst -sha256 -binary | openssl enc -base64From a Certificate Signing Request (CSR)\topenssl req -in my-signing-request.csr -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64From a certificate\topenssl x509 -in my-certificate.crt -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64From a running web server\topenssl s_client -servername www.example.com -connect www.example.com:443 | openssl x509 -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64For now we will assume we got 2 public keys:  Our active production certificate: d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=  Our backup production certificate: E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=Configuring Spring SecurityAs of version 4.1.0.RC1, which will be released March 24th 2016, the HpkpHeaderWriter has been added to the security module. The 2 easiest ways to implement this feature is either by Java configuration or by using the older, but still supported, XML configuration. Below you can find both solutions:Java config\t@EnableWebSecurity\tpublic class HpkpConfig extends WebSecurityConfigurerAdapter {\t\t@Override\t\tprotected void configure(HttpSecurity http) throws Exception {\t\t\thttp.httpPublicKeyPinning()\t\t\t\t.addSha256Pins(\"d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=\", \"E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=\")\t\t\t\t.reportOnly(false)\t\t\t\t.reportUri(\"http://example.net/hpkp-report\")\t\t\t\t.includeSubDomains(true);\t\t}\t}XML config\t&lt;http&gt;\t\t&lt;!-- ... --&gt;\t\t&lt;headers&gt;\t\t\t&lt;hpkp\t\t\t\treport-only=\"false\"\t\t\t\treport-uri=\"http://example.net/hpkp-report\"\t\t\t\tinclude-subdomains=\"true\"&gt;\t\t\t\t&lt;pins&gt;\t\t\t\t\t&lt;pin&gt;d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=&lt;/pin&gt;\t\t\t\t\t&lt;pin&gt;E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=&lt;/pin&gt;\t\t\t\t&lt;/pins&gt;\t\t\t&lt;/hpkp&gt;\t\t&lt;/headers&gt;\t&lt;/http&gt;"
      },
    
      "spring-2016-02-06-generating-spring-rest-docs-without-using-integration-tests-html": {
        "title": "Generating Spring REST Docs without using integration tests",
        "url": "/spring/2016/02/06/Generating-Spring-REST-Docs-without-using-integration-tests.html",
        "image": "/img/spring.png",
        "date": "06 Feb 2016",
        "category": "post, blog post, blog",
        "content": "The problemA couple of days ago I was working on a project of one of our customers.One of their new applications needed to expose a public API, and of course we needed to hand over a set of documentation about those REST endpoints.Some people were already starting to do this manually in Confluence, but after a while (and we‚Äôre talking about a timespan just under 2 hours) this became a tedious job. We had to continuously adjust the input &amp; output contracts, the different endpoints,‚Ä¶Using Spring REST Docs I wanted to automatically document all of the public API endpoints, while we were also testing all of the components in the whole application.For some undisclosed reasons we simply couldn‚Äôt write integration tests, so we were stuck with our unit tests and mocked objects.The solutionImagine you have following service and controller in a simple Spring Boot application:    @Service    public class DeviceService {        public List&lt;Device&gt; getDevices() {        List&lt;Device&gt; devices = new ArrayList&lt;&gt;();            /*                Some business logic here...            */            return devices;        }    }    @RestController    @RequestMapping(\"devices\")    public class DeviceController {        private DeviceService deviceService;        @Autowired        public DeviceController(DeviceService deviceService) {            this.deviceService = deviceService;        }        @RequestMapping(method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE)        public List&lt;Device&gt; getDevices() {            return this.deviceService.getDevices();        }    }    Since this is a Spring Boot application both classes will automagically be instantiated.Because you need to annotate your unit tests at class level with @WebAppConfiguration and @SpringApplicationConfiguration, we can easily create a new Spring Boot application and use this for our documentation.In this new application we set the base package that needs to be scanned to our controller sub package, and create a mock implementation of our DeviceService.    @SpringBootApplication(scanBasePackages = { \"be.ordina.blog.controller\" } )    public class Application {        @Bean        public DeviceService getDeviceService() {            return EasyMock.createStrictMock(DeviceService.class);        }    }    Our DeviceControllerTests class will then look something like this:    @RunWith(SpringJUnit4ClassRunner.class)    @SpringApplicationConfiguration(classes = { Application.class })    @WebAppConfiguration    public class DeviceControllerTests {        @Rule        public RestDocumentation restDocumentation = new RestDocumentation(\"target/generated-snippets\");        @Autowired        private WebApplicationContext context;        @Autowired        public DeviceService deviceService;        private MockMvc mockMvc;        @Before        public void setUp() {            this.mockMvc = MockMvcBuilders.webAppContextSetup(this.context)                                            .apply(documentationConfiguration(this.restDocumentation))                                            .build();        }        @After        public void cleanup() {            EasyMock.verify(deviceService);            EasyMock.reset(deviceService);        }        @Test        public void getDevices() throws Exception {            Device firstDevice = new Device(\"iPhone 6\");            Device secondDevice = new Device(\"Nexus 5\");                        List&lt;Device&gt; devices = new ArrayList&lt;&gt;();            devices.add(firstDevice);            devices.add(secondDevice);            EasyMock.expect(deviceService.getDevices()).andReturn(devices);            EasyMock.replay(deviceService);            this.mockMvc.perform(get(\"/devices\").accept(MediaType.APPLICATION_JSON))                        .andExpect(status().isOk())                        .andExpect(jsonPath(\"$\", hasSize(2)))                        .andExpect(jsonPath(\"$[0].name\", is(firstDevice.getName())))                        .andExpect(jsonPath(\"$[1].name\", is(secondDevice.getName())))                        .andDo(document(\"device\"));        }    }    So this is how I managed to get rid of the manual, tedious work and keep my unit tests - and got back to the more serious part of my life: coding like a monkey. =)  PS: All of the code above is checked in at our public github repo, so you are free to clone the working application! You can find it here!"
      },
    
      "domain-driven-20design-2016-02-02-a-decade-of-ddd-cqrs-and-event-sourcing-html": {
        "title": "A decade of DDD, CQRS and Event Sourcing",
        "url": "/domain-driven%20design/2016/02/02/A-Decade-Of-DDD-CQRS-And-Event-Sourcing.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "02 Feb 2016",
        "category": "post, blog post, blog",
        "content": "  Command and Query Responsibility Segregation is the most misinterpreted pattern in software design. CQRS doesn‚Äôt mean eventual consistency, it‚Äôs not about eventing and messaging. It‚Äôs not even what most people believe about having separate models for reading and writing.  In his talk A decade of DDD, CQRS and Event Sourcing on DDDEurope 2016, Greg Young gives us a retrospective over the last ten years practicing CQRS and event sourcing.CQRS?Before I go any further, let‚Äôs start explaining what CQRS really is. It‚Äôs all about applying a design pattern when you notice that your class contains both query- and command methods. It‚Äôs not a new principle. Bertrand Meyer described Command-Query Separation in his book Object-oriented Software Construction as follows:  ‚ÄúEvery method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, Asking a question should not change the answer.‚ÄùWe can apply CQRS principles in many levels of our application, but when people talk about CQRS they are really speaking about applying the CQRS pattern to the object that represents the service boundary of the application. The following example illustrates this.Although this doesn‚Äôt seem very interesting to do at first, architecturally we can do many interesting things by applying this pattern:  One example is that the separation is more explicit and programmers will not find it odd to use different data models which use the same data. Reading records from the database must be fast and there‚Äôs no problem at all if you can achieve this by using multiple representations of the same data.  CQRS is also an enabler for event-based programming models. It‚Äôs common to see CQRS system split into separate services communicating with Event Collaboration. This allows these services to easily take advantage of Event Sourcing.You should be cautious however not to use it everywhere and only in some Bounded Contexts that need it, as everyone agrees that applying the CQRS principle adds complexity.History  ‚ÄúWhen you searched CQRS on Google a decade ago, it thought it was just Cars misspelled.‚ÄùCQRS is not a new concept. You might even say that event sourcing has been around for thousands of years. The ancient Sumerians wrote accounting info on clay tablets and baked them. That document stored events in time. Immutable events. And documents are built up of this event information.As I said earlier, Meyer talked about the principle in his book which was released in 1988.It‚Äôs QCon San Francisco in 2006 which really gave a boost to the popularity of CQRS and event sourcing. Martin Fowler picked up CommandQuerySeparation in his Bliki and after that, things began to grow.CQRS is more of a stepping stone and you have to put it in its historical context. It was a natural consequence of what was happening with Domain-Driven Design at that time. CQRS is not an end goal, it should be seen as a stepping stone for going to an event sourced system.Good thingsThe community around CQRS and event sourcing is growing to about 3000 people. More and more domains are involved with event sourcing. In other domains, other added values were discovered. These people had breakthroughs by practicing CQRS, eg. in a warehouse system, instead of denying a user‚Äôs request because the system couldn‚Äôt handle the requests anymore, it accepts an event and processes it a later time.Another good thing about event sourcing, once you model events, you are forced to think about the temporality of systems: what happens at a specific time? how will this object behave in this situation?Event Storming exercises help you to figure out which domains you have in your system and give you a clear view on the different events. You can then formalize events and commands.Ideas about Event Sourcing have been spreading. Functional programming gained popularity in parallel with event sourcing. Event sourcing is a natural functional model. Every state is a left fold of your previous history.A lot of other things also pushed Event Sourcing forward:  Cloud computing  Popularity of Actor Models  MicroservicesBad thingsSome people see CQRS as a full-blown architecture, but it‚Äôs not. This is wrong. CQRS and event sourcing is not a top level architecture. You cannot build an Event Sourced system. Instead, you end up into building a monolith which is event sourced internally. Event sourcing is simply not a good solution to every problem. For example, once you deal with immutable events, you need to think about corrections to data. Whenever a user corrects a value and hits the save button again, you would need to have an event for that and it would be too complex to handle.A lot of little things are misinterpreted by the community and this caused dogmas to pop up:  ‚ÄúValue objects can be mutable in some use cases‚Äù - It‚Äôs not because Eric Evans once said ‚ÄúValue objects are normally immutable‚Äù that you have to think that in some situations, you can justify mutable objects. There is never an excuse to create mutable objects and they should be avoided at all times.  ‚ÄúThe Write side cannot query the Read side‚Äù - There are times that you have to. When you have an invariant that crosses thousands of aggregates, you cannot avoid it.  ‚ÄúInputs should always equal Outputs‚Äù eg. if i have an order command, an order event should be the result - This is not always the case and there are situations where input and output aren‚Äôt one on one.  ‚ÄúMust use one-way commands‚Äù There‚Äôs no such way as fire your command, put it on a queue and forget. One way commands don‚Äôt exist! They happened in the real world. They cannot be rolled back. Using commands gives you the opportunity to respond to it. One-way commands can however be changed in events in an event sourced system.Over the years some CQRS frameworks have been created. Greg‚Äôs advice is‚Ä¶ Don‚Äôt write a CQRS framework! It will guaranteed be abandoned after a year. It‚Äôs not a framework, it‚Äôs more like a reference implementation. We also need to pull back away from Process Manager frameworks. You can probably solve your problem with an Actor Model.A queue of messages doesn‚Äôt work for all kinds of systems. You can probably linearize in 90% of the use cases, it will also probably be cheaper. For the other 10%, interesting things are happening. We‚Äôre gonna see a push to occasionally connected systems. When you choose availability and high throughput, you‚Äôll have to move to message-driven architectures and linearization is not an option.Future thingsA lot of interesting things are happening in the software world. We‚Äôre growing to N-Temporal systems, where multiple things happen at multiple timeframes.Greg concluded with a quote of Ernest Hemingway.  ‚ÄúWe are all apprentices in a craft where no one ever becomes a master.‚ÄùRecommended readingGreg wrote a book about this matter, called Event Centric - Finding Simplicity in Complex Systems. In this book, he explains how to use DDD with Command-Query Responsibility Separation to select the right design solutions and make them work in the real world.Other sources  Martin Fowler on CQRS: http://martinfowler.com/bliki/CQRS.html  Greg Young on CQRS: http://codebetter.com/gregyoung/2010/02/16/cqrs-task-based-uis-event-sourcing-agh/ and http://www.squarewidget.com/greg-young-on-cqrs"
      },
    
      "domain-driven-20design-2016-01-29-dddeu16-odds-and-ends-html": {
        "title": "oDDs and enDs: Vaughn Vernon  on software projects in peril",
        "url": "/domain-driven%20design/2016/01/29/DDDEU16-Odds-and-Ends.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "29 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  There‚Äôs an interesting situation you will find in many software development projects.Often there is a team dedicated to keep the software alive.The team patches the system and deals with emergencies day after day.Almost every organization is dealing with this kind of situation.Obviously this is not the situation we want to be stuck in, but how do we alleviate ourselves from this?Vaughn Vernon gave a presentation today at Domain-Driven Design Europe.He started his presentation about the odd things that happen in software projects.Then he shed some light on the (future) solutions using Domain-Driven Design,the so called ends of the problems caused by the odds.oDDsWhat are the odd things that happen in software development projects?Cost centersAn insidious problem is that an IT organization within a company is considered a cost center.The business views software as something that costs a lot of money.You can almost say that the business almost wished they did not have to use computers,or employ software developers.What about the company and you?How does the business view you?Do they view you as a hacker?Someone who is thrilled about technology?If that‚Äôs the case, then you may be viewed as a cost center instead of a profit center.Budgets for software development projects are minimalOften a team only contains one senior developer and a lot of juniors developers.The senior developer must keep everything running and moving.Database-DrivenLooking at the business like a database might be a problem.How often does a developer think like this?Data needs to go from a view into a database and out of a database to a view like this.That is how a lot of developers think.We must be careful to be focused so hard on technology and not on business value.Shiny ObjectsSoftware developers are always looking for shiny objects.They want to learn about the latest technologies and work with them.DDD, BDD, TDD, Big Data, Machine Learning, Deep Learning, AI, Reactive, ‚Ä¶How do we justify those things to the business?Big Data was a big buzzword 5 or 6 years ago.It is still a buzzword, but if you‚Äôre not saying Machine Learning as well, you‚Äôre not cool anymore.Are we using technology when it is appropiate?Sometimes the latest technology isn‚Äôt always the correct solution for the problem.A Not-So-Ubiquitous Language  It doesn‚Äôt matter what you name it. It‚Äôs just code.It is very true for the common developer that they think it doesn‚Äôt matter.But it does matter.The business wants to talk about something this way,but the developer calls it differently anyway. ‚ÄúIt‚Äôs just code‚Äù.Poor CollaborationHow many organizations use JIRA as a collaboration tool and fail at it?Often someone spends days writing specifications and creating JIRA issues,yet developers don‚Äôt use them.Estimates are a big deal.Sometimes it takes longer to estimate than to fix the problem.Task Board ShuffleThis is where software design is entirely comprised of moving sticky notes.You move a sticky note from the Todo column to the In Progress column.After we have done this, we run back to our machine and start coding.Without thoughtful design, the code comes out of our fingertips.If you have a team compromised of a few developers working on the same problem,there will be multiple translations in one day of the same thing.Using the same terms is very important, but often neglected.Big Ball of MudMany organizations are deducing a Big Ball of Mud as software architecture.Everything is part of the same namespace and there are no bounded contexts.The software consists of entangled models that should be separate,but they are all in one place.This is the cause of many problems in the industry.You have to recognize a situation when a Big Ball of Mud is being developed and stay out of it.    Business logic is escaping to everywhereBusiness logic can be found in places outside of the core domain.Business logic in persistence logicYou often see business logic inside of persistence logic.Someone is ready to save an object to the backend storage and there is business logic in the persistence logic.The persistence logic is hiding the important business logic.You lose trace of your business logic because of this.Queries in business logicYou see business logic creating decisions by querying the database.Some part of the decision that is being made is hidden to the business logic, because it is inside that query.These queries can also be broken.Sometimes queries are so expensive, they shut down other operations because the tables are locked.Business logic in the UIThe biggest crime against business logic is putting it inside the UI.The business logic is put inside the view template or model instead of the domain model.CRUDCRUD does not work with complexity.It‚Äôs also an insidious problem where software developments teams think they can solve any problem with a database.Anemic Domain ModelThe Anemic Domain Model is one of the most widespread and adapted architectures.Often there is a domain model with objects which are connected with relationships.This all looks nice on paper, but there is no domain logic or any behavior inside these objects!Services live on top of the domain model. This is often called the Service Layer or the Application Layer.They contain most of the domain logic and use the domain objects for data.This is very contrary to object-oriented design.The data and the processes are combined together and it looks very much like procedural programming.This anti-pattern is so common, because most people have not worked with a real domain model.Wrong abstractionsA lot of times developers are thinking too much about abstractionsinstead of getting down to the business.They form a lot of ‚Äúcool‚Äù abstractions that will make it better in the future.  ‚ÄúWhat if we have this sort of situation in the future?If we come up with this kind of abstraction, then this abstraction will take careof the situation in the future.‚ÄùWe cannot predict the future.The future of software is unknown.Coupled ServicesCoupled services are horrible. What if, for example, a REST controller calls a service, which calls another system.If the other system does not respond, you have a gap in your business logic, even if you use global XA transactions.What to do?It really could be that everybody else is normal.What if writing systems with the odd things actually is the norm?If that is normal, then wouldn‚Äôt you like to be the oddball in the crowd?enDsYou want to be the furthest point away from these problems.You want to come up with solutions that work.The business must not view you as a technologist,but as someone who is interested in  the business.You can‚Äôt just keep throwing technologies at the problem.You must come up with beneficial business solutions.Developer maturityIf you are a cost center, then you must come up with a way to make your advancesmore economical. You have to develop your maturity.You have to seek other ways to get the rest of your team to maturity.Urge them to go to DDD and software meetups.Do whatever you can, because you can only benefit if those around you are more mature than you.Passion is something we can‚Äôt always teach.But you must try to work with people who are passionate about their job.Profit centerYou must try to become a profit center.An entire unit of the business is a profit center.You can only become a profit center if you keep adding business value in a timely manner.Collaborate with the businessDon‚Äôt use JIRA to collaborate with the business.You will be amazed what you can learn if you get away from the desk and into a room.The business will tell you what they problems they have hated for years.Use an ubiquitous languageSome things cannot be explained by anybody. ‚ÄúWhy do we call it this? Can we call it this instead?‚ÄùYou can learn those interesting and beneficial details by forming an ubiquitous language.Make it your goal to find that ubiquitous language with a bounded context.Concrete ScenarioShow concrete users in a concrete scenario and what goal that has to achieve.As developers we have to chase after deep models as shiny objects.It‚Äôs not just technologies.Technology matters.Try to experience with deep modeling through an ubiquitous language.You can use the Gherkin language to achieve this.With these concrete scenarios you can model your domain model and test it.Feature: Coffee Machine    Scenario: Buy Coffee        Given there is coffee left in the machine        When I deposit 1 euro        And I press the coffee button        Then I should be served a coffeeUse bounded contextsTo avoid the Big Ball of Mud, you must introduce bounded contexts and separate models.It is equally important to separate the models as it is to introduce core concepts in the core domain.You have to learn about event storming.You can understand what your bounded contexts are from an event storming event.    Metrics-based EstimatesThe artifacts that come out of an Event Storming event, you can use those as estimation units.If you can‚Äôt finish an iteration according what you‚Äôve planned,move these estimation units in a retrospective and encur modeling debt.This modeling debt must be fixed as soon as possible.Know architectureYou must know good architectures, like the hexagonal architecture or CQRS. These architecture solve many of the above problems.They enforce bounded contexts and give the ability to do context mapping.    Decoupled ServicesServices have to be decoupled.A service which calls a peer service directly, is tightly coupled.It cannot work without the other service.What if the other service times out?You can use domain events and messaging systems to fix this problem.MicroservicesThe microservices architecture is another shiny object that a lot of people are chasing.The thing is, they are extremely similar to bounded contexts.Every microservice is master of their own model and usually has one point of access, like an aggregate root in Domain-Driven Design.Actor ModelThe Actor Model is an extremely powerful tool that we need to use in the very near futureby the majority of software development teams.CPU processing power is not increasing, but the amount of cores keeps increasing.The Actor model is a new way to leverage this power because it fully utilizes these cores with threads.    Summary  Many teams are in peril over poorly designed systems  Software development culture is broken  Developers must gain maturity and passion  DDD can be used to make a difference  Use the Actor model to design DDD based microservicesOne more thingVaughn Vernon announced an additional new book called Domain-Driven Design Distilled.It is a 200 page thick book that explains all of the core concepts of DDD.This is very light weight book, intended to rapidly not only teach your team members,but also the business about DDD. This book will be available within a month."
      },
    
      "domain-driven-20design-2016-01-29-ddd-europe-heuristics-from-the-trenches-html": {
        "title": "Heuristics from the Trenches by Cyrille Martraire",
        "url": "/domain-driven%20design/2016/01/29/DDD-Europe-Heuristics-from-the-Trenches.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "29 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  ‚ÄúCommunication usually fails, except by accident‚Äù - Osmo WiioWith this quote of the Finnish researcher Osmo Wiio, Cyrille opened the second day of the DDD Europe conference. Osmo‚Äôs laws of communication are the human communications equivalent of Murphy‚Äôs law. Basically if communication can fail, it will and if a message can be understood in several different ways it is quite possible that it will be understood in a harmful way. With the quote Cyrille immediately wants to stress that with Domain-Driven Design, deep conversations with domain experts and careful attention to the language are key.Business domains are often very complex and hard to get into for individuals not familiar with the domain. The conversations‚Äô game with domain experts is a game that takes many years and many failures in order to get better at. Cyrille explained that, even though it‚Äôs hard, it‚Äôs perfectly possible over time to extract a growing set of techniques, heuristics and best practices to boost the effectiveness of the interviews with domain experts, to learn faster and to converge more quickly to better models.Practices and tricks for talking to the domain experts‚ÄúWhy is it so hard?‚Äù you might ask yourself or ‚ÄúWe don‚Äôt talk the same language as them and they don‚Äôt have time for us!‚Äù. While it is true that the people with the highest expertise within a certain domain often don‚Äôt have a lot of time for interviews or meetings, it is up to us, the developers, to make the necessary preparations prior to seeing them. You should first take some time in order to teach yourself some basic domain knowledge. It all starts with genuine curiosity, successful people are curious about their business domains. You may not believe this but here this will help you too! Without this you will have a bad time! Do your homework: perform the necessary research about the domain on the internet, Google around, read books, check Oasis, ‚Ä¶ Usage of ubiquitous language is very important.Note taking is also a very important aspect in the whole story. You need to be able to take notes like a pro! Learn to take notes effectively. Listen actively and don‚Äôt distort the stories the domain experts tells you. Keep the words as they are. It really is harder than you would think, so you should turn it into a game:  Write down the stories the domain experts tell you  Underline new words and add a definition for yourself, get familiar with all the domain terms  Take note of side remarks  If you think that you‚Äôve encountered a synonym for an existing new word dig into it and ask for more detail  Show your knowledge to the domain user to establish credibility and to challenge them  But‚Ä¶ Challenge them respectfully!All this is Domain-Driven Design!Talking to people is hardIt is not easy talking to people and it will often be hard to have productive conversations. However this is also something you can grow into and for which you can develop the right toolset. Have interactive conversations, that way you have control over the conversation and the way you can steer it. Start with ‚Äúwhat exactly is the goal?‚Äù. Be precise when asking questions, we want to avoid having to reverse engineer the true need from an expressed solution. Be sure to scan the notes you took during a previous conversation and decide where you want the conversation to go next.  ‚ÄúI keep developers out of conversations about the domain because they always want to know ‚Äòwhy this‚Äô and ‚Äòwhy that‚Äô. Just write your code.‚Äù - The Expert BeginnerYou should question everything: ask why but don‚Äôt go too far!Combine Domain-Driven Development with Behaviour-Driven Development. Both go hand in hand to interview domain experts. You want to discover the ‚Äúunknown unknowns‚Äù as early as possible and to avoid misunderstandings as often as possible. Be sure to ask for concrete examples and genuine sample documents and data and although this doesn‚Äôt always come easy, ask and insist but as mentioned before know your limits and be sure not to push it.People always think that talking abstract is faster and will save time but think about it and ask yourself: ‚ÄúIs it truly faster if we were wrong or missed stuff that matters?‚Äù. If the domain expert seems hesitant about something also take note of this to take into account that the feature in question might be eligible for change later on, this way we can model our software design correctly.The domain expert delusionYou might assume that the person, you‚Äôre having a conversation with, is an actual system expert within the domain but chances are that he/she is not. The worst expert is the one whose expertise was built from the intricacies of the existing systems. It is therefor also our duty to help out the domain expert where possible:  Have empathy, try to put yourself in their shoes  Build a partnership, it should be a two-way conversation you‚Äôre having with the expert  Make it clear that the domain expert is always safe with you, that you have no plan to steal their job  Propose things, it doesn‚Äôt always have to be right but this way you will get somewhere  Look for invariants, for example: ‚ÄúIs there any other outcome that is also important?‚Äù  Always ask for validation of everything, even if you‚Äôre sure about itIt is common in companies that businesses often don‚Äôt want all power concentrated into the same hands. This also accounts for domains, you should therefor assume and probe. Get to know the business you are dealing with and their mentality. Not that if you happen to discover that there are multiple domain experts the situation gets a bit more tricky. Having multiple domain experts may lead to more confusion and makes it even more important asking for validation and challenging the experts. Something we want to achieve is that we want to suggest features from our code which could be very useful for the business. Instead of having software to support the domain we want to have software augmenting the domain.Keep improving yourselfGrow into it and build your own toolbox for implementing Domain-Driven Design. You might ask yourself why you should bother so much with all of this, after all we just want to get to it and write code. This however is a wrong approach because the complexity of the domain is there, it is just hiding behind a wall, you just don‚Äôt see it yet. You will discover the complexity sooner or later so you may as well want to get into it as early as possible to save precious time. After all, you want to become a domain expert too!Other useful resources mentioned by Cyrille:  Conversation Patterns for Software Professionals by Michael Bartyzel  Analysis Patterns by Martin Fowler  Living Documentation by Cyrille Martraire  Slides: http://www.slideshare.net/cyriux  Blog: http://cyrille.martraire.com  Twitter: @cyriux"
      },
    
      "domain-driven-20design-2016-01-28-evolution-of-ddd-html": {
        "title": "Eric Evans about the evolution of Domain-Driven Design",
        "url": "/domain-driven%20design/2016/01/28/Evolution-of-DDD.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "28 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  Why is it that DDD only has its own conference after 13 years? Why is this becoming a sudden hype? Why does it gain popularity and is it mentioned so many times in microservices presentations? Eric Evans talks on DDDEurope about the core idea behind Domain-Driven Design and its evolution over the last few years.But what is DDD?The subtitle of Evans‚Äô book, Tackling Complexity in the Heart of Software, bundles two core principles of Domain-Driven Design:  It describes the process of translating complex real-life problems into software  The heart of software entails the domain that we‚Äôre working onKey in this activity is finding the core complexity in the critical part of the domain and focus on this and only this piece of complexity. Software developers and domain experts collaborate to develop models, simplified representations of the real-life problem. The written software should eventually explicitly reflect the model. Whenever a brainstorm session occurs, it almost always results in an adaptation of the models within the software.When we encounter multiple complex problems, we must think about them separately. Each problem requires its own model representation.When discussing with others about the domain, we must speak a ubiquitous language. You should use the same vocabulary for describing the problem you‚Äôre solving. However, when somebody asks you the meaning of a word, in many cases you have to ask the person: ‚ÄúIn what context is it used?‚Äù. That‚Äôs why the language only means something within a well-defined bounded context.Domain-Driven Design is more like an attitude. Although it gives us principles and terminology to enable talking about it and have discussions, different people will do things differently. Each approach will be slightly different.Bounded contexts?A bounded context is an important principle when applying Domain-Driven Design. As i said earlier, language in itself doesn‚Äôt mean anything. It only means something when it‚Äôs used within a certain context. eg. Item can be a Stock Item, Sale Item, ‚Ä¶Bounded contexts have the following characteristics:  Within a bounded context certain rules apply, eg. validation rules  It needs to be tangible in the software, eg. use packages for each contextAnother benefit of working with separate bounded contexts is that separate teams could take responsibility on separate bounded contexts.ConclusionWhen Erics book was released in 2003, only Java 5 and J2EE were used as a programming language for implementing projects. We only had EJB‚Äôs to solve problems and there was no other way of storing data but with SQL. If the technology is so complex and limited to implement something, you can‚Äôt focus on the real problem of software design.Nowadays, we have a lot of new tools available to implement a project: We can store data with a NoSQL database or not store it at all and keep it in-memory. We can explore other ways of approaching data with eg. event sourcing. On certain levels, Spring makes the technical aspect of writing software components a breeze. With the upcoming of microservices and each microservice having its own database, bounded contexts are much clearer to the teams working on the software. And there are probably tons of other examples on how today‚Äôs tools can help us achieving our goal: write good software.Better tools and a vivid community which masters these tools cause Domain-Driven Design to become more and more popular in ways of designing software. So maybe we can do better now than back in 2003. Maybe‚Ä¶ Or maybe we‚Äôre not there yet. Fact is that everyday we are learning from mistakes in the past to do better in the future."
      },
    
      "devoxx-2015-11-09-devoxx15-docker-kubernetes-html": {
        "title": "#Devoxx Arun Gupta talks Docker",
        "url": "/devoxx/2015/11/09/Devoxx15-Docker-Kubernetes.html",
        "image": "/img/docker.png",
        "date": "09 Nov 2015",
        "category": "post, blog post, blog",
        "content": "DockerDocker is a tool used for container creation for software applications. We have all been aware of the existence of containers for some time, but Docker creates a standard for describing these packages.Docker is used for three things: Build, Ship &amp; Run your software.BuildCreation of a predefined container in a standardized way using the Docker CLI.Use a dockerfile containing a list of commands. The FROM command specifies an OS and additional software packages, eg. FROM jboss/wildfly. All  commands are compressed into one, customized image using the Docker CLI.ShipShare the container via DockerHub or your private repository.  Sharing = caring!RunDocker runs on a minimal operating system and uses the Union File System. On the bottom level, there‚Äôs the Bootfs kernel, on which the chosen base image or OS runs and finally, the user images ontop of that.Hosts running Docker are very environment variables oriented, so by using variables in the commands or on the machine itself you configure your application. Any other communication is usually done over HTTP/REST. The Docker images are stored on the Docker host so the actual client is very thin. A Docker app runs on the Docker engine; this is in contrast with regular VMs, running on full-blown operating systems.Docker MachineDocker Machine allows you to get started with Docker on Mac or Windows. It features the docker-machine CLI and uses the boot2docker image (32Mb small) under the hood.  Docker Machine is preferred over boot2docker for development purposes, but it is not production-ready (yet!)Easy way to set up a Docker host with docker-machine:docker-machine create --driver=virtualbox myhostListing all the installed Docker images:docker-machine lsListing all the environment variables of a newly created Docker container:docker-machine env myhostDocker Machine is also used to start, stop or restart containers. It even allows to update Docker itself.Many existing plugins provide support for various cloud platforms.Boot2DockerAn earlier version of docker-machine. As said above, it is being used by docker-machine under the hood.  My advice: migrate to docker-machine, at least for development purposes.Docker ToolboxEasily the best tool to get started with Docker.  Windows  MacOSHands-on Docker  docker helpfor all your docker needs!  docker psto check the running containers  docker imagesto check your images  docker buildfor quick build  docker run -it ubuntu sh for quick running an image in a shell  Docker images are like diapers: if they get shitty, throw‚Äôem away and take a fresh one.Docker ComposeAllows you to define and run multi-container applications. It has all the commands the regular Docker has and more.It provides a new way to link containers.Configuration is defined in one or more docker-compose.yml (default) or docker-compose.override.yml (default) files.It is a great tool for setting up Development, Staging and Continuous Integration (CI) environments.  Docker container linking is so pass√©A problem with container linking was that there was no possible way to work with different hosts. Docker Compose solves this by using volume mapping.It can help with running multi-host environments:  Bridge network span single host  Overlay network spans multiple hosts  Software defined networking is possible and preferred! Docker Compose solves this problem but it should still be used cautiously in production!Starting a set of Docker images using Docker Compose is easy:docker-compose up -f docker-compose.yml -f production.yml -dDocker SwarmDocker Swarm provides native clustering for Docker, fully integrated with Machine &amp; Compose. It either uses Etcd, Consul, Zookeeper or other solutions to store the cluster ID.Whenever you create a Docker Machine, you can add it to the cluster. It also serves the standard Docker API so anything that works on Docker, will work on multi-host environments.  They say the new Docker Swarm v1.0.0 release is production ready: I still have my doubts!References  Docker Docs are the de facto standard reference and are very well documented. They contain information on Docker, Machine, Compose &amp; Swarm: https://docs.docker.com/  Samples: https://github.com/javaee-samples/docker-javaQuestions or Remarks  Contact @arungupta  or @Turbots on Twitter  Create an issue or start a discussion on the Github repository (or on Gitter)"
      },
    
      "microservices-2015-10-13-microservicespatterns-html": {
        "title": "Applying software patterns to microservices",
        "url": "/microservices/2015/10/13/MicroservicesPatterns.html",
        "image": "/img/jax2015_logo.jpg",
        "date": "13 Oct 2015",
        "category": "post, blog post, blog",
        "content": "  During the week of October 12th, my colleagues Andreas Evers and Tim De Bruyn and me attended JAX London 2015. After attending over a dozen talks in three days, we went home with tons of insights about DevOps, microservices, Continuous Delivery, Spring Boot and much more. This is a write-up of Chris Richardson‚Äôs talk A pattern language for microservices.Chris Richardson is the author of POJOs in Action and is founder of the original CloudFoundry, which was later acquired by VMWare and then SpringSource. Nowadays he is constantly thinking about microservices and founded a startup that is creating a platform for aiding the development of event-driven microservices (http://eventuate.io/). He also launched http://microservices.io/, describing microservice responsibilities and commonly accepted solutions as patterns.Problems in software engineeringChris started his presentation by pointing out a few problems in software engineering. First, we have a lot of sucks/rocks discussions between developers. JavaScript vs. Java, Spring vs. JEE, Java vs. .NET, functional programming vs. object-ori√´nted programming, containers vs. virtual machines, ‚Ä¶ Sounds familiar? But these discussions are usually very subjective and shallow. Back in 1986, Fred Brooks already mentioned in a paper on software engineering that there is no such thing as a silver bullet. So there is no right or wrong answer to which language or framework is better, it depends on the situation.A second problem is that we have a lot of new technologies these days. Typically, these technologies go through the Gartner Hype Cycle.At first, people discover an innovative technology and everybody wants to use it, which drives the technology into the Peak of Inflated Expectations. Docker is a good example of a technology in this phase. Then a huge drop follows, because people didn‚Äôt really understand the technology and misused it. When we start to understand the subject, that‚Äôs when productivity on the market increases.It‚Äôs clear that we need a better way to discuss and think about technology. That‚Äôs where software patterns come in.Pattern languagesPatterns help us to describe a reusable solution to a problem occurring in a particular context. The use of pattern languages is a a great way of talking about technology in general. You can see it as an advice around a topic. Describing what you want to solve and its context is much more important than the framework or tool you choose.A pattern description typically contains:  Pattern name  Context  Problem - The issue which we try to solve  Forces, which are a set of indicators why we want to use the pattern, eg. we need to do CD, run multiple instances, ‚Ä¶  Solution. What would a pattern be without a solution?  Resulting context. Set of both benefits and downsides which we achieved, but also problems which we then have to solve next.Patterns can be related, they can be alternate solutions, solutions to problems that were caused by another pattern or more specific solutions to a certain problem. When you want to read more about writing and understanding patterns, you can read Martin Fowler‚Äôs blog post Writing Software Patterns.Patterns for the microservices worldMicroservices is another one of those new technologies which are in Peak of Inflated Expectations phase. To help us understand the complexity of implementing these kind of systems, Chris founded http://microservices.io/ . Here you can find a collection of microservices patterns.We can group the microservices patterns into several categories:  Core patterns  Deployment patterns  Discovery patternsChris then elaborated on a few microservice pattern categories.Core patternsMonolithical applications tend to be simple to develop, test, deploy and scale. You can just run multiple copies of your monolithical application. When the application is large however, you end up in monolithic hell:  Millions of lines of code undermine developer productivity and knowledge of the system.  As one change might affect other parts of the application, there‚Äôs a fear of changing and refactoring code  Developer productivity decreases, as your IDE gets slower, startup times of the server take very long, ‚Ä¶  Long-term commitment to a technology stackWith X and Z axis scaling, you increase the number of application instancess or you increase server resources. Y axis scaling on the other hand means you break up your application into separate microservices which group business functionality. Some benefits to this are:  Smaller, simpler apps, which are easier to understand  Less classpath hell  Faster to build and deploy  Improve fault isolation  Eliminates the long-term commitment to a single technology stackOf course, each solution always has some drawbacks, to which fortunately solutions exist.  Added complexity of developing a distributed system  We have to handle partial failures  Implement business transactions that span multiple databases  More complex testing: what do we do with transitive dependencies? Do we mock them?  What about managing communication for cross-service development and deployment?Deployment patternsForces to consider when deploying microservices are:  Variety of languages  Building and deploying must be fast  Isolate service instances  Deploying must be cost-effectiveFor these, you can look at the deployment patterns.  Service per VM = Packer.io is a great tool for running each servicein its own VM. Downside is you got the overhead of a whole VM permicroservice. It is very expensive and the deployment itself isrelatively slow. A positive thing though is that the AWSinfrastructure is very mature and reliable.  Service per container = Each service is in its own Docker container, which is started very quickly. A drawback is that these technologies are still very immature.Discovery patternsOne problem we need to address around the area of service discovery is that we need to know the IP address of the server. Simply having configuration files with the IP‚Äôs wont work anymore. On top of that, the set of API interfaces can change. And this is just a tip of the iceberg.There are several patterns related to service discovery:  Client-side discovery = The client will query the service registry,pick one from the load balancing configuration, and then use it.Netflix‚Äô Eureka and Ribbon provide this functionality. MultipleEureka‚Äôs can be clustered.  Server-side discovery = At some level it‚Äôsthe same like client-side discovery. The difference is that the    client makes a request to the router, which then queries t he service registry. You can achieve this with eg. Nginx as the router and an Elastic Load Balancer from AWS. The advantage is that the client code is much simpler. Advantage is that it‚Äôs built in in some cloud/container environments, such as AWS ELB, Google‚Äôs Kubernetes, Marathon, ‚Ä¶ConclusionOver the years, companies like Netflix, LinkedIn, Soundcloud and many others have applied the microservices architecture in their software landscape, with several tools and open-source libraries as a result. But deep down these tools have to tackle the same problems. Chris‚Äô effort to describe these common problems and solutions in software patterns allows us to see the wood for the trees again. Because as I said earlier, knowing what you want to achieve is much more important than the framework or tool you choose."
      },
    
      "join-2015-09-15-join2015-html": {
        "title": "JOIN 2015 from a developer's perspective",
        "url": "/join/2015/09/15/JOIN2015.html",
        "image": "/img/join2015.jpg",
        "date": "15 Sep 2015",
        "category": "post, blog post, blog",
        "content": "  Last week the Oracle/Java unit held its yearly JOIN Event. The purpose of this event is to share knowledge between colleagues and fellow Java- and JavaScript enthousiasts. A total of 83 attendees visited Ordina Belgium‚Äôs headquarters in Mechelen to pack their heads with interesting technology facts.We started off with a couple of Ordina consultants, each giving a lightning talk (a 20 mins. talk) about a hot topic. Amongst others, there were talks about Docker, Polymer, IoT, microservices and Meteor. Afterwards everyone had the chance to attend three keynote talks from international professionals in the Java- and JavaScript world.The past, present and future of ECMAScriptTom Van Cutsem told us about the past and the future of JavaScript and how the ECMAScript standard was born. Tom is a member of the TC39 board, a group of technical people who decide which feature is in or out. There‚Äôs no political game going on in this group, which is a good thing. JavaScript was invented in 1995 by Brendan Eich, who worked at Netscape at that time, and quickly standardized as ECMAScript 1st edition in 1997. ECMAScript 5 was released in 2009 and is well supported by all major browsers, as the following compatibility table illustrates: http://kangax.github.io/compat-table/es5/. By introducing strict mode, which makes you as a developer use a more safer and sturdier JavaScript version, the guys at TC39 guaranteed the expansion of ECMAScript 6.This summer a brand new version was released: they did not call it ECMAScript 6 this time, but rather ECMAScript 2015 because they changed their release model to yearly releases. Features that aren‚Äôt ready for the release, will be skipped and released a year later. Tom then shortly introduced a subset of the new features available in ECMAScript 2015: Arrow functions are a shorter way to declare anonymous functions, a bit comparable to Java 8‚Äôs Lambdas. Second he explained probably the most important feature‚Ä¶ classes! Actually it‚Äôs just about syntax, because they will be treated like regular functions by the engine. Most notable control flow features were promises and iterators. On the website mentioned before to check ECMAScript 5 compatibility, you can also switch to ECMAScript 2015 and see that browser compatibility is still poor. However, Tom also recommended that you can already use ECMAScript 2015 in your application by using compiling tools like Babel or TypeScript. These tools compile your ECMAScript 2015 code into ECMAScript 5. He also briefly mentioned Nashorn, the new Javacript engine developed by Oracle, released with Java 8.You can have a look at Tom‚Äôs presentation on http://t.co/Phwpx3Ig13.Boo(s)t your app developmentSt√©phane Nicoll enlightened us with a Spring Boot talk. He ex‚Äãactly had one slide, but blew us away with a full-packed demo of what Spring Boot‚Äôs auto-initialization features can mean for your project. Starting point is https://start.spring.io/, were you can kickstart a Spring project by checking and unchecking technologies. It‚Äôs as simple as that. When you‚Äôre working with IntelliJ, you can even do it inside your IDE with a wizard. Both result in the same project however.After creating a simple @RestController with a @RequestMapping and a Hello world, ‚Äãhe added the JPA dependency, created an @Entity and a Spring Data repository. Now we only have to add a database to the project. Just by adding the H2 dependency in the pom.xml file, Spring Boot creates a database for you and attaches the created Repository classes to that database instance. It is able to do this by scanning for DataSource classes on the classpath.Actuator endpoints allow you to monitor and interact with your application. Spring Boot includes a number of built-in endpoints. For example, Spring Boot can automatically create a health status endpoint where you can check the health of your database, JMS queue or any other component that is registered with the Spring Boot system. You can even write your own.He also deployed the application to Cloud Foundry, Pivotal‚Äôs cloud platform. He demonstrated the possibility to remotely install another database on that server and bind this database instance to the application. Then he even demonstrated hot code replacement in the cloud‚Ä¶ That‚Äôs really amazing!  We can conclude that Spring Boot makes Java development as it should be. By following the convention-over-configuration approach, we can achieve very much in very little time.A sneak peek at the new Angular 2.0Finally, Pascal Precht from Thoughtram gave some insights on Angular 2. Attention! You shouldn‚Äôt say AngularJS 2, but simply Angular 2. Pascal gave us a quick tour of Angular 2‚Äôs new syntax for property- and event binding. You will need to use square brackets around HTML attribute names for property binding and parenthesis for event binding, which looks a bit weird at first. You can read more about Angular 2‚Äôs syntax in his blog post Angular 2 Template Syntax Demystified.Angular 2 will also support Web Components, a new standard in developing custom components for web applications.On the question whether Polymer and Angular 2 weren‚Äôt tackling the same problems, Pascal replied that Polymer focuses more on Web Components whilst Angular 2 claims to be an end-to-end framework to build applications.Actually, at first the syntax seems frightening, but after hearing the reasoning behind it, it seems to me that the only difference is that Angular will embrace the standard DOM element properties instead of keeping their own in sync like in the previous version‚Ä¶ And that‚Äôs why you could say that we don‚Äôt have two-way databinding anymore in Angular 2. Interesting things, but we‚Äôll have to wait until 2016 to see the final syntax, because everything we saw‚Ä¶ can already be different as we speak.‚ÄãOn the question when Angular 2 would be production ready, he opened up his browser and opened Is Angular 2.0 ready?. That tells enough. Pascal‚Äôs feeling is that a beta version will be released Q1 2016, but this was a non-official statement."
      },
    
      "spring-2015-05-09-springio15-general-html": {
        "title": "Spring I/O 2015 Barcelona",
        "url": "/spring/2015/05/09/SpringIO15-General.html",
        "image": "/img/springio.jpg",
        "date": "09 May 2015",
        "category": "post, blog post, blog",
        "content": "  Last week Barcelona was the place to be for Spring enthusiasts. With tons of Pivotal speakers and many more community leaders it was a two-day goldmine for anyone looking to update their Spring knowledge. This is my report ranging many different topics, including quite some one on one discussions and hacking sessions with the people behind the Spring ecosystem.Sergi Almar (event organizer), me and Josh Long (Spring developer advocate)One on one talk with Juergen Hoeller:Groovy has now become an Apache Incubator project, as Pivotal decided Groovy isn‚Äôt a core project for them. Most of the people behind Groovy are not working for Pivotal anymore but for other companies which means they can only work on it part-time. Juergen doesn‚Äôt like this but it wasn‚Äôt his decision. Luckily Groovy has a huge userbase and they truly love the language. That‚Äôs why Juergen is not too concerned about Groovy dying, but of course the speed of new developments will be a lot slower than the past two or three years.Gradle has strong dependencies on Groovy, as it‚Äôs using a Groovy-based DSL language. The story is different here however, as Gradle is backed by Gradleware, a Silicon Valley company with its own vision. Spring is using Gradle for most of its projects, because for one the devs like to use Groovy, but foremost because it offers more flexibility compared to maven which is necessary when dealing with an open-source framework.The Netflix stack is Amazon based as it‚Äôs the most important player in the market at the moment. Of course there is support for Cloud Foundry but there are no guarantees it will work on all clouds. Especially Google App Engine is kind of a mess as the team behind it doesn‚Äôt really cooperate with Pivotal or anyone else as far as Juergen knows.Currently it‚Äôs hard to find decent books about microservices but they should be coming up soon. For cloud there are some books out there but they could feel outdated already. What‚Äôs written in 2013 isn‚Äôt always completely valid anymore in 2015, especially in a field like Cloud computing.One on one hacking session with Oliver Gierke:Lots of stuff can still be moved from Spring Data REST to Spring Hateoas. This afternoon I‚Äôm sitting down with Oliver Gierke to do some hacking on the subject.It‚Äôs possible to have a resource with many different embedded resources inside by nesting domain POJOs in eachother. The thing Spring Data REST is missing is the possibility to distinguish between domain model nested classes and resource model nested classes (aka embedded resources). There is no way to embed a Car resource as an embedded resource into a Person resource without actually having Car as a property of Person in the domain model. Having the possibility to manually add embedded resources to a resource would solve this.To achieve this we should have an extension of ResourceSupport with a Set of EmbeddedWrappers inside. Using the EmbeddedWrappers class, we can add embedded resources to our resource in its Assembler. This Wrapper will take care of relation resolving, especially handy when dealing with collections which require plural-forms. The relation value can be annotated in the model or passed along as a second parameter.I will go into more detail with examples as a comment on the GitHub ticket about embedded resources which Oliver will try to follow up. An existing stackoverflow issue about the subject can be found here: http://stackoverflow.com/questions/25858698/spring-hateoas-embedded-resource-supportHaving different representations of the same resource with different fields (e.g. summary and full view) can be achieved using jsonViews. However, those jsonViews can be used for versioning as well, and having both at the same time could interfere. The Projection abstraction is a very nice feature of Spring Data REST that was moved to the Spring Data Commons package. This enables us to use this abstraction without the need of a persistence. We can use it in conjunction with Spring Hateoas without Spring Data REST.The main class to use is the SpelAwareProxyProjectionFactory. This factory can be used to create projections. It‚Äôs also possible to use the Page functionality and especially its page.map() function, which can link a projection interface with a domain class. This approach allows us to define an interface with the selection of fields from the domain class as getter methods. This defines the selection of fields which that projection should expose.This pattern is applied as well in the latest Spring MVC where a UserForm is used as a parameter of a POST endpoint method. Defining the UserForm as an interface works exactly by the same principle as the Projections of Spring Data. You can even have default methods in the interface for validation of that form.At my client I am integrating most of the stuff you can find in foxycart‚Äôs HAL browser but in a dynamic way. Exposing a graph of resources can be done using Spring Restdocs‚Äô link documents generated in asciidoc. By parsing the results of these asciidocs and merging the results, a resulting json is aggregated and used to generate a graph of resources and their links using d3js. This graph is integrated in the HAL browser, and each resource links to its documentation. That documentation per resource can of course also be reached from the regular HAL browser documentation links using curies. To generate that documentation we are in turn using Spring Restdocs to show examples with their links, request and response fields and error scenarios. All of this is generated and guaranteed up-to-date (if it weren‚Äôt, it would have failed the build). The improvement which is still possible to do here, is to have our own hook into Spring Restdocs so we wouldn‚Äôt need to go through asciidoc links to generate the full relationship graph. Oliver asked me to open a ticket for Andy Wilkinson to allow hooks in the generation model.Oliver doesn‚Äôt fully agree with versioning on resource level. He prefers versioning APIs or not versioning at all, to avoid having a higher cost later due to legacy and lots of old versions we‚Äôll need to support. The initial win of versioning would be insignificant compared to the technical debt it creates.Implementation of the hypertext cache pattern of HAL is quite straight forward according to Oliver. The client should be smart enough to search for the field it needs in the embedded resource, and if that field isn‚Äôt there, he should follow the link to the full resource. Keeping track of which representation is shown in which place (embedded vs linked) becomes unnecessary using that approach.More on Spring I/O 2015 Barcelona ‚Ä¶.Boot your Search with Spring talk:Speaker: Christoph Strobl - Talk &amp; speaker descriptionSolr feels like an old kitchen sink for anything you want to do. Not exactly a fancy 2015 tool. They are catching up though and documentation is getting better. It‚Äôs scheme based. MongoDB does much more out-of-the-box which you have to do manually with xml configuration. Solr schemaless support exists but as long as it‚Äôs lucene-based, there‚Äôs no such thing as a schema-less index. Their type-guessing only goes so far until you try to add a record with a different type.Spring Data Solr does just what you expect: clean to-the-point interfaces with annotations that do the DAO magic for you. Spring Data Elasticsearch does that as well for the complexity of Elasticsearch. I never really liked the query system that Elasticsearch has so having this abstraction layer could prove really useful.Inside http://spring.io - a production spring reference application &amp; one on one talk with Brian Clozelon this blogReal-time with Spring: SSE and WebSockets talkSpeaker: Sergi Almar - Talk &amp; speaker descriptionSpring WebSockets is better than JSR356 because: there is a fallback with SockJS, there is support for STOMP subprotocol, Spring Security can jump in, and of course flawless integration with messaging components and the Spring messaging style. Security is important because there are no URLs anymore. We have to secure at message level.Spring Data REST - Repositories meet hypermedia talkSpeaker: Oliver Gierke - Talk &amp; speaker descriptionRecommended reading: Domain Driven Design. Although very boring, it introduces vital concepts in the repository world. When combining ALPS and JSON Schema, it should be possible to create a client that is smart enough to discover verbs and even fields of the payload.Building Microservices with Spring Cloud and Netflix OSS talkSpeaker: Dr. Dave Syer  - Talk &amp; speaker descriptionAnother great book is Release It!. It describes a lot of the patterns microservices use such as circuit breaker. It‚Äôs definitely a great book for devops.Master Spring Boot autoconfiguration talkon this blogCan Your Cloud Do This? Getting started with Cloud Foundry talk &amp; Building ‚ÄúBootiful‚Äù Microservices with Spring Cloud workshop &amp; One one one talk with Josh Longon this blogEnjoy reading!"
      },
    
      "spring-2015-05-08-springio15-sagan-html": {
        "title": "A production Spring reference application &amp; One on One talk with Brian Clozel",
        "url": "/spring/2015/05/08/SpringIO15-Sagan.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Sagan is the name of the Spring.io website. It‚Äôs built by Pivotal Labs and maintained and extended by Brian Clozel. The project uses a best-of-breed set of tools. In Brian Clozel‚Äôs talk he sheds a light on which tools are used for which reasons. After the talk I sit down with Brian to discuss some more details.Sagan: A production Spring reference applicationSpeaker: Brian Clozel - Talk &amp; speaker descriptionSagan is the name of the Spring.io website. It‚Äôs built by Pivotal Labs and maintained and extended by Brian Clozel. The project uses a best-of-breed set of tools. Of course it uses GitHub as code repository and issue tracking. But GitHub can become a bit confusing and unclear when a lot of issues need to be tackled and tracked. Sagan uses Waffle to link GitHub issues and commits to scrum &amp; kanban practices. Formal communication goes through GitHub issues, and informal conversations are held through HipChat. Travis is used for continuous deployments. Asciidoctor is used for the guides on the website. They are stored in a GitHub repository, fetched by the website and rendered appropriately.Sagan is using a Gradle plugin which is triggering Green-Blue deployment. It calls Cloud Foundry to see which clone is active (green or blue) and deploys on the non-active one. Once deployment is done, the routing is switched automatically to the newly deployed one. In the short moment where the switch occurs, both clones are being routed to avoid a brief moment of downtime.Cloud Foundry takes the console log output of each application and aggregates everything. Either the result is exposed using webockets or it‚Äôs bound using a service that can show the logs as well as persist them. Redis is used in conjunction with Spring Session. This creates distributed session management for the cloud. Whenever the database needs to be updated, versioned FlyWayDB scripts are used. The upgrades could be small changes where local tests are sufficient, or a staging environment is used for testing.The slidedeck of this talk can be found here: https://speakerdeck.com/bclozel/spring-dot-io-1One on one talk with Brian ClozelThe Netflix guys have a strong implementation of Conway‚Äôs law. The organisation mimics the architecture and vice versa. This closely relates to the microservices pattern, where each microservice can have the best tools for the job, and those can definitely be different for each microservice. In Netflix this applies to teams as well, resulting in different approaches suggested by different people.Regarding database evolution Brian suggests the Netflix guys could have some good ideas about handling backwards-compatibility breaking evolution in a green-blue deployment with zero downtime. In any case there are many cases where teams use feature-switches to make certain user interactions read-only for a short time during migration. This avoids having transactions that have to be forcibly destroyed. After successful deployment the feature is turned on again ensuring the user has a seamless experience.Brian will check with the Netflix guys and let me know what they do for database migration"
      },
    
      "spring-2015-05-08-springio15-microservices-html": {
        "title": "&quot;Bootiful&quot; Microservices in CloudFoundry &amp; One on One with Josh Long",
        "url": "/spring/2015/05/08/SpringIO15-Microservices.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Spring Boot, Spring Cloud and CloudFoundry: a perfect match. Josh Long explains how to build Spring Boot microservices, deploy them in CloudFoundry and manage them using the Netflix OSS stack through Spring Cloud. Including a One on One talk.Can Your Cloud Do This - CloudFoundry talkSpeaker: Josh Long - Talk &amp; speaker descriptionIn Amazon there‚Äôs a good chance you‚Äôll encounter ‚ÄúAMIs‚Äù. These are basically virtualizations with an operating system, and is perceived as a container. These containers need to be disposable. The moment you remove one, another should be ready to jump in. The idea is to treat your servers as cattle, not as pets. The moment you know the name of a specific server, it‚Äôs as if it‚Äôs your pet. And you don‚Äôt want pets because they‚Äôre not disposable.When choosing a cloud platform, it‚Äôs important to avoid vendor lock-in. At Google AppEngine, there was a huge community developing applications for the platform before it was even in GA. Once they got there, Google raised the prices significantly. The problem was however that all those applications were using Google-specific APIs and were really tightly coupled to the Google infrastructure. For most companies it was no longer viable to use Google‚Äôs platform without incurring debts and were unable to quickly move to another platform. Josh compares it nicely with Hotel California: ‚ÄúYou can always check-in, but you can never leave‚Äù.It‚Äôs possible to deploy a jar to Cloud Foundry, but also a war. The war will be automatically wrapped in a container (using Warden, the linux container, which makes it similar to a docker image), but you can also push your own docker image with your war inside.Once you pushed your application into Cloud Foundry, you have to link it to backend services, such as a database. Doing this is child‚Äôs play. One cool backend service in particular is the logging service. You can use Splunk or Papertrail, which you can bind to e.g. your own account on the Papertrail site.An older version of the slidedeck of this talk can be found here: https://speakerdeck.com/joshlong/scaling-with-spring-and-cloud-foundryBuilding ‚ÄúBootiful‚Äù Microservices with Spring Cloud (Workshop)The configprops actuator endpoint is especially useful to figure out what properties are available for certain functionalities. It beats debugging and is a pretty useful form of documentation. For more information check this out: http://docs.spring.io/spring-boot/docs/current/reference/html/howto-properties-and-configuration.html#howto-discover-build-in-options-for-external-propertiesThe configuration server supports configuration which is common to all microservices. If the configuration.yml is inside the resources folder of the configuration server itself, it‚Äôs used only as configuration for the configuration server. But if the server finds an application.yml inside the distributed configuration location (e.g. Git), then those configurations are shared for all other yml files, albeit with lowest priority.The Cloud Configuration Bus is interesting if you would like to have a distributed refresh of the configuration of your microservices. Basically instead of POSTing to http://yourmicroservice/refresh, you‚Äôd call http://yourmicroservice/bus/refresh on any clone and Spring Cloud Configuration Bus will forward the refresh using AMQP to all the other clones. This way you don‚Äôt need to call a refresh on each node separately.It‚Äôs also possible to poll for changes instead of pushing them. Simply inject the refresh endpoint as a bean and annotate it with the @Scheduled annotation.Since the 1.0.1 release of Spring Cloud, it‚Äôs now even possible to have the client microservice wait and retry until a configuration server is registered. (https://GitHub.com/spring-cloud/spring-cloud-config/issues/129)A cool new annotation is @SpringCloudApplication. It groups the following annotations: @SpringBootApplication, @EnableDiscoveryClient and @EnableCircuitBreaker.The high availability principle in Eureka is considered fulfilled when at least two Eureka instances are registered and peer aware in each zone.Ribbon, the client-side load balancer of Netflix, is integrated nicely into Spring‚Äôs RestTemplate. When EurekaClient is enabled on the microservice, Ribbon will be able to resolve the following command: restTemplate.exchange(‚Äúhttp://myservice/books‚Äù, ‚Ä¶). It will look into Eureka for an application called myservice, and route to the appropriate server.The slidedeck of this talk can be found here: https://speakerdeck.com/joshlong/the-bootiful-microserviceOne on One talk with Josh LongIn many cases there are different access rules for different environments. This applies in a great deal on configuration as well. Most guides showcase one configuration server with all properties for all environments into a single repository. When facing more strict policies, there is no reason why you can‚Äôt have a configuration server per environment, or at least for production a dedicated one. That server could have different access rules. It‚Äôs an easy solution worth trying out.When you have both public and private microservices, Josh prefers to do the extra security for the private ones on the microservice level instead of in the gateway. It‚Äôs important to trust the developers, especially in a devops culture which is a requirement in a microservices architecture.At the company I work, management fears that the developers will introduce security issues, so the enterprise architects are looking into more governance-minded solutions. One of those solutions is having a full blown ESB as the gateway. Such an ESB would require you to register new endpoints - in case of REST these would be resources with their allowed verbs - and different types of handlers and interceptors. This is a very process-heavy solution where the microservice would have to request access through the ESB for every new resource, or verb on a resource, probably to another team in charge of the ESB.This goes completely against the microservices principles. The ESB becomes a bottleneck and increases the time-to-market. It also becomes a heavy single point of failure with lots of logic inside. Josh makes a fitting comparison with conductor versus choreographer. If your gateway is an ESB it acts as an conductor. When the conductor goes offline, the entire orchestra fails. When a choreographer drops out, the dancers can still independently continue. The power lies in the individual units, instead of one governing entity. Microservices need to be in charge of their own decision-making instead of an ESB and its separate team.We‚Äôre also facing strong opposition from the infrastructure team against the embedded-container approach. It‚Äôs probably not surprising they‚Äôre trying to push JEE standards as they‚Äôre heavily invested in a certain application server. Josh argues the battle between Spring and JEE was won a long time ago, and by a big margin. Case in point is the new exciting feature of JEE 8: to marshal and unmarshal data between POJOs and JSON. Spring had support for this via Jackson integration already three years ago. Currently there are almost no application servers that implemented JEE 7. It will probably take another five years or so until JEE 8 will be adopted in application servers. That means basic JSON binding on POJO support will only be available eight years after Spring started supporting it. Do you really want to wait that long for such a vital feature? Or should we still use XML-only?Cloud Foundry can be pretty expensive since it‚Äôs primarily aimed towards big enterprises. For other purposes, definitely check out the free OSS stack.Pivotal has a division called Pivotal Labs. It‚Äôs a super agile company that only takes clients that align with their agile views. They have a proven trackrecord that is pretty much unmatched and allows them to stick to their values. Although super expensive, they‚Äôre fast and deliver quality. Aside from taking projects on, the also help companies become more agile.Aside from Eureka, Spring Cloud also supports other registries such as Zookeeper and Consul. The difference between for example Eureka and Zookeeper is that Zookeeper is a shared hierarchical name space of data registers (also called registers znodes). It can be used very well as a service registry, but it offers a lot more features on top of that. One easy example is leader election. Due to the Spring Cloud annotation abstractions it‚Äôs possible to switch out Eureka with any other supported service registry."
      },
    
      "spring-2015-05-08-springio15-autoconfig-html": {
        "title": "Master Spring Boot auto-configuration",
        "url": "/spring/2015/05/08/SpringIO15-Autoconfig.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Spring boot allows you to extend its convention-over-configuration approach by creating your own autoconfigurations. There are some important details you shouldn‚Äôt forget.Master Spring Boot auto-configurationSpeaker: St√©phane NicollIn order to create your own autoconfiguration, it‚Äôs important to remember the spring.factories file in the META-INF folder of the autoconfiguration project.The autoconfiguration class itself should have @Configuration (of course) and utilise conditional annotations as much as possible. Especially on the bean initializations the rule of thumb is the more conditionals the better. This enables users of your autoconfiguration to override specific elements of the autoconfiguration class. Aside from fully overriding beans, you can also expose properties under your own namespace. Together both these concepts allow small configuration-based modifications and bigger bean-overriding modifications by the user.Spring offers many types of Conditional annotations. The regular ConditionalOnClass or ConditionalOnMissingClass and ConditionalOnBean or ConditionalOnMissingBean are the most common, but also ConditionalOnProperty, OnResource, OnExpression and others are possible. You can even have nested conditions and for the most specific needs you can always write your own conditional annotation with whatever logic you require.By looking how Spring‚Äôs autoconfiguration classes are built, it‚Äôs easy to figure out how to do it yourself. Especially the ConfigurationProperties are pretty straight forward once you see an example. Don‚Äôt forget to put the @EnableConfigurationProperties annotation on your autoconfiguration class however.To expose your configurationproperties to IntelliJ, the trick is in the maven dependencies. You should add the ‚Äúspring-boot-configuration-processor‚Äù dependency. This will generate metadata regarding the properties of your autoconfiguration which IntelliJ uses. Once that‚Äôs done, IntelliJ will also autocomplete your properties in the property files.Support for yml configuration autocompletion in IntelliJ is coming by the way.Once your autoconfiguration class is created, it‚Äôs a good idea to bundle it into a maven module that can be used as a dependency by other projects. It‚Äôs important to use the recommended naming convention for your modules - especially if you‚Äôre building autoconfiguration for the community - or it might clash with the modules from spring boot itself. The recommended naming is xyz-spring-boot-autoconfigure and xyz-spring-boot-starter. The former module should contain the autoconfiguration class (and don‚Äôt forget the spring.factories file), and the latter should contain the recommended dependencies to enable the autoconfiguration. That way the user can independently have the autoconfiguration and the classes that enable it. The starter is entirely optional though.The spring.factories file should contain your configuration like this: a key being org.springframework.boot.autoconfigure.EnableAutoConfiguration with value the qualified name of your autoconfiguration class.To further allow the user to customize your autoconfiguration, you can expose a customize hook into your autoconfiguration. This should accept an xyzConfigCustomizer interface (you can create whatever interface you want basically) with a single customize method. This customize hook is executed after the autoconfiguration is executed, but before actual instantiation of the beans. The user then just has to create a bean that implements the customizer interface. An example is available on GitHub.One final concern of autoconfiguration is the order in which they are executed. This applies to conditions inside the autoconfiguration but also to combined autoconfigurations. Always make sure the cheapest condition comes first. So expensive SpEL expression conditions should come after conditionalOnBeans. Between autoconfigurations you can use either the annotation @AutoConfigureBefore and After on the autoconfiguration class, or the @Order annotation. When no order or before or after condition is specified, there is no guarantee when the autoconfiguration will be executed.Conditions are being executed in two phases: PARSE_CONFIGURATION phase and REGISTER_BEAN phase. PARSE_CONFIGURATION evaluates the condition when the @Configuration-annotated class is parsed. This gives a chance to fully exclude the configuration class. REGISTER_BEAN evaluates the condition when a bean from a configuration class is registered. This does not prevent the configuration class to be added but it allows to skip a bean definition if the condition does not match (as defined by the matches method of the Condition interface).The slidedeck of this talk can be found here: https://speakerdeck.com/snicoll/master-spring-boot-auto-configuration"
      },
    
    
      "jobs-dev-ops": {
        "title": "Dev Ops Engineer",
        "url": "/jobs/dev-ops/",
        "image": "/img/jobs/devops.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    Are you passionate about technology? When you hear words like Docker, Kubernetes, and auto-scaling does your heart go aflutter?                    Then this opportunity is right up your alley!  At Ordina we are passionate about our people, our customers, our culture, and especially technology.                                                    This role will focus on support of our client‚Äôs production environments, DevOps automation and CI/CD development.                                Responsibilities                                    As part of a DevOps team, you will be responsible for configuration, optimization, documentation and support of the infrastructure components our                        clients which are hosted in collocated facilities and cloud services such as Azure, Google Cloud or AWS.                                        Design and build tools and frameworks that support deploying and managing our client‚Äôs platforms.                    Design, build, and deliver cloud computing solutions, hosted services, and underlying software infrastructure.                    Assist in coaching application developers on proper techniques for building scalable applications in the microservices paradigm.                    Support and troubleshoot scalability, high availability, performance, monitoring, backup and restores of different environments.                    Support and troubleshoot product and infrastructure issues in production environments.                    Work independently across multiple platforms and applications to understand dependencies.                    Evaluate new tools, technologies, and processes to improve speed, efficiency, and scalability of our client‚Äôs continuous integration environments.                    Mentor and coach your teammates.                                                                                What are the requirements?                Required Skills                                    2+ years experience with cloud-based provisioning, monitoring, troubleshooting, and related DevOps technologies                    Bachelor‚Äôs Degree or equivalent in Computer Science, Engineering or related field                    Understand pros and cons of cloud technologies                    Experience with IaaS and PaaS technologies like Azure, Google Cloud, AWS, Heroku, and OpenStack                    Strong Linux system administration and scripting skills, such as shell, Python or Ruby                    Experience and understanding of the SDLC                    Application operations, including Incident Management, Change Management, Security Management and Capacity Management                    Experience architecting cloud infrastructure and services like AWS and related APIs                    Experience with configuration management or provisioning tools like Chef, Puppet, Terraform, Salt, or Ansible in production environments                    Experience with testing tools such as Cucumber, Inspect, and Serverspec                    Strong experience with containerization/orchestration technologies tools like Docker, Kubernetes, and optionally Mesos and Swarm                    Strong understanding of GIT source control systems such as GitHub or GitLab                    Experience with CI/CD automation tools such as Jenkins, Concourse or Bamboo, and artifact repositories like Nexus or Artifactory                    Strong experience working in an Agile environment                    Excellent troubleshooting and problem solving skills                    Strong track record of learning new technology independently                                Valued Qualifications                                    Excellent written and verbal communication, including mentoring others                    Demonstrate multiple successes from current or past technical challenges                    Experience with multiple provisioning, deployment, cloud computing tools like CloudFormation, Spinnaker and Vagrant                    Windows system administration, ideally including PowerShell scripting                    Experience with messaging technologies such as Kafka, RabbitMQ, ActiveMQ, etc                    Experience with networking concepts like Routing, SNMP, Web Application Firewalls, Load Balancing, and VPNs utilizing products including those from Cisco, Brocade, Foritnet, and F5                    Experience with NoSQL databases such as MongoDB, Redis, Cassandra, ElasticSearch, Splunk, etc                    Experience with service registries such as Consul, Eureka or Zookeeper                    Experience with log management systems like ELK, Splunk or Graylog                    Master‚Äôs Degree or equivalent in Computer Science, Engineering or related field                                                                                What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Ken Coenen                        Backend Practice Manager                                                    +32 472 48 85 01                            ken.coenen@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-experienced-frontend-software-engineer": {
        "title": "Experienced Frontend Software Engineer",
        "url": "/jobs/experienced-frontend-software-engineer/",
        "image": "/img/jobs/angular.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    The current FRONTEND TECHNOLOGIES allow us to create fast, reliable and easy to maintain applications                     that are accessible on a wide set of devices ranging from smartphones to tablets to desktop computers.                     More importantly, they can be created for multiple platforms using a single code base.                     This applies to both web applications and hybrid mobile applications.                     A single code base means less recurring errors and is easier to maintain and test.                     At Ordina JWorks, we embrace technologies such as ANGULAR 2 and IONIC 2                    and apply them in a number of high-profile projects.                                                    At least 3 years of experience with Javascript/HTML5/CSS development                    Plain old JavaScript has no secrets for you                    Solid knowledge of AngularJS                    UX is not foreign to you                    Hands-on experience with OO Design and design patterns                    Experience with continuous integration                    Hands-on experience with Agile development (Scrum) principles & methodologies                    Knowledge of automation, continuous integration and unit and integration testing                    Ability to work independently in an Agile team                    Customer, solution and improvement minded                    Pro-active and can-do attitude                    Team player                    Have a sense of humour                                                                                What are the requirements?                                    You‚Äôre a wizard with following tools:                                                     Webstorm                     Gulp                    Bower                    Java-Script                    AngularJS                     CSS(3)                    Responsive frameworks (Foundation and/or Bootstrap)                    HTML5                     Git                    Jenkins                    Sonar                    Confluence                     Jira                     Rest                                                                                What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Jan De Wilde                        Frontend Practice Manager                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-java-architect": {
        "title": "Java Architect",
        "url": "/jobs/java-architect/",
        "image": "/img/jobs/arch.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    Ordina JWorks is actively transforming companies to cloudnative market disruptors.                     We employ techniques and industry trends such as XaaS, immutable infrastructure and loosely coupled services                    to speed up development and get to market faster than ever. The correct architecture, development methologies and strategy are quintessential to support this.                     Guiding teams through pragmatic leadership and the establishment of a shared vision are enabling strategic and tactic business advantages at an incredible pace.                                       We are the trusted partner of our clients and help them push their organisation forward, while solving difficult and challenging technology problems.                    We believe in evolutionary software delivery through correct service boundaries and APIs.                                                    You play a key role in the successful delivery of our projects                    You make an important contribution in the realization of Java projects, where your architecture is a differentiator                    You create the high-level design for our projects, supporting and steering the customers strategy                    Solving complex problems are a real challenge for you                    You are interested in the latest trends and developments, and can make seasoned decisions on their adoption                    You collaborate in our Competence Centers (Architecture, Cloud, Security, ...) and you share your knowledge with your colleagues                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    Experience with diverse (Java) technology such as Spring (including Spring Boot and Spring Cloud) or JEE                    Thorough knowledge of architectural patterns (event-driven, microservices, n-tier, blackboard, ...), development methodologies, standards, technical risks,...                    Experience with application modernisation or integration platforms is a big plus                    Experience with Application Servers, Containers and Orchestration                    Passion for coaching people and guiding a team                    Strong affinity with setting-up delivery pipelines                    Knowledge of cloud platforms, such as Amazon Web Services (AWS), Cloud Foundry, Google Compute Engine, Azure and microservice architecture are a trump                    Multilingual Dutch and / or French and English                    Strong focus on customer interaction and creating high-level solutions for their needs                    You take initiative and you are very eager to both learn and share experiences                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our down-to-earth culture and no-bullshit way of working.                     Prefer Gliffy over Enterprise Architect or Archi?                    No problem. Want to make an impact on the architectural choices and tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates and visiting software conferences                    Real possibilities to grow within the company and to clearly define your career on the basis of a Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Bart Blommaerts                        Architecture Competence Manager                                                    +32 475 20 41 41                            bart.blommaerts@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-java-software-engineer": {
        "title": "Java Software Engineer",
        "url": "/jobs/java-software-engineer/",
        "image": "/img/jobs/spring-boot.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    We strive to remain thought leaders in the ever changing technology market.                    JWorks makes this a personal goal of each of our colleagues to stay on top of the game.                     This can be done by following or contributing to numerous WORKSHOPS, coding sessions and webinars we organise,                     but what‚Äôs more fun than going to INTERNATIONAL CONFERENCES                    such as Spring I/O in BARCELONA or MongoDB World in NEW YORK?                     Take a couple of days extra to enjoy the surroundings and get to know your colleagues better!                                                    Software development is your true passion                    You contribute in the successful delivery of our project                    You make sure that our clients are proud on your contribution                    Solving complex issues doesn't scare you                    You are interested in the latest trends and evolutions and you want to learn them yourselve                    You collaborate in our Competence Centers (Cloud Computing, Mobile,‚Ä¶) and you share your knowledge with your colleagues                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    At least 2 years experience with Java technology                    Spring stack, including Spring Boot, Spring MVC, Spring Data and Spring Cloud                    Automated testing (unit testing, integration testing and acceptance testing) of applications doesn't have any secret for you                    Application Servers and Containers                    Tools such as Git, Jenkins, Sonar and Maven are not unfamiliar to you                    Knowledge of cloud platforms, such as Amazon Web Services (AWS), AWS Lambda, Cloud Foundry, Google Compute Engine, Azure and microservice architecture are a trump                    Knowledge of methodologies such as DevOps, Scrum, UML, PRINCE2 and ITIL are a trump                    Multilingual Dutch and / or French and English                    You work customer focused and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Ken Coenen                        Backend Practice Manager                                                    +32 472 48 85 01                            ken.coenen@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-javascript-developer": {
        "title": "JavaScript Developer",
        "url": "/jobs/javascript-developer/",
        "image": "/img/jobs/angular.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    The current FRONTEND TECHNOLOGIES allow us to create fast, reliable and easy to maintain applications                     that are accessible on a wide set of devices ranging from smartphones to tablets to desktop computers.                     More importantly, they can be created for multiple platforms using a single code base.                     This applies to both web applications and hybrid mobile applications.                     A single code base means less recurring errors and is easier to maintain and test.                     At Ordina JWorks, we embrace technologies such as ANGULAR 2 and IONIC 2                    and apply them in a number of high-profile projects.                                                    Software development is your true passion                    You contribute in the successful delivery of our project                    You make sure that our clients are proud on your contribution                    Solving complex issues doesn't scare you                    You are interested in the latest trends and evolutions and you want to learn them yourselve                    You collaborate in our Competence Centers (Cloud Computing, Mobile,‚Ä¶) and you share your knowledge with your colleagues                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    At least 2 years experience with JavaScript                    Vanilla JavaScript (ES5), AngularJS or EmberJS, NodeJS, Gulp (Grunt), Bower, Auth0, Firebase                    You have knowledge of testing tools such as: Jasmine, Karma, Protractor, Selenium and Mocha                    Knowledge of TypeScript, ECMAScript 2015 (ES6), Angular 2, React, Webpack and JSPM are a trump!                    Multilingual Dutch and / or French and English                    You work customer focused and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Jan De Wilde                        Frontend Practice Manager                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-junior-java-developer": {
        "title": "Junior Java Developer",
        "url": "/jobs/junior-java-developer/",
        "image": "/img/jobs/spring-boot.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    We strive to remain thought leaders in the ever changing technology market.                    JWorks makes this a personal goal of each of our colleagues to stay on top of the game.                     This can be done by following or contributing to numerous WORKSHOPS, coding sessions and webinars we organise,                     but what‚Äôs more fun than going to INTERNATIONAL CONFERENCES                    such as Spring I/O in BARCELONA or MongoDB World in NEW YORK?                     Take a couple of days extra to enjoy the surroundings and get to know your colleagues better!                                                    Software development is your true passion                    You contribute in the successful delivery of our project                    You make sure that our clients are proud on your contribution                    Solving complex issues doesn't scare you                    You are interested in the latest trends and evolutions and you want to learn them yourself                    You collaborate in our Competence Centers and you share your knowledge with your colleagues                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    Multilingual Dutch and / or French and English                    You work customer oriented and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                                                                Technology stack                                    Spring stack, including Spring Boot, Spring MVC, Spring Data and Spring Cloud                    Automated testing (unit testing, integration testing and acceptance testing) of applications doesn't have any secrets for you                    Application Servers and Containers                    Tools such as Git, Jenkins, Sonar and Maven are not unfamiliar for you                    Knowledge of cloud platforms, such as Amazon Web Services (AWS), AWS Lambda, Cloud Foundry, Google Compute Engine, Azure and microservices architecture are an added bonus                    Knowledge of methodologies such as DevOps and Scrum are nice to haves                                                                                What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our DOWN-TO-EARTH CULTURE and no-nonsense way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                    Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            anja.vanacker@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-mobile-developer": {
        "title": "Mobile Developer",
        "url": "/jobs/mobile-developer/",
        "image": "/img/jobs/android.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    As a Mobile Developer within Ordina you will be part of the team that delivers mobile solutions for leading companies                    Besides the development of applications on mobile platforms for PDAs, smartphones and tablets,                         you will come in touch with peripheral matters that are necessary for setting up a fully functioning circuit for the user                                        These include: setting up and technical management of mobile middleware platforms, setting up database synchronization and integration with existing systems                    Of course security is an important item                    We keep each other sharp and customer satisfaction high. Customer contact and quality are key.                         We also offer you the space for innovative ideas, self-development and entrepreneurship                                                                                                    What are the requirements?                                    Bachelor or Master level                    At least 3 years experience with programming in Java, C++, or C#, or a very broad experience with C                    You have good communication skills, both oral and written                    You have knowledge of modern messaging standards on the Internet, have a thorough knowledge of general web standards and have experience with one or more service architectures                    You have a result-oriented attitude which you combine with independence, creativity and an entrepreneurial attitude                                                                                                    What does Ordina have to offer?                                    We‚Äôre growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We‚Äôre very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Jan De Wilde                        Frontend Practice Manager                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
        </div>
    </section>
</div>

<div id="over"></div>

<footer>
    <div class="contact">
        <div class="address">
            <div class="icon"><i class="fa fa-fw fa-home"></i></div>
            <div class="text">Ordina Belgium<br/>Blarenberglaan 3B,<br/>2800 Mechelen, Belgium</div>
        </div>
        <div class="phone">
            <div class="icon"><i class="fa fa-fw fa-phone"></i></div>
            <div class="text"><a href="tel:003215295858">+32 15 29 58 58</a></div>
        </div>
        <div class="email">
            <div class="icon"><i class="fa fa-fw fa-envelope-o"></i></div>
            <div class="text"><a href="mailto:jworks@ordina.be">jworks@ordina.be</a></div>
        </div>
    </div>
    <ul class="social">
        <li>
            <a href="https://twitter.com/lifeatordinabe" target="_blank">
                <i class="fa fa-fw fa-twitter"></i><span>Twitter</span>
            </a>
        </li>
        <li>
            <a href="https://www.facebook.com/lifeatordinabe" target="_blank">
                <i class=" fa fa-fw fa-facebook"></i><span>Facebook</span>
            </a>
        </li>
        <li>
            <a href="https://www.linkedin.com/company/ordina-belgium" target="_blank">
                <i class=" fa fa-fw fa-linkedin"></i><span>LinkedIn</span>
            </a>
        </li>
        <li>
            <a href="https://plus.google.com/113222464071666722451" target="_blank">
                <i class=" fa fa-fw fa-google-plus"></i><span>Google+</span>
            </a>
        </li>
        <li>
            <a href="/youtube" target="_blank">
                <i class=" fa fa-fw fa-youtube"></i><span>YouTube</span>
            </a>
        </li>
        <li>
            <a href="/github" target="_blank">
                <i class=" fa fa-fw fa-github"></i><span>GitHub</span>
            </a>
        </li>
        <li>
            <a href="/feed.xml" target="_blank">
                <i class=" fa fa-fw fa-rss"></i><span>RSS Feed</span>
            </a>
        </li>
    </ul>
    <div class="copyright">
        &copy; 2017 Ordina JWorks. All rights reserved.
        <br /> Disclaimer: Opinions expressed on this blog reflect the writer's views and not the position of Ordina
        <img id="analyticsImg" src="" width="1" height="1" style="border: 0px"/>
    </div>
</footer>
<!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.magnific-popup.min.js"></script>
<script src="/js/owl.carousel.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/jquery.pin.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]>
<script src="/js/ie/respond.min.js"></script><![endif]-->



<script src="/js/main.js"></script>

<!-- Disqus -->


<!-- Google Analytics -->
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o);
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m);
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-70040502-1', 'auto');
    ga('send', 'pageview');
</script>
<script type="text/javascript">
    window.addEventListener('load', function () {
        if (window.ga && ga.create) {
            console.log('GA loaded correctly');
        } else {
            console.log('GA is blocked or failed to load - tracking manually...')
            document.getElementById('analyticsImg').src = 'https://jworks-techblog-analytics.cfapps.io/collect?title=' + document.title;
        }
    }, false);
</script>

<!-- Vertical timeline -->
<script src="/js/vertical-timeline/modernizr.js"></script>
<script src="/js/vertical-timeline/main.js"></script>


</body>
</html>
