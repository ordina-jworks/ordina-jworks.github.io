<!DOCTYPE html>
<!--suppress ALL -->
<html>
<head>
        <meta charset="utf-8">
    <meta name="description" content="We build innovative solutions with Java and JavaScript. To support this mission, we have several Competence Centers. From within those Competence Centers, we provide coaching to the employee and expert advice towards our customer. In order to keep in sync with the latest technologies and the latest trends, we frequently visit conferences around the globe.
">
    <meta name="keywords" content="Ordina,ORAJ,JWorks,Blog,Java,JavaScript,TypeScript,Angular,DevOps">
    <meta name="author" content="Ordina Belgium">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/img/favicons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/img/favicons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/img/favicons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/img/favicons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/img/favicons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/img/favicons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/img/favicons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/img/favicons/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-128.png" sizes="128x128" />

    

    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Search">
    <meta name="twitter:description" content="Ordina JWorks Tech Blog">
    <meta name="twitter:image" content="http://ordina-jworks.github.io/img/jworks/jworks-400x400.png"/>

    <!-- Facebook Open Graph -->
    <meta property="og:url" content="http://ordina-jworks.github.io/search/"/>
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Search"/>
    <meta property="og:description" content="Ordina JWorks Tech Blog"/>
    <meta property="og:image" content="http://ordina-jworks.github.io/img/jworks/jworks-1200x630.png"/>

    
        
        <title>Search &mdash; Ordina JWorks Tech Blog</title>
        
    

        <!-- Styles -->
    <!-- Font awesome CSS -->
    <link href="/css/font-awesome.min.css" rel="stylesheet">
    <!-- Magnific Popup -->
    <link href="/css/magnific-popup.css" rel="stylesheet">
    <!-- Owl carousel -->
    <link href="/css/owl.carousel.css" rel="stylesheet">

    <!-- CSS for this page -->

    <!-- Syntax highlighting -->
    <link href="/css/syntax.css" rel="stylesheet">


    <!--[if lte IE 8]>
    <script src="/js/ie/html5shiv.js"></script><![endif]-->
    <link rel="stylesheet" href="/css/main.css"/>
    <!--[if lte IE 9]>
    <link rel="stylesheet" href="/css/ie9.css"/><![endif]-->
    <!--[if lte IE 8]>
    <link rel="stylesheet" href="/css/ie8.css"/><![endif]-->

    <!-- Custom CSS. Type your CSS code in custom.css file -->
    <link href="/css/custom.css" rel="stylesheet">

    <!-- Vertical timeline -->
	<link rel="stylesheet" href="/css/vertical-timeline/style.css"> <!-- Resource style -->

    <!-- Favicon -->
    <link rel="shortcut icon" href="#">

    <script src="https://use.typekit.net/gzh0mbm.js"></script>
    <script>
        try {
            Typekit.load({async: true});
        } catch (e) {
        }
    </script>
</head>

<body>
<div id="header-image"></div>


<!-- Page Wrapper -->
<div id="page-wrapper">

        <!-- Header -->
    <header id="header">
        <h1>
            <a href="/">
                JWorks Tech Blog
            </a>
        </h1>

        <nav>
            <a href="#menu">Menu</a>
            <form action="/search/" method="get" class="header-search search-form">
                <input type="text" id="search-box-header" name="query" placeholder="Search &hellip;">
            </form>
        </nav>
    </header>

    <!-- Menu -->
    <nav id="menu">
        <div class="inner">
            <h2>Menu</h2>
            <ul class="links">
                <li class="menu-item"><a href="/">Home</a></li>
                <li class="menu-item"><a href="/about">About Us</a></li>
                <li class="menu-item"><a href="/meet-the-team">Meet the team</a></li>
                <li class="menu-item"><a href="/jobs">Jobs</a></li>
                <li class="menu-item"><a href="/search">Search</a></li>
                <li class="menu-item"><a href="https://join.ordina-jworks.io/">JOIN</a></li>
                <li class="menu-item"><a href="/contact">Contact Us</a></li>
            </ul>
            <a href="#" class="close">Close</a>
        </div>
    </nav>

    <section id="banner">
        <header>
            <div class="inner">
                

                <h2>Search</h2>
                

                

                

            </div>
        </header>
    </section>


    <section id="wrapper">
        <div class="inner">
            <section class="wrapper spotlight style1 search-page">
    <div class="inner">
      <div class="content">
        <h2 class="major">
          Search for
          <form action="/search/" method="get" class="search-form">
            <input type="text" id="search-box" name="query">
          </form>
        </h2>
        <div class="spinner-container" style="width: 100%; height: fit-content; text-align: center;">
          <i id="search-spinner" class="fa fa-spinner fa-pulse fa-3x"></i>
        </div>
        <section id="search-results" class="features search-results">
        </section>
      </div>
    </div>
</section>

<script>
  window.store = {
    
      "agile-2022-10-19-waterfall-vs-agile-html": {
        "title": "Waterfall vs Agile",
        "url": "/agile/2022/10/19/waterfall-vs-agile.html",
        "image": "/img/2022-10-19-waterfall-vs-agile/header.jpg",
        "date": "19 Oct 2022",
        "category": "post, blog post, blog",
        "content": "  Waterfall or agile for a new project? Benefits or disadvantages?Table of contents  Introduction  Software Development Life Cycle          Waterfall      Agile      Scrumfall/Waterscrum/Agilefall/Watergile      IntroductionAfter my graduation until now, about 10 years, I have worked in many different teams and project, for various clients in various sectors.During those years, many different styles of project management were used but they always came down to  the more traditional type of project management, also known as waterfall  a more modern type known as agile  something in between those 2It is widely known that agile is a big buzzword in more recent years, but what is it exactly?What is the difference between agile and waterfall?Which one can be used in which situation?What are the benefits and disadvantages?Software Development Life CycleIt is important to know the purpose of a software development life cycle (SDLC). The usual explanation for this cycle is  A process that produces software with the highest quality and lowest cost in the shortest time possible. It typically provides a well-structured flow of phases that help an organization to quickly produce high-quality software which is well-tested and ready for production use.A mouthful for saying that it provides guidance for delivering software as efficiently as possible, with high quality.In a SDLC there is a focus on the following 6 phases:  Planning  Analysis  Design  Implementation  Testing &amp; Integration  MaintenanceThere are multiple models of SDLC’s, but the most known are the waterfall and agile model.WaterfallThe waterfall model is the most traditional type of a SDLC. It has been around since 1970.It divides the effort into a number of steps and defines that only one step can be active at the same time.While it usually follows the 6 phases as described above, there’s no limit on it.From where does it get its name?As you can see, it looks like a waterfall. Each step can only start after the previous one is done and thus it cascades down like a waterfall.Now, when is a step done? Only when the specified artifacts that were defined at the beginning of the step are delivered and accepted.For the implementation step, this can be a collection of software artifacts that satisfy the design.For the testing step, it can be a test plan that demonstrates that all the requirements and design are working as intended.A common mistake is that people think that you can not go back to a previous step. It is allowed, but it requires that you stop all work on the current step until the errors in the previous step have been resolved.Pro’s  It has detailed documentation and metrics  The requirements are agreed upon and signed off  There are less defects as there is rigorous planning and testing  Defined start and end point which allows for easy measuringCon’s  It starts slow as the requirements need to be defined in detail  Changing those requirements takes a lot of effort  The software is not visible until most of the development work is finished  Less focus on the client because the requirements are the most important itemExamplesWhen do we use the waterfall model today?  When requirements can be reliably, quickly and thoroughly defined up front  For very large teams where common understanding must be put in writing to avoid confusion and miscommunication  When there is a defined budget and schedule given by the customer  When there is not much involvement from the customerBuilding a bridge across a river is a good example of a project that is best done with a waterfall model.That is a project where a clear schedule is needed and where the requirements need to be defined as soon as possible.Saying “We will start with the first part of the bridge, evaluate that part and then decide if and how to continue with the remaining parts” is not a possibility here.AgileThe agile model has been created in direct response to the waterfall model.It puts the focus on adaptive, simultaneous workflows which is the opposite from the linear flow of the waterfall model.Instead of beginning with a complete knowledge of the requirements, the team develops a product in small cycles where small parts are build in a evolutionary way.Each cycle contains the same steps as defined in the SDLC, but they can all be done at the same time, depending on the experience and skills of the team.Contrary to the waterfall model, the customer can quickly see and evaluate how the project is advancing at the end of each cycle.Needed changes to the requirements, based on this evaluation, can be done faster and implemented more easily.This constant feedback from the client allows the team to adjust to the challenges as they arise and not when it is too late.The main idea behind the agile model is delivering business value early in the process to lower the risk associated with the development.The most known implementations of the agile model are Scrum and Kanban, but they all share the same characteristics.  Simultaneous, incremental work  Adaptability  Faster and multiple deliverablesProcessDuring agile development, the process usually looks like this:  Define a few initial requirements  Design  Develop  Test  Deploy  Evaluate the result of the iteration  Collect feedback from the various stakeholders  Start the cycle again with new requirements and the feedbackPro’s  More project visibility at the end of each cycle  It is a collaborative and practical approach for executing complex software development projects  The client and stakeholders have frequent and early opportunities to evaluate the product  Constant communication between team members so issues can be resolved proactivelyCon’s  There is a risk for scope creep as agile projects generally have no set end date and thus additional features may be requested  A multi-skilled resource pool is needed to deliver the project as all knowledge needs to be in the team  Less detailed documentation as it is considered less important  Fragmented output can be a problem as multiple teams may work on different components that then need to be put togetherExamplesWhen do we use the agile model today?  If there are little to no requirements at the start of the project  If your organization does not have strict processes to follow and the existing processes are lenient  If the client or product owner can be highly available to follow the process  When you’re trying to create something innovative that does not exist yet and needs to go to market quickly  When the timeline is short and flexible  When the budget has some wiggle room so that features can be prioritizedAgile wins when the requirements are unclear from the beginning or still need to be discovered during the initial development.More features will be produced in a shorter time frame and the team can be more flexible throughout the process.A good example would be the development of a social media app. While the initial requirements are clear, further development depends heavily on the demands of the user.So a start can be made with some basic features that can go quickly to market. Other features will then be implemented after the feedback of the business and most importantly the reactions of the users.Scrumfall/Waterscrum/Agilefall/WatergileThis model might be known under even more names than the ones above. But it’s basically a software delivery lifecycle that tries to combine the best of both worlds in waterfall and agile.It usually starts with an up-front design phase and ends with a legacy deployment mechanism, with agile development in between.But the name doesn’t matter. Everyone that knows a bit about agile knows that it’s a bad idea to do the entire design up-front and only then start developing. While that might work in a full waterfall model, it doesn’t in agile. Waterfall and agile are 2 fundamental different models with conflicts in interest as explained in the previous chapters.Why?So even though it’s not the best idea to mix these models, how does it happen that organizations use them?The results may not be ideal, but it might be enough for some organizations. The agile model mentions all the time that there is not one solution that fits everyone. Every organization must find an approach that is effective for them and which enables them to deliver value.Who doesn’t know an organization that follows Scrum to the letter, only to find out that it’s not the best approach for them?Agile is for a large part about discovering new ways of working. As a result, the scrumfall model can be a temporary step towards a full agile transformation.Now why would organizations keep using this hybrid form?  The IT department is agile and uses Scrum, but the other business departments have never been convinced. And so the organization is divided.  The organization is stuck with an incomplete transformation and doesn’t know how to continue  Or they’re in the middle of such a transformation and will finish it shortly  The organization’s structure is made in such a way that deployment and operations cannot be done by the development teamsImpactNow, how can this hybrid model impact your organization?Let’s look at the following image:The first phase starts with the design as in every model. However, it’s waterfall design, so it takes a while to get everything designed in detail.This means that during this phase, the development team has nothing to do. They’re just idle.Once that phase is done and the developers can finally start working, they accelerate and can start delivering. But since the deployment process is still traditional waterfall, it takes a while to get it done. Which means that the feedback loop is going to be a lot slower than in modern agile models.Slow feedback loops and long release cycles have a high negative impact on the value of the project as everything takes longer and results of various experimentations are harder to see.ProblemsWhat are the major problems when you use this model?  Risk and waste: when using agile models, you get feedback by interacting with the business and the end-users. When designing up-front, you can’t anticipate changes in the demands. So when you’re not using a tight feedback loop, you might be creating the wrong thing. Which results in waste when you need to restart.  Delayed feedback: since the development team is not doing the releases, it will take more time. Time that could be used to find out how the market responds to the new features.  Long-term damage: the waste that is created has construction and maintenance costs, not to mention decrease in the team’s motivation. All these things have a long-term impact on the organization.Way out?The goal of any organization is to meet the goals of the corporate vision and mission. These goals should be achieved with the greatest efficiency and effect.If that is done with waterfall or agile is less important. Even scrumfall might work for some organization, be it less effective than a “pure” model.As with any model, experimentation must be done and improvements must be made. As in agile, this depends from organization to organization how it can be done. There is no one answer that fits for everyone.µThe most important to keep in mind is that every organization needs to shift its approach to one that decreases waste while increasing quality and predictability."
      },
    
      "conference-2022-10-03-experience-agile-2022-html": {
        "title": "eXperience Agile 2022",
        "url": "/conference/2022/10/03/experience-agile-2022.html",
        "image": "/img/2022-10-03-experience-agile-2022/ExperienceAgile2022.jpg",
        "date": "03 Oct 2022",
        "category": "post, blog post, blog",
        "content": "After 2 years of COVID, it was finally time for another conference. I’ve chosen the eXperience Agile conference in Lisbon, Portugal, from 26/09/2022 to 27/09/2022.It was the 8th edition  of a global conference that focuses on gathering wisdom and best practices on business agility as well as technical agility.Since I’m working with agile teams, I hoped to gather more knowledge and learn new things to apply to my teams.The first day contained 5 talks, agile safari’s, workshops and lightning talks.The second day had the same format, but due to flight timings, I couldn’t see them all.All talks were maximum thirty minutes, so the information was very concise.In this blogpost, I’ll list some talks I found interesting.Table of contents  FAST Agile, by Ron Quartel  Optimise your neurophysiology for agile thinking, by Delia McCabe  Practice does not make perfect, by Gil Broza  Understanding value streams at the gemba, by Nigel ThurlowFAST Agile, by Ron QuartelThe full title of the talk was ‘FAST Agile – The New (and Wild) Kid on the Agile Block’.  “Fluid Scaling Technology aka FAST Agile, is the outcome of an experiment in radical self-organization at scale. Not only did it work, but we also found it solved many of the issues that come with agile at scale that existing methods face. If you like Dave Snowden’s rewilding agile message, you are likely to see the promise in this radical new way of working built on Open Space Technology. Prepare to be surprised…”During the talk, Ron referred a lot to the Cynefin model by Dave Snowden. Explaining that model here would take me too far away from the purpose of this post.In short, it helps to determine in what situation your company/project/team is (complex, complicated, chaos, obvious). And based on that situation, you can take calculated decisions.Scrum can then be placed in the space between complicated and complex.FAST sits in the complex space, next to the chaos border.Now, what is FAST? It uses the principle of the Open Space Technology, where large groups of people come together, self-organise and set up a planning.It stands for Fluid Scaling Technology. The A is just there so it sounds better.How the process works:  It merges teams into a tribe  Everyone throws their work on a wall  Everyone self-organizes around the work  2 days later, everyone meets back and shares their progress  Repeat from the startIt’s an agile method where scaling is built-in. Certain conditions must be met, but that’s also true for Scrum and agile in general.How does FAST help?  It’s a pure complex system, unlike other scaling methods like SAFe which are complicated in nature. The Cynefin model teaches us that complicated systems must be solved with complicated models, and complex with complex.  It has been designed to not transform to zombie agile  It focuses on both Discovery and Delivery while other agile methods focus on DeliveryMore information on FAST can be found at Fluid Scaling Tech.Optimise your neurophysiology for agile thinking, by Delia McCabeA surprising talk about the functioning of the brain and how it correlates with agile.  “Thinking occurs across a sensitive and sophisticated neural network. Our neurophysiology depends heavily on lifestyle choices, which include the nutrients we consume. Other lifestyle factors, which include sleep quality, physical activity, and work and relationships, also impact this sophisticated network, although nutrition  forms the foundation of our neurophysiology. It is therefore the first principle we need to address if we aim to maximise cognitive strategies such as agile, creative, and flexible thinking.”I was most curious about this talk, as it doesn’t immediately connect with an agile conference. She said it herself that she was surprised to have been invited, but took it as a challenge to explain how the brain works and how that affects the agile way of thinking.She started the talk with explaining how our brains have evolved to work in a jungle environment, to survive.But that the current office life is a lot different and our brains have difficulties handling it. Back in the ancient times, stress was present in short bursts (for example when hunting), but not continuously like it is in the current times.She explained how the brain needs to be nourished correctly in order to be able to receive new information.If the brain is not nourished properly, it will not be able to create new neural links and thus not process new experiences.So for example, when you’re explaining something new to another person, if that person’s brain is not nourished as it should be, that person will not process the information correctly, if at all.More information on this subject can be found at here.Practice does not make perfect, by Gil BrozaAlmost every company uses agile nowadays. But not everyone gets it right.  “These days, almost every organization is on an Agile journey. And yet, most companies have trouble achieving real agility. Why is that? Aren’t the ingredients for effective transformations available to everyone? There is no shortage of motivation, established practices, detailed processes, ever-improving tools, literature, consultants, employees with agile experience, and certifications. Gil Broza, author of “The Agile Mindset” and “The Human Side of Agile”, thinks that one particular ingredient has been overlooked in the mad rush to adopt Agile. In this session, he leads us on an exploration of that ingredient and its crucial role in successful Agile journeys.”The talk started with the explanation of the logical levels model.It goes top-down from identity and role, to values and belief, to capability, behaviour and environment.He gave the following example in his current role:He continued explaining the starting point of many agile journeys.            Traditional values      Traditional beliefs                  Get it right the first time      Customers know what they want              Minimize cost and time      Putting a plan together is worth it              Make early commitments      Okay to have multiple constraints              Follow industry standards      People are resources      The above is of course not very agile. But it’s a starting point where many organisations started.He then went on explaining how the above works in the logical levels model and how to adapt it.But either way, as he stated, “Practices don’t matter, mind-set does”.A good example is the table below, where you can see that just giving things a different name does not work if the traditional belief stays.            Practice/role/artifact      Was conceived as      A traditional mindset sees it as                  Daily standup      Regular check-in to maximize the team’s value output      Daily status for maximizing work-the-plan              Product backlog      Prioritized list of valuable deliverables the team might do      Project plan              Pair programming      Collaboration to minimize the risk of employing humans      Under-utilization              Scrum Master      Servant leader, helping the team succeed as a team      Project manager, ensures process compliance              Sprint demo      Feedback, for adaptation      Frequent deadline for sign-off, tracking and accountability      More information of his talk can be found here.Understanding value streams at the gemba, by Nigel ThurlowThe full title of this talk was “Understanding value streams at the gemba, not from the office”.  “He’ll cover what Lean metrics really are, how scaling should work, take you into queuing theory, and then back to the Genba where the real improvements happen. Learn what a real value stream is and how to uncover the real costs of doing business. This and a whole lot more of mind bending topics.”Now this was a talk that was filled with information. There were 71 (!) slides that had to be told in 30 minutes. That is a lot of information to transfer in a short period of time.However, Nigel managed to do so quite well.I’m not going to pretend I understood everything, but it was interesting nonetheless.He talked about the Flow system, about complex versus complicated environments, about various tools and how they’re only visualisations, etc… .There was way too much to tell here in this blog post, but I’ll try to share some things I’m going to keep in mind.Value is cross organisation. You can try to sub-optimize in the IT department, but if you don’t take the other departments with you, the value generated will not be optimal.The value stream contains all the people, machines, technology and skills needed to complete the end to end product delivery.Closely tying in to the above is the genba/gemba and real value stream.He explains that growth, costs, time-to-market and staff attrition are not problems, but outcomes. And that if you want to change those outcomes, you’ll have to change how you do work.This is where genba comes into play. It’s Japanese for “actual place”.It means you need to go the place in your company where the actual value is created. Nigel explained how he helped to transform a corn factory by actually doing an 8-hour shift on the factory floor.This gave him better insights in one shift than his previous 3 weeks of observation. This enabled him to make changes to the production line and added value to the company.More information can be found on his website."
      },
    
      "internship-2022-07-07-snitching-on-your-colleagues-using-cloud-magic-html": {
        "title": "Snitching On Your Colleagues Using Cloud Magic",
        "url": "/internship/2022/07/07/snitching-on-your-colleagues-using-cloud-magic.html",
        "image": "/img/2022-07-07-snitching-on-your-colleagues-using-cloud-magic/Betrayal.jpg",
        "date": "07 Jul 2022",
        "category": "post, blog post, blog",
        "content": "Table Of Contents      Introduction    The Task At Hand  Exploring The Environment          Azure Repos      Pipeline      Library      Service Connections        Into The Rabbit-Hole : Setting Up The Pipeline          Before We Dive In      Basic Tasks      Containerization      Setting Up A Cloud Database      Provisioning A Cloud Cluster      Giving A Signal      A Visual Representation        Out Of The Frying Pan And Into The Fire: Creating Our Own Task          Requirements      Getting The Logs      Filtering And Saving Failures      Factory Fresh        The Berry On Top: Our Physical Feedback          Dollar Store Google Assistant        The Good, The Bad &amp; … The Ugly?Introduction7 weeks ago I started my internship at Ordina Mechelen.I had several project options available all looking to touch new and unknown tech that might be relevant for future operations.My inner Judas spoke to me when I saw a listing about a project that would shoot toy rockets at developers if they broke a build, and it would provide a great opportunity to pull myself out of my comfort zone focusing more on Devops and Cloud platforms rather than pure programming.Sadly due to a global hardware shortage the toy rocket launcher was not available for delivery anymore, so I decided to use a raspberry pi to fetch the build logs and convert them into audio using google text to speech.The Task At HandThe project described a ci-cd pipeline that would trigger a raspberry pi once a build fails, then in response the pi would operate a toy rocket launcher unit that would target the developer responsible for breaking the build.As such the final project can be broken down into these steps:  Create a sacrificial bare-bones spring boot project to put through the ringer  Create a pipeline which performs some cookie cutter tasks  Expand pipeline to setup AWS infrastructure  Expand pipeline to provision said infrastructure  Create a custom pipeline task to fetch and push build logs  Configure raspberry pi to host a node.js server  Listen for new log files  If new log files have been found, search for error messages and convert those via text to speech(TTS)  Broadcast the failures of your colleaguesExploring The EnvironmentI’m going to dive right into the meat and potatoes of this project and write about the pipeline since the spring boot application does nothing more than display some basic html and runs a couple of unit tests.The devops environment that was used consisted of a couple of things.Azure ReposA simple git instance on the Azure platform used for version control of our spring boot project.PipelineThe bread and butter of our operation. Using a data serialization language called yaml we are able to define each individual task we want applied to our code.LibraryA place to store key value pairs that we can group and later reference in our yaml file.Service ConnectionsPredefined connections to internal (Azure) or external services from which we can later extract credentials to gain access in the pipeline.Into The Rabbit-Hole: Setting Up The PipelineIn this section I will initially explain how to set up some basic tasks within our yaml file for building and testing. Next, I’ll move on to containerizing our build. Finally, I’ll explain how I have used IaC (Infrastructure as Code) to firstly spin up an RDS (AWS) instance based on Postgres and secondly use Helm to deploy the application to an EKS (AWS) cluster owned by JWorks.Before We Dive InBefore we dive in, there are some nice to know things about the inner workings and structure of a pipeline defined in a yaml file.I’ll briefly go over some key definitions, so you can follow along when each individual task gets explained.  Triggers :          Triggers are (like the word implies) what starts a pipeline      These can be changes in a branch like main or develop but could also be specific events happening in another pipeline such as failed jobs/stages or a specific variable value        Variables :          A hardcoded variable defined at the start of your pipeline      A variable group containing secret values like tokens that can not be acquired using service connections      Both of these options can be defined at the same time or individually and used anywhere in the pipeline including in additional task commands if the task allows it.        Stages :          Defines what steps your pipeline should take in what order.      Encapsulation of stage sections        Stage :          A section of your pipeline      Can be given a name of your choosing eg: Test,Docker,CloudSetup …      Encapsulation for jobs        Jobs :          Encapsulation of job sections        Job :          Can be given a name of your choosing just like the stage section      Can be given a pool attribute to define which agent OS has to run this job      Encapsulation of one or more task sections        Task :          A pre-built or custom-made task to be performed on your pipeline run.      Has a variety of attributes that can be manually filled up such as credentials or dictating your preferred working directories        Agents :          A machine hosted by the cloud provider (Azure in our case) that runs on a specific OS      Defined in the pool attribute of a job or at the start of a pipeline.      Mostly a clean slate with only some essential software pre-installed eg: Docker      Gets wiped after use in order to accommodate the next job.      All of the above gets combined into a structure that resembles following code.variables:- group: my-variable-grouptrigger: mainpool:  vmImage: ubuntu-lateststages:- stage:  jobs:  - job: myJob    steps:    - script: echo \"Hello\" Basic TasksFollowing are the pipeline tasks used to test our java project, generate a test rapport and then build it using Maven.Maven TestTo test our application we will be using JUnit because of the pre-existing support given by Azure. This will also generate a test rapport during each pipeline run based on the unit tests defined in our project.Maven BuildA standard task using the Maven goal of ‘package’ which returns a JAR file that can later be used by Docker.Copying And Creating An ArtifactIn order to use our build across multiple agents we need to create an artifact out of the build.This artifact gets stored within the pipeline and can get called upon whenever we please.We do this by first copying our build to a directory on our agent that functions as the default staging directory for artifacts.By using a second task we take that build and publish it to our pipeline storage.ContainerizationHere we use the artifact we created during our last job to create an image and push it to Dockerhub.Fetching Our ArtifactFirst we have to fetch the artifact that we uploaded to our pipeline and place it in the appropriate directory on our new agent.Creating An ImageThen we use said artifact together with a Dockerfile that was previously placed within our java project to create and upload an image to Dockerhub.Setting Up A Cloud DatabaseIn this job we will use Terraform to set up an RDS (AWS) database based on Postgres for our containerized java application.Our Terraform files are stored in the java project under a folder called infrastructure and can be called upon directly, alternatively a remote folder containing Terraform files can be specified if you want or need to split up your project files.Credentials needed to get access to AWS services come from a manually pre-defined service connection.Installing TerraformIn order to use Terraform on our agent we have to first install it to our agent since it is not supplied by default.Initializing TerraformThis task performs several initialization steps in order to prepare the current working directory for use with Terraform.Terraform PlanPlans out what configurations and steps will be made once the apply command is given.Terraform ApplyExcecuting our Terraform plan defined in the previous step.Provisioning A Cloud ClusterNow that we have an image of our app and place to store data to only one crucial step remains, launching our application.Our application gets deployed to EKS (Elastic Kubernetes Service) which is another AWS service designed for running cloud based Kubernetes.In order to do so a Helm chart has been made which like the Terraform files is stored under the infrastructure directory of our project.These charts are defined in a yaml format where specifications for Kubernetes are being made eg: Name of the app, Kind , Amount of replicas, Image to use …Helm DeployUsing our Helm chart and a service connection allowing us to deploy to the Jworks cluster, we deploy our application, which gets pulled from Dockerhub, to the “stage-thomas-more” namespace within EKS.Giving A SignalNow that everything has been set up and all the services are up and running it’s time to give our developers a heads-up.I did this using a Telegram bot that will broadcast a message for every build that has run.The bot token was stored in the library as a secret key-value pair.Creating Our BotThis is a prerequisite if you want to work with Telegram since a bot token and a chat id are required to function.Telegram has a neat tutorial on how to create your own bot using the “Botfather” which you can find here : The BotfatherSending Out NotificationsNow that we have our bot token and a chat id we can define a message that gets sent everytime the task is reached.A Visual RepresentationDisplayed below you will find two images representing the pipeline and the goals they accomplish on the Cloud.For now pay no attention to the little logo displaying Eric Cartman, this is the image I used to represent my custom task which we will get to in the following section.Out Of The Frying Pan And Into The Fire: Creating Our Own TaskDuring week 5 all of the above was learned, implemented and configured to a working state so a question was asked by Frederick Bousson the solutions lead at the Jworks Ordina Unit if it was possible to create a custom task for use in the pipeline.Up for another challenge I stepped into the pretty unexplored (And not fully documented) lands of developing a task for a pipeline.The main objective of the task is to get the pipeline logs from a predefined pipeline run or from the current one as default.Those logs get filtered and send to a DynamoDB instance hosted on AWS.Requirements  Node.js  TFX : A packaging tool  Microsoft VSS Web Extension SDK package  Some experience writing in Javascript or Typescript  A Visual Studio publisher account (free)Getting The LogsThis was done using the Azure Devops REST API: DocumentationTo get builds, a couple of things are required:  The Azure Devops organization name  The project name  The build number  AuthorizationWhile the values for point one and two were pretty straightforward gaining access to the build logs required a build id instead of a build number , and since the build id can not be traced through the UI of Azure Devops some nested API requests were required.The Authorization was gained through the creation of a PAT token that can be used as a header in the GET Request.Filtering And Saving FailuresNow that we have our desired logs all that remains is filtering, formatting and sending those logs to DynamoDB.In order to complete this operation the following steps were taken:  Set up authorization in a way that allows developers to use their AWS service connection.  Use the AWS SDK combined with the credentials from the service connection to authorize the user.  Filter the received logs which had a format of plain text using regex to find possible error messages  Format the error messages together with the developer responsible for the build and the time of build.  Push our formatted object to DynamoDBFactory FreshEh Voilà!Our extension does everything we hoped it would do, so it’s time implement it in our pipeline.First we compile our Typescript to Javascript since the index.js file is the default node entry point.We then package our extension, and finally upload it using the management portal where we share it with our organizations of choice.The result:In my publisher account:In the pipeline:Want to check out the code? Take a look at the project repo : GithubThe Berry On Top : Our Physical FeedbackAlmost there!Finally, it is time for the actual dirty work namely snitching on our dear colleagues.To do this I only needed two things, a raspberry pi and a bluetooth speaker.Dollar Store Google AssistantInitially we had planned to use a toy rocket launcher as our physical feedback machine.That idea was scrapped however because of a global shortage (or tremendous price increase) for all hardware components.Next the Google Assistant came to mind which is embedded in Android devices or a Google home speaker.The problem with this idea unfortunately was that the Google Assistant was never designed to take in text input through an API because devices running Google Assistant had no direct endpoints.Now we could in fact work around this and set up a Home automation system like Home Assistant or Node Red but this would mean that our speaker could never change location without reconfiguring access to its new network.A final solution I came up with in secret while my mentor was away on a conference in Valencia, was to run a Node.js server on a Raspberry Pi that after booting takes a reference log from our DynamoDB database.Every minute it would get the latest log available and compare it to the log it had stored, this way when a difference in logs had been found it knew it was time to call out the developer.Through a nifty npm package we are able to send our text message that we want converted to an audio file and get an url in return pointing to an audio file.Using Node file system I programmatically created a text file where each audio url alternated with a path to a local audio file gets written on a line in the previously created text file.What I had just created was a playlist which dictated the order of audio files to be played.All I had to do now was hook it up to a media player.note: This not the ideal solution, but it allowed the Pi and speaker to be portable to any location as long as we could connect it to wi-fi.A succesfull pipeline run broadcast(Sound up):         A failed pipeline run broadcast(Sound up):         The Good, The Bad &amp; … The Ugly? : SummaryLike I mentioned at the start of this blog, there were a number of different project options to choose from as an internship topic.The reason I went with the ci-cd route is that I knew it would pull me out of my comfort zone and broaden my view on development in general.While having touched on basic CLI environments like Docker or Bash it always felt cryptic to use a multitude of flags and if you messed up the error messages weren’t all that clear compared to an error with a web framework like React for example.After 7 weeks of submerging myself mostly in configuration files like .yaml and .tf(Terraform) combined with CLI tools such as kubectl for Kubernetes, psql for Postgres and AWS CLI for AWS Configuration, I’m glad to say that i feel a lot more at home touching on tools not necessarily meant for pure programming.For a total recap of tools and software used during the project, I put together the following image.As a final sendoff I want to say a quick thank you to my mentor Nick Geudens for guiding me through the jungle of DevOps and AWS and to Frederick Bousson for coming up with the project and allowing me the opportunity to execute it."
      },
    
      "leadership-2022-06-30-clean-agile-html": {
        "title": "Clean Agile - Back to Basics",
        "url": "/leadership/2022/06/30/clean-agile.html",
        "image": "/img/2022-06-30-clean-agile/clean_agile_home.png",
        "date": "30 Jun 2022",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  The Reasons for Agile  Business Practices  Team Practices  Technical Practices  ConclusionIntroductionComing from a non-programming background, I was always interested to hear or learn about Agile methodology but with a fear that Agile vocabulary would be too techy for me. This book helped me to get rid out of my fears once and for all. In addition, having been in a High Performance Team using Agile Methodologies at Ordina for 5 years, “Uncle Bob”(Robert C. Martin) helped me via his Back to Basics book to restructure what I experienced and helped me to create my own lessons learned.This book is for programmers and non-programmers alike. It aims not to go into technical details but focusses more on explaining what the fundamentals of Agile are.Agile’s ideology and values emerge from a group of 17 programming experts during the fall of 2000 who were willing to create an alternative of what is called the Scientific Management (which inspired the idea of “Waterfall” development).Four basic values came out of this gathering and are the central idea of the Agile Manifesto that emerged from that moment:  Individuals and interactions over processed and tools  Working software over comprehensive documentation  Customer collaboration over contract negotiation  Responding to change over following a planUncle Bob then gives as an introduction an Agile overview. He does this by responding to the question: “how do you manage a software project”. Therefore, he uses the metaphor of the Iron Cross, a management trade-off that needs to be done in all projects. One has to pick three out of the four goals: Good, Fast, Cheap, Done , in order to manage a project. Reaching all of these in the project is not possible.In this context, he warns that Agile is not the Holy Grail, but only a framework that helps managers and developers to execute the goals they decide to take on by providing real data. This data is needed to make good decisions. Without data, no project can be well managed.This way he introduces some Agile tools (known from everyone that already worked with one of the Agile methodologies): burn-down charts, story points, business value, iterations. These have only one goal: every stakeholder of the project has a constant feedback on how it evolutes. These are thus created not to control the team but to make appropriate adjustments to correctly manage the project.The Reasons for AgileIf you have to read only one chapter of this book, it should be this one. In it, Uncle Bob reminds us that even if many of the Agile tools have been proliferating since 20 years (Scrum, Kanban, XP, SAFe, etc.), it makes no sense to use them without keeping in mind the essence of Agility.There are the two straightforward reasons of using Agile methodologies: professionalism and reasonable expectations of our customer.  Professionalism in the sense that developers are those who design today’s world (software is everywhere). And as the quote of the first Spiderman movie warns: “With great power comes great responsibility”.  Customers have the right to have Reasonable expectations :          Continuous technical readiness: in order to counter artificial delays, the system should be technically deployable at the end of each iteration (Clean Code, Automated Testing)      Stable productivity and inexpensive adaptability: customers and managers don’t expect the project to slow down with time (Continuous Refactoring, Architecture Design, Clean Code)      Continuous improvement: early problems must fade away and the system should get better and better with time (Pair Programming, TDD, Refactoring, Simple Design)      Fearless competence: developers do not have to fear touching an ugly code (Clean Code)      QA should find nothing and Test Automation: If QA finds a problem, development team should figure out what went wrong in the process so that QA finds nothing next time (TDD, Continuous Integration, Acceptance Testing)      We cover for each other: it is your responsibility to make sure that at least one other team member can cover for you (Pair Programming, Whole Team, Collective Ownership)      Honest estimates: developers need to provide estimates based on what they do and don’t know (Planning Game, Whole Team)      You need to say “no”: no matter which pressure is on you, you have to say “no” if the answer is really no (Whole Team)      Mentoring and continuous aggressive learning: as the industry changes quickly, you should follow this flow and learn to teach (Whole Team)      In this state of mind, he concludes this chapter with the XP Customer and Developer Bill of Rights:Customer Bill of Rights  You have the right to an overall plan, to know what can be accomplished when and at what cost.  You have the right to get the most possible value out of every programming week.  You have the right to see progress in a running system, proven to work by passing repeatable tests that you specify.  You have the right to change your mind, to substitute functionality, and to change priorities without paying exorbitant costs.  You have the right to be informed of schedule changes, in time to choose how to reduce the scope to restore the original date. You can cancel at any time and be left with a useful working system reflecting investment to date.Developer Bill of Rights  You have the right to know what is needed, with clear declarations of priority.  You have the right to produce quality work at all times.  You have the right to ask for and receive help from peers, managers, and customers.  You have the right to make and update your own estimates.  You have the right to accept your responsibilities instead of having them assigned to you.Based on those fundamental reasons, the next 3 chapters aim to explain the practices of XP (Business, Team and Technical wise) - keeping in mind that Agile is the foundation of an ethical standard of software. Nothing else.Business PracticesAs Uncle Bob reminds, XP Business-facing-practices are including the concepts of Planning, Small Releases, Acceptance Tests and Whole Team.  Planning and day-to-day management are handled through story points. The concept behind SP is that those points are not estimated time, but estimated effort. The aim for using story points is to give a day-to-day estimation of the workload. Each iteration will help the next iteration to estimate more precisely what still needs to be developed. But, as he highlights: “that estimate is not a promise, and the team has not failed if the velocity is lower. The aim is thus only to produce data necessary to manage the project correctly  Small releases practices are handled through the concept of Continuous Delivery. He mentions that nowadays, the best Source Code Control used is Git thanks to some of its characteristics (no checkout time, no conflicts of committing, tiny decoupled modules, rapid commit frequency, fast-running test suite…)  Acceptance Tests practices are handled through the concept of Behavior-Driven Development (BDD): The business writes formal tests (Given…, When…, Then…) describing the behavior of each story, and Developers automate those tests, which become the Definition of Done  Whole Team practice has been conceptualized to cancel the mental separation between the customer and the developers. “A development team is composed of many roles including managers, testers, technical writers, etc.”Business practices have thus one aim: to increase and facilitate communication between business and developers. “That communication breeds trust”.Team PracticesTeam practices are aimed at governing the relationship between all team members for the sake of the project they work on. These are composed by: Metaphor, Sustainable Pace, Collective Ownership and Continuous Integration.  Metaphor practice is the fact that a model (with its own vocabulary) is created to explain the problem domain in order to get everyone agreed on it (developers as well as management or customers or..)  Sustainable Pace practices remind to keep workload at a life equilibrium, through diminishing as much a possible overtime or “marathons”, and keeping in mind that “sleep is the most precious ingredient in the life of a programmer”  Collective ownership practices defines the fact that even if sometimes a developer needs to specialize in a particular domain, he should also generalize, obligating himself to work on other areas of the code, less known for him  Continuous Integration practices is the fact that the continuous build should never breakFor Uncle Bob, these practices “help small teams to behave like true teams”.Technical PracticesWhile these practices are less used by programmers, Uncle Bob pinpoints that they are the very core of Agile and not using them is creating an “ineffective flaccid shell of what it was intended to be”. These practices are the following: Test-Driven Development (TDD), Refactoring, Simple Design and Pair Programming.  TDD practice are for developers what double-entry bookkeeping is for accountants. Uncle Bob reminds the three rules of TDD but, more important, he explains by an example of how courageous it is to keep the code clean and orderly. Thanks to that, we can act like professionals.  Refactoring is the “practice of improving the structure of the code without altering the behavior, as defined by the tests”. In order to do so, he suggests the Red/Green/Refactor cycle, insisting on the 2 separated dimensions that are writing a code that works versus writing a code that is clean.  Simple design is the practice based on the 4 Kent Beck’s rules:          Pass all the tests      Reveal the intent: being expressive, easy to read      Remove duplication: the code shouldn’t say the same thing more than once      Decrease elements (classes, functions, variables, etc)        Pair programming practice has one goal: to “share and exchange knowledge, not concentrate it”ConclusionAs a conclusion for the book, the 4 core values of Agile are summarized :  Courage: to say no, to rewrite code, to document well, to test well, to take a step aside, etc. “The belief that quality and discipline increase speed”  Communication: whatever form it takes (face-to-face, informal, interpersonal), direct and frequent communication is a must to create a team looking at the same direction  Feedback: giving or receiving feedback is what makes a team working efficiently  Simplicity: through being direct, with the idea of acknowledging a problem when you know there is one.The next pages and last chapter are designed to give ideas on how Agile values are implemented concretely ( how Agile works in small versus big companies, how to transform a non-Agile company to an Agile company, Coaching, Certification, Agile tools) as well as the next direction Uncle Bob thinks Agile should take (Synergy between Agility and Craftmanship). But for me, at this point we divert from the original purpose of this book, which is to highlight the core values of what Agility is.I will thus finish this article with one of his sentences that illustrated to me his mindset while reading the book.  Agile is a small idea about the small problems of small programming teams doing small things."
      },
    
      "development-2022-06-13-connecting-the-pods-html": {
        "title": "Connecting the Pods",
        "url": "/development/2022/06/13/connecting-the-pods.html",
        "image": "/img/2022-06-13-connecting-the-pods/tile.jpg",
        "date": "13 Jun 2022",
        "category": "post, blog post, blog",
        "content": "Although I finished the book “The Connected Company” several months ago, I only recently got around to gathering my thoughts on the matter and putting them to paper. The book itself came out in 2012, which at this point is already 10 years in the past, and it largely builds on the idea of the quantum organization (Ralph Kilmann), on which I already briefly extrapolated when I wrote about the Five Dysfunctions of a Team.The book consists of 5 parts:  Why change? The drivers behind the move towards a Connected Company.  What is a Connected Company? The book offers an attempt at a definition and the listing of the characteristics of such a company.  How does a Connected Company work? The adapted processes and organizational impacts on the company are reviewed in this part.  How do you lead a Connected Company? The hierarchy (and/or lack thereof) of this new type of company.  How do you get there from here? The evolution that the organization needs to go through in order to claim the vaunted title.Part 1: Why Change?The first part aims to explain the different drivers that make the transition to a Connected Company necessary. The idea is to have an abstraction derived from several customer cases. The first chapters go about comparing the organization to a network, where significant events can travel the different nodes of a network and have an impact on all of them. The organization of today is a participant in a multitude of social networks (employees, customers, competitors, shareholders…), and each of these shapes how the power balance within the organization is established.Traditional companies have an information stream that is pretty much one direction. The company would have an unending stream of products that would be shipped out to customers, and so the company is organized and optimized for such an exchange. However, this industrial age has given way to a situation where the feedback from these customers will change how the company goes about things and forces companies to create customer relationships where they will have to listen to these customers. This service economy necessitates new habits, behaviors, and organizational structures.The author states that the change towards this service economy is supported by three factors: product saturation, information technology, and urbanization. There are some issues with this analysis as far as I’m concerned. Or rather some tweaks I would add with the benefit of ten years of hindsight. While product saturation is indeed present in most industries, the notion that Customer Intimacy will always be victorious over Product Leadership or Operational Excellence seems a bit curt. I would half expect Customer Intimacy to work better in a blue-ocean-type of industry where the saturation has not yet peaked. As for urbanization, this is a concept I would switch out for globalization with the social networks of the internet generating a much greater impact. Information technology is then just an extension of this globalization, making such social networks possible.Around the time of this book’s release, there was similarly the notion of Social Business Process Management rearing its head. Social BPM refers to a twofold mindset. On the one hand, it uses Web 2.0 and Enterprise 2.0 style collaboration portals where participants can share and influence process design and certain decision-making activities during process execution. On the other hand, we also speak of Social BPM when we allude to leveraging the organization’s outward social media resources into internal business processes.In 2014 Gartner had retitled the first of the mindsets as Design-By-Doing BPM, as placed it thus named on its Hype Cycle, as seen in the illustration below. While supported by advanced BPM technology and social software in order to make process design more visible and holistic (thus guaranteeing the “emergent” design), it also requires a user experience perspective and their willingness to step into such an endeavor so that social networks of employees can be cultivated and mobilized around the idea of bottom-up process improvement efforts.The critical difference between services and products is this more direct co-creation with the customer. They decide where, when, and how the services are delivered. This makes services highly contextual and places the value of services to the customer in the interaction (or customer experience). To draw the similarities with the social web even further: Products can then be considered the end result of a service to the customer and become the physical manifestation or avatar of the service.It seems like common sense to care about and keep track of customers. But when companies start to grow, a lot of distractions have a way of muddying the waters with other preoccupations, resulting in a decline in customer focus. These distractions can range from a multitude of new opportunities to internal efficiency drives, over-expansion (with product dilution as a result), and so on. This can also lead to an automation of processes to handle the increasing complexity with the caveat of these automations becoming too restrictive, and limiting the creativity and adaptability of your services. Where the author argues this case, I would however stipulate that over-automation is easily mitigated by determining which of your processes are run-of-the-mill and which add value to your unique selling point or differentiator. The former are ideal candidates for such an automation drive (or standardization) whereas the latter are more suited for custom development that allows for easy adaptation. This comes down to finding a balance between efficiency and adaptability/innovation.Another game changer is the number of companies that go belly up. This is largely due to the complexity of the market increasing, with new technologies popping up all over and disrupting the market in addition to the previously mentioned globalization which increases the number of competitors companies have to deal with. This might have seemed like a new reality, but remember the phrase “Innovate or Die” by Peter Drucker who was already advocating such disruptions back in 1985. According to the author, the solution to this disruption is the Coevolutionary Process. Every time you adapt, the others in your industry are forced to do so as well and vice versa. This process can be competitive, but also cooperative with tactical alliances giving your company the leg up it needs to stay ahead of the rat race. There are other forms of disruption, but these are what the author wishes to focus on.Part 2: What is a Connected Company?Companies must stop acting like machines, but should behave as learning organisms who interact with their environment and improve based on experiments and feedback. This is the premise of the author and reminiscent of the argumentation of Friedrich August von Hayek, renowned economist and contemporary (although in theories diametrically opposed) to John Meynard Keynes. F. A. Hayek would go on record stating the economy is organic, not like a well-oiled machine that can be fixed as stipulated by Keynes.The comparison of a company to a machine comes from the scientific management theory of Frederick Taylor and has helped some of the great industry giants get to where they were at the pinnacle of their profit. Knowledge is explicit and absolute and can be represented in operating manuals, documentation, and metrics. These “machines” have the following characteristics:  The machine needs to be controlled by an operator or driver.  It needs to be maintained, and when it breaks down, you fix it.  A machine works the same way its entire life, and when it is no longer useful, you build a new one.I do not think this comparison holds up to a company, but arguments can be made to equate it to value chains or processes within a company. This static nature of the process will come into conflict with the changes all around us, and we need mechanisms to deal with this change. It will also clash with people’s inherent resistance to being controlled. The author will propose the Connected Company approach for this.Where the Connected Company wants to make a paradigm shift, is to move the focus from procedures and rules where there is the idea of an optimal solution that balances the needs of the company with those of the customers towards a company putting the focus more on the customer and to become more like a support system for those customers. Ashby’s Law (also called the Law of Requisite Variety) is mentioned in one breath with this view: Any control system must be capable of variety that is greater or equal to the variety in the system to be controlled. However, this seems to assume that processes within companies are fixed, while the whole discipline of Business Process Management is predicated on the fact that a process is never finished, and that there is always room for improvement, either induced by internal or external factors.In any case, a case is made for customers resisting standardization, and that to meet such required variety, the company needs innovation. One of those ways to achieve this is to give the employees of the company the freedom to experiment. This also benefits the employees in their learning, which is fundamentally different from receiving training. Learning is a way to deal with new situations, and how an employee deals with the uncertainties that might be encountered.It is also quite comical to see that the author later on in the chapters talks of the Net Promoter Score (NPS) as a way to track customer satisfaction as if the NPS is not a metric. NPS is a score that attempts to measure the customer perception of the company by asking the customers to rate it on a scale of 1 to 10. This puts each customer in one of three categories based on the score they gave:  Promoters: The most avid fans of your services. These customers will regularly buy your products or services and recommend them to others.  Passives: Customers that are willing to shift to one of your competitors if a better deal comes along.  Detractors: These are the malcontent customers that might even dissuade others from buying your product or services.Your overall company NPS equals the percentage of protomers reduced by the number of detractors.Going on the findings from The Living Company by Arie De Geus, companies that stand the test of time share certain similarities:  Distributed Control: The company should be decentralized and connected to a wide array of partners.  Strong Identity: Although the organizational structure was loosely coupled, the company culture was known and carried by all employees.  Feedback Loops: Companies should take their cue from the world they inhabit, reacting to feedback and improving their workings based on it.In essence, companies should move from a design by division to a design for connection. It should not be defined by its individual pieces within its own borders, but rather by the partnership it can form and the information it can extract from these partnerships, be they with customers, suppliers, strategic partners, or even competitors.While it is important for companies to turn a profit, there are good profits and bad profits. This is one of the purposes of a company, much like proving to be a worthy investment for shareholders and providing a livelihood for its employees. Good profits are derived from creating value for your customers. Bad profits are those made at the expense of the happiness of customers, prioritizing the maximization of short-term profit over long-term sustainability. While this type of profit maximization might be to the benefit of the stakeholders, the other parties (customers and employees) will feel the backlash. All in all, the company should put maximum effort into aligning its performance to what it promised, and furthermore aligning that promise to what customers actually need or want (purpose). This balancing exercise will aid tremendously in veering towards good profits. This can once again be mapped to the 3 categories of business strategy: Customer Intimacy, Product Leadership, and Operational Excellence.Part 3: How does a Connected Company work?Business agility is the name of the game and the Connected Company tries to address this topic through the standard technological ways that are available in an attempt to make the company more built for agility:  Agile Development with small autonomous teams  Service Oriented Development with service contracts, a focus on composability, and loose couplingThe idea is to transform a business into a podular organization. This is an approach that combines the traditional organization split into divisions with pods where you divide the required work into “businesses within businesses”, which can function independently from other pods, and take on the responsibility for a complete “service”. This allows the organization to represent itself as a group of smaller organizations. Each of these pods is autonomous and can function on behalf of the business as a whole.In a traditional company with divisions, the need for a Corporate Strategy occurs when an enterprise develops multiple divisions (typically linked to separate value chains), each with a distinct business strategy. This is for example the case when a company starts diversifying its products or services. Based on how the corporate strategy is handled across these divisions, we can segment the types of diversification into 4 main categories. Single Businesses are centralized and can be defined as having a predominant business strategy that applies to almost all of their business units. Dominant Businesses still have business strategies that vary and take up a significant part of the enterprise, but overall they follow one centralized strategy that accounts for most of their enterprise. Next, we have the diversified businesses, which are comprised of several business strategies, be they related to each other (for example they are all still targeting the technological markets) and thus able to share a value chain, or be they completely unrelated (typically conglomerates). The podular organization attempts to take this conglomerate idea to the next level, and apply it within individual divisions.So what is a pod? A small, autonomous unit that has the mandate and the capabilities to deliver whatever it is that its customers value. It is designed to take decisions and implement changes as quickly and as close to the customer as possible. Whereas the traditional company employs processes that can be seen as recipes or a series of steps that need to be executed, the podular organization is structured as a net, distributing the workload based on goals instead of allocating individual steps or stages. And the pods are not necessarily part of the same organization or company but could extend across several to couple its inherent benefits with more economy-of-scale oriented benefits.This gives a number of advantages based on its lattice-like structure, exchanging consistency and efficiency for flexibility and creativity:  Pods are flexible: changing the structure and purpose is easy and fast.  Pods are allowed to fail as the network will provide redundancy by adjusting the purpose of similar pods.  Pods can scale up fast by adding additional pods and letting them immediately contribute.This type of flexibility cannot exist in a vacuum. It needs a proper support system (or platform) to make it happen, necessitating a shared infrastructure for all available and future pods, such as for example a proper cloud adoption. Although it might seem quite a daunting task to put in place, this shared infrastructure offers numerous benefits:  More traffic brings more customers and in turn more business.  It allows for a focus on differentiators, making it possible to prioritize certain services and products that are “hot” at the moment, only to switch to others when the need arises.  The commonality between these pods (common technological standards and shared company culture) generates trust with customers who can see a rise in the overall quality of all services and products offered.As with all organizations, the connected company needs to continue to evolve. This is done by learning, and the connected company sees learning as a growth spiral called successive approximation. The connected company takes actions, which leads to feedback and discovery based on the outcome of such actions. Once these are collected, you reflect upon how to change the way actions are taken to improve their result. When compared to more traditional ways of learning such as the Double-Loop Learning, to me, this seems like the reflection part is portrayed as a bit too straightforward. If the results of your actions are the only thing to reflect upon, the reframing of the mindset is missed.To go a bit more into detail on this type of learning: Professor Chris Argyris of Harvard Business School identifies three possible levels of learning in an organization, which he coined single-loop, double-loop, and deutero learning. The idea is that with single-loop learning we adjust the capability so that variances in results are addressed. We can look at this as attacking the symptoms of a supply chain problem. With double-loop learning, we search for the under-lying reason for the symptoms of a problem and try to remedy this reason in order to make sure the symptoms no longer occur. One might conclude that double-loop learning is a sounder strategy for attacking problems, but this is incorrect. Both are needed for a balanced resolution of the issues with a supply chain. Take for instance the occurrence of a fire. One needs to address putting out the flames (symptom) before being able to tackle the underlying reason. The balanced application of both of these levels is called deutero learning, which is the preferred manner of proceeding.The successive approximation approach in the book focuses on three levels of learning: How entrepreneurs learn, how companies learn, and how the platform learns.The entrepreneurial method for learning (as stated by Saras Sarasvathy) states that entrepreneurs focus on the qualities they possess within the organization and attempt to utilize these to their greatest effect. This translates into the following points:  Entrepreneurs do not start with predictions, but rather with their cares, experiences, and network of people. The learning is based on interactions with people, trying things out on a small scale, and asking questions.  Entrepreneurs seek commitment from the people in their projects.  With the commitment of people to projects, these projects gain momentum, new capabilities, and an expanding roster of committed people. This reflects on the projects through realigning the goals with their new reality.  With the accumulation of resources, the project coalesces in a working model typically coupled with a new and innovative offering.Organizations learn through two distinct flows. Tacit Knowledge is the knowledge that is being picked up through doing. Experience in the truest sense. Explicit Knowledge is the knowledge that is being shared, measured, and documented… Both are valuable and the organization needs to come up with ways to combine these for maximum effect, for example installing learning spaces where colleagues can come together and share their experiences to achieve the desired cross-pollination.How platforms learn is likened to the Pace Layered Architecture that was popularized amongst others by Gartner. I have already given my views on this in a previous blogpost: Any Road Will Get You TherePart 4: How Do You Lead a Connected Company?The book states that strategy happens at all levels, but instead of a corporate strategy filtering down to the business units that make up the company, it should be more like a pool of experiments at all levels from which senior management can cherry-pick to achieve success. This would seem to me to imply that there is a definite risk of squandering opportunities where the opportunity cost far outweighs the benefits such experimenting brings with it. On the other hand, this will help innovation tremendously with the possibility of higher returns. So this would be a high-risk high rewards type of situation much in line with blue ocean strategic thinking. It shifts strategy from being driven by prediction to being driven by discovery. This is referenced by the book as the content of Eric D. Beinhocker’s book “The Origin of Wealth”, where he labels these two strategic approaches as deliberate and emergent strategy.Another view on strategic approaches comes from colonel John Boyd, whose OODA loop formed the inspiration for the previously mentioned double-loop learning. He states three facets of conflict (or in our case competition with rivaling companies): attrition warfare, maneuver warfare, and moral warfare.Attrition warfare is when companies will invest huge capital to corner the market and push out competitors. This will work in certain scenarios. Typically in saturated markets where the only discriminator will be price, such tactics can yield the desired result. But this is an expensive proposition.Maneuver warfare focuses on new technologies to leverage them in the same way cash is used with attrition warfare. To translate this concept to the Connected Company, it needs an emphasis on learning and adapting to new technologies to faster seize opportunities, and as such maximize your advantage in the market. The more you distribute control, the faster you can learn and “maneuver”.The last of these is moral warfare where you play on the minds and emotions of the customer base while vilifying your competitors. The greater your moral authority, the more cohesive your company culture and your people are, and the greater the impact they can deliver. In essence, think people first! Attracting the good and competent people, keeping them, and elevating their worth through learning, gives your company a head start in the competition race.Management in the end comes down to having a single purpose for the organization: To design, run and continuously improve the systems that enable it to effectively pursue its goals. A shift needs to be created in traditional management from organizing and supervising (or the well-known command and control management style) to a management that is focused on support (what we label servant leadership) with the previous style on the backburner. Flexibility and structure must be balanced so that the system has enough formalized structure to avoid endlessly reinventing and repeating routine work, and enough adaptability and freedom to not overly constrain work and creativity. This balance is denoted by the so-called tipping points or critical values in your complex adaptive system. Once your cadence moves past such a tipping point, the dynamic of your workforce and productivity shifts, and needs addressing. This is called “tuning the system”, and aims at keeping the organization in the goldilocks zone of that balance.In order to apply all of these strategic and management concepts, a company needs to have the proper tools in the organizational toolbelt. These are such tools as situational awareness (sensing and understanding the context of its ecosystem to judge changes and their potential impact), dealing with adaptive tensions that are constantly happening in its ecosystem, information transparency (with Lord Kelvin’s “measuring is knowing” adage), and diversification both in terms of products and services as well as people to ensure the company can evolve. The book also mentions command intent, a term used in the military to depict a common goal to work towards. For companies, this becomes a broad vision that leaves the individual pods a large degree of freedom in how to put plans into action (leaning back to the principles of deliberate strategy).Part 5: How do you get there from here?The final part of the book is the smallest by far, a mere 25 pages. It deals with what to do to transform your organization into the podular variety, and what risks might be associated with such a transformation. This change can be kickstarted in four distinct ways: organic growth, top-down leader-driven change, pilot pods, and network weaving.The organic growth of a company happens when it starts as a small entity and slowly builds itself up. Typically you will find small teams in such a startup that are acting in a similar manner to the pods in the podular organization. The focus, in this case, should be on a shared purpose, let autonomous pods spring to life naturally, and create additional growth through replicating pods by moving people already familiar with the workings of pods into the newly created ones.Top-down leader-driven change is a more difficult path that demands commitment from the company leadership to steer it in this new direction. There are certain themes that need to be incorporated into this type of change, notably: a focus on customers, avoidance of abundant bureaucracy, moving the decisions closer to where the customers interact with the company, fostering teamwork, and putting your people first as they are the ones that will need to make the pods successful.Pilot pods are proof-of-concept style pods that can be used to feel things out on how to proceed with this change. They are useful to gather experience and find the proper balances that need to be struck in strategy and execution. These will be outside of typical company structures and should therefore have enough leeway to figure out whether they will be successful, and not be written off at the first signs of trouble.Network weaving is the last way to get your company on the podular bandwagon. By forging better networks and more connections, the company becomes more effective and adaptive, as there is a cross-pollination that will deliver the needed expertise and knowledge workers that can precipitate the change. This is however the slow-burn step-by-step approach and will require more time than the previous options to become a full-fledged podular entity.The journey can be perilous, and there are risks that can be expected in the form of failures. With this type of transformation, these risks should be mitigated as soon as possible when they present themselves. The failures can happen at different levels:  Failure at the pod level: The distribution of control is very important. As stated earlier in this post, the balance between deliberate and strategic management determines the proper strategy. The level of autonomy given to each pod is an equally important equilibrium. Too much autonomy for any pod can become a liability for the company and severely limit the ability of other pods to function properly. Too little autonomy will infringe upon the pods’ creativity and the ability to learn. This is a fine line that can only be pinpointed through adequate preparation and trial runs.  Failure at the platform level: As the platform is needed to allow for this podular way of working, there are certain requirements attached to such a support system: The proper amount of investment needs to be made to have a platform that will suit the needs of this type of organization. There is also the same danger at this level as was in the previous: The danger of over-controlling the platform, or making the barriers to entry too strong by asking too much in return for using the platform (be it conditions or cost or any other quid pro quo). There should be a burden shared by all participants, but this should not outweigh the benefits.  Failure of purpose: The focus should remain on the vision of the company, and not just focused on turning a profit. This can lead too easily to the disintegration of the overall company culture as pods migrate further and further away from each other in the pursuit of the bottom line.ConclusionI found the book to be written in a straightforward but not very deep-delving way. As such I would recommend it for non-business-minded consultants to have an introduction to this subject matter, “For Dummies”-style. It is a starting point to get the general frame of mind before delving deeper into organizational concepts, business strategy, and other related areas. It does a good job of lifting the veil enough to get newcomers excited about what else is out there to be learned."
      },
    
      "cloud-2022-06-13-aws-rds-iam-authentication-spring-boot-html": {
        "title": "RDS IAM Authentication with Spring Boot - secure password-less database authentication on AWS",
        "url": "/cloud/2022/06/13/aws-rds-iam-authentication-spring-boot.html",
        "image": "/img/2022-06-13-aws-rds-iam-authentication-spring-boot.jpeg",
        "date": "13 Jun 2022",
        "category": "post, blog post, blog",
        "content": "  Introduction  Database role grant  AWS  Spring Boot  ConclusionIntroductionDatabase authentication in Spring Boot has traditionally always been very simple to implement and configure to your needs.Whether it is with a custom data source, or just using the spring.datasource properties, it was always pretty straightforward to get a working database connection.Handling the sensitive data such as database passwords in different environments can be tricky, but nowadays, there are enough solutions to tackle this problem in a secure and discrete way.Fortunately, if you are deploying your application on the AWS cloud, you don’t have to struggle passwords anymore or need a fancy tool, because you simply don’t need a password.You are now able to establish a database connection by authenticating through IAM. Note that this feature only works for MariaDB, MySQL and PostgreSQL.The feature works with “authentication tokens”, which is a string of characters that is unique and generated by Amazon RDS.One authentication token has a lifespan of 15 minutes and needs to be re-generated before or after it expires to keep the connection alive.This very useful mechanic can deeply strengthen the security of your application by erasing the need of a password and therefore, destroying another opportunity to get your sensitive data in the wrong hands.Note that you need to explicitly enable IAM authentication before you can use this feature.Database role grantEnabling IAM authentication alone is not enough.When you create a new database user for your application, you also need to grant the rights to the database user to fetch an authentication token to use in the authentication process.For Postgres, this can be done with the following SQL query:CREATE USER &lt;databaseuser&gt; WITH LOGIN;GRANT rds_iam TO databaseuser;For MySQL:CREATE USER &lt;databaseuser&gt; IDENTIFIED WITH AWSAuthenticationPlugin AS 'RDS';AWSThe application can only fetch a new authentication token if it has the right to do so.Since we are running our application on an AWS environment, it only makes sense to make use of IAM roles.{   \"Version\": \"2012-10-17\",   \"Statement\": [      {         \"Effect\": \"Allow\",         \"Action\": [             \"rds-db:connect\"         ],         \"Resource\": [             \"arn:aws:rds-db:region:account-id:dbuser:DbiResourceId/db-user-name\"         ]      }   ]}If you are running your application on an EC2 instance, make sure that your EC2 instance has an IAM role attached to it which explicitly gives access to the rds-db:connect action.On EKS (Kubernetes for AWS), you can use IAM roles for service accounts.Spring BootYou will only need the AWS SDK for RDS to fetch an authentication token, which you can add as a dependency in Maven by adding the following code block:&lt;dependency&gt;    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;    &lt;artifactId&gt;aws-java-sdk-rds&lt;/artifactId&gt;    &lt;version&gt;1.12.238&lt;/version&gt;&lt;/dependency&gt;To achieve the token fetching in Spring Boot, I created a custom HikariDataSource which overrides the getPassword() method and used RdsIamAuthTokenGenerator and GetIamAuthTokenRequest to create a request for an authentication token from RDS.public class RdsIAMDataSource extends HikariDataSource {    @Override    public String getPassword() {        RdsIamAuthTokenGenerator authTokenGenerator = RdsIamAuthTokenGenerator.builder()                .credentials(new DefaultAWSCredentialsProviderChain())                .region(new DefaultAwsRegionProviderChain().getRegion())                .build();        URI jdbcUri = parseJdbcURL(getJdbcUrl());        GetIamAuthTokenRequest iamAuthTokenRequest = GetIamAuthTokenRequest.builder()                .hostname(jdbcUri.getHost())                .port(jdbcUri.getPort())                .userName(getUsername())                .build();        return authTokenGenerator.getAuthToken(iamAuthTokenRequest);    }    private URI parseJdbcURL(String jdbcUrl) {        String uri = jdbcUrl.substring(5);        return URI.create(uri);    }}Note that for MariaDB, you do not need your own custom implementation as they have an implementation of their own.To make sure that our connection does not drop due to authentication failures, we are going to set the maximum lifespan for the connection (remember that authentication tokens are only valid for 15 minutes).Don’t forget to set your database properties (host, port and username). Important note: IAM authentication to the database requires an SSL connection.# Sets the max lifetime of each connection to 840000 ms (14 minutes). After 14 minutes, the application does a new request to RDS for a fresh authentication tokenspring.datasource.hikari.max-lifetime=840000# JDBC url with the properties 'ssl' and 'sslmode', this properties are bound to PostgreSQL instancesspring.datasource.url=jdbc:postgresql://&lt;db-instance-name&gt;.&lt;random&gt;.&lt;region&gt;.rds.amazonaws.com:&lt;port&gt;/&lt;db-name&gt;?ssl=true&amp;sslmode=require# Database user which has the 'rds_iam' rolespring.datasource.username=&lt;db user&gt;With this implementation, the getPassword() method will be called every 14 minutes.ConclusionSecurity is an important factor in many companies and environments, and this is definitely a recommended way of securely accessing your database from inside your application as it is a big improvement over the traditional username/password authentication method.You don’t have a password, which means you don’t have to share it, so you have zero risk of exposing your password to unwanted parties. Not having a password also eliminates the need to manage passwords and to rotate passwords every now and then.With the authentication token lifespan of 15 minutes, each generated token is secure and expires rapidly in case the token gets leaked.While it is slightly more of a hassle to set it up, it is definitely worth your while to implement RDS IAM authentication, especially if you are already running your application and infrastructure on AWS."
      },
    
      "cloud-2022-06-10-kubecon-html": {
        "title": "JWorks @ KubeCon 2022",
        "url": "/cloud/2022/06/10/kubecon.html",
        "image": "/img/2022-05-20-kubecon/kccnc-eu-2022-color-resized.png",
        "date": "10 Jun 2022",
        "category": "post, blog post, blog",
        "content": "  KubeCon - CloudNativeCon - JWorks was there!Table of contents  Introduction  Key takeaways  Meet the vendors  ConclusionIntroductionKubeCon | CloudNativeCon is a conference organised by the Cloud Native Computing Foundation twice a year, alternating between Europe and US locations. This edition took place in Valencia in Spain. The conference is the industry conference for anything Kubernetes and container-related technology. Talks range from end-user stories of the usage of Kubernetes, over methods to automate compliance in Cloud to technical deep dives into specific cloud-native building blocks.Next to the talks, there’s a solution exhibition as well.This is where vendors and (CNCF) projects have booths and you can talk to core contributors of most of the projects.Key takeawaysOpenTelemetryOpenTelemetry is a CNCF project focused on capturing different telemetry signals from applications.It’s a collection of tools, SDKs and APIs that enable effective observability.In essence, it’s an ecosystem of tools that can instrument, capture and ship (currently) telemetry data to different systems. The idea is that the tooling to do so is provided by OpenTelemetry and the analysis, usage and storage are performed by other third-party tools like Dynatrace, AWS X-Ray, Elastic, Grafana LGTM or any other tool supporting telemetry data.Supply chain securityThis was more of a general topic than a specific product or talk. During one of the keynotes, it was mentioned that there is a clear increase in software supply chain attacks from multiple vectors. Multiple talks discussed how provenance could be used to determine if an image is safe to run aka went through all the necessary steps to end where it’s being deployed.Especially the talk Fun with Continuous Compliance by Ann Wallace and Zeal Somani of Shopify and Google provides an introduction into how that could work and showcases how Shopify has this setup in their environment. It also showcases how you can link compliance evidence gathered from your systems directly into audit reports using the OSCAL standard.NATSBack in 2019, one the hottest topics of KubeCon (North America) was the introduction of NATS. Now, 3 years later it seemed to have gained a lot of popularity. Most of the interesting projects that where presented during talks used NATS for communication between their distributed microservice architectures.NATS is an infrastructure is a message oriented middleware that allows applications to store and distribute data in realtime in a general manner across various environments, langauges, cloud providers and on-premises systems.Agua de ValenciaAgua de Valencia is not water from the city of Valencia as the name suggests it to be, but rather a different drink. The main thing that can’t be ignored from the Valencian scenery is its orange trees. The juice of these oranges forms the base of Agua de Valencia. Agua de Valencia is a cocktail containing orange juice, cava (the Spanish interpretation of champagne), gin, vodka and sugar, and it turns out to taste very nice in combination with Tapas after a long day of KubeCon.Meet the vendorsCast.ai cost optimizationWe met Cast.ai at their booth because we were triggered by the slogan “half your cloud cost”. Cast.ai is a SaaS solution aimed at reducing the cost of a Kubernetes setup by performing multiple (AI-based) optimizations. They work by installing an agent into your cluster and analysing all workloads that are deployed.Next, they provide a detailed report of the improvements you can make.Their main approach is two-fold.RightsizingBased on the analysis of all workloads, Cast.ai will provide a suggestion with nodes that are sized just right for the workloads on your cluster.Instead of using just a single VM type per node pool, they look at the workloads and which nodes would be the most cost-effective. Since this approach optimizes for the CPU, memory and storage requests in your cluster at a moment in time, you’ll have to set them correctly as well.Spot instancesThe second suggestion or approach they have is the usage of spot instances. When spot capacity is available in your selected regions, they will spot instances and attach them to your cluster.Since spot instances are vastly cheaper, this can generate up to 60% in savings. One of the requirements for this approach to work is that either all workloads have multiple replicas, or the cluster can sustain some workload downtime (e.g. integration cluster, some development cluster, …).VMWare TanzuVMWare was present with their Tanzu team. They showcased how Tanzu solves the software supply chain challenges using open-source software. Ranging from tooling to support an in-house training self-service portal to the native building of images using Contour (CI) and KPack (Tanzu Build service) to managing large fleets of Kubernetes clusters across multiple (public) cloud providers, VMWare Tanzu solves all of it for you.Meet the city: ValenciaOf course, we also explored the city of Valencia during the hours that the conference was not in session. We can recommend having drinks at the bar in the Mercat de Colón. The city center has good tapas throughout and especially if you go to the Ruzafa district, we can recommend Trufa Salvaje.ConclusionIt was clear that no expense was saved by vendors to make their products visible to people joining KubeCon.Portworx advertised “the old way” by buying ad space on old fashioned billboards next to the highway to the conference center and decals on the city metros.Gitlab had bought what seemed like all the advertisement space in the airport. It was amazing, crazy and weird at the same time to realize that those ads were there specifically to target people going to the conference.To us, there was a clear focus throughout the conference on three topics: Supply chain security, observability and FinOps. Both in the talks and the vendor area, it is clear that these topics are hot and a lot of companies are working on improving these spaces.The most exciting CNCF project to watch in the next 12 months is OpenTelemetry for sure. They have a lot of traction in the community and seem to become the defacto standard for shipping any observability signals.Finally, the most important takeaway from this conference is that being together as a community again is the best thing ever.Talking with co-workers about talks over drinks, talking directly to maintainers of projects you only knew by their Github or Slack handle previously and connecting with vendors that solve problems you didn’t even know you had, is simply priceless. Our team came back from this conference full of new ideas on how to improve our different projects and how to support customer use cases even better in the future. Connections with key people at vendors and open source projects have been made which make reaching out and discussing things in the future a lot easier.TLDR; Good talks to watch  Prometheus intro  OpenTelemetry intro  Community Manager talk  ArgoCD overview  Good public post-mortem“Great things in business are never done by one person. They are done by a team of people”. –&gt;"
      },
    
      "leadership-2022-05-05-collective-intelligence-html": {
        "title": "Collective Intelligence",
        "url": "/leadership/2022/05/05/collective-intelligence.html",
        "image": "/img/2022-05-05-collective-intelligence/collective-intelligence-home.jpg",
        "date": "05 May 2022",
        "category": "post, blog post, blog",
        "content": "  Collective Intelligence - The Best of Synergy.Table of contents  Introduction          In which situation is the collective intelligence useful?        Content          Rules      Fundamentals                  Fundamentals for the emergence of collective intelligence          Fundamentals to ensure the quality of the interaction$                    Process at a collective intelligence meeting      Illustration of a potential Intervision at Ordina        Conclusion          What collective intelligence is not      IntroductionHave you already been in a meeting where:  All members do not participate in the same way?  Some speak all the time – even to say nothing – while others barely speak ?  Different opinions lead to battles of the egos that prevent a project from functioning well?  A solution is decided by decision-makers that is beside the point?We live in a culture of expertise and compartmentalization. To face a problem, we usually call on experts, specialists in several domains : we let our decision-making power in their hands.Collective intelligence makes the bet that people in a collective are good enough to combine their expertise, their creativity, their capabilities, and their natural aptitude to apply a solution that they will have generated themselves.In the old hierarchical management system, managers had two contradictory objectives :  They needed to count on the participation, the know-how, and the initiative of collaborators  They were tempted to control themThis management system is sadly still operating in our society.The principle of collective intelligence approach is very different. The aim is no longer to control the people but to control the process. This framed process will create a common and secured envelope around the group, that will support responsibility and freedom of each member.Collective intelligence is based on three values: peace, freedom and responsibility. These values use several tools such as cooperation, trust, sharing, empathy, and kindness in order to create a social intelligence that transcends the sum of individual intelligence.We can use collective intelligence in many interactions, including the professional domain – where Collective intelligence is usually called Intervision – : from a small project meeting to a strategic transformation brainstorming. While keeping his leading role, the leader is no longer there to manage each collaborator, but to support a bigger structure that surpasses him.The processes of collective intelligence / intervision do not put different skills in competition but multiply expertise in a collaborative way. The results are more creative, innovative and rewarding.In which situation is the collective intelligence useful?Collective intelligence is useful in all kinds of situations where we feel that the human being would profit from:  Sharing his resources and utilizing those of others.  Developing more innovation, more shared intelligence, more agility.It is very powerful each time a complex situation needs to be managed with many diverse actors.ContentTo implement collective intelligence, we need to follow simple but rigorous rules. These rules aim not to constrain people but to support their freedom, creativity and responsibility.RulesRules can first be described by the collective’s components:  The place, which needs to comfortably welcome the participants: bright, airy, with enough space to display group productions.  The sponsor, the organizer: he carries the intention that unites the participants.  The team, that takes care of all the necessary materials needed for the meeting.  The facilitator, the keeper that supports this space of co-creation, by verifying the rules are kept and facilitating the process during the meeting.  The process: simple, solid, helping creativity and innovation to pop up through diversity and expertise valorization.FundamentalsFundamentals that help the emergence of collective intelligence are:  Organization in circle: representing humanity equivalence. Keeping in mind that everyone speaks from his role in the group.  Turn to speak: inviting everyone to voice their concern.  Power of silence: through the turn to speak, or proposed during the meeting, this moment helps to mature the ideas that have emerged from the interaction.  Speech in the center: there is no debate in collective intelligence, no judgment. Everyone speaks in his name, his role and his responsibility.  Addition and multiplication of opinions: helping innovation instead of competition.  Rules and processes kept in mind: the facilitator is the guardian to ensure that rules and processes are followed by the collective.  Everyone is responsible for himself: his emotions, his needs and his acts.Fundamentals to ensure the quality of the interaction:To ensure a good and trustful relationship between participants and to avoid overflows, some pre-requisites are necessary :  Attentive, benevolent and active listening to what is being said.  Active listening is made with empathy and kindness.  Differences and disagreements are expressed without judgment or aggressivity.  Everyone’s speech is respected, which does not mean everyone has to have the same opinion.  Speaking in our own name : preferably using “I” instead of “You”, without judging anyone’s opinion.  Silence as well as the need of withdrawal are respected : evolution in communication can be adapted by protocols created by the group.  Communication is fluid : protocols can be created in order to ease communication – time to ask questions, to issue a problem or to emit a proposed solution can be defined by the group.  Confidence is primordial : participants can speak freely during the meeting without fearing that their confidence will be violated outside the meeting.  Speaking should be concise and straightforward : while speaking, we always must keep in mind the interest of the group.Process at a collective intelligence meeting  Step 1 – Introduction - A routine can be decided by the group before beginning a new session. That can be a breathing exercise, or a song we can dance on. This aims to help devote one’s time to the meeting, to be present in the moment before beginning the speaking round.As an introduction, we can realize a “weather tour” in order to understand in which state of mind we are. The goal is to take care about the inclusion of each participant within the group.  Step 2 – Content of the meeting - Everyone at his turn announces a topic, a problem. Within the group, a prioritization is made between all topics. One after the other, they are discussed from the most important to the less one, regarding the time spent on the reunion.  Step 3 – Conclusion - At the end of the session, a moment is given to conclude in order to help everyone to give his feedback about the session.Illustration of a potential Intervision at Ordina  Let’s imagine an intervention between Jworks colleagues to improve the department community. Beforehand, a security framework had been established. This framework will help members to feel in a safe environment to be able to speak without judgment feelings.  The facilitator suggests an introduction tour to let people be present for this moment of gathering.  The facilitator suggests to enter the content where everyone will be able to give the problems they face or the ideas they have in mind to improve the department.  A prioritization is made between all topics. The first tour, everyone can vote for multiple topics. If some are ex-aequo, we only can vote for one of them.  The group begins with the first prioritized topic. The person who gave the topic tries to be concise and precise to explain it.  After first explanations, other members of the group ask clarification questions.  After clarification, the topic owner decides how he wants to handle his topic.   Many tools of collective intelligence can therefore be used. The “if I were you” is one of them.  When the topic owner decides, a closing tour is made, starting with him. The next prioritized topic is addressed. Etc.  The timekeeper keeps his role in mind until the end of the session. He usually pinpoints when it is 5 minutes before the end.ConclusionWhat collective intelligence is not  Working in collective intelligence does not mean there is no structure or hierarchy. Everyone has their role and responsibility regarding the whole. Hierarchy of power leaves space to hierarchy of responsibility. Each member assumes his own responsibility and respects the responsibility of the others with profound respect. This idea can be translated into two principles: “everyone has the right to a place” and “everyone must be at his place”.  Collective intelligence does not always work without a hustle. Cooperation between humans and relational maturity is feasible but, as for every relationship, the way to create innovation, agility and reward is not always easy. Every collaborator needs to find his space of expression while knowing his responsibility within the group. Those processes ask each member to stay at his place, which can be sometimes painful. That is why a facilitator is necessary to welcome and guide those moments.  Collective intelligence is not the Holy Grail. Collective intelligence can be seen as very aggressive from a system that is not ready to transform. This can be the case when the system is based on a power struggle, is very rigid or with fears and limiting beliefs. As collective intelligence is proposed and never imposed on a collective, we usually quickly feel the “go” or  “no go” of the group.  “No go” signs can be expressed as repeated absences or role confusion. In that case, it is preferable to stop the process, wait and try to understand the fears that are expressed.Therefore, while not being the holy grail, collective intelligence can be seen as the hat of Mary Poppins:  we play with processes, we open a space for dreams and creativity, we draw, we laugh, before converging into a collective and rewarding action plan.“Great things in business are never done by one person. They are done by a team of people”."
      },
    
      "cloud-2022-02-14-postgres-ai-html": {
        "title": "Testing with production data made easy",
        "url": "/cloud/2022/02/14/postgres-ai.html",
        "image": "/img/20211101-postgres-ai/logo-resized.png",
        "date": "14 Feb 2022",
        "category": "post, blog post, blog",
        "content": "  What’s the problem?  What is Database Lab?  End user interaction  Behind the curtains  How to setup  How to use it  More cool features  Why should I use it?  LinksWhat’s the problem?Testing data migrations is one of the cases where investing in good testing is crucial. Unfortunatly it’s hard to cover all the scenarios and it can be very time consuming to write test code for all of the scenarios you need to cover, especially in a one off migration.Moreover, if the data quality of your production system isn’t 100%, it’s very easy to miss a few scenarios in the analysis, which leads to missed migration scenarios that lead to bugs and incidents.In most cases, developers don’t have access to production systems directly, especially databases.This requirements has become more important over the years due to GDPR or similar regulations. It makes it almost impossible for developers to have a good insight into what data is available in a production database.In some organisations, regulars dumps are made to a dedicated environment for testing database migrations.Both technically and regulatory, this can be challenging as copying over large portions of data is an expensive and time intensive operation.This blogpost will provide an alternative way to handle this scenario in a safe and fast way using Postgres.ai’s Database Lab Engine.What is Database Lab?Database Lab is software that allows you to easily and quickly create thin clones of databases.At the time of writing, it supports PostgreSQL databases.It has been created by Postgres.ai which was founded by Nikolay Samokhvalov.They are specialised in creating solutions to assist with database management.The software exists in 4 main parts, their engine, their CLI, a GUI and the SaaS platform.This post will mostly discuss the Database Lab Engine (DLE) community edition, so without the SaaS platform.End user interactionLet’s start with what DLE can ultimately bring you.Let’s say a developer, Neal, wants to validate a data migration he has created. Performing an export and import from one database to another isn’t that hard, especially in PostgreSQL.You run pg_dumpall on the exporting database and just run a pqsl with the dump on the importing one and you’re done. Now imagine that the dataset he’s using is not just a few megabytes in size, but consists of 100s of 1000s of rows across a multitude of tables. That import is going to take a while, more than just a coffee break, let’s say 30 minutes. After the import completes, since he’s working on another data migration, he runs his migration script against the clone and as we, developers, aren’t perfect, he finds a bug halfway through his migration. He fixes the bug and wants to run the migration again. To do that, he needs to re-run the import (provided he kept the export) and restart his migration.Again 30 minutes plus his migration time are lost.This is a scenario where DLE shines.Let’s say Neal’s company has a DLE setup for the database he wants to use. The same scenario would look like this:Neal requests a DLE clone: dblab clone create neals-special-clone --username neal --password nealisawesome or through the GUI.This takes seconds to complete. Later in this post, we’ll discuss how that’s possible with so much data.Next, Neal runs his migration script, as before, it fails halfway through and he needs to fix the bug. After the fix, he wants to retest the migration with the original data.To do this, he simply runs dblab clone reset neals-special-clone and the clone will be reset to the original state before the failed migration was executed.Again, taking just seconds to complete.Behind the curtainsThe scenario described earlier almost sounds too good to be true, doesn’t it?In this section, we’ll discuss how DLE can achieve this and where the magic happens.The magic is mostly done by the use of a ZFS filesystem underneath the PostgreSQL instances used by DLE. Specifically the copy-on-write and snapshotting features are primarily used. This is combined with running the instances themselves in Docker containers, which allows multiple PostgreSQL instances to easily exist on a single system. More info on the details can be found hereDLE has 2 operating modes for enabling clones: physical or logical.With the physical mode, a replica of the source PostgreSQL database is created and managed by DLE. This database is treated as the sync instance from which all copies are created.The replication can be set up just like any other PostgreSQL replication.In logical mode, the copy of the original data is achieved by a regular dump (pg_dumpall) from the source PostgreSQL instance. This phase is called the logical dump in the DLE documentation.This dump is restored onto a local PostgreSQL install and from there clone could be created. This is called the logical restore phase in the documentation.After the dump has been completed a ZFS snapshot will be created for the database which is used later for creating clones. This last phase is called logical snapshot.After the initial copy has been created, in whichever mode, clones can be created.These clones are created by first creating a new (ZFS) snapshot of the restored instance. Next, a Docker container with a modified PostgreSQL instance is used in combination with a volume bind of the created snapshot.This results in a seemingly regular PostgreSQL instance running in Docker with some data preloaded in it. When executing changes on the database, the copy-on-write feature of ZFS makes sure that the new changes are only made to the snapshot volume that’s attached to the clone.When a reset is requested on a clone, the snapshot is simply restored to its original state by ZFS and the PostgreSQL instance is restarted.How to setupThe setup process is highly dependent on your starting situation. The first choice you’d need to make is if you want to use a physical or logical setup.The physical setup has the advantage of being an always-online solution. This means that DLE manages a live (async replication) copy of the source instance you want to clone.The downside of this approach is that you need to be able to change some configuration on your source instance to set up the replication and that you need to be able to create a replicated instance from your source instance. For managed PostgreSQL databases provided by cloud providers, for example, this might be an issue.Luckily there is a secondary approach available: the logical mode.In this mode, an initial clone is created by dumping the original database to the DLE instance and restoring it locally onto the main clone instance. The advantage here is that you don’t need additional configuration on the source instance. Unfortunately, due to the nature of the point-in-time copy of the dump, it’s not possible to have a real live clone available.For the example we’ll show here, we’ll use the logical mode as we’re copying from an Azure Database for PostgreSQL managed database. All code for the example is available hereNOTE: This code is only provided for demo purposes and is not to be used as-is for any production systems.All ports are opened by default on the instance and a public IP will be assigned to the VM.The scripts also include cleanup steps that remove ALL DATA from the instances. Use at your own risk.The demo code will create a database and a VM. Next, it will configure the VM to act as a Database Lab Engine instance.Some random test data will be automatically injected into the PostgreSQL database.Finally, you can use the UI or the CLI to interact with the instance.A brief use case will be shown in the next paragraph.How to use itDLE allows three ways of interaction with the engine: SaaS, CLI, local UI. For the scope of this demo, we’ll only show the local UI and mention the CLI counterparts.Once the instance is deployed, you can access the DLE UI through a web browser.You’ll be prompted to enter the token to access the instance.This is the token that’s set in the configuration of the DLE server.Using the demo setup, this token will be outputted by during the installation.Next, you’ll see the dashboard which provides an overview of all the active clones, the state of the DLE engine and a calendar that shows the available snapshots.For this demo, we’ll create a new clone of the database that’s linked to this instanceWe do this by clicking “Create Clone” and filling out the form that’s prompted next.After completing the form, click “Create Clone” and take a very fast sip of coffee as your clone will be available in a matter of seconds.This same process can be achieved by executing the following commands through the Database Lab Engine CLI:dblab init --token &lt;secret-token&gt; --url &lt;public ip of the instance&gt; --environment-id local --insecuredblab clone create --username jworks --password rocks --id testclone Now in a real use-case, you’d connect the thin clone to your application in a development environment or local setup. We’ll simulate the changes being made by connecting to the database using your favourite PostgreSQL client.Make some changes to the database (create a table, delete or change some rows in the provided users table, …).Now let’s imagine we’re testing a migration script and we discovered a bug (like the scenario mentioned earlier).We fix the bug, but now our data in the thin clone is corrupted and useless for further testing.&gt;Now a cool feature of DLE comes into play: clone resetting.Because the database is running on a ZFS snapshot, we can easily revert to the original snapshot and continue working from there again.&gt;We do this by going back to the local UI and selecting our thin clone.Next, we click on reset clone and confirm in the dialogue.Only seconds later, our database is reset to the original snapshot and we can start testing again.From the same view, you can destroy the clone as well if you don’t need it anymore.More cool featuresDLE has added some additional features on top of the cloning process. A very helpful feature is their support for data masking and obfuscation.In short, they support multiple scenarios for using production data in a development environment without exposing any Person Identifiable Information (PII) to the developers using the test systems. This makes it possible to use the clones during normal development without risking overexposing PII and therefore making it easier to adhere to guidelines like GDPR while still being able to test with production-like data.The mechanism to do this is quite simple.DLE supports writing preprocessing scripts that are executed on the main copy during the restore phase.Using this, data can be obfuscated or masked using predefined scripts, which you’ll have to provide yourself. By making it part of the restore phase of DLE, it’s ensured that no data is ever exposed without being obfuscated first.Another option that DLE support is using PostgreSQL Anonymizer. This is a well known PostgreSQL extension that can be used to mask and obfuscate data in a PostgreSQL database. The nice advantage of this solution is that the obfuscation configuration can be part of the original data creation in the original database. It allows DevOps teams to create the obfuscation code directly with the table creation code which of course makes it way easier to maintain properly and correct judge with data should be obfuscated and in which way.Why should I use it?If you’re in a project where access to production systems is not available and/or testing migration scenarios is very time consuming, this might be a good tool for you to investigate, provided of course that you’re using PostgreSQL databases.It provides an easy, scalable and safe way to provide copies of large databases without having to wait hours or days to copy over data from one database to the next to test something.Adding to that the possibility to obfuscate the data with the same tool and allow developers to work with obfuscated data during their developer, makes it an even more compelling choice.The software is opensourced on Gitlab and the community on Slack is very helpful and response if you have any questions or issues with the software.If you want a more in-depth post about how to configure DLE and which pitfalls we found, let me know on LinkedIn!Links  Postgres AI website  Source code  SaaS offering  Anomization setupFeel free to reach out to me or directly to lovely people of Postgres.ai if you want to look into this solution.Special thanks to the Unicorn team for helping with the blogpost and creating a great Database Lab Engine setup!"
      },
    
      "development-2022-01-14-what-would-discord-bot-do-html": {
        "title": "What Would Discord Bot Do?",
        "url": "/development/2022/01/14/what-would-discord-bot-do.html",
        "image": "/img/2022-01-14-what-would-discord-bot-do/discorddanger.jpg",
        "date": "14 Jan 2022",
        "category": "post, blog post, blog",
        "content": "Online communication has been widespread for years already. But the current world situation has made it near ubiquitous. With social contact in real life (IRL) being restricted, our need for communication shifts to the digital world through a wide variety of platforms. One of those platforms that focuses heavily on the online gaming community is Discord. It offers text communication channels, functionality to videoconference with your friends, and share your screen as a live stream, usually showing off your gaming skills. It also offers a way to code interactions with those channels by employing bots.Bots in the Discord world are services that take the input presented on a discord server, and perform some automated tasks based on this input. Popular bots on Discord allow you to moderate the channel, script shortcut commands, play videos or music, or add all sorts of files. There are various possibilities out there that an administrator of a server can add or remove with the click of a button. It makes for a more enjoyable and streamlined experience in your interactions.But what if there is no bot available to suit the needs of your community? For these occasions, Discord offers a development platform where you can code your very own bot, with the functionalities you need. There are many possible frameworks to code such a bot (and even a few solutions that offer this functionality without the need to code), and they come in many different coding languages (Java, Python, .NET, Rust, …). Since my expertise with coding lies mainly in the Java world, I opted to make my bot in the Java Discord API (JDA) framework.To get started we first need to register an application in the Discord Developer Portal. This is freely available to anyone who has registered a (free) account on Discord. It will look similar to the screenshot below, and presents a “New Application” button from where to start. After clicking the button, it prompts to enter a name for the new application, which I dubbed TestBotApplication all the while feeling like the next Shakespeare of the Digital Age.This takes you to a screen that will allow you to fill in general information about your application, such as an image to identify it with, a description on what the application is for, and some tags to align your new application with searches performed by Discord members on the lookout for the next hot bot. It also gives you some technical information that you will need while coding such as the Application ID and the Public Key. To round off this section, you can optionally enter some URLs where you describe the Terms of Use and Privacy Policy.Once this general information is set up, it is time to define a bot that is linked to this application. This is done by clicking on the Bot navigation item on the left side of the screen, which takes us to the Bot section, with a button labeled “Add Bot”. Here you can specify the name under which your bot will be visible on the servers that use it. Once again, in a fit of unbridled imagination, I went for “Developer Test Bot”. Nobel Prize for Literature, you will be mine!Another important piece of the developer puzzle is the token that can be taken from this page. This will be the secret used by our code to access this bot. Other configurations that can be determined in this page are the visibility of your bot, as well as activating the usage of OAuth to authenticate with. At the bottom of the screen we can already see the different permissions we would like to give our bot. But the final piece of the puzzle before trying our hands with the JDA framework is to set up the permissions and connections over at the Oauth2 screen.On this page, there are only a few things that need to be set up. First off, the client id and secret are shown, which we need for our invite URL. The bot also needs a redirect in case of OAUTH2 authentication, should we choose to  activate it. It is clear that this should be: https://www.evolute.be/discord as we do not intend to activate OAuth for the time being. We therefore set the Authorization Method to In-app authorization, which will prompt the administrator of the server where the bot is activated to confirm the rights of the bot. We identify our bot as a “Bot” in the scope section, and decide on the rights the bot will need to perform its duties. Giving it the Administrator permission will allow the bot to do almost anything in the server, and is thus too wide a permission set for a bot in production, but for test purposes, we will leave it as such for now.We head over the the URL Generator submenu item of the OAut2 menu, and fill in the scope “Bot” and permission set “Administrator” once more to get an INVITE URL, which will look something like this (with the client id filled in properly):https://discord.com/api/oauth2/authorize?client_id=&lt;clientid&gt;&amp;permissions=8&amp;scope=botDropping this link in a browser will take you to a web page where you can add the bot to your Discord server. If everything is done correctly, it should show up in the member list of the server. But since there is not yet any code behind it, it will be shown as offline.Time to start coding. We first set up a simple Maven project with the JDA dependency wired in. It has its own maven repository, so that one we need to add as well. I am going for the latest LTS release, although more recent versions are already available (though still in alpha).&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;be.evolute.discord&lt;/groupId&gt;    &lt;artifactId&gt;DeveloperTestBot&lt;/artifactId&gt;    &lt;packaging&gt;jar&lt;/packaging&gt;    &lt;version&gt;1.0.0-initial&lt;/version&gt;    &lt;properties&gt;        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;    &lt;/properties&gt;    &lt;build&gt;        &lt;resources&gt;            &lt;resource&gt;                &lt;directory&gt;resources&lt;/directory&gt;            &lt;/resource&gt;        &lt;/resources&gt;    &lt;/build&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;net.dv8tion&lt;/groupId&gt;            &lt;artifactId&gt;JDA&lt;/artifactId&gt;            &lt;version&gt;4.4.0_350&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;repositories&gt;        &lt;repository&gt;            &lt;id&gt;dv8tion&lt;/id&gt;            &lt;name&gt;m2-dv8tion&lt;/name&gt;            &lt;url&gt;https://m2.dv8tion.net/releases&lt;/url&gt;        &lt;/repository&gt;    &lt;/repositories&gt;&lt;/project&gt;Since Discord expects you to pilot the bot using WebSocket, I do not feel the need to create a service or a web application. I am just going to write a simple java class with a main method, so that I can run this from my IDE. This will connect to the bot by initializing the JDA context. With the JDBBuilder.createDefault method, you set up a WebSocket connection to your bot and to all resources the bot has been given access to (through permissions). This bot is identified by passing the Bot Token we can find in the Developer Portal on our bot page. If the JDA context cannot access the bot, it will throw a javax.security.auth.login.LoginException.public static JDA jda;public static void main(String[] args) throws LoginException {    System.out.println(\"Booting up Developer Test Bot...\");    jda = JDABuilder.createDefault(DISCORD_TEST_KEY).build();    Presence jdaPresence = jda.getPresence();    jdaPresence.setActivity(Activity.listening(\"/status...\"));    jdaPresence.setStatus(OnlineStatus.ONLINE);    jda.addEventListener(new CommandsListener());}For those of you that are not familiar with the WebSocket protocol, it is a way to set up a stable connection between client and server to allow for bidirectional full-duplex communication between them (as opposed to the standard unidirectional requests a client sends towards server to immediately get a response). It is identified by the ws://-scheme (as opposed to the http:// or https:// schemes) and only a single TCP/IP socket is used for this channel (HTTP/HTTPS) throughout its existence.The exchange starts with the client sending a standard request to the server with a number of headers indicating that a WebSocket is needed (protocol switch). The response coming from the server (HTTP 101) indicates that it agrees to this, and passed a verification for the originator to identify it is responding to the client’s request. Once this WebSocket has been established, the server (or client) will send payloads wrapped in frames (ws scheme) to its counterpart. This will continue until either the server or the client sends a closing frame, prompting a closing reply. Once the closing reply is received, the WebSocket is terminated.There are various things you can do to the visualization of the bot in the server member list. The most common are setting the activity status of the bot and its activity label. My main method will set up the label the bot will show by setting its activity. I will set it with the label “Listening to /status…”. Other possible activities are Playing, Watching, Streaming… These activities do not do anything else than setting the label shown on the bot. The status of the bot (similar to your status in Teams) can be set with the setStatus method.The next thing to do, is to create a Listener for my bot that will react to events happening on the server. We add this listener (CommandsListener) to the JDA context with the addEventListener method. The Listener class itself should extend from the JDA class ListenerAdapter class. This gives us a number of methods we can overload to react to specific events. I have chosen only to react to messages posted in the text channels of my server, thus the onGuildMessageReceived method needs to be overloaded. A Discord guild is the collection of all particularities associated with your server: members, channels, etc…public class CommandsListener extends ListenerAdapter {    @Override    public void onGuildMessageReceived(@Nonnull GuildMessageReceivedEvent event) {        Message message = event.getMessage();        String[] commands = message.getContentRaw().split(\"\\\\s+\");        if (commands.length &gt; 0 &amp;&amp; \"/status\".equals(commands[0])) {            TextChannel channel = event.getChannel();            channel.sendTyping().queue();            channel.sendMessage(\"Danger, Will Robinson!\").queue();        }    }}Since the commands I listen to might have parameters in the future, I decide to split up the incoming message with spaces as my delimiter. Then I have to verify that the incoming message is indeed a command, and not just a chat message from the guild members. For this reason, I check that the message is indeed the “/status” command the bot will need to react to. Once those checks have been made, I make the bot return the truth about any robot implementation: “Danger, Will Robinson!”. The messages in Discord can also be styled using a trimmed down version of Markdown. If you want to know what is possible, visit this link.Another interesting command is the sendTyping method. This creates the three blinking dots indicating a bot is typing something. This is turned off when a message is sent to the Discord server. As you might have noticed, there is a bit functional programming going on in these code snippets. In short, you can prep the message you are about send to the server with numerous options, and at the end, when you execute the queue method, it will be sent.Sending back a simple message is maybe not that useful, but you can insert any java code here that you wish for: performing calculations, calling services, … You might even want to pay homage to EverQuest, one of the first successful MMORPGs, and implement a “/pizza” command allowing your members to order pizza, so they don’t have to leave their chairs for anything.TextMessage replies are however very bland. A more fanciful option is to send an Embed message. This allows setting additional components such as a title, a colour board, images, a footer with thumbnail and lots of other stuff. This gives for more options should the need be there.public class CommandsListener extends ListenerAdapter {    @Override    public void onGuildMessageReceived(@Nonnull GuildMessageReceivedEvent event) {        Message message = event.getMessage();        String[] commands = message.getContentRaw().split(\"\\\\s+\");        if (commands.length &gt; 0 &amp;&amp; \"/status\".equals(commands[0])) {            TextChannel channel = event.getChannel();            channel.sendTyping().queue();            EmbedBuilder builder = new EmbedBuilder()                    .setTitle(\"**Lost In Space**\")                    .setDescription(\"The answer to your question: Danger, Will Robinson\")                    .setColor(Color.RED)                    .setThumbnail(\"https://cdn-icons-png.flaticon.com/512/3662/3662817.png\")                    .setFooter(\"Status Alert!\", https://st.depositphotos.com/1008244/1873/v/950/” + \t    “depositphotos_18734623-stock-illustration-skull-danger-signs.jpg\");            channel.sendMessageEmbeds(builder.build()).queue();        }    }}Triggering this listener will add the following message to the channel:One last nugget of wisdom I will impart in this article: If you want to mention the member that typed the command for the bot in your bot’s reply, this can be done by adding the following piece of markup (&lt;@!&gt;):String userId = message.getMember().getUser().getId();channel.sendMessage(“Replying to &lt;@!\" + userId + \"&gt;: Hello there!”).mentionUsers(userId).mentionRepliedUser(true)        .queue();Your bot is ready to go and be enjoyed by all the members that grace your digital playground. Now that it is clear what your bot should do, you should probably go back to the permissions in the Discord Developer Platform, remove the Administrator permission, and activate only those permissions it will need."
      },
    
      "development-2021-11-03-selenium-e2e-testing-html": {
        "title": "Writing End-to-End Tests in Java with Selenium",
        "url": "/development/2021/11/03/selenium-e2e-testing.html",
        "image": "/img/2021-10-04-selenium-e2e-testing/banner.jpg",
        "date": "03 Nov 2021",
        "category": "post, blog post, blog",
        "content": "Table of contents  End-To-End Testing  Selenium  Practical ExampleEnd-to-End TestingEnd-to-end (often referred to as E2E) testing is a testing methodology used to test a complete flow from the beginning to the end of one or more specific use cases within your application.Its main goal is to test your full application from an end user’s perspective and simulate everything that a real user would do.This includes typing stuff, clicking buttons and/or links and test how your application responds to this behaviour.End-to-end testing can avoid, if done correctly, many complex situations and ensures that your application is stable and keeps working how it is intended to work.If a button suddenly does not respond or does not behave in a way that you expect it to behave, end-to-end testing can detect this and warn you that your application is not functioning correctly.Thanks to this, you can drastically improve stability, which gives you more time to implement features and reduces development costs.End-to-end testing often starts with a user interacting with a user interface which then interacts with your backend that executes code behind-the-scenes in order to display the correct information on the user interface.This is done through an automated browser where the end-to-end test clicks and types for you.SeleniumSelenium is an end-to-end testing framework which is available in many popular programming languages including Java, Python, JavaScript, … .It provides a set of tools that allows you to connect with an automated browser that interacts with your frontend.Some of Selenium’s features are:  Cross-platform testing (PC, Android, iPhone, …);  Locating web elements on a page with advanced filtering such as XPath;  Selecting and interacting with found elements;  Capture screenshots at critical moments such as adding an entry in a table, submitting a form, …;  Keyboard &amp; mouse emulation;Selenium offers other products such as Selenium IDE and Selenium Grid, but at the core of Selenium, there is WebDriver.WebDriver is an API provided by Selenium which controls the automated browser and is used to fluently write your test code. Each browser (Firefox, Chrome, …) has its own implementation of WebDriver. WebDriver then interacts with your browser’s driver in order to execute the behaviour that you have specified in your test.Practical exampleThis post comes with a repository (branch selenium) with a basic example of how Selenium works in a Spring Boot application.This explains how you can use the WebDriver API to fetch and interact with certain elements and do various assertions or interactions to write your tests.It also includes a small example of WebDriverWait, which allows to pause the test and wait until a specific condition has been met on the page (i.e. loading an image, adding an entry to a table, …).Both E2E tests capture a screenshot during interaction with the frontend and those screenshots can be found in the target folder. You can check in the implementation (ScreenshotService) on how easy it is to add screenshot support.This can be combined with JUnit functionality (Rule for JUnit 4 and Extension for JUnit 5), for example when you can take a screenshot if your test ends or fails.For simplicity, I chose to use the Firefox browser. You will need the browser installed (or change the WebDriver implementation to your browser) in order for the test to work.You can run the tests yourself and watch how Selenium interacts with the automated browser. Make sure you changed the path of the geckodriver binary in BaseSeleniumE2ETest.test_E2E_AssertVisibleHtmlElements()The first E2E test is a basic test that covers all the HTML elements that are visible (or, should be visible) on the main page.In this test, various methods and approaches are used to find the desired elements and then to assert attributes like visiblity, text, colors, … .Notice how the WebDriverWait pauses the test and waits for the image to load before proceeding.By far the best approach to find elements on the page is to use XPath, which is a language that provides an easy way to find HTML elements on the page.//table[@class='table'] means that we want to find a table element on the page that contains the class table.    @Test    void test_E2E_AssertVisibleHtmlElements() {        // Find the content of the title by going through various HTML attributes on the page        // You can go step-wise through your HTML elements to find the right one by chaining findElement()        var title = driver                .findElement(By.id(\"header\"))                .findElement(By.className(\"navbar\"))                .findElement(By.tagName(\"a\")).getText();        assertThat(driver.getTitle()).isEqualTo(APP_TITLE);        assertThat(title).isEqualTo(APP_TITLE);        // Find the CSS value of a HTML element        var navColor = driver                .findElement(By.id(\"header\"))                .findElement(By.className(\"navbar\")).getCssValue(\"background-color\");        var navColorHex = Color.fromString(navColor).asHex();        assertThat(navColorHex).isEqualTo(COLOR_HEX_ORANGE);        // We can ask the WebDriverWait to wait until the logo is done loading so we can proceed with the test        // No chaining needed here, because 'logo' is only used by the img tag        waitDriver.until(ExpectedConditions.elementToBeClickable(By.className(\"logo\")));        // Find the image through the class name 'logo'        var image = driver.findElement(By.className(\"logo\"));        assertThat(image.getTagName()).isEqualTo(\"img\");        assertThat(image.isDisplayed()).isTrue();        // Easily find the table and the button on the page through XPath        var thead = driver.findElement(By.xpath(\"//table[@class='table']/thead\"));        var theadRows = thead.findElements(By.tagName(\"th\"));        assertThat(theadRows.size()).isEqualTo(2);        assertThat(theadRows.get(0).getText()).isEqualTo(\"Description\");        assertThat(theadRows.get(1).getText()).isEqualTo(\"Delete\");        var tbody = getTableBody();        assertThat(tbody.findElements(By.tagName(\"tr\")).size()).isZero();        var addButton = getAddTodoButton();        assertThat(addButton.getText()).isEqualTo(ADD_NEW_TODO_BUTTON);        captureScreenshot();    }test_E2E_addNewTodoToTable_And_DeleteTodoFromTable()The second (and last) E2E test actually interacts with the browser and asserts various elements based on these interactions.The test is responsible for clicking on the “Add” button, fill in an item in the description text box and confirming the item, adding it to the table of TODOs.Right after adding the item to the table, the test assures that the table contains an item and has the correct content, then deletes the item from the table.Again, we use XPath in order to find the correct element because of the simple approach.This time, the syntax is a little bit more advanced but definitely still easy to  use.//div[@class='modal-body']/form/button[@type='submit'] searches for a button of type submit in a form in a div with class modal-body.We can find the first element in the table by using //td[1]. Notice how it says [1] and not [0]. The starting index in XPath is always 1.The “Delete” button is found by searching for the text inside the button (//button[@type='submit' and text() = 'Delete']).    @Test    void test_E2E_addNewTodoToTable_And_DeleteTodoFromTable() throws InterruptedException {        var addButton = getAddTodoButton();        // interact with the button        addButton.click();        // Wait two seconds to show visible changes        Thread.sleep(2000);        // Find an element on the page by using its id in the HTML element        var formTitle = driver.findElement(By.id(\"modalTitle\"));        assertThat(formTitle.getText()).isEqualTo(MODAL_TITLE);        var descriptionBox = driver.findElement(By.id(\"todoDescription\"));        assertThat(descriptionBox.getTagName()).isEqualTo(\"input\");        // Type \"this is an E2E test\" in the input field        descriptionBox.sendKeys(\"This is an E2E test\");        // Wait two seconds to show visible changes        Thread.sleep(2000);        var confirmButton = driver.findElement(By.xpath(\"//div[@class='modal-body']/form/button[@type='submit']\"));        // Interact with the \"Add todo\" button        confirmButton.click();        captureScreenshot();        var tbody = getTableBody();        var rows = tbody.findElements(By.tagName(\"tr\"));        assertThat(rows.size()).isEqualTo(1);        // Use CSS selectors to find the columns of the TODO entry on the page        assertThat(tbody.findElement(By.cssSelector(\"tr:nth-child(1)\"))            .findElements(By.tagName(\"td\")).size()).isEqualTo(2);        assertThat(rows.get(0).findElement(By.xpath(\"//td[1]\")).getText()).isEqualTo(\"This is an E2E test\");        // Wait two seconds to show visible changes        Thread.sleep(2000);        // Find the delete button based on the input type and the text in the button        var deleteButton = tbody.findElement(By.xpath(\"//button[@type='submit' and text() = 'Delete']\"));        deleteButton.click();        // Verify that the item is removed from the table by checking if there are no &lt;tr&gt; elements        assertThat(getTableBody().findElements(By.tagName(\"tr\")).size()).isEqualTo(0);    }"
      },
    
      "leadership-2021-10-15-5-reasons-not-to-go-to-the-cloud-html": {
        "title": "5 Reasons not to go to the cloud",
        "url": "/leadership/2021/10/15/5-reasons-not-to-go-to-the-cloud.html",
        "image": "/img/2021-08-01-5-reasons-not-to-go-to-the-cloud/intro.jpeg",
        "date": "15 Oct 2021",
        "category": "post, blog post, blog",
        "content": "You were triggered by the title, weren’t you? You came here to look at the stupid arguments I wrote for defending legacy and on-premise solutions.Unfortunately, I have to disappoint you already.The title was indeed to trigger your interest.This article will however provide you with some battle-tested answers to common reasons “not to go to the cloud”.It will provide you with a strategy to counter common misconceptions for why the cloud “isn’t right for this project”.So if you’re working on a project and think you might benefit from moving to the cloud but are hitting walls of reasons not to do it, hang on tight because you might get some comebacks to counter the arguments.This blog will discuss the following 5 common scenarios which are used as reasons not to go to the cloud:  We can’t use the cloud because audit said we can’t.  We can’t use the cloud because it’s too expensive  We can’t use the cloud because it’s not secure  We can’t use the cloud because we’re not the size of Netflix  This works fine, why bother?We can’t use the cloud because audit said we can’tThis is probably one of the most common reasons in the wild. You want to move some workload into a public cloud provider, but someone claims you can’t “because audit said we can’t”.This is a common misconception in a lot of projects. IT audits aren’t designed to prevent solutions from being created nor prevent the usage of any existing solution.They impose a set of requirements that need to be fulfilled to comply with external regulations or internal rules.Simply stating that “public cloud isn’t allowed by audit” is, therefore, a gross overstatement in a lot of cases.How to tackle this scenarioAs with most issues in companies, talking to the right people is paramount to getting something approved or resolved. This case is no different. Before moving workloads or spinning up resources in the cloud, ask (internal) audit about their requirements. Ask questions like the following:  What reporting information do you need to provide?  Who is allowed to perform which actions, and even more important, which roles separation needs to exist?  Which scenarios need to be covered by controls?  What measures need to be in place to prevent which scenarios?  Which public cloud services meet the requirements out-of-the-box and which need additional measures?Make sure to include them in your journey towards the cloud and ask them about what they require additionally while you are moving workloads.Having them on board from the get-go makes it a lot easier to have their buy-in into the solution instead of disapproving certain things afterwards.It also helps to get people with extensive knowledge about your cloud provider to talk to the auditors.They have an overview of what features are already available in the services and even which features are on the roadmap to be implemented.As an example, infrastructure changes need to be logged, audited, and stored in a non-changeable way. On Microsoft Azure, this is already available through the Activity Logs which are provided out of the box.We can’t use the cloud because it’s too expensiveThis one actually originates from a lot of horror stories about companies moving to the cloud and forgetting about the cloud bill at the end of the month. Or some hacker got access to the account and started spinning up expensive machines to mine bitcoins.There are multiple examples small and not so small companies having experienced this: Tesla, Announce and Adobe.To get started, the cloud isn’t for free, even though a lot of providers will give you some free resources every month.It’s still a service that you’re using and there will be a bill for it at the end of the month. But it’s not a reason to stay away from the cloud, quite the opposite.How to tackle this scenarioThe approach here differs somewhat compared to your starting scenario.The easiest way to counter this argument is of course to show that the public cloud is less expensive than the current platform. This can however be quite challenging because you need to have a platform already and the cost of that platform needs to be clear.All major public cloud providers have an online calculator to estimate your costs based on your usage patterns. So if you’re in the position to input this information, this should provide you with a good estimate of the actual cost.Being able to calculate the cost of the current platform is probably the harder part.If you have internal cost centers, using those might be a good start if they reflect the real cost.In some companies, the cost of the platform might actually be paid for by some central IT budget and isn’t really available to the different projects. If this is the case, try to ask the central department to provide you with some rough numbers or make a best-effort estimation yourself based on the budget, headcount and your own footprint in the platform. Make sure to include the headcount in the calculation of the platform as this is often forgotten when making these comparisons. And finally, if you’re in this situation and you are looking at a cloud provider, don’t forget to take the opportunity to look at re-platforming or re-architecting your solution. It might seem obvious, but costs could be heavily reduced by looking into managed solutions for existing components and since you’re changing anyway, you might as well invest in optimizing the solution.A second scenario is when there isn’t an application yet or it’s not live yet. This makes it harder to estimate how many resources you’ll need of course.On the other hand, it will allow you to create solutions that use more integrated solutions from the public cloud provider.These solutions are often more tightly integrated, but can also save you a lot of money because they are easier to operate and faster to build compared to traditional solutions. As most companies only have bare metal machines, a VM or container platform on-premise, the higher-level solutions can provide better operational abstractions and a lower cost of operating them.Especially when you have a fully managed runtime like Google Cloud Run, AWS Lambda or Azure Functions.With these solutions, you only provide the running code and the cloud provider actually operates the application.In both scenarios, the key to success in comparing public cloud against other options is to look at the so-called Total Cost of Ownership (TCO). This means you don’t only include the actual bill at the end of the month but also the cost of managing, operating and developing solutions on the platform you choose, including the headcount of the solutions.Another important practice to safeguard your cloud cost is to actively monitor them. Every major cloud provider allows you to set specific alerts for consumption. They even provide you with recommendations about sizing of resources and getting discounts based on sustained usage for example.Setting these thresholds based on expected or budgeted amounts and acting when they are breached, should provide you with a good method to managing your cloud spends and prevent unexpected costly bills at the end of the month.We can’t use the cloud because it’s not secureThis is one of my favourite ones because it can be both very valid and a totally bogus reason not to use the cloud.To get the easy stuff out first, don’t use the public cloud if you have the following requirements:  You need to be air-gappedAnd that’s about all the valid reasons, security-wise.It’s a common misconception that the public cloud isn’t as secure as your own data center.It’s commonly told that it’s more secure to run software applications behind the firewall within an organisation because the organisation owns all the moving parts.Looking at the multitude of security certifications that all major cloud providers have, it’s a pretty bold statement to claim that an organisation would be more secure compared to public cloud (Azure compliance, AWS, Google).I’d personally argue that most cloud providers are an order of magnitude more secure than most on-premise datacenters. For every security role in your organisation, they probably have dozens. Just because it doesn’t run in your data center, doesn’t mean that it’s less secure.As with the audit requirements, talking to your security people upfront is key here. Make sure that you understand their concerns and see how you can provide the right level of trust in the public cloud solution. Ask them which security requirements you need to fulfil and check if and how the cloud provider fulfils them. If you’re already live with an on-premise solution, make sure to make the comparison with the current solution. You’ll often find that the cloud provides more security features and that especially auditability of these features is a lot easier compared to the on-premise solutions.Lastly, create a set of known good configurations for the services you want to use. For example, if your organisation requires data-at-rest to be encrypted with your own key, create automation scripts or runbooks on how to configure that service in a way that is compliant with the security requirements. Creating governance policies (E.g. on Azure) which are validated on the entire platform might help with proving that you’re in fact working securely. Due to the level of automation that’s possible on the cloud and the integration options between the different solutions, there are quite a lot of scenarios where the public cloud is actually more secure than an on-premise solution.We can’t use the cloud because we’re not the size of NetflixThis reason actually covers more than the title might lead you to believe. Some companies argue that the public cloud is not for them as they only need some small applications. One could actually argue that the opposite is true.For many of the large “internet age” companies, it makes more sense to invest in their own infrastructure instead of paying public cloud providers literal millions each month to host their applications.Important to note is that cloud providers offer way more services than just plain compute capabilities also known as Virtual Machines (VMs). They offer a magnitude of managed services, most of them targetted at reducing the operational overhead and therefore cost, for their customers. Even if your requirement is to have plain vanilla VMs with some software you’re licensing, chances are high that a managed or packaged version is available on the cloud providers marketplace.So public cloud actually makes a lot of sense for a very wide range of customers. If you’re a small SME that basically needs some compute resources to run some licensed software, they have got you covered.If you’re a startup looking to leverage ready to use building blocks and focus as much as possible on your added value and not on operational complexity, they’ve got you covered.If you’re a large corporation that wants to reduce their TCO of owning, managing and operating their on-premise solutions, they’ve got you covered.I’d actually argue that the size of the organisation doesn’t really dictate if you can move to the cloud, it’s the culture and people.Using the public cloud requires a slightly different and less siloed mindset in the organisation. Most of the time, you’ll get end-to-end responsibility and access to the components that you need in the cloud. This provides you with the flexibility to create the solution that fits you best, but it also means that there is no other team that will maintain or operate the resources you’re using.Almost always, this freedom heavily outweighs the additional “management” of the resources and if it’s too much management, you probably need to look into one of the managed solutions offered by your cloud provider as a replacement for the resources you’re using.This works fine, why bother?This question should actually be the first one that should be asked by every manager when a potential move to anywhere (not just the public cloud) comes up.“What’s in it for me?”This question can be answered with all of the above arguments we already discussed, but we’ll highlight a couple of opportunities additionally here.Reducing time to marketBeing able to react to quickly changing markets and circumstances from a business standpoint is already important today, but it will only become more important in the future. Building software solutions that take months or even years before they see the first real usage, isn’t the best strategy in 2021 and it will be a terrible strategy soon. Companies that can respond quickly to changes and can validate new business ideas quickly will win the markets. The public cloud can help here as well. Since it offers a lot of managed solutions that can grow over time, it facilitates this testing and validating strategy. A simple application can be put live in a matter of days or even hours to validate a business idea. When the time comes to scale out the solution, it becomes as easy as allowing your application to scale to the required size and picking up the bill at the end of the month.Attracting talentThe public cloud is a hot topic within the software engineering community and there are a lot of people interested in it.A nice advantage of using the public cloud is that you’ll have an easier time attracting highly skilled talent to work on the project.On-premise solutions are often considered legacy and aren’t as attractive to new hires.The cloud also provides a lot of certification and specialisation tracks, allowing you to effectively find people that have a proper record for using the solutions.ConclusionThe key takeaway of this post should be to talk to people.Ask them why they are opposed to moving to the public cloud. Ask them what is needed to be allowed to move, which requirements you need to fulfil and why these requirements are relevant.Don’t accept a plain NO without valid reasoning. Try to look for similar projects within your organisation and find out if there are examples of projects that have moved to the public cloud already.Failing that, look at others like the Deutsche Boerse or Alphabet International Gmbh that have adopted public cloud as well.Feel free to reach out to me or any other Ordina colleagues if you want help with starting to adopt the cloud."
      },
    
      "leadership-2021-07-06-influential-tech-lead-html": {
        "title": "Tips for being an influential technical team lead",
        "url": "/leadership/2021/07/06/Influential-tech-lead.html",
        "image": "/img/2021-07-06-Influential-tech-lead/influence.jpeg",
        "date": "06 Jul 2021",
        "category": "post, blog post, blog",
        "content": "As a software engineer, I really enjoy creating applications and solving complex technical problems.But some years ago, there it suddenly was: I was no longer only a “do-er” but an “oversee-er” as well.At that point I had received very little formal managerial training, so learning on the job or from mentors was crucial to be successful.The soft skills I needed for my new role turned out to be more difficult than I initially thought.One of those difficult soft skills for me was (and often still is) the ability to influence people.Influencing others is not just about getting them to agree with your point of view or manipulating them to get your way, nor does it involve forcing others to change by using power and control.It’s about noticing what motivates others and using that knowledge to leverage performance and positive results.Taking up the technical team lead role is not always an easy one.Often the skills necessary to be a great developer don’t translate easily to the role of a technical team lead.In this post, I will give some tips on how to be an influential technical team lead.Have a clear visionA vision is a clear image of how you see the future.It’s something that keeps you motivated and excited to do what you do.A technical team lead must be able to both zoom in and zoom out on a project.On one hand, you need to be able to look in detail into a technical requirement.On the other hand, you also need to keep the bigger picture in mind and how the requirement fits into the greater vision.If you’re a technical team lead that has no vision and isn’t going anywhere, then how can you expect your team will follow you?In order to create a vision you can ask yourself questions like ”What do you want to achieve?, “Which (tangible) output do you want when achieving your goal?”, “How would that output change the way things are done now?”, …Setting specific goals together with your team that move towards your vision will help you to achieve it.Once you have that vision and goals, being predictable is key.By doing so, everybody knows what you’re trying to achieve.The Don’t-Repeat-Yourself principle, which is a key principle for each software engineer, does not apply when working with people.Communicate your vision and goals over and over again: repetition and predictability are key.Be collaborativeA top-down approach where nobody has a say in the process and where both vision and goals are pushed downwards, is very easy for team members to reject.A different and more sustainable approach is to be a collaborative technical team lead.This means that you regularly seek out a diversity of opinions and ideas amongst teammates to solve problems.Involve your team in creating a shared vision, and identify the behaviours necessary to accomplish it.It’s all about getting the team to think through these difficulties themselves instead of telling them what to do all the time.As a result they are more engaged, feel trusted, and are more likely to take ownership of their work.Building meaningful relationships is one of the key responsibilities of a technical team lead.Practice active listeningWe often think about influence as being what we say or how we say it, but improving your listening skills is key to gaining your team’s trust and creating psychological safety.As technical team leads we’re often pulled in many directions throughout the day. The most common pitfall is that we listen to reply, not to understand.It’s easy to be a passive listener by multitasking and only listening to the highlights. While it may seem like you don’t have time, making the time to really listen to your team can increase your leadership capacity significantly.Active listening is the ability to focus completely on your team, understand their message, comprehend the information and respond thoughtfully in a relevant way.This also includes not only capturing the message, but paying attention to subtle hints and non-verbal communication such as tone, emphasis, facial expressions and body language.Keep in mind that a true servant leader only speaks about 20 to 30% of the time, the rest should be spent on listening.When your team members know that they will be heard, they are more likely to openly share their ideas and provide honest feedback.Knowing how and when to gather knowledge of your team members is important, since you can then synthesize it into a better solution before deciding the course forward.Influence othersAt a basic level, influence is about compliance.It is about getting someone to do what you want them to do, it allows you to get things done and achieve desired outcomes.However, this can never be accomplished by power and control, but only by genuine commitment of other people.This simplified diagram is based on “The theory of planned behaviour” by Icek Ajzen, a social psychologist and professor emeritus at the University of Massachusetts Amherst.His theory assumes that before every Behaviour, there is an Intention.A simple practical example: you notice a specific behaviour from a software engineer on your team, namely that he/she refuses to write unit tests.As a technical team lead, you are interested in why he/she formed the intention not to write these unit tests because you’re convinced that unit tests have a variety of benefits such as improving of code quality, finding bugs early and facilitating change.There are three elements that can help us trying to understand why your colleague formed this intention:  Attitude a.k.a “Is the behaviour good and is it right?”:  We’re deciding if the behaviour is in our own best interest or if it is the right thing to do. So if we have a positive attitude towards the behaviour, it is more likely that we will perform the behaviour.   Subjective norm a.k.a “What’s everyone else doing?”: We’re asking ourselves what others think of the behaviour and how they judge it. If we think that others consider the behaviour to be normal or good, it is more likely that we will perform the behaviour.   Perceived behavioural control a.k.a “Can I do it?”: When we believe that behaviour is easy to perform, we are more likely to perform the behaviour.So when you’re asking the software engineer on your team to write unit tests, the three elements listed above will cross his mind and he/she will ask himself questions like: “Is it a good thing to write unit tests?”, “Is it in my own interest to write unit tests?”, “Does everyone write unit tests or is it just me?”, “Do I know how to write unit tests and do I have enough information/time to write them?”, …Targeting each and every one of these elements is the key to success when you want your colleague to start writing unit tests.So what can you do to influence the other person’s thinking?  “Is the behaviour good and is it right?”: Identify what the other person cares about, what he/she thinks is good and what he/she thinks is the right thing to do. For example, your colleague might think it’s super important to finish his user story as quickly as possible. The key thing to do here as a technical team lead is not to be manipulative or dishonest, but to genuinely align your goals with his motivations and things he/she already cares about. Writing unit tests facilitates change, provides documentation, and ultimately saves time.  “What’s everyone else doing?”: You can present that writing unit tests is an industry standard and not some kind of crazy idea you’ve just come up with.Presenting it as a good practice and being able to demonstrate why other successful teams or companies do it, is a good way to help your colleague see that what you’re asking him to do is normal in the industry.  “Can I do it?”: If your colleague thinks it is too difficult, he/she will never start writing unit tests. Therefore, present it as a clear and small change, and make it easy for him to do. Offer your support and show examples of good and successful unit tests so he/she gains confidence and knows he/she can do it.  “The key to successful leadership is influence, not authority.” – Ken Blanchard"
      },
    
      "internship-2021-06-15-tc-guard-html": {
        "title": "Ordina Internship TC Guard",
        "url": "/internship/2021/06/15/TC-Guard.html",
        "image": "/img/2021-05-27-TC-Guard/schema.png",
        "date": "15 Jun 2021",
        "category": "post, blog post, blog",
        "content": "IntroductionWe started our internship at Ordina in February 2021 which lasted for 15 weeks.The internship was a part of our thesis, which was necessary to graduate.During this internship, we developed an Android application to assist drivers while driving trough a trajectory control.Alongside the application, we also developed an API using Spring Boot. These two components made it possible to learn and use Kubernetes. Certain technologies were installed on the Kubernetes cluster to enable metrics and centralized logging.During the internship, we were introduced to a wide range of technologies that we’ve never used during our bachelor’s degree.Azure DevOpsAzure DevOps has a plethora of technologies to provide assistance throughout the development cycle.We used a handful of these technologies for this project, including Azure Repos, Azure Pipelines, Azure Artifacts and Azure Boards.It also allowed us to work with scrum using the built-in boards, which ensured that the mentors could track our progress.Overall, Azure DevOps is a very useful and feature-rich platform to use.APIOur first experience with Java was in highschool, where we learned most of the programming basics. In college, we only used Java during our Android Development course.Because of this, we weren’t that well versed when it came down to Java specific features.While working on the API, we learned quite a lot about Java, but also some programming principles.SpringThe Spring framework is a Java platform that provides comprehensive infrastructure support for developing Java applications. We started out by creating a small RESTful API using one of the tutorials on the official Spring website. Alongside the tutorial we also used the extensive online documentation.In college, we had a .NET course where we used ASP.NET to build an API.We quickly discovered while working with Spring Boot that our preference goes to Spring instead of ASP.NET.One of the reasons is that Spring Boot does a lot for the developer while in ASP.NET you have to do more things manually.PostgreSQLThe API’s database is a PostgreSQL database, which is used to store the collected data, the recorded trajectory controls and the drivers.Flyway is used to keep our database structure up-to-date using migrations.API FirstTo develop the API, we used the API-first design principle. This approach includes developing APIs that are consistent and reusable.This can be achieved by creating an API contract that describes the capabilities and behavior of the API.This ensures that more time is spent thinking about the design of the API.It often included additional planning and collaboration with our mentors.We drew up this contract ourselves. Later we found that there is a Swagger Editor that uses the Open API type. Since we already had a contract we decided not to use it.They provided feedback on the design of the API before any code was written.The first code that was written were tests.This method is called Test Driven Development (TDD).Specific attention was paid to the API contract, as the tests are based on them.TDD also allows for bugs to be found at an early stage because after writing code, the developers can run the tests and thus check whether the code works or not.Android ApplicationThe user can add trajectory controls themselves via the record button.Once the user has passed the starting point of their trajectory control, they must press this button to start the process.During this process the starting point is saved and a checkpoint is added every 100 meters.When the user passes the end point of their trajectory control, they need to press this button again to define the end point and the maximum speed.The application will ask the user for the maximum speed.This will be automatically filled in by a speculation performed by the application.Once the process is finished, the trajectory control will be stored in the database via the API.Android NativeAndroid Native was already known because of the Android Development subject.As a result, we did not encounter too many problems in terms of development.The biggest problem we have experienced is that Android Native is not that user friendly in terms of design.If we were to start over, we would prefer a Hybrid approach like Ionic or NativeScript instead of Android Native.Finished Product     KubernetesThe figure above shows the block diagram of this project.This diagram represents all the components of this internship.Each component will be briefly discussed below.All applications required for this internship run within a single Kubernetes cluster.Inside this cluster, Argo CD is used to keep everything up-to-date.The Android application is connected to the TC Guard API using Contour.This is also used to expose the Graylog dashboard to the public.TC Guard and Graylog are connected so that the logs from TC Guard can be sent to Graylog.The data collected by the Android application is stored in the PostgreSQL database via the TC Guard API.When the application needs this data, it is retrieved again via the TC Guard API.The last block connected to the TC Guard API is Prometheus.Prometheus is used to observe the API.The resulting data is then displayed in the Grafana dashboard.Argo CDArgo CD is a Continuous Delivery GitOps tool for Kubernetes.Within the internship assignment, this is used to install Helm charts on the Kubernetes cluster.If a change is made on the Azure DevOps repository, Argo CD will detect this and implement the changes on the Kubernetes cluster via a rolling update.It was quite satisfying to observe the changes being applied.Argo CD made managing our applications much easier.For example when we updated Graylog, the only thing we had to do was change the version number in a .yaml file and Argo CD took care of everything else.IngressIngress provides routing rules to manage remote users’ access to services in a Kubernetes cluster. With Ingress, simple rules can be set for routing traffic without exposing every service on the endpoint.While setting up Ingress, a few issues surfaced due to our inexperience with Kubernetes.Because the online documentation of Ingress nginx was rather limited, setting up Ingress took a fair amount of time.Afterwards, Ingress nginx was replaced by Contour.By this time we had more experience with Kubernetes.Because of this, the switch to Contour went smoothly.GraylogThe centralized logging system aims to obtain the logs of the API and the Android application in a central place so that it would be easier to detect errors.To obtain and view the logs in a central place, Graylog is used.We chose Graylog because it is much easier to use than Kibana.We knew that Kibana has more features than Graylog, but for this project it was not required.GrafanaPrometheus was used to observe the API.Prometheus collects metrics from targets by scraping given HTTP endpoints.Actuator is a Spring Boot plugin that is used to manage and monitor your application by using HTTP endpoints. This makes it possible for Prometheus to gather metrics and monitor the health of the application. Grafana then uses Prometheus as a source to display the metrics on a custom dashboard.We were unsure how to add a target to Prometheus while it was running on Kubernetes. Because we were not sure how to modify the config file, a lot of time was lost.To solve this problem we had to create a secret that contains the target.This secret should then be added to the prometheus.yaml file.When rebooting the Prometheus pod, the target was added.ConclusionDuring this internship there were a lot of new technologies covered such as Spring, Kubernetes, Graylog and so on.The internship assignment consisted of three main parts including the Android application, the Spring Boot API and Kubernetes.As described earlier, the biggest problem that we experienced was that Android Native is not that user friendly in terms of design.If we were to start over, we would go for a Hybrid approach like Ionic or NativeScript instead of Android Native.Once we had the basics of the application, we started working on the Spring Boot API.Spring was completely new to us, but because of the complete and structured online documentation, the learning process went smoothly with some guidance from our mentors.The mentors had a few remarks.The remarks were justified, they were about our structure and programming style.In order to improve the structure, Domain-Driven Design was recommended by our mentors.As a last important part, we deployed the API on Kubernetes.Besides the API, a few other things were implemented on the Kubernetes cluster such as Graylog and Grafana.Kubernetes was completely new to us as it was not covered in the graduate program Application Development.We found that the learning curve is quite steep in the beginning because it were all new concepts.Because of the proper guidance from our mentors, we got through this initial period.At the end of the internship, working with Kubernetes didn’t scare us anymore and dealing with it was less difficult.We would like to thank Frederick Bousson for offering us this wonderful opportunity.Furthermore, we would like to thank Jeff Mesens and Nils Devos.Thanks to their contribution we were able to successfully complete this project.Without their help, we could not have made such a great evolution."
      },
    
      "cloud-2021-05-27-chatbot-immo-html": {
        "title": "Immo chatbot",
        "url": "/cloud/2021/05/27/Chatbot-Immo.html",
        "image": "/img/2021-05-27-Chatbot-Immo/cover_blog.jpg",
        "date": "27 May 2021",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Architecture  Chatlayer          Chatbots in general      Intents, expressions, and entities      Building blocks      The multilingualism of the bots      Flows                  Search a premises                          API implementation                                Report a problem / ask a question                          Database              Repairs              Renovations / design                                          Google calendar integration                  Getting access                          Enable API              Get authorization code              Exchange authorization code for refresh and access tokens              Getting an access token with the refresh token                                Getting the data from the calendar                          Creating new client              Get freebusy schedule              Get free meeting times              Send the meeting times back to Chatlayer                                Create an event                      Solid          Introduction to Solid                  Why do we use Solid?          What is Solid?                    Self hosting Solid pod server                  Register          Login          Edit profile                    Your data                  Sharing of data          Turtle (RDF)          How to access your data?                      ConclusionIntroductionAs we all know, Ordina is a company that is always looking to be ahead of change, by looking for the newest technologies to work with and seeing where these technologies can be useful. This is also one of the main objectives of my internship, investigating a new technology, investigating how fast and easy it is for non-IT users with almost no knowledge of chatbots, to set one up.In this case, I made a chatbot for a real estate agency. At the moment, the real estate agency plays an intermediary role between tenant and landlord. All the questions of the tenants are coming straight to the customer service. Chatbots are a handy tool for customer services because they are able to take over a lot of frequently asked questions, and are available 24/7. Therefore, the customer service can run more efficiently and they will have extra time for their other tasks.My official mentor was Frederick Bousson, he had help from Jasper Rosiers and Hans Vanbellingen who were my daily mentors and the people who I could reach out to if I had any questions.Architecture  This is the setup of the project. This architecture has been changed a few times during the internship. This mostly has to do with the Solid database because this is such a new technology. At the start, we didn’t quite know how to start and how it all worked.So as the internship progressed, the architecture grew bigger. All the different parts of the architecture will be explained in depth in the next chapters.For the APIs, connecting everything together, we built a Spring Boot application.ChatlayerFirst things first, the chatbot. This is where the project begins. But before we dive into the platform I’m using to build the bot, we must have an understanding of how chatbots actually work.Chatbots in generalAlmost everyone has come across a site with a chatbot, maybe without even realizing it. Think about those little popups you see on a website offering you some help with whatever you are doing. That’s a chatbot. There are three main types of chatbots:      Rule-based  This one is the simplest type of chatbot today. People interact with this chatbot with buttons and pre-defined options.        Intellectually independent chatbots This kind of chatbot uses machine learning which helps the bot learn from the user’s inputs and requests.        AI-powered chatbots AI-powered bots combine the best of the previous two types.  So they have  predefined flows but they can also understand free written language.  For the understanding of the language, the chatbot uses NLP (Natural Language Processing).  This involves two processes, NLU (Natural Language Understanding) and NLG (Natural Language Generation). NLU is the ability that a chatbot has to understand the user.  It maps the text that a user enters to a format that a computer can understand.  Then we have NLG, the other important part of NLP.  NLG is the process that, as you can probably get from the name, transforms data into words.  So it’s the generation of text.  When a chatbot is asked a question, the bot will break it down into an intent and entities. Now, what exactly are these two terms?Intents, expressions, and entitiesIntent:Well, an intent is something that a user would like to accomplish with the question. Another way to put it could be the user’s intention or need. Let me give an example.When a user asks a question like: ‘We need tickets to go to Brussels’. The intent is ‘buy a train ticket’.Expression:Another important term related to intents is ‘expression’. In order for the AI to correctly identify an intent, it has to learn a couple of ways to express a certain intent. This is where the expressions come into play. When we add an intent to the chatbot, we must provide the bot with a couple of ways a user could say/express the intent. It’s obvious that we need to provide enough examples that convey the same message for each intent. The more expressions that are added, the more accurate the chatbot will be. When you add expressions to an intent you need to make sure that there is enough variation between the different expressions.Entities:At last, we have the entities. These are small words or even word groups that are very relevant for the flow of the conversation. Entities could be names, cities, companies, etc. If we go back to the example I gave at Intent, Brussels is an entity. It holds the name of the city where the user would like to go to. Be aware that you should only use entities when the value is needed in the continuation of the flow.Building blocksTo build a chatbot, Chatlayer provides us with several different building blocks. Each of these has a different function.On every bot dialog, you can put an intent, a required context, and an output context.An intent is explained before so you should know what that is 😉. If the user sends a message to the bot, the AI will check if it recognizes an intent. If this is the case, the chatbot will display the message where the intent is set.The required context and output context belong together. Think for example about the intent ‘Yes’ or ‘No’. If the bot asks a question where the user can answer yes or no, we can use these intents. But if there are multiple dialogs with the ‘Yes’ and ‘No’ intent, how could the bot know which dialog to go to?If you take a look at this example in the first dialog, the bot asks the user if he/she wants to give some feedback about the bot. The user can answer with yes or no. If the answer is yes, the bot will go to the input validation (green), otherwise, it will go to the bot message (grey). To know to which “general.yes_or_agreed” intent the bot should go to, it can look at the output context and required context. In the dialog where the question is asked, you can give an output context. The bot will now look for a dialog with the right intent and the right context. That way you are sure that you will be going to the correct dialog.A full tutorial on building bots within Chatlayer can be found here.The multilingualism of the botsTo set up a multilingual bot, you first need to make the bot in the primary language. I first made my chatbot in Dutch. Afterwards, you can easily translate all the messages and intents in an overview, retrain the NLP for that language and it is done! My chatbot is set up in Dutch and English.Not every chatbot provider offers this functionality, which is one of the reasons we chose Chatlayer in this setup.FlowsSearch a premisesOur first flow relates to searching for premises. I of course didn’t have access to the database of the real estate agency so I had to find a different solution. So in this flow, the user gets asked a lot of questions about the premises, like maximum price, location, number of bedrooms…. With this information, I could construct a URL where the user could click on. This leads the user to the real estate agency website with the search results. In a more extended practical case, the bot could list the relevant premises right into the chatbot.At the end of each flow, the bot asks the user if he/she needs help with something else. If the user answers with yes, a new flow can begin. Below you can see the diagram of this first flow. As you can see, a lot of the parameters are optional. If the user wants to see all the houses without filling out the other questions, he/she can do so. The decision in the beginning “Does premises have bedrooms” is a decision the chatbot will make based on the type of premises the user chooses. I made this a go-to. If the type of the premises equals “houses” or “apartments”, the bot will go to the dialog where the user could enter his/her preference about the rooms. Otherwise, the bot will go to the next bot dialog.API implementationAt the end of the flow, the chatbot will send an API call with the necessary parameters to construct a URL. There is a basic url to which we can attach a piece depending on which parameter has been entered.In the service the first thing I check is if the ‘status’ parameter has a value. The parameter can contain either ‘buy’ or ‘rent’. If this is not the case, there will be an error message which is sent back to the chatbot. The option if the user wants to buy, or to rent, is the only mandatory parameter. It could be possible that all the other parameters are empty. The rest of the function is all about checking if these parameters are null or not and attaching the correct pieces of the url.It would be nice if we could just send a string back to the chatbot which it will display in a bot message. Sadly this is not the case. Chatlayer expects the API response to be in a specific format. The structure needs to be like the image below but they could be null.As you can see, we need to provide Chatlayer with three things:      SessionHere we can pass along some variables that will be stored in de session storage.        MessagesHere we can specify which messages we would like to send to the chatbot, which will then display those messages to the user. It’s not limited to a normal text message, you could also construct some buttons, a carousel…        ActionThis is used to specify the next dialog the bot should go to.  In this flow, I used this twice. One time to send the URL back to the user and once to display an error message if something has gone wrong. Because this was the first simple flow that we had set up in the chatbot and we knew that this was not the main flow I had chosen to, for now, just send the URL back to the chatbot as a text message.Report a problem / ask a questionThe second flow has a connection to the React application and is able to send problems/questions to the API which saves these in the database. More about the React application, API, and database come later in this blog.To start at the beginning, the first thing a user has to do is log in to his/her Solid environment and specify their role (tenant/landlord). When the user is logged in, the chatbot gets the information about the user and the next bot dialog is being shown based on the role. Currently, only the tenant flow is worked out. The user gets a message from the bot that they can ask their question. It gives a few examples where the user could choose from.DatabaseIn the architecture you can see that there is a MySQL database. You might wonder “If we already use a Solid database, then why would we use another?”. A Solid database is not really designed for searching through large amounts of data. So the data from the Solid is copied to this database in the cloud where we can easily search through the data. Below you will find a simple ERD of the database. I made this together with Jasper. This is the only flow where the database is currently used.RepairsOne of the paths that are worked out is if the user has a reparation that needs to be done or they have a question about who has to pay for the reparation. The user can reach this flow by clicking “Repairs” at the start, or just asking their question. They would have to answer a couple of questions about the reparation, think about the location and what is broken or needs to be repaired. Then, the bot will answer the most important question: “who has to pay for this?”. At the end of the conversation, the chatbot will send a request to the API with the problem and the user information to be saved in the database.Renovations / designThe other path that is worked out is for the design and renovation questions. When the user clicks on “questions about the house”, he/she will get the first choice, is it a question about the design, or renovations. Design questions are for example questions about the repainting off the walls. This is a question that the bot could answer.In the part about the renovations the bot says that all the big renovations must be discussed with the landlord. The bot then proposes to send an email to the landlord with the question. The user only has to type the body of the email. He/she gets to re-read the email to check for mistakes and then the bot will send the email.Google calendar integrationGetting accessEnable APITo get access to the API there are some different steps we need to follow.The first step is enabling the Google Calendar API in the API console. To do this, we need to create a project on the Google Cloud Platform. Then we can navigate to the API library and enable the calendar API for our project.When this is done, the next step is to create a client id. This can also be done in the Google Cloud Platform. The redirect URI is important here because this is where the access token is sent to. As you can see in the image below, when the client id is created, you get the client id and the client secret. These are important for authentication later on in this flow.  With the flow I’m following you won’t only get an access token back from the server but also a refresh token. This is needed because the connection with the API and the collection of the free meeting times all happens in the Spring Boot application without the interference of a user. The first time the user will have to open a url in the browser to authenticate him/her-self and give consent that the application may have access to the calendar of the user. The following times that we need to access the API, we can simply use the refresh token to get a new access token from the server.Get authorization codeThis step only gets executed the first time that the user logs in. So we start the process by asking the Google OAuth 2.0 server for an authorization code.@Overridepublic String getAuthTokenGoogle() throws Exception {    AuthorizationCodeRequestUrl authorizationUrl;    GoogleClientSecrets.Details web = new GoogleClientSecrets.Details();    web.setClientId(googleApiClient);    web.setClientSecret(googleApiSecret);    clientSecrets = new GoogleClientSecrets().setWeb(web);    httpTransport = GoogleNetHttpTransport.newTrustedTransport();    flow = new GoogleAuthorizationCodeFlow.Builder(httpTransport, JSON_FACTORY, clientSecrets,            Collections.singleton(CalendarScopes.CALENDAR)).build();    authorizationUrl = flow.newAuthorizationUrl()            .setRedirectUri(redirectUri)            .setAccessType(\"offline\");    return authorizationUrl.build();}In the code above, we get the url where the user should login. We need to pass the client id and the client secret with the request as well as the redirect URI. You could also see that I only ask for the calendar scope, which is the only one I needed for this project. When the user opens this URL in the browser, the user has to login to his/her Google account after which Google prompts the user for consent to grant access to the calendar. When the login is successful, the server will send the authorization code back to the redirect URI specified in the request.Exchange authorization code for refresh and access tokensWhen the redirect URI is called, the following function is executed:@Overridepublic ResponseEntity&lt;String&gt; callback(String code) {    com.google.api.services.calendar.model.Events eventList;    String message;    try {        TokenResponse response = flow.newTokenRequest(code).setRedirectUri(redirectUri).execute();        credential = flow.createAndStoreCredential(response, \"userID\");        client = new com.google.api.services.calendar.Calendar.Builder(httpTransport, JSON_FACTORY, credential).build();        Events events = client.events();        eventList = events.list(\"primary\").setTimeMin(date1).setTimeMax(date2).execute();        message = eventList.getItems().toString();    } catch (Exception e) {        LOG.warn(\"Exception while handling OAuth2 callback (\" + e.getMessage() + \").\"                + \" Redirecting to google connection status page.\");        message = \"Exception while handling OAuth2 callback (\" + e.getMessage() + \").\"                + \" Redirecting to google connection status page.\";    }    return new ResponseEntity&lt;&gt;(message, HttpStatus.OK);}This is the part where we are exchanging the authorization code for the access and refresh tokens.When I got those tokens from the server, I manually set the refresh token in the application.properties file so I could use it later to get new access tokens.As a test, there is also some code to get the events between two dates, to make sure that I have access to the Google calendar API with the acquired access token.Getting an access token with the refresh tokenIn the actual flow I used the refresh token to get an access token from the Google server. This can be done with a simple POST request to the server with the client id, client secret and the refresh token. We then get a response with a new access token which we can use to send the request to the API.Getting the data from the calendarCreating new clientAs you could see in the code examples above, to have easy access to the calendar and get the events etc. we create a client object. The client object is an instance of the Calendar object.With this object we can get the calendars, the events and much more.Get freebusy scheduleBecause we need the free meeting times of the user, we would need the freebusy schedule. This gives us an overview when the user is busy and we can extract the free meeting times from that. To know for which date I have to get the freebusy schedule, I first get the next 5 working days in the chatbot from which the user can choose. This date is then forwarded to the API to retrieve the schedule.List&lt;FreeBusyRequestItem&gt; items = new ArrayList&lt;&gt;();FreeBusyRequestItem freeBusyRequestItem = new FreeBusyRequestItem();freeBusyRequestItem.setId(\"iebemaes\");items.add(freeBusyRequestItem);Calendar.Freebusy freebusy = client.freebusy();FreeBusyRequest freeBusyRequest = new FreeBusyRequest();freeBusyRequest.setTimeMin(dateTimemin);freeBusyRequest.setTimeMax(dateTimeMax);freeBusyRequest.setCalendarExpansionMax(2);freeBusyRequest.setGroupExpansionMax(2);freeBusyRequest.setTimeZone(\"UTC\");freeBusyRequest.setItems(items);Calendar.Freebusy.Query query = freebusy.query(freeBusyRequest);FreeBusyResponse freeBusyResponse = query.execute();Get free meeting timesTo get the free meeting times, I made a list with all the timeslots starting every hour. Then I looped over this list and the list with the busy timeslots. When the start hour of a busy timeslot matched with the start hour of a timeslot from the first list, I deleted this timeslot from the first list. That way I ended up with a list with only free timeslots.Send the meeting times back to ChatlayerNow that we have a list with all the free timeslots, we need to get this list in the chatbot. I wanted the timeslots to appear in the chatbot as quick replies. For each item in the timeslot list there should be a quick reply item. When I had the correct structure, I could send these quick replies back to Chatlayer with a simple POST request.for(String meetingTime: meetingTimes){        String[]hours=meetingTime.split(\"-\");        QuickRepliesItem quickRepliesItem=new QuickRepliesItem();        String title=hours[0]+\"u tot \"+hours[1]+\"u\";        quickRepliesItem.setTitle(title);        quickRepliesItem.setImageUrl(\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Simple_icon_time.svg/1200px-Simple_icon_time.svg.png\");        SessionDataToSetItem sessionDataToSetItem=new SessionDataToSetItem();        sessionDataToSetItem.setKey(\"chosenTimeSlot\");        sessionDataToSetItem.setValue(meetingTime);        List&lt;SessionDataToSetItem&gt; sessiondata=new ArrayList&lt;&gt;();        sessiondata.add(sessionDataToSetItem);        Payload payload=new Payload();        payload.setNextDialogstateId(\"045eac85-9c67-4b44-a0f5-aaacb724dbc6\");        payload.setSessionDataToSet(sessiondata);        quickRepliesItem.setPayload(payload);        quickReplies.add(quickRepliesItem);}Create an eventThe last step in this flow is of course the creation of an event in the calendar. When the user has selected a day, a timeslot and entered his/her name and address, the event needs to be saved in the calendar. This can also be done with the client object. So I pass the previously stated parameters to the API which will create the event.@Overridepublic void createEvent(String date, String meetingTime, String name, String address, String problemType) throws Exception {        String[] hours =  meetingTime.split(\"-\");        Event event = new Event();        event.setSummary(\"Herstelling bij: \" + name);        event.setDescription(\"Herstelling van: \" + problemType);        event.setLocation(address);        Date date1=new SimpleDateFormat(\"yyyy-MM-dd\").parse(date);        java.util.Calendar calStartDate = java.util.Calendar.getInstance();        calStartDate.setTime(date1);        calStartDate.add(java.util.Calendar.HOUR_OF_DAY, Integer.parseInt(hours[0]));        Date startDate = calStartDate.getTime();        DateTime dateTimemin = new DateTime(startDate);        EventDateTime start = new EventDateTime();        start.setDateTime(dateTimemin);        event.setStart(start);        java.util.Calendar calEndDate = java.util.Calendar.getInstance();        calEndDate.setTime(date1);        calEndDate.add(java.util.Calendar.HOUR_OF_DAY, Integer.parseInt(hours[1]));        Date endDate = calEndDate.getTime();        DateTime dateTimeMax = new DateTime(endDate);        EventDateTime end = new EventDateTime();        end.setDateTime(dateTimeMax);        event.setEnd(end);        String calendarId =\"primary\";        event = client.events().insert(calendarId, event).execute();}After the event is created in the calendar, I’m sending a response message to the chatbot that the event is created and setting the appropriate next dialog id.Have a look at the demo!SolidIntroduction to SolidWhy do we use Solid?There are a couple issues with the way we handle data for which Solid can provide a solution.Some examples of issues today:  In the personal data domain, there are a couple of big Tech companies who own the biggest part of our data          Problems with data-monopolies                  Security risks                          Data-breaches                                Les innovation          Political concerns                          Can affect the public debate and our perception of right and wrong                                          Companies are so successful because they have such a large amount of data, not because of their innovations      Because of these couple big companies, it is difficult for new companies to innovate because they don’t have the data for it                  Examples                          Google              Amazon              Facebook              …                                            People have lost control of their data          Hardly any visibility into what of your data is being retained      Little to no control over how your data is used and who is using it      …      What is Solid?Solid (Social Linked Data), made by Tim Berners-Lee,  is a specification which lets users store their data securely in a decentralized data store called pods. Solid itself isn’t much of an innovation, the innovation comes in bringing together different existing rules. Solid is based on open specifications just like the web itself. Open specifications mean interoperability across a broad ecosystem. The goal of Solid is to bring back data driven innovation and data ownership.In the Solid ecosystem, there are three important concepts: users, apps and pods.User:  Control which apps, organizations and people can access which part their data          Easier data sharing        Retains ownership and control over the data in his/her pod  Can have more then one pod          Pods can be hosted my the same pod provider or be self-hosted or a combination      Applications:  Store and access data in pods  Can get information from different podsPods:  The data stores where you store your data  You can store any kind of data in a Solid pod  Right now you can get your pod from a pod provider or you could host your pod yourselfSelf hosting Solid pod serverThere are different ways to setup a pod server where you can host your pod(s). I used the Docker container to setup my Solid server.To run the docker container I could execute this command in a terminal:docker run -p 8443:8443 --name solid-server.If we navigate to localhost:8443, we visit the homepage of the Solid server. First we get the message that the page is not secure but when clicking through, we arrive at the page where we can register a new account or login to an existing one.{  \"root\": \"./data/localhost.com/\",  \"port\": \"8443\",  \"serverUri\": \"https://localhost:8443\",  \"webid\": true,  \"mount\": \"/\",  \"configPath\": \"./config\",  \"dbPath\": \"./.db\",  \"sslKey\": \"./privkey.pem\",  \"sslCert\": \"./fullchain.pem\",  \"multiuser\": true,  \"corsProxy\": \"/proxy\",  \"server\": {    \"name\": \"\",    \"description\": \"\",    \"logo\": \"\"  },  \"enforceToc\": true,  \"disablePasswordChecks\": false,  \"tocUri\": \"https://your-toc\",  \"supportEmail\": \"Your support email address\"}When you host a pod server, there are some configuration options which you can modify. I think the most important one here is whether or not you want your server to be able to host more than one pod or not. For testing purposes, I set the multiuser setting to true so my server is capable of hosting multiple pods.RegisterWhen you want to register an account you need to provide some simple information like your name, email address, password and username. As you can see below, when creating an account, your webId (unique identifier for Solid pods) is made of your username and the domain (in this case this is localhost:8443). The /profile/card#me is just to specify where your profile information is stored. When registering a new account you need to add an entry for it in you local hosts file like this:127.0.0.1\t\tnewuser.localhostLoginWhen the account is created you could login to your own Solid pod. When you login you first get a popup. You can enter your webId or you identity provider. In this case I’m logging in with my identity provider.And enter your username and password which you specified when the account is registered.Edit profileAs a logged in user you can see your profile, but when you first login there is nothing there. However, you can easily edit your own information.Your dataOf course, the main thing you want to know about Solid is your storage. When the pod is created you have these standard sections ready for you. The sharing settings for these folders have also been set when the pod is created. Profile and public are two folders which everyone can read but not change. And as you would expect, private is only for you.Sharing of dataWithin the Solid interface it is very easy to share data with other people, groups, apps…You could set these sharing permissions on a folder but also on a specific file. In the standard scenario the files inside a folder just inherit the sharing permissions.Solid also has a nice overview of which applications have access to your pod, and also which kind of access they have (read, write, append or control). In this overview you could also revoke access and update the kind of access that these applications have.As mentioned before, you can store any kind of data on your pod, this goes from .txt files, to .html files and even .ttl files. For this project I’ve chosen to store data in .ttl files because that’s what I saw the most in the documentation.Turtle (RDF)Before we can discuss Turtle we need to have a basic understanding of RDF and linked data. RDF stands for Resource Description Framework and is used to describe resources on the internet. A resource is anything on the internet, for example: a person, a book… With RDF every resource is described as a triple. Those triples consist of a subject, a predicate and an object. Using this simple model, data (structured and semi-structured) can be mixed and shared between different applications. This linking structure forms a directed and labeled graph.Turtle itself is a format that allows RDF graphs to be written in a natural text form. Next to N-Triples, JSON-LD and RDF/XML it is one of the four common formats to write RDF. In Turtle you can declare prefixes to use as a shortcut throughout the rest of the file so you don’t have to write full URI’s everywhere.Example:This is the card file in the profile folder which contains all the information about the user.The full notation for the full name would be:#me http://www.w3.org/2006/vcard/ns#fn Iebe MaesBecause of the short notation, Turtle is much easier to understand than the other formats.How to access your data?The login page is built in React.The first step to get information in your solid pod is authentication. Luckily Solid has a library with some nice components that make the login process much easier.import { LoginButton, LoggedOut, LogoutButton, LoggedIn } from '@solid/react';&lt;LoggedIn&gt;        {this.state.needsToLogin ? &lt;GetInformation sendInformation={this.sendInformation} /&gt; :         &lt;Row&gt;                &lt;Col&gt;&lt;h1 className=\"text-  center\"&gt;Informatie is correct binnengekomen! U mag terugkeren naar de chatbot.&lt;/h1&gt;&lt;/Col&gt;        &lt;/Row&gt;}&lt;/LoggedIn&gt;&lt;LoggedOut&gt;        &lt;Row className=\"justify-content-md-center mt-5\"&gt;              &lt;Col md=\"auto\"&gt;                &lt;h1 className=\"text-center\"&gt;Welkom!&lt;/h1&gt;                &lt;h3 className=\"text-center\"&gt;Gelieve u aan te melden.&lt;/h3&gt;              &lt;/Col&gt;        &lt;/Row&gt;        &lt;Row className=\"justify-content-center\"&gt;                &lt;Col md=\"auto text-center\"&gt;                &lt;LoginButton className=\"btn btn-primary\" popup=\"popup.html\"&gt;log in &lt;/LoginButton&gt;              &lt;/Col&gt;        &lt;/Row&gt;&lt;/LoggedOut&gt;The loggedIn and loggedOut component do exactly what you expect them to do, show some content when the user is logged in and show other content when the user isn’t logged in. Then we have the loginButton component which shows popup.html which is the popup screen you see in the login section above.To get the actual data, from for example the profile, you would first need to get the webId. You can get this with the react hook provided by the same library from above.const webId = useWebId();Then we would need the data object.const { default: data } = require('@solid/query-ldflex');And with the data object and the webId, we can get the profile.Once we have this object you can ask for anything you see in the structure off the profile card under :me. Because your webid is something like “username.domain/profile/card#me”. The profile object we would get would actually be the #me part of the card.const profile = data[webId];To get the full name, we place the predicate between [ ] after the profile object.const fn = await profile['http://www.w3.org/2006/vcard/ns#fn']You can also see that for example the address isn’t stored under the :me part but it has an id. To get the properties of the address we would have to do something like this:const addressURL = await profile['http://www.w3.org/2006/vcard/ns#hasAddress']const city = await addressURL['http://www.w3.org/2006/vcard/ns#locality']const postalCode = await addressURL['http://www.w3.org/2006/vcard/ns#postal-code']const street = await addressURL['http://www.w3.org/2006/vcard/ns#street-address']ConclusionFirst of all, I want to thank Ordina for the chance that they have given me to do this internship. Also a special thanks to all three of my mentors: Frederick Bousson, Jasper Rosiers and Hans Vanbellingen! They always provided me with feedback when I finished a part of the solution and would always make time to help me with my questions or to explain things that weren’t clear.It really was an educational internship, where I’ve learned a lot!  I learned more about how to properly develop a Spring Boot application  I’ve been researching Solid          What it is      How to set it up      How to extract data        I’ve learned how to make chatbots with Chatlayer and understanding how they work  Got a more thorough understanding of how to write unit tests  Learned how to work with google API’s  Taken a look at Azure AD  Taken a look at how Ordina approaches the start of a projectThis internship has really given me the feeling that I chose the right study.The only sad thing about this internship was that it was during the Covid-19 pandemic so I haven’t had the chance to really experience the work-life @Ordina. Despite corona, I had a very nice time working on this project."
      },
    
      "iot-2021-05-26-intro-to-mqtt-html": {
        "title": "Intro to mqtt",
        "url": "/iot/2021/05/26/intro-to-mqtt.html",
        "image": "/img/2021-05-26-intro-to-mqtt/banner.jpg",
        "date": "26 May 2021",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  MQTT terminology  Broker options  Basic example  Conclusion  ResourcesIntroductionMQTT or Message Queuing Telemetry Transport is a very lightweight IoT messaging protocol.It was originally designed by IBM and has become royalty free since 2010.It is very lightweight, both on computational and network resources to send and receive messages, making it ideal for use with IoT applications as well as restrained network conditions.The protocol is built on top of tcp/ip so both broker and client require a tcp/ip stack.This allows for reliable bi-directional communication that supports authentication and TLS encryption and uses the publish/subscribe pattern.Using this pattern has multiple advantages:  Space decoupling: Publisher and subscriber clients do not need to know each other.  Time decoupling: Publisher and subscribers do not need to run at the same point in time.  Synchronization decoupling: Operations do not need to be interrupted during publishing or receiving.Also be reminded that MQTT is NOT a message queue!By default, messages will not be stored if there are no clients to consume them, and even retained messages only keep the last retained message, overwriting any previous retained message.                MQTT terminologyLet’s go over some terminology to better understand and grasp the concepts of the MQTT protocol:Broker:The host that acts as the manager for all messages.It is responsible for receiving messages from publishers, performing checks to see which subscribers match the topic and sending the messages to those subscribers.Client:An MQTT client, meaning any instance that implements logic to connect to a broker.Clients can be a publisher, a subscriber, or both.A client who is a publisher sends messages, while a client whom is a subscriber receives and consumes messages.It is perfectly possible, and often used, that a client is both a publisher and a subscriber at the same time.Topics &amp; wildcards:A string that acts as a subject for publishing to or subscribing to.Quality of Service (QoS):Is a setting to ensure a certain level of guaranteed delivery.In MQTT there are 3 QoS options:                  0: At most onceThis service level only guarantees a best effort delivery of messages.Delivery of messages is not guaranteed so data might be lost in transit.No acknowledgements are sent and no data is retransmitted.                  1: At least onceThis service level guarantees that messages are delivered at least once.The sender stores the message until it receives an acknowledgement from the broker.If the acknowledgement is not received in a timely manner the message is retransmitted.                  2: Exactly onceThis service level guarantees that messages are delivered exactly once.To enable this, a four-way handshake is used between the client and the broker.Retained messages:These are messages with the retained flag set to true.The broker will store these messages with their QoS and send it to any client that connects.This enables newly connected clients get an update quicker since they do not need to wait for a new message to be published.Retained messages can also be deleted easily: the client just needs to send an empty (0 byte payload) retained message.Last Will and Testament (LWT):Is a feature to notify clients about a client that has disconnected in an ungraceful manner.The message is sent to the broker when a client connects so it can be sent to other clients later on if required.If the client disconnects gracefully the broker discards the LWT message.Broker optionsAs MQTT requires a broker instance to function, choosing the right one is crucial.There many different options available, both can run locally or hosted in the cloud.There are also different versions of the MQTT protocol and not every broker supports all the different versions:  3.1: The older IBM based version, less used these days  3.1.1: OASIS standard compliant, the most used version nowadays  5: The newest version (2019), not yet widely usedEclipse MosquittoEclipse Mosquitto is an open source implementation of an MQTT message broker.It supports all three major versions of the protocol.The broker supports all three major versions and can run on low-powered devices like the Raspberry Pi.I use this one at home for my home automation projects.It is very easy to install on a Debian based distro:wget http://repo.mosquitto.org/debian/mosquitto-repo.gpg.keysudo apt-key add mosquitto-repo.gpg.keycd /etc/apt/sources.list.d/# Pick the correct URL for your flavour of Debian (we pick buster as the default):# sudo wget http://repo.mosquitto.org/debian/mosquitto-wheezy.list# sudo wget http://repo.mosquitto.org/debian/mosquitto-jessie.listsudo wget http://repo.mosquitto.org/debian/mosquitto-buster.listsudo apt-get updateapt-get install mosquitto# We will add username/password auth for connections to the auth (anonymous is allowed by default, we don't want this, skip this section if you do)# username: myuser, password: enter into the mosquitto_passwd tool (enter any valid password)sudo mosquitto_passwd -c /etc/mosquitto/credentials myusersudo nano /etc/mosquitto/mosquitto.conf# At the end of the file add:# allow_anonymous false# password_file /etc/mosquitto/credentialssudo service mosquitto restartAedesAedes is the follow-up/split from Mosca and is fully open source.It is a node based MQTT broker that is scalable and lightweight.The broker only has support for the 3.1 and 3.1.1 protocol versions, 5.0 is not supported yet.Installing is very simple, just make sure you have node installed and simply install it by using npm: npm install aedes.You are responsible for creating the server instance from code.A very basic implementation of the broker is:const aedes = require('aedes')();const server = require('net').createServer(aedes.handle);const port = 1883;server.listen(port, () =&gt; {  console.log('Server started and listening on port ', port);});HiveMQHiveMQ is an MQTT-based platform that includes a broker.It has the option to be hosted in the cloud (with a free trial tier) or to be run locally.The broker has support for all three major versions of the protocol.It does require you to create an account before you can use the cloud tier or even download the zip package for local installation.EmqttdEmqttd is another fully open source broker.The project is written in Erlang and is fully compatible with the 3.1 and 3.1.1 versions of the protocol.VerneMQVerneMQ is another well-known broker that is also fully open source and written in Erlang.It has the ability to scale very well, both vertically and horizontally.The broker has support for all three major versions of the protocol.In addition to the free-to-use broker, they also have paid tiers of support.Basic examplesBasic C example (for use on an ESP-01):#include \"EspMQTTClient.h\"EspMQTTClient *client;void setup(){  client = new EspMQTTClient(\"SSID\", \"SSID_PASS\", \"BROKER_URL\", \"MQTT_USERNAME\", \"MQTT_PASSWORD\", \"CLIENT_NAME\", 1883);  //client-&gt;enableDebuggingMessages(true);  client-&gt;enableLastWillMessage(\"LAST_WILL_TOPIC\", \"LAST_WILL_MESSAGE\");}void onConnectionEstablished(){    Serial.println(\"MQTT: Connected\");    client-&gt;publish(\"TOPIC\", \"connected\");    client.subscribe(actionTopic, [](const String &amp; payload) {        Serial.println(payload);    });}void loop(){  client-&gt;publish(\"TOPIC\", \"DATA\");  sleep(1000);}This example uses the EspMQTTClient library to enable low powered IoT devices with WiFi connectivity to connect to an SSID and broker.The library handles both the WiFi and broker connections.Basic node example:import mqtt, {Client} from \"mqtt\";import {ISubscriptionGrant} from \"mqtt/types/lib/client\";import {Packet} from \"mqtt-packet\";export class Main {    private readonly client: Client;    constructor() {        this.client = this.connectMqtt();        this.receive();    }    private connectMqtt(): any {        const client: Client = mqtt.connect('mqtt://broker-url:1883', {username: 'username', password: 'password'});        client.on('connect', () =&gt; {            console.log('Connected to MQTT broker!');        });        return client;    }    private receive(): void {        this.client.subscribe('topic', {qos: 0}, (err: Error, granted: ISubscriptionGrant[]) =&gt; {            console.log(granted);            if (granted &amp;&amp; granted.length === 1) {               this.client.on('message', (topic: string, payload: Buffer, packet: Packet) =&gt; {                    console.log(payload.toString());               });            }        })    }}const main = new Main();The node code is a bit more complex but allows you to create more complex applications.It uses the MQTT.js library which has very good and extensive documentation.This is not meant to run on the IoT device but on a separate device that reacts to messages from the IoT devices.Please note that this will not manage the WiFi/ethernet connection of the device that it is running on, which is left to the OS/User.ConclusionMQTT is an ideal protocol to use for lightweight communication on ip enabled devices.The pub/sub architecture allows for a decoupled environment of clients that can operate independently of each other.Thanks to the protocol and its implementations being very lightweight, it is very handy to use in combination with IoT and home automation projects.Getting started with some simple setup is relatively easy and does not require difficult programming to wire things up.In a future blog post, I will go a bit more in depth and show off a project which makes use of MQTT to wire devices together.Resources  MQTTT  MQTT specifications  Beginners guide  MQTT essentials  Broker  Client  Quality of Service (QoS)  four-way handshake  Retained messages  Last Will and Testament (LWT)  Eclipse Mosquitto  Aedes  Mosca  HiveMQ  Emqttd  VerneMQ  EspMQTTClient  MQTT.js"
      },
    
      "event-driven-2021-04-23-sse-with-http2-html": {
        "title": "SSE with HTTP2",
        "url": "/event-driven/2021/04/23/SSE-with-HTTP2.html",
        "image": "/img/2021-04-23-SSE-HTTP2/SSE-HTTP2.png",
        "date": "23 Apr 2021",
        "category": "post, blog post, blog",
        "content": "Table Of Contents  What are Server-Sent Events?  Why SSE over Websockets?  HTTP/1.1 vs HTTP/2  Summary  Demo applicationWhat are Server-Sent Events?Server-Sent Events is a technology where the client receives data (events) pushed by a server over HTTP.This data can be a random update (for example a tweet) or a constant stream of data (stock market price updates).The main thing to note is that the client does not need to poll for this data. There is no communication required from client to server.This technology may have been overshadowed by WebSockets because of SSE limitations in the past, but as you will see and learn in this blogpost, you have nothing to worry about anymore!Why SSE over WebSockets?While it is true that WebSockets have more capabilities than SSE, when these capabilities are not part of your use case, SSE in my opinion is a much better choice. With Websockets, you have the ability to communicate from your client to the server.But you are going to have to take care of this connection yourself.One of the implications is that the connection needs to be  stateful, which is a pretty important thing to take into consideration when you are trying to build cloud-native applications.WebSockets are also more supported by older browsers than SSE but this is easily solved by using the JavaScript EventSource interface to create your own connection to the server and receive the data that way.I will demonstrate how easy it is to use this interface and solve this issue while building the demo application at the end of the blog post.But then why use SSE?One of the key differences between SSE and WS is that SSE uses simple HTTP to send data to the clients.This means it does not require a special protocol like STOMP or MQTT which in turn requires server implementation to get it working making SSE a lot easier to set up.SSE also has built-in support for reestablishing connections and event IDs which WS lacks by design.So the main questions you have to ask yourself is whether your use-case fits into the whole SSE story?Is it OK to have no communication back to the server?Is the application you are trying to build supposed to be used in a cloud-native environment and if so will a stateful connection between client and server be a problem?These questions will hopefully help make your choice between Server-sent Events and WebSockets easier.HTTP/1.1 vs HTTP/2HTTP/1.1 is an old protocol, it loads requests one-by-one over a single TCP connection or in parallel over multiple TCP connections in an effort to decrease loading times while requiring more resources.This was fine when this protocol was new, about 23 years ago, but as time goes by and webpages become more advanced, the limitations of this protocol are really starting to show.This is why HTTP/2 was made, it aims to tackle the limitations set by HTTP/1.1 and be more future-proof.With HTTP/2, multiple requests can be sent over the same TCP connection with responses arriving out of order.HTTP/2 is a binary protocol, removing security issues and error-proneness that come with text-based protocols.It is backward compatible with earlier versions of the protocol and is compatible with almost all browsers.HTTP/2 also avoids the round trip to the server by having the server intuitively sending resources that will be required to render the page.All these advantages eliminate the need for developers to write best practice workarounds to deal with the limitations of older versions of the protocol,they decrease loading times and improve the website infrastructure.This on top of full backward compatibility makes the choice between HTTP/1.1 and HTTP/2 for Server-sent events a no-brainer.SummaryThe key takeaways in choosing WS or SSE are entirely dependent on the use-case of the application you are trying to develop.If you are looking for a stateless approach, or you don’t have a need for client-to-server communication, SSE might be the solution for you!The other takeaway is that you should definitely use HTTP/2 to get the most out of your application and not run into the limitations that HTTP/1.1 lays upon SSE.If after reading this blogpost you have come to the conclusion that you would be better off building a Websocket application to fit your use-case you can read through a blog post on WebSockets made by my colleague Kevin Van Houtte here.Demo applicationIn this part of the blogpost I am going to show you how easy it is to develop your own SSE application.The use-caseWe are going to build a Spring Boot application that consumes a Chuck Norris joke REST API and uses Flux to push joke data from the server using Server-Sent events to any clients that are subscribed.The SSE serverpom.xmlTo start off we are going to make a Spring Boot application and add the following Maven dependencies:&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;        &lt;scope&gt;runtime&lt;/scope&gt;        &lt;optional&gt;true&lt;/optional&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;        &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;ChuckNorrisJoke.javaNow we are going to add our data model, as explained in the use-case this will be a simple Chuck Norris joke object containing a String value.public class ChuckNorrisJoke {    private String value;    public String getValue() {        return value;    }    public void setValue(String value) {        this.value = value;    }}JokeService.javaThe next step is to get the joke data by consuming a public Chuck Norris joke API.First we create a JokeService interface and implementation:import org.springframework.stereotype.Service;@Servicepublic interface JokeService {    ChuckNorrisJoke getRandomChuckNorrisJoke();}import org.springframework.http.HttpEntity;import org.springframework.http.HttpHeaders;import org.springframework.http.HttpMethod;import org.springframework.http.ResponseEntity;import org.springframework.stereotype.Component;import org.springframework.web.client.RestTemplate;@Componentpublic class JokeServiceImpl  implements JokeService{    private RestTemplate restTemplate;    private HttpHeaders httpHeaders;    private final String chuckNorrisJokeUrl = \"https://api.chucknorris.io/jokes/random\";    JokeServiceImpl(RestTemplate restTemplate, HttpHeaders httpHeaders){        this.restTemplate = restTemplate;        this.httpHeaders = httpHeaders;    }    @Override    public ChuckNorrisJoke getRandomChuckNorrisJoke() {        ChuckNorrisJoke joke = new ChuckNorrisJoke();        ResponseEntity&lt;ChuckNorrisJoke&gt; response = restTemplate.exchange(chuckNorrisJokeUrl,                HttpMethod.GET,                new HttpEntity&lt;&gt;(httpHeaders),                ChuckNorrisJoke.class);        if (response.hasBody()) {            joke.setValue(response.getBody().getValue());        }        return joke;    }}ServerConfig.javaThe RestTemplate and HttpHeaders beans are defined in the ServerConfig class as follows:import org.springframework.boot.web.client.RestTemplateBuilder;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.http.CacheControl;import org.springframework.http.MediaType;import org.springframework.web.client.RestTemplate;import org.springframework.http.HttpHeaders;import java.time.Duration;@Configurationpublic class ServerConfig {    @Bean    public RestTemplate restTemplate(RestTemplateBuilder builder) {        return builder                .setConnectTimeout(Duration.ofMillis(10000))                .setReadTimeout(Duration.ofMillis(10000))                .build();    }    @Bean    HttpHeaders httpHeaders() {        HttpHeaders headers = new HttpHeaders();        headers.set(HttpHeaders.ACCEPT, MediaType.APPLICATION_JSON_VALUE);        headers.set(HttpHeaders.CACHE_CONTROL, CacheControl.noCache().cachePrivate().mustRevalidate().getHeaderValue());        return headers;    }}JokeControllerNow for the final part of the Java code, all we have to do is create an endpoint for clients to subscribe to and push the joke data to this endpoint.For this we are going to create a JokeController and use a Flux which is a Reactive Stream publisher to periodically emit Server-Sent events containing ChuckNorrisJokes to this endpoint.import org.springframework.http.codec.ServerSentEvent;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import reactor.core.publisher.Flux;import java.time.Duration;@RestController@RequestMapping(\"sse-server\")public class JokeController {    private final JokeService jokeService;    JokeController(JokeService jokeService){        this.jokeService = jokeService;    }    @GetMapping(\"/chuck-norris-joke-stream\")    public Flux&lt;ServerSentEvent&lt;ChuckNorrisJoke&gt;&gt; streamJokes() {        return Flux.interval(Duration.ofSeconds(5))                .map(sequence -&gt; ServerSentEvent.&lt;ChuckNorrisJoke&gt;builder()                .data(jokeService.getRandomChuckNorrisJoke())                .build());    }}When we run our application and go to http://localhost:8080/sse-server/chuck-norris-joke-stream you can see data coming in every 5 seconds.The web clientNow all that’s left to do is use the JavaScript EventSource interface to open a connection to our SSE server and transform the events into text to display in our basic HTML demo page.index.html&lt;!DOCTYPE html&gt;&lt;html&gt;    &lt;head&gt;        &lt;title&gt;Server-sent events app&lt;/title&gt;        &lt;script src=\"/app.js\"&gt;&lt;/script&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;div id=\"main-content\" class=\"container\"&gt;            &lt;div class=\"row\"&gt;                &lt;table id=\"jokes\" class=\"table table-striped\"&gt;                    &lt;tr&gt;                        &lt;th&gt;Chuck Norris Jokes&lt;/th&gt;                    &lt;/tr&gt;                &lt;/table&gt;            &lt;/div&gt;        &lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;app.jsconst eventSource = new EventSource('sse-server/chuck-norris-joke-stream')eventSource.onmessage = function (e) {    const joke = JSON.parse(e.data);    showJoke(joke.value);}function showJoke(joke) {    var table = document.getElementById(\"jokes\");    var row = table.insertRow(-1);    var cell = row.insertCell(0);    cell.innerHTML = joke;}ResultI will leave the styling up to you but you should now have a working SSE server and client that receives data in the form of Server-Sent events.All that is left to do for you is to enable HTTP2 by adding server.http2.enabled=true to your application.properties file and to enable HTTPS the way you would do it in any Spring Boot application.One final noteYou can find the code for this application using HTTP/1.1 and HTTP/2 as well as an example to achieve the same thing using Websockets on my github.If you have any questions regarding this topic you can reach out to me on my Twitter and I will try my best to help you out."
      },
    
      "leadership-2021-04-14-delegation-html": {
        "title": "How to delegate effectively as a technical team lead",
        "url": "/leadership/2021/04/14/Delegation.html",
        "image": "/img/2021-04-14-Delegation/delegate.jpeg",
        "date": "14 Apr 2021",
        "category": "post, blog post, blog",
        "content": "Don’t be the bottleneck.Although it has been quite some time, I still remember my first project as technical team lead like it was yesterday.Before taking on that responsibility, I had worked hard as a developer and had naturally stepped into some minor leadership roles along the way.I was very honoured to be trusted with the technical team lead role, and soon the team was coming to me for direction while my manager was looking at me for updates on how the team was performing.Quite quickly I found out that management expected me to keep running my team efficiently, but in the meantime I was also trying to be as productive as I used to be when I was a developer by delivering all my user stories on time.Needless to say that my tasks started to pile up, my to-do list was bursting at the seams and my frustrations grew by the day.Common pitfalls new tech leads often experience is that they feel like they must review every change to the codebase, they must have the final say on all technical decisions, or they must be involved in every discussion.By doing this, they unintentionally become a bottleneck and slow the team down.At worst, they even make the team members feel micromanaged and disempowered.I realised that in order to keep my team productive and thus becoming an effective tech lead, delegation was one of the major keys to success.At first, I struggled with learning this new skill.After all, I was not producing anything concrete anymore.There was no rewarding merge request afterwards, nor a notification of a successful build.The new skill I yet had to learn did not have the same tangible output as writing code.  “Sometimes you just have to be willing to delegate and not feel like you’re the only one with the answer.” - Ronald D. MooreNothing is black or white.“Delegating” means that you give someone else the authority to do part of your job.Consequently, it’s not a part of their job, but you are giving them the authority to do tasks that you normally take up yourself.It’s about letting people make decisions for themselves without micromanaging them.The misconception a lot of people often have is that delegation is binary: it’s all or nothing.They worry that others will screw up the things they care about a lot, and therefore rather not take the risk of delegating the task at all.However, there actually are different shades of grey when it comes to delegation.The diagram below shows the relationship between the level of freedom that a team lead chooses to give his team and the level of authority used by the team lead.This is the Tannenbaum &amp; Schmidt Leadership Continuum, a leadership model that was designed by Robert Tannenbaum and Warren H. Schmidt and first published in the Harvard Business Review in 1958.Their leadership model only gained popularity since then and today still is a frequently used tool to identify and select the most appropriate leadership style for any situation.It stipulates that as you move up the slope from left to right on the diagram above, the leader gives up his authority to make solo decisions and he increases team involvement.The team’s freedom increases when the team lead’s authority decreases and he delegates more of his tasks to the team.This means that as you shift from a work-centered leadership style to a people-centered leadership style, your area of control as a team lead relinquishes.This might make you feel anxious and nervous at first, but luckily there are seven different levels of delegation.  Tell: This type of leader does not believe his team should have any say in the decision-making process. The leader himself identifies the problem, picks the solution he considers best, communicates his decision to the team and expects that the team acts accordingly.  Sell: This leadership approach follows the same decision-making process as the “Tell” approach. The main difference however, is that the leader provides the team with the rationale of his decision. He does remain reluctant to give up any control or to allow any additional input into the decision-making process.  Suggest: The leader takes it one step further than the “Sell” approach by inviting the team to ask questions in order to get a deeper understanding of why a particular decision has been made for them. This helps build trust and is the first step towards a two-way communication process between team lead and team members.  Consult: The leader proposes a decision, invites the team to give their input and opens a discussion to ensure that his decision is the right one. The team is able to influence the outcome, but the team lead still remains in control of identifying the problem, possible solutions and making the preferred final decision. This leadership style can build cohesiveness and improves the team’s motivation.  Join: In all previous approaches, the leader was the person responsible for identifying the problem, possible solutions and making the final decision. In the “Join” approach however, the leader presents problems instead of a solution to the team. While ultimately the leader still makes the final decision, this approach enables a collaborative decision-making process. This style is often used when the team has specific expertise that the leader needs in order to make the best decision.  Delegate: The role of the leader is to present the problem to the team together with any constraints. The team can then operate within these guidelines to find solutions and make the final decision. The leader needs to trust the team and provide them with the support and resources necessary to make a solid decision.  Abdicate: The team is seen as a group of experts that identify problems, potential solutions and make the final decision. They are free to do whatever is necessary to solve a problem while still working under reasonable constraints. However, the leader is still accountable for the final decision and therefore must make sure the team is ready for this level of responsibility.Based on these different levels, we can conclude that in any decision-making process, there are three key steps:  Identifying Problems  Identifying Solutions  Making a DecisionThe following diagram shows that as you move through the Continuum, the control over these three steps transfers from the leader to the team.Context is everything.Depending on your personality as a team lead, you will probably feel that some delegation approaches listed above are more in your comfort zone than others.However, sometimes it’s not entirely up to you as the type of delegation to follow might depend on the situation at hand:  The capability of the team in terms of skills, experience, workload, …  The nature of the project or task in terms of various aspects like complexity, difficulty, risk, time or budget constraints, relevance, …So while the Tannenbaum &amp; Schmidt Leadership Continuum does not present a clear guide for what is best, it does provide a framework for leaders. Ultimately, the unique combination of the leader’s own personality, the capabilities of the team and the nature of the project or task will determine which type of delegation is preferred.Although the appropriate level of delegation depends on the context, the advantages of effective delegation for both team lead as well as the team are clear: the team lead gets more time to focus on critical issues, and the team develops new skills and grows into future technical team leaders.  “If you want to do a few small things right, do them yourself. If you want to do great things and make a big impact, learn to delegate.” – John C. Maxwell"
      },
    
      "iot-2021-03-25-getting-started-with-the-pi-pico-html": {
        "title": "Getting started with the Pi Pico",
        "url": "/iot/2021/03/25/Getting-started-with-the-pi-pico.html",
        "image": "/img/2021-03-25-getting-started-with-the-pi-pico/banner.jpg",
        "date": "25 Mar 2021",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Differences with the Raspberry Pi  Getting to know the board  Development options  C++ development  MicroPython development  CircuitPython development  Conclusion  ResourcesIntroduction                By now everybody likely has heard about the Raspberry Pi single board computers.And even more than probable some, if not most of you, have one or more of these.Recently the Raspberry Pi Foundation has released a new type of Raspberry Pi, the Raspberry Pi Pico.Contrary to the regular Raspberry Pi, the Pico is not a single board computer which runs Linux.The Pico is a microcontroller, like the Arduino or Teensy.It’s cheap, it’s powerful, and in this blog post we’ll be exploring what it has to offer.Differences with the Raspberry PiLike I said in the introduction, the regular Raspberry Pi is a single board computer, as in an actual computer which can run a fully fledged operating system.It also has interfacing options like a display and camera port, along side ethernet and USB ports as well as the 40 GPIO pins.                As you can see in the pictures, the Raspberry Pi looks like a tiny computer and the Pico looks like a much simpler board, because it also is.Both devices serve different purposes but do have some overlap.The Raspberry Pi is used for more computationally intensive tasks and can be used to run very complex software.The Pi Pico is used for far simpler tasks where power usage and device footprint are more important, it is a much more low-level device.While they both have GPIO pins, the ones on the Pi Pico are much more capable than those on the Raspberry Pi.Getting to know the board                The main technical specifications of the Pi Pico are:  RP2040: Dual-core Arm Cortex-M0+ processor, flexible clock running up to 133 MHz  264KB on-chip SRAM  2MB on-board QSPI Flash  26 multi-function GPIO pins, including 3 analogue inputs  2 × UART, 2 × SPI controllers, 2 × I2C controllers, 16 × PWM channels  1 × USB 1.1 controller and PHY, with host and device support  8 × Programmable I/O (PIO) state machines for custom peripheral supportThese specs are in line with some of the more popular microcontrollers like teensy and ESP32 devices.The small footprint of microcontrollers like the Pico allows it to be integrated into DIY projects easily.The Pi Pico is built around the RP2040, the actual microcontroller that powers it.There are other boards with varying pinouts and functions available with this microcontroller.In this blog post we will be focussing on the Pi Pico implementation of the RP2040.The dual core chip give a lot of flexibility to create project which require a bit more processing power, as do the PIO state machines.With 26 GPIO boards there are loads of options to connect sensors, screens, inputs and outputs.Development optionsOne of the nice features is that the Pico supports drag and drop programming/flashing.This is accomplished by utilizing UF2 files.The process of doing this is very simple, simply press the BOOTSEL button when connecting the Pico to your computer.It will show up as a storage device, drag the UF2 file onto it and the device will reboot and the flashing is completed.The Pico has several development options available.Three different main options are available to program it:  C/C++ SDK: Oldschool hardcore mode microcontroller programming  MicroPython: More beginner friendly with lots of options  CircuitPython: Adafruit backed variation on MicroPython, made even simplerC++ DevelopmentDevelopment setupSetting up for development:  General C/C++ SDK documentation  Linux: Simply run the script  Mac: Follow the instructions  Windows: Follow the instructionsOne bright point is that Arduino will also be releasing a board based on the RP2040 so there is hope that the Arduino IDE will support it later down the line and enable hassle free C++ development for the Pi Pico and other RP2040 based microcontrollers.Code exampleProgramming microcontrollers has long been done in C and C++ and the Pi Pico forms no exception to this.The basic code for a LED blink example is listed below./** * Copyright (c) 2020 Raspberry Pi (Trading) Ltd. * * SPDX-License-Identifier: BSD-3-Clause */#include \"pico/stdlib.h\"int main() {#ifndef PICO_DEFAULT_LED_PIN#warning blink example requires a board with a regular LED#else    const uint LED_PIN = PICO_DEFAULT_LED_PIN;    gpio_init(LED_PIN);    gpio_set_dir(LED_PIN, GPIO_OUT);    while (true) {        gpio_put(LED_PIN, 1);        sleep_ms(500);        gpio_put(LED_PIN, 0);        sleep_ms(500);    }#endif}This code will include the stdlib.h header file from the Pi Pico C++ SDK and will blink the built-in LED every 500ms.The trick is to compile this code and build the required UF2 file.While the code looks and feels very similar to what you would write for lets say an Arduino board, compiling it and to run on the Pico is a different case.The contrast with Arduino development could not be bigger, where you simply download the Arduino IDE, write your code, click upload and you’re running your code on the device!With the Pico it’s not that simple.It requires a few dependencies which cannot be one-click installed and the instructions are different based on the OS you’re running.It’s do-able but it’s not exactly hassle free and convenient.MicroPython developmentContrary to the C++ development, using MicroPython is like a breath of fresh air.I’m not the biggest fan of regular Python but using MicroPython for simple microcontroller programming, has in my opinion, made it a lot easier.Another big plus of Python based development is having a REPL available.It allows you to write and execute code on the fly, which allows for easier development and testing something quickly.Preparing the boardFirst of all we need to prepare the Pico to accept and run MicroPython files:  Download the latest (stable) version of MicroPython for the pico, this is a UF2 file and can be found here  Plug in your Pico while holding the BOOTSEL button  Drag the downloaded UF2 file onto the mass storage device that has just appeared in your file explorer  The board will reboot by itself and ‘install’ the UF2 fileDevelopment setupThe easiest way to do MicroPython development for the Pi Pico is by using the Thonny IDE.Thonny is a simple code editor that can directly save your code to the Pico and also provides an easy way to access the REPL.Setting up Thonny is very easy.Just download the binary for your operating system, install and start it.Only one more step is required to set it up for use with the Pi Pico.Head into the preferences, select the interpreter tab and from the dropdown select the option MicroPython (Raspberry Pi Pico).Select the correct port from the dropdown.After this Thonny is ready to be used!                                Another option for development is using the great PyCharm IDE.While I view Thonny to be a glorified text editor, PyCharm (from JetBrains) is a fully fledged Python IDE, for our purposes the community edition will do just fine.The setup is a bit more involved and has some caveats, but it is certainly workable.Download and install the PyCharm Community Edition.Once done, open it up and in the welcome screen select the Plugins option.In here, search for micropython and install the MicroPython plugin by JetBrains, this will add support for flashing the device from within the IDE.Now we can create a new project and write some code.To upload the code to the Pico, we need to perform some more changes and enable the MicroPython support for the newly created project.Head into the IDE preferences, select the Languages &amp; Frameworks option in the sidebar and select the MicroPython sub option.In this section, check the Enable MicroPython support and set the device type to ESP8266.Finally, we need to enter the device path.You could try the auto-detect option, but on my machine that did nothing.To get this path, we will open Thonny with the Pico plugged in.It will show the device path under Preferences/Interpreter and we can copy it to PyCharm.With this done, we can now copy over code to the Pico.Make sure your main code file is called main.py, as this will be executed automatically by the Pico.To do this, simply right-click on the file (or entire folder) and select the Run Flash FILENAME.Make sure you do not have the REPL open in the PyCharm terminal window, in contrary to Thonny, PyCharm will fail the copy action!If you have copied too many files or simply want to wipe the device, from the menu, select: tools/MicroPython/Remove All Files from MicroPython Device.To open the REPL and directly interact with the Pico select the tools/MicroPython/MicroPython REPL, this will open up a REPL in a terminal window.                                                                                                Code exampleThe basic code for a LED blink example is listed below.from machine import Pinimport timepin = Pin(25, Pin.OUT)while True:    pin.value(1)    time.sleep(0.5)    pin.value(0)    time.sleep(0.5)This code does the same as the C++ code mentioned above.It however is far more readable.The MicroPython documentation has examples and information on what is available and what is not.Please be reminded that MicroPython support for the Pi Pico is still very new and some APIs may contain bugs, not work at all or even not be available yet.CircuitPython developmentCircuitPython is a variation on MicroPython created by Adafruit industries.It is targeted at beginners and students, and is even simpler than the regular MicroPython.Preparing the boardFirst of all we need to prepare the Pico to accept and run CircuitPython files:  Download the latest (stable) version of CircuitPython for the pico, this is a UF2 file and can be found here  Plug in your Pico while holding the BOOTSEL button  Drag the downloaded UF2 file onto the mass storage device that has just appeared in your file explorer  The board will reboot by itself and ‘install’ the UF2 fileDevelopment setupCircuitPython can also be developed using Thonny, you however need to open the Thonny Preferences once more, select the Interpreter tab and select the CircuitPython (generic) option.Using PyCharm for CircuitPython can be done.However, I’ve not had much success to flash the files to the Pico device.So for CircuitPython the best option remains Thonny.Adafruit recommends using the Mu IDE for CircuitPython development.However, I could never get it to work on my Big Sur installation.Developing CircuitPython does not really differ from MicroPython except for two things:  Language features and API  Project structure          No main.py but a boot.py and code.py file to do bootstrapping and have a main entry point for code execution      Further in-depth information on how to get started with CircuitPython development can be found on their excellent website as we will be focussing on MicroPython in future projects.Code exampleThe basic code for a LED blink example is listed below.import boardimport digitalioimport timeled = digitalio.DigitalInOut(board.GP25)led.direction = digitalio.Direction.OUTPUTwhile True:    led.value = True    time.sleep(0.5)    led.value = False    time.sleep(0.5)Again this code does the same as the C++ code mentioned above.It is also more readable, but differs slightly from the MicroPython flavour of Python.The CircuitPython documentation has examples and information on what is available and what is not.Please be reminded that CircuitPython support for the Pi Pico is still very new and some APIs may contain bugs, not work at all or even not be available yet.Another thing to note is that the CircuitPython implementation has some additional pros and cons:  It does not support _thread, so you can only use one of the cores of the Pi Pico  It supports a lot more devices, if there is an Adafruit peripheral/device/sensor, chances are big they have a library/driver available for itConclusionThe Pi Pico is a nice little microcontroller that has a lot of potential.It isn’t particularly outstanding in features but it’s cheap and is backed by the Raspberry Pi foundation.Being able to use MicroPython, or CircuitPython, is like a breath of fresh air.It allows for faster prototyping without having to deal with C/C++ per se.Resources  Pi Pico Getting started  General C/C++ SDK documentation  C++ setup Linux  C++ setup Mac  C++ setup Windows  Getting started with MicroPython  Thonny IDE  PyCharm  MicroPython documentation  CircuitPython download  Mu IDE  Getting started with CircuitPython  CircuitPython documentation"
      },
    
      "ehealth-2021-02-23-hapi-fhir-html": {
        "title": "Introduction to HAPI FHIR",
        "url": "/ehealth/2021/02/23/hapi-fhir.html",
        "image": "/img/2021-01-23-hapi-fhir/hapi-logo.png",
        "date": "23 Feb 2021",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  HAPI Servers Types          Plain server      JPA Server      JAX-RS Server        Custom Plain server implementation          Modules      Server      Resources      Security                  Cross-Origin Resource Sharing (CORS)          AuthorizationInterceptor                      ConclusionIntroductionThe Fast Healthcare Interoperability Resource, commonly known as FHIR, has quickly become one of the most popular protocols for joining disparate systems together, and holds great promise for the development of an application-based approach to interoperability and health information exchange.If you haven’t heard of FHIR, I recommend you read the Healthcare on FHIR blogpost from my colleague Martin Kwee.In this article we’ll focus on implementing the FHIR specification through the open source Java libraries called HAPI-FHIR.HAPI FHIR is a complete implementation of the HL7 FHIR standard for healthcare interoperability in Java.HAPI Servers TypesHAPI FHIR provides several mechanisms for building FHIR servers. The appropriate choice depends on the specifics of what you are trying to accomplish.Plain serverThe HAPI FHIR Plain Server (often referred to as a Facade) is an implementation of a FHIR server against an arbitrary backend that you provide.In this mode, you write code that handles resource storage and retrieval logic, and HAPI FHIR takes care of:      HTTP Processing        Parsing / Serialization        FHIR REST semantics  This module was originally created at University Health Network (UHN) as a mechanism for placing a common FHIR layer on top of a series of existing data sources, including an electronic medical record system (EMR), an enterprise patient scheduling system, and a series of clinical data repositories. All of these systems existed long before FHIR was adopted at UHN and HAPI FHIR was created to make the process of adopting FHIR easier.JPA ServerThe HAPI FHIR JPA Server is a complete implementation of a FHIR server against a relational database. Unlike the Plain Server, the JPA server provides its own database schema and handles all storage and retrieval logic without any coding being required.JAX-RS ServerThe HAPI FHIR Plain Server (RestfulServer) is implemented as a standard JEE Servlet, meaning that it can be deployed in any compliant JEE web container.The JAX-RS module is a community-supported module that was not developed by the core HAPI FHIR team. Before deciding to use the HAPI FHIR JAX-RS module, please be aware that it does not have as complete of support for the full FHIR REST specification as the Plain Server.For users in an environment where existing JAX-RS services have been created, it is often desirable to use JAX-RS for FHIR servers as well. HAPI FHIR provides a JAX-RS FHIR server implementation for this purpose.Custom Plain server implementationAs you have read above, there are different server types for different needs.Since the JPA implementation is a fully out-of-the-box working implementation with a SQL database, I thought it would be a nice challenge to set up my own Plain server implementation using a NoSQL database.The remainder of this blogpost will cover some practical code snippets of how I’ve set up a FHIR RESTful server using Spring Boot and a MongoDB database.Below a comparison between our custom developed Plain server and an out-of-the-box JPA server:Out-of-the-box JPA server:Compared to the JPA server that is provided out-of-the-box, there are some minor differences but by and large they are similar. The differences are the database technologies and mapping technologies.ModulesFirst of all, we will create a simple Spring Boot project and load the HAPI library using Maven.While there are many subprojects in the FHIR Codebase we only need 3 for the FHIR Façade or as the HAPI FHIR project lead James Agnew puts it, for HAPI Plain Server implementation.  Hapi-fhir-base          This is the core HAPI FHIR library and is always required in order to use the framework. It contains the context,     parsers, and other support classes.        Hapi-fhir-structures          This module contains the StructureDefinitions, ValueSets, CodeSystems, Schemas, and Schematrons for a specific FHIR version.        Hapi-fhir-server          This module contains the HAPI FHIR Server framework, which can be used to develop FHIR compliant servers against your own data storage layer.      pom.xml&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;ca.uhn.hapi.fhir&lt;/groupId&gt;        &lt;artifactId&gt;hapi-fhir-structures-r4&lt;/artifactId&gt;        &lt;version&gt;5.2.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;ca.uhn.hapi.fhir&lt;/groupId&gt;        &lt;artifactId&gt;hapi-fhir-base&lt;/artifactId&gt;        &lt;version&gt;5.2.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;ca.uhn.hapi.fhir&lt;/groupId&gt;        &lt;artifactId&gt;hapi-fhir-server&lt;/artifactId&gt;        &lt;version&gt;5.2.0&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;ServerThe class in the code snippet below is going to be the heart of your custom HAPI FHIR implementation. We will be using Release 4 (R4) of the FHIR specification.Here you will configure most of your HAPI server settings. You can add security, select which resources your server will support, set defaults on the server to use XML or JSON and many more options. We will implement some more later in this post.FhirRestfulServer.javapackage com.example.fhirexample;import ca.uhn.fhir.context.FhirContext;import ca.uhn.fhir.rest.server.RestfulServer;import com.example.fhirexample.providors.ObservationProvidor;import com.example.fhirexample.providors.PatientProvider;import org.springframework.context.ApplicationContext;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import java.util.Arrays;@WebServlet(\"/*\")public class FhirRestfulServer extends RestfulServer {    private ApplicationContext applicationContext;    FhirRestfulServer(ApplicationContext context) {        this.applicationContext = context;    }    @Override    protected void initialize() throws ServletException{        super.initialize();        setFhirContext(FhirContext.forR4());    }}Now lets setup the Servlet Context. We’re using the standard setup of a Spring Boot application and register the FhirRestfulServer web servlet we’ve created earlier.package com.example.fhirexample;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.web.servlet.ServletRegistrationBean;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.Bean;@SpringBootApplicationpublic class FhirExampleApplication{    @Autowired    private ApplicationContext context;    public static void main(String[] args) {        SpringApplication.run(FhirExampleApplication.class, args);    }    @Bean    public ServletRegistrationBean ServletRegistrationBean() {        ServletRegistrationBean registration= new ServletRegistrationBean(new FhirRestfulServer(context),\"/*\");        registration.setName(\"FhirServlet\");        return registration;    }}Once your server has started, open up your Postman and GET the following URL: http://localhost:8080/metadata. A capability statement will appear just like the screenshot below.A Capability Statement documents a set of capabilities (behaviors) of a FHIR Server for a particular version of FHIR that may be used as a statement of actual server functionality or a statement of required or desired server implementation. Congratulations you are now FHIR enabled!ResourcesThe PatientProvider is where the FHIR Patient behaviour is configured. HAPI uses annotations to indicate what kind of REST service a class method provides. The procedure “createPatient” in the diagram is annotated with @Create which indicates that it handles POST/create. This procedure then uses a PatientDAO (DAO - data access object) class which Spring Data uses to persist the Patient resource in MongoDB.package com.example.fhirexample;import ca.uhn.fhir.context.FhirContext;import ca.uhn.fhir.rest.annotation.*;import ca.uhn.fhir.rest.api.MethodOutcome;import ca.uhn.fhir.rest.param.DateRangeParam;import ca.uhn.fhir.rest.param.StringParam;import ca.uhn.fhir.rest.param.TokenParam;import ca.uhn.fhir.rest.server.IResourceProvider;import com.example.fhirexample.dao.patient.PatientDAO;import org.hl7.fhir.instance.model.api.IBaseResource;import org.hl7.fhir.r4.model.IdType;import org.hl7.fhir.r4.model.OperationOutcome;import org.hl7.fhir.r4.model.Patient;import org.hl7.fhir.r4.model.Resource;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import javax.servlet.http.HttpServletRequest;import java.util.List;@Componentpublic class PatientProvider implements IResourceProvider {    @Autowired    private FhirContext ctx;    @Autowired    private PatientDAO patientDao;        @Override    public Class&lt;? extends IBaseResource&gt; getResourceType() {        return  Patient.class;    }    @Search    public List&lt;Resource&gt; searchPatient(HttpServletRequest request,                                        @OptionalParam(name= Patient.SP_BIRTHDATE) DateRangeParam birthDate,                                        @OptionalParam(name = Patient.SP_FAMILY) StringParam familyName,                                        @OptionalParam(name= Patient.SP_GENDER) StringParam gender,                                           @OptionalParam(name= Patient.SP_GIVEN) StringParam givenName,                                        @OptionalParam(name = Patient.SP_IDENTIFIER) TokenParam identifier,                                        @OptionalParam(name= Patient.SP_NAME) StringParam name,                                        @OptionalParam(name = Patient.SP_RES_ID) TokenParam resid) {        return patientDao.search(ctx, birthDate, familyName, gender, givenName, identifier, name);    }    @Read()    public Patient read(@IdParam IdType theId) {        return patientDao.read(ctx,theId);    }    @Create()    public MethodOutcome createPatient(HttpServletRequest theRequest, @ResourceParam Patient patient) {        MethodOutcome  method = new  MethodOutcome();        method.setCreated(true);        OperationOutcome  opOutcome = new  OperationOutcome();        method.setOperationOutcome(opOutcome)               return patientDao.create(ctx, patient);    }    @Search()    public List&lt;Resource&gt; getAllPatients() {        return patientDao.search(ctx);        }      @Delete()    public void delete(@IdParam  IdType  theId) {                patientDao.delete(ctx,theId);    }}In the previous code snippet I want to highlight some important details like:  The MethodOutcome object must be returned on update and create methods. This object contains the identity of the created resource. On a delete and validate method you have a choice between void and MethodOutcome.  Operation outcomes are sets of error, warning and information messages that provide detailed information about the outcome of an attempted system operation.   The operationOutcome can be used as a direct response from the server (or as a component of the response). For example when the method fails the operationOutcome can provide more information about the outcome. This can be used to provide meaningful error messages.Now we configure our HAPI Server to support the Patient resource.FhirRestfulServer.javapackage com.example.fhirexample;import ca.uhn.fhir.context.FhirContext;import ca.uhn.fhir.rest.server.RestfulServer;import ca.uhn.fhir.rest.server.interceptor.CorsInterceptor;import com.example.fhirexample.providors.ObservationProvidor;import com.example.fhirexample.providors.PatientProvidor;import org.springframework.context.ApplicationContext;import org.springframework.web.cors.CorsConfiguration;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import java.util.Arrays;@WebServlet(\"/*\")public class FhirRestfulServer extends RestfulServer {        private ApplicationContext applicationContext;    FhirRestfulServer(ApplicationContext context) {        this.applicationContext = context;    }    @Override    protected void initialize() throws ServletException {           super.initialize();        setFhirContext(FhirContext.forR4());        setResourceProviders(Arrays.asList(            applicationContext.getBean(PatientProvider.class)));    }}In this section we have shown you how to implement a Patient resource on a HAPI FHIR server.SecuritySecurity is a crucial part of setting up a server especially when it comes to sensitive data like health information.To easily facilitate this security concern, there are some out-of-the box features that HAPI provides to ensure the safeguarding of sensitive patient dataThe following code snippets comes from the HAPI FHIR documentation.Cross-Origin Resource Sharing (CORS)The HAPI FHIR server framework includes an interceptor that can be used to provide CORS functionality on your server. This mechanism relies purely on Java configuration. HAPI’s interceptor is a thin wrapper around Spring Framework’s CorsProcessor class, so it requires Spring to be present on your classpath.package com.example.fhirexample;import ca.uhn.fhir.rest.server.interceptor.CorsInterceptor;import org.springframework.web.cors.CorsConfiguration;@WebServlet(urlPatterns = {\"/fhir/*\"}, displayName = \"FHIR Server\")public class RestfulServerWithCors extends RestfulServer {    @Override    protected  void  initialize() throws  ServletException {    // ... define your resource providers here ...    // Define your CORS configuration. This is an example    // showing a typical setup. You should customize this    // to your specific needs    CorsConfiguration  config = new  CorsConfiguration();    config.addAllowedHeader(\"x-fhir-starter\");    config.addAllowedHeader(\"Origin\");    config.addAllowedHeader(\"Accept\");    config.addAllowedHeader(\"X-Requested-With\");    config.addAllowedHeader(\"Content-Type\");    config.addAllowedOrigin(\"*\");    config.addExposedHeader(\"Location\");    config.addExposedHeader(\"Content-Location\");    config.setAllowedMethods(Arrays.asList(\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\", \"PATCH\"));    // Create the interceptor and register it    CorsInterceptor  interceptor = new  CorsInterceptor(config);    registerInterceptor(interceptor);    }}AuthorizationInterceptorHAPI FHIR provides you with an AuthorizationInterceptor which can be helpful to determine whether a user has the appropriate permission to perform a given task on a FHIR server. This is done by declaring a set of rules that can selectively allow (whitelist) and/or selectively block (blacklist) requests. The interceptor works by allowing you to declare permission based on an individual request coming in.The AuthorizationInterceptor is used by subclassing it and then registering your subclass with the RestfulServer. The following example shows a subclassed interceptor implementing some basic rules:package com.example.fhirexample;import ca.uhn.fhir.rest.api.server.RequestDetails;import ca.uhn.fhir.rest.server.exceptions.AuthenticationException;import ca.uhn.fhir.rest.server.interceptor.auth.AuthorizationInterceptor;import ca.uhn.fhir.rest.server.interceptor.auth.IAuthRule;import ca.uhn.fhir.rest.server.interceptor.auth.RuleBuilder;import org.hl7.fhir.r4.model.IdType;import java.util.List;public class PatientAndAdminAuthorizationInterceptor extends AuthorizationInterceptor {    @Override    public List&lt;IAuthRule&gt; buildRuleList(RequestDetails theRequestDetails) {        // Process authorization header - The following is a fake        // implementation. Obviously we'd want something more real        // for a production scenario.        //        // In this basic example we have two hardcoded bearer tokens,        // one which is for a user that has access to one patient, and        // another that has full access.        IdType userIdPatientId = null;        String authHeader = theRequestDetails.getHeader(\"Authorization\");        if (isNormalUser(authHeader)) {            // This user has only access to the Patient resource with id 1.            // If the user is a specific patient, we create the following rule chain:            // Allow the user to read anything in their own patient compartment            // Allow the user to write anything in their own patient compartment            // If a client request doesn't pass either of the above, deny it            userIdPatientId = new IdType(\"Patient\", 1L);            return new RuleBuilder()                    .allow().read().allResources().inCompartment(\"Patient\", userIdPatientId).andThen()                    .allow().write().allResources().inCompartment(\"Patient\", userIdPatientId).andThen()                    .denyAll()                    .build();        } else if (isAdmin(authHeader)) {            // If the user is an admin, allow everything            return new RuleBuilder()                    .allowAll()                    .build();        } else {            // Throw an HTTP 401            throw new AuthenticationException(\"Missing or invalid Authorization header value\");        }    }    private boolean isNormalUser(String authHeader){        return \"Bearer dfw98h38r\".equals(authHeader);    }    private boolean isAdmin(String authHeader){        return \"Bearer 39ff939jgg\".equals(authHeader);    }}After the server was set up and ready to use,I’ve set up a small Angular front-end application where it was possible to retrieve all the Patients and add a Patient.Finally Observations were added which could be linked to a specific Patient.In order not to define all the Javascript interfaces myself I used the following module @ahryman40k/ts-fhir-types.ConclusionFor several years, the FHIR specification has been providing enhancements at various levelsfrom application development to inter-application integrationWe can conclude that setting up a FHIR server in Java is made easy by using the HAPI libraries.They provide different server types for different needs.There are servers that work out-of-the box but there are also possibilities to link existing databases to a HAPI FHIR façade. In addition, the security libraries provided by HAPI make the development process a lot easier and secure."
      },
    
      "architecture-2021-02-17-out-with-the-old-html": {
        "title": "Out With The Old, In With The New",
        "url": "/architecture/2021/02/17/Out-With-The-Old.html",
        "image": "/img/2021-02-17-Out-With-The-Old/logansrun.png",
        "date": "17 Feb 2021",
        "category": "post, blog post, blog",
        "content": "Almost fifty years ago, the movie “Logan’s Run” introduced the phrase “Out with the old, in with the new” into pop culture vernacular. The story depicted a dystopian world where all needs were met by advanced technology, but due to scarcity of resources the population was kept under control by forced termination of the citizens of this society at the age of 30. If we were to flip this around on technology, we could conclude that in order to keep our business landscape manageable we need to address the lifespan of the assets in this landscape.In every organization there is only a finite amount of budget and capacity for changing the way things are done. Especially in the IT domain, the balancing efforts between the operational resources (keeping the lights on) and those resources that can be allocated to new developments is a delicate exercise that the departments responsible for these expenditures have to go through on a recurring basis. Together with other parameters they determine the nature of the initiatives that happen by and for these departments.Oftentimes we see that the operational budget far outweighs the budget that can be allocated to these new developments. This is normal as the need for business continuity reigns supreme in most organizations. This is why it is important to keep technical debt under control. In a past thought I pointed out the dangers of technical debt that could lead to disruption by neglect. But within the space of the new developments budget there dwells an even more esoteric beast. This beast is called innovation. All too often, Peter Drucker’s mantra “Innovate or Die” resounds in the hallowed halls of upper management, but to see it in practice is a different matter.It is the job of the architect tasked with innovation to determine a framework consisting of processes and tools to support the various stakeholders (either individuals and/or teams) that are working within this context. Depending on the level at which the architect can steer innovation initiatives, any number of mechanisms can be devised. The framework you create should be aligned with the particularities of those stakeholders, and as such an understanding of their motivations and skillsets forms its foundation.As with any major initiative, one of the success factors is management sponsorship. In order to properly tackle innovation from a leadership perspective, the manager must first determine the type of leader best suits the needs. The primary focus of a manager concerned with innovation is to weigh the freedom to pursue innovation (or fostering the creativity of talent) versus establishing the internal controls needed to keep the innovation process grounded and avoid chaos. Taking note of the research done in 2019 by Deborah Ancona and Kate Isaacs, both researchers at the MIT Sloan School of Management, most innovative or “nimble” leaders can be categorized as one of these categories:  Entrepreneurial Leaders: Usually located in lower levels of management. They are in charge of creating new products and/or services and as a collective they can influence the direction the organization is taking and as such also take the organization into new areas. The innovation bubbles up from bottom to top. A famous example of this type of leader is Steve Jobs at Apple when he was in charge of the new designs.  Enabling Leaders: Mostly occupying the middle tier of management, these leaders enable innovation by facilitating the entrepreneurial leaders in their organization. This can be done through ensuring budget, resources, and information or meeting other demands needed for the entrepreneurial leader to effect the innovation being pursued. They also tend to take on the role of mentor to make sure employees grow individually and that departments can navigate possible hardships on the horizon. Think of Patrick Lencioni, writer of the “Five Dysfunctions of a Team”.  Architecting Leaders: Usually found in the upper echelons of management, they have a helicopter view of the ecosystem their organization operates in, and a vision (big picture) on how to proceed in such a context. These leaders possess a keen insight into the various innovation initiatives per domain and start to play a role when this innovation necessitates changes on an organizational level (culture, strategy, structure). As such they create a set of blueprints allowing the other leaders to achieve their individual goals, and unifying these initiatives into what the organization and its customers need. A well-known example of this type of leader would be Jeff Bezos at Amazon.These types bear more than a passing resemblance to the types of administrators identified by Robert Katz as he set out his Three-Skill Approach. Where the innovation types focus more on who the leaders are, Katz focuses more on dividing managerial types based on what skills they possess: technical skills (an understanding of certain activities, processes, and/or techniques), human skills (the ability to work with and motivate team members and other human assets), and conceptual skills (forming a vision of the organization as a whole and where it needs to go).Managers do not operate in a vacuum. There is always the as-is organizational structure that forms boundaries of how far innovation can be taken, and innovation initiatives should align themselves to the corporate strategy. Where the architecting leaders set the blueprints, these constraints are distilled from the type of organization the management is a part of. A useful tool for determining how the organization should characterize its initiatives, and how it goes about achieving them is the Innovation Matrix from Board of Innovators. The innovation types proposed by the matrix are characterized by the willingness to invest and the focus for the innovation influx (either from external or internal sources):  Hunters: Organizations that are willing to invest heavily in innovation, but seek to import innovative ideas into their existing portfolio though cooperation with startups, acquisitions, corporate ventures… This is the innovation approach of Google. Techniques to go about this are venture funds, structural partnerships, external accelerators, and co-development tracks.  Builders: Organizations that are willing to invest heavily in innovation, but put their stock in their own capabilities to achieve this innovation, with significant resources (both budget and people) being poured into transforming their organization and even going so far as to create dedicated departments (also called centers of excellence and/or innovation labs). Famous amongst these types of innovators is Apple.  Explorers: Explorer organizations are the low(er) investment counterpart of the hunters. Innovation experimentation in the form of hackatons or casual preliminary contacts with startups give an idea to the organization if and where they want to put their investments. These organizations recognize the need for innovation, but are not willing to go all-in. Orange has been adopting this style of innovation.  Experimenters: These organizations try to organize their innovation around their own resources, but try to limit the effort they expend for these initiatives. They tend to focus on low-cost techniques such as innovation training and design sprints in order to mature their internal resources and build a solid innovation capability and mindset. Spotify and Netflix are the success stories in this type of innovation.Considerations for the Hunter TypeDeciding which merger or acquisition to go for might seem as arbitrary as betting on which innovation horse will win the race, but there are consequences to this approach. Clayton Christensen, father of the theory of disruptive innovation, pointed out that there are several valid reasons for joining two organizations (either by takeover or merger): either to improve current operations by boosting performance or reducing operational cost, and/or dramatically transform the organization’s growth prospects by reinventing the business model, and/or optimizing/diversifying the business portfolio.In any case, the business models of the two organizations will need to be integrated on a number of facets:  Customer Value Proposition: The services/products that customers value above the alternatives within the organization’s market segments.  Profit Formula: The revenue model and cost structure in place to ensure the organization is able to deliver the customer value proposition and generate the revenues needed to sustain the organization.  Organizational Resources: Any resources (employees, customers, products/services, infrastructure, cash flow…) that the organizations have to deliver the customer value proposition.  Organizational Processes: Some domains (manufacturing, R&amp;D, budgeting, sales…) will be easier to match than others depending how much the views of the two organizations in these domains differ.This integration can happen in two distinct ways. The first way is called the Leverage-my-Business-Model acquisition (LBM). This is rather straightforward with all resources extracted from one of the organizations, and infused on the other one. This incurs the least risk and thus the least potential for returns. The current infrastructure of the master organization needs to be extended with the new components of the disappearing organization and it needs to scale its existing components to support the additional load. The second way is called the Reinvent-my-Business-Model (RBM). The business model of the acquired organization is used to revamp the existing business model of the master organization and create a fusion of business models and infrastructure. This is a best-of-breed solution where each individual component of both organizations is considered for optimal use.A consequence of having a steady stream of innovations assimilated into your organizational structure is the need for ways to quickly and, as seamless as possible, integrate the new components and value chains into your existing setup. On a solution architecture level this means providing the tools to achieve this with the least amount of hassle. This leads us to state that integration capabilities should receive fair amount of attention. Examples of such integration capabilities can be API Management, Enterprise Service Buses (ESB) or the Hybrid Integration Platform (HIP) that Gartner places as one of the necessary tools for any digital transformation.Considerations for the Builder TypeHaving to foster innovation from within can be a trickier than to just scrutinize the market and acquire what seems to be working. It requires either making sure that there are products/services in your organization’s portfolio that can be improved upon (sustaining technologies) or detecting disruptive technologies to create a completely new market to capitalize on. These disruptive technologies come with the burden of lower profit margins but higher profit potential as the market matures. To clarify: The term “disruptive technologies” is used here as sort of an umbrella term. In the same reference article where I tackled technical debt I also discuss other numerous forms of disruption, each with their causes and effects on how the bottom line for an organization is achieved.One benefit over Hunter types is that if a working innovation (one that catches on in the market) is discovered within the organization, that organization has a first mover advantage, allowing the organization to establish brand recognition and customer loyalty before competitors are able to reproduce or approximate the innovative endeavor. It allows for setting yourself up as the benchmark and industry standard against which customers will judge copycat products and services.Since disruptive technologies come in at a lower profit margin, partly due to higher costs associated with them, it is only natural to attempt to curb these costs by matching your infrastructure to the needs of the innovations. This requires digital transformation initiatives in order to support them. According to Oracle there are certain indispensable requirements for achieving success in such transformations:  An executive mandate to implement transformations. The Architecting Leaders of the organization should be on board, and even sponsor and propel these transformations forward. The sponsorship and support could be handled by Enabling Leaders if they are backed by the Architecting leader.  The focus of the transformation initiative should be fixed on the final result, and should not deviate from it course once begun.  There needs to exist a sense of urgency to get the transformation done. Time is essential, and the longer the initiative lasts, the likelier it is that it will become irrelevant and superseded by another similar initiative.  The initiative should not only care about modernizing customer touchpoints and enabling infrastructure, but also about the impact the transformation will have on the human factors of the equation. The change management aspect of the initiative dealing with employee experience and corporate culture is an important slice of the overall initiative.Having decided that innovation comes from within means that the innovative teams need to have the proper tooling to support their efforts. Wanting to design new ideas and testing them out requires them to have a lot of sandbox capabilities at their disposal where they can quickly launch a Proof of Concept (POC) and ascertain whether or not to continue in the same vein or drop it to pursue other ideas. The faster the turnover of new ideas, the quicker your innovations can be launched in the market and start paying off. A multitude of environments (typically in the form of a Cloud offering such as AWS or Azure) and the optimized CI/CD pipelines to get them up to date with your newest POCs makes this possible. Getting these POCs mature enough can be achieved through such approaches as for example the Design Thinking Process.Considerations for the Experimenter TypeWhile not wanting to expend quite as much liquidity on innovation as the Hunter and Builder types, there are still some avenues that beckon for the Experimenter type. Hunters and Builders push heavily on innovation in order to attain the patronage of the high-end market. When your product excels over all other similar products in the market, you can sell it at a higher markup as the customers in this segment have heavier purses. A higher markup equals higher profits. This does mean that there are two entire segments for whom the increased product excellence will not warrant the price they have to pay for it. Not everyone drives a Maserati. Some drive Audi, and others go for cars in an even lower price bracket.A valid strategy for Experimenter innovation is to aim for a just-enough innovation to keep up with the product leaders, but aiming for that mainstream market where profit margins are lower. In return, your customer base is much larger to make up for this. It might seem that the same philosophy can be taken on the low-end market. If not for the fact that this drop in profit margin and resulting increase in potential customer base does come with a warning: The lower in cost and product excellence you go on the spectrum, the easier it becomes for new competitors to enter it, and take a piece of the pie. This is what is known in Michael Porter’s Five Forces Model as the Threat of New Entrants, and is a vital consideration in evaluating whether you are adopted a viable business strategy.Taken from EZY EducationThis approach also allows for the emergence of a sleeper hit. These are innovations that the organization wasn’t actively or explicitly pursuing, but kind of crept up next to the main business model. This sleeper hit might even overtake the original business model, and replace it as the main source of revenue. A good example of this is Slack. When its organization developed the technology to support the development of their main product, an online game called Glitch, it was soon discovered that there was a greater market for this tool than for the game they were developing, and they decided to switch gears to pursue this sleeper hit.Banking on innovation to emerge from within without structurally investing a lot of capital and resources, there needs to be fertile ground from which these innovations can spring. Since innovation seldom erupts from the thought and action of a single individual, the crux of the matter becomes getting like-minded people into contact with each other. Cross pollination between their views and ideas is what makes this innovation type possible. So the solutions that should be developed within such organizations are mostly collaborative and social engagement tools that allow for this cross pollination.Considerations for the Explorer TypeOrganizations of the Explorer type opt not to engage in the innovation business on their own. To hedge their bets and spread the risk and cost of the innovation they engage with similar organizations to get traction. This can range from having co-creation sessions to incubators to simply scouting the market for talent that can further their objectives. As there are numerous obstacles that can be encountered on the road to innovative products, these organizations accept the shared benefits to reduce the impact these obstacles also called barriers will have on them.The obstacles can be grouped into four overall categories:  Financial Barriers: R&amp;D can be a costly affair, and can be too much for your organization to bear alone. So to be able to divide costs of innovation, even if it results in a split profit, could be preferable over not pursuing innovation, and sticking with sustaining your products for the foreseeable future.  Unpredictable Success: Success of innovative products is hard to predict. This could complicate finding willing business sponsors or investors from outside the organization. The return on investment (ROI) has been a mainstay in building business cases, and it is almost impossible to calculate for innovation. Thus, spreading the cost and risks through sharing the effort can help in this department.  Missing Marketing Skills: Producing a viable innovative product is one thing. Being able to convince your customers that they need it is another. The marketing team that will help your product become the new need-to-have product should have the same innovative mindset as your innovation team. Seeking these marketing skills outside your organization might be just what the doctor ordered.  Management Barriers: Missing the innovation leaders within the organization can slow the progress made by the innovation initiatives. Looking for these innovation managers in an organization that is willing to cooperate can lessen the hurt with this barrier.The main takeaway of these barriers is that there is a correlation between risk and success in the innovation game. The increased risk of innovation initiatives reflects on what can be achieved in the areas of improving your organization’s market position and competitiveness. Getting a firm understanding of which barriers your organization faces mitigates some of the risks in this type of endeavor.Similar to the Experimenter type, these organizations depend heavily on emergent innovations. This necessitates the same focus on collaboration and social tooling, but with the added complication that this collaboration takes place across organizational boundaries. This adds an increased need for the proper security capabilities to make sure the interaction between participant organizations is limited to those organizations. It also means that the reputation of the organization in the fields of innovation becomes a currency that can buy the partnerships that are needed. This reputation is mostly constructed from the visibility of the human capital and talent your organization brings to the table, and can be positively influenced with getting your people out there, writing blog posts, giving key notes and networking as much as possible."
      },
    
      "architecture-2021-02-12-react-generic-context-html": {
        "title": "Making a generic Context in React",
        "url": "/architecture/2021/02/12/react-generic-context.html",
        "image": "/img/react-generic-context/banner.jpg",
        "date": "12 Feb 2021",
        "category": "post, blog post, blog",
        "content": "A backgroundI have been mostly working with Angular for the past 4 years.I am, however, a firm believer that no one framework or library is better than all others and I love a sporadic challenging dive into the unknown.Those who know a little bit about React will know that, like Angular, Vue and others, it’s a JavaScript library to create web applications by means of components.These components are considered building blocks and can consist themselves of smaller components.These components have properties, being passed by their parents and these properties can influence the rendered view and logic of the components.Those with a bit more React experience, will know that in a larger tree of components, it’s a nuisance to get a property from a component near the root of the tree to a component near the leafs of the tree by simple property binding.It also muddies intermediary components with irrelevant code.Fortunately React includes Context Docs.It provides a way to share value like preferences/themes and authentication data without having to explicitly set those values in each component’s properties.The developer basically creates a Context variable which is provided by a container component (e.g. App) using a Provider, and can be consumed by other components  using either a Consumer or a useContext hook.The ChallengeThe official React documentation mentions that Context is a great use for keeping data about the authenticated user, so it looked like this was the way to go.But we ran into a problem quite soon.We currently use an Nx Workspace with multiple applications.Some of these applications still use Basic Auth for authentication, while others use OpenID Connect.Of course sooner or later, the Basic Auth applications will be converted to use OIDC too.In the meantime, I wanted developers for these applications to be able to have the same structure, with the same basic components.Knowing how it goes in the world of IT, and also as a challenge to myself I did an effort to make the eventual switch from Basic to OIDC as easy as possible for our developers and created an authentication-core library which would house the AuthContext and expose a AuthProvider and AuthConsumer components and a useAuth hook.The libraries authentication-basic and authentication-oidc would have specific implementation of the service being used by the provider.This way, it’s also future-proof for a crazy decision that would have us switch to another authentication system.And this worked like a charm.The developer only had to create an instance of the required service, and pass it to the AuthProvidercomponent.// App.tsximport React from 'react';import { AuthProvider } from '@our-scope/authentication-core';import { OidcProps, createOidcAuthService } from '@our-scope/authentication-oidc';import { MyFirstComponent, MySecondComponent } from './Components';const props: OidcProps = { ... };const myAuthService = createOidcAuthService(props);const App = () =&gt; {  return (    &lt;AuthProvider authService={myAuthService}&gt;      &lt;MyFirstComponent /&gt;      &lt;MySecondComponent /&gt;    &lt;/AuthProvider&gt;  )}export default App;He could then use AuthConsumer to wrap around the components where there was need for the authenticated user data.Or use the useAuth hook to get the user data directly.// Components.tsximport React from 'react';import { AuthConsumer, useAuth } from '@our-scope/authentication-core';export const MyFirstComponent = () =&gt; {  return (    &lt;AuthConsumer&gt;    {(authData) =&gt; (      &lt;div&gt;{authData.username}&lt;/div&gt;    )}    &lt;/AuthConsumer&gt;  )}export const MySecondComponent = () =&gt; {  const { authData } = useAuth();  return (    &lt;div&gt;{authData.displayName}&lt;/div&gt;  )}However, we noticed that the AuthConsumer and useAuth hook couldn’t infer the type of the user data.Not all developers like TypeScript, but I love having it around, especially when creating libraries that are meant to be used by other developers.No type on the authentication data could lead to bugs when switching implementations.So back to the drawing board.I did several attempts at making the functional component AuthProvider generic, but I was unable to get the AuthContext generic.Mostly because the AuthConsumer and the AuthProvider  exposed by the core library had to point to a specific context object, so a function creating this context didn’t make sense.My knowledge of React is still limited and I decided to call it a day and submit the merge request for review.As I feared, type safety was a concern and had to be taken care of.Luckily we have a team with members with a lot more React experience and they guided me to a solution.The SolutionWhen looking back, the solution is fairly simple, but elegant.Instead of letting the core library expose actual components, we let it expose a factory function.This factory function would create a context and in its turn call factory functions of each component and pass the context to it.Finally the function would just return the created components.// authentication-core/context/AuthContext.tsximport React from 'react';import { AuthContextProps } from '../domain/auth-context';interface AuthContextProps&lt;T = unknown&gt; {  signIn: (args?: unknown) =&gt; Promise&lt;void&gt;;  signOut: (args?: unknown) =&gt; Promise&lt;void&gt;;  userData?: T | null;  isExpired?: boolean;  status: 'loading' | 'idle';}export function createAuthContext&lt;T = unknown&gt;() {  return React.createContext&lt;AuthContextProps&lt;T&gt; | null&gt;(null);}// authentication-core/factory.tsimport { createAuthConsumer } from './components/AuthConsumer';import { createProtectedRoute } from './components/ProtectedRoute';import { createWithAuth } from './components/withAuth';import { createAuthContext } from './context/AuthContext';import { createUseAuth } from './hooks/useAuth';import { createAuthProvider } from './providers/AuthProvider';import { AuthService } from './services/auth.service';export function createAuthentication&lt;T = unknown&gt;(authService: AuthService&lt;T&gt;) {  const AuthContext = createAuthContext&lt;T&gt;(); // We type the Context based on the generic AuthService  const useAuth = createUseAuth(AuthContext);  const AuthProvider = createAuthProvider(AuthContext, authService);  const AuthConsumer = createAuthConsumer(AuthContext);  return {    AuthContext,    AuthProvider,    AuthConsumer,    useAuth,  };}The implementation libraries had to be adapted as well.They too expose a factory function, calling the core factory function with some pre-setup data and their respective service.// authentication-oidc/factory.tsimport { createAuthentication } from '@our-scope/authentication-core';import { OidcProps } from './domain/oidc-props';import { OidcAuthService } from './services/oidc-auth.service';export function createOidcAuthentication(oidcProps: OidcProps) {  const oidcService = new OidcAuthService(oidcProps);  return createAuthentication(oidcService);}Due to TypeScript’s type inference, the resulting Authentication and its components will have the same type as OidcAuthService’s implementation of AuthService.Now the implementation for applications can use the factored and typed components instead of static components and the eventual switch from one implementation to another is as easy as ever.The only changes which might have to be made to components, are for those components that actually consume an implementation-specific part of the user data.// auth.ts// This is the only file to change when switching auth providersimport { createOidcAuthentication, OidcProps } from '@our-scope/authentication-oidc';const config: OidcProps = { ... };const { AuthProvider, AuthConsumer, useAuth } = createOidcAuthentication(config);// const { AuthProvider, AuthConsumer, useAuth } = createBasicAuthentication();export { AuthProvider, AuthConsumer, useAuth };// App.tsximport React from 'react';import { AuthProvider } from './auth';import { MyFirstComponent, MySecondComponent } from './Components';const App = () =&gt; {  return (    &lt;AuthProvider&gt;      &lt;MyFirstComponent /&gt;      &lt;MySecondComponent /&gt;    &lt;/AuthProvider&gt;  )}export default App;// Components.tsximport React from 'react';import { AuthConsumer, useAuth } from './auth';export const MyFirstComponent = () =&gt; {  return (    &lt;AuthConsumer&gt;    {(authData) =&gt; ( // Now authData's type is inferred      &lt;div&gt;{authData.username}&lt;/div&gt;    )}    &lt;/AuthConsumer&gt;  )}export const MySecondComponent = () =&gt; {  const { authData } = useAuth(); // Now authData's type is inferred  return (    &lt;div&gt;{authData.displayName}&lt;/div&gt;  )}ConclusionIt’s great to not focus on one framework at the time, but this can confuse too.Because of my background in Angular, I was trying to solve this problem like I would with Angular’s dependency injection.Because components are just functions in React, they can be generic by themselves and typed using factory functions."
      },
    
      "iot-2021-02-10-home-automation-part-2-getting-started-with-home-assistant-html": {
        "title": "Home Automation part 2: Getting started with Home Assistant",
        "url": "/iot/2021/02/10/Home-Automation-part-2-Getting-Started-With-Home-Assistant.html",
        "image": "/img/2021-02-10-home-automation-part-2/banner.jpg",
        "date": "10 Feb 2021",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Preparations  Installing  Setting up and using Home Assistant  Automations  Development  Conclusion  ResourcesIntroduction                After the first general post about home automation it’s time to kick things into higher gear.In this post we’ll go into detail about Home Assistant.Home Assistant is a fully open source home automation platform.It has a web interface as well as apps for Android and iOS.It features over 1.700 integrations at the time of writing and allows for full local control of your smart home without using any of the big cloud vendors.As with many things, Home Assistant is relatively easy to pick up but hard to master.Read on down below for more the details.PreparationsBefore we get to installing Home Assistant it is handy to have some items at hand:  A machine/VM to run the assistant on, can be a Raspberry Pi (3b or 4b recommended)          If you are using a SBC, a decent micro SD card, or even better, an external ssd to install everything on at least 32 GiB in size        The correct image of the Home Assistant OS, download the file best suited for your device.The download page also features detailed information for each specific option          Raspberry Pi (and other board) images      Intel NUC image (or any other intel based board)      Virtual machine                  Virtual Box =&gt; VDI disk image          Hyper-V =&gt; VHDX disk image          KVM =&gt; QCOW2 disk image          VMware Workstation =&gt; VMDK disk image          Proxmox =&gt; QCOW2 disk image          VMware ESXi =&gt; OVA disk image                      An ethernet connection, WiFi can work too but ethernet might be more reliable depending on your personal WiFi setup  Patience, don’t be afraid to start over if things don’t work after the first try!InstallingOnce you have everything downloaded and prepared we can get to installing and doing the basic setup for Home Assistant.If you have downloaded the virtual machine hard disk image, load the downloaded image in the software by creating a new VM and assigning it the downloaded disk image.2GiB of RAM and 1 or 2 cores are usually more than sufficient, these values can always be increased later on if needed.Follow the instructions on the download page for you specific VM technology.If you have downloaded a device image, flash it onto the micro sd card, or even better an external SSD (Booting the Raspberry Pi from USB).An external SSD or decent USB 3.1 stick will last much longer than most SD cards, especially when writing a lot of log files.You can use a tool like Balena Etcher (multi OS) or Rufus (Windows) to flash the image.Once done, insert the SD card or USB device into your Raspberry Pi.Once you have set up your device or VM and powered it on, you should be able to visit it by entering http://homeassistant.local:8123 in the browser, this is where we will continue.Follow the setup guide, this will let you create an account (local, no cloud shenanigans) and set up the details about your home.You can set up integrations during this wizard but we will be doing this later.Click complete to exit the wizard.You will then be greeted with your home’s dashboard amd Home Assistant is ready to be used.                                                Setting up and using Home AssistantYour home has some types of objects in it, these are:  Integrations: These are the building blocks that integrate with physical devices or services.  Devices: Any physical device that is added through an integration has a representation here.  Entities: A device can expose one or multiple entities.  Automations: An automation is an action that is activated by a trigger and when an optional condition is met.  Blueprints: An automation instance that is re-usable.  Scenes: A collection of predefined states for one or more devices/entities.  Scripts: Is what it says it is, a sequence of commands and/or actions to execute.  Areas: A home can have multiple areas, think rooms.Each area can have zero or more devices/automations/scenes and or scripts assigned to it.One of the advantages of Home Assistant is that is has a vast library of available integrations.These integrations allow greater flexibility than most integrations that are made for Apple HomeKit, Amazon Alexa or Google Home.Most integrations are maintained by the open source community and development is quite active.In the previous section we installed the Home Assistant, now we are going to add some stuff to it.Open the dashboard again and in the lower right side menu click on the Configuration link.This will open the configuration view where we can manage the home.First select the Areas option.By default some rooms have been added, you can delete the existing ones if desired or add extra ones.It’s easiest to add all the rooms you want to have available in the Home Assistant web UI and app before we continue.                After we have set up the rooms, we want to pick the Integrations option.If you have your Home Assistant installation correctly configured, it probably will suggest some integrations based on the results of a network scan.Click on the Configure option to add the integration, a wizard will guide you through the process.For the devices in your home that have not been automatically detected, click the Add Integration button in the lower left corner and search for a device you have at home that might be supported.Some often used integrations are:  Sonos (speakers)  Tuya (lights, outlets, sensors)  Nest (thermostat, fire alarms)  Ikea Tradfri (lights)Like with the auto detected integrations, a wizard will guide you through the setup process.The Home Assistant website has a collection of all published integrations, you can browse through and search the list.Each plugin on the site also has all the info to configure it correctly.During the setup process you should be able to assign each device to a specific area.You can skip this if wanted and assign a device to an area later (By opening the device and clicking the settings icon and picking the area there).                                                                 There are integrations which do not provide a setup wizard.To configure these integrations, a more hands on approach is required:  First click on the Supervisor link in the left bottom sidebar.This will open a section.In there select the Add-on Store in the top menu.  In the search field, type File editor, select the one add-on (it’s an official one) and click install.This will install a file editor so we can edit the configuration.yaml file without needing the login on the Home Assistant server itself via SSH.Once the add-on has been installed, you can find it in the supervisor UI under the Dashboard section.  Click on the add-on and click on start, then select open web ui.This will open the file editor.  In the top control bar of the editor, click on the folder icon and select configuration.yaml.Home Assistant uses the YAML format for its configuration.This is a well known format for most developers and has a very basic indentation based syntax.A plugin which does not provide a wizard-based setup will probably describe the configuration that needs to be added manually in the documentation.For more detailed information about editing files and changing the configuration.yaml, see the online documentation of Home Assistant                If you go back to the dashboard, you should see all available devices that have been assigned to a room.You can manage the dashboard manually too but this requires quite a bit of config.Experiment with this as you see fit.You can now control any device that is exposed and visible on the dashboard.It might also be handy to create a number of scenes.A scene is a predefined set of states for one or multiple devices.For example a Good night and Good morning scene could be created to turn on/off lights, lower/raise any blinds or shutters and set any other device to a desired state.The scenes can then be used in automations to simplify the setup.To create a new scene:  Click the Configuration link in the left sidebar  Select the Scenes entry  Click on Add Scene  Give the scene a name and one or multiple devices with itYou set the state of each device when adding it as you want it to be when the scene is activatedHome Assistant has a very extensive documentation.Consult it or the community if you get stuck!AutomationsWhile having all the devices visible is nice, a smart home wouldn’t be a smart home if it didn’t involve some automation.We want to make life easier and having to spend less time controlling our smart devices is one good way to do so.As with most things that involve automation, it will take some initial time investment to get things right.Don’t give up if it doesn’t work from the first time!Home Assistant has an extensive automation framework that has multiple entities to its disposal for automating things.To create a basic automation:  Click the Configuration link in the left sidebar  Select the Automations entry  Click Add Automation and select Start with an empty automation  Give the automation a name and description that tells you what it will do  Set the execution mode, single is a good default.This is mostly used for longer running automations  Select a trigger type, for example when another device is controlled, an event is sent  If desired add a conditionThis will be evaluated after the trigger is fired and before any actions are executed  Select one or multiple actions or scenes to control or activate  Click the Save buttonIt is also possible to edit the automation using the yaml format.In the top right click on the three dots and select Edit as YAML. This will give you an editor to edit the automation in its raw yaml formatting.You can always test the automation by clicking the execute button.This will ignore the trigger but test any of the given conditions and execute the specified actions.                                                                                DevelopmentWhilst there are plenty of integrations available for Home Assistant, some might be missing, or you have a very specific use case that is not available yet.By supporting web hooks, scripts, MQTT,… you can sometimes work around devices and platforms that have no ready to use integration, but sometimes you cannot.It is possible by developing your own integrations for Home Assistant.A special website dedicated to developers is available to get started.To start building a custom integration:  Have experience with Python, YAML and JSON  Set up the Home Assistant dev environment  Read through the documentation first, to prevent any RTFM situations later on.Everything you need is laid out in separate topics to read through.  From the dev environment, execute: python3 -m script.scaffold integration.This will create a new basic integration  Look at the example integrations  Test your integration locally by adding an entry to your integrations __init__.py file to the configuration.yaml file: &lt;config_dir&gt;/custom_components/custom_integration/__init__.pyConclusionHome Assistant is a very extensive home automation platform that is relatively easy to set up.Basic automations are fast to set up and use, while also providing very extensive options for advanced users.The extensive documentation and lively community make sure that most questions can be answered.If you are willing to get your hands dirty, creating your own integrations is also an option.A valid alternative to the home automation systems from Google and Amazon whilst not giving them access to all your home data!Resources  Home Assistant  Downloads  Raspberry Pi USB Boot  Balena Etcher image burning tool  Rufus image burning tool  HA Available Integrations  HA Automations  HA Blueprints  HA Scripts  HA YAML Config  HA General Docs  HA Developer sections  HA Dev Environment  HA Custom Integration File Structure  HA Example Integrations Source Code"
      },
    
      "spring-2021-02-01-springcloudcircuitbreaker-html": {
        "title": "Spring Cloud Circuit Breaker",
        "url": "/spring/2021/02/01/springCloudCircuitBreaker.html",
        "image": "/img/2021-02-01-spring-cloud-circuit-breaker/CircuitBreaker.png",
        "date": "01 Feb 2021",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Types of implementation  Configuring Circuit Breakers with Resilience4j for non-reactive applications  Configuring Circuit Breakers with Resilience4J for reactive applications  Configuring Circuit Breakers with Spring Retry          Differences Resilience4j with Netflix Hystrix and Spring Retry      IntroductionWhen you detect that an application in your landscape is getting slow or starts failing, a circuit breaker can be used to stop all the communication to that application.It is basic function is to interrupt the current flow after a fault is detected and when the circuit breaker is reset (manually or automatically), it can resume its normal operation.You want to avoid that your end users are hitting high load times. That’s why you want to fail fast and have some fallback functionality.By making usage of the Circuit Breaker pattern you can let an application continue to operate when a related service fails, preventing the failure from cascading and giving the failing service time to recover.Types of implementationThe Spring Cloud Circuit Breaker project provides an abstraction API for adding circuit breakers to your application. There are three supported implementations:  Resilience4J  Resilience4J Reactive  Spring RetryConfiguring Circuit Breakers with Resilience4j for non-reactive applications  We set up a Spring Boot application that returns a list of ingredients for making soup.package hello;import reactor.core.publisher.Mono;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.bind.annotation.RequestMapping;@RestController@SpringBootApplicationpublic class CircuitBreakerSoupApplication {  @RequestMapping(value = \"/recommended\")  public Mono&lt;String&gt; ingredientsList(){    return Mono.just(\"Onions, Potatoes, Celery, Carrots\");  }  public static void main(String[] args) {    SpringApplication.run(CircuitBreakerSoupApplication.class, args);  }}We’re going to run this application locally alongside a client service application, so in src/main/resources/application.properties, set server.port so that the CircuitBreakerSoup application service won’t conflict with the client when we start up.ingredients/src/main/resources/application.propertiesserver.port=8090We now configure an Ingredients service application that will be our front-end to the CircuitBreakerSoup application. We’ll be able to view our list there at /basics, and that reading list will be retrieved from the CircuitBreakerIngredients application.package hello;import reactor.core.publisher.Mono;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.reactive.function.client.WebClient;@RestController@SpringBootApplicationpublic class IngredientsApplication {  @RequestMapping(\"/basics\")    public Mono&lt;String&gt; toCook() {      return WebClient.builder().build()                .get().uri(\"http://localhost:8090/recommended\").retrieve()                .bodyToMono(String.class);  }  public static void main(String[] args) {    SpringApplication.run(ReadingApplication.class, args);  }}We also add the server.port property to src/main/resources/application.properties:server.port=8080We now can access, in a browser, the /basics endpoint on our Ingredients application, and see our ingredients list. Yet, since we rely on the CircuitBreakerSoup application, if anything happens to it, or if Ingredients is simply unable to access CircuitBreakerSoup, we’ll have no list and our users will get a nasty HTTP 500 error message.We want to prevent getting this error. This can be done by using the Circuit breaker.Spring Cloud’s Circuit Breaker library provides an implementation of the Circuit Breaker pattern: when we wrap a method call in a circuit breaker, Spring Cloud Circuit Breaker watches for failing calls to that method, and if failures build up to a threshold,Spring Cloud Circuit Breaker opens the circuit so that subsequent calls automatically fail.While the circuit is open, Spring Cloud Circuit Breaker redirects calls to the method, and they’re passed on to our specified fallback method.You need to add the Spring Cloud Circuit Breaker Resilience4J dependency to your application. When using Maven:&lt;dependencies&gt;     &lt;dependency&gt;         &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;         &lt;artifactId&gt;spring-cloud-starter-circuitbreaker-resilience4j&lt;/artifactId&gt;         &lt;version&gt;0.0.1.BUILD-SNAPSHOT&lt;/version&gt;     &lt;/dependency&gt; &lt;/dependencies&gt; Spring Cloud Circuit Breaker provides an interface called Resilience4JCircuitBreakerFactory which we can use to create new circuit breakers for our application. An implementation of this interface will be auto-configured based on the starter that is on your application’s classpath. We will do this by creating a new service that uses this interface to make API calls to the CircuitBreakerSoup application.package hello;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import reactor.core.publisher.Mono;import org.springframework.cloud.client.circuitbreaker.ReactiveCircuitBreaker;import org.springframework.cloud.client.circuitbreaker.Resilience4JCircuitBreakerFactory;import org.springframework.stereotype.Service;import org.springframework.web.reactive.function.client.WebClient;@Servicepublic class IngredientsService {  private static final Logger LOG = LoggerFactory.getLogger(IngredientsService.class);  private final WebClient webClient;  private final ReactiveCircuitBreaker readingListCircuitBreaker;  public IngredientsService(Resilience4JCircuitBreakerFactory circuitBreakerFactory) {    this.webClient = WebClient.builder().baseUrl(\"http://localhost:8090\").build();    this.readingListCircuitBreaker = circuitBreakerFactory.create(\"recommended\");  }  public Mono&lt;String&gt; ingredientsList() {    return readingListCircuitBreaker.run(webClient.get().uri(\"/recommended\").retrieve().bodyToMono(String.class), throwable -&gt; {      LOG.warn(\"Error making request to ingredients service\", throwable);      return Mono.just(\"Onions\");    });  }}The Resilience4JCircuitBreakerFactory has a single method called create we can use to create new circuit breakers. Once we have our circuit breaker all we have to do is call run. Run takes a Mono or Flux and an optional Function. The optional Function parameter acts as our fallback if anything goes wrong. In our sample here the fallback will just return a Mono containing the “Onions”.With our new service in place, we can update the code in IngredientsApplication to use this new service.package hello;import reactor.core.publisher.Mono;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.reactive.function.client.WebClient;@RestController@SpringBootApplicationpublic class IngredientsApplication {  @Autowired  private IngredientsService ingredientsService;  @RequestMapping(\"/basics\")  public Mono&lt;String&gt; toCook() {    return ingredientsService.ingredientsList();  }  public static void main(String[] args) {    SpringApplication.run(ReadingApplication.class, args);  }}When we run both the Ingredients service and the Soup application, and then open a browser to the Ingredients service, at http://localhost:8080/basics. You should see the complete recommended ingredients list: “Onions, Potatoes, Celery, Carrots”.Now shut down the Soup application.Our list source is gone, but thanks to Resilience4J we have a reliable list to stand in.You should see: “Onions”.Configuring Circuit Breakers with Resilience4J for reactive applicationsYou need to add the Spring Cloud Circuit Breaker Reactor Resilience4J dependency to your application. When using maven:&lt;dependencies&gt;     &lt;dependency&gt;         &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;         &lt;artifactId&gt;spring-cloud-starter-circuitbreaker-reactor-resilience4j&lt;/artifactId&gt;         &lt;version&gt;0.0.1.BUILD-SNAPSHOT&lt;/version&gt;     &lt;/dependency&gt; &lt;/dependencies&gt; We can now use the same application as in the previous example.Spring Cloud Circuit Breaker provides an interface called ReactiveResilience4JCircuitBreakerFactory which we can use to create new circuit breakers for our application. An implementation of this interface will be auto-configured based on the starter that is on your application’s classpath. We will do this by creating a new service that uses this interface to make API calls to the CircuitBreakerSoup application.package hello;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import reactor.core.publisher.Mono;import org.springframework.cloud.client.circuitbreaker.ReactiveCircuitBreaker;import org.springframework.cloud.client.circuitbreaker.ReactiveResilience4JCircuitBreakerFactory;import org.springframework.stereotype.Service;import org.springframework.web.reactive.function.client.WebClient;@Servicepublic class IngredientsService {  private static final Logger LOG = LoggerFactory.getLogger(IngredientsService.class);  private final WebClient webClient;  private final ReactiveCircuitBreaker readingListCircuitBreaker;  public IngredientsService(ReactiveResilience4JCircuitBreakerFactory circuitBreakerFactory) {    this.webClient = WebClient.builder().baseUrl(\"http://localhost:8090\").build();    this.readingListCircuitBreaker = circuitBreakerFactory.create(\"recommended\");  }  public Mono&lt;String&gt; ingredientsList() {    return readingListCircuitBreaker.run(webClient.get().uri(\"/recommended\").retrieve().bodyToMono(String.class), throwable -&gt; {      LOG.warn(\"Error making request to ingredients service\", throwable);      return Mono.just(\"Onions\");    });  }}The ReactiveResilience4JCircuitBreakerFactory has a single method called create we can use to create new circuit breakers. Once we have our circuit breaker all we have to do is call run. Run takes a Mono or Flux and an optional Function. The optional Function parameter acts as our fallback if anything goes wrong. In our sample here the fallback will just return a Mono containing the “Onions”.With our new service in place, we can update the code in IngredientsApplication to use this new service.package hello;import reactor.core.publisher.Mono;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.reactive.function.client.WebClient;@RestController@SpringBootApplicationpublic class IngredientsApplication {  @Autowired  private IngredientsService ingredientsService;  @RequestMapping(\"/basics\")  public Mono&lt;String&gt; toCook() {    return ingredientsService.ingredientsList();  }  public static void main(String[] args) {    SpringApplication.run(ReadingApplication.class, args);  }}When we run both the Ingredients service and the Soup application, and then open a browser to the Ingredients service, at http://localhost:8080/basics. You should see the complete recommended ingredients list: “Onions, Potatoes, Celery, Carrots”.Now shut down the Soup application.Our list source is gone, but thanks to Resilience4J we have a reliable list to stand in.You should see: “Onions”.Configuring Circuit Breakers with Spring RetrySpring Retry depends on AspectJ which is not included in the skeleton project, so we will add below dependency in the pom.xml file.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt;    &lt;artifactId&gt;spring-retry&lt;/artifactId&gt;    &lt;version&gt;${version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework&lt;/groupId&gt;    &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt;    &lt;version&gt;${version}&lt;/version&gt;&lt;/dependency&gt;Create a Rest controller which will call the backend service class where we will simulate the exception and the Spring Retry module will automatically retry.In the REST Api we will add two optional request parameters.  simulateretry: Parameter to simulate the exception scenario, so that Spring can retry.  simulateretryfallback: As we are simulating the exception, after retrying a certain amount of time we can either expect a successful backend call or a complete failure.In this case, we will go to the fallback method to get a hard-coded/error response.Now this parameter will ensure all the retries will fail and go to fall back path.package com.example.springretry; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController; @RestControllerpublic class MyRestController {     @Autowired    BackendAdapter backendAdapter;     @GetMapping(\"/retry\")    @ExceptionHandler({ Exception.class })    public String validateSPringRetryCapability(@RequestParam(required = false) boolean simulateretry,                                @RequestParam(required = false) boolean simulateretryfallback) {        System.out.println(\"===============================\");        System.out.println(\"Inside RestController method..\");         return backendAdapter.getBackendResponse(simulateretry, simulateretryfallback);    }}To enable Spring Retry we need to put one annotation in the Spring Boot Application class. So open SpringRetryApplication class and add @EnableRetry at class level.package com.example.springretry; import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.retry.annotation.EnableRetry; @EnableRetry@SpringBootApplicationpublic class SpringRetryApplication {     public static void main(String[] args) {        SpringApplication.run(SpringRetryApplication.class, args);    }}Now we will create one interface/implementation for calling the external service. Here we will not actually call any external service call, but rather simulate the success/failure scenarios by adding some random logic, as below.package com.example.springretry; import org.springframework.retry.annotation.Backoff;import org.springframework.retry.annotation.Recover;import org.springframework.retry.annotation.Retryable; public interface BackendAdapter {     @Retryable(value = { RemoteServiceNotAvailableException.class }, maxAttempts = 3, backoff = @Backoff(delay = 1000))    public String getBackendResponse(boolean simulateretry, boolean simulateretryfallback);     @Recover    public String getBackendResponseFallback(RuntimeException e); }@Retryable: This is the main annotation after @EnableRetry. This annotation tells us that if we get a RemoteServiceNotAvailableException from the method, we retry three more times before sending the fallback response.Also we are introducing a delay of one second in each retry.@Recover: This fallback annotation indicates that if we don’t get any successful response after three retries, the response will come from this fallback method.Make sure you pass the expected exception as a parameter or else Spring will have a hard time finding the exact method.package com.example.springretry; import java.util.Random;import org.springframework.stereotype.Service; @Servicepublic class BackendAdapterImpl implements BackendAdapter {     @Override    public String getBackendResponse(boolean simulateretry, boolean simulateretryfallback) {         if (simulateretry) {            System.out.println(\"Simulateretry is true, so try to simulate exception scenario.\");             if (simulateretryfallback) {                throw new RemoteServiceNotAvailableException(                        \"Don't worry!! Just Simulated for Spring-retry..Must fallback as all retry will get exception!!!\");            }            int random = new Random().nextInt(4);             System.out.println(\"Random Number : \" + random);            if (random % 2 == 0) {                throw new RemoteServiceNotAvailableException(\"Don't worry!! Just Simulated for Spring-retry..\");            }        }         return \"Hello from Remote Backend!!!\";    }     @Override    public String getBackendResponseFallback(RuntimeException e) {        System.out.println(\"All retries completed, so Fallback method called!!!\");        return \"All retries completed, so Fallback method called!!!\";    }}Testing the retry methods from Spring Retry:  Start with browsing to http://localhost:8080/retry?simulateretry=true&amp;simulateretryfallback=false.  Based on the parameter, we are expecting exceptions and because simulateretryfallback is false, we are depending on the random logic (random % 2 == 0 –&gt; even random number) that will random give us a successful response while retrying.  So once we hit the request in the browser, we might get an exception in the backend and spring will retry the same method multiple times.The outcome could be a successful response from the backend.Here are a few lines of the log from one of my requests where Spring is retrying.Console logging===============================Inside RestController method..Simulateretry is true, so try to simulate exception scenario.Random Number : 1 ===============================Inside RestController mathod..Simulateretry is true, so try to simulate exception scenario.Random Number : 2Simulateretry is true, so try to simulate exception scenario.Random Number : 2Simulateretry is true, so try to simulate exception scenario.Random Number : 0All retries completed, so Fallback method called!!!Now try with http://localhost:8080/retry?simulateretry=true&amp;simulateretryfallback=true, you will get a fallback response after you hit the retry limit.Console logging===============================Inside RestController method..Simulateretry is true, so try to simulate exception scenario.Simulateretry is true, so try to simulate exception scenario.Simulateretry is true, so try to simulate exception scenario.All retries completed, so Fallback method called!!!Spring Retry provides declarative retry support for Spring applications. A subset of the project includes the ability to implement circuit breaker functionality. Spring Retry provides a circuit breaker implementation via a combination of its CircuitBreakerRetryPolicy and a stateful retry. All circuit breakers created using Spring Retry will be created using the CircuitBreakerRetryPolicy and a DefaultRetryState. Both of these classes can be configured using SpringRetryConfigBuilder.To provide a default configuration for all of your circuit breakers create a Customizer bean that is passed a SpringRetryCircuitBreakerFactory. The configureDefault method can be used to provide a default configuration.@Beanpublic Customizer&lt;SpringRetryCircuitBreakerFactory&gt; defaultCustomizer() {    return factory -&gt; factory.configureDefault(id -&gt; new SpringRetryConfigBuilder(id)        .retryPolicy(new TimeoutRetryPolicy()).build());}Similarly to providing a default configuration, you can create a Customizer bean this is passed a SpringRetryCircuitBreakerFactory.@Beanpublic Customizer&lt;SpringRetryCircuitBreakerFactory&gt; slowCustomizer() {    return factory -&gt; factory.configure(builder -&gt; builder.retryPolicy(new SimpleRetryPolicy(1)).build(), \"slow\");}In addition to configuring the circuit breaker that is created, you can also customize the circuit breaker after it has been created but before it is returned to the caller. To do this you can use the addRetryTemplateCustomizers method.This can be useful for adding event handlers to the RetryTemplate.@Beanpublic Customizer&lt;SpringRetryCircuitBreakerFactory&gt; slowCustomizer() {    return factory -&gt; factory.addRetryTemplateCustomizers(retryTemplate -&gt; retryTemplate.registerListener(new RetryListener() {        @Override        public &lt;T, E extends Throwable&gt; boolean open(RetryContext context, RetryCallback&lt;T, E&gt; callback) {            return false;        }        @Override        public &lt;T, E extends Throwable&gt; void close(RetryContext context, RetryCallback&lt;T, E&gt; callback, Throwable throwable) {        }        @Override        public &lt;T, E extends Throwable&gt; void onError(RetryContext context, RetryCallback&lt;T, E&gt; callback, Throwable throwable) {        }    }));}Differences Resilience4j with Netflix Hystrix and Spring RetryAlthough Resilience4J is inspired by Netflix Hystrix, it is more lightweight and you don’t have to go all-in.Quoting the official page “Resilience4J is a lightweight fault tolerance library inspired by Netflix Hystrix, but designed for functional programming.”  In 2019 when Spring announced that Hystrix Dashboard would be removed from Spring Cloud 3.1, one year after, Netflix announces that they were putting this project into maintenance mode.Resilience4J provides the following core components:  RateLimiter  TimeLimiter  CircuitBreaker  Retry  Bulkhead"
      },
    
      "architecture-2021-01-04-designing-rest-services-html": {
        "title": "Designing REST Services",
        "url": "/architecture/2021/01/04/Designing-REST-services.html",
        "image": "/img/2021-01-04-Designing-REST-services/servicedesign.jpg",
        "date": "04 Jan 2021",
        "category": "post, blog post, blog",
        "content": "The World Wide Web (WWW) and its underlying architectural style of REpresentational State Transfer (REST), is a hugely successful application platform with an unprecedented adoption scope. As such, it will come as no surprise that architects will look at this structure and attempt to distill the factors that made it so successful and apply these to similar systems. This is what REST service design attempts to do. The book “REST in Practice”, written by Jim Webber, Savas Parastatidis, and Ian Robinson finds its main purpose in the pursuit of these success factors and apply them to software development in general.With REST (based on the doctoral work of Roy Fielding) as its weapon of choice, it seeks to position this style as a viable (and even better?) alternative for Classic SOA. In its purest essence, REST is a state machine, tracking progression within a workflow by transitioning from one state to another. This state machine has one caveat though in that it is not known in advance how many different states there are. However, it displays numerous admirable qualities such as scalability, loose coupling and functional composition across service boundaries.The book pits HTTP(S) against SOAP in the battle of protocols when implementing web services. Where SOAP only uses HTTP(S) as its transport channel, REST embraces all of the principles of the protocol. In order to assess why REST is such a good fit for service design, let’s review what a web service is all about.Web services are software components that are developed to expose business capabilities. To do so, they are comprised of the following elements (available through the provided contract):  Operations: A set of exportable operation signatures that can be accessed by the consumer of the service.  Document Schemas: A definition of the data types that can be exchanged with the service through the provided interfaces.  Non-Functional Specifications:          Conversation Specifics: Indication on how information can be exchanged with the services, such as request-response or fire-and-forget.      Quality-of-Service Characteristics (QoS): Indicators for QoS characteristics such as availability, latency and throughput.      Policy Specifics: Requirements specifications for how to interact with the service. These are stipulations on for example security and transactional contexts.      The first five chapters of the book deal mainly in building a transformative journey for an organization to shift to using REST services according to the Richardson’s maturity model. The model starts from a very basic implementation not following any of the REST standards up to a full implementation of services with the principle of “Hypermedia As The Engine Of Application State” (HATEOS). The remaining chapters deal with how to address any non-functional specifications these services must cover. In these remaining chapters there is a brief intermezzo where the authors delve into the ATOM format as a substitute for transferring data and a means for event driven service design. As these chapters are a bit the odd ones out, this synopsis will not delve into them too deeply.Richardson’s Maturity ModelLevel 1 - ResourcesIn order to clarify the success factors of the WWW as a platform for services on a global scale, the book sketches a high-level overview of the architecture behind it, and the salient points for the REST architecture. Formalized by the World Wide Web Consortium (W3C), the specification can be found here. Services are exposing resources through Uniform Resource Identifiers (URI). Their relation is many-to-one: A resource can be identified by more than one URI, but any URI will only point to one resource, making it addressable and accessible for manipulation via HTTP.This endpoint will follow a specific structure: &lt;protocol&gt;://&lt;host&gt;/&lt;resource&gt;/&lt;identifier&gt; with the identifier piece being optional. To fetch consultant Peter De Kinder from the service would look something like this:    https://www.bestitpeople.com/consultant/peterdekinderThe Web does not differentiate between resource representations. Resources can thus have multiple representations. These representations are views encoded in a specific format (Json, XML, MP3…) to match the needs of the consumers through content negotiation (see further down).This consumer friendliness does not relinquish control on how to represent or modify these resources. This is still the purview of the services that control them. The encapsulation of the resources support isolation and allow for independent evolution of functionality in order to preserve loose coupling, one of the key aspects of the Web. One consideration to make is that you should name your resources in such a way that they are intuitive: They must indicate the intent of the service as well as already provide a rudimentary level of documentation.As stated earlier: HTTP is the spearpoint for this architecture, and this protocol stacks on top of the TCP/IP protocols and a series of WANs and LANs for its communication. These networks are hosted on a set of geographically widespread and commoditized web servers, proxies, web caches and Content Delivery Networks (CDN) that host the resources and manage traffic flow without intricate canonical data models or middleware solutions.Loose coupling in combination with the caching possibilities of the network allow for the needed scalability asked from services and application in today’s software development. But since the Web doesn’t try to incorporate QoS guaranties and other non-functional specifications, there is a need for fault tolerance in the services design, as the Web will always try to retrieve resources, even if they are nonexistent.Level 2 - HTTP VerbsThe manipulation through HTTP is done using the verbs that are supported in the protocol: GET, POST, PUT, DELETE, OPTIONS, HEAD, TRACE, CONNECT and the somewhat “newer” PATCH. These verbs form a uniform interface with widely accepted semantics that cover almost all possible requirements for distributed systems. Add to this a set of response codes that can be returned together with a payload of which the most famous is the 404 – Not Found.Combined with employing HTTP and URIs for this implementation, we are compliant with level two of Richardson’s maturity model. In addition to this we need a way of communicating and handling failures that might occur during the execution of a transaction.For example: If we take a standard ordering system where the transactions manipulate said order (our resource), we get the following contract:VERBURISTATUSDescriptionPOST/order201A new order is created, and the location header returns the new order’s URI. The complete order needs to be provided.409The creation of the order conflicts with the current state of another resource and is rejected. For example: The order being created already exists.GET/order/{id}200Returns the current state of the Order identified by the URI.404Returns a status code indicating the requested resource cannot be found by the service.PUT/order/{id}200/204Updates the Order identified by the URI with new information. Only a partial order is needed (with all fields that need to change). The difference between the 200 and 204 response is just aesthetic and depends on the choice of the organization.404Returns a status code indicating the requested resource cannot be found by the service.409The update has created a conflict with the current state of the order and is rejected.412The update is attempting to update a resource that has been modified since it was last fetched. This signals a concurrency issue.DELETE/order/{id}200/204Logically removes the order identified by the URI, and in the case of a 200 response, we could return the final state of the order.Some response statuses can be generated by all of the above HTTP verbs:STATUSDescription400A malformed request was sent to the service.401Unauthorized Access. The party trying to act in a transaction does not have the proper authorization to perform the requested actions.405Method not allowed. The execution of this verb is not allowed on the current resource type (in our case order). In case of a DELETE this could also mean that the resource is currently in a state that doesn’t allow it to be deleted.500Internal Server Error when the service is unavailable or internally crashing without possible recovery.Sidebar: Singular versus PluralThis book identifies resources using their singular noun in the URI. Many service providers will use the plural noun instead. There are many philosophical debates to be had whether to use singular or plural. For example: Amazon and Google use plural in their API. There is no clear reason or advantage for either to be preferred over the other, so in my opinion it is largely up to the designer(s) to choose as long as they remain consistent across the many components within the organization. The only vaguely logical argument I found was that there are apparently more words in the English language, called mass nouns, that don’t have a plural form (such as “luggage”) than there are words that don’t have a singular form (such as “clothes”). Bear in mind that I didn’t perform any due diligence of this argument, so the author of this reasoning might just be wrong about this.When designing and implementing services we always need to consider whether calling these services should be safe and/or idempotent. Safe services have no server-side side effects that the consumer of the service can be held accountable for. These service calls will not trigger any effects that will change the state of resources. Idempotency is the fact that a service call can be done multiple times without yielding a different result in any of its calls. Each identical call will result in an identical response. The GET of a resource is a call that is considered both safe and idempotent. The service call will return the same result no matter how many times it is called, and it will not alter the state of the resource it is requesting.            HTTP Verb      Safe      Idempotent                  GET      x      x              POST      x      x              PUT             x              DELETE             x              PATCH                    Other verbs (all both safe and idempotent) that are commonly enabled on REST services are the following:  HEAD: Returns only the HTTP headers of the request in order to determine the context of the resource.  OPTIONS: Queries the endpoint for the possibilities it offers. This verb is typically also used by browsers as a preflight request to determine whether Cross-Origin Resource Sharing (CORS) is allowed  TRACE: Allows for a loop-back test with debug information. This method is not included in default authorization checking, and should be disabled in production, as it can be a security risk.The book concludes its elaboration on the second level of Richardson’s Maturity with an overview of some of the more popular technologies and frameworks and how they tackle the creation of a consumer for the services defined with REST. These are rather straightforward for those developers that routinely use them, so we will not go deeper into them.Level 3 - Hypermedia ControlsHypermedia systems extend the resource state that is being manipulated by the services with additional characteristics. The resource state becomes a combination of:  The values of the individual variables that make up the resource  Links to related resources  Links to manipulate the current resource (creating, updating, deleting…)  Evaluation results of business rules encompassing the resource and other related resourcesThe links mentioned make up a domain application protocol (DAP) that advertises all possible interactions with the resource. The consumers of this service use these published interactions to discover how to interact with the resource. Hyperlinks are the weapon of choice when adding such links to interactions. A DAP also consists of two other key components: Media Types (as mentioned before), and HTTP Idioms (which make up the HTTP uniform interface: verbs, standard headers, error codes…). In short: Link relation values tell why the consumer should activate a hypermedia control by stating the role of the linked resource, the media type tells the consumer what it can expect as the response of a link, and the idioms manipulate the resources represented by the links.The resource state can be in any format as REST’s hypermedia tenet does not force any set media type. IANA is the official registry of MIME media types and maintains a list of all the universally accepted MIME types.The resource should indicate in the Content-Type header of its responses the ideal way of interpreting it. Similarly, the Accept header is part of the HTTP spec for requests. In this header the consumer can indicate which media type it wishes to receive. Based on these two headers, a system of content negotiation is set up where the request of a consumer can be matched to the proper implementation to return the desired result and how it is conveyed.The HTTP specification allows for the definition of custom media types as well. This is used to inform the consumer about the result to be expected in greater detail. Where “application/xml” informs the consumer which format to expect, it doesn’t give any insight on which data to expect. This can be done by creating a custom media type in the vendor range (vnd). This type adheres to the format:&lt;media type&gt;/vnd.&lt;owner of custom media type&gt;.&lt;type of data&gt;+&lt;media suffix&gt;For example: If the return of a service would list the detail of a consultant for the company “Best IT People” in XML format, you would get the following media type: application/vnd.bestitpeople.consultant+xml. Beware however not to create a set of customer media types that map directly onto the representation formats in the code of the service. That would create unnecessary tight coupling.By utilizing the HTTP specs, URIs and hypermedia, REST allows for scalability, uniformity, performance, and encapsulation when designing a distributed system. These conventions guide the service design, and thus have an impact on how the service will be exposed to the outside world. This exposure is what we call the service contract. A service contract informs the consumers of the service about the format of the resource (media type), its processing model (protocol) and the links to related resources. The contract also shields the consumer from the implementation details, decreasing the coupling between the service and its consumers, and at the same time heightening security through obfuscation. This loose coupling is further enhanced by applying Postel’s Law: “Be conservative in what you do, be liberal in what you accept from others”, also known as the robustness principle. An example of how to go about implementing this law, is the Tolerant Reader pattern.Although the service contract has these conventions as a foundation to build upon, there is still a need for some thought to be put into the data modeling of the resources that will be exposed. The decisions on how to divvy up the data assets that make up the business context into exposable resources can be a daunting endeavor, and numerous design factors should be taken into account based on the context in which this services and their contract are designed.However, there are some recurring factors influencing the size of the resource representation:  Atomicity: Composite resources that share blocks of data can cause the other resource to enter an inconsistent state.  Importance of the Information: The optionality of certain components could indicate them belonging to a separate resource representation.  Performance and Scalability: the size of the resource and the frequency with which it is accessed, determines how long it takes for it to be passed over the network.  Cacheability: This is greatly enhanced if none of the components of the resource changed at a different frequency from the others.Designing the contract before implementing the service is what is called the contract-first approach. This approach might seem like a lot of design/thinking upfront, and it will require additional development when exposing legacy code as a service or implementing retrofits, but despite this additional preparatory work, it does grant several benefits:  Implementation teams that will make use of the service can work in parallel with the service development team.  The contract is known before starting development so all teams have an idea of what to expect from the service. This can give a healthy discussion between provider and consumer teams on how to tweak the service an get ahead of mismatches in expectations.  An existing contract allows for code generation of stubs and proxies of the service to test for connectivity and availability of the service that will be developed.  Contract-first helps with keeping the contract and the underlying implementation loosely coupled as the contract is not based on the code that is run.There are concepts that I like from this level, such as the use of vendor-specific media types and contract-first design, but mostly this level carries with it a lot of added complexity for a return that only becomes tangible when working with experienced development teams and an organization with sufficient maturity in the field of service design. This is the main reason that level 2 services are the most common.And nothing stops us from cherry picking these concepts and applying them to our level 2 services.Non-Functional CharacteristicsThere are several considerations that weigh on the design and implementation of services that are of a more technical nature. Some of them have already been mentioned, but there is a selection of these characteristics that are worked out in the book, each with its proper impact on the scalability of the services.CachingCaching can be done at numerous points along the request/response chain of a service call. When a resource is requested, and one of the intermediate components has a version of this resource, they will provide it back. Otherwise the service call will reach the origin service and collect the real-time data. The origin service should provide the intermediate components the rules for when and how to cache as well as how long the data would be considered “fresh”. Evidently, the closer the caching component is to the consumer in the request/response chain, the less expensive (reduction of bandwidth and of load on the origin service) the call. The other benefits of caching are a reduced latency (a quicker roundtrip time) and a reduced impact of possible network failures.While there are benefits to caching, there are also reasons not to do so. In these situations, caching will harm more than it benefits, or might simply not be allowed:  If the GET of the resource generates side effects from being accessed (for example a counter system that limits how many times the resource may be requested by consumers or some early bird system that benefits the first so many consumers), the cache would prevent these side effects.  When the system in place doesn’t tolerate any latency in data to be retrieved and we need to be sure that the data received is real-time (for example a heart monitor in ICU wards).  When caching is not allowed for regulatory reasons or security/privacy risks.  When the frequency with which the data changes is so high and the period that the data is not stale is so small that caching would never trigger.Aside from all the caching that can be added on an applicative level, there are already several components capable of caching that are native to the infrastructure of the web:  Local Cache: A store of cached representations from many origin servers at the behest of a single consumer, either in memory or persisted.  Proxy Cache: A server that stores representations from many origin servers to many different consumers. This component can reside either outside or inside the corporate firewall.  Reverse Proxy: A component that is a type of proxy server that retrieves resources from a single origin server on behalf of any number of consumers. It focuses mainly on load balancing, HTTP acceleration and security features.The caching components are instructed on how to handle the caching of the resources they hold by using these specific HTTP response headers:  Expires: Indicates the length of the period the cached resource can be used before being considered stale.  Cache-Control: This header can also be provided by the consumer of the service in the request. It allows for a set of comma-separated directives on how to handle the caching of the resource:          Request-only directives:                  max-stale[=&lt;seconds&gt;]: Indicates the client will accept a stale response. An optional value in seconds indicates the upper limit of staleness the client will accept.          min-fresh=&lt;seconds&gt;: Indicates the client wants a response that will still be fresh for at least the specified number of seconds.          only-if-cached: The cache should either respond using a stored response, or respond with a 504 status code if no cached version of the resource is available.                    Response-only directives:                  must-revalidate: Indicates that once a resource becomes stale, caches must not use their stale copy without successful validation on the origin server.          public: The response may be stored by any cache, even if the response is normally non-cacheable.          private: The response may be stored only by a browser’s cache, even if the response is normally non-cacheable.          proxy-revalidate: Like must-revalidate, but only for shared caches. Ignored by private caches.          s-maxage=&lt;seconds&gt;: Overrides max-age or the Expires header, but only for shared caches (e.g., proxies). Ignored by private caches.                    Generic directives:                  max-age=&lt;seconds&gt;: The maximum amount of time a resource is considered fresh. Unlike Expires, this directive is relative to the time of the request.          no-cache: The response may be stored by any cache, even if the response is normally non-cacheable. However, the stored response MUST always go through validation with the origin server first before using it          no-transform: An intermediate cache or proxy cannot edit the response body, Content-Encoding, Content-Range, or Content-Type.          no-store: The response may not be stored in any cache. Note that this will not prevent a valid pre-existing cached response being returned.                    Extended directives (not part of the official HTTP standard, but universally used):                  Immutable: Indicates that the response body will not change over time.          stale-while-revalidate=&lt;seconds&gt;: Indicates the client will accept a stale response, while asynchronously checking in the background for a fresh one. The seconds value indicates how long the client will accept a stale response.          stale-if-error=&lt;seconds&gt;: Indicates the client will accept a stale response if the check for a fresh one fails. The seconds value indicates how long the client will accept the stale response after the initial expiration.                      ETag: An opaque string token that is associated with the resource to verify its state. This can be used to determine whether or not a resource has changed since the last time it was requested. Therefor this will also be used in concurrency tests.  Last-Modified: The date at which the resource was modified the last time before this request.ConsistencyIt is important that data is kept consistent across various manipulations, especially when it is being exposed to and manipulated by the consumers of stateless services. One of the key risks that threaten this consistency is concurrent writes to the same resource. The scenario is straightforward. Consumer A reads the current resource. Consumer B reads the same resource. Consumer B updates the resource, followed by consumer A updating the resource. In this scenario, consumer A never saw the changes that consumer B made, and more than likely erases them by updating the resource.One way to tackle this scenario is to introduce conditional updates. The idea is that when a consumer reads a certain resource, it gets passed a unique representation of this resource in the “Etag”-header (for example a MD5-hash of the non-transient fields of the resource). Each time any consumer is updating a resource, the consumer needs to provide this value in the “If-Match”-header. The idea is that when the update is processed by the service, it checks whether the content of the header still matches the representation of the resource. If it is still the same, it will update the resource and return the result (HTTP 200 OK status). If between the read and update of the resource by this consumer, another consumer modified the resource, the representation value will have changed, and the update is denied (HTTP 412 PRECONDITION FAILED status). It is then up to the consumer to fetch an up to date resource and decide whether or not still to process its changes. The “If-None-Match”-header checks whether a resource exists with the representation, and if not, proceed with the update. This could be used to prevent the creation of duplicate resources in the database.There are also headers based on the modification date of a resource for indicating to the service that it should behave in a conditional way. Personally, I feel that this adds the additional complication of syncing the date between the service and its consumers, and that using the ETag is more resilient:  The “If-Modified-Since” header only executes when the resource has been modified since the given date. This header works with the last-modified header instead of the ETag.  The “If-Unmodified-Since” header is the inverse of the “If-Modified-Since” header.SecurityThere is an entire chapter dedicated to security in the book. However, since this is one of those areas of expertise that has rapid evolutions, the contents of this book (written in 2015) are somewhat dated. It evaluates authentication and authorization mechanisms based on four characteristics associated with secure distributed systems: Confidentiality (how well data can be kept private), Integrity (how to prevent unlawful changes to data), Identity (how to identify parties involved in the transactions) and Trust (what to allow the previously mentioned parties in transactions).The book offers up the following mechanisms to achieve these characteristics:  Basic Authentication: a very straightforward username/password combination passed along as a base64-encoded string in the request by using the Authorization header. Too easily intercepted and decoded to be useful in production systems.  Digest Authentication: A challenge/response exchange that happens in reaction to sending a request. The initial request is resent with additional information stored in the various security headers (qop, nonce, opaque, username, uri, nc, cnonce, response). This mechanism is safer than Basic Authentication, but still falls prey to man-in-the-middle attacks.  Transport-Level Encryption: This application of HTTPS for service exchange remains up till today a gold standard in security. One caveat is that this does not affect the payload, so this payload is still vulnerable at the termination point of the HTTPS connection. HTTPS is more expensive than HTTP in terms of performance, and does hamper caching in the network components, most of the time this tradeoff is warranted.  OpenID and OAuth: Since the book describes version 1 of Oauth, this part of the book is outdated, and is only of interest as a historical perspective. For an elaboration on OAUTH2, see my blog post on the topic.Web SemanticsThere is a chapter dedicated to Web Semantics. This concept can be characterized as the meaning behind data and information, and stems from a need to make sure that all parties involved in the management of a resource have the same interpretation for it. This shared interpretation is then formalized in a\tcontract (using frameworks such as OWL or RDF), making the resource meaningful for both people and consuming services. It mostly deals with the difference between data, information and knowledge in terms of structure (relationship between different information pieces that make up a resource) and representation (in which format to expose it).Web Semantics was very popular for a time, but as I have described in a previous blog post, for me it no longer bears much relevance. That being said, remnants of the technologies can still be found in for example the twitter card headers used to aptly represent website links on the twitter feed. These metatags use RDF as their foundation.ConclusionThe WS-* protocol stack might still be considered more developed, but this comes with an increased complexity that hinders its uses. It still has some legacy that it cannot shed, such as for example the disregard of encapsulation in exposing its internal workings via the WSDL. But it does come with several strong points to cover the non-functional needs of service design. Not in the least its security features that come with a full suite of cryptographic techniques provides an end-to-end mechanism or transferring information.And we cannot forget that at this time its adoption by organizations is still very widespread.However, REST has gained substantial maturity on these topics as evidenced by this book and has since become the new favorite. REST isn’t a perfect fit for every situation, as no solution ever is. And its distributed nature does necessitate strict monitoring of performance and other metrics such as mean time between failures. But the familiarity with the World Wide Web architecture carries enough weight to add an intuitive aspect to its development and use."
      },
    
      "iot-2020-12-17-ordina-logo-detector-html": {
        "title": "Building a custom YOLOv5 Ordina logo detector",
        "url": "/iot/2020/12/17/ordina-logo-detector.html",
        "image": "/img/2020-12-17-ordina-logo-detector/banner.jpeg",
        "date": "17 Dec 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  YoloV5  Creating a dataset  RoboFlow  Training the model  Testing the model  ResourcesIntroductionMachine learning is here to stay.It’s also a must look into type of thing for a lot of people.However making a fully custom model to do a specific task is very hard.This blog post will go into detail on how to take a prebuilt/trained model and use it for our own purpose.We will take the YOLOv5 model and retrain it with a fully custom dataset to detect the company logo.Read on down below and follow along for the ride.Please note that if you want to test this by yourself it is advised to have a decent computer with an NVidia CUDA capable GPU or use an online platform like Google Colab which will give you a free cloud GPU to test things with.Yolo V5YOLOv5 is the fifth mayor iteration for the You Only Look Once model.It’s a very high performing and popular model for performing object detection.The model is fully open source and is trained on the CoCo dataset and can perform detections of about 80 classes of objects.It’s also relatively easy to retrain the model with custom data so it can perform detection on other things than the CoCo dataset &amp; objects.For iOS users there is an app available on the app store that allows you to use the YoloV5 model in realtime with the camera your device.The speed and accuracy is quite impressive, make sure to give it a try!The model has different sizes that can be used, each specific size has pros and cons.The larger models will perform better but require a lot more compute power.For this example we will be retraining the large model, since I have a decent NVidia GPU I can use.Creating a datasetA good dataset is extremely important when (re)training a model.It has an immense effect on the training process.For our custom logo detection I’ve made about 100 photos of the Ordina logo in different forms and under different conditions.It’s very important that there are a lot of different images and most single images should have different versions with slight alterations.This can be a very time consuming thing to do!As we will later see there are tools to help with this!Making and gathering photos with the logo is only one part of the preparations that need to be done.The second part can be even more tedious but is quintessential to the training process.The photos need to be labelled.This means creating a file that defines where the logos are located in the photo.This can be done by hand, but it’s easier to use a decent tool, one of the tools that can do this is LabelImg.An open source tool for annotating images.It is a python program that can be run on most operating systems.Annotating is simple yet time consuming.We run the program, select the folder where all the images are stored and manually go over each photo, drawing a bounding box over each Ordina logo and saving the data before moving on to the next photo.                                RoboFlowOnce the data is labelled we want to use it to our advantage.A single photo can be skewed, blurred, pixelation added, hue moved (preferably a combination of all).This allows a single photo to become many more versions.The manual method for doing this could involve using Photoshop macros on all the photos, then adding the annotations for the newly generated files.There are however tools that manage this for us.One of these tools is RoboFlow, an online dataset management system.RoboFlow allows you to upload images and add “augmentations” to the images.These augmentations are combined and like in our example we can create 226 images out of 94 base images, not bad.This is greatly beneficial in preventing the model from overfitting when used correctly.The free tier only allows a maximum of 3 augmentations per image so a large starting set of images is recommended.The pictures in the dataset are subdivided into three categories:  Training: Used for training the model  Validation: Used for hyperparameter tuning during the training process  Testing: Used to evaluate the model in each epoch                                                                  Once we have added the photos and the augmentations we can generate a version of the dataset and use the link to the dataset zip file to retrain the model.It’s very important to select the correct export format, being YOLOv5 PyTorch.                                Training the modelFor training the model I used the excellent blog post on the RoboFlow blog as a starting point, combined with the “Train-Custom-Data” section on the YoloV5 github wiki.I did use Google Colab for the first try and it does work, be it slower than on my personal machine.Doing the training locally requires python3 and pip to be installed, virtualenv to be setup and the correct NVidia drivers to be loaded (at least on Debian).I created a new folder in which I cloned the YOLOv5 repo and created a new python virtual environment and started Jupyter notebook.The RoboFlow Google Colab is a great place to start.I copied all the steps over, making edits to allow it to run on my local machine.If you want to test this too, just copy the Google Colab file to your own Google Drive and start it from there.Google will even give you a cloud based GPU to use, for free!The retraining process contains these main steps:  Clone the YoloV5 repo, install any dependencies  Download the dataset zip file and extract its contents  Process the data generated in the dataset, choosing the size of the Yolo model to retrain and setting the number of classes that are in the dataset  Retrain the model by using the ‘train.py’ file in the YOLOv5 repo  Evaluate the training progress by using TensorBoard  Save the best &amp; latest model weights to a folder for later use  Perform some own detection on previously unseen imagesTesting the modelThe last step after the model has been trained is to see how well it does when presented some new photos which were not in the dataset.Performing the detection is simple.We use the detect.py file in the YOLOv5 repo.An example to perform detection on all files in a folder:!python detect.py --weights runs/train/{today}/weights/best.pt --img 416 --conf 0.5 --source ../custom-test --name custom --exist-ok --save-txt --save-confThe used parameters do the following:  weights: This points to the weights file that has been created and saved by the retraining process  img: Specified the size of the image, it will automatically resize any image input to match this number, has to be 416 this model  conf: The minimum confidence level that should be reached to count as a detection  source: A media file, being an image(or a folder containing multiple)/video/stream/  name: Name of the folder to output the results to (will be stored under yolov5/runs/detect/NAME)  exist-ok: Overwrite existing output instead of incrementing the name  save-txt: Save the detection data to a text file (bounding box)  save-conf: Add the confidence level to the text fileThe result will be a folder named custom where all the images and text files reside.Each image will have a bounding box drawn around the detected logo, if any, with a confidence level.Each text file will contain the box coordinates in normalized WHXY format.Example of text outputClass W H X Y Conf0 0.503968 0.540551 0.207672 0.0691964 0.791504The text file contains all the basic info that is needed to further process the detection result:  Class: Class that has been detected  W: The width of the detected object/bounding box, to be divided by 2 and extended from the X-Coordinate in both directions  H: The height of the detected object, to be divided by 2 and extended from the Y-Coordinate in both directions  X: The X-coordinate of the center of the detected object  Y: The Y-coordinate of the center of the detected object  Conf: (Optional) The confidence level, between 1 and 0.000001 (or between 1 and the minimum specified during detection), 1 being 100% certain, 0.000001 being the least certain possibleAs you can see below one of my retrained models was able to detect the logo in all three never before seen images!                                                 Resources  YOLOv5  YOLOv5 IOS app  CoCo dataset  LabelImg  RoboFlow  Train YOLOv5 with custom data 1  Train YOLOv5 with custom data 2  Google Colab document  Overfitting  Train/Test/Validate"
      },
    
      "design-2020-12-16-mobile-design-html": {
        "title": "Mobile Design Done Right",
        "url": "/design/2020/12/16/Mobile-Design.html",
        "image": "/img/2020-12-16-mobile-design-done-right/mobile-design.png",
        "date": "16 Dec 2020",
        "category": "post, blog post, blog",
        "content": "Table of contents  Mobile design  The bad and… the ugly?  Don’ts          Custom gestures      No labels      Hamburger menu      Hidden search bar      Unreadable text                  Contrast          Typeface          Font size                    Clutter      Forms                  Typing too much          Input labels          Break tasks into chunks                    Too small buttons      Underlined links        Do’s          Put focus on what matters      Provide feedback      Thumb position        Tips          Test your design      Optimizing flow      Onboarding      Response time      Prioritize features      Asking for permissions      Tooltips        Trends of 2020          Advanced animation      Personalizations      Password-less login      Dark themes      Skeleton screens        Are we done yet?Mobile designMobile has taken over desktop when it comes to web browsing. Because of this, the importance of user-friendly mobile interfaces is bigger than ever. In this recipe, we will go from a bad design to a user-friendly design in a couple of steps. This will help you make better design choices when developing for a customer!The bad and… the ugly?This is a design created for the fictional webshop Lux, a company that sells luxury items.If you take a look at this design, you will notice that it’s pretty clean. But a good-looking design doesn’t always check off the criteria of a good design. Take a moment to think about what could be wrong.Don’tsFirst, we will discuss the don’ts when creating a design for your application. We will take everything step by step. There will always be imperfections to the design, so keep thinking about what else could be wrong!Custom gesturesGestures are a fun way of working with a touch screen. It might result in faster interactions, but keep in mind that there is a learning curve to custom gestures. Don’t replace UI elements with them, but give the user the choice.In our example, to go to the wishlist page, you have to swipe left. Some people might not know this and never get to this page. However, we can solve this by adding a tab bar that contains all the pages you can go to. Let’s combine this with the swipe gesture.        No labelsDo you see anything wrong with the newly added tab bar?You probably know by now that the heart means wishlist and the user icon has something to do with your account. These are assumptions that our brain makes when seeing those icons in this context, but it’s not always that obvious to other users. This is why labelling preferably all icons is necessary. As you can see, our tab bar is way clearer now and it will gain more engagement!        Hamburger menuThe first thing developers do when they have to make a website responsive is adding the hamburger menu, but this is not always the way to go. Let’s take a look at our Lux application. There are only three buttons in our hamburger menu and there is plenty of space in our header. If you don’t hide them behind a button, people will be more tempted to go to one of these pages. This doesn’t mean that you should never use a hamburger menu, but you should try to make the most important features easily accessible.        Hidden search barAnother thing that decreases engagement, is hiding the search bar behind a button. If you show it all the time, you will see more users using the search functionality.        Unreadable textContrastMore important than ever because of the smaller screen sizes is the readability of text. First of all, you need enough contrast between the text colour and background colour. You can use a tool like Colour contrast to check the contrast between the foreground and background. For the Lux webshop, I used a grey colour that is too light for a white background. To change this, I played around with the colour contrast tool. I decided to go with a darker greyish-blue tone to grey out text and a dark grey colour for text that should be readable.        TypefaceYou need to choose a typeface that works well in multiple sizes and weights to maintain readability and usability. Test your typeface before using it and avoid typefaces that are complicated. In the Lux design, the typeface ‘Stalemate’ was used for the title of the items. This might look fun, but on smaller screens, it can be harder to read handwritten fonts.        Font sizeDon’t use font sizes smaller than 12px. You might be tempted to make your font small on mobile screens, but readability always goes before design!        ClutterWe have a better design than what we started with, but it is still not perfect. A famous saying by Antoine de Saint-Exupéry can be applied to mobile UX design:  “Il semble que la perfection soit atteinte non quand il n’y a plus rien à ajouter, mais quand il n’y a plus rien à retrancher.“which translates to: “Perfection is finally attained not when there is no longer anything to add, but when there is no longer anything to take away.” Get rid of everything you don’t need.Especially when designing for mobile, you have to focus on what matters. In our example, there is a log out button. Logging out is not something people will often do and there is also an easily accessible account button available. That account page also has the log out functionality, so let’s just remove the log out button from our header. Another thing we can remove is the informational text on the sales items. You can click on the item to read more, but there is no need to put this information on the overview.        FormsWhen you want to sign up to Lux, you have to fill in a form. Since we are creating this application for mobile, we have to take a lot of things into account when creating this form.Let’s see what our form looks like right now…Typing too muchOne thing that can scare off mobile users from filling in forms is having to type too much. There are many ways to still get the same information but make it easier for users to fill everything in. You can use their contact information to prefill forms and avoid them having to fill in their name, e-mail, phone number, etc. for the hundredth time. Next to that, you can make sure the right keyboard comes up when they click on a certain input. For example, when they are filling in a phone number, you show the number keyboard. That way, they don’t have to change it themselves. This saves a lot of time. Also, something that a lot of apps are starting to do right now, is using the camera for input. We can use this here for the credit card number. People just have to scan their credit card with their camera and the number is automatically filled in. Next to that, you can change text inputs to different inputs that require less or no typing. Here, I added a date input for the birth date and a custom selector for the categories.        Input labelsImagine filling in a form and having to go get your credit card. When you come back, you might have forgotten what the other input fields were about. In our example, the labels are placeholders which disappear when you’ve entered something in them. The best practice for input labels is to show them at all times, because yes, people tend to forget what they were for.        Break tasks into chunksIf you have a large form, break your tasks into chunks. You can create a step-by-step form. Do this by asking the main questions in the beginning and then asking questions that don’t seem as important in the following steps. Make sure people always know why they are filling in certain information and make sure the info you’re asking for is needed at that time. In the Lux example, I changed the form so that people can sign up with just an e-mail address and a password. I added an explanation for why we need the other information and why it benefits the user to fill in a second form. I added a save button so they can always fill it in later.Too small buttonsThere are two major rules when it comes to using buttons on a touchscreen. The first rule is that you need to create controls that measure at least 7–10 mm so they can be accurately tapped with a finger. Nothing is more annoying than having to tap very precisely with a smartphone. The buttons on our newly created form are very small and you might accidentally click on the wrong button.                Rule number two is that you need to ensure that there is the right amount of spacing between tap targets.Our tab bar looks alright, but I think a little more spacing between the buttons won’t hurt.        Underlined linksFor desktop applications, we use a lot of underlined links. On mobile devices however, it’s best to avoid them. Usually, links are replaced with buttons for mobile applications, because they are way easier to tap on with a touch device. And, one more plus: they grab your attention.        Do’sPut focus on what mattersThe title says it all: put focus on what matters. Try to think about what you want users to do.For our webshop, we want people to buy the Sale items.There is no reason for the titles to be this big because we don’t want the customer to interact with them.We can make them smaller and make sure the Sale title stands out a bit more.Now, our focus goes to the sale items instead of the two titles.        Provide feedbackProvide feedback on interactions so people know what’s going on.Take a look at what happens when someone adds something to their bag:You had to look very close to see there was something added to the bag, right?Using a pop-up message here will grab your user’s attention and make it clear the action was successful.Another example for which you need to provide feedback is when a page is loading.A small loading animation is enough, but don’t let people think they’re stuck on a page when things are loading.It might annoy them and even let them leave your application!        Source: LePraveen Tewatia on DribbbleThumb position        Source: Ranjith Manoharan on DribbbleAlways keep the thumb position in mind.Know what spaces on your screen are more easily accessible with the right thumb and use them for your advantage.What buttons do you want people to click on? As you can see on the thumb zone-chart, our bag is in the red zone.We want people to buy stuff, so we might have to swap the places of the account and the bag button.        TipsOur Lux design is finished. Nothing is ever perfect, but we should always strive for perfection.Here are a few more tips to help you with designing an interface for mobile!Test your designIn our example, we swapped the bag and account icon because we want people to easily go to the checkout.According to our thumb zone-chart, this is the right positioning for our goal, but in practice, this might not work.This is why you should always test your design. The numbers are what counts. You might have done everything by the rules and still not have the engagement you aimed for.Optimizing flowA user flow is a sequence of actions a user has to perform to acquire different functionalities in your app, such as set up, purchasing an item, … Users might find the perfect app but still delete it because there is friction in one of the flows.You can use the Page flows website to learn from proven products!        Source: LeanplumOnboardingGood onboarding is essential and shows the value of your application.A very effective way of onboarding is having an interactive tour of your app.This is a fun and easy way for people to get the hang of using your application.One thing to keep in mind is that your onboarding doesn’t make up for the fact that some things just may not be clear to your users.The app should still be easy to use, even when you’ve skipped the tutorial.        Source: Habitify appResponse timeOne thing that is especially important to keep in mind for us developers is the response time.As technology progresses, people get more impatient and they might leave your app if they have to wait longer than 2 seconds for something to load.We directly influence the response time, and we should continuously improve the quality of our code.It’s also interesting to take a look at your target audience.We are used to fast internet connections in Western Europe but other countries might not have that privilege.        Source: Google/SOASTA Research, 2017Prioritize featuresYou can pack your app with thousands of interesting features to make it more attractive to potential users.While you might have a lot of traffic going to your application, a lot of users will be scared off by the overload of features.The rule is to prioritize the features your application needs and omit the nice-to-haves.Asking for permissionsYour app might need permission for using the user’s location at some point.Don’t ask for this the moment they open your application.The user might not get why they have to give you permission and will decline the request.That would mean they have to go back into their settings later.Instead, ask for it whenever they are starting to use the feature that needs location and provide a clear message as to why you need this permission.        Source: PlotProjectsTooltipsTooltips display information when a user hovers, focus on or taps on an element. Most commonly, a tooltip is shown on hover, but there is no such thing as hover on a mobile device.You can show it on long press, but people might not even know it’s there.        Source: Material DesignAsk yourself the question: do I really need to hide this tooltip?You can show the tooltip by default or conveniently place it alongside the element you would hover over.While on large screens a tooltip might look really small, it will draw the attention on smaller devices.Another good practice for mobile is to hide it behind a hint icon. By using hints like a question mark or info icon, you can make it clear that tapping on this icon could open a tooltip.Trends of 2020Advanced animationAnimation can give the user feedback on their actions and add some rhythm to interactions.Advanced animations are animations that are used as part of the branding of a company.It can help you express your brand and build loyalty.Note: A lot of people get motion sickness because of animations. There should always be a way to turn off or minimize animations in your application.        Source: Asha Rajput on DribbblePersonalizationsArtificial Intelligence is rapidly gaining popularity and is being used in everyday applications.Apps like Spotify and Netflix use AI to recommend certain songs or movies.This way of personalization can add value to your application and might make it easier for users to find what they are looking for.        Source: MediumPassword-less loginAs I already mentioned in the part about forms, mobile users don’t like to type.Because of that, new ways of logging in are coming to the surface.Popular alternatives for passwords are: facial or fingerprint recognition, sign-in links via mail or text and using key generators.        Source: Jason Zimdars on DribbbleDark themesDark themes have two essential advantages: they use less battery power and reduce eye strain.A lot of popular apps give you the choice to swap between a light or dark theme or automatically swap between those two during the day or night.Others just go all the way and embrace the dark theme!        Source: Giovanni Piemontese on SmartLauncherSkeleton screensPeople don’t like waiting and you might have done everything to have a fast load time, but it still doesn’t feel like enough.Skeleton screens are being used by big companies like Facebook to give you a feeling of fast loading.Skeleton screens give you a general idea of what the page is going to look like in a wireframe-like design.        Source: MediumAre we done yet?We will never be done redesigning our website for mobile. There’s always something we could do better.You can find many more tips and tricks for mobile design online, but these key points will always help you:  People are lazy, literally! People don’t grab their phone to write out monologues, but just type some emojis and get it over with.  Mobile screens are really small, don’t pack everything on them and keep removing everything you don’t need.  Have you ever heard anyone say “I’ve got fat fingers” except for when they are using a smartphone? Make your tap targets big enough!User testing is one of the most important things to do when improving your user experience.There is always more to learn about mobile design and it is definitely worth looking more into user testing.If you’re looking for UI/UX experts to help you tackle these issues, don’t hesitate to contact my colleagues at ClockWork!Icons on Lux prototype made by Kiranshastry from www.flaticon.com"
      },
    
      "cloud-2020-12-14-chaos-engineering-html": {
        "title": "An introduction into the world of Chaos Engineering",
        "url": "/cloud/2020/12/14/chaos-engineering.html",
        "image": "/img/2020-12-14-chaos-engineering/thumbnail.jpg",
        "date": "14 Dec 2020",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  What is Chaos Engineering?  Principles of Chaos Engineering          Steady state      Hypothesis about state      Vary real-world events      Design and run the experiment      Learn and verify      Improve and fix it        Manually VS Auto  Chaos GameDays &amp; benefits          Post Mortem        Simian Army          Chaos Monkey (Still available as a standalone service)      Janitor Monkey =&gt; replaced by new standalone service ‘Swabbie’ (Still available)      Conformity Monkey =&gt; now rolled out in spinnaker services (Still available)        Other Tools for Chaos Engineering  Conclusion  ResourcesChaos EngineeringIntroductionIn cloud-based distributed networks we need a certain level of scalability and resilience because unpredictable events are bound to happen.Because these networks are more complex and have built-in uncertainty,it’s essential for software developers to utilize an empirical approach to testing for vulnerabilities that’s systematic and innovative.This can be achieved through controlled experimentation that creates chaos in an effort to determine how much stress any given system can withstand.The goal is to observe and identify systemic weaknesses.What is Chaos Engineering?Chaos Engineering is the discipline of experimenting on a software system in production in order to build confidence in the system’s capability to withstand turbulent and unexpected conditions.You can think of Chaos Engineering as an empirical approach to addressing the question: “How close is our system to the edge of chaos?” Another way to think about this is: “How would our system fare if we injected chaos into it?”It is not meant to break random things without a purpose.However if you and your team are just starting with Chaos Engineering and you are not confident enough to work in the production environment,you can also do the experiments in another controlled environment (TST, DEV, …).Building confidence is key! You do not want to break things in production without being able to find a solution.Why use Chaos Engineering?In software development, a given software system’s ability to tolerate failures while still ensuring adequate quality of service,is typically specified as a requirement (Resilience).However, development teams often fail to meet this requirement due to factors such as short deadlines or lack of knowledge of the field.Chaos engineering is a technique to meet the resilience requirement.Chaos engineering can be used to achieve resilience against:  Infrastructure failures  Network failures  Application failuresChaos Engineering and Traditional TestingWith traditional testing, you are only testing assumptions and not generating new knowledge about the system.You are testing the code correctness and how functions and methods work in your application.Chaos Engineering on the other hand will also explore the many different and unpredictable scenarios that could happen to your systems.In this way you will be able to find new weaknesses before the actual event will take place and make sure future outages will not happen.Prerequisites for Chaos EngineeringTo determine whether your organization is ready to start adopting Chaos Engineering, you need to answer one question:Is your system resilient to real-world events such as service failures and network latency spikes?If you know the answer to that question is no, you have some work to do before using Chaos Engineering.Chaos Engineering is great for exposing unknown weaknesses in your production system,but if you are certain that a Chaos Engineering experiment will lead to a significant problem with the system,there’s no sense in running that experiment.Fix the weakness first! Then come back to Chaos Engineering to uncover more weaknesses you didn’t know about.Also, it is important that there are ways your team can analyze the results of the experiments by making sure there is a monitoring system in place to check the state of your application.Some metrics examples:  service metrics          Example: The time it normally takes to start up your application, the time it takes for a request to the service…      Example: a simple metric for us to determine the overall health of the system is the percentage of 200 responses from the User Service, specifically we want 100%.        business metrics          Example: number of orders on your web shop. When doing an experiment where you are increasing the response times of your service by 100 ms, you see that the number of orders has decreased significantly      It’s always a good idea to have some resilience already built in to your application/service before introducing Chaos.Some key points for resilience are:  Principles of Chaos Engineering  Steady stateDefine a measurable steady state that represents normal circumstances to use as a baseline.The reason you do this, is because after injection failure,you want to make sure you can return to a well-known state and the experiment is no longer interfering with the system’s normal behavior.The key is not to focus on internal attributes of the system like CPU, memory, etc. but to look for measurable output.Measurements of that output over a short period of time constitute a proxy for the system’s steady state.The system’s overall throughput, error rates, latency percentiles, etc. could all be metrics of interest representing steady state behavior.Hypothesis about stateOnce you have your metrics and an understanding of their steady state behavior,you can use them to define the hypotheses and preferred results for your experiment.Start small and choose only one hypothesis at a time.When you are doing this, it is important to bring everybody around the table that is involved with the project.The team, the product owner, developers, designers, etc.It can be tempting to subject your system to different events (for example, increasing amounts of traffic) to “see what happens.”However, without having a prior hypothesis in mind, it can be difficult to draw conclusions if you don’t know what to look for in the data.Think about how the steady state behavior will change when you inject different types of events into your system.If you add requests to a service, will the steady state be disrupted or stay the same?If disrupted, do you expect the system output to increase or decrease?A few examples:  What will happen if this load balancer breaks?  What will happen if caching fails?  What will happen if latency increases with 300ms?  What will happen if we lose connection to our DB?Make hypotheses on parts of the system you believe are resilient — after all, that’s the whole point of the experiment.Also think about what the preferred outcome will be in one of these situations and don’t make a hypothesis that you know you will break!Example:  What if the ‘Shop By Category’ service fails to load in our online webshop?The Preferred Outcome:  Should we return a 404?  Should the page gracefully degrade and collapse?  What should happen on the backend?  Should alerts be sent?  Should the failing dependency continue to receive requests every time a user goes to this page?Vary real-world eventsEvery system, from simple to complex, is subject to unpredictable events and conditions if it runs long enough.Examples include increase in load, hardware malfunction,deployment of faulty software, and the introduction of invalid data (sometimes known as poison data).The most common ones fall under the following categories:  Hardware failures  Functional bugs  State transmission errors (e.g., inconsistency of states between sender and receiver nodes)  Network latency and partition  Large fluctuations in input (up or down) and retry storms  Resource exhaustion  Unusual or unpredictable combinations of interservice communication  Byzantine failures (e.g., a node believing it has the most current data when it actually does not)  Race conditions  Downstream dependencies malfunctionReal World Examples:At Netflix, they turn off machines because instance termination happens frequently in the wild and the act of turning off a server is cheap and easy.They simulate regional failures even though to do so is costly and complex,because a regional outage has a huge impact on their customers unless they are resilient to it.Consider an organization that uses a messaging app such as Slack or HipChat to communicate during an incident.The organization may have a contingency plan for handling the outage when the messaging app is down during an outage,but how well do the on-call engineers know the contingency plan?Running a chaos experiment is a great way to find out.Design and run the experiment  Pick one hypothesis  Scope your experiment (the closer you are to production the more you will learn about the results)  Identify the relevant metrics to measure  Notify the organizationPrioritize events either by potential impact or estimated frequency.Consider events that correspond to hardware failures like servers dying, software failures like malformed responses,and non-failure events like a spike in traffic or a scaling event.Any event capable of disrupting steady state is a potential variable in a Chaos experiment.One of the most important things during the experiment phase is understanding the potential blast radius of the experiment and the failure you’re injecting — and minimize it.You’ll almost certainly want to start out in your test environment to do a dry run before you move into production.Once you do move to production, you’ll want to start out with experiments that impact the minimal amount of customer traffic.For example, if you’re investigating what happens when your cache times out,you could start by calling into your production system using a test client, and just inducing the timeouts for that client.Some good questions you can ask yourself to check the blast radius are:  How many customers are affected?  What functionality is impaired?  Which locations are impacted?Also try to have some sort of ‘emergency button’ you can use to cancel the experiment or to return to the normal state of the system in case you cannot find a solution.Be careful with experiments that modify the application state (cache or databases) or that can’t be rolled back easily or at all.Eventually when you start doing Chaos Experiments in your production environment,you want to inform members of your organization about what you’re doing, why you’re doing it and when.Learn and verifyIn order to learn and verify you need to measure. Invest in measuring everything!After the test you can use your collected metrics to check if your hypothesis is correct.Another important metric during this phase, is the time it takes to detect the problem. You do not want your customers to be the ones that detect the problem. So, use Chaos Engineering as a way of testing your monitoring and alerting systems as well.There are several ways that you can expand the testing in order to increase your knowledge and find potential solutions.Once you’ve resolved one area of concern, reset the testing criteria or parameters and run the experiment again with a new hypothesis.You can also expand the blast radius by increments with each test,introducing new or more powerful stressors into the testing environment in order to gauge the limits of your system.The idea is to introduce as much controlled chaos into the mix, one element at a time,in order to determine the maximum limits of your system before it breaks down completely.This can be done by introducing automation after the initial test.Always do a Post Mortem of the experiment.A few questions the team can ask themselves during this phase:  Time to detect?  Time for notification? And escalation?  Time to public notification?  Time for graceful degradation to kick-in?  Time for self-healing?  Time to recovery — partial and full?  Time to all-clear and stable?At AWS, the output from the Post Mortem is called a Correction-of-Errors document, or COE.They use COE to learn from their mistakes, whether they’re flaws in technology, process, or even the organization.They use this mechanism to resolve root causes and drive continuous improvement.The key to being successful in this process is being open and transparent about what went wrong.One of the most important guidelines for writing a good COE is to be blameless and avoid identifying individuals by name.There are five main sections in a COE document:  What happened? (Timeline)  What was the impact to our customers?  Why did the error occur? (The 5 Why’s)  What did you learn?  And how will you prevent it from happening again in the future?Improve and fix itThe most important lesson here is to prioritize fixing the findings of your chaos experiments over developing new features!Get upper management to enforce that process and buy into the idea that fixing current issues is more important than continuing the development of new features.Manually VS AutoUsing Chaos Engineering may be as simple as manually running ‘kill -9’ on a box inside of your staging environment to simulate failure of a service.Or it can be as sophisticated as automatically designing and carrying out experiments in a production environment against a small but statistically significant fraction of live traffic.So, when starting out, it is a good practice to manually make your experiments and check the results,but running experiments manually is labor-intensive and ultimately unsustainable.So try to automate experiments and run them continuously.Chaos GameDays &amp; benefitsChaos GameDays are often known as days where a ‘Master of Disaster’ or a MoD, often in secret,will decide what kind of failure or disaster will happen on the system.He or she will generally start with something simple like the loss of capacity or the loss of connectivity.You may find, that until you can easily and clearly see the simple cases,doing harder or more complex failures is not a good way to build confidence or spend time.If you follow this process regularly, you will see a transformation in your team.Being first on-call for Chaos GameDays builds composure under pressure when doing on-call for production outages.Not only will all developers gain confidence in their uderstanding of the systems and how they fail,but they also get used to the feeling of being under pressure.There will also be a dramatic change in your systems, since developers will experience failure as a part of their job and thus, they will start designing for failure.They consider how to make every change and every system observable and also carefully choose resilience strategies because this is now something the team knows and talks about during the experiments.Planned FailureThe MoD will gather the team before the ‘start of the incident’ and then will start with the planned failure. Normally there will be one member of the team who will be ‘first on-call’.This person is strongly encouraged to contact the other membersso they can start working together and find out what failure the MoD has caused.Ideally, the team will find and solve the issue in less than 75% of the allocated time.When the team has a solution for the issue or the allocated time has ended,the MoD will reverse the failure and the team will proceed to do a Post Mortem of the incident.EscalationIt is also possible that the team will not be able to find a solution for the problem.Then the MoD can escalate this failure to make it more visible,because often full outages are the only observable failures.Knowing this is the first step in fixing your instrumentation and visualization (FE. Dashboards with monitoring…)Post MortemA Post Mortem is the stage in which the team will analyze the failure and the solution.This can consist of sharing perspectives, assumptions that were made, expectations that didn’t reflect the behavior of the system or observability tools.Following out of the Post Mortem,the team should have come up with a set of actions to fix any observability issues for the scenario and some ideas about how to improve resilience to that failure.The Post Mortem should follow the usual incident process if you have one in your company.Simian Army  Netflix has already developed some tools which they bundled in their suite of tools named ‘The Simian Army’.These tools were made to test reliability, security and resilience of its AWS infrastructure.The Simian Army is designed to add more capabilities beyond Chaos Monkey. While Chaos Monkey solely handles termination of random instances,Netflix engineers needed additional tools able to induce other types of failure.Some of the Simian Army tools have fallen out of favor in recent years and are deprecated,but each of the members serves a specific purpose aimed at bolstering a system’s failure resilience.Chaos Monkey (Still available as a standalone service)Chaos Monkey is a tool invented to test the resilience of its IT infrastructure. It works by intentionally disabling virtual machine instances and containers in the production network to test how remaining systems respond to the outage. (prepares you for a random instance failure in an application managed by Spinnaker)This tool has been in the game for a long time, so there might be better tools for your needs.Chaos Monkey is deliberately unpredictable.It only has one attack type: terminating virtual machine instances.You set a general time frame for it to run, and at some point during that time it will terminate a random instance. This is meant to help replicate unpredictable production incidents,but it can easily cause more harm than good if you’re not prepared to respond.Janitor Monkey =&gt; replaced by new standalone service ‘Swabbie’ (Still available)Identifies and disposes unused resources to avoid waste and clutter.Conformity Monkey =&gt; now rolled out in spinnaker services (Still available)A tool that determines whether an instance is nonconforming by testing it against a set of rules.If any of the rules determines that the instance is not conforming, the monkey sends an email notification to the owner of the instance.Chaos Kong (deprecated or not publicaly released)At the very top of the Simian Army hierarchy, Chaos Kong drops a full AWS “Region”. Though rare, loss of an entire region does happen, and Chaos Kong simulates a systems response and recovery to this type of event.Chaos Gorilla (deprecated or not publicaly released)Chaos Gorilla drops a full AWS “Availability Zone” (one or more entire data centers serving a geographical region).Latency Monkey (deprecated or not publicaly released)Introduces communication delays to simulate degradation or outages in a network.Netflix never publicly released the Latency Monkey code, and it eventually evolved into their Failure Injection Testing (FIT) service.FIT (Failure Injection Testing)FIT was built to inject microservice level failures.Latency monkey adds a delay and/or failure on the server side of a request for a given service.This provides us good insight into how calling applications behave when their dependency slows down — threads pile up, the network becomes congested, etc.Latency monkey also impacts all calling applications — whether they want to participate or not,and can result in customer pain if proper fallback handling, timeouts, and bulkheads don’t work as expected.What we need is a way to limit the impact of failure testing while still breaking things in realistic ways.This is where FIT comes in.Doctor Monkey (deprecated or not publicaly released)Performs health checks, by monitoring performance metrics such as CPU load to detect unhealthy instances,for root-cause analysis and eventual fixing or retirement of the instance.Doctor Monkey is not open-sourced, but most of its functionality is built into other tools like Spinnaker, which includes a load balancer health checker,so instances that fail certain criteria are terminated and immediately replaced by new ones.Security Monkey (Still available but will be end-of-life in 2020)Derived from Conformity Monkey, a tool that searches for and disables instances that have known vulnerabilities or improper configurations.10-18 Monkey (deprecated or not publicaly released)A tool that detects problems with localization and internationalization (known by the abbreviations “l10n” and “i18n”)for software serving customers across different geographic regions.Other Tools for Chaos EngineeringChAP (Chaos Automation Platform)ChAP was built to overcome the limitations of FIT so we can increase the safety, cadence, and breadth of experimentation.Byte-MonkeyA small Java library for testing failure scenarios in JVM applications.It works by instrumenting application code on the fly to deliberately introduce faults such as exceptions and latency.ChaosBlade By AlibabaChaosBlade is a versatile tool supporting a wide range of experiment types and target platforms.However, it lacks some useful features such as centralized reporting, experiment scheduling, target randomization, and health checks. It’s a great tool if you’re new to Chaos Engineering and want to experiment with different attacks.Chaos MachineChaosMachine is a tool that does chaos engineering at the application level in the JVM.It concentrates on analyzing the error-handling capability of each try-catch block involved in the application by injecting exceptions.Proofdock Chaos Engineering PlatformA chaos engineering platform that focuses on and leverages the Microsoft Azure platform and the Azure DevOps services.Users can inject failures on the infrastructure, platform and application level.Gremlin platformA “failure-as-a-service” platform built to make the Internet more reliable.It turns failure into resilience by offering engineers a fully hosted solution to safely experiment on complex systems,in order to identify weaknesses before they impact customers and cause revenue loss.Unlike Chaos Monkey, tools like FIT and Gremlin are able to test for a wide range of failure states beyond simple instance destruction.In addition to killing instances, Gremlin can fill available disk space,hog CPU and memory, overload IO, perform advanced network traffic manipulation, terminate processes, and much more.Facebook StormTo prepare for the loss of a datacenter, Facebook regularly tests the resistance of its infrastructures to extreme events.Known as the Storm Project, the program simulates massive data center failures.ChaoSlingrChaoSlingr is the first Open Source application of Chaos Engineering to Cyber Security.ChaoSlingr is focused primarily on performing security experimentation on AWS Infrastructure to proactively discover system security weaknesses in complex distributed system environments.Published on Github in September 2017.Chaos Toolkit by ChaosIQThe Chaos Toolkit was born from the desire to simplify access to the discipline of chaos engineering and demonstrate that the experimentation approach can be done at different levels: infrastructure, platform but also application. The Chaos Toolkit is an open-source tool.Few tools are as flexible in how they let you design chaos experiments.Chaos Toolkit gives you full control over how your experiments operate, right down to the commands executed on the target system.But because of this DIY approach, Chaos Toolkit is more of a framework that you need to build on than a ready-to-go Chaos Engineering solutionMangleMangle enables you to run chaos engineering experiments seamlessly against applications and infrastructure components to assess resiliency and fault tolerance.It is designed to introduce faults with very little pre-configuration and can support any infrastructure that you might have including K8S,Docker, vCenter or any Remote Machine with ssh enabled.With its powerful plugin model,you can define a custom fault of your choice based on a template and run it without building your code from scratch.Chaos Mesh by PingCAPChaos Mesh is an open-source cloud-native Chaos Engineering platform that orchestrates chaos experiments in Kubernetes environments.It supports comprehensive types of failure simulation, including Pod failures, container failures,network failures, file system failures, system time failures, and kernel failures.Chaos Mesh is one of the few open source tools to include a fully-featured web user interface (UI) called the Chaos Dashboard.However, its biggest limitations are its lack of node-level experiments,lack of native scheduling, and lack of time limits on ad-hoc experiments.Litmus ChaosLitmusChaos is a toolset to do cloud-native chaos engineering.Litmus provides tools to orchestrate chaos on Kubernetes to help SREs find weaknesses in their deployments.SREs use Litmus to run chaos experiments initially in the staging environment and eventually in production to find bugs,vulnerabilities. Fixing the weaknesses leads to increased resilience of the system.While Litmus is a comprehensive tool with many useful attacks and monitoring features, it comes with a steep learning curve.Simply running an experiment is a multi-step process that involves setting permissions and annotating deployments.Workflows help with this, especially when used through the Litmus Portal, but they still add an extra layer of complexity.This isn’t helped by the fact that some features—like the Litmus Portal itself—don’t appear in the documentation and are only available through the project’s GitHub repository.Which tool is right for me?Ultimately, the goal of any Chaos Engineering tool is to help you achieve greater reliability.The question is: which tool will help you achieve that goal faster and more easily? This question of course depends on your tech stack, the experience and expertise of your engineering team,and how much time you can dedicate to testing and evaluating each tool.The following table are just a handful of tools which are interesting for our preferred stack.            Tool      platform      Attack types      App Attacks      Container / Pod attacks      GUI ?      CLI ?      Metrics      Attack Sharing      Attack Halting      Attack Scheduling                  Chaos Monkey      Spinnaker      1      true      false      true      false      false      false      false      true              Gremlin      SaaS      11      false      true      true      true      true      true      true      true              Chaos Blade      K8S, Docker, Cloud, Bare metal      40      true      true      false      true      false      false      true      false              Chaos Toolkit      K8S, Docker, Cloud, Bare metal      depends on driver      false      true      false      true      true      false      false      false              Chaos Mesh      K8S      17      false      true      true      true      true      false      true      true              Litmus      K8S      39      false      true      true      true      true      true      true      true      !! Chaos Toolkit is the only tool you can use to create Custom Attacks with !!ConclusionAny organization that builds and operates a distributed system and wishes to achieve a high rate of development velocity will want to add Chaos Engineering to their collection of approaches for improving resiliency.Chaos Engineering is still a very young field, and the techniques and associated tooling are still evolving.Resources  Principles of Chaos  O’Reilly Chaos Engineering paper  Wikipedia  Chaos Monkey  Simian Army  FIT by Netflix  Chaos Automation Platform  How to run a Chaos GameDay  Chaos Engineering post  Gremlin - Chaos Engineering  Gremlin - Chaos Engineering tools  PagerDuty - Post Mortem"
      },
    
      "cloud-2020-12-10-aws-fargate-serverless-deployments-html": {
        "title": "Serverless Kubernetes deployments with AWS Fargate",
        "url": "/cloud/2020/12/10/aws-fargate-serverless-deployments.html",
        "image": "/img/2020-12-10-aws-fargate-serverless-deployments.jpg",
        "date": "10 Dec 2020",
        "category": "post, blog post, blog",
        "content": "Prerequisites  An AWS account  kubectl, aws-cli &amp; eksctl installed  Some Kubernetes knowledgeIntroductionIntroduced in late 2017, AWS Fargate is a compute engine for serverless container deployments. The engine allows you to run containers without worrying about your infrastructure. Node scaling and configuration is done by AWS, which means that you only have to worry about the health of your own resources, not of your Kubernetes infrastructure.Fargate eliminates the struggle of configuring, scaling and initializing nodes on its own.Because as we know, managing clusters / servers / nodes can be a challenging and expensive task.Pricing is based on the CPU and RAM that is allocated to your pod.Instead of paying for compute nodes you pay for resources allocated to your pods which could drastically decrease computing costs.SetupAWS Fargate was initially introduced for ECS in 2017. Later in 2019 they added support for EKS.You can configure your current cluster to integrate Fargate. This requires a Fargate pod execution role (to communicate with different Amazon services), and a Fargate profile to specify which pods should use Fargate.You can also create a new cluster with instant Fargate support. For simplicity, I will initialize a new cluster.eksctl create cluster --name &lt;cluster-name&gt; --region eu-west-1 --fargateThe initialization process takes a while, so don’t wait around for it and find something else to do in the meantime.It shouldn’t take more than 15 minutes.Fargate in actionAfter your cluster has been created, you should see two Fargate nodes already running.$&gt; kubectl get nodes NAME                                                    STATUS    ROLES     AGE       VERSIONfargate-ip-192-168-124-46.eu-west-1.compute.internal    Ready     &lt;none&gt;    5m       v1.18.8-eks-7c9bdafargate-ip-192-168-97-139.eu-west-1.compute.internal    Ready     &lt;none&gt;    5m       v1.18.8-eks-7c9bdaThis is because of the two CoreDNS pods that get spun up after the setup of the cluster.$&gt; kubectl get pods -n kube-systemNAME                     READY     STATUS    RESTARTS   AGEcoredns-58c89c64-pmjh4   1/1       Running   0          12mcoredns-58c89c64-rm4dr   1/1       Running   0          12mEach pod gets its own Fargate node and represents the resources that the pods get in order to successfully function. When you create a new deployment, you will notice that you have a pod in the status pending creation.$&gt; kubectl create deployment spring-boot-docker --image springio/gs-spring-boot-dockerdeployment.apps/spring-boot-docker created$&gt; kubectl get podsNAME                                  READY     STATUS    RESTARTS   AGEspring-boot-docker-6656b9d9fb-h82pk   0/1       Pending   0          5sThis is because Fargate has yet to create a virtual node for the Pod to run in.After a few seconds you can see that a new node has been added to your cluster.$&gt; kubectl get nodes NAME                                                    STATUS    ROLES     AGE       VERSIONfargate-ip-192-168-113-246.eu-west-1.compute.internal   Ready     &lt;none&gt;    16s       v1.18.8-eks-7c9bdafargate-ip-192-168-124-46.eu-west-1.compute.internal    Ready     &lt;none&gt;    52m       v1.18.8-eks-7c9bdafargate-ip-192-168-97-139.eu-west-1.compute.internal    Ready     &lt;none&gt;    52m       v1.18.8-eks-7c9bdaNow that your virtual node has been added you can see that your pod is starting up.$&gt; kubectl get podsNAME                                  READY     STATUS    RESTARTS   AGEspring-boot-docker-6656b9d9fb-h82pk   1/1       Running   0          2m26sConclusionSo right now we have a fully functioning Kubernetes cluster without having to touch or set up any nodes or configuration.Fargate takes away all the stress of maintaining and scaling worker nodes, so you can concentrate on the actual deployment of your application(s) and not worry about cluster resources.Of course, you would still need additional resources to access your application from outside the cluster, but it’s not covered in this post as this does not fall under the Fargate scope."
      },
    
      "architecture-2020-11-25-quite-the-story-html": {
        "title": "That is Quite the Story",
        "url": "/architecture/2020/11/25/Quite-The-Story.html",
        "image": "/img/2020-11-25-Quite-The-Story/storiesmatter.jpg",
        "date": "25 Nov 2020",
        "category": "post, blog post, blog",
        "content": "October marked the 2020 edition of the BA &amp; Beyond conference. As with all conferences being held in these COVID-ridden times, it was a virtual gathering and there were a variety of people from across the globe that came to share their insights, experiences and opinions on topics from their respective fields of knowledge. This ranged from agile practices to data science to process analysis and management. For me the workshop that struck a chord was given by Lori L. Silverman (Partners for Progress). She is a leading authority on decision management and its underlying data analysis techniques and an advocate for business storytelling. This workshop was titled “Facts Tell, Stories Sell”.A New ApproachA typical data analysis project starts with a business sponsor handing a large blob of data to a team of data scientists, of whom he asks to make heads and tails about the numbers in front of them. The sponsor wants actionable insight on which he can act to enhance whatever goals and objectives the sponsor is chasing. What comes naturally to most data scientists is to employ the OSEMN framework. This framework consists of the following five steps:  Obtain: Gather all accessible data from relevant data sources.  Scrub: Clean up and formalize the retrieved data into a format that can be interpreted by a machine or data analyst.  Explore: Detect significant patterns and trends within the available data sets.  Model: Construct models to predict and forecast future data entries.  iNterpret: Utilize the models to gain actionable insight. In essence we apply the models to detect good and bad cause-and-effect sets and use them to duplicate these results.Lori has a SMARTER approach for this type of undertaking. This approach expands on the OSEMN steps. Where OSEMN just stops after handing over the conclusions of the analysis and hopefully providing the much-needed actionable insight, the SMARTER approach actually takes these insights and executes actions based on decisions based on these insights. This is very similar to the realization that DevOps brought to the development world. A project for the development of a solution doesn’t simply stop when the results are delivered but goes further with actually following up on the solution when it is running in production.The steps that make up the SMARTER approach are the following:  Seek Context: Detail the context in which the data science analysis originated.  Manage the Data: Collect all relevant data and organize it in a useful structure.  Assure Confidence: Clean up the data to increase the level of trust that can be had in said data.  Reveal Insights: Determine the insights that can be extracted from the data at hand.  Take a Stand: Formulate decisions on how to proceed with these insights.  Execute Decision: Act on the decisions that have been agreed upon.  Relay Results: Verify and report on the results of the actions to the different stakeholders.Not only does the SMARTER approach have a higher scrabble score, these additional steps in the process tackle some of the common issues with data science projects and why they tend to fall short of what is needed. The first additional step is pivotal to tackling these issues. More often than not it will transpire that data analysis start off on the wrong foot. They get asked a quandary by a sponsor, but the analysts will not have a proper frame in which to place this. The first step of SMARTER determines the context. Why is the sponsor asking the questions he/she is asking? But learning the context and the business value attached to the questions, a more critical view can be taken on the questions themselves. Are we looking at the right things, and are we asked the correct questions to come to the insights needed to improve on the associated business value? Once the big picture is clear, the plan of attack presents itself much more clearly. Or if you would like to hear it in Lori’s own words, check out her YouTube presence.After performing the necessary steps already formulated by OSEMN, we should not only have results of our data analysis. Data in any form or structure isn’t insight. And we need actionable insights if the business value is going to increase. Actionable insight works on three layers: knowledge (taking stock of what we know at present), the current state (what we need to tackle today), and the future state (what we might innovate to improve future dealings). A plan of action should be formulated to act on the insights we have gathered, and decisions should be taken on how to proceed (step T). Once the plan is clear and everyone is aligned, the team should execute the actions dictated by the decisions that have been made (step E), and the results of these actions should be communicated to the different stakeholders (final step R).Business StorytellingThese last steps are where business storytelling comes in. Although it is certainly already useful to set the context, convincing stakeholders about what the most important actionable insights are and what decisions to take and how to proceed, becomes easier and more relatable in the form of a story. The infographic below taken from the Staying Alive UK website shows to power of a good story, and how it is processed by the brain.Constructing such a story is a skill of itself. Whereas it used to be a common pastime for us as a species, with hunters telling tales of their hunts around the campfire, or priests telling entire myths populated by a pantheon of gods and heroes, nowadays most of us have delegated this to a subsection of society: the writers of novels, movies, and music. So, we might have forgotten how to go about crafting a proper story. In essence, every story consists of 5 elements:  The Setting: This is the framework in which the project will take place. It details the planning, the budget associated with the project, the locality of where the project will take place, as well as any other important factors that will have an effect on it.  The Characters: These are the stakeholders that will be participating in the gathering of insight and the making of decisions afterwards. The story should indicate their involvement and what they expect from the conclusion of the story.  The Plot: The plot strings together the events that happen in a story. It paints the roadmap of how the story will progress. This is a listing of all actions the project will undertake to get to actionable insight as well as the actions needed to be executed once decisions have been made.  The Conflict: During the rollout of the project, there is always conflict that takes the center stage. For the story to have a happy ending, we need to outline the obstacles we will face when trying to get to the needed insight.  The Theme: Where the plot lists and strings together the different actions that need to be taken, the theme gives these actions their why. This is the opportunity or problem that is the initial trigger for starting the project. It is the origin or intro for the story that determines how we go about realizing it.Similarities with this way of thinking can be found in the most data driven world we know: the stock market. Where we have the champion of the efficient markets, Eugene Fama, Nobel prize winner for economics, stating that information gets absorbed and reflected by the market instantly, we also have his co-Nobel prize winner, Robert Shiller, sterling professor of Economics at Yale University painting a different picture. Professor Shiller hearkens back to the days of the first illustrious economist, Adam Smith. In his book “Theory of the Moral Sentiment”, Adam Smith expresses that companies are not solely driven by a need to maximize their profit line, but also be the need to be praiseworthy. Not to get praise but be worthy of it. Professor Shiller also elaborates on his beliefs that narratives can help us understand and predict evolutions that will take place, and in doing so help us to better prepare (or make decisions about) for what is to come. This is further detailed in his book “Narrative Economics”.When telling such stories, it is important to keep a positive tone. Research shows that negative news heavily influences the decision-making process. More specifically, it impacts the willingness of individuals to shift away from their respective opinions towards a more fitting decision. A study by Bradley R. Staats, Diwas S. KC, and Francesca Gino titled “Maintaining Beliefs in the Face of Negative News: The Moderating Role of Experience. Management Science” (2017) published several findings on this topic:  Negative news makes people change their views after hearing it.  People who have a great deal of experience on the topic will be less likely to change their decision when confronted with bad news.  Similarly, people who are surrounded with more experienced peers are also less likely to change their opinions in light of such news.  Negative news gets dismissed more quickly when presented to more experienced individuals.ConclusionStories are powerful tools to guide projects that need to gather actionable insight and form the decisions that are needed to move forward and address the initial requirement be it an opportunity or a problem. They give a sense of familiarity to the different participants. They provide a form of abstraction on the complexities of the project that can serve as a reduction of said complexities when communicating with those stakeholders that don’t need to give into the nitty-gritty details. And last but not least, they present a unified way of thinking about the project that helps with the onboarding of the stakeholders as well as the marketing towards external parties."
      },
    
      "monitoring-2020-11-16-monitoring-spring-prometheus-grafana-html": {
        "title": "Monitoring Spring Boot with Prometheus and Grafana",
        "url": "/monitoring/2020/11/16/monitoring-spring-prometheus-grafana.html",
        "image": "/img/2020-11-16-monitoring-spring-prometheus-grafana/thumbnail.jpg",
        "date": "16 Nov 2020",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Prometheus          What is Prometheus?      Why do we need Prometheus?      How it works                  Prometheus server          Prometheus targets          Micrometer                    Configuring Prometheus        Grafana          What is Grafana      Why Grafana        Demo project          Setup Spring Boot                  Adding our own custom metrics                          DemoMetrics class              DemoMetricsScheduler class                                          Setup Prometheus      Setup Grafana                  Adding a custom metric panel                      ConclusionIntroductionIn a distributed landscape where we are working with microservices, serverless applications, or just event-driven architecture as a whole, observability, which comprises monitoring, logging, tracing, and alerting, is an important architectural concern.There are a few reasons why we want visibility in our highly distributed systems:  Issues will occur, even when our best employees have built it.  Distributed systems generate distributed failures, which can be devastating when we are not prepared in advance.  Reveal mistakes early, which is great for improvement and learning.  It keeps us accountable.  Reduce the mean time to resolution (MTTR).In this blogpost I will explain the core concepts of Prometheus and Grafana.In the last section I set up a demo project, so you can follow along and implement monitoring in your own applications.PrometheusWhat is Prometheus?Prometheus, originally developed by SoundCloud is an open source and community-driven project that graduated from the Cloud Native Computing Foundation.It can aggregate data from almost everything:  Microservices  Multiple languages  Linux servers  Windows serversWhy do we need Prometheus?In our modern times of microservices, DevOps is becoming more and more complex and therefore needs automation.We have hundreds of processes running over multiple servers, and they are all interconnected.If we would not monitor these services then we have no clue about what is happening on hardware level or application level.There are many things which we want to be notified about, like:  Errors  Response latency  System overload  ResourcesWhen we are working with so many moving pieces, we want to be able to quickly identify a problem when something goes wrong inside one of our services.If we wouldn’t monitor, it could be very time-consuming, since we have no idea where to look.An example of a failing serviceImagine that one server ran out of memory and therefore knocked off a running service container, which syncs two databases.One of those databases gets used by the authentication service, which now also stops working, because the database is unavailable.  How do you know what went wrong, when your application that depends on the authentication service, now can’t authenticate users anymore?The only thing we would see is an error message: ERROR: Authentication failed.We would need to work backwards over every service, all the way back to the stopped container, to find out what is causing the problem.A better way would be to have a tool which:  Constantly monitors all services  Alerts system admins when something crashes  Identifies problems before they occurPrometheus is exactly that tool, it can identify memory usage, CPU usage, available disk space, etc.We can predefine certain thresholds about which we want to get notified.In our example it could have been that the memory of our failing server would have reached 70% memory usage for more than one hour, and could’ve sent an alert to our admins before the crash happened.How it worksPrometheus serverThe server does the actual monitoring work, and it consists of three main parts:  Storage, which is a time series database.  Data retrieval worker, which is pulling the data from our target services.  Webserver, which accepts PromQL queries to get data from our DB.  Even though Prometheus has its own UI to show graphs and metrics, we will be using Grafana as an extra layer on top of this webserver, to query and visualize our database.Prometheus targetsWhat does it monitor?Prometheus monitors nearly anything. It could be a Linux/windows server, Apache server, single applications, services, etc.It monitors units on those targets like:  CPU usage  Memory/ Disk usage  Request count  Request durations  Exceptions countThe units that we monitor are called metrics, which get saved into the Prometheus time-series database.Prometheus’ metrics are formatted like a human-readable text file.  In this file we can see that there is a “HELP” comment which describes what the metric is, and we have a “TYPE” which can be one of four metric-types:  Counter: how many times X happened (exceptions)  Gauge: what is the current value of X now ? (disk usage, cpu etc)  Histogram: how long or how big?  Summary: similar to histogram it monitors request durations and response sizesCollecting metrics from targetsThere are basically two ways of ingesting metrics into a monitoring system. We can either push the data from our clients to our monitoring system, or we pull the data from the monitoring system.Prometheus is a service which polls a set of configured targets to intermittently fetch their metric values.In Prometheus terminology, this polling is called scraping.There is no clear-cut answer about which one is the best, they both have their pros and cons, but some big disadvantages for pushing data are:  possibility of flooding the network.  risk of package loss.  The data which gets exposed on the endpoint needs to be in the correct format, one which Prometheus can understand.As stated before, Prometheus can monitor a lot of different things, servers, services, databases, etc.Some servers even have a metrics endpoint enabled by default, so for those we don’t have to change anything.For the ones who don’t have an endpoint enabled by default, we need an exporter.ExportersThere are a number of libraries and servers which help in exporting existing metrics from third-party systems as Prometheus metrics.You can have a look at the exporters and integration tools here.On a side note, these tools are also available as Docker images, so we can use them inside Kubernetes clusters.We can run an exporter docker image for a MySQL database as a side container inside the MySQL pod, connect to it and start translating data, to expose it on the metrics endpoint.Monitoring our own applicationIf we want to add our own instrumentation to our code, to know how many server resources our own application is using, how many requests it is handling or how many exceptions occurred, then we need to use one of the client libraries.These libraries will enable us to declare all the metrics we deem important in our application, and expose them on the metrics endpoint.MicrometerTo monitor our Spring Boot application we will be using an exporter named Micrometer.Micrometer is an open-source project and provides a metric facade that exposes metric data in a vendor-neutral format which Prometheus can ingest.  Micrometer provides a simple facade over the instrumentation clients for the most popular monitoring systems, allowing you to instrument your JVM-based application code without vendor lock-in. Think SLF4J, but for metrics.Micrometer is not part of the Spring ecosystem and needs to be added as a dependency. In our demo application we will add this to our pom.xml file.For a deeper understanding, check out our blog post about Micrometer.Configuring PrometheusTo instruct Prometheus on what it needs to scrape, we create a prometheus.yml configuration file.  In this configuration file we declare a few things:  global configs, like how often it will scrape its targets.  we can declare rule files, so when we meet a certain condition, we get an alert.  which services it needs to monitor.In this example you can see that Prometheus will monitor two things:  Our Spring Boot application  Its own healthPrometheus expects the data of our targets to be exposed on the /metrics endpoint, unless otherwise declared in the metrics_path field.AlertsWith Prometheus, we have the possibility to get notified when metrics have reached a certain point, which we can declare in the .rules files. Prometheus has a component which is called the “Alertmanager”, and it can send notifications over various channels like emails, Slack, PagerDuty, etc.Querying our dataSince Prometheus saves all our data in a time series database, which is located on disk in a custom timeseries format, we need to use PromQL query language, if we want to query this database.We can do this via the Prometheus WebUI, or we can use some more powerful visualization tools like Grafana.GrafanaWhat is GrafanaGrafana is an open-source metric analytics &amp; visualization application.  It is used for visualizing time series data for infrastructure and application analytics.  It is also a web application which can be deployed anywhere users want.  It can target a data source from Prometheus and use its customizable panels to give users powerful visualization of the data from any infrastructure under management.Why GrafanaOne of the significant advantages of Grafana are its customization possibilities.It’s effortless to customize the visualization for vast amounts of data.We can choose a linear graph, a single number panel, a gauge, a table, or a heatmap to display our data.We can also sort all our data with various labels so data with different labels will go to different panels.Last but not least, there are a ton of premade dashboard-templates ready to be imported, so we don’t have to create everything manually.Demo projectSetup Spring BootTo demonstrate how to implement Prometheus and Grafana in your own projects, I will go through the steps to set up a basic Spring Boot application which we monitor by using Docker images of Prometheus and Grafana.      Set up a regular Spring Boot application by using Spring Initializr.    Add dependency for Actuator         &lt;dependency&gt;         &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;         &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;     &lt;/dependency&gt;        Add dependency for Micrometer         &lt;dependency&gt;         &lt;groupId&gt;io.micrometer&lt;/groupId&gt;         &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;         &lt;version&gt;1.5.5&lt;/version&gt;     &lt;/dependency&gt;        Expose our needed Prometheus endpoint in the application.properties file    management.endpoints.web.exposure.include=prometheusmanagement.endpoint.health.show-details=alwaysmanagement.metrics.tags.application= MonitoringSpringDemoProject        After this we can run the application and browse to localhost:8080/actuator, where we can see all the available endpoints. The one we need and will use to monitor this application, is localhost:8080/actuator/prometheus.  Adding our own custom metricsWe can also define some custom metrics, which I will briefly demonstrate in this section.To be able to monitor custom metrics we need to import MeterRegistry from the Micrometer library and inject it into our class. This gives us the possibility to use counters, gauges, timers and more.To demonstrate how we can use this, I added two classes in our basic Spring application.DemoMetrics has a custom Counter and Gauge, which will get updated every second through our DemoMetricsScheduler class.The counter gets incremented by one, and the gauge will get a random number between 1 and 100.DemoMetrics class@Componentpublic class DemoMetrics {    private final Counter demoCounter;    private final AtomicInteger demoGauge;    public DemoMetrics(MeterRegistry meterRegistry) {        this.demoCounter = meterRegistry.counter(\"demo_counter\");        this.demoGauge = meterRegistry.gauge(\"demo_gauge\", new AtomicInteger(0));    }    public void getRandomMetricsData() {        demoGauge.set(getRandomNumberInRange(0, 100));        demoCounter.increment();    }    private static int getRandomNumberInRange(int min, int max) {        if (min &gt;= max) {            throw new IllegalArgumentException(\"max must be greater than min\");        }        Random r = new Random();        return r.nextInt((max - min) + 1) + min;    }}DemoMetricsScheduler class@Componentpublic class DemoMetricsScheduler {    private final DemoMetrics demoMetrics;    public DemoMetricsScheduler(DemoMetrics demoMetrics) {        this.demoMetrics = demoMetrics;    }    @Scheduled(fixedRate = 1000)    public void triggerCustomMetrics() {        demoMetrics.getRandomMetricsData();    }}Now we are able to see our custom metrics on the /actuator/prometheus endpoint, as you can see below.  Setup PrometheusThe easiest way to run Prometheus is via a Docker image which we can get by running:docker pull prom/prometheusAfter we download the image, we need to configure our prometheus.yml file. Since I want to demonstrate how to monitor a Spring Boot application, as well as Prometheus itself, it should look like this:global:    scrape_interval:     15sscrape_configs:- job_name: 'prometheus'  scrape_interval: 5s  static_configs:    - targets: ['localhost:9090']- job_name: 'spring-actuator'  metrics_path: '/actuator/prometheus'  scrape_interval: 5s  static_configs:    - targets: ['192.168.0.9:8080']We define two targets which it needs to monitor, our Spring application and Prometheus.Since we run Prometheus from inside Docker we need to enter the host-ip which is in my case 192.168.0.9.Afterwards we can run the Prometheus image by running the following command:docker run -d -p 9090:9090 -v &lt;PATH_TO_prometheus.yml_FILE&gt;:/etc/prometheus/prometheus.yml prom/prometheus We mount the prometheus.yml config file into the Prometheus image and expose port 9090, to the outside of Docker.When this is up and running we can access the Prometheus webUI on localhost:9090.  When we navigate to Status &gt; Targets, we can check if our connections are up and are correctly configured.  Yet again, we can check our custom metrics in the Prometheus UI, by selecting the demo_gauge and inspecting our graph.  Setup GrafanaTo run Grafana we will use the same approach as with Prometheus.We download and run the image from Docker Hub.docker run -d -p 3000:3000 grafana/grafanaNow we can access the Grafana UI from localhost:3000, where you can enter “admin” as login and password.  After we arrive at the landing page, we need to set up a data source for Grafana.Navigate to Configuration &gt; Data Sources, add a Prometheus data source and configure it like the example below.  For this example I used one of the premade dashboards which you can find on the Grafana Dashboards page.The dashboard I used to monitor our application is the JVM Micrometer dashboard with import id: 4701.  Give your dashboard a custom name and select the prometheus data source we configured in step 3.Now we have a fully pre-configured dashboard, with some important metrics showcased, out of the box.  Adding a custom metric panelTo demonstrate how we can create a panel for one of our own custom metrics, I will list the required steps below.First we need to add a panel by clicking on “add panel” on the top of the page, and yet again on “add new panel” in the center.    Then we need to configure our panel, which we do by selecting demo_gauge in the metrics field.To display our graph in a prettier way, we can choose the “stat” type under the visualization tab.    When we click on Apply in the top right corner, our new panel gets added to the dashboard.Afterwards, we can do the same thing for our demo_counter metric.    After going through all of these steps, we now have an operational dashboard which monitors our Spring Boot application, with our own custom metrics.  ConclusionAfter reading this blogpost I hope you can see that using Prometheus as a data aggregator in a distributed system is not really all that hard.It has a lot of client libraries which integrate seamlessly with our infrastructure, services and applications.Using Grafana on top of his to visualize our data, feels like a breeze when we use pre-existing dashboards to quickly get things up and running."
      },
    
      "backend-2020-11-12-typescript-in-a-serverless-backend-with-nestjs-and-azure-functions-html": {
        "title": "TypeScript in a serverless backend with NestJS and Azure Functions",
        "url": "/backend/2020/11/12/Typescript-in-a-serverless-backend-with-NestJS-and-Azure-Functions.html",
        "image": "/img/2020-11-12-Typescript-in-the-backend-with-NestJS-and-Azure-Functions/banner.png",
        "date": "12 Nov 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Setup  ParametersModule  Authorization Guards  Azure Function and CosmosDB  Conclusion  ResourcesIntroductionWhen I asked a colleague to validate my code structure for this blog, he asked me “Why would one use TypeScript in the backend at all?”. He’s a Java programmer and didn’t know TypeScript’s properties very well. An introduction: TypeScript is an asynchronous, functional programming language which compiles down to plain JavaScript. It supports interfaces, classes and access modifiers like private, protected and public.When first using TypeScript, it felt less easy than using Spring Boot, which I had used prior during Java programming.This is where NestJS comes in, a NodeJS framework built for the backend with Object Oriented Programming in mind.If you have worked with a framework like Spring before, NestJS will be quite easy for you to understand.It requires a modular way of working, which makes sure the application stays well organised.In this blog, we take a dive into using NestJS in a serverless application hosted in an Azure Function and connect it with a CosmosDB.Of course, NestJS can be integrated with other serverless services like AWS Lambda and Cognito.However, I found the process of converting this application to a serverless function to be a very smooth solution requiring only one(!) command.SetupI made a little application which can save and return three parameters of your body: weight, fat percentage and muscle percentage.The app is automatically deployed to an Azure function using Azure Pipelines and saves those parameters to a CosmosDB.I made the code available on GitHub for you to learn from, as we won’t touch on everything in the repository in this blog post.The user can send new data to the application, which will keep the history of the three parameters in a CosmosDB.Apart from the main AppModule, I only added two modules, the ParametersModule and the LoggerModule to the application.This blog post won’t explain building the LoggerModule, as it only serves to create a custom logger.The NestJS documentation provides a very clear explanation of how to use a custom logger.The application has two (basic) guards set up for the HTTP calls, one for authorization and one for role-based access. For this guard, we use a simple bearer token, however, integration with eg. Cognito or JWT tokens is available.ParametersModuleA Module in NestJS provides a clear way of organizing the project and enabling clear dependency injection.There are four important things you can mark within a module:  controllers: classes which capture incoming HTTP calls  providers: classes marked with NestJS’s @Injectable(), made available for dependency injection  imports: modules that need to be imported, again for dependency injection  exports: subset of the providers that need to be exported for use in other modulesThe ParametersModule imports three other modules: the previously explained LoggerModule, the ConfigModule and the AzureCosmosDBModule.The ConfigModule is used to be able to access environment variables from a .env file or from the configuration of the Azure Function. Note that this is also possible with a package such as dotenv, however this isn’t very ideal as we would have to access process.env directly every time.Of course, pushing those environment files to Git is bad practice. You can find a .env-sample file in the repository, which is used to show which variables need to be filled in the .env file.@Module({  controllers: [ParametersController],  providers: [      ParametersService,       ParametersRepository,      {        provide: APP_GUARD,        useClass: RolesGuard,      },  ],  imports: [      LoggerModule,      ConfigModule,      AzureCosmosDbModule.forFeature([{dto: ParametersEntity}])  ]})export class ParametersModule {}The first step when receiving an HTTP request to the application is the ParametersController as shown below.This controller will catch all requests on the ‘parameters’ endpoint.Using annotations, you can:  Make a check for the type of incoming request and divide the traffic accordingly. This is similar to the way annotations work in the Spring Framework (eg. @PostMapping).  Customise which HTTP code you want to return on successful calls, as I did with the createParameters method.  Use the @Res() from express to send a completely customised response, however I did not use that here.  Execute Guards before being able to activate the methodLet’s focus on the Post() method. It first asks the parametersService to check if there is an object with the given userName present in the database.We could ask the parametersRepository for this information directly, however, having this layer of abstraction is essential for having cleaner code.If there is no object present yet, it will create a new one, otherwise, it will update the existing one. For this update, it will map the new info to the existing object. Again, we use the parametersService for its abstraction layer, the controller should only be used for methods capturing HTTP calls.@Controller('parameters')export class ParametersController {    constructor(private readonly parametersService: ParametersService, private readonly loggerSerivce: LoggerService) {        this.loggerService.setContext('ParametersController');    }    @Post()    @HttpCode(HttpStatus.CREATED)    @UseGuards(AuthGuard)    async createParameters(@Body() parametersDto: ParametersDto): Promise&lt;ParametersEntity&gt; {        if(!parametersDto || parametersDto.userName || (!parametersDto.bodyWeight &amp;&amp; !parametersDto.fatPercentage &amp;&amp; !parametersDto.musclePercentage)) {            // could implement @Res() from express to send a proper response to say it should at least contain one of the parameters or a userName            return;        }        const existingParams: ParametersEntity = await this.parametersService.getParametersEntityByUserName(parametersDto.userName);        if (existingParams) {            const updatedParams: ParametersEntity = this.parametersService.mapDtoToEntity(parametersDto, existingParams);            return this.parametersService.update(updatedParams);        } else {            return this.parametersService.create(parametersDto)        }    }    @Get(':id')    @UseGuards(AuthGuard)    @Roles('user')    async findOne(@Param('userName') userName: string): Promise&lt;ParametersEntity&gt; {        return this.parametersService.getParametersEntityByUserName(userName);    }    @Get()    @UseGuards(AuthGuard)    @Roles('admin')    async findAll(): Promise&lt;ParametersEntity[]&gt; {        return this.parametersService.getAll();    }}Moving on to the ParametersService, we’ll only take a glance at the create() function.When receiving an HTTP call, it will contain values for at least one of our three parameters.In this method, we just check the values and add them to the respective array. The update field contains the moment that the value gets updated to track the user’s progress over time.The parameters will then be put into a ParametersEntity (Discussed in Azure Function and CosmosDB) and added to the database using the parametersRepository.@Injectable()export class ParametersService {    constructor(private readonly parametersRepository: ParametersRepository) {    }    async create(parametersDto: ParametersDto): Promise&lt;ParametersEntity&gt; {        let bodyweight: BodyweightDto;        let fatPercentage: PercentageDto;        let musclePercentage: PercentageDto;        if (parametersDto.bodyWeight) {            bodyweight = {                weight: Array.from([parametersDto.bodyWeight]),                update: [new Date()]            }        }        if (parametersDto.fatPercentage) {            fatPercentage = {                percentage: Array.from([parametersDto.fatPercentage]),                update: [new Date()]            }        }        if (parametersDto.musclePercentage) {            musclePercentage = {                percentage: Array.from([parametersDto.musclePercentage]),                update: [new Date()]            }        }        return this.parametersRepository.create(new ParametersEntity(parametersDto.userName, bodyweight, fatPercentage, musclePercentage));    }    // More code}The create() function in the ParametersRepository adds the date the object was created and adds it to the database.We see a good example of the loggerService here too.First, we set the context to ‘ParametersRepository’, so that, when it logs something, it will show that the log came from this class.This way, logs can easily be retraced to its origin.@Injectable()export class ParametersRepository {    constructor(@InjectModel(ParametersEntity) private readonly container: Container, private loggerService: LoggerService) {        this.loggerService.setContext('ParametersRepository');    }    async create(item: ParametersEntity): Promise&lt;ParametersEntity&gt; {        item.createdAt = new Date();        const response = await this.container.items.create(item);        this.loggerService.verbose(`Create RUs: ${response.requestCharge}`);        return response.resource;    }Authorization GuardsThe project uses two guards, an AuthGuard for the authorization, and a RolesGuard to check which roles can access certain resources.A good explanation of both can be found in the NestJS documentation.The RolesGuard is almost an exact copy from the documentation, so let’s take a look at the AuthGuard which doesn’t need to be provided from a module.The canActivate() method is called before executing the method in the controller.It needs to return true, or the method won’t execute and the application will return a 401 Unauthorized code.In this case, we check if the authorization header has the correct value as configured in the environment variables.Other setups, like OAuth, Cognito or JWT tokens are also possible.@Injectable()export class AuthGuard implements CanActivate {    constructor(private readonly configService: ConfigService) {}    async canActivate(context: ExecutionContext): Promise&lt;boolean&gt; {        const req = context.switchToHttp().getRequest();        if (!req.headers.authorization) {            return false;        }        if (req.headers.authorization.split(' ')[0] !== 'Bearer') {            throw new HttpException('Invalid token', HttpStatus.FORBIDDEN);        }        const token = req.headers.authorization.split(' ')[1];        return token === this.configService.get&lt;string&gt;('BEARER_TOKEN');    }}Azure Function and CosmosDBTo convert this app into an Azure Function and make it serverless, we only need a single command:nest add @nestjs/azure-func-httpThis will add some files and folders, including a main.azure.ts through which your app can be started.It will set a global prefix ‘api’ to all your controllers, the standard for Azure Functions.export async function createApp(): Promise&lt;INestApplication&gt; {  const app = await NestFactory.create(AppModule, new AzureHttpRouter());  app.setGlobalPrefix('api');  await app.init();  return app;}Now, you can choose to either run your app as a normal Web App or a serverless Azure Function.The only thing left to do is create an Azure Function in the Portal and set up a pipeline, which is also an automatic process (on Azure DevOps).This will generate an azure-pipelines.yml file containing all necessary information and connect it with the function automatically.On every push to the master branch (pull request), it will automatically start a build and deploy process.For the environment variables, they need to be set up within the ‘configuration’ tab of your function, and then, you’re all set.’Congratulations! You converted your app to a serverless Function!Quite an easy conversion, wasn’t it?The database connection is just as easy.Again, in the portal, you can create a CosmosDB Account.In that account, go to ‘Data Explorer’ and create a new database and add the necessary variables to the configuration of the Function.In the end, your app.module.ts should look like this.Notice that we can’t use the ConfigService in this @Module(), as it needs to be initialised before usage.@Module({  imports: [      ConfigModule.forRoot(),      ParametersModule,      LoggerModule,      AzureCosmosDbModule.forRoot({          dbName: process.env.DATABASE_NAME,          endpoint: process.env.DATABASE_ENDPOINT,          key: process.env.DATABASE_KEY,      })  ],  controllers: [      AppController  ],  providers: [      AppService,  ],})export class AppModule {}That’s it!Your app is now fully functional!ConclusionIn this blog post, we made a small application to discover how NestJS can be used in the backend with some of its neat features.Of course, this was a very basic program to show some of the possibilities.For more information on NestJS and its features, check out the very thorough documentation.Resources  The body parameter tracker project  NestJS docs  AWS Lambda integration  AWS Cognito integration"
      },
    
      "iot-2020-11-06-join-2020-html": {
        "title": "JOIN 2020",
        "url": "/iot/2020/11/06/JOIN-2020.html",
        "image": "/img/2020-11-06-JOIN-2020/banner.jpg",
        "date": "06 Nov 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Our speakers  Tech Track sessions  Agile and Business Track Sessions  In the spotlight Ghelamco alert  In the spotlight Some pitfalls of AI  ResourcesIntroductionAs is annual tradition, even despite Corona, we held our 8th JOIN event in the beginning of October.JOIN stands for JWorks Open Innovation &amp; Networking and aims to bring interesting talks on a wide variety of topics.This years edition was a bit different being that large gatherings of people are a no-no, so we opted for an online only edition.    We stuck to our dual track approach to provide some interesting sessions for everybody:  A Tech track consisting of 4 sessions:          Ghelamco alert: Combining Raspberry Pi and AWS Cloud to get the best of both worlds - Bas Moorkens      Infrastructure is code with the AWS Cloud Development Kit (AWS CDK) - Kevin Azijn      Moving a complex and slow deployment pipeline to a streamlined and lightning fast Azure deployment - Pieter Vincken      Some pitfalls of AI - Joachim Ganseman        An agile and business track also consisting of 4 sessions:          Dancing the BOSSA nova – how to bring a culture of experimentation into your company - Edwin Burgers &amp; Maryse Meinen      The power of Kata - Michaëla Broeckx      Failure Culture – what it means to fail and how we can gain value from it - Anke Maerz      Why don’t you get my code? - Francis Laleman      Read and watch on down below as we go over each speaker and talk.Our speakersLike any event we depend on our contributors, in our case being our speakers.Without them our event would not be possible, they provide us with the awesome and interesting sessions.Bas MoorkensBas is a cloud platform architect at Ordina Belgium who is fascinated by AWS, containers and pipeline automation. He started off as a Java full stack developer but got more and more into the DevOps and cloud world where he currently spends most of his time.Company: JWorks - Ordina BelgiumKevin AzijnAfter spending more than 10 years as a Software Engineer and Tech Lead in both consulting and financial world, Kevin got introduced to public cloud and started building web/mobile solutions and managing DevOps teams on AWS for the Flemish Government. After several years the call of technology over management became too loud to ignore and Kevin decided to join AWS as a Solutions Architect. At AWS Kevin is supporting customers with building a vision and architecting for the future.Company: Amazon Web ServicesPieter VinckenPieter Vincken is a Cloud Automation Engineer with a strong interest in anything related to Cloud Native. He likes to optimize development workflows, from Ideation until code running in production, by enabling CI/CD to be fully automated. Any solutions he creates, will have started as an architectural drawing.Company: JWorks - Ordina BelgiumJoachim GansemanJoachim Ganseman is a computer scientist and has a history as a PhD student at the University of Antwerp, with side jumps to Queen Mary University in London and Stanford University, focusing on digital signal processing, machine learning and audio analysis. Since 2018, he has been working at Smals Research where he focuses on AI-related topics, including Natural Language Processing and Conversational Interfaces, and their potential applications in governmental contexts. In addition to his work, he is an excellent pianist, and as co-founder and organiser of the Belgian Informatics Olympiad, he received the annual prize for science communication from the Royal Flemish Academy of Sciences of Belgium in 2016.Company: Smals ResearchEdwin Burgers &amp; Maryse MeinenEdwin is a Scrum Master, agile coach and leader with 10+ years in the agile field. He has supported many teams and organisations to improve value delivery. To survive in thecurrent highly competitive world agile frameworks, practices and leadership are not enough. Less hierarchy, more autonomy and a higher involvement from thepeople on the floor in strategy and decision making  are crucial. That´s why he is focussing on ways to address these aspects in his work as a coach or leader.Maryse is a 100% scrum master, helping people and organisations increase their agility, so that they can reduce their (organisational) suffering. She coaches teams and executives in solving their own challenges and impediments in their work. As a true corporate rebel, the Socratic method for better conversations is key in her approach. More ancient philosophy should be compulsory in the agile world. And she is not dogmatic: all should use scrum.Maryse and Edwin worked together at Ordina from 2015 and started their own business in Practical Agile in 2018.Company: Practical AgileMichaëla BroeckxMichaëla is an Agile coach with a focus on value and the human aspects of work. She is devoted to Agile &amp; Scrum, not just as a mindset and framework for collaborative product delivery, but also for their general quality as ways to unlearn innate/inbred habits that prevent us from learning efficiently. A true Agile mindset enables enterprises to break down siloes, and build professional human networks throughout the company and beyond.Company: AgileWorks - Ordina BelgiumAnke MaerzI am Anke and I have a mission in my life: To see and enjoy beauty in life, and to help other people to see and enjoy it, too.As a Scrum Master and Agile Coach I live this mission by asking myself every morning THE question: What is it that keeps these amazing people from achieving their goals? – And whatever my answer is, that is where I want to support. Because that is where I can create space for them to see and enjoy the beauty of life.This has proven very purposeful to me since 2014, when I first got introduced to Agile – and through various big and huge companies in Belgium as well as Germany.Before getting into Agile I’ve been project manager and requirements engineer for 2 years, and before that I studied mathematics and psychology at the university of Tübingen (crazy mix? YES! Beautiful, right?!!)Even before that time I’ve already been practicing to be a human, which has proven to be most useful on a personal as well as professional level!Company: AgileWorks - Ordina BelgiumFrancis LalemanToday, I mainly work as a designer of cooperative learning processes and learning cultures. I am a gentle facilitator of (Agile) transformation processes, a train-the-trainer, a cooperative learning facilitator and an Agile coach.Having spent so much time in the Middle East and in the Indian subcontinent, I have now been turning my attention to Japan - studying TPS, Lean and Kata, but also the less obvious, such as ikigai, Taoism and Zen Buddhism, or the shuhari learning process in traditional Japanese craftsmanship.Most probably, however, you will find me in the garden.Company: Agile &amp; Learning Beyond BordersI would like to thank our speakers once more from the whole of Ordina for the interesting sessions they have given.Tech Track sessionsSession 1 - Ghelamco alert: Combining Raspberry Pi and AWS Cloud to get the best of both worlds - Bas Moorkens    In this talk we will present our Ghelamco alerting solution which runs on a Raspberry Pi and is empowered by the AWS cloud to provide additional capabilities.Come discover how we turned our RPI into a managed device by leveraging the AWS cloud.We will demonstrate the power of this technology set by first covering the scenario where the solution is not connected to the internet.In the second part we will do a deep dive on the scenario where the RPI is connected to the internet.In this part we will show you how we can leverage more services of the AWS cloud to bring out the full potential of this solution, effectively giving us the best of both worlds.Session 2 - Infrastructure is code with the AWS Cloud Development Kit (AWS CDK) - Kevin Azijn    The AWS Cloud Development Kit (AWS CDK) is an open source software development framework to model and provision your cloud application resources using familiar programming languages.Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domain-specific languages.AWS CDK uses the familiarity and expressive power of programming languages for modeling your applications.It provides you with high-level components that pre-configure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert.AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation.It also enables you to compose and share your own custom components that incorporate your organization’s requirements, helping you start new projects faster.Session 3 - Moving a complex and slow deployment pipeline to a streamlined and lightning fast Azure deployment - Pieter Vincken    How to automate application platform deployments.Tips and tricks about a migration from an on-premise solution onto Microsoft Azure.Want to know how we went from 6 week lead times on a development environment to only 90 minutes and a single push of a button?Session 4 - Some pitfalls of AI - Joachim Ganseman    The hype surrounding AI and machine learning creates high expectations.There is a strong focus on software that ultimately has to make autonomous decisions and relies on difficult to access and complex statistical conclusions from large amounts of data.This comes with some risk: errors can creep in all steps from design to rollout, which can thoroughly mess up the end result.In addition to the spectacular AI success stories of recent years, there were also quite a few spectacular mistakes.In this webinar we highlight that downside: what can go wrong in an AI project and what should you pay attention to?The agenda includes: bias and fairness, adversarial attacks, explainability, as well as concerns about security, privacy and ethics.Agile and Business Track sessionsSession 1 - Dancing the BOSSA nova – how to bring a culture of experimentation into your company - Edwin Burgers &amp; Maryse Meinen    In the VUCA world that we are in, companies are expected to be flexible and both rapidly responsive and resilient to change, which basically asks them to be agile.The authors of the book BOSSAnova, Jutta Eckstein and John Buck saw 4 recurring problems at the agile transformations they were involved in, where they saw that all that comes with the agile frameworks, principles and practices is not sufficient for company-wide agility.They combine Beyond Budgeting, Open Space, Sociocracy and Agile in practical guide for creating company-wide agility, in a probe-sense-respond manner.This workshop gives a short introduction to BOSSAnova and how it can help to start probes and experiments in your own organization.With an approach and a lot of good practice experiments done in other organizations.Session 2 - The power of Kata - Michaëla Broeckx    We will jointly look for efficient ways to convert good intentions into value, by investigating some personal challenges and how we succeeded or failed.We will examine these techniques and connect them to how our brain works.For every challenge we take on, our human tendency to take the easiest path (the one we imagine to be right) can be counterbalanced by deliberate scientific thinking.Session 3 - Failure Culture – what it means to fail and how we can gain value from it - Anke Maerz    I don’t like failing.I want to achieve a goal, I invest my time and energy, I am looking forward to achieving this goal, I am getting closer, closer - and fail.The goal is not achieved and I want to retreat into a dark corner and just forget.This is a perfectly normal human reaction.Yet, as we all know, no big endeavor happens without its fair share of failure - some huge achievements even happen out of failure alone…Hence, there is something useful to it.Something that even smells like success!This talk analyses different projects, their main impediments and success or failure factors, includes some interactive parts - and in the end (hopefully) leaves us with some renewed courage to face the failures in our lives… to gain value from them.Session 4 - Why don’t you get my code? - Francis LalemanVideo under review - will be added soon!This is a playful, interactive session on the concept of “code” - as a formal, grammar-and-syntax-driven substitute for “meaning”.We all know that code is the central edifice of what it is that we are doing - at work, and, perhaps, even in life.But do we fully understand the profound relationship between code and meaning? Do we really grasp how code, in all its formality and conventions, both pins us down into the confinement of “knowns”, and has the power to liberate us from what is already known and drive us forward?After this session, expect to be aware of more than just “code”, and to be able to understand “meaning”, in life and work, through a variety of grammars and syntaxes, more diverse than ever imagined.In the spotlight Ghelamco alertThe awesome internship project by Kevin Govaerts is a shining example of an end-to-end integrated IoT project.It touches a lot of interesting technologies and integrates everything very nicely and includes managing a Raspberry Pi device via AWS.The project provides a solution to our parking problem at our Ghelamco office in Ghent.Since the parking lot needs to be vacated whenever there is a “home” soccer match or we get a hefty fine per car that remains, we needed an automated solution to warn the people at the office.This project does just that, end-to-end.It reads &amp; scrapes the data and determines when the matches are and commands the Raspberry Pi to light up the alarm light.It uses at least 10 different AWS services and the usage of AWS Greengrass allows you to deploy production ready applications to edge devices without having to manage it yourself.I wont go into all the technical details since you should absolutely read Bas and Kevin’s their excellent blog post about the project!Be sure to check out the blog post and the video of the project!In the spotlight Some pitfalls of AIAs I myself am very interested in Machine Learning and Artificial Intelligence, this talk in particular peaked my interest.Machine Learning and AI are being portrayed as the silver bullet, as the thing every project needs.However developing a Machine Learning model/system is not as straightforward as one might think.This talk goes into detail about what can go “wrong” and what the common pitfalls are when developing such a system.Datasets can be a tricky matter to fully grasp and get right.Hidden correlations or biases make that our trained models can often show the same discriminatory tendencies like normal people do.Removing biases from a dataset is hard, very hard.Even if you think you are not biased, please go on and go over the list of cognitive biases.Removing certain data fields/variables from the dataset in order to prevent biases can have little to no effect due to the fact that there are hidden variables.The talk explains this with a great example, when a model has a gender bias one might think that removing the gender from the dataset might solve the issue.This might not work since other variables might indirectly propagate the same gender bias.If the model handles text, the gender is actually also encoded in the vocabulary and way of speaking.It is very different between women and men.Machine Learning models are vulnerable to attacks:  Data poisoning attacks          When people can enter data or find a way to add data to a source that is being used to train a model.      Verifying and cleaning data is very important to prevent this!        Adversarial attacks          Changing a few pixels in an image might throw off the model completely.      Because the training dataset is so large the model becomes sensitive to small and very particular changes in the input.      These attacks makes using these models for very critical things like autonomous driving tricky and potentially dangerous.While this problem is not fully solved yet, the field is advancing at an extremely high tempo, so these type of problems pose less and less of an issue.Machine Learning can be, and is used, for less legal &amp; nice practices:  Spear &amp; laser phishing          Phishing attacks that use very specific data collected on the target to believe extremely believable phishing vectors.        Deepfakes          Fake news becomes harder and harder to detect because images and even videos can be altered.Faces can be swapped, even audio can be changed to match the voice of someone else.            GPT3          This model can write full texts, code, layouts.Often the text that is generated makes more sense than anything a certain American president would say.            Recommendation engines          Watch a certain type of video, get suggested more of the same but more extreme.This enables a feedback loop, the type of suggested content is in your area of interest just a tad more extreme/strange/unbelievable, so you keep watching, and the next suggestions rinse and repeat the cycle.This is a big issue since it has a tendency to propagate extremes.What you see is not what other people see, it is curated for you by your past consumed content.More than ever it is extremely important to check facts and do some digging on independent sites/new sources.      Resources  JOIN 2020 website  Tech Track playlist  Agile &amp; Business playlist  Ordina Belgium  Practical Agile  Amazon Web Services  Smals Research  Agile &amp; Learning Beyond Borders"
      },
    
      "rest-2020-10-12-resttemplate-vs-webclient-html": {
        "title": "Switching from RestTemplate to WebClient: A Reactive Tale",
        "url": "/rest/2020/10/12/RestTemplate-vs-WebClient.html",
        "image": "/img/2020-09-11-resttemplate-vs-webclient/banner.jpg",
        "date": "12 Oct 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Comparison of RestTemplate and WebClient          RestTemplate      WebClient      Comparison Conclusion                  RestTemplate Summary          WebClient Summary                      Reactive Approach with WebClient          Introduction to Reactive Streams      Legacy Services in your Reactive Environment      Reactive Database Connections      R2DBC: 2 steps forward 1 step back        End-to-end Reactive example          Recipe Service (Reactive R2DBC)      Ingredient Service (Reactive R2DBC)      BestMenuEverGenerator Service (Reactive Rest)      Angular Webapp Consumer        ConclusionIntroductionSince the REST era, most developers have become used to working with Spring’s traditional RestTemplate from the package spring-boot-starter-web for consuming Rest services. Spring also has a WebClient in its reactive package called spring-boot-starter-webflux. This post will help you decide whether you should make the switch from RestTemplate to WebClient. Since WebClient is supposed to be the successor of RestTemplate, we will be looking into it a bit deeper.Comparison of RestTemplate and WebClientFirst off, let us assume we have a Recipe Rest service which we will consume in the following examples.RestTemplateRestTemplate provides a synchronous way of consuming Rest services, which means it will block the thread until it receives a response. RestTemplate is deprecated since Spring 5 which means it’s not really that future proof.First, we create a Spring Boot project with the spring-boot-starter-web dependency.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;We can now inject the default RestTemplateBuilder bean (provided by Spring) in our service and build our RestTemplate with a base URL, or we could create a configured RestTemplate bean in a @Configuration file. With this builder we can also configure things like: maximum data size, message converters for SOAP, etc.@Servicepublic class RecipeRestTemplate {    private final RestTemplate restTemplate;    @Autowired    public RecipeRestTemplate(RestTemplateBuilder builder) {        this.restTemplate = builder                .rootUri(\"http://localhost:8080\")                .build();    }}Now, let’s move on to some example basic methods we can use on the RestTemplate class to communicate with our Recipe Rest service. We apply CRUD operations, specify our return object’s class, some parameters, body, header, etc.public List&lt;Recipe&gt; getRecipes() {    return restTemplate.exchange(\"/recipe\", HttpMethod.GET, null, new ParameterizedTypeReference&lt;List&lt;Recipe&gt;&gt;() {})            .getBody();}public Recipe getRecipeById(int id) {    return restTemplate.getForObject(\"/recipe\", Recipe.class, id);}public Recipe createRecipe(Recipe recipe) {    return restTemplate.postForObject(\"/recipe\", recipe, Recipe.class);}public void deleteRecipe(int id) {    restTemplate.delete(\"/recipe\", id);}public Recipe getRecipeByTitle(String title) {    Map&lt;String, String&gt; requestParameters = new HashMap&lt;&gt;();    requestParameters.put(\"title\", title);    return restTemplate.getForObject(\"/recipe\", Recipe.class, requestParameters);}WebClientWebClient exists since Spring 5 and provides an asynchronous way of consuming Rest services, which means it operates in a non-blocking way. WebClient is in the reactive WebFlux library and thus it uses the reactive streams approach. However, to really benefit from this, the entire throughput should be reactive end-to-end. Let me first show you an example before diving into more details.So, we create a Spring Boot project with the spring-boot-starter-webflux dependency.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;&lt;/dependency&gt;We can inject a builder similarly, configure it if necessary and build our WebClient.@Servicepublic class RecipeWebService {    private final WebClient webClient;    @Autowired    public RecipeWebService(WebClient.Builder builder) {        this.webClient = builder                .baseUrl(\"http://localhost:8080\")                .build();    }}Here is an example similar to our RestTemplate example. Note that this wraps our objects in Mono (a stream of 0 or 1 object) and Flux (a stream of 0 or multiple objects) wrappers. These are reactive types, and we should keep them in these wrappers if we want to keep the reactive stream open and non-blocking. Let’s assume for this example that our Recipe Rest service which we are consuming is reactive.public Flux&lt;Recipe&gt; getRecipes() {    return webClient.get().uri(\"/recipe\")            .retrieve()            .bodyToFlux(Recipe.class);}public Mono&lt;Recipe&gt; getRecipeById(int id) {    return webClient.get().uri(\"/recipe/{id}\", id)            .retrieve()            .bodyToMono(Recipe.class);}public Mono&lt;Recipe&gt; createRecipe(Mono&lt;Recipe&gt; recipe) {    return webClient.post().uri(\"/recipe\")            .body(recipe, Recipe.class)            .retrieve()            .bodyToMono(Recipe.class);}public Mono&lt;Void&gt; deleteRecipe(int id) {    return webClient.delete().uri(\"/recipe/{id}\", id)            .retrieve()            .bodyToMono(Void.class);}public Mono&lt;Recipe&gt; getRecipeByTitle(String title) {    Map&lt;String, String&gt; requestParameters = new HashMap&lt;&gt;();    requestParameters.put(\"title\", title);    return webClient.get().uri(\"/recipe\", requestParameters)            .retrieve()            .bodyToMono(Recipe.class);}Another benefit of working with Flux and Mono is that you can do mappings, filtering, transformations on your data as it is passing through the stream.public Flux&lt;String&gt; getRecipes() {    Flux&lt;Recipe&gt; recipeStream = webClient.get().uri(\"/recipe\")            .retrieve()            .bodyToFlux(Recipe.class);    Flux&lt;String&gt; recipeTitleStream = recipeStream            .log()            .filter(recipe -&gt; !recipe.getTitle().isBlank())            .flatMap(recipe -&gt; Mono.just(recipe.getTitle().toUpperCase()));    return recipeTitleStream;}Let’s say our services, databases, etc are not reactive, but we want to use WebClient anyway. Then Flux and Mono are not much use to us, so we will have to unwrap them. Since our Recipe Rest service doesn’t provide reactive streams, we receive a List of recipes in one response, which WebClient wraps in Mono. We can use block() to block the stream and get the data out of it. Note that this shouldn’t be used in a reactive environment.public List&lt;Recipe&gt; getRecipes() {    Mono&lt;List&lt;Recipe&gt;&gt; recipeListStream = webClient.get().uri(\"/recipe\")            .retrieve()            .bodyToMono(new ParameterizedTypeReference&lt;List&lt;Recipe&gt;&gt;() {});    List&lt;Recipe&gt; recipeList = recipeListStream.block();    return recipeList;}Now let’s say, the Recipe service IS reactive and returns a stream of Recipe objects (Flux) instead of a List, but we still want to block the reactive stream. Because we can only call blockFirst() or blockLast() on Flux, we should collect the data from the Flux stream into List. Now this List is wrapped inside a Mono stream. We can then block the stream to unwrap the data.public List&lt;Recipe&gt; getRecipes() {    Flux&lt;Recipe&gt; recipeStream = webClient.get().uri(\"/recipe\")            .retrieve()            .bodyToFlux(Recipe.class);    Mono&lt;List&lt;Recipe&gt;&gt; collectedRecipesStream = recipeStream.collectList();    List&lt;Recipe&gt; recipeList = collectedRecipesStream.block();    return recipeList;}Comparison ConclusionWe have learned that RestTemplate is in maintenance mode and probably will not be supported in future versions. Even on the official Spring documentation, they advise to use WebClient instead. WebClient can basically do what RestTemplate does, making synchronous blocking calls. But it also has asynchronous capabilities, which makes it interesting. It has a functional way of programming, which makes it easy to read as well. If you are working in legacy Spring (&lt; 5.0) applications, you will have to upgrade to Spring 5.0 to be able to use WebClient.Let’s list both clients’ properties to have a better overview.RestTemplate Summary  In maintenance mode since Spring 5.0  Synchronous (blocking)  In spring-boot-starter-web library  Built on Servlet stackWebClient Summary  Synchronous &amp; asynchronous (non-blocking) capabilities  Reactive streaming  Mono and Flux: both implement CorePublisher which extends Publisher  From Spring 5.0  Functional programming  In spring-boot-starter-webflux library  Built on Reactive stackNext, we will look more into the WebClient’s reactive streaming capabilities.Reactive Approach with WebClientIntroduction to Reactive StreamsReactive streaming works a bit different under the hood than traditional request-response communication. Data only passes through these streams when we block or subscribe them. Calling one of these methods we saw earlier, only creates a connection to the stream. Data only starts passing through the entire stream when there is a ‘subscription’ or ‘block’.Note that you should never block a reactive stream if you want your services to be reactive end-to-end.In the next sections, we will see how we can consume it.Legacy Services in your Reactive EnvironmentTo use WebClient to its full potential, you should create end-to-end reactive streams. This might be a problem when working with legacy services. If you are creating fully new services, back-to-front, and only a small portion of the services you consume are legacy. Then you can still create your new services reactive and wrap the responses of your legacy services in Flux and Mono wrappers as close to the source as possible.public Flux&lt;Recipe&gt; getRecipes() {    List&lt;Recipe&gt; recipeList = webClient.get().uri(\"/recipe\")            .retrieve()            .bodyToMono(new ParameterizedTypeReference&lt;List&lt;Recipe&gt;&gt;() {            })            .block();    Flux&lt;Recipe&gt; recipeStream = Flux.fromIterable(recipeList);    return recipeStream;}This way, you can still partly have a reactive setup and maybe when these services will be upgraded or replaced in the future, they might become reactive as well, and you can easily implement the necessary changes on your side as well.Reactive Database ConnectionsThere have been supporting libraries for reactive NoSQL connectivity for a while, which can retrieve data from the database in a reactive manner. Which means they start returning data as the database is querying, and not when the entire database has been queried. So you are good to go. But, with Relational databases (which you are probably using as well), there weren’t any in Spring until recently. Before, you could wrap your database response in Flux and Mono wrappers as soon as possible when calling your database, but if these databases are the major sources of your data and not just a small portion, it kind of ruins the appeal and benefit.R2DBC: 2 steps forward 1 step backNot until very recently (December 2019), spring-data-r2dbc dependency got released. It basically is JDBC’s reactive counterpart. R2DBC stands for Reactive Relational DataBase Connectivity. This does not offer a lot of features of ORM (Object-Relational Mapping) frameworks at the moment of writing. For now, it is not yet capable of mapping relations (eg: @OneToMany, @ManyToOne, …), lazy loading, etc. You would have to manage this yourself. However, with this connector, we can query data as streams.End-to-end Reactive exampleWe’ve learned about WebClient, reactive streams and R2DBC. Let’s use these in an example to really see how it works. We are going to create a 2 microservices with a R2DBC connecting to MySQL databases and a Rest API. Then we’ll create another microservice which consumes both of these Rest API’s. Finally, an Angular frontend that consumes that microservice to display the data.Recipe Service (Reactive R2DBC)Let’s dive into our Recipe Rest service and see how we can create a reactive rest service. We add the following dependencies.&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-webflux&lt;/artifactId&gt;    &lt;/dependency&gt;        &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-data-r2dbc&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;dev.miku&lt;/groupId&gt;        &lt;artifactId&gt;r2dbc-mysql&lt;/artifactId&gt;        &lt;scope&gt;runtime&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;mysql&lt;/groupId&gt;        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;        &lt;scope&gt;runtime&lt;/scope&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;We configure the database connection and enable R2DBC. The ConnectionFactory comes from the package io.r2dbc.spi. Since we are connecting to a MySQL database, we enter mysql as the Driver. This will create the following connection url r2dbc:mysql://root:password@127.0.0.1/recipe.import io.r2dbc.spi.ConnectionFactories;import io.r2dbc.spi.ConnectionFactory;import io.r2dbc.spi.ConnectionFactoryOptions;// Other imports are omitted@Configuration@EnableR2dbcRepositoriespublic class ReactiveDatabaseConfig extends AbstractR2dbcConfiguration {    @Bean    @Override    public ConnectionFactory connectionFactory() {        return ConnectionFactories.get(ConnectionFactoryOptions                .builder()                .option(DRIVER, \"mysql\")                .option(HOST, \"127.0.0.1\")                .option(USER, \"root\")                .option(PASSWORD, SUPER_SAFE_PASSWORD)                .option(DATABASE, \"recipe\")                .build());    }}For simplicity of the example, a Recipe contains only an id, title and description.public class Recipe {    @Id    private Integer id;    private String title;    private String description;/* Constructors, Getters and Setters not displayed for simplicity */}We create a Repository which extends a ReactiveCrudRepository with a query that retrieves all recipes in a random order.public interface RecipeRepository extends ReactiveCrudRepository&lt;Recipe, Integer&gt; {    @Query(\"SELECT * FROM recipe ORDER BY RAND()\")    Flux&lt;Recipe&gt; findAllRandomized();}We add a RestController with a method that provides a text/event-stream of all the recipes. I have added a delay of 1 second between each element to give you a better visualization later.@RestController@RequestMapping(\"/recipe\")public class RecipeController {    private final RecipeRepository recipeRepository;    @Autowired    public RecipeController(RecipeRepository recipeRepository) {        this.recipeRepository = recipeRepository;    }    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)    public Flux&lt;Recipe&gt; getAllRecipesRandomized() {        return recipeRepository.findAllRandomized().delayElements(Duration.ofSeconds(1));    }}When we cURL -N our Recipe service, it displays a random recipe every second (because of the added delay).Ingredient Service (Reactive R2DBC)We create another microservice Ingredient Service, which is similar to our Recipe Service, but queries an ingredient database that provides us a stream of Ingredients in a random order.public class Ingredient {    @Id    private Integer id;    private String name;/* Constructors, Getters and Setters not displayed for simplicity */}As you can see, the Ingredient controller works almost the same, but without a delay.@RestController@RequestMapping(\"/ingredient\")public class IngredientController {    private final IngredientRepository ingredientRepository;    @Autowired    public IngredientController(IngredientRepository ingredientRepository) {        this.ingredientRepository = ingredientRepository;    }    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)    public Flux&lt;Ingredient&gt; getAllIngredientsRandomized() {        return ingredientRepository.findAllRandomized();    }}BestMenuEverGenerator Service (Reactive Rest)Our BestMenuEverGenerator service is going to put together our menu for a given amount of days. What makes our BestMenuEverGenerator the best, is because it adds a random special ingredient to every recipe.public class Menu {    private Recipe recipe;    private Ingredient specialIngredient;/* Constructors, Getters and Setters not displayed for simplicity */}We have a RestController that consumes both our services, Recipe and Ingredient. Even though it calls all Recipes, it is not going to retrieve and load all recipes in memory, as would be the case in a non-reactive environment. However, it limits the number of objects it receives, before completing the stream. The ´zipWith()´ method waits for one element of both streams and combines these elements in a BiFunction and returns its results in in new stream. It continues to zip until one of the streams completes. In this case, the results of the BiFunction are Menu objects. There are plenty of other interesting methods to use for combining or transforming streams.@RestController@RequestMapping(\"/menu\")public class MenuReactiveController {    private final RecipeWebService recipeWebService;    private final IngredientWebService ingredientWebService;    @Autowired    public MenuReactiveController(RecipeWebService recipeWebService, IngredientWebService ingredientWebService) {        this.recipeWebService = recipeWebService;        this.ingredientWebService = ingredientWebService;    }    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)    public Flux&lt;Menu&gt; getMenusForGivenDays(@RequestParam(\"amountOfDays\") int amountOfDays) {        Flux&lt;Ingredient&gt; ingredients = ingredientWebService.getAllIngredientsRandomized();        Flux&lt;Recipe&gt; recipes = recipeWebService.getAllRecipesRandomized().take(amountOfDays);        return recipes.zipWith(ingredients, (recipe, ingredient) -&gt; new Menu(recipe, ingredient));    }}Angular Webapp ConsumerI built a simple Angular web application, which consumes our BestMenuEverGenerator service.Instead of using a HttpClient, I use an EventSource to call our service.  Every time an event is received, it is parsed and pushed onto the Observable.getMenusForGivenAmountOfDays(amountOfDays: number): Observable&lt;Menu&gt; {    return new Observable&lt;Menu&gt;(menuSubscriber =&gt; {      const eventSource = new EventSource('http://localhost:8081/menu?amountOfDays=' + amountOfDays);      eventSource.addEventListener('message', (event: any) =&gt; {        menuSubscriber.next(event.data !== null ? JSON.parse(event.data) : event.data);      });      eventSource.onerror = () =&gt; {        eventSource.close();        menuSubscriber.complete();      };    });}We enter the amount of days we want to retrieve a menu for and when we push the button, the following method is triggered.  It subscribes the Observable we created in the previous example from the EventSource.  We push each element in an array which is displayed on our webpage.export class RecipesComponent implements OnInit {  menuArray: Menu[] = [];  amountOfDays: number;  constructor(private recipeService: RecipeService) {}  ngOnInit(): void {}  generateMenu() {    if (this.amountOfDays &gt; 0) {      this.menuArray = [];      this.recipeService.getMenusForGivenAmountOfDays(this.amountOfDays)        .subscribe({          next: menu =&gt; this.menuArray.push(menu),          complete: () =&gt; console.log('complete')        });    }  }}The delay of 1 second we added in the Recipe service earlier offers us a good visualization of the data passing through the stream.  We immediately receive each Recipe with its special Ingredient and see it in the webpage when it arrives, even if the rest of the data has not even been processed in the controller.ConclusionHopefully, this gives you a basic understanding of RestTemplate and WebClient and its capabilities. To summarize, we have learned that RestTemplate is in maintenance mode and Spring advises us to use WebClient instead. WebClient offers the same synchronous way of working as RestTemplate does, but using functional programming. Besides that, it also offers asynchronous reactive streams, which works in a non-blocking way.We have looked into R2DBC, which supports reactive connections with relational databases. Bear in mind that this does not offer a lot of features of ORM frameworks at the moment."
      },
    
      "cloud-2020-09-28-ghelamco-alert-html": {
        "title": "Ghelamco Alert",
        "url": "/cloud/2020/09/28/ghelamco-alert.html",
        "image": "/img/2020-09-25-ghelamco-alert/rpi-front.jpg",
        "date": "28 Sep 2020",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Practicalities  Solution design          Backend RPi                  Spring Boot application          Connecting our RPi to the cloud          AWS IoT Greengrass          CICD pipeline RPi Backend                    Frontend web application                  Angular          AWS Amplify          AWS Cognito          The application itself          Hosting the application          CICD pipeline Frontend                    Serverless framework                  Why the serverless framework?          Serverless.yml          CICD pipeline Serverless                      Application flow  ConclusionIntroductionAt Ordina, we have a beautiful office in the Ghelamco arena in Gent.  The drawback of having an office in this stadium is that whenever KAA Gent plays a game in the stadium we have to make sure our parking lot is empty 3 hours before the start of the game.If the parking lot isn’t cleared in time we risk fines up to €500,00 per car.Of course we don’t want to spend our money on fines when instead we could be buying more pizzas.So we came up with a solution called Ghelamco Alert.The idea for the project is pretty straightforward.We will run a Raspberry Pi device that is hooked up to an alert light.The Raspberry Pi will run an application that parses the game fixtures on the website of KAA Gent to search for any home fixtures.Whenever a home fixture occurs, the RPi will start turning on the alert light on a preset schedule.This way the employees in the office will have a visual warning on game day and they will be reminded to leave the parking lot on time.Since we wanted to build a user-friendly solution we added a serverless backend in the AWS cloud combined with an Angular application so that our users can also look at the web application to get some additional feedback and even manipulate the alerting schedule.This enables users to do some additional operations through the web application:  Snooze alerts  Create additional alerts  Create custom eventsPracticalitiesI received my assignment from my internship supervisor Frederick Bousson.Bas Moorkens was my mentor for the internship.He designed the high-level architecture of the solution and then helped translate it into smaller blocks of work for me.We decided to set up the project using these three big components:  An IoT module on the RPi.  A serverless backend that runs in the AWS cloud using Node.js.  A frontend application built with the Angular framework.The architecture was tweaked several times to account for lessons learned during the internship.We ended up using over 10 AWS services which over the course of the internship, made me develop a real interest in everything that is AWS and cloud related.Once you get the hang of serverless, the speed of setting up infrastructure is absolutely mind-blowing!Solution designBelow you can see the high-level design of our solution.  As you can see the architecture has lots of moving parts.We will zoom in on these in the next sections.Backend RPiThe core part of our solution consists of the Raspberry Pi device with our backend Spring Boot application that runs on it.This application controls the actual alert light and does most of the heavy lifting in the overall scheme.The RPi works with a microSD card on which you can easily install any Linux distribution.You install the distribution of your choosing on SD card and then plug the card into your RPi device and you’re good to go.We used a model 4 Raspberry Pi and installed the default Raspbian Linux distribution on it.Spring Boot applicationThe actual business logic of our solution runs as a Spring Boot project on our RPi device.Basics of the applicationThe core of our application revolves around events which happen at the Ghelamco Arena.These can be of type GameEvent or CustomEvent.  The GameEvents are all the KAA Gent home games.  As an added feature, I also added the possibility to create custom events, which are handy for scheduling concerts or alarming a pizza party.For each event we generate a couple of standard alerts based on the event time.These alerts are responsible to set off the alarm light in the office.This means that we will use the GPIO interface of our RPi to turn on the alarm light every time an alert gets triggered in our backend application.To set off an alert, the GPIO interface changes the voltage on a GPIO-pin to 0V or 3.3V.This pin gets coupled to a relay, which acts as a regular switch, but electrically controlled.If we put 0V on the relay, the circuit remains open, when we put 3.3V on it, the circuit gets closed and thus starting our alarm light.WebScraperOur Spring backend is running a scheduler that is configured to run a service every hour that checks the website of KAA Gent for the up-to-date fixtures of the games.The approach I took for scraping the website is by using X-Path with a library called HtmlUnit.In our application we fetch every game from the website, filter them on home games, attach alerts and save those games to our H2-database using Spring Data.Every time we scan the website, we compare the scraped games’ data with the games that already were saved in our database.Games can get updated or rescheduled on the website, but our application will recognize it and update the database records accordingly.  H2 databaseOne of the requirements for the project was that the RPi should be able to function on his own without constant internet connection.This meant that we needed to use a database that can run locally on the RPi.If we would use a remote database we wouldn’t be able to send any data to the RPi when it is not connected to a network.We chose to use a H2-database because it is a SQL database which is easy to set up for local use.Then we configured the H2-database to be backed by a flatfile on our local filesystem.This way the database is persistent even when the application restarts.As the last step we just need to add the H2 dependency to our projects pom.xml .&lt;dependency&gt;    &lt;groupId&gt;com.h2database&lt;/groupId&gt;    &lt;artifactId&gt;h2&lt;/artifactId&gt;    &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;Add some configuration in the application.properties file.### H2 Storagespring.datasource.url=jdbc:h2:file:c:\\\\h2-ghela\\\\h2dbspring.datasource.driverClassName=org.h2.Driverspring.datasource.username=kevinspring.datasource.password=adminspring.jpa.database-platform=org.hibernate.dialect.H2Dialectspring.jpa.hibernate.ddl-auto=create-dropspring.h2.console.enabled=truespring.h2.console.settings.web-allow-others=falseThis just works out of the box, it even comes with a console to check your database if enabled.  MetricsOur application generates metrics to provide us with some data about our backend.We collect data and push it into AWS CloudWatch as metrics for:  Number of events added  Number of events updated  Number of events cancelled  Number of alerts snoozed  Number of alerts started  Number of alerts stopped  A basic heartbeatWhen our application is running, we are constantly gathering this data which we push to Cloudwatch metrics every 5 minutes.Afterwards we reset our data counters and start collecting again.This allows us to create a dashboard which has data-points every 5 minutes for all the metrics we defined.Connecting our RPi to the cloudWe have two ways of connecting back to the AWS cloud.  Use AWS credentials and the AWS APIs.  Use AWS IoT MQTT protocol and the according certificates.AWS Cloudwatch integrationThe CloudWatch integration is done entirely by having a set of credentials with limited permissions on our RPi device.We use these credentials inside our application to access the AWS SDK from our Java code.We integrate with CloudWatch in two different ways:  We push custom metrics from our application to CloudWatch metrics.  We send the logs from our application to CloudWatch logs.CloudWatch metricsThe metrics that we generate in our application should of course get sent to AWS CloudWatch, so we can actually make use of them in the AWS cloud.We use the metric data to build dashboards and alerts within CloudWatch for added visibility into our application.  We have created an alarm that triggers when our RPI_BACKEND_STATUS status metric is missing for three data points in a row.This means that after 15 minutes of not receiving this metric, the CloudWatch alarm will trigger and notifies us via email that the RPi is either down or disconnected from the internet.This enables us to respond quickly and take action to restore connectivity to the device.  CloudWatch logsThe logs from our Spring Boot application get sent to CloudWatch logs. This allows us to check the log files in the AWS cloud without needing access to the RPi device itself.  AWS IoTThe reason for using AWS IoT is for the sake of “being connected to the cloud”.When we are connected to the cloud, we are able to communicate with our device “through the cloud”, meaning from anywhere we want! In our case this means that we want our RPi device to be connected to AWS IoT whenever it is up and running and has internet connectivity.Of course the communication between our RPi and the AWS cloud has to be secured.AWS IoT uses authentication and authorization workflow by using certificates that you issue from AWS IoT and upload to your edge device.We treat our RPi as an edge device in this project.Authentication works based on the certificates that the edge device presents to AWS IoT.Once the certificate authentication is successful, AWS IoT checks which policies got attached to those certificates to grant it specific authorizations within the AWS cloud.Authentication: certificatesWhen you add a new edge device in AWS IoT it is called “a thing”.When registering “a thing” it generates two keys for us: a public key and a private key.AWS IoT will provide a certificate signing request for the public key, which will sign the generated certificate with the root certificate’s private key.Our thing-certificate and our private key are our credentials when we try to communicate with AWS IoT to access our edge device.The only additional input that we need to provide on top of the generated signing request is the root certificate to check the signing.  Certificates  Private Key  Root CASince these certificates get generated per thing that you register they should only be used for one device.When I developed the application I had to register another thing for my laptop.Connecting from the RPi and the laptop with the same certificates caused some unwanted behavior like connection interrupts.  Authorization: policiesAfter creating our certificates and keys we need to handle the authorization part.We do this by adding some policies to our certificates.When presenting our certificates on connecting, AWS IoT now also knows what services we can access within the AWS cloud.An example of such a policy:{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Action\": [        \"iot:Publish\",        \"iot:Subscribe\",        \"iot:Connect\",        \"iot:Receive\"      ],      \"Resource\": [        \"*\"      ]    },    {      \"Effect\": \"Allow\",      \"Action\": [        \"iot:GetThingShadow\",        \"iot:UpdateThingShadow\",        \"iot:DeleteThingShadow\"      ],      \"Resource\": [        \"*\"      ]    },    {      \"Effect\": \"Allow\",      \"Action\": [        \"greengrass:*\"      ],      \"Resource\": [        \"*\"      ]    }  ]}We define which services we want to access:  iot:Publish  iot:Subscribe  iot:GetThingShadow  iot:UpdateThingShadow  iot:DeleteThingShadowThis system of generating certificates and coupling policies is a very secure and easy way of working with edge devices.IoT jobA crucial part of AWS IoT is the job section. We can create a job by using the AWS CLI.aws iot update-job    --job-id 010    --description \"updated description\"   --timeout-config inProgressTimeoutInMinutes=100  --job-executions-rollout-config {exponentialRate:{baseRatePerMinute: 50, incrementFactor: 2, rateIncreaseCriteria: &lt;{numberOfNotifiedThings: 1000, numberOfSucceededThings: 1000}, maximumPerMinute: 1000}}  --abort-config { criteriaList: [ { action: CANCEL, failureType: FAILED, minNumberOfExecutedThings: 100, thresholdPercentage: 20}, { action: CANCEL, failureType: TIMED_OUT, minNumberOfExecutedThings: 200, thresholdPercentage: 50}]}            --presigned-url-config {roleArn:arn:aws:iam::123456789012:role/S3DownloadRole, expiresInSec:3600}We use AWS IoT to create these jobs and send them to our RPi where they get executed by our application code.  We have the possibility to put multiple “things” in a group of devices.This way we can send jobs to the entire group and these jobs get executed on every device from that group.These jobs could be software updates, reboot commands, rotation of certificates, … Anything we want really!In our project we used IoT jobs for a multitude of processes:  Update our backend application - over the air  Create new events  Snooze and update existing alerts  It is pretty straightforward to create an IoT job:  Create a job  Add a job document (JSON file) which defines the content of the job  Push the job onto the job queue to send it to your devicesExample job document:{    \"operation\": \"snoozeAlert\",    \"local_alert_id\": 31,    \"event_title\": \"test from lambda snooze alert\"}MQTT protocolAWS IoT can communicate with its registered devices through two protocols: HTTP or MQTT.So why would we choose MQTT over the more familiar HTTP protocol?The HTTP protocol has some severe limitations for our use case:  HTTP is a synchronous protocol, the client waits for the server to respond.That is a requirement for web browsers, but it comes at the cost of poor scalability.In the world of IoT where we have numerous devices and most likely an unreliable / high-latency network connection this synchronous communication is problematic.An asynchronous messaging protocol is much more suitable for IoT applications.The sensors can send in readings and let the network figure out the optimal path and timing for delivery to its destination devices and services.  HTTP is a one-way protocol. The client must initiate the connection.In an IoT application, the devices or sensors are typically clients, which means that they cannot passively receive commands from the network.  HTTP is a 1-1 protocol. The client makes a request and the server responds.It is difficult and expensive to broadcast a message to all devices on the network, which is a common use case in IoT applications.  HTTP is a heavyweight protocol with many headers and rules. It is not suitable for constrained networks.MQTT on the other hand defines two types of entities in the network: a message broker (AWS IoT) and a number of edge devices (clients).The broker is a server that receives all messages from the clients and then routes those messages to relevant destination clients.A client is anything that can interact with the broker to send and receive messages.Our client is the RPi but it could also be an IoT sensor in the field or an application in a data center that processes IoT data.In our backend Java code we have an MQTTJobService which connects to AWS IoT by using the AWS SDK and subscribes to the relevant topics to receive and respond to AWS IoT jobs.Every 30 seconds we will read these topics to see if there are any new jobs to be processed.AWS IoT GreengrassAWS Greengrass is a service that extends the AWS cloud onto your edge device.This was fascinating for us since we had some tough problems to solve:  How can we deploy our application on the RPi device?  How can we make sure our application recovers from failures?  How can we make sure that our system keeps itself up to date?  How do we find the network address from our device when we are not in the same network?AWS Greengrass offers solutions to all these challenges!Greengrass is an extension of the AWS IoT service and since we had already set up our device in AWS IoT it was easy for us to set up Greengrass.To get started with Greengrass we had to do 2 additional steps:  Define a Greengrass group. This group will contain your IoT devices and deployments.  Define a Greengrass core. The core is the device that you will use to run the additional AWS capabilities on.The Greengrass group allows us to push deployments to all the devices in our group.It also allows you to run lambda functions on your core device or install additional connectors with your own runtime of choice.  When you register your device as a core device in Greengrass you immediately get some nice additional benefits from this.For example, you can immediately see all the network interfaces on your core device and what IP addresses got allocated.This is especially useful if you want to ssh to your device and do not have a fixed IP attached to it.  Now we that Greengrass is installed and our RPi is configured as a core device our Greengrass group we can start making full use of the capabilities that Greengrass offers.We decided that we wanted our application to run as a Docker container, so we installed and configured the Docker connector for Greengrass. This plugin allows you to run Docker containers on your core device and makes use of Docker and Docker Compose.  For this to work you have to install Docker and Docker Compose on your core device.Since we are using a RPi it was a bit harder to install since the RPi uses the ARM7 chipset instead of x64/x32.We had no trouble installing Docker itself, but for Docker Compose we had to clone the source code from GitHub and compile the binaries ourself on the RPi.After all the setup was done we could just create a Docker container in our CICD pipeline and tell Greengrass to run that container on RPi.CICD pipeline RPi BackendTo make sure we did not need to bother ourselves with manual builds and installs of our code on the RPi we built a CICD pipeline to automatically deploy our software onto the RPi.We trigger the pipeline whenever a push to our master branch hapens in our Git repository.We used Azure DevOps as our CICD system.  The pipeline has a lot of pipeline steps defined and integrates with several AWS services as well.We drew up a chart to provide additional insight into how this pipeline works:  As you can see there are two major parts of this pipeline:  the Azure DevOps pipeline  the Greengrass deployment process.Azure devops pipeline  Build and test our Java code.We use Maven to build and test our code.  We use AWS SSM - Parameter Store to safely store our config files and certificates for the RPi.In step 2 and 3 we fetch those config files and secrets from the parameter store and store them locally for use later in the pipeline.  In order to control the alert light hooked up to our RPi we need to download the WiringPi library as we used that to communicate with our alert light. We stored this library in our release S3 bucket.  We have to register an ARM hardware emulator as our Docker image has to be run in the pipeline to install additional software.Without the emulator our pipeline would just crash as Azure DevOps would have no way to run the ARM7 Docker image.  We build our Docker image.We install all the config, certificates and dependencies into our container in this step.  We push the container into AWS ECR - Elastic Container Registry.  We fill in the container name and version from the previous step in our template docker-compose.yml file and upload it to our S3 releases bucket.  We use the greengrass create-deployment CLI command to deploy our Docker container to our RPi device.This deployment fetches the docker-compose.yml file from our S3 release bucket and then launches the container that we defined in that compose file.Some additional explanation on the Docker image and running containers on ARM7 processor architectures.Our application’s Dockerfile looks like this:FROM balenalib/am571x-evm-openjdk:11-jre-bullseye-20200901ADD ./wiringpi-2.52.deb /tmp/wiringpi-2.52.debRUN apt-get update &amp;&amp; \\     cd /tmp &amp;&amp; \\     dpkg -i wiringpi-2.52.debCOPY ./target/ghela-business-0.0.1-SNAPSHOT.jar /app.jarADD ./config /configADD ./certificates /certificatesENTRYPOINT [\"java\",\"-jar\",\"app.jar\"]Because we are running this container on our RPi we need to make sure it can run on the ARM7 processor.This is why we started from an ARM7 image that already has JRE11 installed on it.As you can see we need to run apt-get update**` and some other commands in our container to get it fully functioning.This means that our container will be actually executed during the build phase.This is the reason why we need to register the ARM7 emulator in our pipeline.Otherwise, Azure DevOps would have no way of running our Docker image since it cannot interpret the ARM7 processor instruction set natively.In our docker-compose.yml file we describe how the container should behave when it’s running:version: '3.3'services:  web:    image: \"${ecr.imageTag}\"    privileged: true    ports:      - \"8080:8080\"    volumes:      - /home/pi/.aws/:/root/.aws:ro      - /ghela/db:/ghela/dbvolumes:  db-data:Several things are happening in this file:  We dynamically inject the image name and version in our CICD pipeline into the  ${ecr.imageTag} field.  We run our container in privileged mode as this is needed to access the RPis native interface to control our alert light through GPIO with wiringPi.  We expose our application port 8080 to the outside world.  We mount our AWS credentials and H2 database as volumes from the RPi host system.Greengrass deployment processFor the Greengrass deployment process we did not have to create any code.This works out of the box when you set up Greengrass.However, for full transparency I will briefly describe the process here:  AWS Greengrass receives a new deployment (this is the last step of our Azure DevOps pipeline).  Greengrass sends the new deployment to the Greengrass agent on our device.  Download the Docker Compose file from S3 which is associated with our deployment.  Download the Docker image and version from ECR that is defined in the Docker Compose file.  Terminate the current running container.  Start the new container.  Set up the mounted volumes defined in our Docker Compose file.  Start the Spring Boot application in the container.All these steps get done automatically for us by the Greengrass agent running on our RPi device.As you can see this CICD pipeline makes it incredibly easy for us to deploy new software to our RPi device.Frontend web applicationTo get data from our RPi into our web application we needed a way to connect to our H2 database on the RPi.It would be pretty complex to set up our RPi device to be accessible from the internet, so we chose to build a backend in AWS to function as a proxy for our RPi backend application.This allows us to use this proxy backend in AWS to access the data from our RPi device and send new commands to update existing data on the RPi.More about this backend in the serverless part.First let’s take a look into our frontend application.AngularI wanted to create a simple web application to get more insight in the data of our RPi device.Any modern frontend framework would be suitable for this but I decided to go with Angular since I had already had some basic experience with it.We did however use some extra frameworks and AWS services to make it easier to build our frontend application.AWS AmplifyTo help us bootstrap our web application, we decided to use the AWS Amplify framework.The open-source Amplify Framework provides:  Amplify CLI - Configure AWS services needed to power your backend through a simple command line interface.  Amplify Libraries - Use case-centric client libraries to integrate your app code with a backend using declarative interfaces.  Amplify UI Components - UI libraries for React, React Native, Angular, Ionic and Vue.For our project we only used the libraries and UI components from Amplify for our web application.When we use the Amplify CLI to bootstrap our project it generated a new file in the ./src folder of our project called aws-exports.js.This file contains configuration of various backend AWS services that we will be using:  Cognito user pool id  Cognito identity pool url  Region  API gateway urlThe backend itself is build and deployed with the serverless framework.More about this in the section regarding serverless.AWS CognitoWe used AWS Cognito as authentication and authorization provider.This means that new users can sign up to our Cogito user pool and login to our web application by authenticating themselves via that user pool.There are several excellent reasons to use Cogito instead of building a home-grown identity solution:  Secure and scalable user directory.It scales dynamically as you would expect from AWS.  Fine grained access control through the use of Cogito user pools and identity pools. You can define users and groups in a Cogito user pool which you then can map to AWS IAM roles.This is a straightforward and very secure way to allow users of the web application a federated access into your AWS account.  Easy integration into our frontend application through components that are provided in the Amplify UI library.  Cognito User poolUser Pools are user directories used to manage sign-up and sign-in functionality for mobile and web applications.Users can register directly in our user pool or they can register themselves through web identity federation via a well-known 3rd party like: Facebook, Amazon, Google.Whenever a user logs in to our user pool they receive a JWT token that contains information about their identity and authorizations.We call this information JWT claims.Cognito identity poolTo be able to use the claims in this JWT token we created an AWS identity pool as well.The identity pool allows users to exchange their valid JWT token for temporary AWS credentials.These temporary credentials then get used by our application, to call AWS services in our account. For example: call API gateway, update a dynamodb table , fetch a CloudWatch metric.This process is called federated access and is very powerful to expose services from within your AWS account to your end users.The application itselfNow I covered all the technologies that I used for the web application, I will walk you through the application itself.Dashboard viewOn the dashboard view we can see which events are coming up next.When we click on an event we also get to see all the alerts which are set for that event.We can update or snooze alerts from this screen, BUT…This works a bit different from how you would expect it to work.In a normal setting you would expect us to update the records in our DynamoDB table, right?Since we want our H2 database on the RPi device to be the single source of truth this is not the case here.  When you update or snooze an alert we actually create an AWS IoT job for our RPi to process.The backend application on the RPi knows how to handle these events and updates his H2 database accordingly.Together with our sync component we built on the RPi backend, those changes get updated directly to our DynamoDB table.Create Custom eventOn this tab we can create our own custom events.This works in the same way as snoozing or updating on alert on the dashboard view.  Job overviewIn the job overview view we query our API gateway to fetch us all the pending jobs from our DynamoDB job table.The lambda function that listens to this endpoint reads the ghela-jobs DynamoDB table and list all jobs that have a status of not yet “COMPLETED”.The moment this list is empty all changes from our webapp have been processed successfully by our RPi backend application and will have updated the ghela-events and ghela-jobs DynamoDB tables.Important note: our RPi can only receive new jobs from AWS IoT if it is connected to the internet.So if we do not have a working network connection, jobs will stay queued within AWS IoT and not get pushed to the RPi backend application.For the same reason it is important to know that our RPi device is connected and functioning well, to make sure our commands from our web application get processed by our RPi backend application.  MetricsOn this tab we can see the status of the RPi backend as a graph.Remember the metrics we talked about before?Via the /metrics API endpoint we invoke a lambda function that fetches this dynamic graph and sends it back to our frontend as a Base64 encoded stream of byte data.We can then render this data as an image in our frontend web application.By adding this graph in our web application users of the application can see that the RPi backend device is up and running and functioning well without ever having to access our AWS account itself, pretty cool huh?Hosting the applicationIn order to make our web application accessible to the public we needed some kind of hosting service.In our case we used AWS S3 to host our website.Our S3 bucket is prefaced by a AWS CloudFront that acts as our global, scalable CDN.CloudFront delivers our content through a worldwide network of data centers called edge locations.When a user requests content that we’re serving with CloudFront, the user gets routed to the edge location that provides the lowest latency.We used AWS Route 53 as our DNS service so that we can access our application on the url  ghelamco-alert.ordina-jworks.io.CICD pipeline FrontendSince CICD pipelines were completely new to me when starting this project, Bas created the pipeline for our RPi backend.For the frontend CICD pipeline he wanted me to create it.Even though he still helped me, he let me think about how this pipeline would run, what steps we needed and had me create the azure-pipelines.yml.  The pipeline is less complex than the pipeline to build and deploy our backend on our RPi device.We created a diagram of all the steps and interactions with AWS services for this pipeline as well.    Install Node.js.  Download the needed configuration files from AWS SSM parameter store and save it locally.In this case we download an entire aws-exports.js file from SSM and inject this in the final build of our application.This file contains all the endpoints for wiring our frontend application to our production backend.  Install npm dependencies.  Build our Angular application. We use the npm build command to do this.  Override the configuration file in the application we just built with the configuration file we downloaded from SSM.  Empty the S3 bucket we are using for hosting.  Upload the newly compiled web application to this S3 bucket.  Invalidate the AWS CloudFront cache to make sure our application gets updated on each AWS edge location. If we don’t do this, our old version could be cached for up to 24h!Serverless frameworkOur frontend application would be pretty useless if no backend existed for it.Since it is difficult to access our RPi backend application directly we decided to build a proxy backend in the AWS cloud as explained before.We decided to use the serverless framework for its ease of use and speed of development to build this backend.What exactly are the requirements for our backend in the AWS cloud? We listed following list of requirements:  DynamoDB tables for our events and jobs.  Cognito UserPool and a Cognito IdentityPool to provide authentication and authorisation for our frontend application.  AWS API Gateway to expose REST endpoints from our cloud backend to our frontend web application.  Lambda functions that actually implement our backend code. We used the AWS Node.js SDK to program our lambda functions.  AWS IAM roles and permissions to perform operations inside of our Lambdas (connect and create jobs on AWS IoT, scan and put to our DynamoDB tables, etc…).Why the serverless framework?In the beginning we configured everything by hand in the console.This approach has several drawbacks:  It is very time-consuming to click in the AWS console  It is an error prone and not easily repeatable process to set up  It does not adhere to industry best practices as we want our infrastructure to be defined as codeBas then introduced me to the concept of IaC or Infrastructure as Code and the Serverless framework.There are several options to do IaC on AWS but for our particular project serverless seemed the best fit.Our entire cloud backend gets configured in our serverless.yml file, that is versioned on GitHub in our cloud backend project.This way we can set up our infrastructure through IaC in a repeatable manner.Serverless.ymlThis file is the heart of our serverless setup.Everything that is needed to run our serverless stack got described in this file.We used the yaml notation as it is less convoluted than JSON, but you can also use JSON or TypeScript to build this. We will explain the different parts of our serverless setup to show you in detail how everything works.Functions and Api GatewayThe essential part of our serverless setup is the functions section.A function is an AWS Lambda function.It’s an independent unit of deployment like a true microservice.It is generally a very small piece of code that does one thing and does it well.In our project, for example, we have a lambda function list-events which does exactly that, list the events from our DynamoDB table.  Our API gateway is generated based on the configuration of our functions in this section.Everything under the events property is used as configuration for our API gateway.In this example you can see that we define a path of event and a method GET.We also enable CORS and tell the API gateway to authorize calls made to this endpoint with the AWS_IAM authorizer.This results in the path /event getting added to our API gateway which then maps to our lambda function we defined.This path is CORS enabled and is secured by the aws_iam authorizer.So you can only invoke this function if you have a valid AWS access credential from our Cogito identity pool.If you don’t have a valid credential you will receive a 403 denied response from the API gateway.  Our function needs several additional pieces of configuration to be able to do its job well.AWS IAM RolesTo allow our lambda function to have fine-grained access to our AWS services we defined IAM roles in our serverless project that are attached to our lambda functions.We defined the IAM roles in a separate file lambda-iam-roles.yml which gets included into our serverless.yml file.We can then freely use any roles we wish.  AWS DynamoDBWe decided to use AWS DynamoDB as our datastore in the cloud.AWS DynamoDB is a key-value, document and NoSQL database, that delivers single-digit millisecond performance at any scale.DynamoDB is a very cost effective and low maintenance way of storing data so it looked perfect for us.Since our lambda function relies on these DynamoDB tables for its data we pass the table name to the function as an environment variable.This allows us to have separate tables for different environment like development, integration, production.Our DynamoDB tables get created by the serverless framework as well.They are described in a separate file dynamodb-tables.yml and gets included in the serverless.yml file.Our DynamoDB tables get created by a naming policy which allows us to easily reuse that naming scheme in our environment variables for the lambda functions.  Data syncOur data gets pushed from our RPi backend into DynamoDB tables so that our web application has access to the latest data.To achieve this push mechanism I made a service in our backend application that syncs the local H2 database to our DynamoDB table.We added some triggers in our RPi backend application that allows us to sync the current state of the H2 database to the DynamoDB table.For example, when we are done with our website scraping process, we trigger a sync to DynamoDB if there are any updates or inserts into our H2 database.This ensures that our RPi H2 database acts as the single source of truth.The DynamoDB is just a read-only copy of the data in the AWS cloud that is kept up to date by our RPi backend application.So as long as the backend application is running on the RPi we have access to the latest data in our web application.AWS Lambda functions with AWS Node.js SDKThe handler is the entry point in our lambda function.This is defined in our serverless stack which handler should be invoked by the API gateway.Following example illustrates how our lambda reads data from our DynamoDB table (using the environment variable from our config) and returns the result to our API gateway.  AWS API Gateway additional configurationWe needed to add some additional configuration for our API gateway that was not possible to include in the functions part of our serverless template.To make sure our application does not get CORS errors when it receives a 4XX or 5XX response from the API gateway we had set up CORS for these error responses.As you can see below we added response headers to allow all origins and headers.  CICD pipeline ServerlessSince the serverless framework is so easy to use, and we only had one environment available, we decided not to build a CICD pipeline for this part.Instead, we just use the serverless CLI to deploy and update our stack from our local development machines.Application flowTo finish off the technical part of this blogpost I would like to show you the end-to-end workflow of the application.For example let’s say the user logs in to the web application and wants to snooze an alert.  We click snooze alert in the frontend web application.  This calls our API gateway endpoint /alert/snooze.  This call is authorized by the userpool JWT token and identitypool temporary AWS credential.  This API gateway endpoint invokes an AWS Lambda function which creates a job and job document in AWS IoT for the RPi.It also inserts a row into a second DynamoDB table (ghela-jobs) to register the job we created on the frontend.  The job gets send to the RPi and processed by our backend application, thus snoozing the alert in the H2 database.This will effectively turn the alert light off.  The RPi synchronizes its H2 database with the event DynamoDB table. Thus updating the status of the alert in the DynamoDB table.  This new status will then be reflected in the frontend web application so our user can see the result of his action.This same workflow applies to all commands that can be sent from the frontend application to our RPi device.This is a prime example of how a distributed system works and operates.ConclusionThis internship gave me such a big overview of all best practices and new technologies, that this was a real eye-opener and educational experience.  I’ve had the chance to develop an application in Spring, with different databases;  learned how to properly test my code with unit and integration tests;  learned how to use the Java and Node AWS SDKs;  APIs, connecting it all together…;  came in touch with a lot of Linux, sharpening my command-line skills;  developed an Angular app from start to finish;  learned a lot about AWS: IAM, IoT Core, DynamoDB, API Gateway, SNS, SSM, EKS, S3, CloudWatch, CloudFront, lambdas, Route53, …;  CICD pipelines and their best practices in the industryI’m very thankful for being able to do this and having the chance to work with Bas on a real project.He always motivated me to go the extra mile !And he could destroy a day of hard work in a few sentences … :)  But afterwards he would always take the time to explain why, and how, I should do it to make my solution comply with the industry best practices.Being able to have someone who makes you self-reflect on the work you did and takes his time to give you proper feedback.That is the biggest asset to provide to a junior programmer.Also, a big thanks to Frederick Bousson &amp; Ordina for the opportunity and resources provided.This really was a great project and I enjoyed every minute of it!"
      },
    
      "frontend-2020-09-18-scaling-css-with-bem-html": {
        "title": "Scaling CSS with BEM",
        "url": "/frontend/2020/09/18/scaling-css-with-bem.html",
        "image": "/img/BEM.jpeg",
        "date": "18 Sep 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  What is BEM?  BEM combined with SASS  Is BEM still viable in the era of web components?  Conclusion  Resources and interesting readsWhat is BEM?BEM is a methodology that helps developers to create reusable components and code sharing in front-end development.BEM is an abbreviation as you could have guessed.It stands for the three core elements of the methodology: Block, Element and Modifier.Blocks are standalone entities that are meaningful on its own.For example:header, container, menu, checkbox, inputElements are a part of a block that has no standalone meaning and are semantically tied to its block.For example:menu item, list item, header titleModifiers are a flag on a block or element.They are meant to change appearance or behavior.For example:disabled, highlighted, color yellow, size big, fixedBEM: a naming conventionBEM is in short a highly useful, powerful, and simple naming convention.It makes your front end code easier to scale, more robust and explicit, and a lot more strict.Which makes it easier to read and understand and also easier to work with.Namings of BEM are as followed:            BEM      Naming      HTML      CSS                  Block      .block      &lt;div class=\"block\"&gt;&lt;/div&gt;      .block {...}              Element      .block__element      &lt;div class=\"block__element\"&gt;&lt;/div&gt;      .block__element {...}              Block Modifier      .block–modifier      &lt;div class=\"block block--modifier&gt;&lt;/div&gt;      .block--modifier {...}              Element Modifier      .block__element–modifier      &lt;div class=\"block__element block__element--modifier&gt;&lt;/div&gt;      .block__element--modifier {...}      For example:You have 2 images.One on it’s own as a ‘block’ and the other is an ‘element’ inside a profile section which is another ‘block’ to keep it simple.This is how the HTML would look like:&lt;img class=\"image\"/&gt;&lt;div class=\"profile\"&gt;    &lt;img class=\"profile__image\"/&gt;&lt;/div&gt;The CSS would look like this:.image {...}.profile {...}.profile__image {...}If you want to add different versions, states or ‘modifiers’, for example a rounded or smaller image. Your code would look like this:&lt;img class=\"image image--rounded\"/&gt;&lt;div class=\"profile\"&gt;    &lt;img class=\"profile__image profile__image--small\"/&gt;&lt;/div&gt;.image {...}.image--rounded {...}.profile {...}.profile__image {...}.profile__image--small {...}The modifier class should only be added to blocks or elements you want to modify and you should keep the original class.Why use BEM?As stated above, BEM makes your code easier to read, understand, work with et cetera.But how does it achieve that?First of all, BEM avoids inheritance and provides some sort of scope by using unique CSS classes per element (like .profile__image).It reduces style conflicts by keeping CSS specificity to a minimum level.It avoids the use of element type selectors like div &gt; ul &gt; li and keeps your CSS loose coupled from your HTML.BEM avoids nesting and keeps your CSS flat (even with preprocessors).When not to use BEM?You should always question yourself if it is really necessary to use BEM notation for a certain CSS class.If we would add two buttons to the example above like so:&lt;img class=\"image\"/&gt;&lt;button class=\"button\"&gt;Default button&lt;/button&gt;&lt;div class=\"profile\"&gt;    &lt;img class=\"profile__image\"/&gt;    &lt;button class=\"profile__button\"&gt;Profile button&lt;/button&gt;&lt;/div&gt;If the second button should be styled differently because it lives inside the profile block you SHOULD use BEM notation.Otherwise if it should be styled the same as the other button and it just happens to live in profile you definitely DO NOT need BEM notation there..underline { text-decoration: underline; }This CSS would never fall into any BEM category, as it is merely a standalone rule.BEM combined with SASSIs the BEM methodology easy to combine with preprocessors like SASS?The answer is: ‘Yes, absolutely!’With the help of SASS and its ‘parent selector’ we could transform the above CSS to the following code:.image {    &amp;--rounded {...}}.profile {    &amp;__image {        &amp;--small {...}    }}The parent selector of SASS makes it easy to add suffixes to the outer selector resulting in an improvement of readability and cleaner style sheets.All while the resulting CSS stays flat.The above SASS compiles to:.image {...}.image--rounded {...}.profile {...}.profile__image {...}.profile__image--small {...}As you can see it is the same CSS as above.You want to be avoiding CSS combinators like this:.profile .profile__image .profile__image--small {...}Is BEM still viable in the era of web components?Yes and no.First of all BEM was created to make large scale style sheets easier to scale, read et cetera.It avoids class name collisions and the quick fixes with ‘!important’.When you are working component based, you can make use of Shadow DOM to make the styles encapsulated in that component.This prevents them from affecting outer elements.Angular, Vue or React use similar approaches, either using Shadow DOM or appending unique attributes to the class names.This ensures that the styles are scoped to that certain component.Since this makes class name collisions no issue anymore you could consider BEM obsolete.BEM helps you create reusable components.But that is also no longer necessary here, because we are already using seperate components.Although, I still believe BEM could be useful as a developer.Take our profile component for example.With the traditional approach, profile would be seen as the Block and we would end up with following classes:.profile {...}.profile__header {...}.profile__header__title {...}.profile__header__title--short {...}.profile__header__name {...}.profile__bio {...}.profile__image {...}.profile__image--small {...}.profile__image-description {...}By encapsulating the styles, we could drop the profile part as we already are inside the profile component.We do not need to worry about class name collisions, so we can shorten our class names and keep them meaningful:.profile {...}.header {...}.header__title {...}.header__title--short {...}.header__name {...}.bio {...}.image {...}.image--small {...}.image-description {...}Now the responsibility of each class is much clearer just by looking at it.It is easier to split the component later if needed, and overall it is much easier to read.So is BEM still necessary in component driven development?No, not at all, but I do recommend using a convention, be it BEM or anything else.The most important part is that the team agrees on it and follows it.Consistency is ‘mucho importante’ to keep the code base lean and clean.ConclusionBEM is a useful class naming convention to keep gigantic style sheets organisable and readable and to avoid class name collisions.It is easy to use with preprocessors as SASS which makes your code even cleaner.In CDD (component driven development) BEM might be considered obsolete, but a naming convention is still recommended.This counts for any language, including CSS.Resources and interesting readsBEMSASSBEM and SASS a perfect matchWhat is the Shadow DOMCDD (Component Driven Development)"
      },
    
      "cloud-2020-08-28-kubernetes-clients-comparison-html": {
        "title": "Kubernetes clients and dashboards: a comparison",
        "url": "/cloud/2020/08/28/kubernetes-clients-comparison.html",
        "image": "/img/2020-08-06-kubernetes-clients-comparison/banner.jpg",
        "date": "28 Aug 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  K9s  Octant  Lens  Kubenav  Infra.App  ConclusionIntroductionImagine the following scenario: you’re writing code for your amazing new take-your-bike-to-work platform and you’ve just finished implementing a new feature to allow users to send unicorns to each other.Your CI/CD pipeline has nicely tested, packaged and deployed the updates to your development Kubernetes cluster, you load the URL and are greeted by a very nice error page stating “Oops, my bad, we lost some unicorns”.Clearly, you broke something, somewhere.Most of the time, this means you’ll open up a terminal, run some commands to login into the cluster and start firing two dozen kubectl-commands to figure out which microservice broke and check the logs to figure out where your code has broken.You make some changes to the broken service and push your code to the repository and the CI/CD flow takes over again.This process works quite nicely, but figuring which service is broken and which logs to check can be quite challenging.Typing the kubectl-commands into the terminal probably takes half of the time you spend on debugging the issue.As developers are always optimizing their workflow, using kubectl just takes to much time, even with the k alias for the command and perfect auto-complete features.As the famous mantra goes: “anything worth doing twice is worth automating”.Therefore, quite some tools were created to make the process of navigating through a cluster easier than typing a lot of commands.This blog post aims to provide a very brief overview of some of the more common tools that are available as replacements or additions to kubectl to allow developers to look into a Kubernetes cluster.All tools can be installed locally and don’t require any components to be installed in the cluster to operate.K9sK9s is a Kubernetes client built by Fernand Galiana.The client is fully terminal-based so you’ll only be using your keyboard when operating it.For those who are familiar with Vim, you’ll feel right at home in K9s.It uses similar hotkeys to the popular editor.There is a (quite steep) learning curve when you start using this client.Once you have read the brief readme on the project’s home page and memorized the commands you’ll use the most, it is an absolute joy to use.It feels like using kubectl without the requirement to type all commands every time you need to get a deployment.The tool is quite feature-rich at the time of writing.You can port-forward, view secrets in plain text, edit resources directly, and “drill-down” from deployments into the logs of a container.:deploy takes you to the pod overview, &lt;enter&gt;-ing into the deployment takes you to all pods in that deployment.&lt;enter&gt;-ing again into a pod reveals all containers, including (completed) init-containers.&lt;enter&gt;-ing a final time into a pod takes you straight into a view with live logs.This hierarchical approach feels very natural and follows the architectural design of Kubernetes.A similar approach can be used for service (:svc), statefulsets (:sts) and deamonsets(:ds).Another very familiar shortcut is the usage of / to filter on the context you’re currently in.This works on basically any screen where you’d expect it, even in the logs view!After a few days of use, it feels very natural to use.However, for those of us who rather use their mouse to navigate through resources and hate memorizing commands, this tool is not for you.The project is still under very active development and quite some people are contributing to the codebase.Since the project doesn’t seem to be backed by a company directly, there are no real support guarantees nor is there a fixed release schedule.The maintainer however accepts fixes quite fast and releases are very frequent, sometimes multiple a day.Demo: Go from deployment to all the way into pod logsDemo: Switch between two Kubernetes contextsDemo: Find out where a configmap is usedOctantOctant is an open-source Kubernetes web dashboard built by VMWare Tanzu.It is written in Go, which is a trending programming language made by Google.It has lots of features, a clean user interface, and has the support of a big company in the IT industry behind it.This dashboard runs locally, which means you do not need to install it on your cluster, unlike the official Kubernetes dashboard.Installing and accessing the dashboard is very straight-forward; it is as easy as running an install command (on Mac, we used Homebrew) and then running the octant command from your terminal.Once it’s running, you will find that the navigation is simple. We didn’t need to look around and search a lot to find what we need.You are greeted by the application with a nice and well-ordered summary of your cluster resources.Navigating through the dashboard feels very comfortable and swift. They offer both a dark and light theme, which you will be able to see down below.This client offers features that other traditional clients have as well such as log streams, a graphical display of all resources, switch between clusters,…It does have unique features as well, such as in-app port forwarding (say goodbye to kubectl port-forward!).Octant port forwardingThe most singular feature they have is their plugin system, which allows you to design plugins based on the information that you want on your interface. Although this requires knowledge of Go and their plugin system, you can customize your experience entirely to the way that you want it to be.For example, there is a Helm plugin available and Jenkins X has a repository of Octant plugins.If you are a fan of web dashboards, then you should give Octant a try. Once you start to work with it, you will feel that it has a lot of potential. If you do prefer something more natively and working in a separate application window, then we suggest you to continue reading this blog post.Octant slideshowLensLens app is a Kubernetes client with a proper GUI.It was created by Kontena Inc and later sold to Mirantis, the owners of Docker Enterprise.When first starting Lens, it immediately feels very easy to use.Adding a cluster can be done by hitting the + and selecting a cluster from the dropdown.Lens leverages the contents of the kubeconfig it finds on the system to discover and authenticate with clusters.This way, no additional magic is needed to get started.After connecting to a cluster, you’re dropped into the cluster overview (see screenshot).This view provides you with an easy overview of the resources within the cluster and (super useful) provides a list of the last seen error events in the cluster.Navigating to the list of pods provides an overview of all pods in the cluster.By clicking on a pod you’re provided with the details of that pod (kubectl describe).From here, you can directly dive into the pod logs, shell into the pod, make edits or remove the pod from the cluster.Similar support is available for most common resources within the cluster: statefulsets, deployments, configmaps, secrets, …The workflow is always the following: open the type in the sidebar on the left, click on an object to get details.From that detailed view, certain actions can be performed on the object.As with most of the tools in this comparison, Lens is quite feature-rich.The most common types are supported and common actions are available.But Lens has another trick up its sleeve which makes it different from the other tools: Metrics/Prometheus integration.The integration relies on a Prometheus instance being installed in the cluster that exposes the supported metrics.You can opt for Lens to install Prometheus (and other required components) for you, but in real scenarios, you either don’t have those rights or you’ll already have a Prometheus instance installed in the cluster.In those cases, you can just add some configuration to that instance and point the Lens app to that Prometheus instance.It will use port-forwarding under the hood, so no need to expose the Prometheus instance to the outside world.The integration works nicely and instantly provides some metrics about your cluster and deployed components.This provides good insights for developers to figure out their resource consumption without leaving their Kubernetes client.The charts and data seem to be very rudimentary, but improvements are expected to arrive over time.As most of the clients described in this post, Lens app is an open-source project.Mirantis is behind the development of Lens, but at the time of writing, no supported (paid) version is available.There is continuous active development on the app and releases are about one month apart, so bug fixes and new features should be available regularly.Screenshot: List of pods in LensScreenshot: Details about a pod in Lens, including Prometheus supplied metricsScreenshot: Overview of a cluster in Lens, including the last error eventsKubenavKubenav is a rich featured, open source Kubernetes client created in early 2020 to manage your cluster(s) with. The application is under active development by the open-source community on their GitHub repository.There is cross-platform functionality, which means they provide you a desktop AND mobile client, which is a unique feature in the world of Kubernetes clients.They share the same codebase, so the navigation should be similar on all platforms.The navigation menu is self-explanatory as it categorizes every Kubernetes resource, which you can then filter by namespace in the top-right corner.They support all widely used Kubernetes resources and the status and configuration of those resources. You can easily switch between Kubernetes clusters by using the drop-down menu available in the menu.There is a window that is available at all times where you can consult different deployment logs and a terminal. Each window is categorized under a tab.When you minimize this window, you will notice a blue shell icon in the right bottom corner of Kubenav, which allows you to open your current logs or terminal again.The app is made in the Ionic Framework using Capacitator as cross-platform runtime, which is something you notice when you start using the app.We personally don’t really like this feeling, as we prefer our clients to have a more native feeling, although we understand why the developers chose this approach.The primary reason why we would use this application is because they support almost every resource combined with a GUI if you don’t like using the terminal.However, I’m not a fan of the interface in general, as it seems to be messy and overwhelming at times when a resource provides you with a great deal of information in an uncategorized way.If you want to know every detail about your resource at all times, then this application is definitely for you.We didn’t have a reason yet to use the mobile client, but we are sure it depends on your use case.Kubenav slideshowInfra AppInfra App is a new addition to the list of Kubernetes clients.It is made by the people over at Docker Desktop &amp; Kitematic and is being developed behind closed doors, which has been addressed as “unpleasant” within the Kubernetes community.It provides you with a clean, simplistic user interface that groups everything you need to know about a single resource together.Everything is self-explanatory and all the information you need is available within a few simple clicks.When you open the application for the first time, you are greeted with a prompt asking you for your e-mail address.Although this is probably for newsletters and updates, we wish this step was optional.You quickly notice that only basic functionality is available in the application, which makes sense as the client is still in early access at the time of writing.You can browse resources per namespace, go through application logs, read and edit YAML configurations, and check the current resources used by your deployment.There is a metrics interface for the whole cluster as well, which supplies you with a structured and detailed view about your nodes.Since the application is still very young, it is lacking some functionality that you might expect or find in other clients.If you want something with more than basic functionality right now, this might not be the application you are looking for.However, if it has what you need, you will find that it will be very easy and straightforward to manage your Kubernetes cluster with this client.Infra slideshowConclusionNow the real question: which client should you use?As with any question about software, it depends.If you like to be lightning-fast and don’t mind struggling through a steep learning curve, K9s might be a tool for you.It’s the personal favorite of the authors this post, mainly because of its shortcuts and lightning-fast load times.If you are using some software that has plugins available for Octant, definitely give it a try. One of the authors prefers Octant as non-terminal GUI.The plugins add a lot of value to the tool and might make it a very compelling option for your use-case.If you often need to optimize your resource usage, want a client that just works, and is easy to use, go for Lens.This definitely hits the sweet spot between ease of use, stability, and available feature set.If you need cross-platform functionality (especially mobile support), then Kubenav is the application you want.If you want to get used to the UI and dive into this software, we would suggest trying it on all platforms.While Infra is still in early access, you can still use it in a production environment. If you like a simplistic yet structuredinterface with not too many features (yet), then Infra is the right choice for you.But our final advice is: just try them out yourself and see which fits your workflow best.Most of them share the same basic functionality and it just depends on your use-cases and workflow which one fits best."
      },
    
      "young-20professional-20programme-2020-08-24-ordina-young-professional-programme-2020-light-edition-html": {
        "title": "Ordina Young Professional Programme 2020 Light Edition",
        "url": "/young%20professional%20programme/2020/08/24/Ordina-Young-Professional-Programme-2020-Light-Edition.html",
        "image": "/img/kicks.png",
        "date": "24 Aug 2020",
        "category": "post, blog post, blog",
        "content": "IntroductionThe Young Professional Programme is made for starting developers who want to take their skills to the next level. This year, the light edition started on the 9th of March 2020 and lasted for about 3 months. We started with 4 junior developers. Youri was freshly graduated from school, while Eduard, Pieter and Hannelore had professional experience in IT.Every one of us is mainly interested in front end web development by sheer coincidence.In the first month of the Young Professional Programme we followed different workshops about the preferred stack used at Ordina. Sadly, about a week in, our sessions were held remotely due to the coronavirus. This made things more complicated, but because of the flexibility and effort of the mentors, the sessions were still clear and educational.SetupThe first few days of our training were centered around building a base for the upcoming weeks. Since we all just got our laptops the day before we didn’t have anything installed on them so that was the focus on the first training day.We got the chance to install all programs and tools required for the rest of our training, like Docker, Visual Studio Code, IntelliJ, Git,…A variety of different editors and IDEs were shown and we were allowed to pick the ones we preferred.GitWe also had a session about Git which is obviously an essential tool to understand for any modern developer. We saw the ins and outs of most of the Git commands and we practiced the most common ones. We didn’t just see Git but also how it came to be and its predecessors. Overall, it was an interesting session giving us a decent understanding of Git.Back endJavaJava is a general-purpose programming language that was released in 1995, which means it has been around for 25 years. It has become very popular over these years. In 2019, Java was one of the most used programming languages according to Github.Java is also the preferred back end language of JWorks. Yannick De Turck gave us a workshop around this programming language. We talked about the new features that were released with each version from Java 7 until 14. In between the theory lessons we made some hands-on exercises on the new features like lambdas, streams and optionals. This was also the last day before the corona outbreak in Belgium. This meant the rest of the Young Professionals Programme was given remotely.SpringThe Spring framework is an application framework and inversion of control container for the Java platform. It has become wildly popular in organizations and the Java community. We followed a three-day self study course, where we read the book ‘Spring in action’ by Craig Walls. This book gives you a general understanding of the Spring framework. On the second day there was a short Spring Presentation by Ken Coenen. Ken explained how all the Spring magic works behind the scenes and talked about the common components of the full Spring framework.Unit TestingWe also had a course about testing. We had an interactive hands-on session about multiple subjects like Test Driven Development, goals of testing, what to test, fixtures, mocks and assertions.DatabasesDatabases are essential to the development of applications, you will almost always need some form of data storage. So we followed a session with Tom Van den Bulck who showed us all kinds of databases. First we saw a traditional relational database, in this case PostgreSQL, a database which is widely known and used in many different projects around the world.Then we stepped away from relational databases to look at other kinds. More specifically MongoDB which is a non relational database, otherwise known as a NoSQL database. In MongoDB’s case it’s a document store. Here you don’t have a fixed schema, instead you upload JSON documents which you can then easily query using an SQL style query syntax. Apart from MongoDB we also learned about Cassandra, which is a NoSQL column store, and Redis, which is a key value store. Last but not least we studied a graph database, namely Neo4J.Front endHTML, CSS, JavascriptSince the Young Professional programme teaches a wide array of technologies we have seen quite a few front end technologies.Starting off with the basics of almost every front end technology, HTML5, CSS and JavaScript. In the first part of this session we took a dive into the basics of HTML and CSS, and after this we learned about the more recent features of HTML5 and CSS3. The remainder of the session was mainly about JavaScript. Throughout the session we had some exercises to have a bit of a hands-on approach.TypescriptBuilding on the basics we learned prior, we had a session about TypeScript, a derivative of JavaScript that is becoming more and more popular amongst most front end frameworks because of its similarity to JavaScript but also having types allowing stricter rules to be enforced and less random unexplainable errors. Similarly to the previous session we got an explanation on the basics of TypeScript which was then used in some exercises.AngularEventually we reached the end of our front end training with a 2-day Angular course. Everything we learnt the previous days was now being poured into a framework. In this course we, again, saw the basics of this massive TypeScript framework. We saw everything from dependency injection to property binding. Angular is a framework built by Google so there are lots of features, because of this we didn’t have enough time in those 2 days to really explore Angular in-depth. This is the preferred front end technology in JWorks. We were also going to use it in the dev case, so we would have enough time to understand it on a deeper level.CloudDockerTo start off our cloud education we had a session about Docker, which is essential to many cloud technologies. Tom Verelst gave an interesting hand-on session where he first explained the concepts of Docker, then we started writing our own yaml deployment files to set up our first full stack application.KubernetesKubernetes is an open source container orchestration framework, which is commonly used to deploy applications. Pieter Vincken taught us the basics of Kubernetes. We started with a presentation about the core concepts like pods, secrets and more. Then we learned how to deploy and manage our Docker containers using Kubernetes. During this session we got the hang of how you could manage your clusters by using various techniques provided by Kubernetes like auto scaling, load balancing,… It was really insightful to see how easy it is to deploy and manage your applications on a Kubernetes cluster.Security PrinciplesIn this day and age security is a hot topic. During this session we went over the basics on how to protect your application. From validating user input to database security, but also how to protect the users’ data.Clean CodeToday, one of the most important skills a developer has is Clean Code. To explore this complex topic, we read the book ‘Clean Code’ by Robert C. Martin, which is one of the must reads for every programmer.To be able to write clean code, an understanding of what bad code is is needed. This is also explained in the first chapter of the book.Some examples of topics in the book we still use everyday are:  Avoiding comments but using descriptive names  Functions should be small and do one thing  Use one word per concept  …Dev caseIntroductionFor our dev case, our task was to make a fully functional web application for MFC Combo. MFC Combo is an organization in youth care who guide families and children with a difficult situation at home. They use the “Combobox” in their meetings with the families. This is used to make communication easier and more fun.Right now, they have physical stickers and papers to let the child express their feelings, but they want it to become digital so that it’s easier to follow up a child.Technology &amp; MethodologiesWe worked Agile in two-weekly sprints with all the Scrum Ceremonies. Azure DevOps made this process easy and made sure our code was always clean and working as expected using CI/CD. For the front end, we used the Angular Framework to develop the application, combined with Jasmine and Karma for testing. Authentication was done using AWS Amplify, which is a development platform. This allowed us to quickly set up the authentication and authorization of our app. Sentry.io was used for front end logging and monitoring after deployment. This way, we had full control over our code, from development to deployment and even after deployment.For the back end, we used Java Spring Boot 2.3.1. Our Database was a NoSQL DynamoDB, for user management we used Cognito and finally for our file storage we used an S3 bucket. All of these services were set up in AWS. This means we used the AWS Java SDK to manipulate these services in an easy and structured way.ResultAfter 10 sprints we created an online platform for Combobox. This platform would assist them in their day-to-day operations. The application included various features like: drag &amp; drop capabilities, user management, file management, sticker management, overview of current cases, authorization &amp; authentication. One of the most challenging parts of the project was the implementation of the Drag &amp; Drop feature, for which we created a new directive. The result can be seen in the following screenshots:"
      },
    
      "conference-2020-08-19-percona-html": {
        "title": "Percona Live Europe 2019",
        "url": "/conference/2020/08/19/percona.html",
        "image": "/img/2019-12-20-percona/percona-live-europe2019_2.png",
        "date": "19 Aug 2020",
        "category": "post, blog post, blog",
        "content": "Percona Open Source Database Conference 2019  Percona, a leading provider of unbiased open source database solutions, organized its yearly European Open Source Database Conference in Amsterdam from September 30th till October the 2nd. This conference is an excellent opportunity to discover and discuss the latest open source trends, technologies and innovations, and of course Percona’s own solutions. After Amsterdam in 2016 and Dublin in 2017, it was the third time Ordina JWorks signed up for this conference so it is gradually becoming a tradition.Table of contents  Introduction  Keynotes  The state of the elephant: PostgreSQL  The state of the dolphin: MySQL  The state of the sea lion: MariaDBIntroductionNowadays, a company has to make many choices when determining an optimal database strategy. For example, does a company choose to lower the TCO (Total Cost of Ownership) or avoid vendor lock-in by switching from a commercial to an open source database management system? Another choice your company faces is whether a relational or non-relational database management system or both should be chosen. And does a company bring its data to the cloud, or keep it on-premise? Choosing the right solution for the database strategy can sometimes be overwhelming.We looked for answers to these questions during the Percona Open Source Conference. Of course this conference is not only an excellent opportunity for Percona to showcase its own products such as Percona Server for MySQL, Percona XtraDB Cluster, Percona Backup for MongoDB and much more, but it also shows that there are many interesting things to tell about databases and more specifically about open source databases. As a supporter of commercial database systems, you’re not in the right place here.As in previous editions of the conference, the concept remains unchanged.The conference starts with a ‘Tutorials Day’ on the first day followed by 2 days of actual conference. Percona is the organizer of the conference so their tools and services will be discussed in detail, but with about 90 talks about everything that has to do with relational and non-relational open source databases, everyone will get their money’s worth. It only makes it more difficult to choose which sessions to attend.KeynotesEach self-respecting conference starts with one or more keynotes. Percona Live was no exception to this and kicked off with 3 keynotes :    The state of open source databases  MySQL The state of the dolphin  Dynamic / Cloud Infrastructure AutomationThe first keynote was a great plea for the cloud and open source software and the benefits of both were well discussed. Also it mainly summarised the results of The Open Source Data Management Software Survey conducted by Percona in the run-up to the conference. More insights and useful information can be found in the Open Source Data Management Software Survey.To gain a good insight into how the open source database landscape is evolving, various sources such as db-engines and Stack Overflow were consulted. The graph below shows the most commonly used databases according to Stack Overflow.  In these disruptive times, the popularity of open source databases is undoubtedly increasing. But how are the elephant, dolphin and sea lion currently doing? Which one is the most popular and how do they relate to each other? A small tour along the mini zoo of relational open source databases.The state of the elephant: PostgreSQLPostgreSQL, as we know it under this name, is celebrating its 24th anniversary this year and the chart below clearly shows that its popularity is still growing. According to DB-engines that measures database popularity, PostgreSQL is now the fourth most popular database, after Oracle, MySQL and Microsoft SQL Server, and was referred to as ‘Database of the year’ in both 2017 and 2018. In 2019 it had to pass on this title to MySQL. The last major release, version 12, was launched the day after the Percona conference.  Although the focus of the conference was on MySQL, PostgreSQL was also well represented at the conference with more than 37 talks on the subject. The PostgreSQL sessions carried away my preference not least because it is a popular migration target, but also because the popularity increased in the most recent years, many developers think it is the coolest database out there.Day 1 (Tutorials):      PostgreSQL For Oracle and MySQL DBAs and For Beginners    From a Database administrator point of view this was the most interesting training.   After a brief overview of the different features within Postgres, the specific terminology was also discussed.  Within this training the necessary attention was also paid to the installation of PostgreSQL, the architecture, backup and partitioning within PostgreSQL.          Introduction to PL/pgSQL Development    While the morning tutorial was intended for DBAs, the developers were given due attention in the afternoon.   The standard PostgreSQL distribution includes four procedural languages, PL/Tcl, PL/Perl, PL/Python and PL/pgSQL.   In addition, there are a number of procedural languages that are developed and maintained outside the core PostgreSQL distribution, like PL/Java, PL/PHP, PL/Ruby and a few others.   This tutorial only focused on development in PL/pgSQL.   Because of the many similarities between PL/pgSQL and the programming language of Oracle PL/SQL, this 3-hour session was a good repetition for a developer experienced in PL/SQL programming.   Fortunately, the differences between the two were also sufficiently addressed.   And just these differences are important if you want to port an application from Oracle to PostgreSQL.  Day 2:      Why PostgreSQL is Becoming A Migration Target in Large Enterprises    The driving reasons for migrating to PostgreSQL are not only initiated from the management\tof an organization, who is particularly interested in the lower TCO, the no-vendor lock-in and the licencing policy.  Also developers, operation teams, architects all have their reasons to migrate from proprietary databases or noSQL databases to PostgreSQL.  PostgreSQL is not only the most loved relational database among developers, it contains the latest SQL standards, contains many programming languages and it supports JSON.   Operation teams especially appreciate the legendary stability, the options for High Availability and the good backup tools.   For architects, the support of procedures in PostgreSQL is very interesting.   These are very helpful for Oracle to PostgreSQL migration.   The interoperability and sharding capabilities are also a plus.   The session ended with a demo of Oracle Foreign Data Wrappers.        Pg_catalog Unrevealed! That Part of PostgreSQL You Are Probably Underusing    This talk was intended to review the possibilities offered by the PostgreSQL catalog.  Plenty of information is available there but it is unknown to many users, and therefore unused.   The speaker demonstrated in an enlightening way how to exploit the catalog, how to send the information to other monitoring tools, and how the tables in pg_catalog are fundamentally interconnected to other topics such as performance, replication, MVCC, security, the universe and everything.        Join Heterogeneous Databases Using PostgreSQL Foreign Data Wrappers    I followed this presentation because I was especially interested in how PostgreSQL can communicate with other databases like Oracle, MySQL, PostgreSQL itself or even NoSQL data sources like MongoDB.  The feature to achieve this are Foreign Data Wrappers.  Simply put, it allows you to create foreign tables in a PostgreSQL database that are proxies for some other data source.  With a few examples it was shown how you can easily set up these Foreign Data Wrappers.  An extensive overview of the different data wrappers that exist can be found on the wiki.  Day 3:      Parted Ways with Partitioning? It’s Time to Reconsider    When a DBA has to deal with very large tables of which the performance has to be improved, partitioning comes into the picture.   Partitioning splits a table into multiple tables, and is generally done in a way that applications accessing the table don’t notice any difference, other than being faster to access the data that it needs.   Before PostgreSQL 10, table partitioning in PostgreSQL was only possible via table inheritance, but with release 10, declarative partitioning made its appearance, which actually means that natively partitioned tables were introduced.   Declarative partitioning in PostgreSQL 10 initially provided built-in support for Range partitioning and List partitioning and in version 11 has also been added.   Many more improvements were added.   During this talk it was shown how Partitioning has evolved within the latest versions of PostgreSQL.        Handling Transaction ID Wraparound in PostgreSQL    Transaction IDs in PostgreSQL are stored in a 32-bit counter.   But that involves potential risks.  Without any form of intervention, transaction ids could get exhausted after 2 billion transactions.   A way to deal with this is described in detail on the following blog.        Top 10 Mistakes When Migrating From Oracle to PostgreSQL    Very interesting session that exposes a handful of grammatical differences, for which the functionality is the same, between both database systems.   This way you learn, for example, that synonyms or nulls do not exactly mean the same thing in Oracle and PostgreSQL.        PostgreSQL Plan at Execution Time: A Quick Show    The last lightning talk about PostgreSQL on the conference showed a look behind the scenes of Explain plan in PostgreSQL and what decisions could be made from it during the execution phase.   This was demonstrated by means of a demo.  Migrating to PostgreSQLPostgreSQL has become a mature database and is known for its reliability, feature robustness, and performance. Able to handle all levels of workload with thousands of tools, extensions, connectors and community-contributed add-ons, it is a popular choice for Oracle database migration. But one of the main drivers is of course the cost, because Oracle’s licensing model is often a nightmare. PostgreSQL commends itself as the world’s most advanced open source relational database.So when the company’s management has decided to migrate to PostgreSQL, is it a difficult task as Oracle DBA to make the switch? Not really. Because the fundamentals and features remain largely the same, the learning curve is not steep at all. A good starting point is the wiki of Postgres.There are several tools on the market that can assist you in converting an Oracle database into a PostgreSQL database, but keep in mind that there is no tool that can convert a 100% Oracle database into PostgreSQL.Some manual changes are required. Below is a list of frequently used conversion tools, but a detailed description of each tool is beyond the scope of this blog post.  Ora2pg : Ideal for large migration projects  PostgreSQL Foreign Data Wrapper for Oracle : Ideal to move schemas and data  Orafce : Compatibility functions  AWS Schema Conversion ToolThe pitfalls you often encounter as an administrator were discussed in the session “Top 10 Mistakes When Migrating From Oracle to PostgreSQL”.The state of the dolphin: MySQLIn 2019, MySQL took over the torch again from PostgreSQL as Database of the year. MySQL is one of the most widely used databases in the world and, in terms of popularity, only has to accept Oracle in front of them. The latest release, release 8, was released on April 19, 2018. With no less than 250 new features it was a big leap forward from version 5.7. An overview of these new features can be found here. During several sessions some of these new features were discussed in more detail.  A selection of some interesting talks on MySQL :  How to Upgrade Like a Boss to MySQL 8.0  Enhancing MySQL Security  Billion Goods in Few Categories How Histograms Save a Life?  MySQL 8.0: The New Replication Features  MySQL 8.0.18 latest updates : Hash join and EXPLAIN ANALYZE  RUNNING MYSQL IN K8SThe state of the sea lion: MariaDBFrom the MariaDB graph we can deduce that the popularity has risen sharply in recent years, but the rate of increase has slowed down over the past year. MariaDB has not followed the same release path as MySQL in recent years and the latest stable release of MariaDB is 10.4.  There were only a handful of sessions on MySQL’s fork at the conference. But with a talk that takes a look at the query optimizer of MariaDB and another one that goes into the security setup within MariaDB, two very interesting topics were covered.  How a database optimizer gets your data, fast  MariaDB Security Features and Best PracticesTo see the full schedule of the Conference have a look here: Percona Live Europe Schedule.Useful links &amp; further reading  Percona site  PostgreSQL site"
      },
    
      "security-2020-08-18-securing-applications-azure-ad-html": {
        "title": "Securing Angular and Spring Boot applications with Azure AD",
        "url": "/security/2020/08/18/Securing-Applications-Azure-AD.html",
        "image": "/img/azure-ad/azure-ad.png",
        "date": "18 Aug 2020",
        "category": "post, blog post, blog",
        "content": "  Azure Active Directory (Azure AD) is Microsoft’s cloud-based identity platform.In this blogpost, we will discuss how to use it to secure web applications with OAuth 2.0 and OpenID Connect (OIDC).More specifically an Angular single-page application (SPA) which makes calls to a Spring Boot back-end.Table of contents  Finding the perfect OAuth flow for your needs          Basic OAuth Terminology      The OAuth Client      OAuth flows        The Azure AD part  The Spring Boot Part          Azure Starters for Spring Boot      The Spring Boot Implementation        The Angular Part          The Angular Library      Example Application: Tour of Heroes      The Angular Implementation      Finding the perfect OAuth flow for your needsBefore diving into Azure AD and how to use it for authentication and authorization of your apps, it’s important to think about the OAuth set-up that you want.From personal experience, the libraries and documentation provided by Microsoft can be rather inconsistent and confusing. To straighten things out, we’ll start by discussing which OAuth set-up we need and why.Then we’ll configure Azure AD and the applications to make everything work together.Please note that this post assumes you have some notions of OAuth. Don’t worry if your knowledge is a little lacking as we’ll recap on the necessary parts.Basic OAuth Terminology    Resource Owner    You: a user who interacts with the system and has some resources on a (resource) server.    Resource Server    Server where (your) protected resources are served from.    Authorization Server    Server which validates your credentials. Hands out tokens to registered clients.    Client        An application which uses tokens from the authorization server to access the resource server on behalf of the resource owner.    It orchestrates the process to obtain these tokens.    The OAuth ClientDo we want the single-page app to be the OAuth client or should the Spring Boot back-end fulfill that role?If the SPA is the OAuth client, the Spring Boot application will be configured as a resource server.This means it’s up to the Angular application to orchestrate the process of obtaining access tokens from the authorization server. These tokens then grant access to resources from the Spring Boot back-end.For the other scenario, where the Spring Boot back-end acts as the OAuth client (and resource server), this orchestration will be performed in the back-end. In that case, the Angular application will only maintain a session with the back-end.There are multiple advantages and disadvantages to both scenarios.However, the biggest trade-off for our scenario is:SPA as OAuth client / back-end as resource server:✅ Back-ends can be stateless: no session required❌ Less secure: access token stored in the browserSpring Boot back-end as OAuth client:❌ Has to be stateful: session required✅ More secure: tokens only in the back-endHaving a stateless back-end makes it very easy to create or destroy new instances of it. Requests can go to any of those instances without the need for sticky load balancing or distributed sessions.The downside is that the tokens have to be stored in the browser, which can leak more easily than from a secure back-end. We control the back-end but not the user’s computer, network, browser or its plugins. On top of that, browsers also lack a secure storage mechanism, unlike apps on mobile devices.In this blogpost, we will go with the first approach where the Angular app is the OAuth client.This means our set-up will be as follows:            OAuth term      Concrete application                  Resource Owner      You              Resource Server      Spring Boot app              Authorization Server      Azure AD              Client      Angular app      OAuth flowsOAuth has multiple flows.The flow determines how tokens will be obtained from the authorization server by the client.The original OAuth specification defines four flows:  Authorization Code: The client sends the user to the authorization server to obtain an authorization code. The client then exchanges this code for an access token.  Implicit: Simplified authorization code flow. The client sends the user to the authorization server to obtain an access token.  Resource Owner Password Credentials: The client uses the user’s credentials to ask the authorization server for an access token.  Client Credentials: The client ask the authorization server for an access token on its own behalf.The OAuth 2.0 Security Best Current Practice document makes it very clear: use the client credentials flow for client-to-client purposes, where a client acts on its own behalf.In all other cases, the authorization code flow with PKCE is the way to go.Proof-Key for Code Exchange or PKCE (pronounced ‘pixy’) is an extension to OAuth which prevents interception attacks and enables the authorization code flow for public clients. If you are interested in what public clients are and how PKCE works, you can learn more about it in this blogpost.In our case, a user is involved, so the right flow is the authorization code flow with PKCE.  PKCE is (nearly always) mandatory in the current OAuth 2.1 proposal by Aaron Parecki.The Azure AD partWe already discussed that our Angular app will be an OAuth client.All clients have to be registered at the authorization server, so this is what we have to configure in Azure AD.  A client is often called app(lication) in Azure AD.We can do this via the Azure Portal.Log in and then navigate to Azure AD.You should find the App registrations button on the left.  Click New registration and fill in the form:  Pick a name that’s appropriate for your client. We can also set up the redirect URI here. This is the URI where the user will be redirected to after logging in on the authorization server. It’s important that this matches the URL in our Angular configuration later. The supported account types option depends on who should be able to log in to your app.Notice how we don’t need to configure a client secret? Single-page apps can’t keep secrets hidden very well, which is why they have to be a public client. The authorization code flow used to be for confidential clients only, which use a secret or certificate to authenticate with the authorization server.PKCE is what makes the authorization code flow possible for these kinds of clients.Wait.. That’s it?Yes.Well, kind of.Later on, we will have to make an adjustment, so don’t close the portal just yet.Once the client has been registered, we also need the client id and tenant id values for our application configuration.I’ve changed mine to &lt;&lt;&lt;client_id&gt;&gt;&gt; and &lt;&lt;&lt;tenant_id&gt;&gt;&gt; for demonstration purposes, so don’t forget to change these values to yours in the configuration examples.  The Spring Boot PartOur example Spring Boot application is a small API which serves some resources for the Angular application which we’ll discuss further down.This is probably the easiest part to arrange, but also where I see most people get really confused.Azure Starters for Spring BootIf you want to set up the Spring Boot application as an OAuth client, you could use the Azure Active Directory starter from the Spring Initializr.It’s relatively hassle-free, given that you adjust some things left and right.However, we want to set up our Spring Boot application as a resource server (rather than an OAuth client). For this, we will only use the spring-boot-starter-oauth2-resource-server dependency from Spring itself.This further limits our dependencies on the Microsoft libraries.The Spring Boot ImplementationWe start by adding some extra libraries to the existing application.Note that there are no versions defined as these should come from the Bill Of Materials (BOM).// build.gradledependencies {\t...\timplementation 'org.springframework.boot:spring-boot-starter-oauth2-resource-server'\timplementation 'org.springframework.security:spring-security-oauth2-jose'}or if you prefer Maven:&lt;dependencies&gt;    ...    &lt;dependency&gt;      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;      &lt;artifactId&gt;spring-boot-starter-oauth2-resource-server&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;      &lt;groupId&gt;org.springframework.security&lt;/groupId&gt;      &lt;artifactId&gt;spring-security-oauth2-jose&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;We can now set up the authorization part: who has access to what.import org.springframework.context.annotation.Configuration;import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter {    @Override    protected void configure(HttpSecurity http) throws Exception {        http.cors().and() // (1)            .authorizeRequests().anyRequest().authenticated() // (2)            .and()            .oauth2ResourceServer().jwt(); // (3)    }}There is a lot happening in a few lines here.Let’s break it down:  http.cors() allows Cross-Origin Resource Sharing (CORS) preflight checks to succeed.  We want all requests to the application to require authentication. If no authentication is provided, a 401 status will be returned.Note that this is different if you configure the Spring Boot application as an OAuth client. In that case, the caller would be redirected to the login page.  Here we tell the application to behave as a resource server. Authentication should be provided via JWT access tokens.To learn more about JWT tokens, you can check out my other blogpost about OAuth.Our JWT access tokens are signed by Azure AD and our application should check if their signature is correct.Azure AD has an endpoint with the public key to do so, which we have to configure in our application.A first option is to configure the issuer URI so that it can find the correct endpoint in the discovery document.The discovery document is a convenience endpoint where a lot of the client configuration can be found, including the web keys endpoint.  You can find the discovery document by appending .well-known/openid-configuration to the issuer URI.# application.propertiesspring.security.oauth2.resourceserver.jwt.issuer-uri=https://sts.windows.net/&lt;&lt;&lt;tenant_id&gt;&gt;&gt;/Alternatively, we can search the keys endpoint ourselves in the discovery document and then provide this JSON web key (JWK) endpoint straight away:# application.propertiesspring.security.oauth2.resourceserver.jwt.jwk-set-uri=https://login.windows.net/common/discovery/keysIn a real production configuration, I personally prefer to use the issuer URI as it offers most configuration via a single configuration property.This will issue a network call to the discovery document when the application starts, so when testing in an environment where Azure AD is not reachable, this will cause the application to crash.This is where the JWK URI can save the day.The Angular PartWhen we now browse to any back-end endpoint, we receive: HTTP 401 Unauthorized.Let’s fix this in our example Angular application.The Angular LibraryAzure AD has quickstart guides for different kinds of applications. For Angular, however, the msal-angular library currently only supports the implicit flow.Since the current best practices draft strongly discourages the implicit flow in favour of the authorization code flow with PKCE, we will look for an alternative.  The Microsoft Authentication Library for JavaScript (MSAL) should have support for PKCE soon, but at the time of writing, this feature was still in alpha.Even when there will be Microsoft libraries that can solve this problem, I try to stay vendor-neutral whenever possible.This makes it relatively easy to switch from one OAuth provider to another one like Auth0, AWS Cognito, Okta, Keycloak, …The only downside is that vendor-specific features will not be available.An example of this is the On-Behalf-Of flow (OBO), which is only supported by the Microsoft libraries.Since OAuth and OIDC are standards, we should be able to use any (certified) library which supports these.I say “should”, as the specifications left a lot of room for tinkering and additions. This will become clear during the implementation.My favourite go-to library is angular-oauth2-oidc by Manfred Steyer. This is also the one we’ll use in this example.  Alternatively, you can use the msal-angular library if you are fine with the implicit flow for now.Example Application: Tour of HeroesAs an example of an Angular application, we will use the Tour of Heroes Angular tutorial application. Feel free to use your own application as there should not be too many differences.Because the Tour of Heroes application uses an in-memory API instead of a Spring Boot application, we should change this in the code.Of course, if you are using your own application or checked out the code from the repository, you can skip this step.// app.module.ts// REMOVE this part:// The HttpclientInMemoryWebApiModule module intercepts HTTP requests// and returns simulated server responses.// Remove it when a real server is ready to receive requests.HttpclientInMemoryWebApiModule.forRoot(  InMemoryDataService, { dataEncapsulation: false })and also change the url in the HeroService:// hero.service.tsprivate heroesUrl = 'http://localhost:8080/api/heroes';Now we’re all set to go.The Angular ImplementationWe start by installing the angular-auth2-oidc library:npm i angular-oauth2-oidc --saveNext, we import the OAuthModule module:// app.module.tsimport { HttpclientModule } from '@angular/common/http';import { OAuthModule } from 'angular-oauth2-oidc';// etc.imports: [\t// etc.    HttpclientModule,\tOAuthModule.forRoot({      resourceServer: {        allowedUrls: ['http://localhost:8080/api'],        sendAccessToken: true      }    }),]This is also where we define which APIs need the access token. In our case, this will be a Spring Boot application that’s running on port 8080 and will serve from /api.Next up is the OAuth configuration.// auth.config.tsimport { AuthConfig } from 'angular-oauth2-oidc';export const authConfig: AuthConfig = {    issuer: 'https://login.microsoftonline.com/&lt;&lt;&lt;tenant_id&gt;&gt;&gt;/v2.0',    redirectUri: window.location.origin + '/dashboard',    clientId: '&lt;&lt;&lt;client_id&gt;&gt;&gt;',    responseType: 'code',    strictDiscoveryDocumentValidation: false,    scope: 'openid api://&lt;&lt;&lt;client_id&gt;&gt;&gt;/app',}The issuer, redirectUri, clientId and responseType are pretty straightforward. All you need to do is to fill in the placeholder with the values from Azure AD. You can copy the values from the overview of the app in the Azure Portal.  This is where we need to tweak some configuration settings for the library to work with Azure AD.strictDiscoveryDocumentValidation needs to be disabled due to the fact that not all URLs in the discovery document start with the issuer URL. This makes strict parsing fail, so we disable it.  Strict discovery document validation is a best practice which protects against a threat where an attacker manages to fake the discovery document.You might also have noticed the weird looking api://&lt;&lt;&lt;client_id&gt;&gt;&gt;/app value in the list of scopes. The reason why we do this is explained very well in this Medium blogpost but boils down to the fact Azure AD uses a nonce in a special way in its JWT header.This breaks the standard JWT validation.If we include an application specific scope here, this will no longer be the case. Our Angular application won’t actually care for this as it just passes access tokens to the Spring Boot back-end.The validation there will fail, resulting in a 401: Unauthorized. You can define this scope in the Azure Portal, under Expose an API &gt; Add a scope.    This application-specific scope can have any name, so it doesn’t have to be app. Just make sure you use the same scope in the application as the one you defined in the Azure Portal.Another requested scope we configure is openid.This indicates that we also want to log in the user.We will not only receive an access token to contact the back-end API, but also an id token with information about the logged-in user.  Azure AD will serve an id token, regardless of the open-id scope. But we include it anyway to respect the specification.The next step is to trigger the login when a user has not logged in yet:// app.component.tsconstructor(private oauthService: OAuthService) {    this.oauthService.configure(authCodeFlowConfig); // (1)    this.oauthService.loadDiscoveryDocumentAndLogin(); // (2)    this.oauthService.set-upAutomaticSilentRefresh(); // (3)}  We set up the OAuthService with the configuration from the previous step.This makes sure it uses the authorization code flow + PKCE with the correct parameters.  The discovery document will be loaded, which is the issuer URI plus the .well-known/openid-configuration suffix and then start the login process.  As access tokens have a short lifespan, we want them to be automatically refreshed in the background.We can now try out the application and should be redirected to the Microsoft login page.After logging in, when we browse to the heroes page, we can see the 401 is gone, and the heroes are fetched again.Implement these steps or download the final front-end and back-end code from Github to try it out."
      },
    
      "frontend-2020-08-15-tailwindcss-html": {
        "title": "Using Tailwind CSS as a base for a Design System",
        "url": "/frontend/2020/08/15/tailwindcss.html",
        "image": "/img/2020-08-15-tailwindcss/tailwind-500-293.jpg",
        "date": "15 Aug 2020",
        "category": "post, blog post, blog",
        "content": "  In the beginning of the year we had the opportunity to build a design system for a large customer.So how did we start?Well, putting together designers and developers to find the correct tools and frameworks to get it done.Table of Contents  Choosing the correct CSS strategy  Evaluating Tailwind CSS  ConclusionChoosing the correct CSS strategyWhen we (developers) talked to the designers, they wanted a clean CSS sheet without unnecessary classes.This means we only need to foresee the classes they wanted to be available in the design system, nothing more.      Building the CSS from the ground up was the first idea.That way we could only write classes we actually needed.Since we didn’t have all the time to write so much CSS classes, we decided we couldn’t do this.        CSS libraries like Bootstrap, Foundation, Materialise, … have too much classes by default. As a side note, we looked for ways to disable certain classes, but those libraries are too complicated to customise.        Another solution could be the use of ‘utility-first’ CSS frameworks.We liked the idea of Tailwind CSS to use a single config file and generate all the necessary classes.  Evaluating Tailwind CSSWhen we had a closer look at Tailwind we saw a lot of positive points, but some negative as well.Below you can find some valuable points we like to share:Configurable in JavaScriptWe’re JavaScript engineers, so we love the fact we can just write our config in JSON.We used constants to define the various configurations and then put it in the module exports:// tailwind.config.jsconst colors = {    error: {        100: '#fce9ea',        500: '#e72f3c',        700: '#971e26',    },    success: {        100: '#e7f5ed',        500: '#34ab66',        700: '#226f42',    },};const fontSize = {    s1: '0.75rem',    s2: '0.875rem',    s3: '1.125rem',    m1: '1.25rem',    m2: '1.5rem',    m3: '1.75rem',    l1: '2.25rem',};const fontWeight = {    regular: '400',    bold: '700',};const screens = {    sm: { max: '640px' },    md: { min: '641px', max: '960px' },    lg: { min: '961px', max: '1280px' },    xl: { min: '1281px' },};module.exports = {    theme: {        colors,        fontSize,        fontWeight,        screens,    },};As you can see this is very customisable.You can use your own namings for almost everything like colors, fonts, screens, …More info can be found in the configuration documentation.ResponsiveYou can completely customise your responsive breakpoints in the config file.By default, Tailwind’s breakpoints only include a min-width and don’t include a max-width, which means any utilities you add at a smaller breakpoint will also be applied at larger breakpoints.If you’d like to apply a utility at one breakpoint only, the solution is to undo that utility at larger sizes by adding another utility that counteracts it.Or you simply overwrite the defaults, so they include a min and max value.// tailwind.config.jsmodule.exports = {  theme: {    screens: {      'tablet': { max: '640px' },      // =&gt; @media (max-width: 640px) { ... }      'laptop': { min: '641px', max: '1024px' },      // =&gt; @media (min-width: 641px) and (max-width: 1024px) { ... }      'desktop': { min: '1015px'},      // =&gt; @media (min-width: 1025px) { ... }    },  }}Once you configured the screens, you can use the screen prefix everywhere, so your design can be responsive.&lt;div class=\"bg-error-500 tablet:bg-error-100\"&gt;    Error message&lt;/div&gt;Component classes can be extractedWhile you can do a lot with just utility classes, as a project grows it can be useful to codify common patterns into higher level abstractions.Keeping a long list of utility classes in sync across many component instances can quickly become a real maintenance burden, so when you start running into painful duplication like this, it’s a good idea to extract a component.&lt;!-- Repeating these classes for every button can be painful --&gt;&lt;button class=\"bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\"&gt;  Button&lt;/button&gt;For small components like buttons and form elements, you can use Tailwind’s @apply directive to easily extract common utility patterns to CSS component classes.Here’s what a .btn-blue class might look like using @apply to compose it from existing utilities:.btn-blue {  @apply bg-blue-500 text-white font-bold py-2 px-4 rounded;}.btn-blue:hover {  @apply bg-blue-700;}Usable with preprocessors like SassSince Tailwind is a PostCSS plugin, there’s nothing stopping you from using it with Sass, Less, or other preprocessors, just like you can with other PostCSS plugins like Autoprefixer.The most important thing to understand about using Tailwind with a preprocessor is that preprocessors like Sass run separately, before Tailwind.This means that you can’t feed output from Tailwind’s theme() function into a Sass color function for example, because the theme() function isn’t actually evaluated until your Sass has been compiled to CSS and fed into PostCSS.ReadabilityA negative point could be the readability.Sometimes a list of classes can grow big and is unable to be understood properly.The need to extract a component class can be a solution (as mentioned above), but this is not always possible.// example of a big class-list&lt;div class=\"grid sm:col-span-2 md:col-span-3 lg:col-span-3 xl:col-span-3 w-full bg-blue-500 text-white font-bold h-20 py-2 px-4 my-5 mt-4 rounded\"&gt;&lt;/div&gt;Custom Tailwind pluginsAlthough we could do everything with Tailwind, we needed to have some Sass variables separately available to use in our web components.Therefore we used a plugin to generate Sass variables based on your Tailwind config.More information to write and use your own plugins can be found on Tailwind’s documentation website.// tailwind.config.jsconst plugin = require('tailwindcss/plugin')module.exports = {  plugins: [    require('tailwind-scss-variables')(        ['theme.colors', 'theme.screens'],        './src/scss/partials/_tailwind-variables.scss',    ),  ]}ConclusionAfter using Tailwind CSS for almost 6 months now.We still find we made the correct choice.Given the fact we still don’t have the time to write css ourselves, finding the correct utility class is fairly easy using the documentation, or even on a (printed) cheat sheet."
      },
    
      "architecture-2020-08-15-angular-custom-control-html": {
        "title": "Creating a custom form control in Angular",
        "url": "/architecture/2020/08/15/angular-custom-control.html",
        "image": "/img/2020-08-15-angular-custom-control/qyt3dcd64l.png",
        "date": "15 Aug 2020",
        "category": "post, blog post, blog",
        "content": "A part of good application architecture is using the correct tools for the job.With forms in Angular, this means knowing when to use Template Driven Forms vs. Reactive Forms.In short, this depends mostly on the size of your form.A simple login screen may use Template Driven Forms, whereas a more advanced web form should use Reactive Forms.These forms consist of FormGroups and FormControls, keeping the form value organised.By default, Angular already allows to bind a HTMLInputElement or HTMLSelectElement to a control using the FormControl and FormControlName (in combination with FormGroup) directives.This is enough for most forms, but sometimes there is a need for something more specialised, for example a date picker or a slider.There are numerous packages on npm providing these and most component libraries also include the most common controls for your development pleasure.But sometimes you can’t find the correct package to match your needs.I still see many developers use a default text input field, and parsing the value after the form is submitted.Obviously there is a better way and I’ll describe it below.Case studyAs a case study, I’ve chosen to create a color picker.This picker should not have any traditional input control, but instead display the selected color in a rounded circle.Clicking on the control should open a color picker and allow the user to edit the value.The control should be part of a FormGroup, so that Validators can be added.For those interested, the full code of this article is available at StackBlitz.The color pickerLet’s start with creating a color picker.Because developers shouldn’t do everything themselves, I opted to reuse an existing color-picker dependency.I just took the first one I came across.To challenge myself, I tried to look for a control picker with as little helping functions as possible.The only thing I wanted from it, is to show it on demand, hide it on demand and get/set the value.Luckily my first pick was just that.No TypeScript, no open/close status, nothing fancy.This is what it can do:AColorPicker.from(selector)  .on(eventName, callback)  .off(eventName, callback);The possible events are “change”, “coloradd” and “colorremove”.The last two are supposed to help with a color palette, but that’s out of scope, which leaves the only event being “change”.Creating a color picker componentAs a rule of thumb, a component that will be used as a control should be a dumb component.I mostly set the template and style inline for dumb components to keep them from growing too big.This is the eventual code of the component.import { AfterViewInit, ChangeDetectionStrategy, ChangeDetectorRef, Component, OnDestroy, ViewChild, ElementRef } from '@angular/core';import * as AColorPicker from 'a-color-picker';@Component({  selector: 'my-color-picker',  template: `    &lt;div class=\"picker-icon\" [class.disabled]=\"isDisabled\" style=\"background: {{ color }}\" (click)=\"openPicker()\"&gt;&lt;/div&gt;    &lt;div #pickerElement class=\"picker\" acp-show-rgb=\"no\"     acp-show-hsl=\"no\"     acp-show-hex=\"no\"&gt;      &lt;button *ngIf=\"open\" type=\"button\" (click)=\"closePicker()\"&gt;close&lt;/button&gt;    &lt;/div&gt;  `,  styles: [`    .picker {      display: flex;      flex-direction: column-reverse;      position: absolute;    }    .picker-icon {      width: 1em;      height: 1em;      margin: 2px 0;      border: 2px solid white;      box-sizing: border-box;      box-shadow: 0 0 1px 1px gray;      border-radius: 50%;    }    .picker-icon.disabled {      opacity: 0.5;    }  `],  changeDetection: ChangeDetectionStrategy.OnPush})export class ColorPickerComponent implements AfterViewInit, OnDestroy {  open = false;  color = '#fff000';  isDisabled = false;  private _picker = null;  private get picker() {    return this._picker ? this._picker[0] : null;  }  @ViewChild('pickerElement') pickerElement: ElementRef;  constructor(    private changeDetectorRef: ChangeDetectorRef  ) { }  ngAfterViewInit() {    this.initializePicker();    this.addChangeListener();  }    ngOnDestroy() {    this.removeChangeListener();  }  openPicker() {    if (!this.open &amp;&amp; !this.isDisabled) {      this.picker.show();      this.open = true;    }  }  closePicker() {    if (this.open) {      this.picker.hide();      this.open = false;    }  }  private initializePicker() {    if (this.pickerElement) {      this._picker = AColorPicker.from(this.pickerElement.nativeElement);      this.picker.hide();      this.picker.color = this.color;    } else {      throw Error('Picker could not be initialized');    }  }  private addChangeListener() {    this._picker.on('change', event =&gt; {      this.color = event.color;      this.changeDetectorRef.markForCheck();    });  }  private removeChangeListener() {    this._picker.off('change');  }}This component can now display the color picker, by clicking the icon.It can also be closed by clicking the close button that’s added to the picker.I like to keep ChangeDetection.OnPush on dumb components, but that means that I need to inject the ChangeDetectorRef to update the color while the picker is being used.Without the markForCheck, the color would only be updated when the picker is closed.It's working! It's working!Making a control out of itNow comes the interesting part.I want to be able to use my-color-picker with the formControl and formControlName directives.If you’d just use these directives, you’d get the error “No value accessor for form control with name: &lt;the name of your control&gt;”.Simply put, this means that the control does not know what to bind to.So we start by implementing the ControlValueAccessor interface from @angular/forms into the component.This interface has three functions that need to be implemented and one optional function.  writeValue(obj: any): void  registerOnChange(fn: any): void  registerOnTouched(fn: any): void  setDisabledState?(isDisabled: boolean): voidThe function setDisabledState is optional, but also the easiest to understand.It’s triggered by the formControl.enable() and formControl.disable() functions.The other functions may be a bit more difficult to understand.setDisabledState(isDisabled: boolean): void {  this.isDisabled = isDisabled;  if (this.isDisabled) {    this.closePicker();  }  this.changeDetectorRef.markForCheck();}The function writeValue is called every time formControl.setValue(obj) or formControl.patchValue(obj) is called.The type of obj is by default any, because ReactiveForms are still not strongly typed.You can for example call numberControl.setValue('not a number') and your application will build correctly.Even at runtime you’d probably not get an error, but that doesn’t mean this is a valid value.That’s why there are validators.It’s best to parse this value to something understandable for your component.In this case, the color will be a string containing either the hexadecimal value like #ff0000 or a word like red.The a-color-picker-package luckily allows all string values and will convert to a hexadecimal value that matches.A downside of this package is that it has to be bound after view init.However writeValue will be called before ngAfterViewInit, so the initial form value might not be set correctly.That is why the value is also stored in color, which is used to set the initial value when the picker is initialized.writeValue(obj: any): void {  if (this.picker) {    this.picker.setColor(`${obj}`);  } else {    this.color = `${obj}`;  }}The functions registerOnChange and registerTouched are used to set a callback function when the control’s value changes or when it’s being touched without a change.These callbacks can be called whenever you want in your component and they are usually the setValue and markAsTouched properties.They are usually implemented like this:private _onChange: (color: string) =&gt; void;private _onTouched: () =&gt; void;registerOnChange(fn: (color: string) =&gt; void): void {  this._onChange = fn;}registerOnTouched(fn: () =&gt; void): void {  this._onTouched = fn;}We’ll be using them in our change event, like this:closePicker() {  if (this.open) {    this.picker.hide();    this.open = false;    this._onTouched();  }}private addChangeListener() {  this._picker.on('change', event =&gt; {    this.color = event.color;    this.changeDetectorRef.markForCheck();    this._onChange(this.color);  });}If we test the control now, we’ll still get the error “No value accessor for form control with name: &lt;the name of your control&gt;”.The reason is that, even though we implemented our component as a ControlValueAccessor, we still didn’t specify that the control should bind to it.There are two ways to do this.The easiest way is to provide the component as a value accessor, by adding the following to the component’s decorator:providers: [       {    provide: NG_VALUE_ACCESSOR,    useExisting: forwardRef(() =&gt; ColorPickerComponent),    multi: true  }]The injection token NG_VALUE_ACCESSOR can be imported from @angular/forms and forwardRef can be imported from @angular/core.Simply put, the function forwardRef allows the dependency injector to refer to a reference that hasn’t been defined yet.Now we have a fully functioning color picker control.Validating the control valueValidating a control with a custom ControlValueAccessor works exactly the same as validating any other FormControl.You can have synchronous and asynchronous validators.An evident example is the required validator.Something more specific might be a validator to have at least 50% blue in the color:myForm = this.formBuilder.group({  bgColor: [null, [Validators.required, ValidateMinimumBlue]],  fgColor: [null, Validators.required]});const ValidateMinimumBlue = (control: FormControl) =&gt; {  if (!control.value) {    return null;  }  const { b } = parseColor(control.value);  return b &gt;= 128 ? null : { minimumBlue: true };}&lt;div class=\"error\" *ngIf=\"myForm.get('fgColor').hasError('minimumBlue')\"&gt;Not blue enough&lt;/div&gt;I like the color blue, ok.Another example is a contrast validator on the form group itself:myForm = this.formBuilder.group({  bgColor: [null, [Validators.required, ValidateMinimumBlue]],  fgColor: [null, Validators.required]}, {validators: ValidateContrast('fgColor', 'bgColor')});const ValidateContrast = (leftControlName: string, rightControlName: string) =&gt; (formGroup: FormGroup) =&gt; {  const leftControl = formGroup.get(leftControlName);  const rightControl = formGroup.get(rightControlName);  ...  return contrast &gt; WCAG_2_0_AA ? null : { contrast: { value: contrast, expected: WCAG_2_0_AA } };}Styling invalid and touched controlsMost of the time you want a visual indication that a control is valid or invalid.You can do this using css in your component like this::host.ng-touched .picker-icon {  position: relative;}:host.ng-touched .picker-icon:after {  position: absolute;  transform: translate(100%, -50%);  top: 50%;} :host.ng-touched.ng-invalid .picker-icon {  box-shadow: 0 0 1px 1px darkred;}:host.ng-touched.ng-invalid .picker-icon:after {  content: '❌';}   :host.ng-touched.ng-valid .picker-icon {  box-shadow: 0 0 1px 1px green;}:host.ng-touched.ng-valid .picker-icon:after {  content: '✅';}Reacting on more behaviorIf you want to your code to react on more control behaviour, like statusChanges, you cannot do this out of the box with NG_VALUE_ACCESSOR.However, you can inject NgControl from @angular/forms instead.Angular uses this instance to represent the control within the object graph that has been created for the form.To use this, you have to remove the provider for NG_VALUE_ACCESSOR and instead add this in your constructor:constructor(ngControl: NgControl) {  if (!!ngControl) {    ngControl.valueAccessor = this;  }}Then you can do things like this in ngAfterViewInit:this.ngControl.statusChanges.pipe(  takeUntil(this._destroyed$)).subscribe(status =&gt; {  console.log('The current status of the control is', status);});Or if you really want to hook onto specific methods, you can do this:if (this.ngControl.control) {  const markAsDirty = this.ngControl.control.markAsDirty;  this.ngControl.control.markAsDirty = (options) =&gt; {    markAsDirty.bind(this.ngControl.control)(options);    console.log('Mark as dirty has been called');  }}I would not recommend these kind of hooks, but if you do need them, be extra careful for infinite loops and other bugs!"
      },
    
      "design-2020-07-28-10-best-practices-in-ux-ui-design-html": {
        "title": "10 Best Practices in UX and UI Design",
        "url": "/design/2020/07/28/10-Best-Practices-In-UX-UI-Design.html",
        "image": "/img/2020-07-28-Best-Practices-In-UX-UI-Design/design-banner.jpg",
        "date": "28 Jul 2020",
        "category": "post, blog post, blog",
        "content": "  UX and UI design principles often lead to people thinking ‘well duh!’, until they actually have to try and create a user-friendly website or app. The basic gist of creating a user-friendly design is making sure the user doesn’t have to think twice about doing something. The longer it takes for a user to be able to execute an action, the more annoyed they will get. The usability of your website or app can already be improved with just a few tweaks. Below is an overview of ten ways to make your users happy.  As I’m just taking my first steps in the UX/UI design world, I still have lots to learn. Feedback is greatly appreciated!Table of contents  Company logo placement and usage  Make clickable elements obvious  Clearly show which elements on a page belong together  Be honest  Accessibility  Stick to conventions  Breadcrumbs  Less clicks = better  Dropdown lists  Include visual indicators of length and size1. Company logo placement and usageFirst things first. Your user always needs to find their way back to the homepage in a heartbeat. Providing a ‘Home’ link in your menu is not enough. It’s surprising how often users click on the company logo on the top of your page so make sure it’s visible from every page and provide a link behind it to the homepage.The company logo is typically placed in the top left corner (on sites designed for left-to-right reading).2. Make clickable elements obviousButtonsConfirmation buttons need to stand out from the rest of the page so they immediately draw the user’s attention.Adding a shadow effect to the buttons lets the user know it’s clickable and they can interact with it.Notice the difference between the following examples:Example 1:Example 2:In the bottom example the label to save the form is still concise and clear, users should thus still be able to eventually save the data, but it takes a moment longer to register which button to click to make this happen. Remember, the user wants to spend as little time as possible determining the next course of action.So-called secondary actions like ‘Cancel’ should look the least appealing of the options because it usually isn’t the next action the user wants to take.Making them the least appealing minimizes the chance of misclicks and nudges the user further towards a successful ending, such as a purchase.Let’s also discuss the location and order of the ‘Cancel’ and ‘OK’ (or other confirmation actions such as ‘Save’, ‘Yes’, ‘Submit’, etc) buttons.In Windows apps, the ‘OK’ button should come first and the ‘Cancel’ button second. This implementation follows the ‘natural reading order’ (again, on sites designed for left-to-right reading).Additionally, keyboard users will also reach the ‘OK’ button sooner.On Apple machines however, ‘Cancel’ will come first and ‘OK’ second.The idea here is that the flow feels more natural since the ‘OK’ button closes that section and the next step (‘OK’) is placed most right, whilst the previous step (‘Cancel’) is placed left.When designing for the web, we usually opt to put the buttons on the left-side of the screen, with the confirmation button first. ‘Example 1’ in this section illustrates this as well:HyperlinksClickable words on a web page are best underlined and shown in a different color than the rest of the text. Using solely a different color might be interpreted as an emphasis on that word and another problem that this presents is that colorblind people will have issues finding the clickable links in your text.Just underlining a word is already a good visual indicator that it’s clickable, but your best bet is to make sure that the link ‘pops’.By going for a combination of coloring the text and underlining it, you’re making sure there’s no doubt in the user’s mind that the word is clickable.To be clear, the same goes for the other way around as well. Underlining words that aren’t clickable might cause confusion. If you want to emphasise certain parts of the text, you can for example use an italic font for that.As a side note, sometimes the location of the links already provides the user with the knowledge that they’re clickable, for example your menu items or links in your footer.3. Clearly show which elements on a page belong togetherTitles and subtitlesTitles and subtitles are a must-have when your website needs to convey any sort of information to your user.Walls of text are very hard to get through and will not, I repeat, will not keep your user’s attention. Split your text in sections and be sure to use consistent font sizes for the titles. If they aren’t used consistently, there’s no way to know which sections belong together. Take a look at the following example:It’s clear from this example that ‘Subtitle 1’ and ‘Subtitle 2’ both are part of the ‘Title’ section.The following text is much more confusing:What is supposed to be communicated here? ‘Subtitle 2’ using the same coloring as ‘Subtitle 1’ but it’s just as large as ‘Title’. So does the author mean it’s still part of the ‘Title’ section or did he or she mean to create a whole separate section with a different ‘Title’? Instructions unclear.For screen readers (and other assistive technologies), it’s best to show titles by using the different title tags that are available in HTML, such as &lt;h1&gt;, &lt;h2&gt;, etc. This way, the screen reader is aware that that specific text is a (sub)title and can communicate this information to the user.4. Be honestOne of the biggest irks of customers is being presented with crucial information and extra costs late in the ordering process. Customers want to be able to make an informed decision about your product as soon as possible, preferably on the product page itself. Once they make the decision to order, the last thing they want to encounter is new information later on that might give the perception that they have been deceived.For their sake, and the company’s, it’s vital to show the shipping costs, any fees, delivery moment, etc as early as possible.Take a look at how Coolblue handles this:Bookdepository handles this similarly with a link to extra information on delivery since they’re providing worldwide shipping:In the above examples, shipping costs are free. If your shipping costs depend on the items in the shopping cart or other factors, the customer should to be able to view the costs when viewing the cart.Coliro shows this in the next example where you can select your country to determine the shipping costs:5. AccessibilityAccessibility is a whole different and separate topic to discuss. We’ll cover this in a future blog post, but for now, let’s focus on some low-hanging fruit.Websites should preferably be designed for impaired persons as well, whether it’s a temporary impairment (like a broken arm) or a permanent one (like blindness).It’s practically impossible to please everyone, but we can at least try to make everyone’s lives a little easier by making our website and apps as accessible as possible.  Colors: Provide a strong color contrast between the text and the background. This will make the text more readable for visually impaired persons.  Keyboard use on websites: Users should be able to navigate through the website using only the keyboard so that the use of a mouse isn’t necessary.  Text size: It should be easy for the user to increase and decrease the size of the text so visually impaired persons can select their optimal text size.  Alt text: Provide alt text to every image that screen readers should read out loud.  Captions on videos: Hearing impaired persons benefit greatly from subtitles and captions on videos.6. Stick to conventionsThe rule regarding icon usage and link labeling is pretty simple: stick to conventions! The icons for search, shopping cart, profile, etc.. are well-known and you shouldn’t try and reinvent the wheel by using ‘innovative’ icons.Also, unless you’re using icons that are well-known and absolutely clear on their purpose, like the icons above, it’s always a good idea to add labels to your icons.If you’re unsure about the use of labels, user testing will tell you whether users are experiencing issues with your icons without labels or not.How you label your links and buttons is important as well. When users are looking for job opportunities on your website, typical keywords are ‘Careers’ or ‘Jobs’.By using for example ‘Employment’, users might not be able to find your career page (as easily).7. BreadcrumbsLike Hansel and Gretel, we all need to find our way back sometimes. Breadcrumbs, and in our case meant specifically online, help us to do so, if done right.Take a look at the following example from ASOS:With breadcrumbs, users can see in the blink of an eye in which (sub)category the current page belongs. This is interesting for your users who ended up on a specific product page through a Google search. By using breadcrumbs, your users are able to quickly understand where they are situated within your website.Being able to actually click on a (sub)category in the breadcrumbs will help your users to easily retrieve an overview of similar products.Even when you’re just mindlessly browsing a website and you clicked on twenty different items within a category, you don’t want to click the back button twenty-one times to go back to the actual results within the current category.Breadcrumbs make sure that your user can browse your website more easily, which creates less frustration for your user, which hopefully creates a higher conversion.8. Less clicks = betterUsually.Try to save the user clicks as much as possible. Like Steve Krug describes in his Don’t Make Me Think, instead of only providing a tracking number for a shipment, you can add a link to the e-mail that opens the carrier’s website with the tracking number already filled in.Another example would be to provide a Google Maps preview on the contact page instead of just providing the address.When possible, also try to mention any form errors while the user is entering the data. Whether it be the phone number format or a password that is too short, anything that can be detected client-side should immediately be communicated to the user instead of waiting until the user clicks ‘Submit’.Now, I said ‘usually’ at the beginning of this section. There are of course a few exceptions. One of them is the FAQ page.If you want to provide quite some questions and answers, it’s best not to throw it all into one big overview.A good idea here would be to provide all the possible questions in a list and make these questions clickable.The answer should then only show up after the user has clicked on the question. This will provide a clean, clear, concise and easy to browse FAQ page for your user, even though there are a few more clicks involved.9. Dropdown listsDropdown lists are simple to implement and easy to use so they are present on countless websites.The key here is to only start using dropdown lists when we’re talking about five or more options. When there are less options present, radio buttons might be a better choice for you so the user can see all the available options immediately.10. Include visual indicators of length and sizeAlways assume that users are in a rush. What users will typically do when encountering a web page that contains a lot of text, is to check how long that text actually is before starting to read it. They need an idea of length or time to know if it’s worth to start reading at all. Medium.com does this by adding the number of minutes it takes to read their articles:Providing a progress indicator also helps the customer complete the order. It shows where the user is in the process and what information still needs to be entered before the order can be completed.Users quickly lose their interest if they don’t have an idea how much longer the process will take. Progress bars are essential and adding detailed information like Coolblue does in the below example will keep your user’s focus towards the end goal (in this case completing the purchase).The same also goes for questionnaires by the way. How often do you complete an online questionnaire if there isn’t any information available on how long it might take to fill in?Exactly.ConclusionAs said before in this blog post, it’s practically impossible to please everyone with your design.What’s aesthetically pleasing to one person might not be so for the next one.But user experience is more than the look, the layout, the colors used. We’re also talking about flow here, about creating an experience for each user, regardless of impairment, that helps them reach their goal flawlessly, without frustration, with little nudges in the right direction.If you’re looking for some UX/UI design experts to help with your projects, don’t hesitate to contact my colleagues at Clockwork!"
      },
    
      "architecture-2020-07-08-book-five-dysfunctions-html": {
        "title": "Pondering The Five Dysfunctions of a Team",
        "url": "/architecture/2020/07/08/Book-Five-Dysfunctions.html",
        "image": "/img/2020-07-08-Book-Five-Dysfunctions/bookreview.jpg",
        "date": "08 Jul 2020",
        "category": "post, blog post, blog",
        "content": "Being triggered by a colleague to read a book called “The Five Dysfunctions of a Team”, as well as remembering it being referenced in a book I read earlier this year, titled “The Phoenix Project”, I set myself to tackling the 230 or so pages that make it up. The main body of the book is comprised of a story about a new CEO for a tech company that is brought in to reforge their current team of C-level managers into a coherent group with their noses pointing in the same direction. Very similar to the story style of the Phoenix Project, it is clear to see the heavy influence on its writer, Gene Kim in terms of spinning an interesting tale. It is a compelling story for those in the consultancy business, and the patterns are easily detectable in a large number of companies. And while the last chapter of the book does offer some techniques for remedying this phenomenon, it does not do a deep dive into a methodology that approach it.There are additional books by the same author that go deeper into detail, such as “Overcoming the Five Dysfunctions of a Team”.Applying the organizational theory as postulated by William Richard Scott, professor emeritus in sociology at Stanford University, it is important to realize the book primarily addresses teams in a rational organization. He defines organizations as follows:Organizations are conceived as social structures, created by individuals to support the collaborative pursuit of specified goals.In essence, organizations consist of members that coordinate their behavior in order to accomplish shared goals or to put out a product or service. They contain roles, rules, and clear boundaries of where a group starts and ends. Three types of organizations are determined by him, each with their own particularities: Rational, Natural and Open. Examples of these types of organizations are respectively a traditional for-profit company, a government, and a highly diversified conglomeration. An overview of the most differentiating characteristics per type can be seen in the table below:As the typical enterprises of today are still in a rational structure, the lessons that can be derived from the book are very relevant. They become more complicated when looking at natural organizations. These organizations are characterized by ever-changing groupings of teams and collaborations within themselves and makes improvement techniques such as defining your primary team a lot trickier, forcing the reapplication of these techniques at an elevated frequency. Another nomenclature for this type of structure is the quantum organization as defined in the book “Quantum Organizations” by Ralph Kilmann and referenced by Keith Swenson in the book “Social BPM” or the Holacracy management system coined by Brian J. Robertson in his book with the same name.Coming back to the model described in the book to chart and tackle the dysfunctions within team, it is visualized as a pyramid where each value needed in teams is correlated to a dysfunction the team needs to recognize and resolve. This pyramid can be found in the website of The Table Group of which Patrick Lencioni, the author, is the founder.Screenshot taken from Table Group websiteAbsence of TrustTrust is the foundation for any team performing as a cohesive group. The team needs to trust one another enough to risk owning up and discussing their mistakes and weaknesses. This is needed to create the necessary trust to be willing to have conflict and disagreements with each other in order to come to valuable insights and results. This is often exacerbated by self-doubt and hinders the team members from seeking each other’s help. Realizing there are things you don’t know or aren’t succeeding in should trigger team members to own up to this fact and try to resolve this as a team effort. The sooner help is sought out after this realization, the more efficient the team will work. But this works in the other direction as well: Have enough trust in team members to figure things out for themselves and get the job done. The idea is not to force your expertise onto other members of your team, but to offer it freely when asked for.A worthwhile exercise is to determine the broad characteristics of each team member, what their strengths and weaknesses are, and how to go about communicating with people that act on different paradigms. There are multiple ways of drawing up a broad profile of the members in your team. The Myers-Briggs Type Indicator (MBTI) is a stock standard way of charting this, categorizing people on the four principal psychological functions (sensation, intuition, feeling, and thinking)  which gives us 16 distinct personality types, each with their typical strengths and weaknesses. Other common profiling tools are the Business Insights analysis, Everything DiSC or even the Interpersonal Circumplex (also known as the Rose of Leary).Myers-Briggs Type Indicator Personality Types (taken from Adioma blogpost)When there are clear key figures within the team, they can further this trust through demonstrating vulnerability first. When these key figures of the team are open about their own weaknesses but also strengths, this behavior will become more natural and acceptable to other members of the team that might not yet have had experience with this openness. When considering George Kohlrieser’s Secure Base Leadership paradigm, he would phrase it as such: A Secure Base Leader builds trust, delivers change and inspires the focus that people need to actively engage in their work and create conditions for innovation. Secure Base Leaders generate high performance by acting as a “secure base” for the people they lead.Fear of ConflictWhen there is no trust, there is no fertile ground for discussion within the group that may result in open conflict as understandings and opinions might differ. These discussions are necessary to come to a mutual conclusion in how to approach and resolve the vision set forth by the group. This discussion gives a platform for every member of the group to be heard and increases buy-in drastically. The idea is similar to the thesis-antithesis-synthesis concept of Johann Gottlieb Fichte, the German philosopher, who postulated such a progression of ideas to come to the best solution for any problem or proposition.The meeting culture in most companies is a very good example of this. They are mostly led by the organizer trying to convey information of his point of view in an almost ex cathedra capacity, with very little interactions between those attending the meeting. There will some vocal members, but the majority will sit there accepting what is being told at face value. While some meetings are all about disseminating information, most of them are organized with the idea of getting to a consensus about certain topics.A good meeting culture greatly enhances the productivity of such meetings. Always keep in mind that all meetings should have each of these characteristics (to be added to the meeting request):  Context: what is the meeting about? And what will the mindset (informative/collaborative/…) be?  Goals: what will be the results of this meeting?  Added Value: an indication of why each of the invited people to this meeting are there.  Agenda: A clear agenda of how the meeting will progress.Lack of CommitmentWhen members of the team do not participate in such discussions, they often feel erroneously that their ideas are spurned by the rest of the group, growing resentment, as well as failing to get their buy-in for the chosen approach. This resentment might even fester into subconscious attempts to disrupt the efforts of the team in the spirit of a “I knew it wouldn’t work”-reasoning. The two most contributing factors to commitment or lack thereof are consensus and certainty on future actions.Consensus amongst team members on how to do things might look desirable. If everyone agrees, the lack of commitment should not exist. However, such a consensus is not always achieved and striving for it might lead the team into an impasse. Especially when having to take decisions that are limited by deadlines. In these cases, once all arguments have been presented and no more avenues of discussion are open, a leader should make a decision and the rest of the team should abide by it. This uniting behind the banner of the chosen route is where certainty comes in. Even when members do not agree, they should realize that similar to playing chess: A bad plan is better than no plan at all. So even if they do not fully agree with the decision, the alternative of no decision at all is worse.One way to get consensus is to quantify people’s opinions on the possible decisions. For example, when choosing a specific solution to a problem like which technology to use for an implementation. This is done by first establishing a list of properties a solution should adhere to. Next, we weigh these properties on a scale of 1 to 5. Then each team member gives points on that same scale for each solution. The average of these attributed values is multiplied by its weighing factor and all the weighed values are added for a total score of the solution. Whichever solution holds the highest score is the solution the team will go with. There are some caveats as we need to make sure that each of the possible solution does indeed cover all requirements and there are no showstoppers associated with this solution. But this will give an overview like the one below.                   Weight      Framework #1      Framework #2      Framework #3              Property #1      3      4+3+3      2+2+2      3+4+5              Property #2      2      2+3+4      3+2+3      3+4+5              Property #3      4      2+1+2      4+2+3      2+5+4              Property #4      5      3+3+3      3+5+4      3+3+4              Property #5      1      4+5+2      5+4+4      2+2+4                                                         Total             124      143      162      Avoidance of AccountabilityAnother subconscious result to not being heard and having the proper buy-in for the chosen approach, is not feeling accountable for it. This is twofold: On the one hand, one might not be inclined to call into question the tasks he/she is performing in the approach, and on the other hand the adoption of a laissez-faire attitude towards efforts of team members that could be counter-productive. Or even a lackluster remark on these efforts that could be construed as a passive-aggressive comment from someone who isn’t pulling his or her weight in the approach. The interpersonal discomfort that calling out other team members might cause is often a deterrent from this type of peer performance questioning.Some mitigating actions can be taken to reduce this level of discomfort by doing regular progress updates on the team efforts. In this way issues can be brought to the group and in this way mentioned. This does pose the danger of the team member whose progress is called on feels like the rest of the team ganging up on him. Other ways of dealing with this is to stipulate goals and standards from the very start of the team effort. If everyone knows what standards to follow and what results to attain there is afterwards less of a fuss on whether or not the team goals have been reached.Another popular set of techniques stemming from 1994 called the Oz Principle tells us that self-management of your accountability can be done through these straightforward steps: See It, Own It, Solve It, and Do It. If these steps are applied, you will remain above the line and achieve desired business results. Consequently, the opposing actions will push you below the line with a negative impact on these business results. To state it more elaborately: Everyone needs to recognize and acknowledge a situation going wrong (See It). Then an honest self-appraisal of one’s own part in the situation as well as that of other members of your team (Own It). When this appraisal of the problem has been executed, you and your team should think of all possible solutions and weight them for success rate and applicability (Solve It). The final step is to muster the courage and commitment of all involved to actually implement the solution(s) you have decided upon.Inattention to ResultsThe final dysfunction is that team members are not committed enough to the decided upon approach to give it their full backing. Often, they might prioritize their personal goals (such as career opportunities of reputation) over the success of the approach and the collective goals of the team. As with all dysfunctions, this one is caused and augmented by the others. The lack of buy-in and not being convinced of the chosen direction for the team makes one question its eventual results and how this reflects on him. One way to combat this has been the time-honored result-based rewards, linked the success of the team to the evaluation of the individual. However, scientific research has indicated that for knowledge worker or creative endeavors rewarding good results has a negative effect on performance. Those interested in this phenomenon should watch Dan Pink’s TED talk on the subject back in 2009. He states several scientific studies demonstrating this principle.Coming back to the MBTI personalities, this result-driven attitude does not come naturally to all types of people. This personality trait is associated in the highest degree with the Commander/Field Marshall personality and in various degrees to the others. In DiSC they are one of the four possible categories: Dominance (Green). In Business Insights, these are the Fiery Red personalities or the Directors, Motivators and Reformers.ConclusionTake from this book what you will. It certainly isn’t a fix-all model, but then what model is? If it works for your team use it. If it kind of works for your team, tweak it. If it doesn’t fit your team dynamic at all, go for something else. There is potential here to be wielding for greater team results if used with common sense."
      },
    
      "event-driven-2020-06-30-user-feedback-websockets-html": {
        "title": "Enabling User Feedback with WebSockets on RabbitMQ and Spring Cloud",
        "url": "/event-driven/2020/06/30/user-feedback-websockets.html",
        "image": "/img/sockets/feedback.jpeg",
        "date": "30 Jun 2020",
        "category": "post, blog post, blog",
        "content": "Reading time: 7 minutes and 31 secondsWhen working with event-driven applications, you tend to see this on the screen:  We are processing your request and will notify you when it was treated successfully.Let’s chop the sentence down into processing the request…This means that the server is processing your request, but the client is not sure if it was entirely successful because it got transformed into an event and published on a queue never to be seen again (fire and forget).The client wonders what has happened and needs a way to give his users the state of the request.The next statement tells the user it will notify him/her when the request was successful.This part can get complex because you want to give a rapid response as soon as possible.  The first thing that comes to mind is, can our client not poll the state of the data from the table until it’s ready?When our client receives a high amount of load, and it’s polling the database for the state of the data, it can put the database under unnecessary stress.Since polling is a periodically check, it is not real-time, and we want to bring feedback to our users as soon as possible.To let your client behave in real-time, we need push events.Push events can be enabled by the concept of WebSockets, this bilateral communication connects the server and the client in an open connection with each other.This tech post will explain how we enabled push events with RabbitMQ, MQTT, and Spring Cloud Stream.Table Of Contents  WebSockets for communication  Spinning up a RabbitMQ  Subscribing with a JavaScript client  Publishing events with Spring Cloud Stream  ResultWebSockets for communicationWe chose WebSockets because it provides a bilateral open connection between the client and the server.Because handling data becomes complex over TCP and requires hard work to do it yourself, WebSockets offer support for subprotocols.These solutions offer us easy ways to transmit data over the wire. First, let’s talk about opening a WebSocket connection.To establish one, we need the client to send a WebSocket handshake request, for which the server returns a WebSocket handshake response.The handshake starts with an HTTP request/response.Once the connection is established, communication switches to a bidirectional binary protocol which does not conform to the HTTP protocol. The switch happens with the HTTP Upgrade Negotiation, this header allows us to tell the server to switch to the protocol the client desires and open up two-way communication between a client and server.At a minimum, a successful WebSocket handshake must contain the protocol version, and an auto-generated challenge value sent by the client, followed by a 101 HTTP response code (Switching Protocols) from the server with a hashed challenge-response to confirm the selected protocol version:  Client must send Sec-WebSocket-Version and Sec-WebSocket-Key.  Server must confirm the protocol by returning Sec-WebSocket-Accept.  Client may send a list of application subprotocols via Sec-WebSocket-Protocol.  Server must select one of the advertised subprotocols and return it via Sec-WebSocket-Protocol. If the server does not support any, then the connection is aborted.  Client may send a list of protocol extensions in Sec-WebSocket-Extensions.  Server may confirm one or more selected extensions via Sec-WebSocket-Extensions. If no extensions are provided, then the connection proceeds without them.Choosing a subprotocolWhen I was searching for a suitable subprotocol for handling the data, I first experimented with STOMP.STOMP has a rich messaging mechanism for handling data and great support for Spring and RabbitMQ.I stumbled against an issue with our API gateway.To do a security scan, the API gateway had to parse it to XML, which didn’t go well with the UTF-8 text-based messages of STOMP.Some further research brought us to our next candidate: MQTT.MQTT, designed as an extremely lightweight pub/sub messaging transport for IoT and mobile devices, could offer us a way to enable WebSockets.When experimenting, I stumbled on support with RabbitMQ MQTT plugin and RabbitMQ Web MQTT plugin.In MQTT over WebSockets, the MQTT messages are transferred over the network and encapsulated by one or more WebSocket frames.To communicate with an MQTT broker over WebSockets, the broker must be able to handle native WebSockets.To provide such support, we decided to use our own managed RabbitMQ.The plugin enables the possibility to use MQTT over a WebSocket connection.To enable this easily in your broker, you just enable an internal plugin from RabbitMQ itself.rabbitmq-plugins enable rabbitmq_web_mqttSpinning up a RabbitMQTo try it out you can just run RabbitMQ in a Docker container.Define the commands in a Dockerfile and off you go!FROM rabbitmq:3.7-managementRUN rabbitmq-plugins enable --offline rabbitmq_web_mqttEXPOSE 4369 5671 5672 25672 15671 15672 15675 1883ConfigurationWhen accessing RabbitMQ via MQTT, credentials have to be given to authenticate yourself.Because we will be accessing it from a JS client, we do not want to expose our credentials to our client because it can be exploited.To avoid giving credentials, MQTT supports us to connect anonymously.Add these to your rabbitmq.config file, and you’re good to go:mqtt.default_user = $RABBITMQ_DEFAULT_USER  mqtt.default_pass = $RABBITMQ_DEFAULT_PASS  mqtt.allow_anonymous  = true  Subscribing with a JavaScript clientEclipse offers us a JavaScript client library to use for opening a WebSocket over MQTT.With some basic setup, we can fix ourselves a quick WebSocket to the Rabbit to test the handshake.As the JavaScript client, we will be subscribing to a queue and listen for any notifications from the backend. To configure our client, we need to know what properties we need.A list of properties can be found in the documentation.The most important ones are enabling SSL and using the keep-alive period as described above.var wsbroker = '{rabbitmq_hostname/ws}'var wsport = 443; // port for above// you can use randomizer to be unique \"myclientid_\" + parseInt(Math.random() * 100, 10));var client = new Paho.MQTT.Client(wsbroker,wsport, \"?access_token={token}\",\"{client}\");     client.onConnectionLost = function (responseObject) {    console.log(\"CONNECTION LOST - \" + responseObject.errorMessage);};client.onMessageArrived = function (message) {    console.log(\"RECEIVE ON \" + message.destinationName + \" PAYLOAD \" + message.payloadString);};    client.connect({    useSSL: true,    onSuccess: function () {        console.log(\"CONNECTION SUCCESS\");        client.subscribe('events', {qos: 0});    },    onFailure: function (message) {        debug(\"CONNECTION FAILURE - \" + message.errorMessage);    }});Keeping the heartbeat aliveAt any point after the handshake, either the client or the server can choose to send a ping to the other party.When the ping is received, the recipient must send back a pong as soon as possible.You can use this to make sure the client is still connected.A best practice is to set the heartbeat between 20-30 seconds, see https://tools.ietf.org/html/rfc6202#page-13.The client sends a ping every 10 seconds, and the server waits 10 seconds to send back a pong.MQTT keep-alive periodThe keep-alive period is the answer from the MQTT protocol to the WebSocket heartbeat.The keep-alive is a time interval measured in seconds.It is the maximum time interval that is permitted to elapse between the point at which the client finishes transmitting one control package and the point it starts sending the next.It is the responsibility of the client to ensure the interval between the control packets being sent does not exceed the keep-alive value.The client can send a ping at any time, irrespective of the keep-alive value, and use the pong to determine that the network and the server are working.To configure the keep-alive period, the client can add the property to enable the feature.client.connect({        keepAliveInterval: 20})TLS over WebSocketsTo achieve a secure connection, we need to enable TLS.Like HTTP, WebSockets supports TLS with using the prefix wss:// instead of ws:// and port 443 instead of 80.The client can enable TLS by adding a property.client.connect({        useSSL: true})Authorization tokenThe best practice for securing your resources is to propagate your token via the query parameter.If you are targeting a backend, the backend can handle this token but for this use-case, we need a reverse proxy/API gateway to validate this token for us.var client = new Paho.MQTT.Client(wsbroker,wsport, \"?access_token={token}\",\"{client}\");Clean SessionClean session in the MQTT protocol means that if turned on, the server does not know on what topic the client has subscribed to.When turned off, the client just needs to reconnect to its session that is stored on the server.  Default is trueQuality Of ServiceIn combination with the clean session property set to false, the QoS makes your messages durable.When the client is offline, the server holds these messages until the client reconnects.  Default is 0MQTT ClientWhen opening the WebSocket on RabbitMQ, the broker will create a new queue on the default topic amq.topic with a routingKey as the subscriber endpoint.Publishing Events with Spring Cloud StreamSo now we have our RabbitMQ up with the enabled plugin for MQTT over WebSockets, Spring Cloud Stream offers an abstraction for messaging with RabbitMQ as a binder.Because RabbitMQ is our MQTT broker, we do not need any special configuration to handle MQTT messages. You can just set up a Spring Boot application with the https://start.spring.io.We start by adding both the dependency for Spring Cloud Stream and the binder of choice.This indicates that auto-configuration and abstraction are done for RabbitMQ.Dependencies&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt;Message ChannelFollowing up, we need a channel to publish our messages on, so we create an interface to define our channels.You can have two kinds of channels, one for everyone (broadcast) or one-to-one (private).public interface UserFeedbackChannel {    String NOTIFICATION_EVERYONE = \"globalNotificationChannel\";    String NOTIFICATION_USER = \"specificNotificationChannel\";    @Output(NOTIFICATION_EVERYONE)    MessageChannel globalNotificationChannel();    @Output(NOTIFICATION_USER)    MessageChannel specificNotificationChannel();}To let Spring know it is a custom channel, we need to annotate our configuration class with @EnableBinding({UserFeedbackChannel.class}).ConfigurationWith RabbitMQ, some custom configuration needs to be taken care of.Since MQTT takes the topic amq.topic as default, we need to target this as our destination for our messages.The routingKeyExpression enables us to broadcast or privately send the message.The headers.routingKey is bound to the user we want to message to.Our pojo event consists of audit fields that we know of whom the message belongs to.This way, we can give feedback to the user who did the transaction.If the header is filled with events, it broadcasts the message.spring:  application:    name: notifications  cloud:    stream:      bindings:        globalNotificationChannel:          destination: amq.topic        specificNotificationChannel:          destination: amq.topic      rabbit:        bindings:          globalNotificationChannel:            producer:              routingKeyExpression: '''events'''              declareExchange: false          specificNotificationChannel:            producer:              routingKeyExpression: headers.routingKey              declareExchange: falsePublisherCreate the pojo you need, so we can start publishing!Be aware, before pushing the pojo, it needs to be converted to a String for MQTT to understand the format.@Componentpublic class NotificationSocketPublisher {    private final UserFeedbackChannel channel;    private final ObjectMapper objectMapper;    public NotificationSocketPublisher(UserFeedbackChannel channel, ObjectMapper objectMapper) {        this.channel = channel;        this.objectMapper = objectMapper;    }    public void sendPrivateNotificationToUser(NotificationSocketEvent event) {        String object = convertToString(event);        var notification = MessageBuilder.withPayload(object).setHeader(\"routingKey\", \"events.\" + event.getCreatedBy().toUpperCase());;        channel.specificNotificationChannel().send(notification.build());    }    }ResultWhen the JS client, RabbitMQ, and Spring Cloud backend are running, you can try ìt out by triggering messages from the backend onto the RabbitMQ.This will result in communication to the correct subscriber.The JS subscriber will interpret these messages and parse readable content from it."
      },
    
      "iot-2020-06-15-3d-printing-during-times-of-corona-html": {
        "title": "3D Printing During Times of Corona",
        "url": "/iot/2020/06/15/3D-Printing-During-Times-Of-Corona.html",
        "image": "/img/2020-06-15-3D-Printing-during-corona/banner.jpg",
        "date": "15 Jun 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Shortage in equipment  3D printing to the rescue  Experimenting with TPU  ResourcesIntroductionThese are trying times we are living in.Since the outbreak of the Corona virus we have been living in some form of lockdown.We are limited in our ability to go out and do anything, and when we do, it is recommended to take the necessary precautions like keeping a safe distance and wearing a mask.The masks are among the things we didn’t have enough of, certainly in the beginning of the outbreak.In this blog post I’ll dig a little deeper in using 3D printing tech to make masks and other equipment/material that can be of use.Shortage in equipmentThe strain on our healthcare system and the supplies has been massive.Masks were kept for healthcare workers even if they were just surgical masks and not the FFP2/3 masks.Masks are slowly becoming more widely available for the general population again, however the government relied on people to make their own masks in the beginning.Luckily our country was spared from the overwhelming load on the hospitals like in Italy.While the situation was very serious in the hospitals, we never had a lack of ventilators or had to turn people away because the hospitals were at or already beyond their capacity.3D printing to the rescue3D printing is a very handy technology.It allows for rapid prototyping and small-scale production without the need to set up a full production line.The 3D printing community is a very large one, and also a very active one.Because of the high load on the intensive care units in Italy with many people requiring to be put on a ventilator for life support, it became clear that the supply line could not meet the demand.Because the hospitals were in need of these valves and the supplier could not provide them, the 3D printing community jumped in, and soon after the first version was designed and 3D printed.However the valve design is patented and this hindered individuals and companies from stepping in and helping with printing extra valves.The hospitals can request production of these masks, sidestepping the patent in times of emergency, but they have to be present and patient consent is also needed (depending on local laws).The pictures below show the masks.The first picture shows the real valve (left) versus a 3D printed one using filament printing technologies.The second picture shows later iterations of the 3D printed valves printed using polymer material that is laser fused.                                                 These valves are not officially approved for use in medical devices, but since “necessity knows no law” it might be better to use these 3D printed valves and saving lives instead of letting people die.The STL files have not been shared publicly, except for some imitations, linked for educational purposes only!The manufacturers have not taken any legal action against this 3D printed design.Using the original valves is still recommended and once the supply from the manufacturers can meet the demand these will once again be used as some of the intricate details of the valve are hard to mimic with current 3D printing technologies.But not only valves are being 3D printed.Since there is also a great demand for face masks, and they were, certainly in the beginning, in very short supply.So mask designs started popping up in the online communities, at first very basic and rudimentary designs but over the following days/weeks the designs became more specialized and optimized.These masks are not meant to replace the actual FFP2/3 or N95 masks but can offer people at least protection, if not for themselves, at least for others by preventing particles from the nose or mouth to spread as far as without wearing a mask.3D printing materials are also (usually) not medically certified so precautions have to be taken.Especially when printing masks with FDM technology.It might be possible for contaminants to get in between the layers and in microscopic cavities in the printed material.Therefor it is highly recommended to either disinfect the masks after use, and even more preferred to do this in addition to also sealing the mask with a sealant.This sealant can be varnish or any other material which does not cause irritation when touching human skin.Below are some of the designs I experimented with and my personal findings of each mask in terms of fabrication easy and wearing comfort.This was one of the first masks I printed, designed by The 3D Handyman on YouTube and Thingiverse.                                The mask is modelled to the face of the creator, as can be seen in the instructional video:    It fits rather well, but when printed in PLA (or ABS) the edge can be rather uncomfortable.I tried and succeeded in printing this mask in TPU which greatly improves the comfort when wearing it.The TPU printed version does rely quite a bit more on the elastic strap to keep the lid with the filter pinned to the mask as the clips do not work that well when printed in this material.I also designed an inlay for the cap/mask to hold the filter in place so that the filter cannot be blown out or sucked inside the mask.                                                                 This was the second mask design I printed, also designed by The 3D Handyman on YouTube and Thingiverse.                                                 This mask is much more complex, not modelled to one specific face and requires additional steps after printing.It comes with a mold on which silicone caulk should be applied to create a comfortable seal with the wearers face.The instructional video explains it all in great detail:    This mask fits very well and although it requires quite a bit of extra work, I prefer it over the first version!It sits very comfortable and does not require printing with TPU.The silicone molding process does take some trial an error but the mask shown in the pictures is from the first attempt!                                                                                                   There are many, many more mask designs online.You can always design a custom mask or look on thingiverse or any other 3D modelling site for designs from other people.The community is very large and very engaged when it comes to prototyping.A lot of trial and error is involved before getting a mask design right, and even when printing before the print is just right so it can be worn.Printing with different materials like TPU can also be tricky and time consuming to get right, in the section below I talk a bit more about what TPU is and why it’s harder to print with.Disclaimer: These masks are not the real deal, they are not meant to be used in hospitals, they are meant to provide a means to protect oneself when not better materials/equipment is at hand.They also only offer an additional protection at best, and it is recommended to maintain all social distancing measures, even when wearing a mask!All the masks shown here have been sealed with varnish or paint to allow for safer use and easier disinfection.In addition to masks there are also tools/adapters being printed to more easily hold masks or holder for plastic face shield, to use in combination with a mask.The straps are a very small piece that can hold the bands of a mask, this allows the mask tightness to be adjusted and provides greater comfort for the wearer.    These can be printed quite fast on a very rough quality setting.The second much printed item are the holders for face shields.These face shields provide an additional layer of protection to the wearer, usually this is combined with wearing a regular face mask.The holders fit around the wearer’s head and the plastic face shield clicks onto the holder by the pins.                                    Experimenting with TPUOne of the disadvantages of regular materials like PLA, ABS or PETG is that they are fairly rigid, they do not flex very much or at all.When printing masks or devices that need to be worn on the head, some form of flexibility is advisable to make the printed object conform to the shape of the head/face.Enter TPU or Thermoplastic Polyurethane.This material is flexible and can also be 3D printed.This makes the material suitable for use in designs that need to be able to conform to the contours of a human face.                The flexibility of the material is denoted on a hardness scale: The Shore hardness scale.This scale has three scales:  Scale 00  Scale A: Soft rubber types  Scale D: Hard rubber typesAs you can see, these scales do somewhat overlap.For 3D printing the harder the TPU material is, the easier it is to print.The lower the A or D scale value of the TPU material, the lower the printing speed and the shorter the retraction distance/speed has to be!TPU is harder to print than regular PLA or ABS.It is recommended to print TPU on a direct extruder at a lower than normal printing speed.However, it can also be printed on bowden fed extruders, but the printing speed needs to be lowered even further to prevent the filament from spooling up or clogging in the bowden tube.Retraction distance also needs to be lowered to prevent the extruder from clogging up.This will increase stringing to some degree though.3D printing is already trial and error, but for printing flexibles like TPU this is even more so.The material is a lot more unforgiving than PLA or even ABS and the settings need to be fine-tuned for each different printer and TPU material before prints will come out looking somewhat decent.The image below shows the difference between a bowden extruder (left), which has a guiding tube made of teflon and a direct drive extruder (right).Below is a video of my 3D printer with a bowden type extruder printing TPU just fine, albeit very slowly.                Resources  Italian hospital saves Covid-19 patients lives by 3D printing valves for reanimation devices  These Good Samaritans with a 3D printer are saving lives by making new respirator valves for free  Venturi Valve STL files  Custom Fitted Respirator Filter Mask - COVID-19: Files  Custom Fitted Respirator Filter Mask - COVID-19: YouTube  Universal Respirator / Face Mask w/ Mold for a Silicone Seal: Files  Universal Respirator / Face Mask w/ Mold for a Silicone Seal: YouTube  Surgical mask strap  Face shield holder  Shore hardness  Direct drive vs Bowden"
      },
    
      "cloud-2020-06-02-terraform-html": {
        "title": "From 6 weeks to 90 minutes: let Terraform do your work",
        "url": "/cloud/2020/06/02/terraform.html",
        "image": "/img/2020-05-30-terraform/terraform.png",
        "date": "02 Jun 2020",
        "category": "post, blog post, blog",
        "content": "Reading time: 7 minutes and 2 secondsTable of contents  How much infrastructure can you get in 6 weeks time  Why use Terraform to supercharge infrastructure provisioning  How to structure Terraform to allow for a two second tire change  How can you beat us  Where to draw a line, everything in terraform  Finally, two golden tips when using TerraformHow much infrastructure can you get in 6 weeks timeThe landscape discussed in this post is used to host a set of applications for a large corporation to assist one of their core products.The end-goal of this platform is to support their target market across Europe.A single environment consists of a namespace on a shared OpenShift cluster, a database and a reverse proxy.In order to spin up a new environment to onboard a new development team the following steps need to be executed:  Order resources on the shared OpenShift cluster: ticket 1 for the OpenShift team  Order a reverse proxy: ticket 2 for reverse proxy team  Order a database: ticket 3 for the database team  Order a DNS record for the environment to point to OpenShift routers: ticket 4 for the DNS team  Create a new environment in environment repository  Run a Jenkins job to create certificates for the environment  Update the credentials for the new database in online tooling  Update the credentials for the new database in offline password storage  Git-encrypt the database credentials and put them in the environment repository  Update the database connection details in environment repository  Run an Ansible playbook to create a new namespace in OpenShift and set up the base configuration for the namespace (Docker registry credentials, custom service accounts, …)  Sync OpenShift service account credentials into Jenkins credential store  Roll out Jenkins to add the new credentials by starting a Jenkins job executing an Ansible playbook  Create reverse proxy configuration in reverse proxy repository  Use a self-service portal to request access for Jenkins to update the configuration on the reverse proxy  Roll out the reverse proxy configuration using a Jenkins job  Roll out the environment configuration and application landscape to the new namespace through a Jenkins job that runs OC process and OC applies using the configurationThis sums up the best-case scenario.Since it requires multiple teams to perform disconnected tasks at different times, errors are frequent and slow to resolve.Next to that, there is a lot of manual work.This work includes copy and pasting configuration from different sources into different repositories and systems.Since any manual action, especially involving copying data between locations, is prone to errors, this introduces even more failure points.Combining the time it takes for tickets to be executed, copying the manual configuration to the correct locations and the debugging involved in getting the environment online results in lead times expressed in weeks.On one occasion the lead time to set up a single environment for a new team escalated to 6 weeks.Due to the high lead times at least 1 or 2 spare environments are provisioned in order to provide teams with at least a minimal environment to start working with.This of course introduces additional cost.High lead times, human errors and unnecessary operational costs were the main issues of this manual process.Since the decision was made to migrate to the public cloud, timing was perfect.Moving to a different infrastructure provided the perfect opportunity to optimize and automate this process.The expectation was to reduce the lead times to hours, minimize human errors, and minimize capital and operational costs when providing a development team with a new environment.Why use Terraform to supercharge infrastructure provisioningAs mentioned, one of the main triggers to revisit our infrastructure setup and look into automation was the move to public cloud.We selected Azure as the cloud provider.The reasons why Azure was selected are beyond the scope of this blog post.We might discuss this in a future blog post though.The logical choice for automating the infrastructure provisioning would be ARM templates, since this is the native provided way for infrastructure as code in Azure.Together with an external ARM expert, we attempted to set up the target infrastructure using this template technology.Unfortunately, multiple walls were hit due to the nature of the corporate setup on Azure.One of the limitations that were hit, was the inability for ARM to make changes to resources that were provided by a central team.More specifically, using a VNET that wasn’t managed by ARM to deploy an Azure Kubernetes Service proved to be difficult or even impossible.Using the CLI and scripts to create the setup was attempted as well, but this didn’t fit the vision of a declarative setup for provisioning infrastructure.Re-applying the same scripts either broke the setup or created additional, unneeded resources.A clear no-go.As a third option, Terraform was investigated.It adhered to the vision of declarative definitions and all the components required by the landscape were available in the Azure Terraform provider.Some basic setups were created and it showed great potential for the required setup.Terraform roll outs proved to be more stable and the import mechanism properly supports using components which aren’t fully managed by Terraform.So Terraform to the rescue!How to structure Terraform to allow for a two second tire changeOne of the challenges of creating a Terraform setup is to determine a way to structure the code.Looking online provided more questions than answers.A lot of documentation can be found about how to structure a specific module or how to create modules, but almost no resources discuss how to structure the modules into logical, reusable components.The setup was divided into two different categories: managed services wrappers and standardized setup modules.The first category is quite straight forward.For every managed service that is being used, a module is created.This module includes all the required Terraform resources for that service to operate.For example, the key vault module contains the Azure Key Vault Terraform resource, but also the role assignments for the different Active Directory groups that require access to that key vault.Another example is the container registry module.It contains the Azure Container Registry (ACR) Terraform resource, some role assignments and the Azure Monitoring Diagnostics Settings for making sure the ACR logs are shipped into the correct logs analytics bucket.This abstraction allows for opinionated grouping of resources that are required for a managed service to operate.The second category is a grouping of modules of the first category in order to provide a complete package of features.This set of modules are the ones that are actually provisioned during a deployment.Currently, only two of these modules exist: a cluster module and a namespace module.The cluster module contains all modules required to set up the shared resources for all environments with a similar purpose.For example, all resources shared by all development environments.This module contains the Kubernetes cluster module, multiple resource group modules, a key vault module, networking modules and modules that configure identity and role management.This module is used once per cluster, meaning once for development, acceptance and production respectively.The second module is used to set up a namespace for a specific purpose: an environment for a development team, a specific testing environment or a rock-solid production environment.This module contains the DNS zone configuration, an API gateway module, another key vault module and a database module.These cluster and namespace modules are then used in a single Terraform module per cluster: meaning a single module for the development cluster together with all namespaces in that cluster.This makes making changes to the infrastructure as easy as running a single Jenkins job executing that module.We decided to version the modules and created specific modules for development, acceptance and production.This separation, in combination with the versioning, has allowed us to test module updates and upgrades of the configuration upfront.Similar to deploying application code to an acceptance or test environment before rolling it into production.Where to draw a line, everything in terraformWhen we first started using Terraform, we were tempted to configure the entire landscape using it.A good example was the API manager setup we were using.The Terraform setup included detailed configuration of the application it was hosting.Another example was a Kubernetes Terraform module we created, that beside infrastructure related setup for storage classes, was also creating service accounts for operators we were running.This leakage of concerns made it hard to maintain, but it also introduced duplication of configuration towards the application deployment.We decided to take a stricter approach, and consider two main responsibilities: orchestration and provisioning.Orchestration could be translated to an engineer that would take a screwdriver and set up all the required infrastructure.The outcome of this work would be the minimum needed setup to start using the services and configure them for actual use.Provisioning is the work that comes after this, and doesn’t require a screwdriver, or being near the box.We decided that the orchestration part, setting up the infrastructure, is where we use Terraform exclusively.The provisioning part, where we start to configure the infrastructure to work with our application, is where we use tools that are native to the application deployment.We didn’t get there immediately though.In early iterations, we had split it into two Terraform runs: an orchestration run and a configuration run.In later iterations we were able to replace the configuration runs with components that were closer to the application landscape, which was a combination of privileged Kustomize deployment runs and building our own Kubernetes operators.A good rule of thumb is; if you can tie the component you are configuring to something that has meaning in the application domain, it should be managed in the application landscape.Another good indication that the application domain leaked into the infrastructure domain is when you have to re-orchestrate your infrastructure together with application changes.How can you beat usWith this setup, lead-time for provisioning an environment went down from 6 weeks of ticketing magic to just a 90 minute Jenkins run.The decision to adopt Terraform helped us to minimize human errors, and has enabled us to deploy new environments by a single push of a button.Infrastructure upgrades have become more and more stable over the adoption period and the different modules have matured to the point where they barely change anymore.The work being done by the Azure Terraform provider community has helped tremendously.They release new versions of the provider every week.They aren’t at feature parity with ARM, not by a long shot, but at the pace they are adding support for features, they will catch up very fast.The provider is also very stable and if an issue occurs (like this AKS bug) it’s fixed within the next release.Finally, two golden tips when using TerraformThe first is to start using remote state storage as soon as possible when using Terraform.It provides an easy way to get an accurate Terraform plan which in turn provides an accurate overview of the actions Terraform will execute during the roll-out.The second one is to roll out the modules often and validate their effects by running tests against them.Test either outcome, not just the configuration.The further up the application stack these tests run, the better.In our current setup, the infrastructure runs are part of a nightly test which performs the following steps:  Completely trash the test infrastructure  Deploy the infrastructure based on the latest configuration  Deploy the application landscape on top of it  Run the application landscape end-to-end tests against that freshly created setupEvery morning the team smiles when the build is green or (less often now) starts figuring out which component broke and fix it immediately.Special thanks to the amazing Unicorn team for creating this setup and to Vincent van Dam for co-writing this post!"
      },
    
      "conference-2020-05-15-frontend-developer-love-2020-html": {
        "title": "Frontend Developer Love 2020",
        "url": "/conference/2020/05/15/frontend-developer-love-2020.html",
        "image": "/img/frontend-developer-love-2020.png",
        "date": "15 May 2020",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  How thinking small is changing software development big time  How to pack your Webpack  Svelte: the last framework we need?  Serverless gives you wings  Modern solutions for e2e testing  Practical a11y for web apps  Micro-interactions with React Spring  But, you’re not Facebook  Beats, rhymes and unit tests  GraphQL without a database  DX is the new black. Learnings from using Nuxt and Storybook at scale  Refactor your life  Blazor with WebAssembly  Audio Streaming - Using WebRTC for building your own Voice AIs  The future of real-time, offline, data  The state of WebAssembly  ConclusionIntroductionThe Frontend Developer Love conference day was the first of 3 conference days that I was visiting in Amsterdam.While the last 2 days were focussed on VueJS, the first day was filled with more generic frontend topics.These topics ranged from Webpack, serverless to micro-interactions and more inspirational talks.With a big countdown clock, the conference was about to start.How thinking small is changing software development big time, by Sander HoogendoornAs a keynote, we started the day with a non-technical talk.These days we developers face mainly two challenges: the speed at which we have to deliver results and the legacy code that we must handle.With the constant changing technology landscape, we can expect that software will not survive for 1000 years.Regularly adapting and delivering is a key component of delivering quality software.Sander talked about his current and previous experiences in the volatile world of software development.Like how a 9-to-5 mentality is not always the right choice as not everybody is that productive during the day.Factors such as strict working hours ruin productivity.But the same can be said about the usage of open floor plans which explains the popularity of noise-cancelling headphones.We need to think for ourselves and don’t copy what other big corporations might be doing.In the end, it all boils down to trying to release as fast, as often and as small as you can.Even if that means following weird office hours…  Check out Sander’s talk, Sander’s Twitter, his slides and the geek productivity chart resourceHow to pack your Webpack, by Johannes EwaldIn this talk, Johannes explained more what Webpack is and how you can define your own Webpack config.While most frontend developers use CLIs these days, a lot can be learned from actually writing a Webpack config so that you know what your favourite CLI is generating.With the latest Webpack we now have TypeScript support in our Webpack config file.He also described the several key components of a Webpack config.Such as the entry element which denotes the base of your module tree.In the end Johannes gave a couple of tips to create a good Webpack along with some tips for your code to decrease the bundle size that Webpack will generate:  Most apps will have a good bundle size just by setting the Webpack mode to production. You don’t need to overthink your configuration, Webpack already optimises a lot for you  Lazy load modules with the use of import()  Check bundlephobia.com to determine if a bundle could be replaced by something more lightweight or more tree-shakeable  Measure the performance with a tool like Lighthouse  Don’t overestimate long-term caching. It is ok for fonts, images and CSS but caching whole pages could be not as rewarding for the amount of effort you need to put into it  Minify your CSS with a tool like optimize-css-assets-webpack-plugin  Check out Johannes’ talk, Johannes’s Twitter and his slidesSvelte: the last framework we need?, by Alexander EsselinkOn its website, Svelte  claims to allow you to write “cybernetically enhanced web apps”.But what does that mean?Alexander tried to explain why Svelte is such a great framework.To start with, Svelte is truly reactive.While in most frameworks you need to call certain functions like a ‘set’ or ‘setState’, Svelte parses your JS to add reactivity.So a basic statement like count += 1; will be reactive as all code that relies on the value of count will be updated.An example Svelte component could look like this:&lt;script&gt;\tlet count = 0;\tfunction handleClick() {\t\tcount += 1;\t}&lt;/script&gt;&lt;button on:click={handleClick}&gt;\tClicked {count} {count === 1 ? 'time' : 'times'}&lt;/button&gt;&lt;style&gt;button {\tborder: 1px solid red;}&lt;/style&gt;As you can see, we have grouped everything of the component into one file: styling, the script and the HTML code.Between the style tags, you can put your regular CSS.Between the script tags, you can put JavaScript while your HTML can reference anything that you’ve defined within your script tag such as count.This allows for the creation of small readable components that anybody, even someone without prior knowledge of Svelte can read.  Check out Alexander’s talk, Alexander’s Twitter, Svelte.devServerless gives you wings, by Yan CuiThese days we can expect that we will have users of our web applications that are distributed around the world and they will use our web app 24/7.Because of this, deploying your web application into the cloud is very interesting.It is resilient, scalable, fast and secure.As we always need to prepare for success, these are all contributing elements to choose for something in the cloud.For some, using Docker seems to be the holy grail, especially in combination with container services like Kubernetes.But to quote Matt Klein: “Unless you’re an infrastructure company, infrastructure is basically overhead”.Going serverless means that:  you don’t pay for it if no-one uses it  you don’t need to worry about scaling  you don’t need to provision and manage serversWith the help of Functions-as-a-Service tools like Google Cloud Functions, AWS Lambda and more, frontend developers can leverage whole blocks of business logic into functions that live in the cloud.As a result, they can decrease the amount of work that backenders need to do, thus allowing them to focus on more critical stuff and not being occupied with providing some basic API.  Check out Yan’s talk, Yan’s Twitter and his slidesModern solutions for e2e testing, by Anastasiia DragichDuring her talk, Anastasiia gave an overview of all current end-to-end testing frameworks.While the Selenium WebDriver can be considered as Genesis, we have seen a steady rise of different e2e frameworks.If you’re looking for an all-in-one solution, Cypress is the obvious choice.But we have some other options although they might not be all-in-one solutions.Puppeteer for example is a quicker alternative as its only task is to control a browser so you can add your own testrunner like jest.But while Puppeteer is a valid choice, there is a new kid on the block called PlayWright.PlayWright is built by the same Puppeteer people who now work for Microsoft.It is a Node library to automate the Chromium, Webkit and Firefox browsers with a single API.An example of how powerful PlayWright is, lies in the fact that we have full control over the browser context.For example, we can emulate that we visit a website from a specific location with a specific type of mobile browser and take a screenshot:const { webkit, devices } = require('playwright');const iPhone11 = devices['iPhone 11 Pro'];(async () =&gt; {  const browser = await webkit.launch();  const context = await browser.newContext({    viewport: iPhone11.viewport,    userAgent: iPhone11.userAgent,    geolocation: { longitude: 12.492507, latitude: 41.889938 },    permissions: ['geolocation']  });  const page = await context.newPage();  await page.goto('https://maps.google.com');  await page.click('text=\"Your location\"');  await page.waitForRequest(/.*preview\\/pwa/);  await page.screenshot({ path: 'colosseum-iphone.png' });  await browser.close();})();In the end, there are multiple tools available to perform e2e tests on your web application, you just have to pick the one that best fits your needs.  Check out Anastassiia’s talk, Selenium WebDriver, Cypress, Puppeteer and PlayWrightPractical a11y for web apps, by Bob BijvoetEven before you start writing a11y specific HTML such as the aria-label attribute, we can take certain tips into consideration to make our web apps more accessible.As a start, our pages should be perceivable:  Logical ordering of elements: what’s important such as an article header should come first  Don’t rely on colour: be aware that not everybody can perceive colour the same way as you can. Use shapes and different sizes to distinguish between elements, colour in itself is not enough  Use contrast: all different elements should be distinguishable one from the other  Don’t rely on orientation: with the rising number of mobile users, you should keep in mind that not everybody uses their phone in portrait mode, make sure that your web application also works in landscape modeWhile trying to make your page as perceivable as possible, don’t forget to focus on making your page operable.A lot of users rely on a keyboard so make sure that they can tab through your content easily.Having a logical focus order makes most sense.To make your page more operable, add labels to UI elements to help them describe what they do.For example, it is of no use to add a search icon to a search button without adding the “Search” text somewhere.Screen reader users will be very grateful for such small adaptations.Most of the tips that Bob gave were easy to verify on your own.For example, try to use your web application with only your keyboard to check if it’s easy to operate.Or how about turning of your CSS to see if the order is logical and if your application is still usable.And if you can, try to use your web application with just a screen reader.On a Windows, NVDA is a good option while MacOS X has the VoiceOver option.  Check out Bob’s talkMicro-interactions with React Spring, by Emma BostianMicro-interactions are small animations.They are important in your web app as they have a lot of added value.  They enforce perceived performance  They illustrate a state change  They draw attention to something  They inform the user about the status of a task  They build habits  They delight our usersIdeally, for each interaction that results in a state change you should have an animation.This will help improve the user experience as the user will better perceive what has changed.It is best to keep in mind that:  Animations should be accessible. Accessibility should never suffer from the introduction of an animation  You should make them relatable  You should be intentional with the placement  You don’t let your users wait. Why not already start an animation while for example your backend call is going?During her talk, Emma showcased her live coding skills as she took a full screen menu and animated it with the use of React-spring.React-spring looks to be an interesting library that exposes hooks such as useSpring() to easily configure animations.Having for example a menu slide in from the top of your page with a change in opacity, could be something as simple as this in your component:const [fullMenuVisible, setFullMenuVisible] = useState(false);const fullMenuAnimation = useSpring({\ttransform: fullMenuVisible ? `translateY(0)` : `translateY(-100%)`,\topacity: fullMenuVisible ? 1 : 0});After that you can add fullMenuAnimation to your HTML with the react-spring factory animated.A menu can thus end up like this:&lt;animated.div className=\"menu menu--full\" style={fullMenuAnimation}&gt;\t&lt;nav&gt;\t\t&lt;ul className=\"menu-list menu-list--full\"&gt;\t\t\t&lt;li className=\"menu-list-item menu-list-item--full\"&gt;\t\t\t\t&lt;a href=\"/\"&gt;Home&lt;/a&gt;\t\t\t&lt;/li&gt;\t\t\t&lt;!-- more elements --&gt;\t\t&lt;/ul&gt;\t&lt;/nav&gt;&lt;/animated.div&gt;  Check out Emma’s talk, Emma’s Twitter, React-spring, the Codesandbox with the end result, Emma’s blog post about micro interactions part 1 and Emma’s blog post about micro interactions part 2But, you’re not Facebook, by Kitze“This is going to be the most entertaining talk of the day, mark my words”, said the friend next to me.And he was right, Kitze was able to give a fun yet very interesting talk about the current culture in IT companies.We tend to aim for certain goals that are just not necessary to achieve.Why should you have a PWA portfolio for example?Or what’s the point in having a 100 score on all Lighthouse tests?But what should you do?Stop solving solved problems! There are already enough methods to implement button styling, state management and so on.There are complete design systems that you can reuse, so why not reuse them?Because ultimately, your end-users don’t care about the technology.Look at your analytics once in a while and see if your users like your app or not.Because even if you have overengineered your app, if the end-users don’t like it then there’s no point.  Check out Kitze’s Twitter, Sizzy, Kitze’s TwitchBeats, rhymes and unit tests, by Tony EdwardsThe Web Speech API consists of 2 parts: the Speech Recognition API and the Speech Synthesis API.In short, the Speech Recognition API allows you to transform speech into text and the Speech Synthesis API allows you to transform text into speech.In his talk, Tony asked himself: how good would the Speech Recognition API be in analysing hip-hop lyrics.Tony showcased his abilities to bring a live demo of the implementation of the Speech Recognition API.Would it be able to transform his live lyrics into text?Even though the technology still has a long way to go, it was still impressive that it was able to transcribe more than half of his live lyrics.  Check out Tony’s talk, Tony’s Twitter and a Spotify list with songs from the talkGraphQL without a database, by Roy DerksGraphQL is one of those technologies that a lot of frontenders want to use.But most backenders are not that keen to add a GraphQL API to their existing REST APIs.But why would that stop you?There are multiple options to integrate a GraphQL API in your project without having to bother your backenders.Roy highlighted two of them.The first one was apollo-link-rest.This allows you to call REST endpoints from within your GraphQL queries while having all your data managed by ApolloClient.These REST endpoints can be bundled together in the same GraphQL query so that getting information about a product with ID 3 and data about its rating and categories, might end up like this:query getProduct {\tproduct @rest(type: \"Product\", path: \"product/3\") {\t\tid\t\tname\t\tprice\t\tthumbnail\t\tcategories @rest(type: \"Category\", path: \"products/3/categories\") {\t\t\tname\t\t}\t\trating @rest(type: \"Rating\", path: \"products/3/rating\") {\t\t\taverage\t\t\tcount\t\t}\t}}Another way to integrate a GraphQL API is to use the package OpenAPI-to-GraphQL.The idea behind this is to use an OpenAPI specification coming from something like Swagger that will be used to generate a schema which will build a GraphQL server.To help with all this, there even is a CLI to make your life even more easy.With these two tools, you can already start integrating the usage of GraphQL in your frontend code without having to rely on backend.The backend is not forced to immediately start making the transition from classic REST APIs to GraphQL.This is the perfect way to test out if GraphQL brings any added value to your project.  Check out Roy’s talk, Roy’s Twitter, his slides, apollo-link-restDX is the new black. Learnings from using Nuxt and Storybook at scale, by Aurélie VioletteAurélie used Storybook in her projects as a tool to demo stuff, have live documentation and to enable visually driven development.Storybook in itself is already a great tool to showcase your components with their different use cases.But Aurélie extended its functionality by adding the Knobs addon as well as the Docs addon.The concepts that she uses, is to bring “Nuxt logic” to your Storybook project.By adding components into Storybook, developers are tempted to just write the visualisation of their components with some mock data.But why not add some business logic into it instead of just being occupied with showcasing your components?If your actual component uses data coming from a store, why not implement a store in your Storybook stories to help you mimic the real use case of your components?Storybook’s functionality can be extended by writing decorators.An example she gave of a decorator to add a store to your components is this:\timport addons, { makeDecorator } from '@storybook/addons'import { STORY_CHANGED } from '@storybook/core-events' export const withStore = makeDecorator({  name: 'withStore',  parameterName: 'store',  skipIfNoParametersOrOptions: false,  wrapper: (getStory, context, { parameters = {} }) =&gt; {    const { modules = {} } = parameters    return {      created() {        for (name in modules) {          this.$store.registerModule(name, modules[name])        }        const channel = addons.getChannel()        channel.on(STORY_CHANGED, () =&gt; {          for (name in modules) {            this.$store.unregisterModule(name)          }        })      },      template: '&lt;story&gt;&lt;/story&gt;',    }  },})After activating the decorator with addDecorator(withStore), your components will have access to the store.So now you can make your examples in Storybook even more linked to the real use of the components.  Check out Aurélie’s talk, Aurélie’s Twitter and her slidesRefactor your life, by Noer Paanakker &amp; Sima MilliNoer and Sima talked about Hack Your Future, a coding school for people that have limited access to education and the labour market.They talked about a few of the heart breaking stories of their students and highlighted how the program helped these people try to build a brighter future for themselves.In just over 4 years, they’ve helped get 120+ people land a good tech job.With Behind The Source, they highlight a couple of the other stories on how being a refugee wasn’t a choice but becoming a developer was a choice.  Check out Noer &amp; Sima’s talk, Hack Your Future and the Behind The Source videoBlazor with WebAssembly, by Don WibierBlazor lets you build interactive web applications in C# instead of JavaScript.In fact, it allows you to create a component-based UI with a combination of C#, HTML and CSS.Both the client and server-side code are written in C#.This has one big benefit: you can share code and libraries between your front- and backend code.A really basic example of Blazor code would be this:&lt;h1&gt;Counter&lt;/h1&gt;&lt;p&gt;Current count: @currentCount&lt;/p&gt;&lt;button class=\"btn btn-primary\" @onclick=\"IncrementCount\"&gt;Click me&lt;/button&gt;@code {\tprivate int currentCount = 0;\tprivate void IncrementCount()\t{\t\tcurrentCount++;\t}}Those familiar with C# and mainly .Net Razor pages, will recognise the syntax.Razor allows you to write both your HTML and C# in the same file.One of the interesting features of Blazor is the ability to compile it to WebAssembly.In fact, your client-side C# code is being run by WebAssembly in your browser.In a Blazor project, you even have your client-side and server-side code right besides each other.A typical Blazor project has the following structure:  /Client  /Server  /SharedIn this structure, your whole frontend is situated in /Client while all your backend code is situated in /Server.If you are in need of any code sharing, you can put it into /Shared and it will be available in both /Client and /Server.  Check out Don’s talk, Don’s Twitter and BlazorAudio Streaming - Using WebRTC for building your own Voice AIs, by Lee BoonstraTools like the Google Assistant, Amazon Alexa and Apple’s Siri are becoming more popular.The usage of voice assistants is becoming more mainstream as prices are dropping and they are becoming less of a gimmick and more of a tool to use during your everyday life.And while there are lots of developer tools available to integrate your app with these particular voice assistants, this might not be your best course of action.You’re limited to the technical requirements of these assistants, they might be overkill for your use case or they might not fit for your enterprise usage.WebRTC is an open web standard and is available as a regular JavaScript API in all major browsers.It allows for real-time communication (RTC) in the form of audio and video communication via direct peer-to-peer communication.RecordRTC is a WebRTC JavaScript library for audio, video, screen and canvas recording.By combining RecordRTC together with a tool such as Dialogflow that can parse voice recordings to text, we can create our very own voice assistant.Tools such as Dialogflow use machine learning to parse voice recordings to achieve intent matching.An intent categorizes an end-user’s intention for one conversation turn.By trying to match phrases or parts of phrases, Dialogflow tries to classify the end-user expression to the best intent.It then tries to parse the input with the help of the intent to extract information.For the example of weather forecast queries, if Dialogflow is able to match the queries to the forecast intent, it knows it can try to extract information such as time and location.Linked to this intent, one can add actions to perform or responses to give.By training the system, the classification of intents can be improved as to ensure that the voice assistant gives back the correct answer.During her talk, Lee demonstrated the Airport SelfService Kiosk, a demo in which microphone streaming is used to give the end-user information linked to their flight in an airport.  Check out Lee’s talk, Lee’s Twitter, her slides, RecordRTC, Dialogflow and the Airport SelfService KioskThe future of real-time, offline, data, by Nader DabitWhen trying to write an offline-first app, you should take three things into account:  Code should work offline &amp; online  Write your data locally and replicate it to a database: you should always have a local copy of all the relevant data  Provide good user experience in case of bad internet: your app should still be usable when your user has internet issuesYour app should feel real-time:  Give your user a sense of real-time: don’t delay things and add animations to enhance the feeling of real-time  Allow for synchronisation between multiple devicesTools such as AWS AppSync and AWS Amplify can help you with that.AppSync is a managed service that uses GraphQL to make it easy for applications to get the data they need from multiple sources with the option to have real-time updates.Amplify is a framework to build cloud-based full-stack serverless apps.By combining the forces of these two tools, we can create real-time, offline data, especially if we combine it with GraphQL.Your data model can be defined by a GraphQL schema.In your application, you can use GraphQL subscriptions to have real-time updates for your application data.There are options to get updates such as long polling, server sent events and web sockets.It depends on your use case to choose what option is best for you.But getting real-time updates also forces you to think about conflict detection and resolution.Like what do you do when you get multiple updates at the same time?What if your connection is down for a while?There are a couple of popular ways to tackle these issues.AppSync already uses the solution of monotonic counters combined with a base table that contains all your base data while also maintaining a change table to log all operations that happen on the base data.AppSync will auto-merge everything for you while also offering other options if necessary.Check the Amplify DataStore documentation for more information.  Check out Nader’s talk, Nader’s Twitter, AWS AppSync, AWS Amplify and the Amplify DataStore documentationThe state of WebAssembly, by Sendil KumarnThe final talk of the day was by Sendil who came to explain what the current state of WebAssembly is.WASM, short for WebAssembly, is a high-level definition of how to run bytecode in your JavaScript engine.At its core, it’s a stack machine that uses a linear memory model by using a shared array buffer.Contrary to what you might expect, WASM is not faster if you would use it for lots of DOM operations.The advantage of WASM lies in the fast calculations that can be done.To write WASM, you can look at multiple higher-level languages that compile to WASM code.If you’re a fan of C/C++, the tool Emscripten helps you out by compiling a C/C++ module to an HTML page.For the Rust lovers, you can simply annotate a function with #[wasm_bindgen] to activate the wasm-bindgen library and allowing to compile a WASM function.For the TypeScript lovers, there is a strict subset of TypeScript called AssemblyScript that allows for compilation.So the tools are already there to write WASM code but the future looks really bright as multiple features are currently being specified:  Interface types to help describe higher-level values such as strings and records  Single Instruction Multiple Data so that WASM will be more efficient on newer instruction set architectures up to 128-bit.  Garbage collection  … and more!All this will lead to an even better performance of WebAssembly combined with more use cases so that WASM can become a more popular standard on the web.  Check out Sendil’s talk, Sendil’s Twitter, WASM.org getting started with Emscripten, wasm-bindgen library and AssemblyScriptConclusionThe first of three days in Amsterdam was packed with lots of great talks on a multitude of subjects.Combine those interesting topics with a great venue (the screen opened for access to the break room!) and you have a killer combination.If this was day 1, I could not wait for day 2 and 3 that were going to be more focused on VueJS.I returned to my hotel satisfied with the amount of stuff I had learned that day and was excited for what was still to come."
      },
    
      "docker-2020-05-07-jib-html": {
        "title": "Jib: The next big thing to build your Docker images",
        "url": "/docker/2020/05/07/jib.html",
        "image": "/img/2020-03-30-jib/jib-logo.jpg",
        "date": "07 May 2020",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Jib  Usage  Conclusion  Useful linksIntroductionNowadays, you hear a lot about the term ‘containerization’ and the benefits of this technology.Containerization has become a very popular thing in software and cloud development as more users and companies start to adopt it. Because of this, user experience and usability also have to grow. In other words, how do we make the creation of a Docker image as easy as possible for a developer? After all, we do love it when things just work.The process of creating a Docker image has always evolved in the years to be as simple as possible. With the amount of resources and information we have today, it shouldn’t be hard to write your own Dockerfile. You can also use a Maven / Gradle plugin which interacts with your Docker CLI if you want to avoid writing Dockerfiles.But what if it could be even more simpler?JibEnter Jib. Jib is an image builder plugin released in 2018 and made and maintained by Google. To explain what it does, I will use a quote from the official Google Blog announcement:  Jib is a fast and simple container image builder that handles all the steps of packaging your application into a container image. It does not require you to write a Dockerfile or have docker installed, and it is directly integrated into Maven and Gradle.This seems like a big thing, doesn’t it? Overall, the biggest benefits of Jib are:  No need for a Docker daemon anymore in your environment;  No seperate docker build / push commands in your pipeline;  Supports both Maven and Gradle projects;  Dockerizes your application into multiple layers and separates your dependencies from your own classes to significantly reduce build time (in contrast to building fat JARs);  Jib only builds the changed layers, so unless you add / update dependencies, it won’t rebuild the dependency layer!UsageUsing the Jib plugin is pretty easy and straight-forward as it was designed to be so. All properties and settings work perfectly out of the box. This means that I can just add the plugin to my build tool and it will be able to create a Docker image without further ado. However, if your container setup is more complex, Jib allows you to override every property that best suits your needs. You no longer need a Dockerfile in your project, Jib is perfectly capable of handling your image setup on its own.You can try the Jib plugin by either using your current Java project or by cloning the Spring Pet Clinic Sample Application.Once you have your project open in your IDE, open the pom.xml (Maven) or build.gradle (Gradle) file to start adding the Jib plugin. Don’t forget to build a fresh JAR!GradleTo use Jib in Gradle, you first need to add the plugin to your build.gradle file. You can do this in the plugins block.plugins {  ...  id 'com.google.cloud.tools.jib' version '2.1.0'}Afterwards, you can add your Jib plugin configuration to the file, with the settings that you prefer.jib {  to {    image 'yolanv/jib-demo:gradle'  }}---jib.to.image = 'yolanv/jib-demo:gradle'Two ways of specifying your Jib configuration in GradleNow you can run the following command:gradle jibMavenWith Jib for Maven, the configuration is more or less the same as in Gradle, except that you need to write it in XML.In Maven, you have to add your preferred configuration with the plugin declaration. Add the following in the plugins element:&lt;plugin&gt;  &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt;  &lt;artifactId&gt;jib-maven-plugin&lt;/artifactId&gt;  &lt;version&gt;2.1.0&lt;/version&gt;  &lt;configuration&gt;    &lt;to&gt;      &lt;image&gt;yolanv/jib-demo:maven&lt;/image&gt;    &lt;/to&gt;  &lt;/configuration&gt;&lt;/plugin&gt;You can trigger the Jib plugin with the following command:mvn compile jib:buildThe plugin configuration tells Jib that, whenever Jib gets triggered, it should build an image named yolanv/jib-demo with the corresponding image tag.Running the command will trigger Jib to start building and pushing your Docker image.ConclusionIn a short time, we were able to successfully build a Docker image and push it to our registry. It requires very little configuration (depending on your setup ofcourse), it only builds the changed layers and we don’t even need to have a Docker daemon installed!Docker Hub repository with two pushed tags done with JibPersonally, I think this is a very powerful plugin that will be used in new and old projects. While I think that it is important for people to understand how Docker works and how Docker images are built, Jib still gives a lot of advantages over using regular Dockerfiles.Useful links  Jib Repository  Jib Maven Documentation  Jib Gradle Documentation"
      },
    
      "architecture-2020-05-06-datawarehouse-in-cloud-html": {
        "title": "Data Warehouse in the Cloud",
        "url": "/architecture/2020/05/06/datawarehouse-in-cloud.html",
        "image": "/img/2020-04-29-datawarehouse-in-cloud/datascience.jpg",
        "date": "06 May 2020",
        "category": "post, blog post, blog",
        "content": "IntroductionAlmost all the companies have a kind of data warehouse available for reporting purposes. Those reporting environments are most of the time hosted in an on-premise infrastructure, nowadays a lot of companies are investing money to upgrade their existing environment to a more flexible and scalable infrastructure that is hosted in the cloud. This can be the Azure Cloud, AWS, Google Cloud, etc.Not only the Data Warehouse itself is important, we need to look at the complete picture.DataOpsDataOps can help to give better insights, reduce the time to go to production and help in creating the whole package due to its process-oriented methodology. Different teams will work more intense together to improve the quality of the delivered value.Data is moved from the source to the Factory (Data Warehouse). In this factory a lot of steps and processes will take place to improve the quality of the data and gain us insights into how the data flows.Quality: The quality of data must be guaranteed; this can be done through automated tests during the processing of the data. Statistical Process Control (SPC) is there to monitor and control the data analytics pipeline.Centralization &amp; Freedom: The data is stored in a central place, which will give the different users (Data Engineer, Data Analysts, Data Scientists, Data Visualization users, Business users) the ability to play with the data to create insights on the data. Those new insights can result in new processes that needs to be implemented to make them available for all users.Catalog: A data catalog will be one of the crucial parts. The catalog maps the complete data lineage. It gives visibility on how the data has been transformed during the whole process, and the catalog contains all the definitions.Automation: The days of installing new versions of software or processing logic manually must be mini. Automation can be implemented to improve the time to go to production. Continuous Integration (CI) and Continuous Delivery/Deployment (CD) are key here. Azure DevOps can be used for creating the CI/CD pipelines.Security &amp; Privacy: Data needs to be protected to make sure that not everybody may/can see all the data. Also, the data must be GDPR compliant. Some questions to ask here: “Can we restore production data on our other environment without anonymization?”, “Do we need all the data in another environment?”, “Who can access what data?”.Monitor &amp; Improve: In every process, monitoring is key to improve the quality of the implemented processes and helps to guide the team to deliver more and better products.At the end of the factory chain, Value is delivered to the end-users in the form of Actionable dashboards that will provide them insights on the data that can be used for further analysis.Data WarehouseWhen a new modern Data Warehouse needs to be designed. It is important to create a cloud and technology agnostic architecture. The tools to use, the platform that is needed, are less important. The picture below shows the different phases and how the data flows.Source SystemsYou need to consider all the sources that are available for designing the data warehouse. It can be existing sources that are hosted on on-premise infrastructure or already in the cloud. IoT devices are becoming more and more popular these days. These devices generate a lot of data and the system needs to be able to process/store large amounts of data.Data IngestionWhat technology can we use to ingest the data from the sources to an Information Reservoir (Storage system)? Processing Streaming Data requires a different approach than data that only needs to be processed once or several times a day/week.Information Reservoir &amp; ProcessingWhere do we need to store the ingested data? That depends on the kind of data that is processed. When processing structured data, a relational database might be a good idea. But what to do with unstructured data? This data shouldn’t be stored in a database and the choice should maybe better go to a kind of file system.Information Consumption &amp; Analytic MethodsThis phase in the process is not always required, because the business is not ready to do some Data Science and Machine learning. When in the design phase it might be a good idea to think about it. What is not needed now may be applicable within a few weeks or months.Data Access &amp; Target ApplicationsThe main question here is: How will the processed data be made available to all the end users? It is possible that you need to support different kind of tools. MS Excel is the most used BI-tool for analyzing data.If you want attractive and interactive dashboards another visualization tool is needed like Power BI or Tableau.Choosing the Technology and ToolsAfter the global picture is completed, and all the phases are cleared out, it is time to choose the platform and components that are needed. The picture below shows an example of a Modern Data Warehouse in the Azure Cloud.  The structured and unstructured data will be ingested in the Data Lake Storage Gen2 using Azure Data Factory.          Data Lake Storage Gen2 is the world’s most productive Data Lake. It combines the power of an Hadoop compatible file system with the integrated hierarchical namespace with the massive scale and economy of Azure Blob Storage to help speed your transition from proof of concept to production.      Azure Data Factory is a cloud-based data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformations.        Ingested data needs to be clean and maybe the data must be transformed and combined with other data. For example, some unstructured and structured data must be combined to create a clean dataset.          Azure Databricks is an Apache Spark-based analytics platform optimized for the Azure cloud platform. It is an interactive workspace that can be set up in minutes, auto scales and collaborates on shared projects. It supports languages such as SQL, Java, Python, Scala and R as well as other Data Science libraries such as TensorFlow and PyTorch.        The cleaned data must be ingested in the DWH. As DWH we choose Azure Synapse Analytics (Former Azure SQL Data Warehouse). For filling the Data Warehouse, PolyBase or Azure Data Factory can be used. The stored data in the DWH can be modelled with Data Vault 2.0, or a star schema. Our preferred choice is storing the data in a Data Vault model. This gives us much more flexibility.Optionally, one or more Data Marts can be created. It is not required to store the Data Marts physically, virtual Data Marts are also possible          Azure Synapse Analytics is the fast, flexible and trusted cloud data warehouse that lets you scale, compute and store elastically and independently, with a massively parallel processing architecture.        The stored data, from the DWH or the Data Marts is the source of our data visualization tool. Because we have chosen to use the Microsoft stack, Power BI is our chosen tool for this purpose.The picture above does not contain all the needed components. Another component that is required in all our approaches is Azure Key Vault. This component will store all our connection strings, Certificates, Secrets and Keys. To secure your resources, Azure Private links enables you to access resources over a private endpoint in your own virtual network.The architecture can easily be extended to enable Data Science.Scalable machine learning/deep learning techniques will derive deeper insights in the data using Python, Scala or R using notebooks in Azure Databricks. To create insights, the (un)cleaned data and even the data stored in our DWH is used. Results can be stored back in the Data Lake or in the DWH, and Power BI is also be able to connect directly to your Databricks environment."
      },
    
      "architecture-2020-03-31-nx-sparse-checkout-html": {
        "title": "Managing the size of your shiny monorepo",
        "url": "/architecture/2020/03/31/nx-sparse-checkout.html",
        "image": "/img/2020-03-31-nx-sparse-checkout/header.png",
        "date": "31 Mar 2020",
        "category": "post, blog post, blog",
        "content": "The problemUsing a monorepo has some great advantages, including no longer having to manage npm dependencies for each individual project and the ability to easily create libraries and reuse them between applications.You can create libraries, for example about authentication, that each team in your organisation can include in their application and if an update should be needed to that library, the tooling you use can automatically build each (and only those) application that include that library.When talking about monorepo’s within frontend development communities, Nrwl’s Nx solution will certainly be mentioned.It allows to create a workspace in which all your applications might exist together, using one single package.json, no longer solely focused on Angular, but also React, simple web applications or NodeJS applications.It also includes some useful tslint extensions and scripts to automate build steps using a dependency graph.But what happens when you have a lot of applications in your organisation?Your IDE might start to work slower, its intellisense might get cluttered, or when searching for a specific file you receive a lot of results you don’t need for the application you’re working on.This happens because there are just too many files in your repository.The solutionGit has a little-known (experimental) feature called sparse checkout.From the git-scm.com:  “Sparse checkout” allows populating the working directory sparsely.It uses the skip-worktree bit (see git-update-index) to tell Git whether a file in the working directory is worth looking at.If the skip-worktree bit is set, then the file is ignored in the working directory.Git will not populate the contents of those files, which makes a sparse checkout helpful when working in a repository with many files, but only a few are important to the current user.In short, it allows you to checkout only a part of the repository, and when committing, only those folders that are checked out will also be committed to.It’s a bit like .gitignore, but local only.It doesn’t affect the repository itself.The information for which files to checkout is stored in a file $GIT_DIR/info/sparse-checkout and the contents have a similar syntax of the .gitignore file.So using a line like !/folder/my-folder-to-ignore would remove that folder from your working directory, while never affecting it on the remote repository.Now the idea is to remove those folders that are irrelevant to the project you’re working on by adding their paths to the sparse-checkout file.When a new checkout is performed, the changes in the file are applied and you would have a little less clutter in your workspace.If you want to know more details and some extra commands, read this GitHub blog post from Derrick Stolee.The toolOf course it’s always better to automate these things to reduce errors.I developed a tool nx-sparse-checkout, which can be added to a workspace using Nx 8.11 and above.I used the idea from KwintenP, updated it for use in Nx 8 and made some other enhancements to improve the user experience.  The tool allows you to choose the projects you want to checkout (either comma-separated or interactive) and then uses Nx’s dependency graph to determine which projects your selection is depending on and adds those to the list of projects to checkout.It then sets all projects that are not needed to be ignored using the sparse-checkout file.This way all other files (like package.json, tsconfig.json, etc…) and other folders (like tools) are still available to you.Resetting can be done by either selecting everything or passing the --all parameter.Using this technique, you have the advantages of a monorepo, while not having to deal with an enormous folder structure."
      },
    
      "conference-2020-03-31-devopsdays-2019-html": {
        "title": "DevOpsDays 2019",
        "url": "/conference/2020/03/31/DevOpsDays-2019.html",
        "image": "/img/2020-03-31-devopsdays/devopsdays-amsterdam-2019.png",
        "date": "31 Mar 2020",
        "category": "post, blog post, blog",
        "content": "  The 7th edition of DevOpsDays Amsterdam in ‘Pakhuis de Zwijger’ was my first experience with DevOpsDays. I’d like to share some talks and my impressions about the conference, because sharing ideas worth spreading is what we aim for at JWorks.Table of contents  Observability for Emerging Infra: What Got You Here Won’t Get You There, by Charity Majors  How Convenience is Killing Open Standards, by Bernd Erk  Come Listen to me, I’m a fraud, by Joep Piscaer  Fight, Flight or Freeze – Releasing Organizational Trauma, by Matty StrattonObservability for Emerging Infra: What Got You Here Won’t Get You There, by Charity MajorsCharity Majors is a co-founder and engineer at Honeycomb.io, a monitoring and analyzer tool that can gather information from various sources to provide an overview of what’s happening in your (production) environment.Most of you have experience with sieving information through logs and analyzing all kinds of dashboards to find and fully understand what went wrong in your application. Finding and understanding the data can be really hard. You lack the bigger picture because the tools for doing metrics, logging and tracing (what she calls the “three pillars of observability”) have their own benefits and drawbacks.Logs can be useful for compliance reasons but they are not so great to picture the context. Metric tools might be more visual than logs, but they are less suitable for debugging purposes or to completely understand why something failed.We as human beings like visual representation to easily perceive why certain situations occurred. For example during and after deploying new versions because the chance is higher that something will go wrong. According to Charity, deployments are not like ‘flipping a binary switch’. It’s more like a step in a long process of being confident in your code that it will work on other environments rather than just on your local machine or development environment.Especially deployments to production will not always go as planned and it will be even more difficult to do so, since infrastructure has become more complex over the recent years. We see our infrastructure changing from simple LAMP stacks to replica sets, nodes and micro-service architectures.It would be very difficult to quickly grasp what’s going on with your applications and its underlying systems without good observability.Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.It has the advantages of logging, tracing and metrics, and could be achieved with the right tools, if set up correctly. This is what Honeycomb is trying to achieve.Charity states that observability is not the same as monitoring. That is because monitoring is more like a ‘post-talk’ approach in her eyes. You’re typically too late and the damage has already been done upon discovery.Monitoring systems have not changed significantly in twenty years and have fallen behind the way we build software. Software nowadays consists of large distributed systems that are made up of many non-uniform interacting components, while the core functionality of monitoring systems has stagnated.How Convenience is Killing Open Standards, by Bernd ErkYou may know that migrating configurations from one cloud provider to another can be challenging.Cloud providers are mostly lacking an ‘open standard’ to make it easier. Bernd’s talk wants us to think about what Open Standards and Open Source could bring to us and how they have led to the success of Linux and Open Source communities we have today.In his early career, Bernd was a Solaris system engineer who maintained more than just Solaris systems within his company such as Unix-like systems AIX and HP-UX.Managing unfamiliar platforms was hard, but luckily, most *NIX systems are relying on the POSIX standard. POSIX made it possible to develop other libraries such as GNU tools so they weren’t forced to use Sun’s (proprietary) freeware for instance. Fast-forwarding to 2019, we see that open source is becoming more popular than ever before. However, regarding Open Standards, there is still some work to do.The market has changed over the recent decade. Public and hybrid cloud are emerging and on-premise deployment is declining. We see that especially AWS and Azure have been expanding in the recent years as more and more customers are using their services. All those cloud providers like AWS, Google Cloud, Azure,… have their own APIs and those are not interchangeable since they are lacking a standard. The fact that an API is open does not imply that it is a standard, it’s only something you have access to. That is why tools like Terraform are becoming popular. People have a need for a unified standard to manage their resources. Bernd thinks that Open Source is very important to have open standards although those are not the same.There is also a problem if a certain cloud provider uses Open Software. If they use a certain piece of open source software, there is no direct connection between the customer and the original developer of that software, because that layer was cut. No matter what business model, the creator of the software has to earn money.Open Source tools in the cloud make sure that you have direct contact to the creators of the software to help you out. AWS, on the other hand, has their own versions of Open Source Software like Open Distro which is basically their own distribution of ElasticSearch. This removes the connection with the original developer, because you can only contact them for support. It brings fear and protection to the developers of the software because they are afraid that money and intellectual property goes away. That’s the reason why they came up with own licenses like Server-Side Public License (SSPL) that MongoDB uses, for instance. This tells you that if you alter it to sell it as a SaaS offering, you have to contribute to that code.And Redis prohibits you to sell their enterprise model as their cloud solution.Bernd doesn’t like that people come up with new licenses, but he also understands that they are afraid that their money stream would go away, and they see this as a possible solution. We have to see that we support enough Open Source and Open Standards, because they need it to improve and maintain their software or service.In today’s oligopoly of cloud providers, it’s really difficult to new players to emerge their new business, although this will be good to have a more diverse internet. Most companies are focusing on the more human side of things when it comes to diversity, which is great, but there is a lot of work on the technology side as well.He compares it to when there are only a few companies that control the availability of Insulin because there’s no market law to reduce prices. It would be a lot cheaper if there were more players that provide Insulin.“Abuse of Power comes as no surprise”.At the end of his talk, he draws some conclusions:Be reasonable  Think about what you are doing.  It must be easier for new players to enter the cloud market, with the help of open standards.Interoperability is important  You can easily work with different services from different providers so you can work together.Support variety  Try to be out there and have a look on the industry on how it is. Think about what your next move will be.Demand contribution  Demand as a customer what you want to obtain from your provider.Diversity is up to you  Demand more diversity on the technical level.Come Listen to me, I’m a fraud, by Joep Piscaer  “Do you tend to chalk your accomplishments up to luck or timing”; “do you hate making a mistake, being less than fully prepared or not doing things perfectly or do you even fear feedback?”Joep’s talk started with an exercise to see if the people in the audience have some form of Impostor Syndrome by asking several questions like the one mentioned above.People had to raise their hand if the question applied to them and to keep them up during the next questions. At the end of his question round, more than half the audience had raised their hand.A collection of feelings of individual doubts that overwhelmed their successes and accomplishments which makes them think they are a ‘fraud’. He described Impostor syndrome as an internal measuring stick that is broken or Pluralistic ignorance. Doubting yourself privately, believing you are alone in thinking that way because no one voices their doubts. It is very likely that one or more of your colleagues have Impostor Syndrome although they will not admit they suffer from it. Some are more open about that topic than others. No matter how successful they are in their personal life and career, they ‘suffer’ Impostor Syndrome in some way or another. Mike Cannon-Brookes, CEO of one of the larges DevOps companies in the world is very open about it during his TED talk.  “Other people feel like this, too. And apparently it doesn’t go away with more success.” - Mike Cannon-Brookes, AtlassianAtlassian started, like most big tech companies, from a garage. It grew bigger and bigger and at some point, they needed an HR-person. While Mike was interviewing his potential HR-person, he felt a bit unsure, because he has never worked in a company with an HR-person before. So how will he know if the potential HR-person has the right skills and attitude?Like with many things, there is always a beginning where you have to figure out what way is the right way to do things. There is mostly not one best practice since everyone has their own. It is sometimes hard to know if your way is the right way, the good way or just bad. That will mostly lead to discussions and insecurity with people, because they don’t know or they are not really sure if their statement is well grounded.Especially technology can change rapidly over time. By the time you have learned a new thing it’s already obsolete.Joep learned about Novell back when it was the latest and the greatest server Operating System you could have.By the time Joep became a guru, it was already obsolete and surpassed by other Operating Systems.The tech landscape changes every couple of years and so sometime, you have to learn things again. That doesn’t really help when the Dunning-Kruger effect applies when learning new stuff. Most people don’t really discuss that certain things are new to them and that they are not really good at it yet. They feel insecure sometimes because they are unsure or afraid that their idea or statement is incorrect or a best practice.There are certain things you can do to reduce the feeling of the Impostor Syndrome (or to recalibrate your ‘internal ruler’ so to speak).Nothing compares to you. Every time you say something negative about yourself, say two things that are positive about yourself. It is as easy as that to retrain your internal NLP (Neuro Linguistic Programming) filter. At some point, your negative thoughts will diminish, and the positive ones will stay. Make sure to try this in front of a mirror!I will not compare myself to strangers on the internet. You shouldn’t compare yourself to others, especially to people on the internet, because they are the worst.Practice on giving compliments to others. And learn to receive them. They can come from different perspectives. For a kid it’s an accomplishment if he or she is able to tie his/her own shoelaces because it is new to him/her.Learn how others make stuff and let others learn from you. Other people, no matter how professional they are, make mistakes as well and can learn things from you and you can learn from them.  Pair programming  Pair review  Celebrate your failure  Speak publiclyFind something you’re passionate about and keep being tremendously interested in it.Like a hobby where you can screw things up without consequences.Fight, Flight or Freeze – Releasing Organizational Trauma, by Matty StrattonImagine a zebra grazing in the savanna of the African wilderness when suddenly a predator goes after the zebra. Of course, the zebra will run for its life (the flee state). When the zebra is caught by the predator, it ‘freezes’ because the zebra’s nervous system is overwhelmed and it has no further solutions anymore, hoping the predator will drop him and go on. After the zebra has managed to get away, it literally shakes all his stress off and goes back to grazing like nothing happened. Humans can react the same way zebras do, although we are, of course, not a zebra. We humans on the other hand can experience traumas long after a dreadful situation has occurred.Animals are not traumatized by routine threats to their lives, where humans on the other hand, are really overwhelmed and often subject to traumatic symptoms of hyper arousal, shutdown and dysregulation, as Peter Levine’s calls it. During day two, Matty Stratton, DevOps advocate at PagerDuty, was talking about crisis management and traumatic events within organizations and teams based on his own experience with post-traumatic stress.Let’s start on how traumas can occur, although it’s rather complex to explain.      Traumas occur when one’s solutions (active response to threat) does not work. The case where your nervous system can’t handle it.Traumas can result from both real or perceived threats. Even a perception of a ‘threat’ can also cause or recall traumatic situations.        Trauma is subjective and relative. Everyone experiences and handles stress in a different way. People from the military, for example, experience stress and traumas in a different way than people in software projects. But that doesn’t mean that your experience isn’t real, even though the experience from a soldier is much worse.  But how does that apply to an organization? You can deal with common situations and everything will go back to normal. It’s always business as usual (the gray line in the normal range), but severe outages or a similar event, where the team’s capacity for a solution does not work, can lead to non-discharged stress (the red line).Hyperarousal organizations are hyper-vigilant (like they are fighting Voldemort). They are hyper-aware of threats which causes a slowdown in moving forward. It’s not necessarily bad for an organization to be vigilant but it is bad when it loses a sane balance between vigilant and the will to move forward and innovate.Organizations can also be in a ‘freeze’ state when they believe that they are ‘immune’ to a standstill so they won’t make any changes on how they operate.Humans like pattern recognition. We see signals that might remind us of events that happened in the past and motivate us to respond the way we did last time. But the thing is that nothing is the same as it ever was back in the day as systems are becoming more complex as they grow.Today’s challenge is to make organizations more resilient against dreadful events. Resilient organizations are not traumatized by routine threats to their mission or business. Non-resilient organizations are readily overwhelmed and often subject to the symptoms of overreaction, shutdown and lack of regulated effort.What you can do to be more resilient is to learn from your mistakes. Organize game days where you can practice an incident or outage in a relax and safe way.ConclusionDevOpsDays has a different but interesting format.There are talks in the morning and lightning talks and open spaces during the afternoon/evening where you can bring your own topics that you want to discuss with others. What I also like is the fact that there’s a good variety between the amount of technical and soft-skills talks.The badges that DevOpsDays handed out were actually seed mats that you can plant afterwards (which I’ve done already)! UPDATE: Unfortunately nothing came out of it.Overall, it was a good and informative conference that I can recommend to anyone who is interested in DevOps and teamwork optimizations within organizations.More conference pictures can be found here."
      },
    
      "ehealth-2020-03-28-healthcare-on-fhir-html": {
        "title": "Healthcare on FHIR",
        "url": "/ehealth/2020/03/28/Healthcare-on-FHIR.html",
        "image": "/img/2020-03-28-Healthcare-on-FHIR/FHIR_logo.png",
        "date": "28 Mar 2020",
        "category": "post, blog post, blog",
        "content": "And then there was FHIRThe healthcare industry is currently buried under mountains of data, and much of it is unorganized or out of reach. For instance, care givers who are responsible for emergency care may not always have access to the patient’s history they need.This might force them to resort to guessing or basing their treatment on the information the patient provides. If the patient is unable to give a medical history or other information such as allergies, the problem gets worse.Exchanging healthcare information in a safer and faster manner is therefore a primary goal in the healthcare industry.Software developers and IT professionals in this industry are tasked with integrating software applications without sacrificing security of sensitive patient information.Today’s health IT environment is also very fragmented. Integrating with different health systems as well as sharing data is a difficult and expensive process.Each of these systems tend to favor flexibility of their specific function or department over interoperability with external applications.Fast Healthcare Interoperability Resources (FHIR, pronounced “fire”) is a set of standards providing a mechanism for exchanging data between healthcare applications. It was first sponsored by Health Level Seven International (HL7) in 2011, and now incorporates the best features from previously developed standards.The introduction of the FHIR specification was driven by the following:  Patient-centric healthcare: the patient is in control of his own medical data therefore sharing data across organizations and disciplines becomes more important.  Shift from offline to online, from desktop to cloud and from desktop to tablet.FHIR is based on the REST architectural style, thus being suitable for lightweight devices.  Data transparency becomes more and more important. FHIR acts as an ‘Open API’ to access data silos so systems can more easily collaborate with each other.  Data analytics is a hot topic and requires data transparency but also for the data itself to be in a format which is optimized for analysis.Standardized and well-documented models as well as support for JSON-like formats by modern databases make FHIR an interesting choice.  “The intended scope of FHIR is broad, covering human and veterinary, clinical care, public health, clinical trials, administration and financial aspects. The standard is intended for global use and in a wide variety of architectures and scenarios.”  - HL7 (Health Level Seven International)This relatively new protocol has several advantages:  The FHIR specification is free to use, open source with no restrictions and entirely available online at https://www.hl7.org/fhir/.It has been licensed under the Creative Commons Public Domain License, a license that permits the following: “You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.”  It has a strong focus on fast and easy implementation by using common tools, formats and web-based technologies without a steep learning curve. Multiple implementation libraries are available with many examples to kickstart development.For instance, the HAPI FHIR library is an open source implementation of the HL7 FHIR specification for Java.  The FHIR format is human-readable.Although it is not intended for direct human viewing, being directly understandable helps both implementers and medical personnel.The base out-of-the-box interoperable resources can be used as is, but can also be extended and adapted for local or regional requirements.  FHIR leverages modern web-based communication technologies such as XML, JSON, HTTP, Atom, OAuth, …It supports RESTful architectures, but other information exchange architectures/paradigms as well. The Document paradigm allows a system to send over a collection of resources about a Patient, for instance a referral to a specialist which involves the patient’s history including medications and diagnoses. FHIR also supports the Messaging paradigm which is often based on real-world events like a Patient being discharged from the hospital.Why should you be excited about FHIR?FHIR has the potential to make healthcare much more similar to other internet-based experiences that consumers nowadays enjoy in different industries.The Internet of Medical Things (IoMT) is a subset of IoT devices that captures and transmits patient-generated health data (PGHD). IoMT represents one of the largest technology revolutions and is growing at a lightning pace. The mountains of PGHD are growing every day but remain meaningless to healthcare providers if they’re not able to access the essence of the data quickly and easily. FHIR may be the glue between your electronic health record (EHR) on the one hand and your smart electric toothbrush, blood glucose monitor and fitness tracker on the other hand.It allows connecting PGHD to streamlined healthcare provider workflows and filtering the bulk-load of data in a way that makes the data useful and actionable for your care provider.Patients who see multiple care providers in different health systems might no longer have to worry about having three or four patient portals from organizations using different EHRs.One single personal health record, which integrates data from different formats, can deliver a comprehensive view of all medications, problems, and allergies.FHIR includes all aspects of healthcare-related interoperability through RESTful APIs and a common format for hundreds of clinical data models. This is useful for:  healthcare integrators: Transitioning to FHIR formatted XML/JSON objects in a RESTful architecture will enable you to have atomic data access to individual items within a resource, for example the Patient demographics or Observations for lab results.  healthcare systems: Building and running applications on this API standard will result in richer products with data connected from external systems.  patients: You can get and share your medical data in more ways than ever before, including with apps that you use.What about the tech giants?  Microsoft recently released FHIR server for Azure, an easy way to manage and persist health information in the cloud.It allows you to create and deploy a FHIR service in minutes, and leverage the elastic scale of the cloud.  Google Healthcare API bridges the gap between care systems and applications built on Google Cloud.It supports the industry standard interoperability protocols such as HL7, DICOM, and of course FHIR.Google lets its customers use this medical data for analytics and machine learning in the cloud which can unlock insights that lead to clinical improvements for patients.  Apple’s Health Records app uses FHIR to let consumers download data from their health care providers in the U.S.  Amazon Comprehend Medical works through Amazon Web Services and is a natural processing service that uses machine learning to extract relevant medical information from unstructured text and map it to FHIR resources.… and the community?A community is important for knowledge sharing and connecting experts to people and teams that need help. Luckily the FHIR specification has a strong community that helps connecting people for both giving and getting value:  Lots of public FHIR servers are available for testing.  FHIR developers participate actively on StackOverflow.  DevDays is the most important and largest FHIR-only event in the world.The DevDays mission is to give health IT professionals around the the world the opportunity to learn about FHIR, to meet with colleagues and exchange ideas, and to apply what they have learned in their day-to-day work.What does FHIR look like?The following patient browser application gives an idea of what FHIR looks like and how easy it is to exchange data: https://patient-browser.smarthealthit.org. Definitely check out this demo app!FHIR ResourcesA resource is the smallest unit of exchange with a defined behavior and meaning in interoperability, such as a Patient, a Device, an Observation, an Allergy Intolerance, … .It has an identity and location (URI) where it can be found.Furthermore, it is made up of elements of a particular datatype, and can be represented either as an XML document or a JSON document.An exhaustive list of FHIR base resources is described here: http://www.hl7.org/implement/standards/fhir/resourcelist.html. The FHIR development team has adopted the 80% rule for resources: only define and include concepts applicable to 80% of normal implementations.Adherence to the 80% rule is key to keeping the standard usable and not too overwhelming.What about the other 20%? FHIR has a built-in extensibility capability for specific requirements of a particular region, discipline or organizational process.To make extensions manageable, a set of requirements is defined that must be met as part of their use and definition.Each resource is annotated with a number or an N letter.This is the FMM (FHIR Maturity Level) which goes from 0 to 5 and finally N (Normative).0 means that the resource is still a draft.5 means that it has been published in two formal publication release cycles and has been implemented in at least five independent production systems in more than one country.A resource is Normative when it is considered stable.Other metadata also include a Security Category which represents the sensitivity level (i.e Anonymous, Business, Individual, Patient).The Boundaries and Relationships metadata describe when to use it and which resources are related to or referenced from this one.Example of a Patient:FHIR Resource structureThe full specification can be found on the HL7 FHIR website.Example FHIR architecturesBuilding your solution with FHIR does not change your development process nor does it enforce a specific architecture or technology stack. FHIR can be used in a lightweight or heavyweight client, in a monolith or a microservices architecture, in a push or pull-based design, …In any case, you will still need to complete a project discovery phase, formulate a project vision and scope. You can continue to use your favorite agile practices, test driven development, CQRS and Event Sourcing, DevOps and Continuous Integration.The following section gives an example of 2 possible FHIR architectures.FHIR server with existing back-endThis is the most common scenario where you supply an interoperable FHIR API on top of an existing solution so it can be easily understood and consumed by clients.This approach results largely in a mapping effort as the existing data model needs to be converted to FHIR resources.It is possible to have a mix of both a FHIR and a proprietary API.This can have several reasons like:  the backend does not (yet) support the FHIR capabilities the client needs  the FHIR specification does not describe your healthcare domain or use caseMixed FHIR/Proprietary API with existing back-endNative FHIR server with FHIR back-endThis architecture is a FHIR-native solution because FHIR becomes the central design element of the system. FHIR is used as a platform specification that is stored directly in the SQL or NoSQL back-end data store and comes with some powerful features:  a healthcare domain model (aka the Resources) and its extension capabilities  description of the types of search capabilities supported by a FHIR server  description for a FHIR server to advertise its capabilities to other systemsNative FHIR server with FHIR back-endSummaryToday, there’s lots of buzz about FHIR.You might find it anywhere from federal agencies to major technology companies.Google, Microsoft and Apple have all thrown in their considerable weight and cloud-based resources behind FHIR and are improving interoperability in healthcare.The underlying concepts behind FHIR are important drivers of the push towards interoperability.It supports the exchange of data between software applications in healthcare, combining the best features of HL7’s existing interoperability protocols while leveraging the latest web standards and applying a tight focus on implementability. FHIR is worth paying attention to because it is a huge step forward in working with healthcare data and is likely to have a significant impact on health IT."
      },
    
      "architecture-2020-03-24-charting-non-functionals-html": {
        "title": "Charting the Non-Functional Waters",
        "url": "/architecture/2020/03/24/Charting-non-functionals.html",
        "image": "/img/2020-03-24-Charting-non-functionals/nonfunc_background.jpg",
        "date": "24 Mar 2020",
        "category": "post, blog post, blog",
        "content": "IntroductionWhere functional requirements are relatable to most stakeholders as they are derived from their areas of expertise, specifying the brunt of the non-functionals tends to fall to the solution architect. As these requirements are more technical in nature, the affinity of other stakeholders with them is not as pronounced. When inquiring about their relevance, the typical answer will be in very generic fashion, for example: “The application needs to be fast.” Trying to get more concrete and measurable statements is not an easy endeavor. We’ll get back to this further on in the blog post.Formalizing the RequirementsOnce we have the appropriate numbers, we need to formalize them and get them validated. Just as the structure and formalization of the functional requirements is expressed as user stories, or in some cases even more detailed in the form of Business Processes and use cases, in a similar manner will we structure and formalize the non-functionals based on the ISO 25010 standard. Some older architecture documents could also still make a reference to the ISO 9126 standard, which is its predecessor.ISO 25010 StandardSince the ISO standard focuses on quality and acceptance of the delivered product, the same model should be the basis to compose requirements as well. As a result, the quality attributes can be considered as the non-functional requirements of the product focusing on expectations about functionality, usability, reliability, efficiency, maintainability and portability. However, in order to make the factors more complete and representative, each of the characteristics has been split up into its proper set of sub-characteristics.Prioritizing the RequirementsNote however that not all characteristics and associated sub-characteristics are as important to each and every project. On a case-by-case basis, some of them can be given a very low priority or even entirely neglected. Hence, the priority indication is very important in the specification for every requirement. These are the tradeoffs that an architect needs to chart to arrive at the proper solution. One of the ways to weigh them was described in the book “Software Requirements” by Karl Wiegers and Joy Beatty. The idea is to chart them in a matrix which marks the requirements that take precedence over others when they have conflicting impacts on the solution. For example: When business stakeholders ask for a speed of delivery that is contradicted by security concerns demanding a mandatory waiting period for customer due diligence purposes, a decision needs to be taken on which of these requirements gets the upper hand for the implementation.A reflection of this type of requirements contradiction can be found in the CAP theorem. The CAP theorem, also named Brewer’s theorem after computer scientist Eric Brewer, states that it is impossible for a distributed computer system to simultaneously provide more than two out of three of the following guarantees: Consistency. Availability. Partition tolerance. Consistency is the characteristic that determines that a service will always give the same answer across multiple nodes. Availability reflects that every service request should get a response. Partition tolerance is the ability to handle the occasional failure in communication between two services. A balance matching the needs of the solution must be determined between these three requirements in order to implement a solution that fits the bill.There are some frameworks that already have some form of priority already built in. They implement this by reducing the number of non-functional categories to only those relevant in their point of view. For example, when we take the AWS Well-Architected Framework, the determining non-functionals are reduced to five categories based on Amazon’s experiences and best practices. For each of these pillars AWS has published a white paper on how to improve and optimize them. IT also provides tooling and labs to help the architect make the proper tradeoffs.Pillars of the AWS Well-Architected FrameworkWhen maturing the requirement list set out for the solution, it might become a difficult and complex task to get accurate numbers to quantify the requirements and make them measurable. We can gather the logs of existing systems that we are replacing to get a grip on what traffic we can expect. But this is not always an option. Suppose we are developing a solution for registering timesheets of employees in a small business that have just gotten a significant enough growth in its sales to justify the cost of such a solution. It doesn’t have numbers on how many times the services of such a system should be called and it doesn’t have an existing system from which to extract them. This is where we start to rely on deduction. We start from relevant data that we do have access to or make assumptions based on experience and we extrapolate from there. In the case of our timesheet solution, we count the number of employees (relevant data) and multiply it by the number of times a week we assume they will access the timesheet solution resulting in a quantified requirement.Data Solution RequirementsMost of the requirements stipulated by the ISO 25010 standard apply to software solutions. The game changes a bit if this solution is primarily or even solely focused on the data facet. When designing a data solution such as for example a solution that gathers and evaluates the available data for a specific topic, we look at a sister standard of the one previously mentioned. The ISO 25012 is part of the same family and offers requirements fitting two categories with some these shared by both. These categories are:  Inherent Data Quality: This category gathers all requirements that deal with the inherent data quality of the data sets that are being used in the solution.  System-Dependent Data Quality: This category deals with the requirements that have to do with the capabilities of maintaining the quality inherent in the data sets.ISO 25012 StandardOperational RequirementsWhen a solution finally goes live, it will have an impact on the existing way of working within the organization. Not only are its users a stakeholder, but there is also the team that will support the solution to consider as well. Therefore, a set of operational requirements is usually determined by analyzing how the managed services team will keep the solution up and running, how they will cope with bug fixing and evolutionary maintenance, and how to cope with negative impact.As with non-functional requirements, so can these requirements be categorized by using an ISO standard, in this case the ISO 25022 standard, or the “Quality in Use” Model. This standard defines measures for the characteristics of the previously mentioned ISO 25010 standard and consists of a basic set of impact measures and methodologies to quantify each of them. This standard is a collection of suggested measures and is by no means an exhaustive list. A visual representation of these requirements can be seen in the illustration below.ISO 25022 StandardConclusionIn summary, an architect needs to be able to get a handle on all the angles of the solution as soon as possible in the development process, and requirements, both functional and non-functional, allow him or her the overarching vision to guide development along. Be wary of analysis paralysis though. To quote the former US Secretary of Defence Donald Rumsfeld: “There are known unknowns, that is to say, there are things that we now know we don’t know them.” So, avoid trying to get the perfect picture of the requirements, and instead factor into your solution that new requirements can pop up and hidden ones can be revealed during the later stages of the project."
      },
    
      "cloud-2020-03-16-zpr-explained-html": {
        "title": "Zero Plastic Rivers - explained",
        "url": "/cloud/2020/03/16/ZPR-explained.html",
        "image": "/img/2020-03-16-ZPR-explained/zpr-banner.jpg",
        "date": "16 Mar 2020",
        "category": "post, blog post, blog",
        "content": "Table Of Contents  Introduction  Architecture  Security  Developer experience  ConclusionIntroductionAs a society we treat plastic irresponsibly. Because of us, enormous amounts of plastic waste end up in our rivers.And if we don’t remove this plastic from the rivers before it reaches the estuary, this plastic will inevitably end up in the plastic soup, which in turn gets bigger.The Zero Plastic Rivers (ZPR) initiative  tries to solve this problem by doing three things: measure, prevent and clean up.Plastic pollutes our seas and oceans.Currently at least 150 million tons of plastic waste floats in our oceans, forming the infamous plastic soup.And it is getting worse; it’s estimated another 8 million tons is added every year.That’s about one truck of plastic per minute that get’s dumped into our oceans.ApproachThe Zero Plastic Rivers initiative  wants to make sure that our rivers no longer bring plastic waste to the seas.We want to do this based on a scientific and structured approach, inspired by the principles of quantitative optimization as defined by Six Sigma.EndgoalOur rivers are severely polluted with plastic, making them the largest source of plastic soup in our oceans, even up to 80%.We can only solve this problem by ensuring that our rivers no longer supply plastic to the sea.And that is why we are striving for Zero Plastic Rivers.To be more effective in the fight against the plastic soup it is important to get more insight in how plastic moves through the rivers.This is where Bert Teunkens together with the University of Antwerp comes into play.The subject of his PhD is: Quantification and characterization of the plastic fluxes in the Scheldt basin, \u000bwith the ultimate goal of setting-up an efficient remediation.To help him reach his goal of creating strategies to combat the plastic pollution he needs insights in how different kinds of plastic moves through our waterways.Bert came up with different ways to gather this data:  using citizen science  IoTCitizen science  A large quantity of plastic objects were dipped in fluorescent paint and were tagged with water resistant stickers containing metadata about that object.These objects were released in the waterways with the assumption they would wash ashore eventually.Because they were brightly colored, they would be easy to spot by passers-by.These people could then enter various information about the object such as the unique object identifier, GPS location, pictures, description, …IoT  Another great way to gather datapoints was through GPS trackers.Various industrial grade battery powered GPS trackers were put in a waterproof casing and then deployed into the river.Using a 2G network, these GPS trackers would travel along the river and transmit a new GPS fix every hour.Various measures were taken to optimize battery consumption as we expected the trackers to travel for an extended period of time. This was done by putting a specific configuration on these devices.PartneringBert approached Ordina because of our deep IoT knowledge and user centric end-to-end project approach.Ordina helped create an application to gather the datapoints and visualise them so that he could formulate ways of setting up efficient remediation.This was quite an exciting and important project for Ordina and the JWorks crew as it checks 2 boxes at the same time: doing a project with the latest and greatest technology while having a significant impact on society!This project would allow us to use all our skills to build a solution that would solve a major problem for society.With our multi-disciplinary team we were able to tackle following domains: user experience, application and cloud architecture, frontend and backend development, security and managed application hosting.User ExperienceFrom the get-go it seemed very crucial to nail the user experience for the citizen science part of the application.The success of the project depended on benevolent strangers to pick up our brightly colored plastic waste, read the instruction and input a significant amount of data into our system.This process needed to be clear, painless and concise. Bad user interaction would lead to no datapoints and thus doom the project.The initial idea was to put QR tags on all the plastic objects and have users scan them.Altough everyone deemed this an elegant and efficient way of working, we decided to test this on “regular people”.This was done by conducting guerilla testing: talking to random people outside our office building, showing them a ZPR plastic object with a tag and seeing what they would do.Turns out very few people instinctively know what to do with a QR code.To counter this, we opted to add a very short url on the object: www.zpr.oneThis allowed more users to reach our application and fill in all the data we needed.Once they were in the application we had to make it straightforward for them to collect all the data we needed.Various rapid iteration of the UI were made using wireframes and mockups.These were tested and validated to create an optimal flow through the various screens as we wanted a very low threshold for users to input the data.We opted to create a Progressive Web Application (PWA) instead of a native application as we felt that users did not want to install yet another app.As we required access to native features such as GPS and camera a PWA seemed perfect for the job!ArchitectureTo build and run this modern and complex project we opted to use the AWS platform.For a considerable amount of time we at JWorks have been investing our efforts and resources to build up our AWS portfolio.This means we work on building up the AWS skills of our people and in parallel we work on building up our portfolio of AWS enabled solutions.We have worked out several reference architectures that we prefer to use now.The advantage of these architectures is that every consultant within our unit knows how to use them and develop applications using them.The Zero Plastic Rivers project proved to be an excellent opportunity to put some of these into practice.You can view the architecture we opted to build in the following picture.  This big architectural picture can be divided in 3 big sections:  Backend java application  Frontend ionic app  IoT sensor data ingestionWe will highlight some key features of each architectural section in the following paragraphs.Backend application  The backend itselfSince we are called JWorks and we mainly focus on Java/Javacript development it should be no surprise that our backend application is written in Java with the Spring Boot framework.In general we prefer to write backends in the microservices paradigm, but in this case the backend was sufficiently small that it only consists of 1 microservice.The application itself is a pretty standard spring boot application.We use a postgreSQL server hosted in RDS as our persistent datastore on the backend, supplemented with an elasticache Redis cluster to cache database queries and configurations for the IoT sensors used in the IoT sensor data ingestion part.Our backend service is reachable over a REST interface for the outside world, we will talk more about this interface when we discuss the frontend application.Hosting of the applicationThe backend application is hosted on our Kubernetes cluster in the AWS cloud. This cluster is an EKS cluster that we use to run several projects for customers and is also used for some of our internal applications.The EKS cluster is a multi-worker node cluster setup with multiple Auto Scaling Groups so we can guarantuee almost 100% uptime on our applications that run on this cluster.We have been using Kubernetes in different forms (on-premise, AKS, PKS,  …) for a long time now which means we have a very clear image of how to use it and how to run applications on a cluster.We make heavy use of several key features like: secrets, configmaps, …Our EKS cluster is running several plugins that allow us to quickly configure infrastructure components on the AWS cloud from within our cluster.For example the REST interface of the application is exposed through a Kubernetes ingress which is hooked up to the ALB controller plugin.This means that whenever we create a new ingress a new Application Load Balancer will be automatically provisioned in the AWS cloud to expose our deployment to the outside world. This makes it very easy to work with and allows us a lot of flexibility.Frontend  Our frontend application consists of two parts.The first part is aimed at citizens who wish to help the cause. They can feed data in the system via the citizen science application when they find a bottle as shown in the image below.This is the first way that data from the plastic bottles comes into our system. We allow the user to upload an optional image when submitting this data. These images are stored in a secure S3 bucket.The second part is aimed at the researchers, and could be seen as the backoffice of the project, where the data given by the GPS trackers and the citizens is visualized in a clear and orderly way.  To develop this application we have chosen to use Ionic. Ionic is a free-to-use web-based framework that allows you to build hybrid mobile apps for iOS and Android, all from one codebase. In other words, Ionic is a tool for cross-platform mobile development. Ionic enables you to develop mobile apps using web technologies and languages like HTML, CSS, JavaScript, Angular, and TypeScript.Data visualization  One of the most relevant components in this application is the map where the sensors and the plastic bottles in the river are visualized by means of the coordinates registered in these items as shown in the image above. For this we have chosen to use Leaflet which is an open source JavaScript library for adding interactivity to maps. They have a ton of features and plugins to support doing pretty much anything with a map that you can think of.Ionic offers a wide variety of ready to use plug-ins and one of them is the camera that enables users who decide to participate in this project to take pictures of the bottles to update the status and deterioration of each bottle in the river.Frontend TestingIn reference to software testing we have mainly used Unit Testing to reduce the number of errors that are released during deployment, which we consider critical for effective software development.Frontend deploymentOriginally we planned to host this application in a nginx webserver in our EKS cluster. We changed to S3 as it is an easier to maintain solution than running your own webserver on Kubernetes. We have setup a hosted zone in Route53 which serves as the entry point of users into our application. Route53 then forwards users who visit zpr.one to our Cloudfront distribution. Cloudfront serves the ionic app from our S3 bucket which has static webhosting enabled. This setup seems optimal as it is low maintenance, tightly secured and highly scalable.Low maintenanceTo explain why this setup is low maintenance let us take a look at the components used in this architecture.We are making use of Cloudfront, S3 and Route53 in this setup.All of these services are managed services provided by AWS.This means that there is no maintenance required on our part as AWS guarantees uptime and makes sure that everything is running smoothly.The only manual actions that have occurred on our side in this setup so far was to clear the Cloudfront cache after releasing a new version to have the new version more quickly available to users of the app.Tightly securedSince we are using only managed services from AWS the burden of patching those services and making sure they are secured is on AWS itself.AWS has an excellent reputation on this regard so we feel very comfortable in this regard.We also make use of several additional features provided by AWS to secure our application further.For example the S3 bucket that is used to host the website is only accessible through the Cloudfront distribution.So users do not need access to the S3 resources itself, we implemented this nicely through Bucket policies and IAM access control.Highly scalableSince we are only allowing traffic to our application from the Cloudfront distribution this means that we get all the benefits from this global CDN.Cloudfront operates on the AWS edge locations which are spread throughout the world.Because our application is mostly Belgium based this was not as important to us but the fact that Cloudfront routes its requests over the internal AWS backbone makes a huge difference in speed which is a nice feature if you are working with global applications.The S3 service which acts as the origin for our Cloudfront distribution is nearly infinitely scalable as proclaimed by AWS itself.The interaction between our frontend and backend happens over REST services provided by our backend in the EKS cluster which is exposed over an ALB so we are very confident that we can scale up as needed.IoT sensor data ingestionIoT is all about processing a large quantity of messages.What makes IoT data challenging from a developer perspective is threefold:  Protocol  Data format  Message ContentImagine you have a device that captures and delivers GPS data. Seems simple enough right? Guess again!A hardware vendor can decide to mix and match these 3 components.The vendor can those over which protocol he wants to send the data.Some examples are: HTTP(S), TCP, UDP, MQTT, COAP, …He can also use different kinds of data-serialization formats to get the information across the network of choice: JSON, XML, Hex, Binary, something proprietary, … Different kinds of parsers will be needed.And last but not least: he can organise the way a message is structured. He can name fields any way he wants and use any kind of data type. Imagine two vendors reporting battery capacity. One could report it by sending a field called “battery” and reporting battery voltage.Another could use a field called “power” and return a battery fill level percentage.Soon, it can become quite complex due to the number of combinations possible.Some of our plastic containers send their location via the 2G cellular network at regular intervals.These messages reach us via a public network through the tcp protocol.As various protocols such as TCP and UDP are quite prevalent in IoT solutions, we do see that they are not yet first class citizens in the cloud.Eventhough it is possible to modify the ingress to kubernetes on our NGINX to allow TCP data to pass through, this is not a scalable solution. Imagine having thousands upon thousands of devices starting new TCP connections. This would kill our NGINX.To solve this problem we used a native AWS component: the network load balancer. This allowed limitless scaling of TCP connections. These TCP connections would then end up on an Spring Boot application hosted on AWS Beanstalk, which is basically a managed horizontally scalable Tomcat server. This application has to handle the interactions with the devices and acts as a “sensor gateway”.The sensors can receive instructions and updates, but this has to happen inside the same open tcp connection within a very short timeframe.This gateway consults the Elasticache for any needed instructions or updates.If a return message is needed, it is sent through the open tcp connection.The sensor detection message is then passed on to an SQS queue. From here on out, the focus of handling the message is less time-sensitive.A Lambda function decodes the message on the queue and then pushes it to another SQS message queue.A Spring Boot backend that is deployed in our kubernetes cluster handles these last events and persists them to our database.SecurityOne key element of the security is controlling who has access to an application. To strengthen security, reduce risk and improve compliance, it is essential that only authorized users get to access specific data in an application and that authentication is required before that access is granted. This means that authentication is a critical component for most applications and in this project it was no exception, as we needed to secure the data visualization part of the application so that only researchers have access to advanced functionality.To perform this authentication, we have chosen to use AWS Cognito as it dramatically simplifies application development by providing an authentication service that is simple to integrate into any modern application. In addition to storing login information, Cognito can store standard and custom user account settings. Learn more about AWS Cognito and its advantages here.Another advantage of AWS Cognito is that it supports OpenID Connect which is a simple identity layer built on top of the OAuth 2.0 protocol, which allows clients to verify the identity of an end user based on the authentication performed by an authorization server or identity provider (IdP), as well as to obtain basic profile information about the end user in an interoperable and REST-like manner. Learn more about OpenID Connect here.AWS Cognito and OpenID ConnectTo carry out authentication using the OpenID Connect standard with Cognito we have chosen to use the Authorization Code Grant which is the preferred and most secure method for authorizing end users. Instead of directly providing user pool tokens to an end user upon authentication, an authorization code is provided. This code is then sent to a custom application that can exchange it for the desired tokens. Because the tokens are never exposed directly to an end user, they are less likely to become compromised.The image below illustrates the flow, and, in this blogpost, you can find more information about this approach.To secure our frontend we have used Manfred Steyer’s Angular-oauth2-oidc library but you could use any library as long as it is OpenID certified.Our colleague Jeroen wrote a fantastic blogpost that was very helpful to us. Jeroen shows the necessary steps to follow to secure any web application using OpenID Connect.D-Day      Tuesday December 17th was D-day. That day the bottles and sensors were finally thrown into the water. We had a tight timing because the bottles had to be thrown in the Scheldt river at high tide, at 3 different locations. It was a nice dry day and our client was quite nervous. Are all the signals coming in properly, is the sensor packed waterproof, …?  Especially because we were not able to test all that much with the sensors due to the tight timing. At high tide, it was time to throw the bottles in the water and register the sensor via our Ionic App. Everything runs smoothly and the signals from the sensors come in. You see the customer cheer up and leave satisfied to the next location. Everything goes as planned all day long and after just a few days the first users start registering the objects on our website.And today, so many weeks later, we still receive new registrations. It was a nice ending of a fascinating and instructive project.Developer-experienceFor some developers on the team, Zero Plastic Rivers was the first experience with AWS and even their first cloud project.In the beginning it was quite intimidating because a lot of different technologies of AWS were used.But soon it turned out to be quite easy to configure and with some help from other colleagues (thanks guys) I got everything up and running pretty quickly.In the beginning I was quite sceptical about the use of lambdas in our application, I didn’t immediately see the advantage of it but in the end it turned out to be the best option, especially if we want to build applications with many more sensors in the future. Although it was sometimes difficult to find the correct documentation.My favorite technology was definitely Cognito. In a few lines of code you have a user administration of an entire application without having to worry about possible security holes.In the end it was a very pleasant experience to get started with AWS.Due to this eye-opening experience several developers are looking forward to becoming AWS certified and gaining a deeper and more complete AWS skillset.ConclusionAll in all we were very pleased with how we delivered this project. As this project was fully staffed with an Ordina High Performance Team, we were able to do everything by the book. We used the best methodologies for software delivery coupled with our preferred technology stack to build a true cloud native application.We embraced the devops mindset: you build it, you run it.Next to that we also embraced the agile mindset: respect, collaboration, improvement and learning cycles, pride in ownership, focus on delivering value, and the ability to adapt to change.We had a great team dynamic: experienced developers coaching and mentoring younger colleagues and helping them grow.Meanwhile the senior developers could work on their coaching and mentoring skills while discussing advanced architectures, also allowing them to grow.Seems like a win-win, right?This scientific project will run for at least two years and we can’t wait to see what kind of insights will be revealed and the impact we will make on our environment and society!We also ended up getting some national press coverage. As you can imagine, this made us very proud!https://www.vrt.be/vrtnws/nl/2020/02/28/opnieuw-fluoplastic-in-schelde/https://www.hln.be/in-de-buurt/antwerpen/wetenschappers-gooien-plastic-in-de-schelde-in-strijd-tegen-plasticvervuiling~a39b64e0/"
      },
    
      "machine-20learning-2020-02-24-chatbots-introduction-and-a-practical-case-html": {
        "title": "Chatbots - Introduction and a practical use case",
        "url": "/machine%20learning/2020/02/24/Chatbots-Introduction-and-a-Practical-Case.html",
        "image": "/img/chatbot.png",
        "date": "24 Feb 2020",
        "category": "post, blog post, blog",
        "content": "  Chatbots, they seem to be everywhere, and yet, there are a lot of people who have no idea what they are.I came home one day in October and told my parents and sisters I was building a chatbot at work.Their reaction: “You’re building a what now?”. So I took ten minutes to explain them what a chatbot is and does.In the end, they just said “So… It is a computer person?”.  This post will feature a high-level explanation of what chatbots are and what they do, as well as a deep dive into chatbot providers and a technical implementation. In the end, we’ll look at a practical case of a chatbot: FleetBot Dina.Table Of Contents  Introduction  Essential terminology  Chatbot providers  Practical case: FleetBot Dina  Technical implementation  Conclusion  ResourcesIntroductionThe concept of chatbots has existed ever since Alan Turing designed the Turing test in 1950, with the original concept of the test dating back to 1936.The question asked in this test was “Can machines think?”. The test proposes a way to measure if the testee can know whether he’s talking to a human or a machine.Joseph Weizenbaum was the first to design a real chatbot, ELIZA, in 1966 at the MIT AI laboratory, which he called a ‘chatterbot’.Even though it failed the Turing test, the main idea behind a chatbot has since then remained the same: recognise what a user says using pre-programmed keywords and phrases, and respond accordingly.Technology has obviously evolved a lot since 1966. Modern chatbots do not only look at keywords and phrases anymore.They use a technology called Natural Language Processing (NLP) and Understanding (NLU) to understand the meaning behind what a user says. NLU uses complex algorithms to analyse the input, not only with predefined keywords but by using various aspects of the language and the sentence structure of the given input.This way, not only can a chatbot understand what a user says, but it can also do a sentiment analysis and extract useful information out of the input.An example, when the user says “Damn, someone drove into my Car Brand earlier today, but it wasn’t my fault. I don’t know what I should do now.”, the chatbot would know that the user was quite angry, he had an accident, and he drives a Car Brand. The bot’s response could be: “I hope you’re okay. Here is a list of garages you can go to. Since it wasn’t your fault, insurance will help you figure this out.”In 2006, the world saw the first big tech company launch a chatbot called Watson.IBM’s Watson went on to winning a game of “Jeopardy!” against all-time champions Ken Jennings and Brad Rutter in 2011.Since then, tech giants like Apple (Siri), Google (Google Now), Amazon (Alexa) and Microsoft (Cortana) have all launched their own personal assistant chatbots with their respective NLPs.In Belgium, Antwerp company Chatlayer provides a custom platform to build chatbots. They use whichever NLP is best in a given language.For Flemish &amp; Dutch, they built a custom NLP.Like all chatbots, these personal assistants have a specified set of use cases, setting alarms, playing music, reading the weather report, telling jokes,… If they don’t know the answer to a question, they will usually just look up whatever you asked on the internet, and give you the best response.Essential terminologyThese are a few terms you should know when studying or working with chatbots.They will be used throughout this blog.  Natural Language Understanding          NLU is the Artificial Intelligence (AI) which tries to understand the meaning behind what a user says.It can do a sentiment analysis, extract relations between words and phrasing, do semantic parsing,…        Natural Language Processing          NLP consists of NLU, along with extra factors like syntactic parsing. The key takeaway here is that NLP translates the user input to machine language and determines what a user has said.        Natural Language Generation (NLG)          NLG comes after the computer has understood the meaning behind the input and formulates an answer. This answer is usually predefined, but can vary depending on the input and the entities the bot has saved so far.        Entities          Entities are data that a chatbot can save during a conversation, e.g. your car brand, your tire type,…        Expressions          An expression is anything that a user sends or says to the bot.        Intents          The intent is the purpose behind a user’s message, e.g. does the user ask for a garage, does he want to know about how to refill his AdBlue,…        Context          A context is used when you expect the user to reply to a bot question.By using contexts, you can direct the conversation into a certain flow, helping both the user and the computer along.      Chatbot providersThere are multiple chatbot providers you can use when building bots. Each bot has its own NLP and User Interface (UI), but to the core, they all expect the developer to provide the same things: expressions, intents and flows to guide a user through the conversation.Chatbots can be implemented via various chat clients: Messenger, Microsoft Teams, Slack, Google Assistant,…Chatbot providers provide you with integrations in different of these clients, and usually enable you to program a custom integration using (REST) APIs.Below, the biggest chatbot providers of today are listed with their properties (at the time of writing). We will not discuss each in detail with how bots are built, since it is equivalent for all, only varying in the UI.There will be links in the Resources section of this blogpost.Google DialogFlow      Easy and free to start up;    Events possible to trigger via API, making it able to move the conversation into a certain flow easily depending on user input;    Parameters can all be asked within same intent;    Intent structure can become very complicated for bigger bots, especially when first getting into an existing project;    Responses can be customised based on the supported chat clients;    Complicated responses with images, buttons, cards,… and jumping from intent to intent requires a lot of coding;    No sentiment analysis;    About 20 integrations supported for free out of the box, both chat and voice, possible to code custom integrations via API;    Multilingual, Dutch relatively ok, Flemish is difficult.  Amazon Alexa/Lex      Easy and (practically) free to start up (you do have to make an account linked to a credit card);    Built to work with Amazon Lambdas, booking appointments etc;    Integration with Amazon Connect, CRM tools,…;    Parameters can all be asked within same intent;    No easy way to build big flows, no context possibilities;    Best for small conversations like booking an appointment;    Integration with Messenger, Slack and Twilio + API;    Single-language – only English.  Microsoft Luis/Power Virtual Agents      Easy and free to start up;    Built to work with Dynamics 365;    Easy off-handing to live chat when bot fails;    Parameters can all be asked within same block;    Easy to build big flows – Tree structure using various blocks depending on use case;    About 15 integrations supported for free out of the box, both chat and voice, possible to code custom integrations via API;    Single-language – only English.  IBM Watson      Easy and free to start up (30 day trial);    Relatively easy to build big flows – Treelike structure using various blocks depending on use case;    Integration with Messenger, Slack, WordPress and Intercom + API;    Multilingual with Watson Language Translator, Dutch relatively ok, Flemish is difficult.  Chatlayer      Easy and free to start up (30 day trial, Chatlayer has to manually approve your request for a trial);    Easy to build big flows – Tree structure using various blocks depending on use case;    Parameters can be asked using ‘Input Validation’, on faulty type of answer. You can reply with a custom message to make sure the user uses the correct format;    Extensive reply capabilities;    Integration with Messenger, Google Home, WhatsApp and Intercom + API;    Multilingual, Custom NLP for Dutch/Flemish, uses best available NLP for other languages. Can automatically detect and understand all languages, regardless of the language you programmed the bot to understand. Can answer only in the languages you programmed.;  Practical case: FleetBot DinaAt Ordina, we have about 800 cars in our Fleet. These, along with everything concerning alternative modes of transport and phone subscriptions, are managed by only two people.As you’re probably thinking right now, this sounds like quite a hefty workload, and you would be quite right.At the time of writing this blogpost, they have to deal with up to 150 emails on a daily basis, which is only expected to rise as Ordina is constantly growing.Since we at JWorks also have questions from time to time, but wanted to lift a bit of their workload, we decided to pitch them the idea of building a chatbot.The chatbot is to be launched to Ordina’s Microsoft Teams.This way, everyone at Ordina could contact it without having to bother with adding it as a separate user or having to create an account on a chat client they don’t use.Since JWorks uses Telegram for a lot of their internal communications, and we like experimenting, we decided to also build a connection to a Telegram bot.This Telegram connection was only used in the testing of the FleetBot and to improve the accuracy of the NLP.We tested all of the chatbot providers listed in the Chatbot providers section, and ended up choosing Chatlayer for its outstanding Flemish/Dutch NLP, because most of Ordina Belgium’s employees are Flemish speaking.The possibility to extend that functionality to Ordina in the Netherlands was an attractive bonus.We also wanted to be able to hand the bot over to the Fleet division when it was finished, so they could keep it up to date without us.We felt that Chatlayer offered the best UI for maintenance by non-developers.Below is a short example of the Fleetbot in MS Teams.  Technical implementationThe general program structure is shown in the image below. We will discuss the MS Teams bot in detail later on. As you can see, both the Teams and Telegram adapter have their own repo &amp; deployment. The reason behind this is that we wanted to be able to provide custom connections for any chat client without having to edit in an adapter used for another one.Using the gateway, we can easily route the response from chatlayer to the adapter it is supposed to go to.The image below shows the production environment of the FleetBot in MS Teams. As you can see, Teams sends an API call to our adapter, which then converts it to a message format Chatlayer accepts.For these API calls, as well as testing, we used NestJS, an extension of NodeJS in TypeScript.For the Telegram adapter, we used Spring Boot and Java instead to experiment with different technologies.To get all useful information out of the Teams message, we use the Microsoft BotFramework. During this conversion, we add a prefix to the id: teams-prod:. This prefix makes it possible for the Gateway to know that the message came from the Teams bot in the production environment and thus send it back to the correct bot.The BotFramework provides methods to get a conversation’s context, and reply to the correct user using that context.Hence, we see the turnContext.sendActivities() method used to reply to a user.We used the NodeJS version of the BotFramework, and used a map to save the conversation references. The reference gets deleted right after the reply has been sent, so we didn’t need to worry about using a database to save the conversation.Since Chatlayer supports many types of replies, all of these had to be converted to the possible Teams messages.Depending on the message type Chatlayer sent, the sendActivities() method has to send a different Activity.As you can see, there are the normal text messages, but also carousels and cards (of which there are multiple types). This variety can deliver very fun and engaging conversations, which can improve your user retention and experience.Of course, the use of all these features depends heavily on the conversation topic and context.You can see an example of all possible messages below.ConclusionAfter working with chatbots during the past 6 months, I’m proud of what we’ve achieved so far. The FleetBot is being actively tested at JWorks and looking to go live for all Ordina Belgium employees.Because of working closely with Chatlayer on the FleetBot project, we have initiated a partnership for future chatbot projects.There already are a number of interesting projects coming up that we’re happy to be part of.Having tested many chatbot providers, we can confidently make an assessment of which one would be best in any use case.What I’m most excited about is the versatility of chatbots. Want one in Telegram or Slack? No problem, we can build that connection! Want one that speaks Chinese, German and English? Can do!Do you think your company could use a chatbot to raise user satisfaction, reduce your FAQ workload, or for anything else it could help out with?Get in touch with Frederick Bousson! We can help you evaluate if a chatbot is indeed the way to go for your specific case and choose the correct provider for your needs.After that, we will assist you in building the bot and build all necessary chat client connections.Resources  Chatbot History  Watson Jeopardy!  NLP Intro  NLP vs NLU  10 Lessons Learned From Building Big Chatbots"
      },
    
      "cloud-2020-02-19-combining-mongodb-and-aws-lambda-html": {
        "title": "Marrying MongoDB Atlas and AWS Lambda",
        "url": "/cloud/2020/02/19/Combining-MongoDB-and-AWS-Lambda.html",
        "image": "/img/2020-02-19-Combining-MongoDB-Atlas-and-AWS-Lambda/featured-image.png",
        "date": "19 Feb 2020",
        "category": "post, blog post, blog",
        "content": "Reading time: 10 min 30 secTable of contents  MongoDB and AWS Lambda: a successful marriage?  Why?  Performance  Cold start vs warm performance  Performance conclusions  VPC peering between AWS and MongoDB Atlas  Useful linksMongoDB and AWS Lambda: a successful marriage?  Can we use MongoDB Atlas when working with AWS Lambda?Yes we can! It’s simple enough to setup and above all also performing well!This blog concerns mainly two things:  Why would you use MongoDB + AWS Lambda and how does it perform  How to create a production grade setup with vpc-peeringWhy?As I am both a fan of AWS Lambda and MongoDB Atlas it was fun for me to marry them.However in the real world we don’t do things for fun alone.  What are the motives to combine MongoDB Atlas and AWS Lambda?  Combine the serverless capabilities of Lambda with the MongoDB strong points  Payment model - In case of MongoDB you provision your cluster and you know what you’ll pay for it, clusters can grow with your business without downtime or code changes.  Flexible Data Acces - MongoDB has a rich query language and aggregation framework. On top of your data in MongoDB you can build nice dashboards for business intelligence. (eg. MongoDB Charts)  Indexes - Supports up to 64 indexes per collection with a wide variety of index types like hash, compound, unique, array, partial, TTL, geospatial, sparse, text and wildcard indexes  Large documents allowed. A MongoDB document can be up to 16 Mb.  Performance - a built-in cache and support for lots of secondary indexes that can span across arrays and subdocuments, making virtually all queries very fast  Tunable consistency1  Observability - MongoDB exposes more than 100 different metrics and has a built-in performance advisor. Because “you can’t optimize what you can’t measure.”  Platform capabilities2  Joinable documents 3PerformanceWe are using AWS Lambda to store items in our MongoDB collection.I want to measure the performance in two ways:  Our Lambda Function has to setup a connection with the MongoDB cluster.Since we are using Lambda we have to deal with a cold start if the lambda has not been invoked shortly before.How is the cold start behavior when connections to the cluster have to be setup?  When the Lambda has been invoked shortly before the connection is cached. How quickly does a save to the database happen when the Lambda Function is warm?Suppose you have setup access from your Lambda Functions to your MongoDB Atlas Cluster (If you want to know how to do just that, read more about it in the second part of this post).Since we are dealing with Lambda Functions we also have to deal with a phenomenon called “cold start”. This is the case for any Lambda Function no matter what database it connects to.MongoDB mentions on there website that there is an initial startup delay due to this.I am testing using the following setup:  Items are coming in via requests through the API.The APILambda drops the items on a queue.A second Lambda Function SaveMongoDBLambda stores the items in the MongoDB database.Of this second Lambda Function I will measure the performance:  during a cold start  when the Lambda is already warmOther things important to know:  In the above setup I limit the maximum number of concurrently running Lambda Functions instances of the SaveMongoDBLambda to 10.  The Lambda Functions are written in Java which will add time to the coldstart performance compared to Python or NodeJS.  In this case I am reading the items from the queue one by one. Also storing them one by one.To optimize we would do this in batch.Here we want to measure performance and we don’t want the batch size to vary.So we store them one by one.  The lambda function runs in a VPC to be able to make a peering connection to the MongoDB Atlas Cluster.  The items that are being stored are only a few tens of bytes large.  In total 1000 items were insertedThe results: analyzing the results using XRAYI am using the AWS Xray SDK to trace and analyze the requests flowing through the application.Here is a graph showing you the Response Distribution of the SaveMongoDBLambda. Hence, this is the time that the Lambda Function took to execute.  We notice two things.  The graph is heavily balanced to the left. Most of the requests took very little time.  On the right end of the spectrum we also see a couple of request. This are the cold starts that occur the first time a Lambda Function is invoked.From the XRAY service map we can see that on average the Lambda Function took 174 milliseconds to execute.  Cold start vs warm performanceWe can further dissect a cold start with XRAY.  We see that:  Bootstrapping the runtime and code in the vpc lambda took 1.7 seconds.  The initial connection is being made to the database which takes 5 seconds. This connection overhead is also there when using AWS native databases.Though then it will be smaller.  Saving the actual item took 1.1 second.That’s for the first execution of the Lambda Function.How does this compare against a Lambda function that is already “warm”.This is how most invocations typically hit your Lambda Function.This means the boostrapping is already done and the connections are initialised.We can also analyse a warm lambda with XRAY.  Now we notice:  Total execution took only 18 ms!  There is no longer any initialization overhead.  Storing the item took 6 ms!Performance conclusions  Storing items in the database when the Lambda Function is already warm is blazingly fast.  Initializing the connection in case of a cold start adds time to the cold start.VPC peering: connect your Lambda Functions with your MongoDB Atlas ClusterWant to know how to set up a VPC peering connection between your AWS VPC and MongoDB Atlas Cluster?Read on..We want to deploy a production grade setup.This means we won’t connect over the open internet.We’ll setup a VPC peering connection between our Atlas Cluster and our AWS VPC.The AWS sideLet’s setup a new VPC.In this VPC we will create a public subnet.A route table will be associated with that subnet.In the route table we’ll define that we want to route all database traffic through the VPC peering connection towards the Atlas cluster.In the AWS User Interface navigate to the VPC dashboard and click Launch VPC Wizard.  Select that you want to create a VPC with a single public subnet.  Specify how big you want the IP range of this VPC to be.If you have trouble figuring out the relation between the CIDR block and the Network Range use one of the online converters to help you. (https://www.ipaddressguide.com/cidr) )Give your VPC and public subnet a name.Make sure that enable DNS hostnames is enabled.  Your VPC has been successfully created.  If you now navigate to the subnet tab you see that a new subnet has been created.When going to the route tables tab you see two new route tables.That is a route table for your VPC and a route table specifically for your public subnet.    Before we go to MongoDB Atlas get some specific data about your VPC:  From the subnet tab write down the VPC-id and IPv4 CIDR for the new subnet that was just created.  Under the security group tab find the security group that is associated with your vpc.Write this security group identifier down.  The MongoDB sideSetup the MongoDB cluster.This has to be a dedicated cluster which means you’ll need at least an M10.  Wait till your cluster is set up.  In the Atlas UI navigate to Security -&gt; Network Access.Hit + new peering connection and select AWS as cloud provider.The below screen will pop up. Here you have to specify some configuration.  Account ID: your AWS account Id which you can find under the ‘My Account’ in the AWS console  VPC-id: Fill in the VPC-id that you copied from the vpc that you just created in AWS  VPC CIDR: specify the CIDR block that you used to configure your vpc with on AWS  region: the region where you created the AWS vpc  Hit Instantiate peering !  Notice that MongoDB created an Atlas CIDR which specifies the IP range in which your Atlas cluster will reside.  Write this down, you will need it later on.We are connecting the Atlas IP range with the IP range of our AWS VPC, hence VPC peering.The peering connection is now pending.Go back to AWS.The AWS side (again)In the VPC service of AWS go to Peering Connections.You will notice a new peering request with status Pending Acceptance.  Accept this peering request!Now you have to update your routing tables.AWS will also ask you Do you want to update your routing tables when you accept the peering request.  Click Modify my route tables now.We will deploy our Lambda Functions in the public subnet of our VPC.So we want to modify the route table that is associated with that subnet.You can recognize that route table because it has an explicit subnet association.  Selecting the route table with an explicit subnet association and click edit routes.Add a route towards your Atlas cluster as indicated in the image below.What we are actually saying here is that we want to route all traffic to our Atlas cluster through the VPC peering connection.As Destination choose the Atlas CIDR and under Target choose your VPC peering connection.  Updating the route table will update the status of the peering connection to available in the Atlas UI.This takes a couple of minutes.  Deploy your lambda functions!Now it is time to deploy your Lambda Function in the VPC that you just configured.This Lambda Function will connect to your MongoDB Atlas Cluster via the vpc-peering we have set up.I created a project that you can use to deploy a Lambda Function in your own vpc.You can then use it to store items in your MongoDB collection.The repository can be found here.Let me be clear and state that in a real world project you don’t want the password hard coded in the connection string.An option is to put it into AWS Secret Manager and have your lambda retrieve it there.You need to update certain config values in this project to make it work for your own vpc!To deploy a Lambda Function in your VPC you have to configure the VPC config:  use the pubic subnet that we just created  specify the security group of you AWS vpcYou also have to update the connection string.The following instructions can also be found in the README of the project.In template.yaml:  update the environment variable that specifies the connection string, database  and collection to your own connection string, database and connection.        Environment:      Variables:        MONGODB_CONNECTION_STRING: mongodb+srv://&lt;user&gt;:&lt;password&gt;@&lt;your-cluster&gt;.mongodb.net/test?retryWrites=true&amp;w=majority        DATABASE: yourDatabaseName        COLLECTION: yourCollectionName        update the VpcConfig with your own vpc security group and subnet:        VpcConfig:      SecurityGroupIds:        - sg-01004aee8e2eb4f33      SubnetIds:        - subnet-028397e077f1f8e7a      Deploy your Lambda functions to your VPC and test them out!Run ./deploy.sh to deploy the Lambda Function to your account.Running this script successfully will output the URL on which you can send an item through the API towards the Lambda Function.  In the Lambda User Interface of the AWS Console you will now see that the Lambda Function has been deployed in the correct subnet with the right security group.  Use this URL that was outputted to trigger the Lambda Function.This will return the ObjectId of the item in your MongoDB collection!  Yihaa! MongoDB and AWS Lambda are happily married!Useful links  https://docs.atlas.mongodb.com/security-vpc-peering/  https://aws.amazon.com/xray/  https://www.mongodb.com/compare/mongodb-dynamodb  https://www.educba.com/mongodb-vs-dynamodb/  https://www.mongodb.com/blog/post/optimizing-aws-lambda-performance-with-mongodb-atlas-and-nodejsFootnotesOperations can be grouped in full ACID-compliant transactions at any scale. In any case, indexes are always kept in sync in realtime with the data so your users will always find and work with the latest, correct data.            Tunable consistency: a variety of consistency levels allowing client applications to select the trade-offs they want to make when it comes to the strongest consistency or the lowest latency. &#8617;              Platform capabilities such as Full Text Search with Lucene, Stitch Serverless Platform with GraphQL support, Charts, managed triggers, more than 30 programming language drivers, Data Lake, analytics, Kafka 2-way connector &#8617;              Joining documents: when rich documents that are loosely coupled (users and invoices for instance) need to be queried, MongoDB can join documents together inside the database, making your code more light. &#8617;      "
      },
    
      "conference-2020-02-12-experience-agile-2019-part-2-html": {
        "title": "eXperience Agile 2019 - Part 2",
        "url": "/conference/2020/02/12/experience-agile-2019-part-2.html",
        "image": "/img/2019-11-18-experience-agile-2019-part-2/ExperienceAgile2019Part2.png",
        "date": "12 Feb 2020",
        "category": "post, blog post, blog",
        "content": "Our first blogpost about the eXperienceAgile conference covered four interesting talks.However, those were not the only talks so this post will dive deeper into some of the other ones.Table of contents  Leadership in a scaling agile environment, by Leonoor Koomen  Building a Customer Value Engine for a more Successful Company, by Mario Moreira  Agile Fluency Project - Why focusing team rock, by Diana LarsenLeadership in a scaling agile environment, by Leonoor Koomen - written by Wouter NivelleAnother great talk from the eXperience Agile conference came from Leonoor Koomen.She talked about how leadership works in an agile environment.Why do companies want to start working in an agile manner?It could be these 3 very important reasons:  It reduces the time to market / volume. A product can be released much faster.  It breaks down silos. People need to work together and share knowledge.  It increases engagement from the people working in the company.But very often, it’s more because agile is in fashion and because of the myth that agile will bring twice as much in half the time.When you read the above, it’s no surprise that agile projects often fail, as shown below.See the third reason, with 38%? Those projects fail because management doesn’t support them.So leadership is very important in an agile environment!Leaders should:  Bring a compelling why! Why do we do this project? What do we hope to achieve?  Why does the company exist?  Bring focus, give clarity on what needs to be done, a certain direction, with measurable goals.  Break silos, make teams work together.  Be accessible, be open for questions, criticism.So leadership is very important and needs to inspire the various teams. Spotify is a big name and is used as an example for many companies. But Leonoor said:  I hate to break it to you, but you are NOT Spotify!While Spotify can be used for ideas, you probably have different dependencies, stakeholders, situational problems and so on.It’s thus very important to come up with a plan to scale.How are you going to grow and handle problems?Transparency is essential, but gets more difficult when you scale.While growing, how are you going to handle the measurement of progress?The most important measurement of progress is… a working product.So stop asking for progress reports! Rather, create an environment for autonomy and alignment.Give the teams autonomy to create the solution, while still providing the vision and alignment. Tell them what to do, but let them figure out the solution.The image below describes it perfectly.The agile transformation is a challenge, but there’s hopeful news.It’s a long journey, but it is possible to achieve.  Every change, no matter how big or small, starts with the same word… YOU!Building a Customer Value Engine for a more Successful Company, by Mario Moreira - written by Astrid LegrandIntroBuilding a Customer Value Engine for a more Successful Company aims to give enterprise leaders tools and advice to maximize customer value with a high level of agility in their companies.Some methods explained during this workshop can also be easily used at a project level like, for instance, the five Rs model that helps to visualize the enterprise idea pipeline.This tool allows to visualize ideas and treat them from the moment the idea is evoked until the moment it is realized.The process consists of five stages, Record – Reveal – Refine – Realize and Release, which corresponds to a break down of the idea into a piece of work.Recording an ideaAt this stage of the process, when a new idea arises, it is documented. No decision has been taken on it so far, we do not know yet if this idea will be useful, if it will be accepted, developed, if it is a priority or if there are sufficient resources to work on it.During this first stage, the idea is simply recorded and documented. In an agile environment, not much time should be invested in this stage and particularly in the documentation because we don’t know if this idea will go further through the process.This stage should then allow to understand the idea, to determine who the users and beneficiaries are and to quickly estimate the costs linked to this idea.Revealing the ideaDuring this stage of the process, the idea is added to the pool of ideas based on its value and priority. This is the moment when it is discussed and challenged among the stakeholders.The idea is refined in order to decide if the team will continue to work on it. Even if the idea is great, it doesn’t mean that it will be developed as constraints and dependencies may exist.It is at this stage of the process that those constraints are highlighted by the stakeholders or product owners. Following the discussions, the idea can be adapted in order to be realized and a team is selected to work on it.Refining the ideaDuring the third stage, the idea is going to be more and more understood by the dedicated team. It is broken down into smaller and more precise increments in order to have a clear and detailed view of the idea.It is challenged and cut into new pieces of increments that are challenged as well and refined in order to create a backlog of clear functionalities the team will work on.Realizing the ideaThis refined idea is then decomposed into user stories.Those user stories are documented and prepared by the team with the purpose to be developed.The user stories are then added to a sprint, developed and tested in a team effort.Releasing the ideaThe idea has now been transformed into a product that will be presented to end users. At this stage, the plan drafted by marketing and sales is executed and the new increment that is now on the market, ready to be used is communicated to the potential customers.Use of the five Rs in my daily workAs an analyst within a Scrum team, my work can be structured around the 5 Rs model as explained below.Recording the ideaLet’s use a concrete example of my work. The ‘UnIt’ application we are building automates as much as possible the management and payments of the indemnities to insured members when they are unable to work.However, this automation has limitations. For instance, not all claim amounts can be calculated automatically in some complex cases. The idea of allowing users to make certain calculations manually has therefore been evoked and added to the pool of ideas. The idea has been discussed and challenged.Revealing the ideaThe idea is now in the pool of ideas, the backlog, according to its value and priority. Discussions over the idea are ongoing and questions are raised.  Should users be allowed to make (all) calculations manually?  What are the risks? Are there limitations to this idea?  Are there legal constraints?  Should the idea be developed in priority, or should it wait in the pool of ideas?Those discussions are conducted among product owners, analysts and developers. A sub-team is then created (with one analyst and developers) and is dedicated to the topic.The needs are refined and the amount of work needed to develop the whole concept of manual calculations is estimated.Refining the ideaThe functionality needed to implement the manual calculations is now better understood by the sub-team responsible for this idea.The functionality is discussed, refined and cut into smaller increments to obtain workable user stories. All user stories together make the manual calculations possible, but can be developed separately.The user stories are described in detail with preconditions, post-conditions, use cases for testing and dependencies, if any. The user stories are also technically discussed at this stage.Realizing the ideaUser stories about the manual calculations have been presented to the entire team during a backlog refinement. They have been estimated with story points and because they were ready according to our Definition of Ready, they have been developed through several sprints.Releasing the ideaIn my example, this stage happens in the demo of the new functionality for the manual calculations at the end of the sprints.The manual calculations can be tested in the acceptance environment by the product owners and the final users.  An enterprise idea pipeline provides transparency of your options and allows you to quickly be aware of and respond to high-value workM. MoreiraAgile Fluency Project - Why focusing teams rock, by Diana Larsen - written by Michaela BroeckxDiana Larsen, or @DianaOfPortland, was an honourable guest at eXperience Agile conference. Browsing her website FutureWorks Consulting, and googling her name, it becomes clear she had an amazing journey leading up to this Agile Fluency Project, throughout which you can move the Agile Fluency Model from idea to implementation, a model she co-created with James Shore.The concept of fluency is omnipresent in most of her talks and articles, and related to that team collaboration has been one of her primary topics throughout her career.What is fluency?Fluency is routine practice mastery that persists under stress. In Lean, we would call this kata.You could say it is the skillful ease that comes from investing in learning.That investment comes down to taking the time and making the effort for deliberate practice. By regularly and consistently practicing a skill with increasing levels of challenge and with the intention of mastering that skill, a state of fluency can be reached. The key is to not give up easily, and with that intent, to keep up your practice until it becomes a second nature… a bit like practicing your cycling skills as a kid, because you long to get rid of those silly training wheels.From team to organisationThe Agile Fluency Model fist focuses on team fluency, a form of fluency that transcends the individual practice, and just like a team is more than the sum of its parts, team fluency also depends on management structures, relationships, and organizational culture, as well as the tools, technologies, and practices the teams use. Team fluency is what you get when highly performant teams become unconsciously competent at collaborating and co-creating. Fluency is the outcome of investment in learning and deliberate practice, and for team fluency this means learning together as a team.There are a few stages before your team gets to fluency. You need to invest in mastering the agile fundamentals to help focus as a team and you need a shift in team skills to be able to build sustainability while delivering value. The Fluency model helps you discover the topics you can explore and practice to improve these aspects and make these shifts.From there onwards, the picture becomes broader and from teams we move to the organisational structure and culture. In the model, Diana and James offer the tools to help you dig deeper into these topics, and make it more clear for organisations what could be worth investing in to optimise organisational Agility, on a systemic level. To embed fluency in the organisations, they point out the need to invest in a cross-organisation focus and generate a shift in organisational culture.What is the aim of this model?There are no recipes for the perfect agile transformation. And so the creators of this model wanted to move beyond the agile methodology ‘wars’ on what is the best way to implement agile. They wanted it to be a positive, inclusive model that promotes improvement, so that it can be used to continuously grow towards more agility, as an individual, as a team, as an organisation. In that sense it is a model that can guide your team to help create alignment with management, and chart your own agile pathway. But it doesn’t have to be a path to perfection. The way James Shore puts it, the idea is that you get off at the right bus stop, the one that fits your needs, and offers the benefit you want for your company. No need to go all the way to the terminal bus stop if that is not required in your story or context. Therefore it is not a maturity model per se, but more of a map for a hop-on-hop-off bus.Discover more about this topic on the Agile Fluency Project website.Or buy a ticket to ride at Ordina. Our colleagues of AgileWorks can help you to get on that bus!"
      },
    
      "architecture-2020-01-28-architecture-in-projects-html": {
        "title": "Architecture Effort in Projects",
        "url": "/architecture/2020/01/28/Architecture-in-Projects.html",
        "image": "/img/2020-01-24-Architecture-in-Projects/architectureeffort.jpg",
        "date": "28 Jan 2020",
        "category": "post, blog post, blog",
        "content": "An architect is someone whose job it is to link various things together in a consistent, integrated, maintainable and sustainable way dixit Tom Graves of Tetradian Consulting. It is the job of the architect to translate the requirements into an architectural model, and to keep the noses of the different stakeholders in the development process pointed in the same direction. He does this for numerous reasons:  Guiding thought in himself  Guiding thought in others  Being able to answer questions asked of the architect  Being able to examine the results of requirement gatheringThe architect doesn’t guide action, he guides thought. Normally, thought precedes action, but in real life this is by no means an absolute truth. Numerous times in software development we might dive in headfirst and see where we get without thinking of how to go about it first. The term cowboy is sometimes colloquially used for this type of software engineer.  Additionally, the role of the architect is to suggest action, as well as oversee that action to ensure that it achieves its goal (quality assurance). So, the architectural process serves to support a reasoning process (guiding thought). The architect repeatedly runs through the reasoning process, either for guidance on his own decisions or for framing into context the decisions he requires from others. The overall picture of the solution needs to be coherent over all components.When thinking about when during a project the architect should play a role, the tried and tested methodology of the Stage Gate Process immediately pops up. This approach divides product development process into five main stages. In between these stages, a number of gates are defined as guardians of the progression to the next stage. They outline the considerations to be taken into account in the decision to move forward to the next stage in the process. These considerations range from quality checklists to budgetary assessment, resources availability, market competence and even compliance with company guidelines and policies. The list can be quite extensive. The architect alongside several other stakeholders is an essential gatekeeper. He enhances the list of quality assurances with considerations from an architectural standpoint and makes sure they are met before moving on to the next phase of the project.The Standard Stage-Gate process (Source: Stage-Gate International)The stages typically used in this approach are the following:  Stage 0 – Discovery/Idea Stage: The organization assesses its opportunities and capabilities in order to determine what is possible and advisable. This can be done through marketing research, innovation management, ideation sessions, blue ocean strategic efforts and other similar activities.  Stage 1 – Scoping Stage: The stakeholders determine the scope of the new product and assess its feasibility and potential.  Stage 2 – Business Case Stage: The stakeholders assess the financial aspects of costs and gains and weigh them against each other. There are numerous business case methodologies to do this, such as for example the Val IT framework.  Stage 3 – Development Stage: Once the product is fleshed out in a positive business case, the development on the new product is done by one or more project teams.  Stage 4 – Test &amp; Validation Stage: Often called Acceptance Testing, the various stakeholders assess the correctness and effectiveness of the newly developed product.  Stage 5 – Launch: The final stage for the product is to be put into production to start earning value for the organization.This approach is very much keyed on the waterfall approach of software development. In order to take this approach to the new insights gathered from iterative development and agile thinking, where there is a need for smaller iterations, greater scalability and accelerated development, the people at Stage-Gate International developed a NexGen Stage Gate Model which allows for reduced stages after the initial Minimum Viable Product (MVP) launch. These smaller iterations are however also guarded by gates between each iteration.The NexGen Stage-Gate process (Source: Stage-Gate International)It might seem from the different illustrations that there is no gate after the Launch Stage. This is not the case. After going live there are several checks that are typically built in during the Go Live deployment as well as during a grace period after Go Live. There are no more stages to come in this model, but that doesn’t mean there is no more work to be done. A retrospective on the past project will benefit greatly from a gate checklist performed at this time, and down the line when doing a business case verification to see whether its initial assumptions holds up might also detect indicators of success/failure in this list that can be taken up the next time a business case in this context is written up. Should the need to upgrade or decommission the new solution in the future arise, this checklist could also highlight particularities otherwise forgotten that have could have an impact on these actions.Although the architect should and can play a role in every stage of the project, he tends to regard the project in a slightly different set of stages. It starts in the Plan phase where it aligns with the global analyses. It goes along with the entire design portion of the Build phase and extends further till the architecture is constructed, documented, validated and accepted. The effort ends when the solution enters the last step in its lifecycle and enters the Dispose phase. Its main activities do however change depending on which phase of the project is currently happening.Architectural Phases of a ProjectThe first activities of the architecture effort focus mainly on gathering all relevant requirements that might influence the design. These requirements will also form the basis for the acceptance criteria stipulated by the gates between the phases. They can be divided into functional and non-functional requirements, where the latter can be divided into technical (such as integration, quality, and infrastructure requirements) and operational requirements (such as documentation, training, and managed services requirements).The solution architect works together with domain specialists, both business and technical, to guide and constrain the business and technical analyses from a technical perspective and should assist the analysts by informing them of technical information and possibilities. Through this exercise, business knowledge is acquired and high-level business and technical requirements are produced. The requirements should be listed as SMART statements: Specific, Measurable, Achievable, Realistic, and Timely. These requirements do not only structure the technical effort, but other disciplines such as business architecture and testing as well.The first version of a Solution Architecture (and its corresponding document) should be drafted as early as the Plan phase, when a first set of requirements becomes known, and an attempt at setting the scope ensues. This is sometimes called the Solution Architecture Blueprint. Next, throughout the project lifecycle the architecture version matures with the architect gaining more insight and detail of the to-be situation of the solution. As with most deliverables of a project, the solution architecture document will mature well into the Operate phase and even a bit of the Dispose phase with activities to keep the documentation up to date with reality.The Solution Architecture will be reassessed several times during the architecture effort. It should be considered a living document. Each time new requirements are detected, new insights are gathered, or new constraints are introduced, the architecture needs to go through a cycle of validation of the new requirements/constraints, which feeds into a new version of the architecture design, followed by an architecture presentation and review event. For instance, these new requirements and insights can be derived from proofs of concept (POC), which have been executed following an earlier version of the architecture and have exposed gaps in the solution.The following activities will be undertaken by the architect to achieve a steadily maturing architecture document:  Based on the previous architecture version, the detailed analysis will be executed by the analysts and/or more specialized domain architects. The solution architect has further the responsibility to streamline the correlation between this detailed analysis and his architecture. The solution architect provides technical information to the analysts; influences and aids the analysts’ decision making.  The solution architect gathers further information concerning the business and technical requirements. The solution architect thinks together with the business analysts and technical analysts in order to make the architectural decision, which can have immediate consequence on the analyses.  The solution architect is responsible for the validation and acceptation of the final requirements documents. The acceptation of the requirements means the solution architect agrees that the requirements documents are relevant, correct, complete and unambiguous not only for architectural decisions, but also later for design and construction phases.  The architect is also responsible for the follow-up of any Proof of Concepts that are to be performed as validation for the decided architecture. Based on the results of these POCs, an adapted version of the Architecture Document might be written out containing the conclusions of the POC.  The solution architect organizes the presentation and the review session for the proposed architecture. The presentation and review session can be omitted upon the agreement and decision from a technical project manager, sparring architect and project manager.These revisitations of the solution architecture should however be limited as much as possible in order to avoid ‘scope creep’ and unhealthy amounts of rework. And if they are not avoidable, efforts should be made to detect these changes as soon as possible in order to limit the impact these changes will have on the existing solution.In summary, an architect will have a varying workload during all phases of the project. At the beginning of the project, the architect works together with the business and technical analysts to coordinate and guide the requirements gathering and analyses, resulting in a first mature version of the architecture. Further on, the technical analysis will be based on this version of the architecture document and will consolidate all the requirements in detail under the architect’s vigil. The architect oversees the detailed technical designs and organizes any POCs that are to be performed. Recurring actualization efforts are coupled with quality assurance of the implemented designs."
      },
    
      "architecture-2020-01-23-nx-html": {
        "title": "One repo to rule them all",
        "url": "/architecture/2020/01/23/Nx.html",
        "image": "/img/2019-12-02-Nx/nx-logo.png",
        "date": "23 Jan 2020",
        "category": "post, blog post, blog",
        "content": "  Intro  Monorepo  Nx  Full stack applications  ConclusionIntroImagine yourself working in a large organization with multiple teams, all working on an application that is part of the platform that the organization offers to its clients.You get a set of requirements and start implementing your features.After setting up your repository and all the tooling, you’re ready to go.All goes well during the development process and you’ve come to the end to deliver your hard work.After you’ve deployed your application on the platform, you’re introduced with a blank screen and a console log full of errors.How can this happen?Your team followed the requirements and all the contracts that were defined to communicate with several services.Maybe these contracts got outdated in the meantime without you knowing it?Were there breaking changes introduced?Or does a library you are using locally differ from the ones used in production and isn’t compatible anymore?It’s really difficult for a large organization to manage all the separate projects and to maintain the overview of all of them.The complexity will increase and after a while the whole platform becomes unmanageable.MonorepoThe problems mentioned above led large software companies to transition the management of their separate projects by bringing them together under one repository: the monorepo.Inside the monorepo, you have all the applications working together while you’re developing your app.You’ll get feedback immediately when the service you’re calling returns a different answer than expected or when the page you are navigating to is located on another path.When you’re not sure how a service of the platform is working, you can just hop in the source code inside the repo to find out more about it.Code sharing becomes a breeze, because all the code is available in one repository.You don’t have to deal anymore with npm packages for shared code in your organization or with the separate configurations, pipelines and versions of the shared libs.Because there is only one package.json file, you can easily manage the versions and don’t have to deal with conflicting versions of the same dependency inside of your project.But how can you start setting up a monorepo and what tools are required to do so?Having to manage all this code in one repo can be really tedious to set up but this is where Nx comes into play!NxNx provides you with the tools needed to help you develop in a monorepo.Based on their own experiences, the creators of Nx set up the best practices to structure your repository in a maintainable way.Nx is built on top of the Angular CLI, offering you a way to create workspaces and scaffolding out of your apps.Next to Angular apps, Nx does support creating React apps, NestJS apps, and more.You can even write your own builder to configure creating an app with the technology of your choice.The setup of the repo has to happen only once, so you don’t have the hassle of doing it yourself each time when starting a new app.Moreover, you won’t have the risk of teams using a whole set of different tools.This makes it more convenient to switch projects as well, because the experience over the monorepo will be more consistent.The versions of dependencies will be consistent over the whole repository because you only have one package.json file, so you don’t have to deal anymore with version conflicts of libraries that are being used in different apps.Starting with a new workspace is pretty straightforward.Let’s imagine we want to start a webshop where we will sell Nx-phones:npx create-nx-workspace@latest nx-phoneNx will scaffold our workspace.The root of the project contains apps where our applications will reside, libs for code sharing between our apps and tools that enables us to write our own schematics.You will write most of your code inside the apps and libs directories.AppsNow that our workspace is set up, we can add apps to it.This can be done by different ways.We will focus now on using the Angular CLI.To add an Angular app, we first have to include the capability to create Angular apps with Nx.Afterwards, we can create our first app called “shop”.ng add @nrwl/angular --defaults // Add Angular capabilityng g @nrwl/angular:application shop // Add the shop appYou can choose the stylesheet format and automatically set up routing while doing so.When creating a new app inside your workspace, Nx will generate both the shop and a shop-e2e folder for your end-to-end tests. Running the following command using the Nx CLI will start up the shop app locally:nx serve shopLibsThe libs folder is where your shared code will reside.If your organisation has a component library or a design system, then that would be a good candidate to be placed under libs.You can also add all your code to the libs in separate modules like feature modules while using the app as a container.Later on, you can use the features in the modules to compose your app.If you want to take a look at the full example, please check out the nx-phone workspace here:https://github.com/DimiDeKerf/nx-phoneTestingNx supports modern tools that you’re familiar with, without the sometimes tedious way of setting them up.You can easily use Jest and Cypress to cover your testing needs, with great CLI support.The unit and end-to-end tests are both configured for the shop app that we’ve just created. They can be run by using nx test shop and nx e2e shop, respectively.If you want to learn more about Jest or Cypress, be sure to check out the awesome blog posts that my colleagues have wrote:  Testing Angular with Jest  Cypress: a new kid on the E2E blockCode formattingHow do you like to have your code formatted?There will be some differences between how others within your organization like to format their code.Just thinking about the merge conflicts that will arise when people use different indentations may already give you shivers.Prettier comes included with Nx and is here to help you out with this.It’s an opinionated code formatter that integrates well with most editors.You’re tired of formatting your code before you want to commit something?Prettier can also run in a pre-commit hook, so your code will get formatted before your commit.More info can be found here.Dependency graphWhat if you change something in your code?How can you be sure that your modification didn’t break another app?This is hard to do when your apps live in separate repositories but becomes much more convenient with the help of Nx.All your apps and libraries will be part of a dependency graph.The dependency graph will give you an up-to-date version of all the applications and services, and the dependencies between them.It can also help you to get a better understanding of the architecture.BuildingWhen new code is introduced or something has been refactored, Nx will be able to find out which dependencies are affected by your changes.This gives you the opportunity to only build the affected dependencies and only run the tests of those affected ones.For your build pipeline, that means that testing the affected apps can greatly improve the build time when dealing with large workspaces.For example, if we edit something in the shop app, our dependency graph will look like this:Targeting the affected code can be done using the affected command.For example, if you want to run the unit tests for the affected code:nx affected:testInside of your CI, you would like to compare the changes between your branch and master.This can be done by appending the two branches to the previous command:nx affected:test --base=origin/master --head=your-branchRestrictionsYou may want to categorize libraries in different domains and limit the dependencies between those domains to improve maintainability.Nx can help you with this, by restricting access between domains or by allowing the dependency flow to go only one way around.To configure and manage these restrictions, you can apply tags in the nx.json file for apps and libraries.Be aware to also update the linting rules in tslint.json to get feedback in your editor when violating those rules.Full stack applicationsWith a monorepo, you can have both your frontend and your backend applications under the same repository.This enables you to share code between them.With Nx, you can do this by using libraries like mentioned before.These libraries will expose the code using a public API.The code inside these libraries can be imported afterwards in your apps, without having to fetch them from a npm registry.Think about how easily you can share an interface between your frontend and backend, without having to break it on one side.Whenever something changes in the API, both sides will get updated so they will both remain in sync.ConclusionHaving worked within a monorepo myself for the last months, I really appreciate the way it improved the daily workflow.It encourages me to write more reusable code and to keep features small.Other teams may have the same requirements, so code can be easily migrated to libs.It is important to manage the different features and libs though, before they turn up to be a massive dump of code that isn’t maintainable anymore.Switching between features is much easier and accessible, since I don’t have to check out another repository or getting familiar with the way of working of another team.Overall, I feel more connected with other developers and with the platform while building apps in the same repository.If you’re interested in setting up a monorepo, be sure to check out Nx!It will help you tremendously with getting started."
      },
    
      "java-2020-01-02-spring-data-jdbc-html": {
        "title": "An Introduction to Spring Data JDBC",
        "url": "/java/2020/01/02/Spring-Data-Jdbc.html",
        "image": "/img/spring.png",
        "date": "02 Jan 2020",
        "category": "post, blog post, blog",
        "content": "  A new member is added to the Spring Data family. Spring Data JDBC is positioned between Spring Data JPA and Spring JDBC using the best elements of both.This post will describe the current state and the future of this product. It will also explain which problems this product is trying to solve and how.Table of Contents  Introduction  Spring JDBC vs Spring Data JDBC vs Spring Data JPA          Project Differences      Inserting Data      Querying The Database      Updating an Instance        Advantages of Using Spring Data JDBC          Better Design      Easier to Understand      Performance        Should I Use ItIntroductionSpring Data JDBC is a new member of the Spring Data family. It is created to fill a void that sits between Spring JDBC and Spring Data JPA.If you look at Spring JDBC, you could argue that it is too low level to work because it only helps with the connection to the database.Spring Data JPA could seem too complex because it gives you a lot of options and it can be difficult to master all these options. Spring Data JDBC is a framework that tries to give you the same power you get from using Spring Data JPA but makes it more understandable by using DDD principles.It also gives you more control by working on a lower level and by letting you decide when database interactions need to be done like Spring JDBC, but in an easier way. In the rest of this article the differences between Spring Data JPA, Spring Data JDBC and Spring JDBC will be shown.This will hopefully show you that Spring Data JDBC is a very nice product with a lot of potential that is designed to help you.Spring JDBC vs Spring Data JDBC vs Spring Data JPAProject DifferencesSpring JDBC, Spring Data JPA and Spring Data JDBC are all three based on a different mindset. Based on these mindsets, you can see differences in how these technologies need to be used and how some parts need to be structured.Spring JDBC only helps with the connection to the database and with executing queries on this database.Spring Data JPA wants to help you manage your JPA based repositories. It wants to remove boilerplate code, make implementation speed higher and provide help for the performance of your application.Spring Data JDBC also wants to provide help with access to relational databases, but wants to keep the implementations less complex.Spring JDBCThe help that Spring JDBC provides is by providing a framework to execute SQL. Spring JDBC handles the connection with the database and lets you execute queries using JdbcTemplates.This solution is very flexible because you have complete control over the executed queries.You are also free to define your class structure because you are in complete control of the mapping.Spring Data JPASpring Data JPA uses entities, so the class structure needs to be comparable with the database structure because some mapping is done automatically.In the most simple form, the database tables will each represent an entity and can be mapped almost directly on an entity class.This mapping can be done by using Java configuration. By using annotations you can define on which table the class is mapped but also how the tables are linked together.A class is seen as an Entity when it is annotated with the @Entity annotation.The most common links are @OneToMany, @ManyToOne and @ManyToMany.For example if class A has a @OneToMany relationship to class B then the database scheme will have a foreign key in table B to table A and the JPA implementation will keep a list of linked B elements in entity A.There are almost no restrictions on how entities can be linked together.This means there is total freedom to design your class structure how you want, but it also means that you need to be wary that you don’t cause problems like circular dependencies.It also means that nothing prevents you from creating a complex structure which is not easily understandable, especially understanding how your class structure translates to your database structure.So you will always need to think how your class structure decisions impact your database.For example it is easy to define a data structure with a many-to-many relationship or with bi-directional relationships. When you translate this to your database structure you will need intermediate tables and create references to the unidirectional relationships.Example:In this example there are some classes that can be used in Spring Data JPA. They represent a part of the domain model that will also be used in the rest of this article.A RentalCompany contains rentals. These rentals hold the information about which Car is rented and when.    @Entity    public class Rental {             @Id        @GeneratedValue(strategy = GenerationType.AUTO)        private Long id;             @ManyToOne(fetch = FetchType.LAZY)        @JoinColumn(name = \"company_id\")        private RentalCompany company;             // ...         }        @Entity    public class RentalCompany {             @Id        @GeneratedValue(strategy = GenerationType.AUTO)        private Long id;             @OneToMany(fetch = FetchType.LAZY, mappedBy = \"company\")        private List&lt;Rental&gt; rentals;                 // ...    }Spring Data JDBCWhen you use Spring Data JDBC, you will also need to create entity classes which will be mapped to the database.The big difference is that there are more rules that you need to follow when you create the class structure.The class structure needs to follow the rules of aggregate design of DDD.Spring Data enforces this because this will lead to the creation of more simple and understandable projects.If you don’t know this concept or why it is useful, you can check a very good article about this: Effective Aggregate Design by Vaugn Vernon.Basically we group different entities together which have a strong coupling and we call them aggregates.The top entity of the aggregate is called the aggregate root.There are some other rules that need to be followed:  An entity can only be part of 1 aggregate.  All relations inside an aggregate need to be unidirectional.  The aggregate root needs to manage the top relation.This means that by following links starting from the aggregate root, every entity inside the aggregate can be found.Because of this, we do not need a repository for each entity like in Spring Data JPA, but only for the aggregate roots.To link entity classes together to form an aggregate, you need to use object references.Entity classes inside an aggregate can only have one-to-one relationships and one-to-many relationships. If you have a one-to-one relationship your entity only needs an object reference to the other object. When you have a one-to-many relationship your entity needs to contain a collection of object references.To create relations to entities outside the aggregate, id’s need to be used to get a low coupling between these classes.A big difference in creating the classes used by Spring Data JDBC versus Spring Data JPA is that no @Entity and no relation annotations like @OneToMany need to be used.Spring Data JDBC knows a class is an aggregate root when it contains a repository for that class. And because of the rules that the aggregate entities are connected through object references, Spring Data JDBC also knows what the aggregates are and can transfer data to the database as aggregates.Example:In the following example you can see how the domain model that was introduced in the example of Spring Data JPA can be implemented in Spring Data JDBC.In this implementation there are 2 aggregate roots RentalCompany and Car. Rental is part of the aggregate of RentalCompany. This domain model will also be used in the other examples inside this article to compare the differences between the three frameworks.    public class RentalCompany {            @Id        private Long id;            private String name;        private Set&lt;Rental&gt; rentals;        }        public class Rental {            @Id        private Long id;                private String renter;        private Long carId;        private LocalDate startDate;        private LocalDate endDate;    }        public class Car {            @Id        private Long id;                private String color;        private String brand;        private String model;        private String licensePlate;        private CarType type;    }    Inserting DataFor the insertion of data you need to use the tools that were created in the previous section. If you use Spring JDBC you will write queries that are executed directly on the database by JdbcTemplate.If you insert data with Spring Data JPA or Spring Data JDBC, you can use the entity or aggregate system that was created.Spring JDBCWith Spring JDBC you write your insert statements yourself and execute them with a JdbcTemplate.The advantage of writing all the queries yourself is that you have complete control over them.Example:    SingleConnectionDataSource dataSource = new SingleConnectionDataSource();    dataSource.setDriverClassName(\"org.hsqldb.jdbcDriver\");    dataSource.setUrl(\"jdbc:hsqldb:data/jdbcexample\");    dataSource.setUsername(\"sa\");    dataSource.setPassword(\"\");            JdbcTemplate template = new JdbcTemplate(ds);    template.execute(\"create table car (id int, model varchar)\");    template.execute(\"insert into car (id, model) values (1, 'Volkswagen Beetle')\");        dataSource.destroy();    Spring Data JPAIf you use Spring Data JPA for inserting data, you will need to use the repositories and the entities.This makes it possible to think on a higher level and let Spring Data JPA handle the creation of queries.When you want to create data for an entity, the only thing you need to do is create an object with the correct values and call the save method on your Spring Data repository.Spring Data JPA will then look at your entities with all their annotations to map them to the necessary insert or update statements.ExampleThe example below shows how to insert Rental data in the database with Spring Data JPA.    @Service    public class RentalService{                private RentalRepository rentalRepository;                public RentalService(RentalRepository rentalRepository){            this.rentalRepository = rentalRepository;        }                public Rental create(Rental rental){            return rentalRepository.save(rental);        }    }    Spring Data JDBCSpring Data JDBC uses a syntax that is comparable to Spring Data JPA.The biggest differences are under the hood.The management of the persistence is handled by the repository like in Spring Data JPA, but only the aggregate root has a repository.This means that if you want to insert or update data, the entire aggregate needs to be saved.You will need to call the save method of the repository of the aggregate root and this will first save the aggregate root and then all of the referenced entities get saved.If you want to insert only a part of an aggregate, for example only create a new Rental, then the whole aggregate will be updated and the referenced entities will be deleted and inserted again.ExampleThe example shows how a Rental is added. If you want to create a new instance of the aggregate root, then the code is comparable to that of Spring Data JPA.    @Service    public class RentalCompanyService{                private RentalCompanyRepository rentalCompanyRepository;                public RentalCompanyService(RentalCompanyRepository rentalCompanyRepository){            this.rentalCompanyRepository = rentalCompanyRepository;        }                public RentalCompany addRental(Rental rental, Long rentalCompanyId){            RentalCompany rentalCompany = rentalRepository.findById(rentalCompanyId);            rentalCompany.getRentals().add(rental);            return rentalRepository.save(rentalCompany);        }    }Querying The DatabaseTo retrieve data from our database, we write queries.Spring JDBC will let you use the JdbcTemplate and let you map the result with a RowMapper.Spring Data JDBC and Spring Data JPA will also let you create queries, using JPQL or SQL queries, but you will write them in the repositories and the frameworks will help you with the mapping.Spring JDBCThe main tool that Spring JDBC uses for querying is the JdbcTemplate.The downside of using this is that it only provides the connection, everything else you need to do yourself.If you search for objects, you will need to map the results to Java objects by implementing a RowMapper.You will also need to do the exception handling by creating a ExceptionTranslator.I will show you a simple example of how a query is created.ExamplesThis response needs to be mapped by implementing a RowMapper.    public class CarRowMapper implements RowMapper&lt;Car&gt; {            @Override        public Car mapRow(ResultSet resultSet, int rowNumber) throws SQLException {            Car car = new Car();                 car.setId(resultSet.getInt(\"ID\"));            car.setColor(resultSet.getString(\"COLOR\"));            car.setBrand(resultSet.getString(\"BRAND\"));            car.setModel(resultSet.getString(\"MODEL\"));                 return car;        }    }    This mapper can be passed to the JdbcTemplate that will use it to create populated Java objects.    List&lt;Car&gt; cars = jdbcTemplate.queryForObject(        \"SELECT * FROM CAR WHERE ID = ?\", new Object[] {id}, new CarRowMapper());    Spring Data JPAWhen you use the Spring Data framework, it will help you with building your queries and fetching the right data.The Spring Data JPA framework uses implementations of the JPA specifications like Hibernate. They make it possible to query the database using user friendly interfaces.When you want to query the database, instead of writing the entire query yourself, Hibernate will help you. There are multiple ways to query the database using Spring Data JPA, but they all need you to extend the repository of the entity you want to query.Some basic queries can be written using derived queries. An example of this is findById. For these methods Spring Data will generate the SQL entirely on its own. More information about how derived queries can be defined can be found in the Spring Data documentation.If you need to write more advanced queries that can’t easily be defined as a derived query, you can define the query yourself using the @Query annotation.Inside the @Query annotation you write JPQL or SQL statements. JPQL is an SQL-like syntax that provides an abstraction layer on top of regular SQL. When JPQL is used, it is possible for Spring Data to help you with handling the data. For example paging and sorting can be done by simply adding a parameter.If you want to have a bit more control, you can use SQL by setting the native property of the @Query annotation to true. Then you don’t use this extra layer, but it then also can’t help you anymore.Be aware that even though you use SQL directly, you will still return entities that are managed by Hibernate.Apart from helping you with more easily defining which query you want to execute, it also helps you with fine tuning performance.You are using entities when you query using Spring Data JPA. These entities have connections to other entities.Spring Data JPA will help you with defining whether you want to return these connected entities directly or not. It can help you with searching for these entities when you do need them.This is called eager and lazy loading and this can all be managed by Spring Data JPA.It will also try to improve performance by giving you the option to turn on the query cache.When this is turned on, Spring Data JPA will try to reuse the generated SQL queries, and if possible the results of these queries.ExamplesThe service that calls a repository to get all rentals with a given CarType.    @Service    public class RentalService {            private RentalRepository rentalRepository;            public RentalService(RentalRepository rentalRepository){            this.rentalRepository = rentalRepository;        }            public List&lt;Rental&gt; getRentalsByCarType(Long rentalCompanyId, CarType carType) {            return rentalRepository.findByCompanyIdAndCarType(rentalCompanyId, carType);        }    }    The repository uses a derived query to expose this functionality.    public interface RentalRepository extends PagingAndSortingRepository&lt;Rental, Long&gt; {            List&lt;Rental&gt; findByCompanyIdAndCarType(Long rentalCompanyId, CarType carType);    }    Spring Data JDBCSpring Data JDBC has less abstractions than Spring Data JPA, but uses Spring Data concepts to make it easier to do CRUD operations than Spring JDBC.It sits closer to the database because it does not contain the most part of the Spring Data magic when querying the database.Every query is executed directly on the JDBC and there is no lazy loading or caching, so when you query the database, it will return the entire aggregate.Currently when you want to add a method to your repository in Spring Data JDBC, you need to add an @Query annotation which contains the query.This needs to be pure SQL instead of JPQL which is used in Spring Data JPA. You can compare this best with the native queries that are added to Spring Data JPA.Because Spring Data JDBC sits so close to the database, it is, like native queries in Spring Data JPA, not possible to have paging and sorting repositories.These types of repositories will be added in the future. This is like the derived queries functionality which is also not yet implemented but will be in the future.When you query the application using Spring Data JDBC, instead of entities, you will receive the entire aggregate. This makes the application easier to understand. The application doesn’t need to rely on an application context to get the state of properties of returned entities.Because the entire objects are fetched, there are no extra calls needed to receive the field values of objects that were not loaded yet because all the fields are already filled in.The disadvantage of this system could be that too much data will be loaded. But if that happens, it could be that the boundary of your aggregate is too big and it is possible that you need to split up your aggregate.ExamplesWhen we try to do the same thing as we did in Spring Data JPA in Spring Data JDBC, then you need to go at it differently.The first difference is that we can’t use the RentalRepository because it does not exist. It does not exist because Rental is not an aggregate root, but is part of the aggregate with aggregate root RentalCompany. This is why we now need to use the RentalCompanyRepository.    @Service    public class RentalCompanyService {            public RentalCompanyRepository rentalCompanyRepository;            public RentalCompanyService(RentalCompanyRepository rentalCompanyRepository){            this.rentalCompanyRepository = rentalCompanyRepository;        }            public List&lt;Rental&gt; getRentalsByCarType(Long rentalCompanyId, CarType carType) {            return rentalRepository.findByIdAndCarType(rentalCompanyId, carType);        }    }    The other difference is that we cannot use derived queries, so we need to use the @Query annotation.    public interface RentalCompanyRepository extends CrudRepository&lt;RentalCompany, Long&gt; {            @Query(value = \"SELECT * \" +                \"FROM Rental rental \" +                \"JOIN Car car ON car.id = rental.car_id \" +                \"WHERE rental.rental_company = :companyId \" +                \"AND car.type = :carType\")        List&lt;Rental&gt; findRentalsByIdAndCarType(@Param(\"companyId\") Long companyId, @Param(\"carType\")String carType);    }    Updating an InstanceWhen you want to update an instance in the database, you will need to write a query and execute it using the JdbcTemplate or use the domain model in Spring Data JPA or Spring Data JDBC.Spring JDBCSpring JDBC again only provides a framework when updating data from the database. The JdbcTemplate exposes an update method. This method can accept a query and optional parameters.ExampleA simple code example where we update the color of a given car.    String query = \"update Car set color = ? where id = ?\";    jdbcTemplateObject.update(query, color, id);Spring Data JPASpring Data JPA provides more tools to update the data, like the proxy around the entities.The state of these entities is stored in a persistence context.By using this, Spring Data JPA is able to keep track of the changes to these entities.It uses the information of these changes to keep the database up to date. Spring Data JPA makes managed entities from these entities.Instead of always needing to create queries to update data in the database, we can edit these entities. These changes will then always be persisted automatically. This tracking is called dirty tracking because when you change the entities, these updates are making the entity “dirty” since the state is different than in the database.When the Hibernate session is flushed, these changes will be persisted and the entity will be “clean” again.This will only be done for changes within a transaction. If the changes are not done within a transactional context, you will have to call the save method of the repository to persist those changes.If you want to make bigger changes, it is also possible to create update methods in the repositories. Like querying the database and creating entities, you can also create methods in the repository. Using JPQL you can create a query which can update multiple entities at once. Please make sure to add the @Modifying annotation. This is a security measure so you cannot modify something by mistake.ExampleIf you want to change the color on all hatchback cars, you can do this in different ways. I will give three.If you do it in a transaction, the dirty tracking will take care of the changes.       @Service       public class CarService {                  public CarRepository carRepository;                  public CarService(CarRepository carRepository){               this.carRepository = carRepository;           }                  @Transactional           public List&lt;Car&gt; updateColorOfCarsWithCarType(CarType carType, String color) {               List&lt;Car&gt; carsWithType = carRepository.findByCarType(rentalCompanyId, carType);               for(Car currentCar : carsWithType){                    currentCar.setColor(color);               }               return carsWithType;           }       }If you do it without a transaction, you also need to add an implementation to instruct Hibernate to persist the changes.       @Service       public class CarService {                  public CarRepository carRepository;                  public CarService(CarRepository carRepository){               this.carRepository = carRepository;           }                  public List&lt;Car&gt; updateColorOfCarsWithCarType(CarType carType, String color) {               List&lt;Car&gt; carsWithType = carRepository.findByCarType(rentalCompanyId, carType);               for(Car currentCar : carsWithType){                    currentCar.setColor(color);                    carRepository.save(currentCar);               }               return carsWithType;           }       }A third way you can do this is by queries.       @Service       public class CarService {                  public CarRepository carRepository;                  public CarService(CarRepository carRepository){               this.carRepository = carRepository;           }                  @Transactional           public List&lt;Car&gt; updateColorOfCarsWithCarType(CarType carType, String color) {               return carRepository.updateColorOfCarsWithCarType(rentalCompanyId, carType);           }       }              public interface CarRepository extends CrudRepository&lt;Rental, Long&gt; {                  @Modifying           @Query(value = \"Update Car car \" +                   \"SET car.color = :color \"                   \"WHERE car.type = :carType \")           List&lt;Car&gt; updateColorOfCarsWithCarType(@Param(\"color\") String color, @Param(\"carType\")String carType);       }Spring Data JDBCSpring Data JDBC does not have a persistence context like Spring Data JPA.This makes Spring Data JDBC in my opinion more straightforward than Spring Data JPA.If you want to make changes to the data, you are responsible for handling the persistence.If you do not call the save method in the repository, the changes will not be persisted.You also have the choice to update the aggregates using self-written queries. Like I already mentioned, these queries are executed directly on the database, without the use of an abstraction like Hibernate.Updating the data is done by calling the save method on the repository of the aggregate.Because Spring Data JDBC does not contain a persistence context like Spring Data JPA, it does not know which part of the aggregate is updated. Therefore it will update the aggregate root and delete all the referenced entities and save them all again.As a downside, entities will sometimes be deleted and inserted even if they were not updated, which could be a waste of resources.The big advantage is that you are sure that the entire entity will be up to date after saving the aggregate.ExamplesSuppose we want to change the colors of all colors of a same type like we did in the example of Spring Data JPA.You can do it like example 2 of Spring Data JPA.       @Service       public class CarService {                  public CarRepository carRepository;                  public CarService(CarRepository carRepository){               this.carRepository = carRepository;           }                  public List&lt;Car&gt; updateColorOfCarsWithCarType(CarType carType, String color) {               List&lt;Car&gt; carsWithType = carRepository.findByCarType(rentalCompanyId, carType);               for(Car currentCar : carsWithType){                    currentCar.setColor(color);                    carRepository.save(currentCar);               }               return carsWithType;           }       }              public interface CarRepository extends CrudRepository&lt;Rental, Long&gt; {                        @Query(value = \"SELECT * \"                  \"FROM Car car \" +                  \"WHERE car.type = :carType \")          List&lt;Car&gt; findByCarType(@Param(\"color\") String color, @Param(\"carType\")String carType);      }Another way you can do this is by an update query which is the same as the Spring Data JPA example. If we want to update a part of an aggregate, we would need to go through the aggregate root since the aggregate root maintains consistency for the whole aggregate.       @Service       public class CarService {                  public CarRepository carRepository;                  public CarService(CarRepository carRepository){               this.carRepository = carRepository;           }                  @Transactional           public List&lt;Car&gt; updateColorOfCarsWithCarType(CarType carType, String color) {               return carRepository.updateColorOfCarsWithCarType(rentalCompanyId, carType);           }       }              public interface CarRepository extends CrudRepository&lt;Rental, Long&gt; {                  @Query(value = \"UPDATE Car car \" +                   \"SET car.color = :color                   \"WHERE car.type = :carType \")           List&lt;Rental&gt; updateColorOfCarsWithCarType(@Param(\"color\") String color, @Param(\"carType\")String carType);       }Advantages of Using Spring Data JDBCBetter designOne of the biggest advantages that you can get from using Spring Data JDBC is that it will force you to follow the rules of DDD design like using aggregates.A great explanation why this design can help you can be found here.Because only one-to-many relationships are used, it makes it easier to see what the exact relationships are between the classes.Classes can only be part of 1 aggregate. Together with the one-to-many rule this makes it impossible to create circular dependencies.If you need to create relationships between aggregates, you need to use id’s. This makes the coupling between the aggregates as small as possible.It is also clear where the logic of the interactions with the data can be found because only the aggregate roots have repositories because they are responsible for these interactions.Easier to UnderstandOnly the aggregate roots are responsible for handling the persistence. This makes it clear what needs to be persisted and who is responsible for doing this.Because the persistence always needs to be initiated by calling the save method, it makes it easier to understand when changes will be persisted than with Spring Data JPA.It is easy to see what classes are part of an aggregate since aggregates are connected using object references. When id’s are used, those classes are part of different aggregates.When you query the database, instead of the lazy loading which is standard in Spring Data JPA, every call using Spring Data JDBC is done using eager loading. Every time you need data, a call to the database will be done because no caches are used.Together these rules make sure that it is easy to know when a call to the database will be done (always), what parts of the data are returned from the database calls (entire aggregate) and it is easy to know what these aggregates are composed of.Because you are responsible for saving when something needs to be saved, and when you do a call through a repository, the entire aggregate is returned. The result of this is that you need to do a little bit more yourself, but it also gives you complete control of the entire data flow.PerformanceWith Spring Data JDBC you have a little more control which query will be executed on the database since it is executed directly on the JDBC instead of going through a middle layer.All the queries are eager, this is also an advantage because less queries need to be sent to the database.When you create or update entities in an aggregate through Spring Data JDBC, it will do this by deleting and again saving these entities.Spring Data JDBC needs to do this since it does not have a persistence context and wants to make sure that everything is up-to-date. The downside of this is that it is possible that sometimes unnecessary operations will be executed.With Spring Data JPA you have more possibilities for fine tuning performance. For example with the possibility of using the lazy loading and the usage of a cache.Because of these possibilities it is also more difficult to create a configuration for a good performance.Should I Use It?Spring Data JDBC is in my opinion a very nice addition to the Spring Data family with lots of potential. With the release of Spring Data JDBC 1.1 it became a lot more stable. If you know the rules to follow, it is now very easy to create a simple application using this technology.There are already a lot of advantages to using Spring Data JDBC and these advantages will only grow when features will be added to it.There are also some drawbacks. The biggest drawback is that it is rather new, version 1.0 was released less than 2 years ago. It is also still rather difficult to find a lot of information about this technology so if you will use this technology you will need to reserve some time to experiment.In this case I would advise to at least try the technology out because of the big potential.I have had a lot of fun trying out this technology and I noticed that it really helped me create a good application.This technology hits a sweet spot between Spring JDBC and Spring Data JPA by using the best of both worlds. On top of that they also added some nice DDD principles.So in short, go out and try it out!"
      },
    
      "iot-2019-12-20-ar-signpost-html": {
        "title": "Using Augmented Reality to create an indoor navigation system with ViroReact",
        "url": "/iot/2019/12/20/ar-signpost.html",
        "image": "/img/2019-12-20-ar-signpost/ar-signpost-banner.jpg",
        "date": "20 Dec 2019",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Context  Overview  How it works  Conclusion  ResourcesIntroductionMobile applications based on GPS such as Waze or Google Maps have proven to be very useful in guiding us to our destination.With such applications, it’s easy to find your way to a destination, even in an unfamiliar city.However, it is still easy to get lost indoors, where GPS satellite signals are not accurately trackable for navigation applications.For example, I’m sure that more than one of us has been lost more than once in a shopping mall or airport looking for our way to our destination and not knowing in which direction to go, it’s a frustrating situation right? Well this is the problem we are trying to solve with this proof of concept (PoC) project.In this blogpost, I will introduce the concept of using augmented reality with the aim of creating a navigation system that can help people orient themselves and navigate within large buildings.ContextBefore beginning to explain the workings of this PoC project, let me introduce the two main concepts, which are augmented reality and Indoor Positioning Systems, to give a little context so you can better understand the overall working.Augmented realityOne of the terms that will appear a lot in this blogpost is Augmented Reality or AR which is the foundation of this PoC.AR allows us to add layers of visual information about the real world around us using devices such as our own smartphones.This helps us to generate experiences that provide relevant knowledge about our environment, and we also receive that information in real time.Speaking of AR what better example to give than Pokémon Go.This game has revolutionized the industry of AR-based games and has become, in a matter of days, the most popular Smartphone game in history.Another good example is IKEA Place which is an application that includes a wide variety of furniture, sofas, armchairs and stools in 3D so that we can see thanks to the AR how they would look in our house, showing the size that would occupy each product.Isn’t it amazing?These two examples show that AR can be as much fun as it is useful.Here’s an interesting article from Forbes showing how revolutionary this technology can be in industries like education, health care, tourism and more.Also in 2017, Kevin Van den Abeele together with Michael Vandendriessche wrote a fantastic blogpost that introduces the concept of Augmented Reality in an understandable way.Indoor Positioning SystemBefore explaining what Indoor Positioning Systems is, let’s first look at the types of Positioning Systems that exist today and their uses.These systems are mechanisms that allow us to detect the position of objects or events in a context or in a coordinate system.We can differentiate between global and local:      Global Positioning Systems (GPS) consist of a network of satellites in geosynchronous Earth orbits that send signals that allow the receiver to calculate the distance at which these satellites are located and thus be able to calculate their position.        Local Positioning Systems (LPS) allow us to reach the same objective in a similar way but using local mechanisms instead of satellites (telephone towers, WiFi access points, …) and calculating the position locally.  Indoor Positioning Systems (IPS) are specific cases of LPS whose particularity is that they aim to position objects or events within a space not exposed to the open air as shown in the image below. The principles are similar in all cases, but their particularities make them different.In this PoC we combine IPS with augmented reality without any external mechanism like sensors to determine the position, just the IPS principle.Later below I will explain how we did it.OverviewThis PoC project consists of two frontend applications and a backend:  Mobile application  Web application  REST API  DatabaseThese parts work together to accomplish the goal of helping people to navigate inside large buildings with the help of AR and they are presented below along with a brief description of the role each plays and its most important functionalities. The interaction between them will be explained later in order to understand the functioning of the system as a whole.Mobile applicationThe main functionality of this application is to guide users with signals and paths drawn visually with the help of AR as you can see in the image below. Main functionalities are:  The ability to scan QR code markers to locate users within a building.  Guide users to their destination with visual indications such as lines and arrows using AR.  The possibility to search and select destinations within a building.Used technologiesThe reason for choosing React Native for the mobile application is due to its compatibility with the two major mobile platforms, Android and iOS.Another reason is its native performance as our goal was to develop an application capable of general AR natively on Android and iOS and that was thanks to the ViroReact platform.Next, the technical parts of these technologies are explained in more detail.React NativeReact Native is a multi platform framework to develop native mobile applications that is based on JavaScript and React that uses a concept similar to React’s VirtualDOM, since we also have JSX components, which will be different from HTML components and will have other tags and other names because HTML is not used.What happens is that the React Native compiler will convert them into native interface elements for Android and iOS, which will allow these applications to have a look and feel similar to native applications, a practically equal performance and a navigation and user experience very similar to native applications, since what is being generated is native interface.ViroReactThe technology used to create AR in this project is ViroReact which is a platform to develop augmented reality and virtual reality applications using React Native.The platform supports ARKit and ARCore for the development of AR, thus encompassing the two major augmented reality development platforms under one platform.ViroReact consists of two main components, a native 3D rendering engine and a custom React extension for AR and VR development.Some of the advantages of ViroReact are:  Create an application from scratch or add AR/VR features to an existing application.  Possibility of mobile multi platform using the same code base.  Like React Native, it allows to check the changes made only by updating the app.  Easy to learn as it uses a markup language, which makes it quite intuitive.  It is an open source platform, which allows you to find code that can be reused in your application.  It is not necessary to use Xcode or Android Studio.In 2018, Ryan De Gruyter together with Michael Vandendriessche  wrote an interesting blogpost that teaches how to create augmented reality with ViroReact.Web applicationThis web application is intended for administrators who are allowed to manage the system as a whole as you can see in the image below.Main functionalities are:  Upload a plan of a building to have a reference on which to work.  Ability to manage destinations and routes within a building having as reference the plan of a building.  Ability to manage and print QR code markers to be used as reference points in the physical world.Used technologiesReactReact is a library written in JavaScript, developed on Facebook to facilitate the creation of web applications in a more orderly way and with less code than if you use pure Javascript or libraries like jQuery focused on the manipulation of the DOM.It allows views to be associated with data, so if the data changes, so do the views.React uses what is called the virtual DOM which is a representation of the DOM but in memory, which React uses to significantly increase the performance of components and frontend applications.Basically, when a view is updated, React takes care of updating the Virtual DOM, which is much faster than updating the browser’s DOM (real DOM).When React compares the Virtual DOM with the DOM of the browser, it knows exactly which parts of the page to update and saves the need to update the entire view.REST APIThe REST API is responsible for connecting the two frontend applications with the database to provide the functionalities and data necessary to carry out navigation in a building.Used technologiesOur goal here was to develop a REST API capable of carrying CRUD operations to serve our frontend with the necessary data and functionalities such as providing points of interest to the mobile application or creating them from the web application with the help of an administrator.NestJSThe framework used to create our backend is NestJS which is a framework based on NodeJS and TypeScript that abstracts you from the use of Express and Socket.io through decorators, has injection of dependencies “inspired” in Angular and allows to modularize our applications applying concepts of orientation to objects and functional and reactive programming.The official documentation is another of its strong points that you can find here and there is an official repository with many didactic examples.DatabaseWe needed a database capable of representing real-world entities such as points of interest and their relationships in a graphical way and that’s when we came across the fantastic database Neo4j. Below are the advantages offered by this graphical database.Used technologiesNeo4jNeo4j uses graphs to represent data and the relationships between them.A graph is defined as any graphical representation formed by vertices (illustrated by circles) and edges (shown by intersection lines).As shown in the image below.Databases oriented to graphs such as Neo4j perform better than relational (SQL) and non-relational (NoSQL).The key is that, although data queries increase exponentially, Neo4j’s performance does not decrease, as opposed to relational DBs such as MySQL.How it worksNow that these applications have been introduced, we are going to look at the interaction between them in order to better understand the functioning of the system as a whole.Let’s start from the perspective of the user who has access to the mobile application as seen in the image below. Let’s put your favorite mall on the scene for a moment.In this mall you would find markers with QR codes strategically placed so that they are visible, and you have easy access to them. Each QR contains contextual information such as a unique identifier and the building and floor in which it is located.Imagine you are looking for a particular shop, the first step would be to scan the marker closest to you with the mobile application.This would send the information contained in the QR to our REST API and it would take care of obtaining the necessary information from the database by querying the information obtained from the QR.This information will be sent back to the mobile application, which contains all the points of interest around you, such as shops, escalators and much more.Now it’s time for the funniest part of the process, to visualize these points of interest with AR as shown in the following video.    Are you wondering how this happens? Well this process is carried out with the ViroReact platform.First of all, we must take into account that the AR world is three-dimensional, so we have x, z and y-axis as shown in the image below. The AR scene starts from the point where the camera is located, which is usually [x=0, y=0, z=0] and the objects are located around it with different coordinates.In this case we are going to focus only on x and z-axis because y-axis represents the height of the objects so it will be constant.The points of interest we receive contain geographic coordinates (latitude and longitude) where they are located in the real world.The AR world does not understand geographic coordinates, so we have to convert these GPS coordinates to point x, z and y-axis in meters taking as reference the initial position of the AR camera.For this we use a technique called Web Mercator projection and in this way, we have our points of interest integrated into the real world as shown in the video above.Going back to our example before the mall, you wondered how this system would be implemented in a mall, didn’t you? Well there, the web application intended for administrators or operators who are responsible for indicating where each point of interest is located in the building comes into play.The process is shown in the following video.    ConclusionAs shopping malls, hospitals, airports, universities and other indoor spaces become more complex, the need for indoor navigation systems increases. The lack of GPS support in indoor environments has always made this a challenge. But with augmented reality, it is possible to solve this problem as demonstrated in this blogpost.Applications that implement Augmented Reality are able to improve the user experience through their numerous current applications and the potential they offer for the future. These systems can significantly improve many areas of our lives. From navigating airports to shopping malls, AR can take us to our destination much faster than ever before.Resources  The 10+ Best Real-World Examples Of Augmented Reality  Positioning system  Mercator projection  React Native  ViroReact  React  NestJS  Neo4j"
      },
    
      "conference-2019-12-18-kotlinconf-2019-html": {
        "title": "KotlinConf 2019",
        "url": "/conference/2019/12/18/KotlinConf-2019.html",
        "image": "/img/kotlinconf-2019/kotlinconf-2019.png",
        "date": "18 Dec 2019",
        "category": "post, blog post, blog",
        "content": "  KotlinConf is thé conference to visit if you’re into Kotlin development.JetBrains introduced KotlinConf in 2017 in San Francisco with over 1.200 participants and 50 speakers.In 2018 it took place in Amsterdam with over 1.300 participants and 60 speakers.This year the conference was hosted in Denmark’s Copenhagen at the Bella Center, Scandinavia’s largest conference center.Five colleagues of Ordina Belgium’s JWorks unit were very enthusiastic to attend the conference for the first time.Table of contents  Day one          Opening Keynote by Hadi Hariri and Andrey Breslav      Putting Down the Golden Hammer by Huyen Tue Dao      Kotless - Kotlin serverless framework by Vladislav Tankov      What the F(P) is Kotlin? by Shelby Cohen and Katie Levy      Your First Server with Ktor by Big Nerd Ranch      New Product Announcement        Day two          The Shuttle Case by Stephen Carver      The state of Kotlin support in Spring by Sebastien Deleuze      Using Kotlin for Data Science by Roman Belov      What’s new in Java 19: The end of Kotlin? by Jake Wharton      Kotlin in Space by Maxim Mazin      Going Native: How I used Kotlin Native to Port 6 years of Android Game Code to iOS in 6 months by Ana Redmond        Summing it all up  ResourcesDay oneOpening keynote by Hadi Hariri and Andrey BreslavIntroductionHadi kicked off the keynote by saying that this year is the third time that KotlinConf has sold out once again with over 1.700 participants.A free live stream of all the different rooms would be made available for anyone to attend.Some additional numbers were given: the conference counted over 60 speakers, 35 volunteers and there were people visiting the conference from over 86 countries.The final session of the day would be about a new product announcement of JetBrains, followed by an evening party.Hadi then introduced Andrey Breslav, CTO and Lead Language Designer of Kotlin.Evolution of KotlinAndrey continued with some numbers on the usage of Kotlin for each of the major versions:            Kotlin version      Usage                  1.0      200.000 people              1.1      500.000 people              1.2      2.100.000 people              1.3 (current version)      4.000.000 people      We can clearly see that Kotlin keeps growing at a really nice rate which looks very promising for the language.For JetBrains, important numbers like these, validate the quality of their product.For them it was clear from the beginning that Kotlin had to have an open ecosystem.One welcome to every body and open to anyone who wants to build on top of Kotlin.It was always meant to be more than just a piece of software, it was meant for people united by ideas.Andrey went a bit deeper on the Kotlin ecosystem and explained that they wanted Kotlin to become a default language regardless of the level of experience, platform, scale or type of application.For them to achieve that, it was all about lowering barriers.Different platformsServer-sideKotlin has been friends with Spring for a long time by now.The first official support arrived since 2017 with Spring Boot, Web MVC, and Web Flux coroutines support.Ktor, used for building asynchronous servers and clients in connected systems and also developed by JetBrains, is gaining more traction and is also used more and more at JetBrains.Some other companies that have been using it are Expedia, Intuit and the Norway Tax Office.AndroidAndroid has been Kotlin-first since this year.53% of professional Android developers work in Kotlin and 60% of the top 1.000 apps have been written in Kotlin.JetBrains wants to further invest in mobile multi-platform for both Android and iOS by allowing business logic reuse between the different platforms.Planboard, PlanGrid, Careem Driver’s app, Cash App, Yandex Disk &amp; Yandex Maps, Quizlet, and VMWare Workspace One are some of the apps where business logic is already been reused between the different platforms.What’s coming in 1.4Kotlin 1.4 is said to be released in Spring 2020 and a lot of focus is being put on quality and performance.Currently, 93% of users get code completion results under 500ms but in 1.4, code completion is said to become about three times as fast.IDE Gradle imports should also happen twice as fast and it should use 75% less memory.A lot of improvements are also said to be done for Kotlin/Native compilation time.A new compilerOne of the biggest pains in Kotlin projects is the build speed, especially as the project get bigger.This is why JetBrains has been working on a completely new compiler!It is going to be fast, uniform and pluggable.Some parts are coming in Kotlin 1.4 whereas others are coming in later versions.They are aiming on 5x faster compilations.This however won’t be for version 1.4.The new compiler also comes with a new type inference and will ship in 1.4.This will fix numerous bugs that they couldn’t fix in the previous implementation as it was not flexible enough and it will enable new language features.It can be evolved and extended to enable even more language features later on.Kotlin has three different backends: Kotlin/JVM, Kotlin/JS and Kotlin/Native.In 1.4 they want to offer one single unified backend for all three of them.Kotlin/Native is built from the beginning in this new unified infrastructure so Kotlin/JVM and Kotlin/JS both need to be migrated.This way, if a bug gets fixed this will mean that it will be fixed for all platforms right away!An experimental feature in 1.4 is KLIM, a new portable format.This includes a new unified format used within the different platforms.Multi-platformJetBrains want people to be able to share code and skills across the whole platform.The idea is that you write common Kotlin logic that can be used in any platform that works everywhere.If you want to interact with a platform, you can interaction with platform specific versions of Kotlin.Seeing as libraries are somewhat the bulk of the Kotlin ecosystem, it is clear to JetBrains that they need to pay enough attention to help people writing libraries.They plan on doing this by adding a few tools to help people make libraries and to make their APIs robust and stable over time.This will be achieved with a new library called Author’s Mode that will demand explicit visibility and explicit public types where required.JetBrains is also working on Dokka, a documentation generation tool that will support multi-platform libraries.Mobile multi-platform.Coming in 2020 will be the possibility to run and debug iOS Kotlin apps in Android Studio through a closed-source plugin.Note that this is not going to replace Xcode as you will still need it for certain things, but for the normal development cycle you will be able to stay within one IDE.Kotlin/Native already works on TvOS and WatchOS, and to demonstrate this, an app called Kotlin Locator was made to play a treasure hunting game during the conference.The app comes with a WatchOS companion app so that you could try it out yourself.Browser multi-platformA new feature has been added to allow quick reloading in Kotlin/JS, similar to other frontend frameworks.A lot of improvements were also done on the JS binary size, compressing it further down by quite a bit, reducing the final size of an app.Work is also done on JavaScript operability with support for ES modules and Dukat which will connect Kotlin types with TypeScript types.WebAssembly + KotlinWebAssembly is a new web standard supported by all major browsers.Basically it is a special virtual machine to run all kinds of different code and Kotlin wants to be run in there too.Currently they have a prototype that they are working on.Data science in KotlinNot all users of Kotlin are programmers.Data scientists have also been using Kotlin in combination with Jupyter, Spark and lets-plot.Language changesKT-7770 was mentioned.A ticket about function interfaces created by Sergei Lebedev three years ago.A function interface has a single method and you can use it as a function type.This also improves the Java inoperability if you want to migrate Java code from Kotlin code without things breaking.Language evolutionsIt is important for JetBrains to keep the language modern although they are aware that it is not very desired to have breaking changes in between updates.If something becomes less useful it will be faded out and removed later to avoid legacy from building up.In general, updating from one version to another should remain easy and desired.JetBrains will provide feedback loops to keep developers informed of upcoming changes and to stay in touch with the community.If migrations are necessary, they will provide the necessary guides or tools to help developers migrate their code.For all details, rewatch the keynote on YouTube:    Putting down the golden hammer by Huyen Tue Dao  I suppose it is tempting, if the only tool you have is a hammer,to treat everything as if it were a nail. - Abraham H. MaslowHuyen started her talk with the quote above.Given a new tool or framework, it is common for people to get a bit too enthusiastic with it and to exaggerate in the usage of it, treating it as a golden hammer.Huyen explained to us that she questioned herself on how to write better Kotlin code.And in particular by avoiding her own hammers and biases.As background information, she told us that she worked with Java and C++ before she got into touch with Kotlin.The main anchor that caused her to question herself was the keynote from KotlinConf 2018 where the following was mentioned by Andrey Breslav:  Our main focus, our main motto was to make a pragmatic language.Getting things done, and turning your thoughts into working software without jumping through hoops.Experienced developers praise the following regarding code:  Readability over concision  Reuse over Expressiveness  Interoperability over originality  Safety/tooling over soundnessBased on that, the following questions came up to her:  “When might we be using Kotlin features as golden hammers?”  “How can we use these features more pragmatically and hopefully write better Kotlin”Some hammer-proofing points to ponder:  Readability  Reusability  Maintainability  IntentionalityType inferenceKotlin has a smart compiler that can infer things like the type of a variable.The hinting feature in IntelliJ helps a lot with showing the exact type of a value or object as sometimes it might be possible that you expected a certain type but it’s actually a different one.For example, when declaring a list of numbers but also adding a string to it, causes it to change the list’s type to a list of Any.If we would want to have a compiler error for this we would need to define the type explicitly like val crewlist: List&lt;String&gt;.Something to keep in mind in this case is that we should not depend too much on the hinting feature as you can only see these in IntelliJ and not for example when reviewing a pull request.It is important to have the code speak for itself.In that way it is important to explicitly specify the type in certain cases.Implicit parameterDepending on the context, it is not always easy to tell what the it stands for.Especially in a complex context, you should rather specify the variables to be more explicit.Scope functionsIt is easy to exaggerate in the usage of scope functions such as apply, also, let, run, with.We should be weary of combining too much of these as it causes us to lose track of what this exactly represents and it creates unnecessary complexity making maintenance more difficult.Do not feel obligated to stretch yourself or your code because it does not feel Kotlin-y enough as Kotlin is meant to be a pragmatic language.The code above can be rewritten as follows:Extension functionsWe can define extension functions to extend existing APIs that you may otherwise not control without subclassing.A good practice is to make use of this to keep classes and APIs minimal, only containing intrinsic properties and behaviours and to keep your class abstractions pure.Huyen mentioned the Kotlin for Java Developers Coursera course in which the question was asked on why extension functions are so popular.We can exaggerate with extension functions.For example if you make every single function an extension function of an existing class.We should think about what makes a good utility to a class and we should not make extension functions of too specific functions but keep them as regular functions.Nullability and nullable types? and !! can be valid ways of handling nulls with thought to intentionality and maintainability.Kotlin has nullable types whereas Java does not.If you pull in Java methods into the Kotlin world, you can stumble upon cases where it is not clear if something is nullable or not, resulting into unchecked nullability issues.Null safety checks with ? can be bad too and can obfuscate both good assumptions and bad assumptions.Failing silently is often a bad and undesired thing.Because of that, it can be a good approach to prefer to use !!.!! is not inherently evil but requires careful use based on good assumption or because you just KNOW that a value is non-null.    Kotless - Kotlin serverless framework by Vladislav Tankov  Serverless is a cloud computing execution model, in which the cloud provider runs the server and dynamically manages the allocation of machine resources - wikipediaVladislav mentions that there are three simple steps in building a serverless application:  Take small elements: stateless functions  Compose them with events into an application  Deploy them to the cloud runtimeThis deploy step is often done with Infrastructure as Code (IaC).However, deploying serverless applications with IaC is often not so simple.It can require more than 100 lines of config, for a simple hello world application that contains 5 lines of code.For a simple website it can even go to more than 1.000 lines of config.On top of that, IaC often uses a separate language to express the configuration.This requires developers to learn yet another language.As a solution to these concerns, the Kotless framework is introduced.KotlessKotless is an IaC tool for Kotlin serverless applications.It consists of two parts:  A DSL that provides annotations to define routing, scheduled events, etc.  A Gradle plugin to configure how the serverless application should be deployed.The Gradle plugin configures which application code should be scanned to generate Terraform code, and all the other necessary information for your deployment.By scanning your application, the Gradle Kotless plugin can determine a lot of the Terraform code that is required for your deployments.This greatly decreases the amount of deployment config that you have to write yourself, and the things that you do have to write, you can write in Kotlin.Seamless serverlessAnother advantage of Kotless is that you can write your serverless applications like a normal application.Then when it is time to deploy to the cloud, Kotless will split your application into multiple lambdas.This means that you can deploy the same application as a standalone app in-house, or as a serverless app to the cloud.Kotless code analysisBy analyzing your application code, Kotless can automatically configure the following things in your cloud environment:  API interactions - based on annotations like @Get  Events handling - based on the @Scheduled annotation  Permissions requirements  Queueing systems - based on usage of Queue and List types  Calls of other serverless apps - based on async {...}ConclusionKotless looks like a promising framework to make it easier to deploy serverless applications.It already has quite some interesting features, but they are working on it to make it even better!    What the F(p) is Kotlin? by Shelby Cohen and Katie LevyShelby Cohen and Katie Levy shared with us the story of how they managed to introduce functional programming with Kotlin into their organisation.They presented an 8-step guide on how to influence colleagues and your company to adopt Kotlin based on their experience at Intuit:1. Find your PassionFind something you want to be passionate about.In this case, it is Kotlin.2. SocializeShare your passion with other people.Try to find some things you like about Kotlin - or anything new for that matter - and tell about it to your colleagues.Give some examples on how Kotlin removes ceremony compared to Java.Spread the word on how Kotlin has null-safety built into the type system.3. Proof of ConceptDo not only talk about your passion.Do as you preach and have something to show to your peers.Develop a new feature and have it reviewed and discussed by the others.This should power the discussion with seniors developers and architects on whether the new technology should be adopted, put on hold or avoided.Having a POC and other developers on board helps this conversation.4. Create a PlanThe light is green and you are allowed to make the switch. But other developers will still feel insecure about their Kotlin skills.Katie and Shelby put together a plan to migrate from Java.They provided online training courses to teach some basic features which helped the onboarding.This increased the comfort and confidence of the team to make the jump.By setting goals, they were able to switch at a comfortable pace:  Start with migrating and writing Kotlin tests which do not impact production code  Migrate easy wins such as data classes  Only then start adding new features in KotlinThey also monitored what their changes were doing to the stability of the product:  Less crashes because of the improved type system (less NPEs)  Features were written faster in Kotlin  Less lines of code (to maintain and read)Try to provide feedback but watch out to not push learners too much.Some Kotlin code will look a bit Java-y at first, but this will improve over time.5. Persuade with DataThe other engineers on the team might not (yet) share your passion for Kotlin, so use some data to persuade them even more.Some numbers that can be used:  Kotlin uses 40% less lines of code than Java  More concise code takes less time to write and less time to read  Show examples on how Kotlin removes boilerplate code  The growing amount of big corporations who made the leap to Kotlin  4th Most loved language on the 2019 Stack Overflow Developer Survey6. Deconstruct MythsEven with all this data, there are still some myths that exist concerning the language which you do not want to see spread around.Shelby and Katie broke down these myths in a technical white paper which they talk about on Medium.During the closing keynote of the conference, Maxim Shafirov (Jetbrains’ CEO) also proposed to make an official white paper to help companies migrate.7. Can’t stop, Won’t StopAt this point, the team was on board and the train started going full-speed. This is when they started a learning community to get other teams on board to spread the word.They would give presentations, do live coding and code reviews to help others get started too.8. Connect with Larger CommunityKotlin has an amazing community, which allows you to grow even further.Katie mentions working on an Intuit OpenSource project, attending conferences and podcasts.Why Functional ProgrammingFunctional programming is a programming paradigm where computations are treated as mathematical functions.Shared state and mutable data are avoided which improves the quality of the codebase.It is also very helpful for multi-threaded workloads.Functional programming is easier in Kotlin because functions are so called first-class citizens.A good third party library which enhances the functional capabilities is arrow-kt.io.It is similar to the Vavr library for Java developers but with the benefits of the Kotlin idioms.    Your First Server with Ktor by Big Nerd RanchThis was a workshop on trying out Ktor and serves as a nice introduction to the framework.Before starting, it is required that you install the IntelliJ Ktor plugin via the Plugins menu.After doing so, you can start a new Ktor project from within IntelliJ.An alternative way of doing so is by going via the Ktor Project Generator.Something worth noting is that the application.conf file is written in HOCON.The instructions of the workshops together with the TXT file that is supposed to be read during the exercises can both be found on Google Drive.You will learn how to set up your IDE, starting your Ktor application, and routing and serving data.New product announcement by Hadi Hariri, Maxim Shafirov and Maarten BalliauwAt the end of the first day it was time to announce JetBrains’ newest product!Everything started with a ticket created 15 years ago by Maxim Shafirov, the CEO of JetBrains.The ticket was about a request to improve collaboration between teams.Maxim shortly went over how JetBrains came to be.20 years ago, three developers came together and created a product to help them refactor some code.By now, JetBrains has created over 20 products based on their own needs.During the growth of the company from three developers to 1.200 people, JetBrains faced many challenges on working together between all the different teams.Creating a product involves more than just a development tool.You have to take several things into account like version control, chat applications for communicating, write documentation and issue tracking.Besides developers, you have other teams within a company such as testers, HR and marketing.A challenge many companies face is to efficiently have all teams cooperate with each other which is why JetBrains wanted to create a tool specifically for this.Meet Space!    Space is an integrated team environment that provides teams and individuals all tools necessary to create and collaborate efficiently in software development.Demo time!Maarten Balliauw demonstrated how Space can be used within a company and gave us an impression on the look and feel.At the end of the demo, Maxim returned to share the prices of the different subscriptions.Note that a free version is available!On the website you can request an invite to try out Space.Attendees of KotlinConf have a guaranteed spot to try it out.More info is available in the blogpost on JetBrains’ website.We think that JetBrains is very ambitious with Space but given how good their products have been until now, we are convinced that this will make another great product.Rewatch the Space announcement video:    After the announcement, the KotlinConf party started with different types of food, drinks and nice music for all the attendees.Day TwoThe Shuttle Case by Stephen CarverThe second day was kicked off with another keynote given by Stephen Carver, Senior Lecturer, consultant and speaker in Change and Crisis Management at Cranfield University School of Management.One of the topics of the keynote was risk management and Stephen started talking about NASA.In 2003 there was the accident with the Space Shuttle Columbia exploding during atmospheric entry causing the death of all seven crew members.Another similar accident happened in 1986 with the Space Shuttle Challenger.NASA knew this would happen beforehand.They knew about this and let this happen instead of dealing with the change culture, causing human lives to be lost needlessly.People just didn’t listen.It is the fault of the culture and the system.Stephen said that stories are vital for leaders and that a story expresses how and why life changes and how it can be.He mentioned the following quote:  I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the Moon and returning him safely to Earth. - John F. Kennedy, May 25, 1961“By the close of this decade”, quite an ambitious target.Stephen went over the space progress of the Soviet Union and how the space race came to be between them and the Unites States.The Soviet Union launched the Sputnik 1 into space, the first satellite.There was Laika, the first dog that made it to space in the Sputnik 2.We had Yuri Alekseyevich Gagarin, the first human to make it into outer space and Valentina Tereshkova, the first woman in outer space.And finally there was Alexei Leonov who was the first human to do a spacewalk.Next up was the Moon landing of the Apollo 11 and the importance of planting the US flag by Neil Armstrong and Buzz Aldrin.Stephen also mentioned the Apollo 13 movie and he recommended watching it some time.He showed a clip of the energy drain part and that they needed to conserve maximum energy in order for the crew to make it safely back to Earth.      Failure is not an option.More Moon landings followed and to put it in a blunt way, people were getting bored.So what was supposed to be the next thing…? Commercial spaceflights, only this time the cost would need the same considerations as weight safety and performance.However, NASA was getting old. There was too much management, too little leadership, silos, politics and management by fear.With the Space Shuttle Columbia, it was about getting the weight of three times the Statue of Liberty into space with all the fuel needed.The solid rocket boosters, also called “white tubes”, were responsible for granting the space shuttle enough trust during the first few minutes of flight.However they were always very fragile seeing as about one out of 20 explode during launch.They were made in Utah and they had to go to Cape Canaveral.Each one of them was about 40 stories high so moving them was quite challenging as they had to be moved in pieces.In 1981, the first Space Shuttle Columbia was launched.Given the odds, NASA got very lucky.Flights continued until 1986 when the temperature was very low, about 25ºF (-4ºC).Icicles were actually hanging off the Space Shuttle Challenger the day before the fateful launch.Months before the launch, Roger Boisjoly, an American mechanical engineer, raised his concerns and objections to not have a launch in low temperatures like this as the O-rings on the rocket boosters would fail.The management had no ears to this and Boisjoly remained ignored.After the flight got confirmed for the 28th of January 1986, Boisjoly and his colleagues tried to stop the flight.This was discussed with Morton-Thiokol managers who agreed that the issue was serious enough and NASA got called.NASA went crazy and told them to prove it to them that it was going to blow up.In the end, NASA overruled their warnings and the flight continued.It was clear beforehand that the O-ring system was going to fail.First O-ring failed upon ignition and a second one failed during throttle up.Moments later the vehicle got disintegrated.The crew actually survived the explosion as they were in a separate crew compartment, but they could not move and knew they would die by falling into the ocean at huge speed.In 1992, NASA got a new CEO, Daniel S. Goldin, who remained there until 2001.With his “faster, better, cheaper” approach he wanted NASA to continue delivering a wide variety of aerospace programs with costs being cut.He got told it was going to impact on safety but he did not believe it and replied that it was nonsense.In 2003 there was the Space Shuttle Columbia Disaster.During the launch, a piece of foam insulation broke off from the external tank and struck the left wing of the orbiter, breaking off multiple tiles.NASA managers were aware that this happened but didn’t inspect the damage nor did they inform the crew as they thought that the foam could not have caused a lot of damage.Some engineers suspected that the damage was more serious.  We did not believe pictures would be useful to us, we felt that we could have done nothing anyway so we decided not to even take them. - Ron DittemoreAccording to the CAIB Report Appendix D.13 a rescue mission was challenging but entirely feasible to save the crew.However, NASA had the Columbia return to Earth and as it re-entered the atmosphere, the damage done to the left wing caused hot atmospheric gases to penetrate the heat shield and destroy the internal wing structure.This caused the spacecraft to become unstable and break apart, killing the crew of seven.In 2011 the shuttle program got shut down because the software programs got unreliable and the programmers that made it during the 60’s were either retired or dead.Stephen started talking about Elon Musk and his SpaceX company that got founded in 2002.What was remarkable was Musk’s persistence and the way he sets his mind on things.  Failure is an option here.If things are not failing you are not innovating enough. - Elon MuskHe even had a small video made of all the different failures they had.    During an interview with Musk, the interviewer mentioned that Musk received criticism from Neil Armstrong and Eugene Cernan.Both of them were heroes to him so it was a painful thing for him to hear them say.He would invite them over to come visit him to see how work was done at SpaceX.Musk’s successes came one after the other:  April 2016: First stage landing on a drone ship  Feb 2018: Shooting a Tesla Roadster into SpaceStephen asked the audience who had heard about Blue Origin.About 10% of the people in the room raised their hands.This is obviously the space company of Jeff Bezos.Stephen explained that Bezos puts about $1 billion of personal budget each year into Blue Origin and that giving anything of it away during his divorce was out of the question for him.With Musk and Bezos, we now have a second space race happening.Two of the richest men of the world, racing to the Moon by 2024.To conclude, Stephen asked the audience who would have joined the last successful flight of the space shuttle in 2011 if they were given a ticket for it.About 5% of the people raised their hands.  Some of you are mad… and it’s not necessarily the 5% that raised their hands.Last thing Stephen wanted to give us was the importance of trusting your own intuition.  And the trouble is, if you don’t risk anything, you risk even more. - Erica JongAll in all, a very strong keynote to start the second day of the conference.We loved it!    The state of Kotlin support in Spring by Sebastien DeleuzeBig kudos for Sébastien Deleuze for thinking about the environment.He took a train from France to Copenhagen to lower his CO2 emissions.Even though his train got hit by a boar, he still managed to give us an update about the current state of Kotlin support in the Spring ecosystem.Why Kotlin for Spring?Sébastien explained that Kotlin has less noise than Java.It gets to the point a lot better and has better (null-)safety.He also considers Kotlin to be more fun in general.Kotlin also has a lot of traction in the Android ecosystem, which might end up influencing and changing the server world.Spring ♥️ KotlinStarting from Spring 5.2, all the Spring Framework reference documentation now has Kotlin code examples.The new examples are no simple cut and paste from Java but also take into account the Kotlin idioms and DSLs.Extending the reference documentation for Spring Boot and Spring Security are estimated for next year.The Spring Data documentation might take a little longer as it is a rather big project.Since a few months, the Spring Initializer project also supports Gradle with Kotlin DSL.Kotlin allows for nice DSLs which gets reflected in the number of new DSLs in Spring:  MockMVC DSL by Clint Checketts and JB Nizet  Spring Cloud Contract DSL by Tim Ysewyn  Spring Security DSL by Eleftheria Stein.It is developed by the Spring Security team but still experimental.This will be shipped with the Spring Security library somewhere next year.  Spring MVC DSL and functional API.The Router DSL is now available for both Spring MVC and Spring WebFlux.A quick example of this last bullet point.@Configurationclass RoutesConfiguration {    @Bean    fun routes(): RouterFunction&lt;ServerResponse&gt; = router {        GET(\"/hello\", ::hello)    }    fun hello() {        ServerResponse.ok().body(\"Hello, world!\")    }}The routes can be created dynamically, which means that routes can be constructed from data.This is handy for webshops or CMS applications in general.The overall Kotlin DSLs’ status is shown in the following picture:CoroutinesSébastien stated that coroutines allow consuming the Spring Reactive stack with a nice balance between imperative and declarative style.Coroutines are a new way to consume the reactive stack and have following characteristics:  Operations are sequential by default  Concurrency is explicit  Three Building blocks:          Suspending functions are the most important concept and can be used when performing blocking calls such as accessing a database.  This is an example of what it would look like:          suspend fun hello() {      someExpensiveBlockingCall()      println(\"done!\")  }                      Structured concurrency provides building blocks like coroutines scope which define asynchronous boundaries.  This is important because it allows to define behaviour for when an error occurs in one of the asynchronous tasks.                    Flow  is the coroutines equivalent of Flux in the Reactor world.  It is interoperable with Reactive Streams and has support for back pressure.            Spring BootThere is now support for loading vals from properties without using lateinit.This means the following code:@ConfigurationProperties(\"blog\")class BlogProperties {    lateinit var title: String    val banner = Banner()        class Banner {        val title: String? = null        lateinit var content: String    }}Can now be re-written using @ConstructorBinding as:@ConstructorBinding@ConfigurationProperties(\"blog\")data class BlogProperties(val title: String, val banner: Banner) {    data class Banner(val title: String?, val content: String)}Kofu: the mother of all DSLsKofu is an experimental DSL that allows to define the Spring Boot application configuration.Traditional Spring Boot performs a classpath scan and enables some libraries conditionally (such as Jackson).With Kofu DSL, this is not the case, which allows for a more fine-grained setup.Kofu also allows for a faster application startup and less memory consumption.As it is still experimental, it should not be used in production yet.    Using Kotlin for Data Science by Roman BelovWhile Kotlin is traditionally used for server-side development and Android development, it can also be used to do data science related projects.In this talk, Roman Belov demonstrates what libraries and tools we can use to do data science with Kotlin.Tools and frameworksOne of the most popular data science tools is Jupyter.Jupyter notebooks are a popular data science tool that allows to easily share code and the resulting output with third parties.It also serves as an interactive shell that allows you to write code and print the output (text, graphs, …) directly underneath it.Jupyter can already be used for Python, R and Scala, and recently Kotlin kernel has been created for this as well.Roman also shows the integration of Kotlin with Apache Spark, a popular framework for analyzing big data.It is usually used with Scala or Python, but can also be used with Kotlin.He goes on to demonstrate Spark on Apache Zeppelin (a notebook like Jupyter) as well, for which he wrote a Kotlin interpreter.It is also possible to connect to a Zeppelin server in IntelliJ and do most of the things in your favourite IDE instead of on the web-based notebook.LibrariesRoman demoed the Kotlin Jupyter implementation by using some interesting libraries such as let’s plot and Krangl.Let’s-plot is a library for plotting statistical data.Krangl is used for data wrangling, inspired by the popular dplyr package in R.Another interesting library in data science is Numpy, used in the Python language.Roman created a library called Kotlin Numpy, a wrapper for the Numpy library.Kotlin Numpy has a couple of advantages over Numpy, such as better performance and type safety.ConclusionKotlin is a general-purpose language, and can thus be used for data science purposes as well.Quite a few interesting libraries have been created by now and are being developed further,making it an interesting alternative for the mainstream data science languages such as Python, Scala and R.    What’s new in Java 19: The end of Kotlin? by Jake WhartonA talk on KotlinConf where the title contains the words “the end of Kotlin” was bound to lure people in.Jake Wharton takes a look into the future to see what features Java could have in 2022, which is when Java 19 will be released, and compare it with the current state of Kotlin.Variable Type InferenceJava 10 already has type inference for local variables although there are limitations to when you can replace the type with var.Kotlin has no such limitations.Local functionsMoving on to features that aren’t there yet for Java.In Kotlin you can put a function inside another function, called a local function.This approach allows to reuse code that is only used in the wrapper function, making it clearer that it only belongs to this function.Java will have a similar feature that will probably be in Java 15 or 16.Multiline StringsKotlin allows for multi-line strings with a few options (e.g. trim margin).Multiline strings are currently available in Java 13 (experimental) but will probably be stable in 2020.fun main() {println(\"\"\"    |SELECT *    |FROM users    |WHERE name LIKE 'Jake %'    |\"\"\".trimMargin())}Value based classesData classes are one of the best features in Kotlin.However, records are coming to Java in the near future, which are similar to data classes in Kotlin.Kotlin also has sealed classes, which can only be extended by classes defined in the same file.Java will have these as well, and will have sealed interfaces on top of that.Type MatchingTo check if a variable matches a certain type in Kotlin, you can use this:val o: Any = 1if (o is Int) {    println(o + 1)}Kotlin also has a smart casting system, which automatically casts it to the type you checked for within the code block.Java is getting a similar feature, but without smart casting.DestructuringDestructuring in Kotlin works as follows:data class Person(val name: String, val age: Int)val alice = Person(\"Alice\", 12)val (name, age) = aliceYou can extract the individual components of the class into local properties.You can also use this in a for each loop (for ((name, age)) in people { }) or on lambda arguments.Java will also support destructuring in the future, although its usage might be restricted.CoroutinesCoroutines are a great feature of Kotlin, allowing lightweight asynchronous tasks without starting up new threads.Java might get a similar feature (virtual threads) in the distant future.They will take blocking methods (e.g. Thread.sleep()) and turn them into asynchronous methods.The end of Kotlin?While Java might catch up on some features in the future, Kotlin will also evolve in the meantime.On top of that, there are currently no plans in Java to tackle nullability in Java, which is arguably one of the best features of Kotlin.Kotlin also has first-class multi-platform support, which Java doesn’t have (and doesn’t plan to at this point).Furthermore, VM changes and new Java APIs are also beneficial for Kotlin.Finally, Kotlin has IDE support to migrate codebases progressively into Kotlin.  In 3 years, is this going to be the end of Kotlin? I don’t think so. - Jake Wharton    Kotlin in Space by Maxim MazinAt the end of the first conference day, JetBrains announced their new collaboration tool, Space.Maxim covered how they’ve been using the tool themselves at JetBrains.Space is actually full-stack Kotlin.There is a shared Kotlin model: data models, view models, validations, RPC, utils and common libraries.Basically everything besides the UI, as it was always an important factor to have it look native, and the storage is shared.Used frameworksExposed allows you to have type-safe SQL and ORM.Initialize your database in an easy way and have it track your model to help you with migration scripts.You define entities that map to tables, for example EUser.Ktor provides asynchronous servers and clients.It makes it easy to initialize and run a server from code.You specify routes in a simple way based on a path and a suspend code block.There is also WebSocket support.The onConnect, onMessage, onClose and onError are all implemented in a single flow thanks to Coroutines instead of having to implement separate methods.Kotlin full-stackHaving a Kotlin full-stack architecture makes it easy to check for usages to find dead pieces of code.Within Space there is an administrator playground API to play around with things.OpenAPI (Swagger) JSON is generated in case you want to use something else other than Kotlin.JetBrains has a GitHub repository kotlin-wrapper that contains a bunch of Kotlin wrapper for popular JavaScript frameworks.kotlin-react, kotlin-css and kotlin-styled are used:  kotlin-react: inherit from components and implementing a render function.  kotlin-styled: a Kotlin DSL for styled components.  kotlin-css: offers type-safe CSS declarations.TestingIn the backend, the only testing library used besides JUnit is AssertK.In the frontend, Enzyme is used to test the React components. For UI tests, Selenium is used with Allure for reports with screenshot support in case a test failed.    Going Native: How I used Kotlin Native to Port 6 years of Android Game Code to iOS in 6 months by Ana RedmondAna Redmond of Infinut presented us with a really nice feel good story about how she made her Android games available for iOS.She noticed her daughters’ faces when they came home with another math assignment.Needless to say, it wasn’t the most happy face.She noticed her kids were trying to memorize the rules instead of really grasping the material.That’s when she realized she could make small games so her kids would not only learn maths better but also enjoy the process.She created a vast library of over 60 educational games and 120+ lessons for Android devices over a period of six years.Many kids were learning math and enjoying it.Unfortunately, the many kids who use an iPad were left out without access to the material she brought to the Android platform.So when Ana heard about Kotlin/Native she decided the time was right to serve those kids as well!Kotlin/Native is used to compile Kotlin code to platforms where virtual machines are not desirable or possible,such as iOS.In order to write the individual games once for both platforms, she needed a common interface for the platform specific APIs for drawing on the screen and for playing sound.Luckily, both platforms had very similar APIs for drawing 2D images.The games are then written purely in Kotlin making use of these interfaces.To keep the native experience on both platforms, a few screens, such as the login screen, are still developed using their native platform.You might be wondering “Why did she go through all the trouble to create a multi-platform 2D game engine from scratch when there are many alternatives available?”.Well, you’re lucky.She already addressed this before we had the chance to ask!There were two big reasons why she did not opt for the Unity game engine, which is an industry standard for making small-scale, multi-platform games.The first reason is that the games would drain much more battery on their target devices.Teachers who employ tablets with these educational games in the classroom were very happy to hear that Infinut’s math games are much more battery-efficient.Another reason is that while Unity is great for rapidly building a game, it is not as easy to build a library of games with shared codebases.The fruits of her labor proved successful as she was able to port her 60+ Android games to iOS using 80% shared code!There are two ways how Ana used shared code for leveraging platform specific APIs.One way is to make an interface in Kotlin/Native and implement it separately for each platform.The preferred way is to make use of expect and actual to call the native APIs from Kotlin.This way you don’t need to write a wrapper for functionality that is already supported.This method was used for implementing drawing images, playing sound, fonts and transformations.Since the back button works differently in Android and iOS, she had to fall back on using an interface.The back button exits the game and returns to the View or Activity in Android or the Controller in iOS to clear everything up.In the code of her game she then just has to write gameplayer.back() which will call the correct implementation depending on the platform.She then stumbled on memory leaks in iOS because of a cyclic reference.This was easily solved using a weak reference.In the process of porting her 60+ games to Kotlin she learned some valuable lessons.  Don’t use mutable objectsUsing a mutable Singleton, like the SoundManager from the example, led to threading issues as only immutable objects can be shared among threads.  Use List instead of ArrayThis is because the Array in Swift works like a List.So wherever Kotlin code is called from Swift code, it’s better to use List instead.  Initialization orderSomething strange happens with parent-child classes where the parent uses a function to initialize the child in an init block which the child overrides and sets a property of the child class.Ana discovered a situation where a property is overridden as expected in Android but not on iOS.This was caused by the initializations of the child class being run in a different order.She recommends not creating child classes this way and to just use the init blocks for initialization.Because she already had a lot of code written in the above way, she opted to just move the property from the child class to the parent class.More details can be found on her 12-part Medium blog post series.    Summing it all upThere was a good variety of interesting talks to choose from.The first keynote gave a nice overview of the current status of Kotlin whereas the second keynote really got you pumped for the second day of the conference.Noticeable was how much focus was spent on providing healthy food options and snacks.Bella Center Copenhagen was a beautiful venue to host the conference at.It was roomy enough for all the participants and it was convenient having a hotel room in there.All in all, we had a great time and picked up quite some things.Definitely worth another visit!Resources  KotlinConf website  KotlinConf 2019 YouTube playlist"
      },
    
      "machine-20learning-2019-12-16-our-devoxx-demo-app-html": {
        "title": "Our Devoxx Demo App with realtime object detection",
        "url": "/machine%20learning/2019/12/16/Our-Devoxx-Demo-App.html",
        "image": "/img/2019-12-16-devoxx-demo/banner.jpg",
        "date": "16 Dec 2019",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Overview  Setup  Code  Conclusion  ResourcesIntroductionLike every year we were present with Ordina at Devoxx as a sponsor.For this edition I had prepared a demo application with realtime object detection.Our demo was a huge success and is the reason I’m writing this blog post.In this blog post I’ll explain how to get everything set up to make your own application that uses realtime object detection!All code is available online on our ordina-jworks Github and contains a number of branches with specific implementations.OverviewBefore we dive into the actual code, I’ll explain what technologies I’ve used and how they all come together to form this demo application.The demo uses the following technologies:  Electron: This serves as the multi-platform app container which allows us to write what is essentially a web application, and run it on a variety of operating systems.  TensorFlow.js: This is the machine learning library used to run the model for object detection.  CoCo Dataset: Stands for Common Objects in Context and is a very large, highly curated dataset of images which have been annotated with one or more of 90 classes.          CoCo-SSD: A default implementation by TensorFlow itself utilizing an SSD detector (Single Shot MultiBox Detection).I have downloaded the pre-trained model and it is included in the Github repo      YoLo V3: Is an object detection system that looks at the whole image at test time and derives its predictions based on the global context of said image.        Node.js: Needed for npm and building the demo app, Electron also provides the option to use bare Node code/packages in the code.SetupThe demo app uses Electron as the main ‘framework’.The setup for the application is pretty simple, there are just a few dependencies in the package.json:\"engines\": {    \"node\": \"&gt;=12.13.0\"  },  \"dependencies\": {    \"@tensorflow/tfjs-node\": \"^1.2.11\",    \"typescript\": \"^3.6.4\"  },  \"devDependencies\": {    \"electron\": \"7.0.0\",    \"electron-packager\": \"^14.0.6\"  }The main entry point for the application is just under the src folder of the root folder of the project.The bootstrapping code is also very simple:const { app, BrowserWindow } = require('electron');function createWindow () {    let win = new BrowserWindow({        width: 1920,        height: 1080,        webPreferences: {            nodeIntegration: true        }    });    //win.webContents.openDevTools();    win.loadFile('./src/devoxx/site/index.html');}app.on('ready', createWindow);We import the app and BrowserWindow and create a function that creates a new instance of the BrowserWindow class.This instance specifies the configuration for the Electron app.We set the width and height and set the nodeIntegration to true so we can use the file system from within the browser code.We also tell the BrowserWindow instance to load a certain html resource.Finally we bind the app ready event to create the BrowserWindow instance.Additionally for debugging it can be handy to uncomment the line win.webContents.openDevTools(); as that will open a Chrome DevTools window when the application launches.                                CodeThe basic code is not too difficult:window.onload = async () =&gt; {    const detector = await Loader.loadCoco(false, path.resolve(__dirname, '../../../'));    const stream = await navigator.mediaDevices        .getUserMedia({            video: {                width: 1280,                height: 720,                frameRate: framerate            }        });    let video = document.querySelector('video');    video.srcObject = stream;    video.onloadedmetadata = () =&gt; {        video.play();    };    const canvas = document.querySelector('canvas');    canvas.width = 1280;    canvas.height = 720;    const context = canvas.getContext('2d');    context.drawImage(video, 0, 0, canvas.width, canvas.height);    await update(video, canvas, context, detector);};Once the page is loaded we create our detector instance.This will be used to feed in the image data from the webcam.We also need to fetch a video stream from the webcam, in this example at 720P, as that’s the max resolution for my webcam.Please note that a higher resolution will require considerably more processing power.After we have a stream, we fetch the video tag from the page, assign the stream to it and let it play.We also get our canvas in which we will display the actual video output, the video tag itself is hidden.The canvas allows us to annotate the image with a bounding box and some extra information about the detected objects.Finally we call the update method which will be called for each update.async function update(video, canvas, context, detector) {    context.drawImage(video, 0, 0, canvas.width, canvas.height);    const detectedClasses = await detector.detect(canvas);    Loader.anotateCanvas(canvas, detectedClasses);    updateList(detectedClasses);    if (enableLiveUpdate) {        setTimeout(update, 1000 / framerate, video, canvas, context, detector);    }}This function will function as our update loop/tick.We draw the image to the canvas, just as it was seen by the webcam.We then pass the canvas into the detector which will detect any objects in the image data contained in the canvas.This returns an array of detected classes which contain the type of objects, the score and a bounding box.That information is fed to a utility method that will annotate the canvas with a red rectangle with the coordinates of the bounding and the type of object with its score.The update function will be called again with a very basic (and very limited) framerate timeout.There is other code, mainly in the coco folder which is mostly taken from the default google implementation and modified to match the needs of this application.One thing worth noting is the changes required to a small piece of code to make the model work faster by allowing it to work asynchronously:    const [maxScores, classes] = this.calculateMaxScores(scores, result[0].shape[1], result[0].shape[2]);    const indexTensor = tf.tidy(() =&gt; {        const boxes2 = tf.tensor2d(boxes, [result[1].shape[1], result[1].shape[3]]);        return tf.image.nonMaxSuppression(boxes2, maxScores, maxNumBoxes, 0.5, 0.5);    });    const indexes = indexTensor.dataSync() as Float32Array;    indexTensor.dispose();Needs to be changed into:    const [maxScores, classes] = this.calculateMaxScores(scores, result[0].shape[1], result[0].shape[2]);    const boxes2 = tf.tensor2d(boxes, [result[1].shape[1], result[1].shape[3]]);    const temp = await tf.image.nonMaxSuppressionAsync(boxes2, maxScores, maxNumBoxes, 0.5, 0.5);    const indexTensor = tf.tidy(() =&gt; {        return temp;    });    const indexes = indexTensor.dataSync() as Float32Array;    indexTensor.dispose();Because the tf.image.nonMaxSuppressionAsyncreturns a Promise the replacement is not as simple as just replacing the method call to the async variant.This however is the only big change that needs to be done (apart from changing some dependencies) to get the application working with full WebGL acceleration.The application is actually really simple and very easy to understand and tinker with. I strongly encourage you to check out the repo and have a go at getting it up and running by yourself.The other branches contain different solutions with some tweaked code, be sure to also check those out!ConclusionMaking a cool demo that utilizes machine learning and pre-trained models is not at all that hard. Rapid prototyping with these pre-trained models allows one to quickly see if a certain strategy or desired functionality is workable and merits further development effort.It also provides a way to get started easily in a matter that is extremely hard to master. Retraining or tweaking these models can be very hard and time consuming as it requires an in-depth knowledge of the matter at hand (both mathematics and the actual data).Im my opinion these pre-trained models and other machine learning ‘building blocks’ provide an extremely valuable toolset for developers.Resources  Demo Github repo  TensorFlow.js  CoCo Dataset  tfjs-CoCo-SSD  YoLo  tfjs-YoLo-V3"
      },
    
      "conference-2019-11-29-aws-dev-day-2019-html": {
        "title": "AWS Dev Day 2019",
        "url": "/conference/2019/11/29/AWS-Dev-Day-2019.html",
        "image": "/img/AWS-Dev-Day-2019/aws-dev-day-2019-banner.png",
        "date": "29 Nov 2019",
        "category": "post, blog post, blog",
        "content": "AWSome Dev Day 2019!  The AWS Dev Day is a free conference day hosted by Amazon where architects and developers from Amazon and other companies talk about the most popular AWS services, the newest features and everything related to cloud development. These sessions explain a lot about what AWS has to offer and you will see live demos on how you can implement them in your own use cases.Amazon held their Belgian Dev Day in Mechelen this year. This was very convenient for us as the Ordina Belgium HQ is also located in Mechelen. This means that there was a very good reason for Ordina consultants to attend the event. We went with a small group of 8 people to make sure we didn’t miss the latest AWS news. We were treated with delicious food &amp; drinks throughout the day to make sure their guests were happy.CI/CD for Modern Applications by Sébastien StormacqIACIn an application development environment, it is important to maintain your Infrastructure as Code (IaC) to allow for repeatability, and doing so in a declarative way.Doing it in an imperative way does not allow for deltas and requires your system to be rebuilt from scratch every time your pipeline gets triggered, which results in worse impact for your end users.There are a number of ways to provide your infrastructure as a code declaratively.We have Terraform, CloudFormation, etc. But Amazon has a new kid on the block since this year: the Cloud Development Kit (CDK).This allows developers to provide your infrastructure in their favorite programming language.You can communicate with AWS in a way other than the AWS CLI or the Web Console, as you can now create services as you need them and when you need them.All of this gives you the best toolkit to NOT create snowflake servers.  A snowflake server is a server that requires additional configuration in its environment that is often done manually. These are configuration steps that are done after the automatic pipeline process.The CDK also has a CLI which can be installed by running npm install -g aws-cdk.PipelineAmazon’s pipeline service is called AWS CodePipeline.This service allows you to create a pipeline which executes all the necessary steps to automate your releases and to provide infrastructure updates.This allows you to quickly provide new releases and deliver new features to your end users.You can integrate this service with other external services (ex. GitHub, where your repository is hosted).To effectively build your application, you can make use of AWS CodeBuild. This is their continuous integration service that packages your application by building it and running the tests. Pricing is very effective, as you will only pay by the minute when you trigger a build. CodeBuild automatically scales and can process multiple builds at the same time, so you won’t have to deal with build queues.DeployAWS CodeDeploy is a deployment service which basically allows you to deploy (almost) anywhere, including (but not limited to) on-premise servers. This service makes it easier to provide updates to your end users and to avoid downtime during the rollout of a new update.One of the most impressive features is the Blue/Green deployment. This splits the traffic going to your application into traffic to your original environment and the new environment. The best use case for this would be during a new production release. You have a new release that you want to deploy? Simply let 10% of your traffic go to that release to make sure everything works right. After monitoring the release, you can choose to either revert or to fully allow traffic to go to the newest release.ConclusionAmazon provides a lot of tools that help you manage your builds and deploys, making sure that everything works according to plan. We are definitely fans of their new Cloud Development Kit and their Blue/Green deployment option is able to stabilize and fully control the rollout of a new deployment.Integrate your front end apps with serverless backend in the cloud by Sébastien StormacqServerless is a big trend nowadays and rightfully so as it takes away a lot of operational work from you! The biggest pros of serverless are that you no longer need to provision or manage your own servers, it automatically scales with usage, it has built in fault tolerance and fallback methods and last but not least, you only pay for the actual usage. This means that if you are not using your lambda, you are not paying for it!Sebastian filled the session mostly with a demo in which he built a React app to communicate with his lambdas.To provide his infrastructure (auth, GraphQL API etc.), he used the AWS Amplify CLI which is a new CLI tool that can add cloud features to your web or mobile application. For example, by simply typing amplify add auth and amplify push, you have created your own user authentication provider (Cognito). If you want to set up a GraphQL API (AWS AppSync), you just have to type amplify add api followed by amplify push. This creates an API and stores its data on a DynamoDB database.To finally deploy his application, he simply used an S3 bucket that acts as a web server.After all, React just builds static HTML / CSS / JS files which need a web server (Nginx, Apache) in order to be displayed.Using an S3 bucket is very cost effective and doesn’t require anything else other than a bucket and correct bucket configuration.So no Linux configuration, load balancer setup &amp; costs, ….By using amplify add hosting and amplify publish, these steps are automatically done and it doesn’t require us to do anything else.You can also enable production mode, which enables AWS CloudFront integration to send your website to edge locations, so you can provide faster loading times to your users from all over the world.ConclusionAmplify seems like a really handy CLI tool if you enjoy working with AWS. The commands are very similar to each other, and really simplifies deploying your application or integrating your application with AWS components. A new feature that has been introduced to Amplify is Predictions, which provides a solution for Artificial Intelligence &amp; Machine Learning cloud services to let your application make use of. Overall, the future for Amplify is very bright and I can see it growing more as more developers adopt it in their current workflow. After all, who doesn’t like to just type one command and everything is taken care of?Breaking the Monolith: road to containerizing your app by Juan LageA monolith is basically one big application that handles everything that your project needs to do. From security to having all your services in one package. This is simple, has no over-engineering (on the architecture part), faster communication as everything happens in-memory and you have one code base. BUT, monoliths grow old as well, and the older it gets, the more code it has. They scale pretty poorly and are tightly coupled systems. Your developers are also required to learn more business logic and your deployments are all or nothing, meaning that, if your application goes down, your whole infrastructure is down, which isn’t interesting for an application where a lot of people depend on it.Dividing your monolith into a microservice architecture improves your workflow by splitting up your team to work on each microservice. This also helps your architecture in a way that each microservice has their own single responsibility. However, splitting a monolith is not a simple task and requires a lot of work to make sure you split it up the right way.Once you have your microservices, the goal is to containerize each service.FargateFargate is a service that helps you with this. It’s basically a serverless implementation for containers, meaning that it manages servers and clusters for you. Simply provision your container and Fargate does the rest for you by using Amazon ECS to provision your container on the cloud. It’s a really useful tool if you don’t want to manage your own server or cluster.Code base, config, dependencies, …The rest of the session was a very interesting take on how you actually can split up your monolith into microservices. It’s useful to have multiple code bases per service with their own deploy pipeline to deploy to your environments.It’s also a good practice to isolate your application from your dependencies. You should also explicitly say which dependencies you want and which version that you want. This helps you to build and create immutable images with explicitly defined dependencies, so you can run the same image in multiple environments.Your configuration is also very important in this. Configuration (database credentials, authentication keys, …) changes with each environment. If you hardcode those values, you will never be able to create an immutable image. It is important to externalize your configuration by storing it in your environment or by using an external service that stores your configuration. AWS Secrets Manager is a service where you can store sensitive configuration that your application can pull up when needed.ConclusionHe gave a lot more hints and associated AWS components that can help you achieve your goals in splitting up your monolith. Make sure that you take your time with this, as this is a very important factor in your project that needs to be done right. When done right, you will see an exponential rise in productivity and the addition of features and thus, making your end users happy with your service. AWS has a lot of services that can help you and you should take your time to get familiar with them.EndingWe ended the day with a Gouden Carolus, which is the most famous beer that is brewed in Mechelen. Overall, it was a very nice experience which I would love to attend again in the future. You get familiar with a lot of AWS components that you didn’t know before and they showcase a lot of services."
      },
    
      "agile-2019-11-25-agile-reporting-html": {
        "title": "Agile And Reporting - 101",
        "url": "/agile/2019/11/25/agile-reporting.html",
        "image": "/img/2019-11-20-agile-reporting/agile-reporting.png",
        "date": "25 Nov 2019",
        "category": "post, blog post, blog",
        "content": "  Thoughts and ideas about reporting for agile projects.Table of contents  Introduction          My situation      Context        Forecasting          What      Throughput Forecasting        Metrics          Velocity      Team Happiness      Sprint Spread      Cumulative Flow Diagram        ConclusionIntroductionMy situationThree years ago, Ordina presented me the opportunity to start with a new team for a new project, as team lead.Up until then, I was a programmer who participated in Scrum teams.Of course, I accepted the proposition and at the time of writing this article, our project is entering its last phase and approaching the production deadline.Over the course of these three years, I’ve learned a lot:  Managing a team  Coaching  Handling discussions  Making decisions  ReportingWhile the first four have been a challenge, the last one, reporting, remains a big challenge up to today.Which is exactly why I’m writing this, in the hopes that someone might learn from it.Another reason why I’m writing this is because I’ve recently been to the eXperience Agile conference in Lisbon, Portugal.I went to a talk of Doc Norton that was called Escape velocity. The talk was essentially about how and why velocity is incorrectly used as a forecasting and diagnostics tool.The day after, I also followed his workshop about the subject and so you’ll see references to his talk throughout this blogpost.ContextThe project started as an agile project, so the whole Scrum framework was used. But in hindsight, there were a few problems:  It would replace an existing product.  It could not go into production until the functionalities of the old product were implemented.  It had an enormous backlog of around 2.500 story points.This blocked a major feature of Scrum, being able to quickly release a product and gather valuable feedback from the end-users.We estimated the entire backlog to give management a rough estimation, 1.5 years of work with five developers and two analysts.Now, for those reading this and having done such an estimation before, you know that the 1.5 years was probably wrong. And it was.After about a year of development, we had to re-estimate the entire backlog which resulted in doubling the amount of story points to 5000. And right now, we’re at 6000.The backlog keeps growing, which is normal as we continue to learn about the business and the requirements.ForecastingWhatSo during the course of the project, I’ve had to report the progress to management.For this, I used the velocity of the team to generate a burnup chart so that they could see by what date we would arrive at the end. That’s called forecasting.But there’s immediately a problem there, velocity is a lagging indicator.It shows long-term patterns and can confirm such patterns - for example every December, the velocity would drop due to team members going on holidays.It is difficult to use velocity as a planning tool since you can’t see in the future. And it’s based on estimations of a backlog, which are subject to change.So the end date I had to report to management would typically move forward each meeting.Needless to say, they weren’t happy with that. Every single time, I had to explain the fundamentals of Scrum, the backlog, refinements, story splitting, etc…Throughput ForecastingA useful tool I learned with Doc Norton’s talk is the throughput forecasting tool.It’s an Excel file where you can enter the following data:  The min/max backlog/feature size.  The % of story splitting (hard to calculate, not required).  The start date (typically today).  The duration of your sprints.  The min/max velocity.  With an estimate.  Or with actual data from your project.The tool then runs 500 simulations where it randomly picks numbers between the minimum and maximum you provided. Those simulations are then used to give you a probability chance of reaching a certain date.The past few meetings I’ve started to use this tool and it helped me to provide a better, probable end date of the features.Let’s take an example. In our backlog, we have three features waiting, a total of 150 story points. Now, you have to estimate how many sprints it would take to finish and by what date.Assume we start 01-01-2019 and work with sprints of two weeks. The past five velocities were 60, 55, 40, 45, 51, averaging in 50. Three sprints, great!Estimated end date = 01-01-2019 + six weeks = 12-02-2019Now, let’s put that in the tool.The result:So there’s a 55% probability that the features will be done in three sprints.However, there’s a 100% probability that it will be done in four sprints. And that’s just for three features worth of 150 story points.MetricsDuring my role as team lead, I’ve also had to measure the performance and happiness of the team. To do so, it helps to have raw data to analyse.VelocityThe tool we use for managing our scrum process is Jira. It contains various reports that can be generated based on your agile board.But mostly,  I used the export to spreadsheet functionality.This provided me with a spreadsheet file with all the raw data of the entire backlog in Jira.With that spreadsheet, it’s just a matter of using the various functions to analyse the data. This gave me results such as the image below.I also asked all team members to enter their hours per sprint in another spreadsheet. Which resulted in the following:All that data can then be combined into a chart.As always with velocity, it tells us what happened in the past. But what does the chart above tell us?  The story points done per sprint are rising.  The last 2 sprints, the team did a very good estimation as they reached or surpassed their forecasted story points.  The relative velocity (what the team could have done if everyone was present) is always much higher          This could for example indicate that team members are often absent, which explains the difference between the velocity and the relative velocity.      What you do with this data is, as always, up to your team. It can for example help to determine the impact of certain decisions in the past.Team HappinessFor measuring the happiness of the team, I used five questions with a score of 1 (not great) - 5 (great) and used that data in another spreadsheet.Every sprint, the team had to fill it in and I gathered the data.As with velocity, this is a lagging indicator so you can see what happened in the past, but it can’t predict the future.But it’s useful to see patterns.For example, you can see happiness peaks in sprints where team activities were done. Difficult sprints create happiness lows, due to stress and pressure.The questions I used were:  Quality of meetings          Did we have too many meetings? Not enough? Were the ones we had meaningful?        Team collaboration          How did the team members work together? Were there any problems?        Correct focus          Did the team focus on the correct backlog items? Or did they do useless items?        Team flexibility          How well did the team respond to change?        Velocity team          Did the team achieve what they forecasted during the planning?        Created value          Did the team have the feeling that they produced extra value?      When you put all this data in a chart, it could look like the chart below for the last few sprints.If you use the last five sprints, it can help to locate problems in the team. The chart below shows that the team wasn’t happy about the achieved velocity. So it could help to find out why and avoid that problem in future sprints.If you do it for a longer period, it could look like this:It shows that sprint 7 was a very bad sprint, together with 13.The longer we worked together, the more stable the chart gets. Which is a normal evolution as everyone needs to get used to each other in the beginning.Sprint SpreadA chart that I started to use in the last few months is the spread of the various types of backlog items in sprints.The extracted data from Jira can be used to filter on types and then combine it in a chart.What does the image above tell?  Sprint 56          No bugs, but a lot of stories and a few tasks, as well as some stories that were rejected by the business        Sprint 57          Quite a few bugs and stories, mixed with some tasks and few rejected stories        Sprint 58          We did a lot of tasks due to a technical refactoring, with less stories and some bugs        And so onI find this very interesting to report to management, as it clearly shows that the team did a lot of work, even if that doesn’t translate in raw velocity.Cumulative Flow DiagramDuring the workshop with Doc Norton in Lisbon, I also learned of the cumulative flow diagram.This is a diagram that can be automatically generated in most Scrum tools, such as Jira.Unfortunately, I’ve not been able to generate a readable diagram for my project due to Jira malfunctioning, but for completeness, I will include it here as well.So, what does the diagram look like? (Information taken from Zepel)It basically shows you the process of an item from backlog to completed.And more importantly, it can tell you when there are problems in that process. The image above shows lines that go up evenly, with no drastic changes.The image below however, shows that the development line is straightening out, which means no new stories are picked up for development from the backlog. This could be due to a number of reasons, such as impediments or unforeseen complexity and generates a bottleneck. This creates hiccups in the value flow, and a lower flow rate in the next statuses, such as QA and Deployed.The chart does not explain how to fix it, but it does tell that there’s a problem.And so the Cumulative Flow Diagram is therefor a perfect tool for a scrum master to start a good discussion with the team, about flow, blockages and dependencies. Eventually this will most definitely lead to better flow management, more t-shaping, an optimized WIP limit… and a happier team!ConclusionReporting in an agile environment remains difficult. There are a lot of possible options, but it always depends on your specific situation.In more traditional, waterfall projects, reporting is a bit easier as everything should be known beforehand (although that produces other problems…).In this blogpost, I have given some examples of metrics and tools that I use to report progress and performance of my project, which work for me.As with all things agile, they might or might not work for your situation, but at the least, it might have given you some ideas!"
      },
    
      "conference-2019-11-18-devoxx-2019-html": {
        "title": "Devoxx Belgium 2019",
        "url": "/conference/2019/11/18/devoxx-2019.html",
        "image": "/img/2019-11-18-devoxx-belgium-2019/logo.png",
        "date": "18 Nov 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Keynotes  Talks  ConclusionIntroductionDevoxx Belgium is a Java community conference in Antwerp, Belgium.Every year in November it is organised and always has a great assortment of bleeding edge technology talks, methodology talks and architectual insights.During this edition, about 30 JWorks colleagues joined talks at Devoxx.Ordina again had a very popular booth by providing attendees nachos, the venue is a cinema after all.KeynotesThe Hitchhiker’s Guide to Diversity (Don’t panic!)Audrey Neveu gave this inspirational keynote.This talks, following the same format as the similarly titled book, provides a chapter by chapter overview of the struggles and issues with diversity in our industry and what steps our community can take to improve.The key takeaway is to be at least aware of the issue and to try to be sensitive to undertones in jokes and messages towards all members of our community, especially as they might not seem harmful at first sight.YouTube VODQualities of a Highly Effective ArchitectA typical Venkat Subramaniam talk.In a funny but straightforward way he explained the pitfalls, do’s and don’ts for architects.He provided a 12 point guideline that all architects should adhere to.‘Prototype beats any arguments’ and ‘Every technical decision should have an expiration label’ are just a few examples.This talk is a definitely must-see as he provides his view on how architects should behave in an organisation with the usual portion of humor.YouTube VODTalksRunning Kafka in Kubernetes: a practical guideThis is a talk by Kate Stanley from IBM.She works in the IBM Event Streams project.This talk provides a good overview of common pitfalls with setting up Kafka on a Kubernetes cluster.Katherine introduces all basic concepts, so following this talk is possible without a ton of Kafka and Kubernetes knowledge.She discusses how several Kubernetes constructs are used to support and enabled the usage of Kafka and Zookeeper.Liveness and readiness probes, Stateful sets, Pod anti-affinity, Persistent volumes and headless services are all discussed.Lastly Katherine discusses the different options for running and managing a Kafka cluster on Kubernetes.Both the IBM Events Helm charts and the Strimzi operator are shown and discussed.Strimzi is an operator that allows Kafka cluster to be created and managed through CRDs in Kubernetes.She also provided us with some tips about what you need to monitor when running your Kafka cluster:  Make sure to verify your brokers on the In Sync Replicas (ISR) metrics, especially when doing rolling upgrades so you will not lose any data.  Verify that your Zookeeper instances have reached a quorum.YouTube VODBest Practices to Spring to Kubernetes Easier and FasterRay Tsang is a developer advocate for the Google Cloud Platform.He is a Java Champion and works on improving the usability of GCP for Java use cases.Ray shows some tools and their use cases when developing Java applications on Kubernetes.Most tools are shown in a small demo.Discussed tools  Testcontainers  Java Memory Calculator  Jib  k3s  Kind  Skaffold  Graceful shutdown libraryHe also provides some common pitfalls when developing Java for Kubernetes.He refers to his talk about common docker pitfalls.Finally, he showed a demo of how an IntelliJ plugin, when combined with Skaffold, can allow a developer to debug a container running on an actual Kubernetes cluster, straight from their local machine.Side note, this is super cool!YouTube VODApache Pulsar 101: architecture, concepts &amp; benchmarksA talk by Quentin Adam and Steven Le Roux.Apache Pulsar is a pub sub messaging system built on top of Apache Zookeeper.Bookkeeper is a distributed writer log on which Twitter Manhattan has been based.Because Pulsar uses Bookkeeper as the underlying storage engine, its brokers are by design stateless freeing them up from having to deal with storage constraints.A broker can have different roles:  broker: this is the default option and can be compared in functionally with a Kafka broker.  proxy: this deals with external communication between the brokers.Currently a proxy just handles outside communication with the brokers, but there is a proposal PIP-46 to let it handle much more, including dealing with the Kafka binary protocol and MQTT.By default, a topic will not be partitioned, but if you want you can create these without any problem as you can see here.A difference with Kafka is that a partition is subdivided in segments, with the oldest segment containing the oldest entries.Pulsar feels like a nice mix between streaming and standard message queuing patterns, something worth looking into.YouTube VODObserving ChaosJuraci Paixão Kröhling is a SE at RedHat working on the Kiali team and a maintainer of Jaeger Tracing.He provides a brief overview of the basic concepts of a services mesh, its inner workings with a sidecar and their advantages and issues.Next, he discussed how different features of a service mesh can solve common concerns w.r.t. routing, resiliency, security, observability and distributed tracing.The last part of the talk is an extensive demo of the four tools/technologies discussed in the talk: Jaeger, Istio, Quarkus and Kiala.A demo service is deployed and a distributed tracing example scenario is shown and discussed.YouTube VODTop Tips for Terrible Tech TalksIn this talk by Chet Haase, he provides us with the guidelines for what makes a good technical talk for him.Don’t provide an agenda, your material should speak for itself and have a logical begin, middle and end.Make sure that you are not reading slides or even speaker notes, as at that moment you will loose the focus of the audience.A lot of hints also focus on simplification:  a small, catchy title  no walls of text  no complicated diagrams  simplify code, it doesn’t need to compileHave eye contact with the audience, but not with an individual as that might become creepy.Finally, learn how to project with your voice and movement, bring some power into your talk.Be in the room before your talk and have a chat with some of the audience, it can be pretty rewarding for the attendees, but most importantly the first words you utter will be the most nervous ones.Don’t follow a rehearsed script, do not fake emotions which you don’t have.But by no means skip on practice and preparation, as you need to own the material you are bringing.And most importantly, be real and have fun.YouTube VODWhat happens after your startup is acquiredThis was an amazing talk by Renata Tamasi, founder of Samebug and currently working at Rollbar.During this talk, she took us through the journey of running and later-on selling her start-up to Rollbar.She starts by showing the struggles they had to get the start-up going in Europe and what the pitfalls they uncovered during journey.Simply put, getting a SaaS based start-up off the ground and making is successful in Europe is almost impossible without the founders moving to the US.During their journey, Renata explained the different paths they tried with Samebug ranging from trying to raise funding for their start-up to finally getting acquired by Rollbar.This is a must see talk for everyone who wants to join a start-up and it will be very familiar for anyone who was in a start-up at one point in time.YouTube VODResiliency and Availability Design Patterns for the CloudSebastien Stormacq is a senior Technical Evangelist as AWS.Your system should still work when a failure occurs, even if it’s just in a degraded state.Your business should not rely on a single person being available and present.Geo redundancy is explained and how you can use availability zones (AZs) to have geo redundancy.He also explained a scenario where an AZ fails and a hot failover is performed using an Elastic Load Balancer.Always spread architectures over multiple AZs or use regionless services (S3, ELB) which are multi-az by default.Load shedding is a technique to drop all traffic once a service becomes loaded beyond a preset point of healthy load.This prevents the service from being overloaded and crashing.Shuffle sharding is a technique to reduce the blast radius for an outage by creating isolated compartments in the infrastructure.Finally Sebastian provides two very good resources:  AWS Well-Architected  Chaos engineering blogTakeaways:  Use Availability zones to achieve Geo redundancy  Use autoscaling mechanisms to have self-healing VMs  Test your backups  Use a split database setup. Split logically or functionally (read-write / sharding)  Set and sync timeouts  Implement backoffs for retries with jitter  Make sure database operations are idempotent  Use circuit breaks to prevent overloading an already degraded system  Implement health checks  Reject traffic when load is high on the service (load shedding and throttling)  Apply shuffle sharding techniques to limit blast radius of outage  Perform GameDays  Experiment with Chaos engineeringYouTube VODSSL/TLS for mortalsMaarten Mulders provides this talk for mortals.SSL is outdated and should be not be used anymore.Documentation can be confusing as SSL is sometimes used to discuss SSL and TLS setups.Only TLS should be used as SSL has been broken (cracked) multiple times.Maarten goes on to explain the basic concepts of public and private keys.He also explains all the mathematics behind the RSA key creation.This talk is ideal for people wanting to get a brief overview of how TLS works.YouTube VODBack to the Future: How 80s Arcade Games Taught me ClojureIn this talk Mey Beisaron explained how she learned Clojure.If you often learn a new programming language, it becomes much easier.For her latest position she needed to start using Clojure.A very good book to get started with learning Clojure is Clojure for the brave and true, which is actually a funny book.After the book the website 4clojure is a good source of exercises you can follow.But she still felt that that wasn’t enough to understand the language completely.Some years ago, for Java, she was able to learn it in one weekend, by writing a simple 2D game.Writing the game made her feel the need to use objects much more naturally.Not only that, with the game you also need other concepts likes threads, etc.When writing a game, do focus on a simple game, so you can truly focus on the language you need to learn.But Clojure did take more then a weekend, immutability changes everything, it messes with your head.In order to write a game there are several steps you need to follow:      Understand the 2d game structureIt should be pretty simple, identify the framework functions and the game logic.The framework functions deal with the outside world: network connections, loading resources, player input, …While the game logic identifies the win conditions.        Find code of a game written in Clojure.So you can learn from an existing source.  An interesting game library to use with Clojure is Oaks/Play.clj.      Map that gameFind the code which moves the character, which loads the assets, …        Add a featureStart coding        Write a new gameWrite your own game with the knowledge you have acquired.    Dream big, start with tiny steps!A nice talk brought with great enthusiasm.YouTube VODDefence Against the Docker ArtsJoe Kutner works at Heroku and is a core maintainer of Buildpacks.io.He starts this talk by providing a set of common pitfalls and best practises for creating Dockerfiles in Rapidfire.Multi-stage builds, combining multiple run commands into one and using proper base images for all docker builds are just a few examples.Joe then discusses the pros and cons of Buildkit and Jib as tools for building Docker images.Lastly, Joe explains what Buildpacks are, their origin, their use cases and what the architecture is.The different advantages of Buildpacks are extensively explained and the lifecycle of a typical Buildpack is shown.Pack is discussed as an easy way to use Buildpacks for a Spring Boot application.Joe demos the usefulness of Buildpacks with the use case of the HeartBlead CVE.This talk is recommended for an audience that is interested in building Docker images without having to deal with Dockerfiles.YouTube VODFlowable business processing from Kafka eventsTijs Rademakers and Joram Barrez are both employees of Flowable.The talk starts by providing a brief overview of Flowable as a product.They introduce the different standards that Flowable supports: BPMN, CMMN and DMN.Tijs continues by explaining the architecture of Flowable and its evolution.Next, Joram explains how (Kafka) events can be used in the Engine.He goes on by providing a demo of the platform using an example process.A flow diagram of the information flow and the use of Kafka and Flowable is shown.He also explains all code that is required to execute the demo process.This talk provides a good starting point for Flowable, even for people who have very little to no experience with BPM engines.YouTube VODAPI First development with OpenAPI - Should you practise it !? by Nick Van HoofOur very own Nick Van Hoof gave a talk at Devoxx as well.Nick is a Cloud Engineer at Ordina, currently working at Nike.In this 15 minute quickie, Nick explains why API first development is a good practise.He start by explaining the advantages through an example.Next, he goes on to show a short demo and some examples of an Open API design and tooling that can be used to ease the usage of Open API.This talk gives a good introduction into why you should embrace API first development and what tooling and benefits you can expect.YouTube VODA container platform for the banking industry - why, what &amp; howThis talk is presented by Wiebe de Roos, an CI/CD Consultant and Engineer at ABN AMRO Bank.Wiebe starts by sketching the scale of the project and their container journey.The different requirements are discussed and platform landscape was used.Next, he discusses what pipelines are used to provide the end-to-end CI/CD workflow.He continues to explain what measure they have taken to adhere to the hard audit requirements in their regulated banking environment.Container security, compliancy, monitoring and logging and their solutions for these requirements are shown and explained.This is an amazing talk, a definately must watch for anyone using Kubernetes in a regulated environment.YouTube VODHow to get hacked properlyThis talk is presented by Julien Topcu, a member of OWASP.Julien uses an example Spring Boot webapp to showcase a few common security issues.For each of the issue, he also presents and live codes the remediation.These exploits are all part of the OWASP top 10.This is a nice talk to get an interesting overview of a few common security issues and their fixes in Spring Boot.YouTube VODScaling Sustainable Innovation through Team AutonomyTom Stoepkens from Bol.com gives this talk.(Tech) Innovations should be used as a highly potent enabler for business goals.A common culture and goal is key to be able to use team autonomy.Tom provides an overview of how they automate and the technologies they use in his team.Next, he explains how testing helps with supporting sustainable innovation.This talk provides an insight into how Bol.com and especially the team of Tom actually uses automation to allow their team to innovate quickly.YouTube VODBetter software, faster: principles of Continuous Delivery and DevOpsThis talk is presented by Bert Jan Schrijver, the CTO of OpenValue.He starts his talk be providing definitions for CI, the two types of CD and DevOps.Next, he dissects CD into the required components and explains how these can be fulfilled.He defines all the different components and what the ideal setup would look like.It’s a nice overview of the requirements for a DevOps / CD organisation.YouTube VODBroken Buildtools and Bad Behaviors; The Maven StoryWe tend to keep doing things the same way because they were required in the past.Eventually something breaks and we go to Stack Overflow or we ask our colleagues and we find a fix or workaround,but how long are we going to use this workaround?These workarounds become a pattern and these patterns control people.“Works on my machine” is mostly caused by local code changes, different OS, JDK, Maven, Files and/or Properties.Maven manages files for your through the Local Repository. The local repository is a dependency cache without time-to-live and can be located at ${user.home}/.m2/repository.The local repository of Maven is actually broken.Maven 2 had a dumb cache, once something was downloaded it would stay there forever.Maven 3 introduced cached artifacts that got verified for existence with _remote.repositories.Artifacts that got deleted from Maven Central will be detected and fail your build.The Maven repository has checksums, but they are not being checked by default.You can enable this using -C for strict checksum checking causing a build failure or lax checksum checking using -c which will only warn for checksum mismatches.Are you running mvn clean install by default?You probably said “yes” like most developers.This a workaround for Maven 2 which is no longer needed in Maven 3.Dependency resolution in Maven 2 was broken.It was unaware of the reactor and dependencies had to exist in the local repository.Maven 3 is reactor aware meaning install is no longer required.I/O is slow, so logging, files and downloading the internet with Maven is slow as well.As logging is slow, you should aim for a clean Maven output.So use your logging framework and do not use System.out or System.err.Disable logging during testing (using loglevel=off) and in the extreme run Maven with -q and –quiet which will disable all output and just tell you when it’s done.When you enable debugging on Maven you’ll notice your build time will increase even though nothing has changed.As files are slow, we should not use mvn clean.Most of the Maven plugins are aware if they must execute their task.The Maven CI extension (stands for Maven clean install extension) which will register clean and install when you keep executing clean install, your build will eventually fail as this is a bad practice.Properties resolution are mostly defined in your pom.xml, but also in your settings.xml and system properties.By adding a parameter to mvn deploy -Dspring.version you can change your deployed Spring version.This is however not limited to Spring.You can also set this for the Java version, but please NEVER EVER do this.Versions could also break your builds due to conflicts with branch merging.You should isolate versions and make them unique.Maven 3.5.0 introduces CI-friendly placeholders which allow you to use placeholders for revision, SHA1 and change list, but multi-modules require a hack to rewrite your pom.To resolve this issue, there will be something new called maven.experimental.buildconsumer which will land in Maven 3.7.0 and it will require Java 8 or above.Treat your pom as production code so remove dead code like unused dependencies, plugins, properties and reports.Keep improving and locate and optimize time consuming processes, automate as much as possible and keep having fun.Be like a responsible pilot and try to understand every plugin and think if you still need it.Stay curious and keep yourself informed.Watch out for questions with outdated answers (on Stack Overflow) and find information at the source.Maven is up for grabs.About 60~80% of Java project/developers use Maven.About 95% (sub)projects have just ~5-10 active volunteers, there is no company behind this project.If you feel like contributing to the Maven project check out Maven’s up for grabs.YouTube VODConclusionOne of the takeaways from this conference is that Kubernetes now is the de facto deployment platform for Java applications moving forward.During the conference a lot of tools, Jib popped up a few times, and integrations with IDEs have been showcased and every talk seems to assume that the deployment platform is Kubernetes.Event-driven talks are also in a lift.There were quite a few talks about Kafka or talks about software that can integrate with Kafka.Finally, the conference was a great success again.Ordina handed out tons of nachos, every attendee has caught up with the trends in our community and all vendors have been able to showcase their newest products.See you next year, Devoxx people!"
      },
    
      "conference-2019-11-14-oracle-code-one-html": {
        "title": "Oracle Code One 2019",
        "url": "/conference/2019/11/14/oracle-code-one.html",
        "image": "/img/oracle-code-one-2019/code-one.png",
        "date": "14 Nov 2019",
        "category": "post, blog post, blog",
        "content": "  Oracle Code One is the successor of the well-known JavaOne conference, organised each year in San Francisco, California.Although still quite Oracle- and Java-heavy, the rebranding (and collocation with Oracle Open World) means that there were also a lot of talks about other programming languages, developer tools, … Our colleague Tom van den Bulck and ex-colleague Tim Ysewyn were invited to give their Workshop on Kafka Streams and enjoy the rest of the conference.In this blog post we share our impressions and experiences.    Table of contents  Keynotes  Usual Suspects  GroundBreakers Hub  Using Istio and Kubernetes to Build a Cloud Native Service Mesh  Monitor Kafka Like a Pro  Distributed Tracing in Kafka  DevOps Theory vs. Practice: A Song of Ice and Tire Fire  Progressive Web Applications VS Native  ConclusionKeynotesNext to all activity in the conference center, workshops and talks, of course there were (a few) keynotes.These took place in a separate great hall and we summarized the best ones below.Java Keynote: The Future of Java Is NowAt the Java keynote which opened the conference on monday a few speakers were invited. These talks mostly dealt with outlooks on the future. Firstly Jessica Pointing talked about the current state and prospects of quantum computing.She gave an explanation of what Quantum Computing actually is, and how modern computers can consist of special components (equivalent to transistors in classical computers) to take advantage of the special properties of so-called qubits. A qubit or quantum bit is a unit which can exist in a superposition of states, which means it can be 0 and 1 at the same time, as opposed to a classical bit, which can only be one or the other. You can use these properties to devise really specific algorithms like Shor’s or Grover’s algorithm which can solve certain problems like prime factorization much faster than classical algorithms (here’s a really good explanation on how Shor’s algorithm can break encryption on Youtube).Keeping the title of the keynote in mind, one of the most important questions is “When will we have Quantum Computers?” and as you’ve already guessed the answer is indeed “Now”.A fun part of this talk was that there was a live link for the audience to answer a poll to estimate how far along the technology stands in this area. Most of the audience correctly guessed that at present we have quantum computers which consist of 10-100 qubits. Current quantum computers do come with the caveat that they can still only keep their state for a short time.The advances which still need to be made mostly consist of making sure the quantum properties of the qubits can be kept intact for longer before we can really speak of sustainable quantum computers.Nevertheless Jessica really made the point that the age of quantum computers is in the near future if not now already. She even showed a GitHub repository which enables everybody to experiment and writealgorithms or programs which can theoretically be run on quantum computers (in Java of course).    Next Georges Saab, Vice President of the Java Platform Group at Oracle, officially announced what was already expected, namely that Java 13 was out as of that day:Java 13 is live: https://t.co/YqCVg3CTSg&mdash; Brian Goetz (@BrianGoetz) September 17, 2019After that he invited a lot of people from different backgrounds on stage (or on video) to talk about how they experienced the new, faster release cadence of the Java language, which means there is a new version ofJava out every 6 months. These were people who worked on Java at Oracle, with Java on Open Source projects, were part of the JCP, …The conclusion of most of these testimonials was that almost everybody was quite suspicious at first about the idea to do a release every 6 months but that all were pleasantly surprised by how well it seemed to work and that they urged everybody to upgrade to the latest versions as soon as possible because there are almost no downsides to it.Lastly it was Brian Goetz’ turn to talk about the future of Java, in which he demonstrated some of the new language features which have been included in the releasessince Java 10, as well as previewed some upcoming stuff. He also gave an extended version of this talk later.Code One Community Keynote: Game OnOne of the less serious but certainly entertaining talks was the community keynote, in which a lot (and we do mean A LOT) of well-known people in the Java community (Java Champions, developer advocates, JCP members, …)came on stage and acted out a history of the Java language as it has been used in the past. The through line was the idea that Java has been used through the years to help develop video games and it continues to be relevant today thanks to a host of new projects which are springing up everywhere.It was a bit of a silly show but it was good fun and at the same time interesting to see how far the Java community has come. At some point, Henri Tremblay even did some livecoding on a VM running Windows ‘95 while chatting to his past self via a chatbot, which was truly impressive.There was a lot of buzz on Twitter about the event:AN OUTSTANDINGLY HILARIOUS #CodeOne #community KEYNOTE - #JUG leaders @Java_Champions @groundbreakers celebrated the grande finale with shirts @starbuxman @venkat_s @brjavaman @eMalaGupta @RafaDelNero @_tamanm @neugens @miragemiko @dervis_m @nljug + #usualsuspects @OracleCodeOne pic.twitter.com/I1gCVrfVVC&mdash; Benjamin Nothdurft (@DataDuke) September 19, 2019 @venkat_s live coding in an angry bird suit at the community keynote 😂 #CodeOne2019 pic.twitter.com/TUtVADuhCx&mdash; Billy Korando (@BillyKorando) September 18, 2019 Game on community keynote #codeone thanks to the participants! pic.twitter.com/la6vOYy2Xg&mdash; Oracle Code One (@OracleCodeOne) September 18, 2019 Usual SuspectsAs always, there are a few speakers at every conference who are household names and who always deliver.Some of these you can also meet and experience in Belgium at Devoxx or at our very own JOIN conference, like Benjamin Nothdurft.These talks are always very interesting and of a high standard, therefore we will only highlight them briefly.Venkat Subramanian gave no less than six talks at the conference, two of which we were able to attend.These were about A Dozen Cool Things You Can Do With JVM Languagesand “Functional Programming Idioms in Java” in which Venkat shows you things which are really useful but look so obvious when he points them out in his incomparable style.Mark Heckler talked about How to Use Messaging Platforms for Scalabiliy and Performance and did an impressive live demoin which he built an application which produced, transformed and consumed events in three seperate projects in less than 15 minutes, using Spring Cloud Stream.Furthermore, Josh Long talked about The Reactive Revolution and “Bootiful Testing” while Stephane Maldini gave us an overview of the reasonswhy you should (or should not) start using reactive programming.Groundbreakers HubIf you wanted a break from the (literally hundreds of) talks, there was a large area called the Groundbreakers Hub where a lot of like-minded tech enthusiasts could gather and discussthe ongoing conference (in some cases accompanied by a Blockchain Beer).As always, there were booths of all big representatives at which you could gather information about new products, do some demo coding or collect some nice gadgets. There was also a cornerwith arcade game consoles to relax, and also a Raspberry Pi supercomputer on display.A few impressions on Twitter:Shout out to all @OracleCodeOne @oracleopenworld attendees. The real action with the fun people is in the @groundbreakers Hub. Come join us and remind yourself of why you got into tech in the first place 🤓 All are welcome 🤗 #CommmityFirst pic.twitter.com/W1rQgH8oWv&mdash; Vincent Mayers (@vincentmayers) September 17, 2019 LIFE IS GOOD at the @groundbreakers hub at @OracleCodeOne - we have arcade machines, @hackergarten, a beer blockchain, a 1k #RaspberryPi cluster, escape rooms, code cards and IoT devices. Let&#39;s #BreakNewGround #CodeOne pic.twitter.com/p5PI4q1XI4&mdash; Benjamin Nothdurft (@DataDuke) September 18, 2019 Kafka Streams WorkshopThe workshop we gave ourselves was once again a success, with about 30 people from around the world attending. An explanation of what the workshop contains has already been given in a previous blogpost.Using Istio and Kubernetes to Build a Cloud Native Service MeshAnother workshop which we followed (which was guided by Ray Tsang) showed us how to easily set up an Istio service mesh on a Kubernetes cluster on Google Cloud.What is Istio?  Istio is an open platform-independent service mesh that provides traffic management, policy enforcement, and telemetry collection.  Open: Istio is being developed and maintained as open-source software. We encourage contributions and feedback from the community at-large.  Platform-independent: Istio is not targeted at any specific deployment environment. During the initial stages of development, Istio will support Kubernetes-based deployments. However, Istio is being built to enable rapid and easy adaptation to other environments.  Service mesh: Istio is designed to manage communications between microservices and applications. Without requiring changes to the underlying services, Istio provides automated baseline traffic resilience, service metrics collection, distributed tracing, traffic encryption, protocol upgrades, and advanced routing functionality for all service-to-service communication.It was surprisingly easy to set up an entire Kubernetes cluster with service mesh in less than an hour, after which there was still time to try out some of the nice features of a service mesh, like setting upspecial routing rules to enable Canary deployments or A/B testing scenarios, fault injection / circuit breaking, tracing, … Which can all be handled by Istio itself without making anychanges to your application (except for propagating the headers which are required for tracing). Also a lot of added on applications like Prometheus, Grafana, Jaeger, … all worked straight out of the box.For the workshop we received special temporary accounts but if you use your own account (which has a free trial option) you can certainly try and follow the steps in the very extensive Slides and Read-throughto set this up yourselves.Quarkus Trivia NightAll work and no plays makes us dull boys so there was also time for some relaxation. We were invited for a Trivia Night organised by OpenShift and Quarkus at a brewery close to the conference center, which was a nice change of pace from all serious presentations.Good food and beer at the @QuarkusIO trivia reception #CodeOne pic.twitter.com/zUKamBK08L&mdash; Jaap Coomans (@JaapCoomans) September 18, 2019Monitor Kafka Like a ProA subject close to our own workshop was presented by Viktor Gamov, a developer advocate for Confluent.He talked about best practices for monitoring your Kafka clusters in a very interesting presentation.The exact slides were not made available but are very similar to these.Some basic monitoring is just to verify that your producers and consumers are able to read and write data from Kafka.You can set up a specific topic on which you can just send a message and consume it again every 15 seconds, then raise an error after for example four failures et voila, you have basic monitoring.Measuring latency is already more advanced, but every time you experience performance issues, always check your latency because it might be a waste of time to start looking into Kafka errors or your own service when your network connection is acting up.In order to check if your brokers are up, just execute a grep java on the brokers.Kafka exposes a ton of JMX metrics, which you can export with Prometheus, the Confluent cloud uses the same mechanism to acquire metrics.The most important metrics to monitor are to verify the state of your partitions:      kafka.controller:type=KafkaController, name=OfflinePartitionsCount Offline partitions is an actionable metric, as this indicates that a partition has no leader and thus is no longer readable or writeable.        kafka.server:type=ReplicaManager, name=UnderReplicatedPartitions: Underreplicated partitions is the most important metric, something is cleary wrong. Whenever there are no longer enough replicas of a partition in sync you can no longer produce messages to that topic.This corresponds to the configured min.isr.partitions of a broker.        It is also important to verify if your brokers have enough resources like: CPU, Bandwidth, Disk, …Always monitor your disk usage, always.        Other metrics which are pretty important:kafka.controller:type=KafkaController, name=ActiveControllerCount: this indicates wether an Active Controller is present.This is the most important node of your kafka cluster.1 = OK, 0 is NOT OK, 2 is VERY NOT OK.        kafka.server:type=SessionExpireListener, name=ZooKeeperDisconnectsPerSec: Zookeeper disconnects.        kafka.controller:type=ControllerStats, name=UncleanLeaderElectionsPerSec: the rate at which unclear leader election occurs, though this option is disabled by default.        kafka.server:type=ReplicaManager, name=IsrShrinksPerSec: When a broker goes down the ISR will shrink, when it comes back it will expand again (kafka.server:type=ReplicaManager, name=IsrExpandsPerSec).        kafka.server:type=KafkaRequestHandlerPool, name=RequestHandlerAvgIdlePercent / kafka.network:type=SocketServer, name=NetworkProcessorAvgIdlePercent as this verifies how often your request and processor threads are idle.        For your consumers an important metric is records-lag-max, because when this is growing it will indicate that your consumers are lagging behind and can not process the same amount of messages as your producers are producing.        It is also important to set a performance baseline for your producers and consumers.This allows you to verify configuration changes you have made and their impact on your Kafka system.        Finally, you should also pay attention to old producer and consumer versions, as these force the broker to convert messages between these versions everytime, which will have an impact on the heap memory usage.  Victor also pointed us to a nice tool to use when you want to profile your java application: async-profiler.Besides creating cool flame graphs, this tool can inform you about what your java application is doing and where you might have a problem.Distributed Tracing in KafkaThis was another workshop by Viktor Gamov, of which the recording and slides are shared.Over time the complexity of your system and subsystems starts to grow and it becomes too complex to contain all the logic in your head.In the past, each system would have its own monitoring system, every system was different in the how, what and when something is monitored.Troubleshooting therefore could take hours, days, weeks, …Observability is just a fancy word for monitoring because monitoring sounds boring to devs.Cindy Sridharan wrote a nice book about distributed tracing.She also writes blogposts like Distributed tracing - we’ve been doing it wrong.From her book come the pillars of observability:  A log: A raw, immutable sequence of events of a single instance of a service.  Metrics: Numerical measures aggregated at a given point of time, sourced by your log events. These tend to be actionable.  Distributed tracing: Allows us to have a total picture of a request within the application. This answers which services are involved and where there are failures.Kafka tends to become the de facto standard to handle data within organizations, becoming the central nervous system of any company, which makes it more and more important to include Kafka within your tracing.Some ways how you can implement tracing in kafka are:      OpenTracing Java API for Kafka can be found on https://github.com/opentracing-contrib/java-kafka-client        Another approach is to use interceptors like https://github.com/riferrei/kafka-tracing-support  Tracing can be visualized with Jaeger UI.DevOps Theory vs. Practice: A Song of Ice and Tire FireOne of the last, slightly more light-hearted talks took place in the last afternoon of the conference. Baruch Sadogursky and Viktor Gamov openedtheir talk with a skit in which Baruch spouted well-known DevOps clichés, like “Everything must be 100% automated”, “We do Continuous Security well” or “Your problems are so unique, no vendor can possibly understand them”while Viktor translated this into the harsh truth that none of these are absolute or applicable in every situation and you have to adopt your approach accordingly. While entertaining, it also laid bare the truth that none of thesedevops principles act as a Silver Bullet.Next they explained the idea of the Cargo Cult, a term which originated in Oceania when the people there, after the Second World War was over, started building airplanes out of straw.They believed that this was the cause of the technological advancements and prosperity (cargo) which arrived on these islands at the same time as the actual airplanes, and that when they rebuilt these planes, more cargo would automatically follow.The analogy with the modern tech landscape, is that some people might believe that by merely adopting the latest techniques and tools (Kubernetes, Docker, DevOps, …), all other problems will automatically solve themselves.From experience, we all know this isn’t true.Instead, we should analyse where we are as an organisation and determine which steps to take to progress to where we want to be before just adopting something like Kafka because people at conferences give nice demos and workshops and tell you all will be fine. Before we take a decision like this, we first need to be aware of the answers to The Four Questions:Answering the last two questions requires an entirely different talk, but the first one we can try and tackle using a maturity model, popularized by Martin Fowler.Maturity models have their own share of problems and critics, but Baruch and Viktor pose that, if done right, they can help your organization or team make signifant advancements towards being ready to tackle your business problems in a more efficient way.Now the question is “How do you write a good maturity model?”. Using an example, simply created in Excel, Viktor and Baruch try to explain that the most important things totake into account for a maturity model are:  It should describe a process, not an end goal.  It should be tailored specificly to your case, not exactly as prescribed in “the book”.  It should focus on outcomes, not specific tools to use.  It should constantly be evaluated and evolve, not written once and then forgotten.The answer to the question “Is it even a good tech?” can be answered by advisory / research companies like Forrester or Gartner but because these are for-profitcompanies their suggestions might be skewed towards the companies which are already established / rich or more in the limelight. A better option might be to lookat the Thoughtworks Tech Radar which is publicly available and provides an assesment of a whole array of moderntechnologies and determines if you should Adopt, Assess, Trial or Hold these.Moreover, you could argue that it’s even better to extend the question to “Is this even a good tech for our team”, in which case it’s a good idea to build your own tech radarwhich at JWorks we also did.Progressive Web Applications VS NativeOne of the few frontend (but still a bit Oracle) related talks was about PWA’s, a topic which we are also starting to explore at JWorks, given by Marta Hawkins.Ionic gives a good definition of a Progressive Web Application:  A Progressive Web App (PWA) is a web app that uses modern web capabilities to deliver an app-like experience to users. These apps meet certain requirements (see below), are deployed to servers, accessible through URLs, and indexed by search engines.The premise of the talk was mainly about comparing the capabilities of a PWA compared to a classic native application, and in most cases refuting the arguments which could be brought against PWA’s.The first such argument is that you cannot access all native phone functionality, but recently the support for almost all of these functions, like taking pictures, watching videos, using location information, using gestures, … have been made available for PWA’s.All major browsers are now also supporting almost all PWA functionality. The next obvious argument is that if you choose to write a native application you have to maintain multiple code bases,whereas you only need to write one application and distribute it independently of app stores in the other case. This also means you can release new functionalitywithout having to go through the process of getting it approved on the respective app stores, and your application will be more easily discoverable on your search engine of choice. All of these arguments result in the fact that PWA’s are better to drive user engagement.Next Marta talked about the preconditions you need to have your web application recognized as ‘progressive’, of which there are only three:A service worker is what provides the PWA with its special capabilities.This is what enables offline functionality, installability, notifications, request caching, … Google’s developer guide describes it as follows:  A service worker is a type of web worker. It’s essentially a JavaScript file that runs separately from the main browser thread, intercepting network requests, caching or retrieving resources from the cache, and delivering push messages.Of course not everything is sunshine and rainbows, so next Marta also warned for some pitfalls they already encountered in the past:  Browser support is good, but not complete. Chrome is the frontrunner and Safari is notably lagging behind.  The same is true for the different OS’s. Android treats PWA’s as first class citizens, on iOS you need to really go and find the ability to install the app to your home screen.  Developing and debugging the applications is also not trivial. Chrome is developing and delivering better debugging tools.  Caching is never trivial, but especially in service workers it sometimes seems to behave in mysterious ways.Slides of this talk are also available.ConclusionIn conclusion, we had a great time in San Francisco (and also at the conference).It was noticeable that Oracle products were given preferential treatment, and the fact that it is so big means that sometimes it was not easy to keep track of which talks were interesting to follow.On the other hand, almost all the talks we followed were of a high standard and we learned a lot.And the Groundbreakers Hub was an awesome place to connect with other developers.10/10 would go again."
      },
    
      "conference-2019-11-12-experience-agile-2019-html": {
        "title": "eXperience Agile 2019",
        "url": "/conference/2019/11/12/experience-agile-2019.html",
        "image": "/img/2019-11-12-experience-agile-2019/ExperienceAgile2019.png",
        "date": "12 Nov 2019",
        "category": "post, blog post, blog",
        "content": "This year, it was already the 6th edition of eXperienceAgile, organised in Portugal, Lisbon by Radtac and Hugo Lourenço - a DevOps and Enterprise Agility Advisor. This global conference focuses on gathering wisdom and best practices on business agility as well as technical agility, and therefor it was the perfect conference for JWorks Agile &amp; DevOps practitioners and enthusiasts.The first day contained fourteen talks on team and business agility with speakers from Europe and America, whereas the second day contained DevOps talks and deep-dive workshops with some of the speakers.@experienceagile / #xa19 / experienceagile.orgTable of contents  Half-Agile:  5 Leadership Mistakes Diluting Your Transformation, by Jesse Fewell  The Toyota Flow System, by Nigel Thurlow  The Agile Futurist, by Mario Moreira  Escape Velocity, by Doc NortonHalf-Agile: 5 Leadership Mistakes Diluting Your Transformation, by Jesse FewellJesse shares with us five patterns on our journey to an Agile organisation. He explains why this journey can be so hard, especially for leaders (team leads, scrum masters, technical leads, managers or executives).You start with a good decision, then for some reason you get frustrated and agitated. Once you discover the mistake made, you can take action to correct it.Buy-in, EngagementVery often you have a good idea and just tell the people around you to execute it. But along the way you forget to explain the reasoning behind this idea, which leads to misunderstanding and frustration.As leaders it’s your job to create context for the change and communicate methodically and excessively. This is hard, because it takes time, repetition and requires patience.ImpactThe first good step, as a leader, is to take initiative. Other departments may not want to be Agile, so you will start alone. You’ll be the role model to set the tone for the organisation.However, this leads to a team trying to be Agile in an environment that is not Agile. And you end up doing Water-SCRUM-fall or Modern Waterfall.So you need to take the next action: invite others to the table.If you want the Agile journey to move forward it has to be about Agile, not just YOUR Agile. Allow the journey to evolve instead of following a fixed methodology.PracticesYou often tell colleagues to follow standard techniques, including daily stand-ups, spring planning, retrospectives, etc… These are practices that everyone uses. So, if you want to be like everyone else, you should definitely apply these techniques.Let me tell you a little secret: if you try to be like everyone else, you’ll never be ahead of everyone else!You need to experiment, try things out. Not just do things because others do them or because you read them in a book.You need to find YOUR Agile. Don’t just copy-paste your Agile.WorkloadEverybody is struggling with more complexity, more change and more requirements, but there are loads of good opportunities and you want to do all of them.People get overwhelmed by this, because there is so much work and now we want to do an Agile change or transformation.You should try not to over-commit.Find your One Thing that you can focus on, so that you can make more progress with the same effort.Learn to say NO, with diplomacy.TalentWhen you want to go on an Agile journey, you want everyone else to get better. As a leader, you take the role of a mentor. You show and teach that you are Agile and they need to follow you.But you get frustrated, because they are not changing fast enough. You feel like you are Agile, but they are not.Because you forgot about yourself as the leader. If you want to transform your organisation, you need to first transform yourself as a leader in the organisation.You need to challenge your own assumptions. Take a leadership training or workshop and get inspired.Maybe you need to think about:  delegating more instead of doing;  shifting from explaining methods to articulating goals; or  instead of telling people what to do, give them the ability to do it themselvesThis personal growth requires that you go on a long-term journey of transforming yourself as a leader.The Toyota Flow System, by Nigel ThurlowWe all know that Toyota has always been a frontrunner with regard to Lean thinking, bringing the customer in the center point of attention and establishing flow in the value creation pipeline. The Toyota Production System (TPS) - with its two pillars: continuous improvement &amp; respect for people - as developed by Taiichi Ohno and Eiji Toyoda, is globally recognised as a strategic and human centered foundation to become a Lean and thriving enterprise. On this foundation ‘the Toyota Way’ has been developed as a philosophy with a set of principles and behaviors that can guide companies through their change and towards the right mindset.Recently, with the Toyota Flow System (TFS) the Toyota thinkers take this strategy yet a few steps further with the purpose of being an inspiration for companies with human centricity, continuous improvement in their strategic roadmap. Together with co-facilitator Dirk van Goubergen, Nigel Thurlow - Chief of Agile, Toyota Connected - has given us a hands-on introduction to the extra dimension that this evolution entails.Building on lessons learnt and extensive researchNigel Thurlow puts it this way:  There are fantastic lessons to be learned all around, from Systems Thinking to Scrum. The body of knowledge at our disposal is immense, and there are countless ways to apply it. But to truly leverage this wealth of information, we must change the narrative and realize that context is everything: different environments call for different approaches and tools, and with something as complex as a business transformation, sticking to a one-size-fits-all methodology is dangerous. Instead, we need to be able to mix and match the techniques that best suit our situation, while ensuring that key aspects of a transformation are considered.”Get the basics rightWith the Red Bead experiment, we experienced that we need to verify that companies apply the right kind of Lean, and not the evil version, when trying to eliminate waste. Waste is not caused by the workers most of the time, but by the system. Failing to recognise this can bring leadership to take the wrong decisions and apply counterproductive measures - like hiring or firing employees, or introducing contradictory KPIs among workers, thus creating steady erosion of psychological safety within teams - and in the meantime not tackling the actual process issues in the pipeline.Three questions can help us define waste, explained by Dirk:  What is value for the customer?  What products are we making?  Which activities contribute to this result?Listing activities and highlighting the ones that are simply uselessly getting tired is a very important exercise for every contributor in the process, whether it be workers in a factory, developers, testers, consultants, or third parties or managers. Show respect for people by allowing them to focus on value adding activities. Since the management and people with leadership roles often organise the system, it is key they recognise this responsibility.Once the activities are inspected and the process of value creation is established, the next step is to enable flow, by designing your value stream like a river, allowing the value to grow without interruption, without waiting time or blockages. Here again, managers have a task to focus on not being a flow stopper. It is important for them to acknowledge that resource efficiency can kill flow efficiency. Just like a train company should focus on timely arrival and departure and not on getting as much travelers in one train by waiting until it’s completely full before taking off again.For Toyota, as well as for any development team, the key is to ensure a flexible workforce, with engaged cross-trained employees that have a thorough understanding of the value adding processes throughout the company. This enables them to flexibly partake in flow efficiency whenever and wherever needed.Toyota Flow System, the DNA for organisationsThe TFS model aims to sustain the flow of value to the customer. And Toyota offers a body of knowledge that helps companies to understand the different aspects of customer first value delivery with a systemic approach to optimize for the whole, and not the subparts, or silos.These aspects - visualised as three pillars - are supporting this motto and form the DNA of the Toyota Flow System. But let it be clear that these three cannot exist without each other. The helix structure of the pillars points to their intertwined importance, making the structure trustworthier and stronger.  Complexity thinking: understanding uncertainty and complex adaptive systems  Distributed Leadership: the behavior patterns of those who lead people and teams  Team Science: the science of teams, their interdependencies and interactionsFor every pillar, Toyota has listed a number of possible theories and practices that need to be considered when trying to achieve the motto. For every context, the combination of selected theories, practices and models can be different. By giving this list of researched and tested material, they offer a good starting point for companies on a transformation journey, allowing them make the founded decisions within their own particular business context.Take a good look at what Toyota suggests for every pillar. There sure is a lot to discover!Key takeaways were:  Teams need to learn how to do team work and be more than the sum of the team members. Teams do not have the set of skills and behaviors automatically, and so they need coaching to focus on team communication and collaboration aspects as well as team personality dynamics.  Intent based leadership - as coined by D. Marquet - helps you to move authority where the information is available. It is about designing an environment where people give intent to each other and they feel valued and proud of their work. It is about actively giving control to people who maintain the information so they can make informed decisions.  Scrum is disciplined PDCA - the Plan/Do/Check/Act (or Adjust) approach from William Edwards Deming  Beware of failure demand, which is waste disguised as value for the customer and service excellence.  People are spending the best years of their lives in companies.. take up the responsibility to make it as enjoyable as possible.More reading: https://planet-lean.com/introducing-the-toyota-flow-system/.The author of the mentioned article wraps it up saying:  This is the first time someone has brought all these elements together and made sense of them, explaining how they fit within a company like Toyota. Complexity thinking is a change in mental models and management practices. The Toyota Flow System is the first to externalize it with tools and in a contextual setting. The next step will be testing it in the field in different contextual settings, to see what works and what doesn’t. It will be exciting to see how it evolves, which we are sure it will. That’s the beauty of it.The Agile Futurist, by Mario MoreiraThis was an energizing talk, giving us a view on the trends in the Agile movement for the coming ten years. Mario Moreira is an enthusing and influential Agile transformation Leader, Agile enterprise coach and change agent who has written four books a.o. Being Agile: Your Roadmap to Successful Adoption of Agile, and The Agile enterprise: Building and Running Agile organizations.What is Agile?… is what he asked the eager crowd.  It’s a set of Values and Principles  focused on the delivery of value to the customerWhat is Agile not?… on the other hand.  a certification  a tool  a silver bullet  a process  it’s also not merely redefining roles  and… it is definitely not undisciplinedAgile trends for 2029Bring Back the BasicsWe will all go back to the core, to rediscover and be inspired by the central purpose and initial intent of the Agile movement.Returning to the core implies that we need to make sure we understand the heart of the Agile manifesto, that we try and urge to live the principles and lead with the items on the left in mind.Stop doing Agile for Agile’s sakeAgile is not the goal or outcome. So let’s stop talking about it as if it were.Rather we should look at the purpose for applying Agile.  Better interactions  Better working software (or services and products for that matter)  Better customer collaboration  Ability to respond to changeAnd people will gradually focus on better business outcomes:  Deliver increased Customer Value  Optimize the Flow for faster delivery  Increase Quality with Feedback LoopsLeading with uncertaintyBecause leading with uncertainty is the smarter thing to do, in a world where we - as customers - don’t always know what we want, we - as product creators - don’t always know exactly how to build the product, and where things and people tend to change along the way.Uncertainty requires the right behavior and mindset to tackle it: e.g. cultivate and kindle a discovery mindset, experimental and incremental thinking, and implementing feedback loops.We all need to practice and keep on practicing, to walk the talk, and eventually lead by example.Agile throughout the enterpriseThe way organisations are structured today is a remnant of what was required in the industrial age. But focusing on the customer and tracking the value stream has led to insights on how work and collaboration should be organised in a smarter, more valuable way, for all parties involved. Working together cross-departmental with an empowered team, focusing on a common customer inspired goal, helps to limit handovers, approval flows and delayed communication. To make this happen, every person who’s directly involved in this end-to-end value stream, from any silo throughout the company, needs to be allowed and be able to work dedicatedly, and transparently as well as make local decisions on matters that require her/his expertise.This way of working depends upon efficient alignment and trust… in people and their abilities, to make the magic happen. Trust is not given, it should be a given that people perceive and recognise, from the moment they first enter the company building.When trust is present, people take decisions, and cultivate ownership.Solve Holistic ProblemsThe sky is the limit. And it always has been for the intrepid.No business domain or sector is simple and predictable enough to allow it to acquiesce in the industrial way of working.With IT and AI permeating every nook and cranny of this global society, and with companies being ushered by market disruptions to the verge of survival, we need to realise that focusing on value creation and optimizing the whole (the team, the company, the system) to service the customer is the smartest way forward.Seeing and understanding the bigger picture of what we - as a team, as a company, as a sector - deliver to society is the next step.From then on we will be solving holistic problems, in a holistic way. With our thriving society as the customer.Escape Velocity, by Doc NortonA recurring problem with agile projects is reporting. When you search for information online, everyone seems to encounter the same problems.  What metrics should I use?  How can I make the team’s performance visible?  How can I identify possible bottlenecks?  How do I forecast?The most common metric that everyone uses is velocity. And, as the title of this section already indicates, that’s not always a good idea. Doc Norton had a good talk at this conference as well as the accompanying workshop the day after.In short, velocity is a lagging indicator and thus not good for predictions. Now, what does this mean, a lagging indicator? It indicates data from the past, it lags behind.Another problem when using only velocity as a metric is that it tries to explain a complex system. A dip or peak in velocity doesn’t explain anything, it’s just an indication that something might be wrong. To find the real cause, more metrics are needed.So as Doc said:  Velocity is a lagging indicator of a complex system.And as a result:  Velocity is not good for predictions and not good for diagnostics.PredictionsEveryone has already experienced the typical management question during the lifecycle of a project:  How long will it take to deliver feature X?The usual solution is to take the velocity of the past sprints, estimate the feature and then simply divide the feature by the velocity. That gives you the amount of sprints needed to complete the feature.Now, think about this:  Are these forecasts accurate? They might be, we don’t know for sure.  Are these forecasts definite? Possibly.  Are these forecasts probable? Again, they might be.An example given by Doc was the following:  Velocity (11, 10, 9), backlog size of 130 and start date is today.Now, the velocity in this case is 10: 130 / 10 = 13.So the estimation would be that the backlog is finished in 13 sprints. My experience is that these estimations are usually incorrect since 13 weeks is too far in the future.A great tool that he showed was the Throughput Forecaster, an Excel file in which you can enter a lot of data and in return it shows the probability of achieving the goal.More details about the tool above will follow in a future blogpost.DiagnosticsAs we have already said above, a dip or peak in velocity can indicate that there might be a problem with the team.But when looking at this chart, how you do know what’s going on?Exactly, you don’t. You just know something’s probably not okay.A metric that helps is the cumulative flow diagram.It shows the different stages of sprint items during the lifecycle of the sprint. You could see that it takes too long to validate an item or to deploy it.For example, the image above shows that the items stay too long in Ready for Approval. The team can use this to address the problem.This picture was taken during the workshop from Doc about this subject where he went into more detail.ConclusionVelocity is a metric for Agile teams, but only velocity doesn’t indicate much. It needs to be combined with other metrics in order to resolve problems.And use the metrics together with the team so they can detect themselves when there are problems, so they can be resolved as soon as possible.  Metrics are not just for managers, metrics are for teams."
      },
    
      "young-20professional-20programme-2019-10-24-ordina-young-professional-programme-2019-summer-edition-html": {
        "title": "Ordina Young Professional Programme 2019 Summer Edition",
        "url": "/young%20professional%20programme/2019/10/24/Ordina-Young-Professional-Programme-2019-Summer-Edition.html",
        "image": "/img/kicks.png",
        "date": "24 Oct 2019",
        "category": "post, blog post, blog",
        "content": "Introduction  65 young professionals started the Ordina Young Professional Programme this summer, on the 1st of August. JWorks counted 8 trainees: Mohammed, Jasper, Nicholas, Lennert, Duncan, Lore, Brecht and Imad. All looking for a challenge and a fresh start. Most of them just graduated, Brecht on the other hand already had other work experience. During the young professional programme, we were introduced to a wide range of technologies.Technologies that play an important role in IT. The courses were focused mainly on providing a very hands-on experience as to learn how the different technologies work in practice.During the three months long programme there were two main periods:  The first six weeks were filled with various trainings: security, backend, frontend, soft skill trainings,…  During the remaining six weeks we got split up into two teams to work on different dev cases: Chatbot Dina and Zero Plastic RiversThis blogpost will talk about both periods separately below. The first period is further separated into technical and non-technical trainings.  Technical trainings  Non-technical trainings  Dev casesFirst DayOn our first arrival at Ordina, we were warmly welcomed by Anja, our Resource Manager, and two of the Practice Managers: Ken and Yannick. We were introduced to the structure of Ordina and got to know some of the other trainees. After the introduction, we were given a tour of the entire building, during which we met some of the colleagues at JWorks.Four of the trainees already did an internship at Ordina before coming to work officially, so they already knew a lot of the colleagues in Mechelen.This didn’t make any difference however, since everyone was very open and welcoming.Finally, we got our laptops and the keys to our cars so that the next day we could get started with the courses focused on both technical and soft skills.Technical trainingsDuring the six weeks of trainings there were a lot of very interesting technical sessions. The most important ones to us are discussed below.Other than those mentioned below, there are a lot of others, such as the course on Git, DevOps, Java,…Spring BootDuring the young professional programme the Spring Boot course was taught by Ken Coenen.He explained what sits at the core of Spring and how Spring Boot works. Before taking a deep dive into Spring Boot, we were taught the ins and outs of the Spring Framework.Spring Boot takes away a lot of configuration by providing defaults based on industry standards. Therefore, Spring Boot makes it easy to create stand-alone, production grade Spring based applications that need minimum configuration. They still allow you to take the whole configuration in your own hands and of course provide a handful of third-party libraries to get you started. When generating a new Spring Boot project, you get to pick between a Maven or Gradle based project.Day 1On the first day, we took a first dive into the core fundamentals: Inversion Of Control (IOC), Bean Injection, and so on. In the afternoon, we went further and looked into Spring Web Services and Spring Security.Day 2The second day, we built a small application to test the range of possibilities Spring Boot offers. The app was built to provide a system to save houses with their address, the inhabitants and some of the features it has, like the type of garage.It was a fun little project which taught us a lot of what Spring Boot can be used for. We’ve all used Spring Boot in our dev case, so it was definitely worth following.Duncan Casteleyn  What I really liked about Ken’s Spring Boot session was that we did a lot of hands-on live coding.This made the course very interactive, providing possibilities for both feedback to Ken as input of our own.I learned a lot from this session, even though I already had a lot of prior knowledge of Spring Boot.The theoretical start on Spring was a very useful refresh to get up to speed again.DockerThe Docker course, given by Tom Verelst, started with a theoretical explanation about the advantages of containerization, the difference with virtual machines and how a container is built. The main part of the course was a hands-on session. We learned to create an image of our application and push it to Docker hub, spin up a Docker container and write efficient Docker compose files.Brecht Porrez  During this training I really experienced the advantages of Docker. I now use it almost daily during the Zero Plastic Rivers dev case.For example, if I need a test database, I quickly start up a Docker container.If I have written a backend application, I quickly turn it into a container so my frontend colleagues can test with it and so on.KubernetesTo better understand the use of Docker, Tom also gave us an introduction to Kubernetes (K8s). In the K8s session, we learned to work with the commands of Kubernetes by using them on Minikube, a tool to run Kubernetes locally. Later that day we learned to deploy a prebuilt application to Minikube. We wrote our own deployment files for the frontend, backend and RabbitMQ. By doing this we had more insight into the possibilities of Kubernetes.Nicholas Meyers  I’m very interested in how applications are built and deployed, which is why I found the Kubernetes session very interesting. I’d love to learn more about this technology in the future. The hands-on way of working helped me a lot, because this was quite new to me, which made it not the easiest course to follow.Test-Driven DevelopmentIn the DevOps track, we received an interesting lesson about Test-Driven Development from Pieter Van Hees.In school, TDD is usually not taught and testing comes after developing. What TDD aims to do is speeding up the development process by thinking about what exactly you want your program to do and which exact results you want.After pouring those requirements into unit tests, you can start developing and immediately testing whatever you wrote. There are many advantages of working with TDD, but it mainly makes it easier later on in the development process. In the beginning, there’s more work involved because you need to write all the tests. In the long run however, it saves a lot of time because you can immediately spot mistakes using your unit tests.Another good use case is refactoring code. With the test already in place, you can be sure that the behaviour of the functionality is still the same and no regressions are introduced as a result of the refactoring.Lennert Peeters  I’ll be looking more into TDD in the future and continue to develop using this philosophy. We’ve used the method in our Zero Plastic Rivers dev case.It worked out great, saving us quite some time.  Non-technical trainingsOrdina organised some non-technical trainings alongside the ones above. These were focused on Agile and Scrum, as well as some soft skills like how to present yourself in front of others.Agile &amp; ScrumThe first of the courses in the soft skill department was an introduction into Agile and Scrum. Projects ran by Ordina teams get planned in short sprints of two weeks (or even less), making sure the Product Owner is able to give frequent feedback and the team has preplanned time slots for reflection. A Scrum team consists of three major parts: the developer team, the Product Owner and the Scrum Master. The dev team isn’t broken down into multiple roles but works as one whole. This is a very powerful and important part about how a Scrum Team works, since having the team work as a whole allows them to be fluent in their activities. This gives the project a more versatile approach with less frequent congestions and problems compared to the Waterfall methodology.Jasper Rosiers  What I found most interesting about the Scrum framework and the Agile way of working is that there are many moments to reflect on how the work is going and how well the team is working together. The daily scrum is a very powerful moment, which made us use it in our Chatbot dev case. Frequent meetings with the Product Owner and keeping him close to the project is another aspect I love about the Scrum framework. I will definitely look more into it in the future, since I’m aiming to become a Scrum Master.  Agile Hands-OnMichaëla Broeckx gave us an introduction to how Agile development works in practice. It was a very hands-on session that helped us gain more knowledge and experience in the world of Agile development. First, we saw how the waterfall method worked, but then quickly noticed it wasn’t perfect and had a lot of flaws. This is why Michaëla introduced us to Agile which helped us to communicate and work better as a team.She did this by means of a productivity game.The game worked as follows:  Everyone stands in a circle and the group gets one (small) ball.  The team was to throw around the ball during two minutes, while a metronome was ticking in the background.  Every time the ball got caught on a tick, one task was completed.  After two minutes, the team got 30 seconds to decide on a new strategy, but were only allowed to change one thing at a time (an extra amount of balls, a different way of throwing, reverting back to a previous way of working,…)Playing this game for 6 rounds, the productivity went up exponentially. The team had matters in its own hands, which made them think for themselves.At the end we refreshed a couple of famous agile practices such as the SCRUM framework, which is a popular way of working together to quickly and reliably release new features.  Lore Vanderlinden  The agile session was a very enriching experience. We learned the basic concepts of agile the right way. Michaëla was a very inspiring agile coach, making the learning process easier by using a hands-on way of teaching. She used real life examples to show us the advantages of working in an agile manner.Final dayOn the day after the final course day, all the trainees gave a short presentation about themselves in front of the others and the management. This way, everyone present got to know the others, with both their professional interests and achievements, as well as a little on the personal side. Afterwards, there was a moment for networking and socializing with everyone, and an official graduation. The next day, the dev cases started, which we’ll explain below!  Dev casesWith two different dev cases, the team of 8 JWorks trainees was divided into two:  Brecht, Imad, Lennert, Lore and Mohammed worked together on the Zero Plastic Rivers case for the University of Antwerp  Duncan, Jasper and Nicholas were set on the task of designing Chatbot Dina for internal useZero Plastic RiversAt the end of the young professional programme, we were asked to develop a web application to monitor the plastic as it travels through the Schelde. For this purpose, we’d be using GPS trackers alongside QR-scanners.This application is aimed at a PhD carried out at the University of Antwerp that consists of visualizing the plastic flow through the entire river, from the basin to the mouth. After visualizing it, an efficient remediation strategy could be made.The main objective of the application is to create a monitoring network to collect plastic waste, for example, in dams, locks or water treatment plants. This way, plastic flows can be calculated for example in sub-basins or piers to estimate the total flow to the estuary.To activate this system, plastic bottles in the Schelde river will be released at different strategic points with GPS trackers and personalized labels. These contain relevant information such as the identifier or the url to the application.The application consists of two parts.The first part is aimed at citizens who wish to help the cause, who can notify this surveillance network when they find a bottle as shown in the image on the right.The second part is aimed at the researchers, and could be seen as the “backend” of the project, where the data given by the GPS trackers and the citizens is visualized in a clear and orderly way.Chatbot DinaIn the second DevCase, we built chatbot Dina for the Fleet department of Ordina. The Chatbot team set off using the Chatlayer bot framework, later to be joined by an implementation in Dialogflow. Since chatbots are a relatively new technology, we wanted to keep our options open and look for the best possible implementation.The Fleet department at Ordina gets a lot of repetitive questions on a daily basis, which often have easy to research answers. To reduce this workload and make possible a better layout of their time, we designed a chatbot using two different bot services. The chatbot is made accessible via multiple online channels, such as Microsoft Teams, Telegram and Slack. The implementations of these social media weren’t integrated within Chatlayer natively, so we had to build adapters to take care of the communication back and forth between the different platforms.The bot interprets what the user says and formulates its reply depending on the subject. Dina can also ask questions to get more information, use API calls to look up tire centers etc. Using a well-designed chatbot, conversations should feel natural to the user, as if he was talking to a human. An example can be found below.  ConclusionThe past three months have been a really busy, but great experience. We met new people every day, got to learn (and teach!) new things every day and dive deeper into our interests. We would like to thank Ordina and the whole JWorks unit for welcoming us to the team and for giving us this opportunity!"
      },
    
      "conference-2019-10-18-vuejs-london-2019-html": {
        "title": "VueJS London 2019",
        "url": "/conference/2019/10/18/vuejs-london-2019.html",
        "image": "/img/vuejs-london-2019/vuejs-london.png",
        "date": "18 Oct 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  VueJS London 2019: time to get hyped about Composition API and Vue 3  Animations any Vue app can use  Scripting in style, what’s your Vue?  Identifying and solving performance issues in Vue applications  Scalable data visualisation with D3 and Vue  A new router to guide your apps  Lightning talks  Vue3’s Composition API explained visually  The new Composition API  Live coding: the new Composition API  Evan You’s keynote: launch of Vue3 open-source  Workshop: hands-on with the Composition API, by Vue Vixens  ConclusionVueJS London 2019: time to get hyped about Composition API and Vue 3As it is a goal of me to focus more on Vue this year, I had the opportunity to travel to London to attend VueJS London 2019.With a conference on the 4th of October and a workshop provided by VueVixens on the 5th October, I was hoping to learn a lot about the future developments of Vue.The conference and the workshop delivered on my expectations as we learned about accessibility, animations, data visualisations, performance, the new Composition API and much more.We even had a conference call with Evan You where he announced that Vue 3 was going open source.During the workshop, I had the opportunity to play around with the brand new Composition API.In this blog post, I will highlight some key elements that I have learned during the conference and workshop.Conference dayWith 8 talks, 1 keynote and 4 lightning talks, the conference day promised to be quite busy.The conference was held at O2 Cineworld, which is not that far from the city center.As a venue this was exquisite as the accommodation was top-notch, the visuals were beautiful, the talks were clearly audible and the food was great.  Before noon, a total of 5 talks were planned with topics ranging from animations, styling, visualisations and performance to the future of the Vue router.Animations any Vue app can use, by Adam JahrIn a scattered world, the attention of the user of our web applications is often lost quickly.We really need to capture the user’s attention and make sure that they focus on what we want them to focus on.For example in a web shop when the user is looking at the details of a product, we want to make sure he knows exactly where the buy button is.To achieve this, we can divert his attention to this button by adding a small but effective animation.We want to inspire a certain action that will keep the user on the happy path of our application, whether that is selling a product, successfully entering the details for a client registration or something other specific to our application.With the help of the Vue directives v-enter, v-enter-active, v-enter-to, v-leave, v-leave-active and v-leave-to we have the tools to add the necessary CSS classes that define certain states such as the default state, the hovered state and others.    Course on VueMastery and an introduction on VueMastery’s MediumScripting in style, what’s your Vue?, by Maya ShavinCurrently CSS has a lot of scaling issues:  Everything is put into a global namespace  There are implicit dependencies  A lot of dead code can exist and is not easily foundWith the use of scoped CSS, we can already tackle a lot of these issues.Vue not only provides us with the tools to add CSS styling next to our component definition but also CSS that is specific for a component.This is with the help of the scoped attribute that we can add to our styling:&lt;style scoped&gt;/* your CSS here */&lt;/style&gt;But we still need to watch out as some other rules might be more specific from within another component that is not scoped.We are also lacking certain functionalities such as scripting in our styles.So is there a way to be able to script in our CSS while also solving certain issues currently present in CSS?With the use of CSS modules, we can already alleviate several scaling issues as we get certain functionalities such as composition.Sadly we can not solve all CSS scaling issues such as the fact we do not have an easy way for theming.Also, scripting is still not possible.One method of solving the remaining problems is by using Styled Components for which there exists a Vue plugin called vue-styled-components.The idea behind styled components is to utilise tagged template literals to write actual CSS in your JS.We can thus write a styled input tag that has a switch to set a primary and secondary state like this:import styled from 'vue-styled-components';const btnProps = { primary: Boolean };const StyledButton = styled('button', btnProps)`  padding: 0.25em 1em;  border: 2px solid palevioletred;  border-radius: 3px;  background: ${props =&gt; props.primary ? 'palevioletred' : 'white'};  color: ${props =&gt; props.primary ? 'white' : 'palevioletred'};`;export default StyledButton;So with CSS in JS we can script in our styles.It also allows us to solve certain scaling issues since we now have typing, composition and more.But does this mean that we should use CSS in JS by default? Is there still a place for vanilla CSS?CSS in JS is ideal if you need full control over component styling and when you want dynamic styling via props.Using the scoped CSS that is available in Vue is a good option as you isolate the styling per component.If you reuse components between projects, you have all relevant styling close to your reused component.The conclusion is that you should use the right tool for the right job.  View slides and code demosIdentifying and solving performance issues in Vue applications, by Filip Rakowski“Every millisecond counts”, a statement that holds a lot of truth as studies have shown.To make sure that every millisecond is won and that you get the most out of your application, there are certain actions we can take on our source code to directly improve the speed of our application.Lazy loading routesBy default most people will define their routes as such:import Foo from 'pages/foo.vue';import Bar from 'pages/bar.vue';const routes = [\t{ path: '/foo', component: Foo },\t{ path: '/bar', component: Bar }];By changing our import statements for our components we can directly impact the bundle size.When we used to have this:import Foo from 'pages/foo.vue';we can now rewrite it to this:const Foo = () =&gt; import('pages/foo.vue');The result is that our bundle size can get a lot smaller as components are only loaded when they are actually requested for that specific page.With the help of Webpack Bundle Analyzer, we can visualise this.Using tree-shakeable packagesA package that is often used in web applications is lodash.By default, lodash is not tree-shakeable but with the introduction of lodash-es, we now have access to all functions of lodash that are tree-shakeable.So instead of:import { last, zip } from 'lodash'We can now use:import { last, zip } from 'lodash-es'With the help of bundlephobia.com, we can see the individual sizes of all the lodash packages.If we do not use functions such as differenceBy from lodash, we can shave of a couple of kilobytes from our bundle size.Activate dynamic imports of child componentsWhen a page is loaded, not all child components are required to already be loaded in.The reason for this is that not all child components are directly visible.For example when they are somewhere on the bottom of the page or they are off screen.But another big reason is that they are used to render a modal.As not all modals are always needed from the moment the page is opened, we can add a v-if to that component.For example in a product page where we can have a modal with more product details, we might have something like:&lt;template&gt;\t&lt;div class=\"product\"&gt;\t\t...\t\t&lt;ProductDetails v-model=\"product\"&gt;\t\t...\t&lt;/div&gt;&lt;/template&gt;By adding a v-if that holds a boolean to tell if the model should be visible, we delay the actual execution of all the ProductDetails code, thus achieving a performance increase:&lt;template&gt;\t&lt;div class=\"product\"&gt;\t\t...\t\t&lt;ProductDetails v-if=\"isProductDetailsOpened\" v-model=\"product\"&gt;\t\t...\t&lt;/div&gt;&lt;/template&gt;The reason behind this, is that Vue doesn’t render elements contained in a v-if until the v-if returns true.PrefetchingPrefetching is a technique in which we use the idle time of our web page to load other assets.This is a much used technique in for example an infinite scroll where we can already preload one of the next elements that would be shown when the user scrolls down.Another example is to prefetch the components that we are lazy loading for our pages.Thanks to Webpack we can add a specific comment to accommodate this behaviour:const Product = () =&gt; import(/* webpackPrefetch:true */ \"product.vue\")More performance winsIn order to further improve the performance of our web application, we can:  Lazy load images  Use functional components for lists  Optimise initial state when using ServerSide Rendering  Cache static assets in a Service WorkerWith some basic changes, we can already achieve a lot of performance improvement.  More information can be found on his Medium blogScalable data visualisation with D3 and Vue, by Ramona BiscoveanuWhen wanting to do data visualistions with D3 in a Vue web application, it often happens that Vue is only used as a shelf for the actual D3 code.This is inherently a bad idea as the D3 code gets complex, not maintainable and it is also not reactive.We can use the functionalities of Vue to generate part of our D3 code.Since a lot of D3 can be rendered declaratively in HTML code, we can use Vue to bind all the data to the necessary attributes.In our component, we could have something like:&lt;template&gt;\t&lt;svg :width=\"width\" :height=\"height\"&gt;\t\t&lt;path\t\t\tv-for=\"(d, index) in data\"\t\t\t:d=\"generateLine(d.values, index)\"\t\t\t:key=\"d.country\"\t\t\t:stroke=\"colours(index)\"\t\t\t:stroke-width=\"selected(index)\"\t\t\tfill=\"none\"\t\t\t@mouseover=\"onSelected(d, index)\"\t\t\t@click=\"onClick(d)\"\t\t&gt;&lt;/path&gt;\t&lt;/svg&gt;&lt;/template&gt;By using data and computed we can retrieve the correct data from within our Vue component.In our mounted lifecycle hook, we can do all necessary D3 code to use all of our generated SVGs to create the diagrams that we want.  Code example on Ramona’s GitHub with the live application available online and the slides of her talkA new router to guide your apps, by Eduardo San Martin MoroteThere are three kinds of routers possible:  Imperative: we define the routes and add a callback function to determine how the page should be rendered. An example of this is page.js.  Declarative: routes are defined on a component level by using for example a specific attribute. The Reach Router is an example of this.  Configuration based: routes are defined separately with the components that they are linked to. The Vue router is a configuration based router.Because of this configuration based nature of the vue-router, we are missing the option to add or remove routes and we also lack declarative routing.The current implementation of vue-router has a lot of functionalities built in that are specific to managing the history while we actually want to separate this.In an ideal situation, we want the router to be the connection between the components and the code that manages the browser history.When a user navigates to a new page, the router calls the history code to add the new page to the history.When the user navigates back, the router retrieves the previous page from the history.In the current implementation, this is all not clearly separated.So what are the next steps for the Vue router?One major improvement would be the use of TypeScript as this would help developers on using and extending the Vue router in the right way.This is in the same spirit as Vue 3 that is also written fully in TypeScript.Another improvement lies in the use of a ranking system for the routes.Currently routes are checked for a match based on the order in which they are defined.This means that a catch-all route such as (.*) in the beginning of the routes definition would render the other routes useless.A major addition to Vue router would be the ability to have dynamic routing.Components could thus add or remove routes.A final addition would be the full use of the Composition API so we have functionalities such as useLink, useLocation, onBeforeRouteLeave and onBeforeRouteUpdate.The focus of the new router will be mainly on supporting Vue 3 while smaller improvements will be done for Vue 2.  View slidesLightning talksHow to get your product owners to write your functional tests, by Callum SilcockWith the help of the Cypress Cucumber preprocessor Callum was able to demonstrate how code like this could be processed by Cypress:Feature: Logging In  Tests the user can successfully login and log out  Scenario: Logging In Sucessfully    Given I am on the \"login\" page    When I input my \"email\" as \"contact@csi.lk\"    And I input my \"password\" as \"hunter2\"    And I click the \"login\" button    Then I should be on the \"dashboard\" pageThe idea behind this, is that functional tests can be written in a very clear and understandable format so that even product owners can help with writing them.  View slides and an example implementationAwesome JS is Awesome, by Guillaume ChauIn the awesome-vue repository we can already find a curated list of interesting resources for when you want to develop an application with Vue.Guillaume decided to create a specific website that has an overview of all good packages for your project, whether it is a Vue project or a Nuxt project or others.The website will keep on growing with new project types and packages added regularly so that it could grown into a knowledge base on what good packages would be for your future projects.  The website is available at awesomejs.devFocus Management with Vue, by Maria LamardoMaria is very enthousiastic about accessibility on the web.One of the quirks she has found is that when a page is loaded or something dynamic happens on the page like opening a modal, the focus is incorrect.Basically this means that people who rely on a screen reader, often have issues with navigating when for example a modal is opened.By adding a basic directive to your component that uses the inserted hook, you can alleviate this issue:export default {\t// ... other component code\tdirectives: {\t\tfocus: {\t\t\tinserted: function(el) {\t\t\t\tel.focus();\t\t\t}\t\t}\t}}She also talked about how you can have hidden HTML elements to help users with a screen reader to have feedback on what just happened when they for example submitted a form.  Maria’s repository for her talk and for working examplesApplication Shortcuts with a Renderless Event Component, by Rolf HaugBy using $listeners in a Vue component, Rolf managed to create a renderless event component that handles everything you need to act on global events.The sourcecode can be found on his GitHub.One example use could be this in the template of a video player component:&lt;event-listener @keydown.space=\"toggleVideo\" /&gt;Now whenever the user presses on the spacebar of his keyboard, the handler toggleVideo will be called to toggle the video from/to playing/pausing.The same can be achieved by using the vue-global-events package.  Rolf’s repository for his talk and the vue-global-events packageVue3’s Composition API explained visually, by Gregg PollackWith the upcoming arrival of the Composition API, a visual explanation of the possible changes to your code was very useful.The Composition API allows you to focus more on the features instead of only the components.If we would colour code every feature in our code, a component in the options-based API with a lot of features would end up having elements of the same feature all over the component.For example our sorting feature would be red, our searching feature would be purple and so on.This is because the current options-based API forces you to group everything into elements such as data, props, methods, computed and watch.  With the Composition API we would introduce the use of a setup method for each component.In this setup, we declare the scope that will be available to our template.While in the options-based API we defined our scope in the data, props, methods, computed and watch properties and they are bound to the object instance, we now can group all this scope into setup.With the Composition API we have the freedom to group our features in the setup method and even extract them into separate functions so they can be reused in other components.This is because setup returns the scope that can be used in our template.Parts of that scope could be retrieved from separate functions.Each colour in the image above can thus be potentially extracted into separate functions such as useSorting, useSearch and so on.  Vue 3 Composition API cheat sheetThe new Composition API, by Thorsten LuenborgAfter we had the visual explanation of what the Composition API encompasses, it was time to dive into more details and see some example code as can be found on Thorsten’s GitHub.While in the past your components would become a clutter of all your features together spread out over your component, the Composition API allows them to be grouped together.For example when you have a component that supports multiple features such as sorting and pagination, these features would be spread out over the component.The data necessary for the sorting and pagination would be put into data and props.The functions that actually implement these functionalities would be scattered over methods, computed and watch.Thanks to the portable reactivity of the Composition API, we can simplify our components that use for example pagination to:import usePagination from './use-pagination'export default {\tsetup() {\t\tconst pagination = usePagination({\t\t\tperPage: 10,\t\t\titems: [1, 2, 3, 4, ..., 100],\t\t});\t\t// other features defined here\t\treturn {\t\t\tpagination,\t\t\t// other scopes returned here\t\t};\t}}usePagination is a function that encapsulates all functionality specific for pagination:export function usePagination(settings) {\tconst data = reactive({\t\tperPage: 20,\t\tcurrentPage: 1,\t\t...settings\t});\tconst amountOfPages = computed(() =&gt; Math.ceil(settings.items / perPage));\t// other functionalities here\treturn {\t\t...toRefs(data),\t\tamountOfPages\t};}In our component we will then be able to use currentPage, amountOfPages and such, just like when we would have defined them in data and computed.Currently Composition API is already available in Vue 2 thanks to the @vue/composition-api package.  View slides and a full explanation can be found in the Composition API RFCLive coding: the new Composition API, by Jason Yu  The best way to get to know the new Composition API is seeing it live in action.So Jason took the challenge upon himself to live code something called a KeyboardKeyboard.As a musician and a programmer, he is very enthusiastic about keyboards.So he decided to program his computer keyboard to sound like a piano keyboard.With the use of Web Audio API he managed to program a web app in Vue with Composition API in just over 30 minutes.He mainly used Composition API to encapsulate certain key features such as useMusicNote and useKeyDown. It was a fun live coding session as he was able to really captive the audience’s attention.  The source code is on his GitHub and a screen capture can be found on YouTubeEvan You’s keynote: launch of Vue3 open-source, by Evan You  We ended the conference with a keynote from Evan You, the creator of Vue.There was already a lot of talk about Vue 3 in the last couple of months and in this keynote he had a great preview of what the power of Vue 3 will look like.For Vue 3, he and his core team have been very active to rewrite the whole project.By rewriting the whole runtime and compiler from scratch, they were able to add native TypeScript support everywhere.They were also able to make the compiler completely modular.With some clever tricks during compilation time such as hoisting static elements in the DOM tree, the runtime can be optimised to only take the dynamic elements into account.A major announcement about the upcoming Vue 3 is the fact that it is fully tree shakeable.Tree shakeability in combination with all the other upcoming features will result in a major performance improvement as Evan briefly demonstrated.The current speed improvement is already sevenfold.With some features still in development, Evan expects an even bigger improvement in speed before the actual release of Vue 3.As a special treat, Evan had set the visibility of the repository from private to public.From the 4th of October onwards, Vue 3 is fully open source.Workshop: hands-on with the Composition API, by Vue Vixens  With almost half of the conference day spent on talks about the new Composition API, the opportunity to actually incorporate it into a workshop was taken by the Vue Vixens.Vue Vixens are foxy people who identify as women and who want to learn Vue.js to make websites and mobile apps.The workshop was held at CCT Venues, not that far from the O2 Cineworld.The first goal of the workshop was to implement our own version of a Spotify client in the browser, built with the Composition API in Vue.Sadly Spotify has limited the use of their SDK to only premium users of Spotify.So after having gotten a good explanation about what the Composition API is made of along with some interesting discussions on how to use it, we changed it up a bit.As Vue Vixens also organises other workshops, we took one of the solutions for one of those workshops.We rewrote that solution from an implementation in the options-based API to the Composition API.Since almost everybody present had been at the conference the day before and the Composition API was well explained, we were able to finish up quickly.  Thanks to the organisers Maria Lamardo and Kristin Ruben for the great workshop!  Vue Vixens website and the Vue Vixens workshopsConclusionLondon was a great city to visit and with an excellent organised conference, this was a pleasant experience.With the future release of Vue 3, it was no surprise that most of the conference was focused on Vue 3.I’ve learned a lot and with the help of the workshop given by the Vue Vixens, I am convinced that Vue 3 has a bright future ahead.I look forward to putting this new knowledge into practice."
      },
    
      "cloud-2019-10-15-monitoring-serverless-apps-on-aws-html": {
        "title": "Monitoring serverless apps on AWS",
        "url": "/cloud/2019/10/15/Monitoring-serverless-apps-on-AWS.html",
        "image": "/img/2019-10-15-Monitoring-serverless-apps-on-AWS/featured-image.png",
        "date": "15 Oct 2019",
        "category": "post, blog post, blog",
        "content": "Table of content  What about  Challenges of Serverless applications          Challenge 1: Finding the error in a distributed serverless landscape      Solution 1: Structured logging      Challenge 2: Finding performance bottlenecks      Solution 2: Distributed tracing with Xray      Challenge 3: Testing whether our application still behaves as expected      Solution 3a: Smoke Testing      Solution 3b: Load Testing        Side note on CloudWatch Dashboards  Third party tools  Conclusion  ResourcesWhat about?Serverless is a great technology that comes with the advantage of being scalable, durable and high available.It allows you to decouple functionality into multiple serverless Functions.But with new technologies come new challenges.Having an application that exist of a lot of decoupled lambda functions means that your serverless landscape will be heavily distributed.I mean that there is a lot of stuff happening in a lot of different places.We still want to be able to monitor our landscape though.This means that a distributed serverless landscape has to be observable.Let’s see some of the best practices on how to make your serverless landscape observable.Challenges of Serverless applicationsWhat does a typical serverless application look like?Let’s look at an app that was build for a conference. Speakers can create a session that they want to speak about. People can also retrieve all sessions that have already been submitted.When a new session is created a slack notification is sent out.  We can identify certain milestones that indicate that a request has passed this milestone.  The serverless architecture above is actually quite small.I’ve seen architectures containing tens of Lambda Functions and other AWS services.Some of the challenges that come with a serverless architecture are:  It might crash somewhere in my distributed landscape.If it does, where did it go wrong?  Which part of my flow is performing poorly. Let’s find the performance bottlenecks.  I cannot run all cloud services on my computer.So I can’t run my system locally anymore.How do I test whether my system is behaving as it is supposed to?In the next part we’ll focus on solving these challenges.Challenge 1: Finding the error in a distributed serverless landscapeWhen things go wrong we want to be notified.We can do this by configuring a CloudWatch alarm that will go of when an error appears.Below we see that Cloudwatch is ‘watching’ our cloud for errors.When an error event appears an alarm will go of and message will be pushed to a topic.We can then listen on this topic using a Lambda function.This Lambda function will send out the notification to our slack channel.  AWS provides a Lambda function that can send out these alerts that are triggered by an alarm.If you look in the Lambda blueprints via the AWS Lambda Console you’ll find the cloudwatch-alarm-to-slack-python Lambda function that you can use.Digging into the logsWhat do we do when stuff goes south?We check the logs!Right, logging tells us the story of what happened in our application.The logs contain information about this story.Only now the logs are not coming from one place. The story is told in multiple Lambda functions.On top of that the logging might tell multiple stories at once.Multiple execution environments of the same Lambda function can run at the same time.This is that scalability of the cloud.Lambda functions can run concurrently.We need two things:  We need to correlate the logs coming from different places.  We need to get the valuable information out of our logs.Solution 1: structured loggingStructured logging to the rescue!Below you see a normal log versus a structured log.Normal plain text log:  Structured log:  Yes, the structured log is a lot more bloated.But it is also a lot more machine readable and contains much more information.You recognize the JSON format.  It contains contextual information like functionName which is the function that created the log and AWSRequestId which is the identifier for the invocation of the lambda function.  We see the milestone key which refers to a certain milestone that the request passed while processing.  We still recognize the message and timestamp  The logs contain a traceId which we can use to correlate logs.AWS offers us a service to get insights in our logs, CloudWatch Logs Insights. (What’s in a name right?)Since we used structured logging CloudWatch will pick up all JSON fields from our logs automatically.Now we can use these logs to query the milestones that a request passed.We can correlate these milestones since we have the traceId correlating logs over multiple functions.Our logs are generated by multiple functions.CloudWatch Logs Insights allows you to query over multiple logGroups related to these functions.Suppose that something went wrong for session with sessionId: a2db023e-6565-4a5c-b7dc-b53a420898e7.We now can lookup the traceId to track the concerning request in our landscape.fields traceId| filter sessionId=\"865ccaad-ced0-4de5-aec3-b3692b2e06a0\"| limit 1  Then we can use this traceId to find the milestones that the request has already passed.fields milestone, functionName, timestamp| filter traceId=\"bf769e94-4d48-4994-8c04-ebd00b51ecbd\" and ispresent(milestone)| sort timestamp asc  We see that we never got the milestone SAVED_IN_DATABASE.So it went wrong somewhere in the conference-save-session-dynamodb-lambda.We can checkout the logs of this faulty execution using the traceId.fields @message| filter traceId=\"bf769e94-4d48-4994-8c04-ebd00b51ecbd\" and functionName=\"conference-save-session-dynamodb-lambda\"Or we could check for an exception that occurred.fields exception, traceId, functionName| filter traceId=\"bf769e94-4d48-4994-8c04-ebd00b51ecbd\"| limit 1Both will lead us to the exception.  The outcome of this queries can be visualized and added to a CloudWatch Dashboard.More on that later.Structured logging helped us querying our logs for information and finding the error in our flow.Challenge 1 completed!  Challenge 2: Finding performance bottlenecksI wrote a Logs Insights query that allows me to check how long it took for a request to pass through the whole landscape.That means from the moment the creation request arrived till the moment we send out a slack notification for it.fields @timestamp, @message| filter  milestone=\"CREATE_REQUEST_RECEIVED\" or milestone=\"SLACK_NOTIFICATION_NEW_SESSION_SENT\"| stats (latest(@timestamp) - earliest(@timestamp))/1000 as LeadTimeInSeconds by traceId| filter LeadTimeInSeconds!=0| sort LeadTimeInSeconds desc| limit 20  We see that even when the system is warm, it takes us up to 10 seconds to send out a slack notification.We need to dig into the performance of our lambda functions using AWS Xray.Solution 2: distributed tracing with AWS Xray  Xray helps us understand the behavior of our system and thus allows us to analyze the performance of specific parts.It does this by visualizing the flow and dividing the flow into traces and segments.A trace is actually build up from multiple segments.It does this by:  Sampling your requests. By default Xray will trace 5% of your requests.  Tracing calls made by ths AWS SDK.This happens automatically when you use Xray as a dependency for your project.  Creating custom segments.You can create your own segments as you see below.Subsegment subsegment = AWSXRay.beginSubsegment(\"Sessions.saveSessionDynamoDB\");subsegment.putAnnotation(\"storeInDatabase\", \"DynamoDB\");subsegment.putMetadata(\"company\", \"Ordina\");mockingIssues(sessionDynamoDao);repository.saveSession(sessionDynamoDao);AWSXRay.endSubsegment();Here is an example of the Xray service map.  Xray doesn’t trace async requests (yet).That means that publishing on an SNS topic or going via a DynamoDB Stream is not part of the full trace but will show up as a new client in the service map.Recently tracing over SQS was added.When we click the lambda service we can see the response distribution.This visualizes how quickly the lambda function responded.  Something is definitely wrong here since even the quickest executions take more than 3 seconds.We can dig deeper by clicking view traces.  Digging even deeper into one of these traces we can see how long every segment of this trace took.  Below we see that first some data was saved to the caching table.This happened blazingly quick in 8.0 ms.We see however that the Sessions.saveSessionDynamoDB segment took over 3.0 seconds.Of these 3 seconds, only 8 ms was spent actually saving the request.We found our performance bottleneck.Something is waiting around in the Sessions.saveSessionDynamoDB segment.In this case it was me introducing an artificial Thread.sleep().Hooray, we found the performance bottleneck.Challenge 2 completed.  Challenge 3: Testing whether our application still behaves as expectedWe can’t run our complete cloud infrastructure on our local machine.So when we make changes and redeploy, we should test if our system is still behaving as it should.This includes:  Running smoke tests to detect hazards.  Running load tests to view if the system can still handle the load.Solution 3a: smoke testingYou should automate testing your system.In the image below you see how I automate a test to check if a new session that is entered via the API is still forwarded.  I create this test using JUnit and mocked the http endpoints with wiremock.Wiremock is a great tool to mock http endpoints that I personally use a lot.You can ask Wiremock to create certain endpoints and configure the response for it.Below you see me creating the /sessions/forward endpoint.curl -X POST \\  $wiremock_url \\  -H 'Content-Type: application/json' \\  -d '{    \"request\": {        \"method\": \"POST\",        \"url\": \"/session/forward\"    },    \"response\": {        \"status\": 200,        \"body\": \"I have received the session correctly\",         \"delayDistribution\": {                    \"type\": \"lognormal\",                    \"median\": 100,                    \"sigma\": 0.1           },        \"headers\": {            \"Content-Type\": \"text/plain\"        }    }}'Solution 3b: load testingYes, serverless scales automatically.But things might not always behave as expected.Listening on events of a Kinesis stream for example is only possible with one Lambda function per Shard.Thus limiting your throughput if you don’t watch out.To run my load test I use artillery. Below you find the file that I use to configure this load test.It ramps up the amount of request per second from 1 to 10 during 2 minutes.config:  target: 'https://your-own-url.com'  phases:    - duration: 120      arrivalRate: 1      rampTo: 10      name: \"Ramp up to warm up the application\"  payload:    path: \"sessions.csv\"    fields:      - \"subject\"      - \"firstName\"      - \"lastName\"      - \"companyName\"      - \"companyCity\"scenarios:  - flow:      - post:          url: \"/sessions\"          json:            subject: \"\"            duration: 20            timestamp: \"1570202335000\"            speaker:              firstName: \"\"              lastName: \"\"              company:                companyName: \"\"                companyCity: \"\"              questionPhrase: \"\"Again I checked the lead time (time between incoming request and sending out the notification) and found these huge numbers.  Using structured logging and CloudWatch Logs Insights we could start looking deeper into the cause of this delay.I already showed you how to work with Logs Insights, so I’ll get straight to the cause here.The reason it takes so much time to send out all slack notifications is that the Lambda function which listens on the DynamoDB stream is sending out these requests one by one.It takes about 1 second for every request.But the requests come in much faster. This means that they are queueing up in front of the conference-slack-notification-lambda to be send out.  We reached our goal.We found another bottleneck in our system by running the load tests.Challenge 3 completed!  Side note on CloudWatch DashboardsAlong the way we wrote a lot of Logs Insights queries.AWS allows you to bundle the results of these queries into dashboards via CloudWatch Dashboards.Below you see how I made a dashboard that visualizes the number of invocations and associated costs per lambda function.  To find the total cost I used the following query:filter @type = \"REPORT\"| fields @memorySize/1000000 as MemorySetInMB, @billedDuration/1000*MemorySetInMB/1024 as BilledDurationInGBSeconds, @logStream| stats sum(BilledDurationInGBSeconds) as TotalBilledDurationInGBSeconds, sum(BilledDurationInGBSeconds) * 0.00001667 as TotalCostInDollarTo get the stats per Lambda function I did:filter @type=\"REPORT\"| fields @memorySize/1000000 as MemorySetInMB, @billedDuration/1000*MemorySetInMB/1024 as BilledDurationInGBSeconds| stats count(@billedDuration) as NumberOfInvocations,ceil(avg(@duration)) as AverageExecutionTime,max(@duration) as MaxExecutionTime,sum(BilledDurationInGBSeconds) * 0.00001667 as TotalCostInDollarThird party toolsWe just saw the things that we can do to increase the observability of our serverless landscape.To achieve this, you’ll have to do some custom work:  setup structured logging  pass on a traceId  create dashboards in CloudWatch  get familiar with the Logs Insights query language  configure the right alarms in CloudWatch  setup the infrastructure to notify you when an error appears in your landscapeThese are things that take time.And time is money.You can also consider using this money to work with a third party tool.This tool then allows you to monitor and troubleshoot your serverless applications.Personally I have used Lumigo  to achieve just that.The things you have to customize, will be provided out of the box.You get a high-level dashboard to visualize the problems in your application.  It will also automatically trace a request through your landscape by creating a transaction.This is about the same as we did by forwarding traceIds.It comes with a handy visualization.On the left you see the transaction while on the right it gives you the logs that correspond with the error that appeared.  This allows you to drill down to the source of the error quickly.On top of that you can also create the necessary alerts to notify you in case things go south.These tools often have a free tier  that allows you to explore the product.ConclusionWe improved the observability of our system by implementing structured logs and using appropriate testing and tooling.By doing this, it becomes way easier to monitor your system and create visibility on its behavior.Remember that:  you need structured logging to get the maximum out of your logs  CloudWatch Logs Insights allows you to query your logs and analyze them for errors  distributed tracing with AWS Xray helps you identifying bottlenecks in your system  you can create smoke tests and load tests to check if your system is behaving as it is supposed toIn case you want some of these things being done for you automatically, choose a third party monitoring tool to help you with it.Resources  https://lumigo.io/blog/  https://theburningmonk.com/2017/09/tips-and-tricks-for-logging-and-monitoring-aws-lambda-functions/  https://theburningmonk.com/2018/01/you-need-to-use-structured-logging-with-aws-lambda/  https://www.loggly.com/blog/why-json-is-the-best-application-log-format-and-how-to-switch/  https://stackify.com/what-is-structured-logging-and-why-developers-need-it/  https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html  https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html  https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-nodejs-subsegments.html  https://www.slideshare.net/AmazonWebServices/monitoring-and-troubleshooting-in-a-serverless-world-srv303-reinvent-2017  https://lumigo.io"
      },
    
      "cloud-2019-10-02-api-first-development-with-openapi-or-swagger-html": {
        "title": "API first development with OpenAPI/Swagger",
        "url": "/cloud/2019/10/02/API-first-development-with-OpenAPI-or-Swagger.html",
        "image": "/img/2019-10-02-API-first-development-with-OpenAPI-or-Swagger/featured-image.png",
        "date": "02 Oct 2019",
        "category": "post, blog post, blog",
        "content": "Table of content1.API first development - Why, how and what2.API design: an example3.API first development with OpenAPI/Swagger4.Hosting your visualisations5.Integrating with Postman6.OpenAPI Generator: generate API compliant code7.Integrating with your build process: maven or gradle plugin8.Serverless on AWS: OpenAPI, API Gateway Lambda and SAM9.Springfox 10.ConclusionIntro  I’ll start of with a lecture about API first development and it’s advantages.(Jump to section)  We will discover how to visualize our API specs. (Jump to section)  Generate a postman collection from your OpenAPI definition. (Jump to section)  Next I’ll show you how to generate code that is completely compliant with your API specs. (Jump to section)  We then dive into how to integrate this in your build process by using maven or gradle plugins. (Jump to section)  To finish it up I’ll demonstrate how to use it in a cloud native serverless product with AWS SAM and AWS Lambda. (Jump to section)  Wrapping it up with a conclusion.  An API or Application Programming Interface is a way of exposing your company’s digital services.It is the layer through which your services communicate with other services.API first development - Why, how and what  Great communication is key to great software engineering.That also goes for applications and services.If your applications cannot communicate properly, you’ll never be able to expose the functionality that is key to a good product.We also see the following trends in software engineering:      Shift towards the cloud.Instead of big monolitic applications we are building lot’s of smaller (micro)services.All communication between those services goes through the API.        Multiple frontend applications use the same backend.Often these applications are created by separate teams.        An API carries business value.  There is real money in your API.Defining APIs gives us the opportunity to expose our application’s functionality and create bridges between our provider and consumers.The easier it is to integrate with your API, the higher the acceptance rate of consumers for your product will be.  In all of the cases above, there is value in good communication between services.And that’s why you should practice API first development.Put your communication first!  The first priority in your API first development story is a clear API definitionHow do you practice API first development?  Design your API before implementing it.This will allow teams to develop their applications separately because they both know and understand how communication between the services will happen.The contract between services is set.  Understand that the API is the interface for your application.It is the intersection where multiple services join hands to couple their functionality.  Visualize your APIAn image says more then a thousand words. We’ll see how OpenAPI can help you with this.  API first development allows teams to develop separately against a common interface, the API.Now that we understand the importance of and value of API first design let’s see how the Swagger/OpenAPI spec can help you with that.Top-down vs bottom-up  API first development implies a top-down approach to build your API.Basically there are two approaches:  Top-down aka Design First  Bottom-up aka Code FirstTo quote Swagger.io:  Design First: The plan is converted to a human and machine readable contract, such as a Swagger document, from which the code is built.  Code First: Based on the business plan, API is directly coded, from which a human or machine readable document, such as a Swagger document can be generated.In this blog post I am using the Top-down Design First approach to facilitate API first development.In the last paragraph of this blog I’ll briefly show an example of a Code First approach with Springfox.API design: an exampleSuppose that we, Ordina, are hosting a conference where multiple technical and agile sessions will be given.Users can check session information and register for sessions.The backend service is accessed by a web application and two mobile apps.  The applications are created by different teams and they all embrace the API first approach.They read this blog and realised that by agreeing on the common interface first, they could develop separately without impacting each other.So hooray for API first development!API first development with OpenAPI/SwaggerLet’s continue with creating the backend application.The functional analysts and a couple of developers of the team are sitting together to agree on how the API should be defined.  A client should be able to fetch all sessions via the API  A client should be able to create a new session via the APIThis is a crucial part of the API first mindset.We need to clearly define and communicate the API before starting to implement.Designing an API is easier when you can visualise the API.Let’s bring in the OpenAPI spec.  The OpenAPI specification allows you to define your API in a descriptive language (JSON or Yaml) and nicely visualise itLet’s now use OAS to help us with our API First approach and design our API.Note that by OAS I mean OpenAPI Specification.  OAS stands for OpenAPI Specification (formerly known as Swagger Specification)If you are confused about the difference between OpenAPI and Swagger, check out this page.Time to introduce you to https://editor.swagger.io, a portal to visualise…Easy to use and offering all the functionality we need for this example.I’ll keep it simple, we will create the OAS for exposing the endpoint to let consumers fetch the sessions of the conference.The OAS allows you to use JSON or Yaml to describe your API.  Find the descriptive yaml via this gist.As you can see from the example, the OpenAPI specification is very readable.Even if it’s new to you, you should be able to deduct what is written in the yaml.You like looking at raw yaml?Sure you don’t! There is a great visualisation to the right of it.This clearly visualises what your API can do.Clear visualisations mean clear communication.The API in the example is small.When describing a whole real-world API, the file might become quite large.But that’s no problem.The OpenAPI spec allows you to split your definitions over multiple files which you can reference from within other files.  OpenAPI Takeaways: Easy descriptive language &amp; great visualisationsYou want to expose your beautiful visualisation to your clients.They shouldn’t have to paste a yaml file in a window of their browser all the time.How do we do that? Let’s find out next.Hosting your visualisationsThe API specifications should be easily accessible for you and your clients.The specification which you agreed upon, should be hosted somewhere for everyone to see.Sometimes companies have there own in-house tools to visualise OAS.If your company has no such tool there are plenty of other tools to visualize your API defined with OAS.  Choose a visualisation solution that allows you to show a diff between versions of your APIA couple of hosted solutions:      swaggerhub.com: Platform for API design and hosting by SMARTBEAR itself        next.stoplight.io      readme.io        apiary        Redocly:  Redoc allows you to host via github pages. You can also host locally and integrate with Github pages for publishing your API.Use this Generator to create a repository for your API spec.    Integrating with Postman  You can create a working Postman collection from the OpenAPI specYou don’t have to tell me how difficult it is to keep a Postman collection up to date with an evolving API!More so, you have to make sure that every member of your team has the latest version of your API collection.Good news!Postman can import a collection directly from the OAS.In the Postman UI go to import and import from raw text.Just like I did in the image below.  As you can see on the background of the image above, the request was correctly imported from the Swagger file.OpenAPI Generator: generate API compliant code  Generate code that is compliant with your API spec with OpenAPI GeneratorWhen you’ve agreed upon the specification of your API it is time to start implementing it!The specification is shared across the different teams and they can each start implementing separately.Time to code!If I write code, I might make mistakes.So let’s generate code that is completely compliant with the specs.OpenAPI Generator is a hugely popular repository on github.It allows you to generate code that is completely in line with your API specification.On mac you can just install the openapi-generator-cli by installing it via brew.brew install openapi-generator-cliYou can also checkout the github project, build it and use that jar.You have the cli installed and created a directory which contains your api.yml file.  Let’s generate the code!You could generate a whole project.openapi-generator generate -i api.yml -g javaThat would generate a whole Java project with a bunch of files.Let’s start a little smaller.If you followed along clear the directory withfind . ! -name api.yml -deleteFor starters we only want to generate the model classes:openapi-generator generate -i api.yml -g java -DmodelsModels here refer to your DTO’s (Data Transfer Objects) or Resources.These are different from your domain models or entity models.Let’s see what we did here:generateGenerate is the command that we give to the openapi-generator cli to instruct it to generate the code.-i api.ymlThe input file that contains our API specifications.-g javaThe generator to use.Here we specify that we want Java as output language.-DmodelsWe are telling the generator to only generate the models for our API.If you want help or you forgot one of the options you can look here or execute:openapi-generator help generateIf you execute this command you’ll see that there are a lot more options.We could for example do the followingopenapi-generator generate \\-i api.yml \\-g java \\-Dmodels \\-DmodelTests=false \\--model-name-suffix Dto \\--model-package \"com.ordina.conference_app.model\"  \\-p useBeanValidation=trueThis generates the java models without creating test classes and puts them in a package com.ordina.conference_app.model.It suffixes them with Dto since that’s what they are.These classes are used to transfer data in and out of the application (Dto aka Data Transfer Object).  Bean Validation  Keep your generated class files in sync with the requirements of the API specs by setting the useBeanValidation option to true.In the last example I also specified a property useBeanValidation=true.Requirements specified in the API documentation like a required field are now translated to the code.The getter on the  speaker field in the SessionDto class is now annotated with @NotNull.You can now use a framework like JSR 380, known as Bean Validation 2.0., to validate input and output.This is a Java specific example, but the same will happen when you change to other languages by using eg. -g python.  Now that we got acquainted with code generation I am going to show you how to include it in your build process.Integrating Swagger/OpenAPI with your build process: maven or gradle pluginThere are maven and gradle plugins that support the openapi-generator project. (maven and gradle)I started a maven project and included our api.yml on the classpath.Now it is a matter of configuring the openapi-generator-build-plugin in our maven pom.xml.I want to configure it to behave the same way as the example above.&lt;plugin&gt;    &lt;groupId&gt;org.openapitools&lt;/groupId&gt;    &lt;artifactId&gt;openapi-generator-maven-plugin&lt;/artifactId&gt;    &lt;executions&gt;        &lt;execution&gt;            &lt;goals&gt;                &lt;goal&gt;generate&lt;/goal&gt;            &lt;/goals&gt;            &lt;configuration&gt;                &lt;inputSpec&gt;openapi.yaml&lt;/inputSpec&gt;                &lt;output&gt;${project.basedir}&lt;/output&gt;                &lt;generatorName&gt;java&lt;/generatorName&gt;                &lt;addCompileSourceRoot&gt;true&lt;/addCompileSourceRoot&gt;                &lt;skipOverwrite&gt;false&lt;/skipOverwrite&gt;                &lt;modelNameSuffix&gt;Dto&lt;/modelNameSuffix&gt;                &lt;modelPackage&gt;be.ordina.conference.api.model&lt;/modelPackage&gt;                &lt;generateModels&gt;true&lt;/generateModels&gt;                &lt;generateModelTests&gt;false&lt;/generateModelTests&gt;                &lt;generateModelDocumentation&gt;true&lt;/generateModelDocumentation&gt;                &lt;generateApis&gt;false&lt;/generateApis&gt;                &lt;generateSupportingFiles&gt;false&lt;/generateSupportingFiles&gt;                &lt;library&gt;jersey2&lt;/library&gt;                &lt;configOptions&gt;                    &lt;dateLibrary&gt;java8-localdatetime&lt;/dateLibrary&gt;                    &lt;java8&gt;true&lt;/java8&gt;                    &lt;useBeanValidation&gt;true&lt;/useBeanValidation&gt;                    &lt;sourceFolder&gt;src/java&lt;/sourceFolder&gt;                &lt;/configOptions&gt;            &lt;/configuration&gt;        &lt;/execution&gt;    &lt;/executions&gt;&lt;/plugin&gt;Important to note:  The generated models will appear in the current module under /src/java/be/ordina/conference/api/modelThat is caused by the combination of multiple options:          &lt;output&gt;${project.basedir}&lt;/output&gt;      &lt;sourceFolder&gt;src/java&lt;/sourceFolder&gt;      &lt;modelPackage&gt;be.ordina.conference.api.model&lt;/modelPackage&gt;        Only the API models will be generated, with markdown documentation and no test classes.That’s a combination of:          &lt;generateModels&gt;true&lt;/generateModels&gt;      &lt;generateModelTests&gt;false&lt;/generateModelTests&gt;      &lt;generateModelDocumentation&gt;true&lt;/generateModelDocumentation&gt;      &lt;generateApis&gt;false&lt;/generateApis&gt;      &lt;generateSupportingFiles&gt;false&lt;/generateSupportingFiles&gt;      Check out this gist for the xml.Testing your APITesting you API would normally involve setting up a larger integration test.In the case of AWS Lambda this means that you’d have to deploy your application since you cannot run it locally (Not that easily at least).Luckily we have set the useBeanValidation property to true.This allows us to write unit tests that validate the incoming and outgoing requests of our function.After you deserialize the incoming request you can validate it against your API specs:Set&lt;ConstraintViolation&lt;SessionDto&gt;&gt; violations = Validation.buildDefaultValidatorFactory().getValidator().validate(sessionDto);If some violations are detected you can return them wrapped in a 400 response.You could easily check this functionality by writing a unit test that:  checks that no violations are found in the case of a valid request body  checks that violations are found in case a payload is sent which is not compliant with the API specs.The same goes for the responses. In your code validate the response against your API specifications by using the responseDto that was generated from the specs:Set&lt;ConstraintViolation&lt;CreateSessionResponseDto&gt;&gt; violations = Validation.buildDefaultValidatorFactory().getValidator().validate(response);If this finds any violations throw a ConstraintValidationException.Again a unit test can validate that:  no violations are found when the response is validated  no exception is thrownServerless on AWS: OpenAPI, API Gateway Lambda and SAMIt’s fairly easy to create an API Gateway from an openAPI specification.In the API Gateway console under Create select Import from Swagger or Open API 3 and upload your specification.  Of course we want to use the specification programmatically.Suppose we create our backend service with AWS Lambda (serverless).I’ll be using AWS native tools and use SAM to deploy the Lambda functions and my API.SAM allows you to use an OpenAPI specification to create your API Gateway.In your SAM template define the API Gateway resource by referencing your OAS.  ConferenceApiGateway:     Type: AWS::Serverless::Api    Properties:      StageName: dev      DefinitionBody:        Fn::Transform:          Name: AWS::Include          Parameters:            Location: ./openapi.yamlIn the Lambda function resource specify that the lambda should be triggered from this API Gateway.  GetAllSessionsFunction:    Type: AWS::Serverless::Function    Properties:      ...      Events:        CreateSessionApi:          Type: Api          Properties:            RestApiId: !Ref \"ConferenceApiGateway\"            Path: /sessions            Method: GETAdd the x-amazon-apigateway-integration extension in your api.yml to specify how the api has to integrate with the backend Lambda service.paths:  \"/sessions\":    get:      ...      x-amazon-apigateway-integration:        type: \"aws_proxy\"        httpMethod: \"POST\"        uri:          Fn::Sub: arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${GetAllSessionsFunction.Arn}/invocationsFor a full template example including the Lambda resources, check out this gist.Code First with SpringfoxI promised you an example of a code first approach.Here I set up a Spring boot application with Springfox dependencies.&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;io.springfox&lt;/groupId&gt;        &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;        &lt;version&gt;${springfox.version}&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;io.springfox&lt;/groupId&gt;        &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;        &lt;version&gt;${springfox.version}&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;io.springfox&lt;/groupId&gt;        &lt;artifactId&gt;springfox-bean-validators&lt;/artifactId&gt;        &lt;version&gt;${springfox.version}&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;Implementing the API and using the right annotations leads to an endpoint of your application on which your API spec is visualised: /swagger-ui.htmlThere is also an endpoint to download the Swagger / OpenAPI specification:  api-docs  ConclusionHere are some takeaways about API first development:  Design the API before implementing it  Visualize your API so that dependent teams can easily consult it  API first development implies a top-down approach  Swagger/OpenAPI can help you with API first developmentBelow the pros and cons of practising API first development using Swagger/OpenAPI.Pros  Strong tooling support — AWS, Postman, visualizing the API, generate skeleton classes, …  Strong consistency between API spec and Web layer of the code  Example support  Documenting API descriptions is separated from code. Annotations are added to the generated code but you won’t be responsible for constantly updating them to keep documentation in sync.  Functional analyst can assist with creating the API specs because it’s a human readable formatCons  No support for complex/variable request/response scenarios  Little extra documentation can be added in the API specs  If you add a new enum in the specification, your clients have to regenerate their code in order to be able to accept the enum.  Development can only start after API is designedResources  https://scratchpad.blog/serverless/howto/configure-aws-api-gateway-with-swagger/  https://github.com/jthomerson/cloudformation-template-for-lambda-backed-api-gateway-with-dynamodb  https://medium.com/capital-one-tech/how-to-make-swagger-codegen-work-for-your-team-32194f7d97e4  https://github.com/OpenAPITools/openapi-generator  https://howtodoinjava.com/swagger2/code-generation-for-rest-api/  https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#globals-section  https://www.47northlabs.com/knowledge-base/generate-spring-boot-rest-api-using-swagger-openapi/  https://swagger.io/blog/api-design/design-first-or-code-first-api-development/"
      },
    
      "iot-2019-09-30-home-automation-part-1-html": {
        "title": "Home Automation part 1",
        "url": "/iot/2019/09/30/Home-automation-part-1.html",
        "image": "/img/2019-09-30-home-automation/banner.jpg",
        "date": "30 Sep 2019",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Platforms  Example  Conclusion  ResourcesIntroductionIn this series of blogposts we are going to take a deeper look into home automation.Home automation is the wonderful art of automating your home so it becomes easier to do certain tasks.It is a very wide field where there are many available options, platforms and devices.If you are willing to get creative almost everything can be automated!In this first blogpost we’ll present some of the platforms that are available and some of the pros &amp; cons of each one.To close off this blogpost we’ll start by looking into Apple’s HomeKit and by extension HomeBridge.PlatformsIn the world of home automation there are many ways to automate stuff.We will be focussing on either cloud-enabled platforms or those that can be run on premise.The hardware vendor based platforms (e.g.: Niko) will not be included!The platforms can be split up into two main parts:  Cloud enabled, mainly from manufacturers and larger phone companies  On-premise platforms (that do not require internet/cloud access)Security is a very important aspect in the home automation world!The best way one would go about to automating one’s home would be completely independent from any cloud-based services/APIs.Internet access can be intermittent, cloud based API’s can be down or even be shut down permanently.Even worse, accounts could be breached giving unknown third parties access to your home’s devices!However, most people already have some smart devices and most of these have their own app, so ditching any cloud-based systems is going to be a lot harder/more expensive.As with any online service use a strong unique password for each different service and enable multifactor authentication whenever possible!Using all these different apps can be very cumbersome: using app A to turn on the lights, using app B to turn on the TV, using app C to change the thermostat, you get the picture.Some of these smart devices support home automation platforms, some do not.As we will see later on, even those that do not support a home automation platform out of the box can mostly be made to do so!The platforms:  Cloud enabled (more well known):          Apple HomeKit      Google Assistant/Google Home      Amazon Alexa      Xiaomi Mi Smart Home        On-premise solutions (lesser known, but safer)          Home Assistant      openHAB      Apple HomeKitApple’s home automation platform is called HomeKit.It can be used on all iOS and macOS X devices, giving us access to it from everywhere.By default, HomeKit control is limited to your local network only.If we add an Apple TV/HomePod/iPad as a hub it will allow for remote access through iCloud.For the best results I would recommend to use an Apple TV (gen 4 or later) connected through ethernet.Automations can be made using the app (some special automations require a hub though).These automations can be based on a number of different triggers &amp; conditions (sensors/state change/people coming home or leaving/time of day/…)An automation can change the state of a device, or change the state of multiple devices if they have been grouped into a scene.HomeKit supports many devices, which can be identified by ‘works with Apple HomeKit’ logo.Some of these include:  Hue  Tadoo  Eve  and many others…If your device is not supported but has its own app/API there is a big chance you can get it to work by installing Homebridge and adding that to your home app in iOS.Scroll down to the example section of this blogpost if you want to learn more on how to set this up yourself.Google HomeGoogle’s home automation platform is called Google Assistant / Google Home. Both assistant and home are used to control smart devices.It can be used on Android and iOS devices.To make best use of all the features, a Google Home Hub or a Google Home (Mini) is required.Google Assistant supports many devices (much more than HomeKit), these can be added via the app.After setup you can ask the assistant to perform certain tasks and change the state of devices.Automations with Google Assistant are called routines.For the time being, these are not quite as extensive as HomeKit or Home Assistant/openHAB.You can make routines that are based on a time of day event or people coming home/leaving.Triggering routines based on state changes of the smart devices is not supported for now.This will supposedly be added in the future.Amazon AlexaAmazon’s home automation platform is part of its Alexa assistant.It can be used on Android and iOS devices.To make best use of all the features an echo device is required.Alexa supports many devices, much more than HomeKit.These can be added via the app.After setup you can ask Alexa to perform certain tasks and change the state of devices.Automations with Alexa are called routines.These routines are like the scenes and routines that exist with HomeKit &amp; Google Assistant.The routines can be triggered like in HomeKit, by devices/sensors, coming home or leaving, time of day.You can get very creative with the routines, a funny example below, pity the intruder that is caught up in this encounter!Home AssistantHome Assistant is a fully open source home automation platform that is fully focussed on the user’s privacy.It can be used in the browser as well as on iOS and Android devices.It can be run locally with no need of any cloud service. A Raspberry Pi 3B is all you need.Home Assistant has support for over a thousand integrations that are supported by the platform.If you want to create your own integration, a fully documented developer portal will assist you.The integrations are written in Python 3.Automations are called as what they are, automations.They can control devices or perform actions (calling services etc.) based on triggers.These triggers can be various things like in the previously mentioned platforms, but Home Assistant takes it one step further and allows for very fine-grained and very detailed/specific triggers.Location tracking &amp; presence detection is also possible but requires the integration and use of Owntracks.openHABLike Home Assistant, openHAB is a fully open source home automation platform.It can be used in the browser as well as on iOS and Android devices.It can be run locally with no need of any cloud service.A Raspberry Pi 3B is all you need.It also supports well over a thousand existing integrations and can easily be extended.It is written in Java and can be configured with a DSL.Automations are called rules.These can be configured with the platform’s DSL.They can control devices or perform actions (calling services etc.) based on triggers.These triggers can really be anything you can think of, any integration or trigger action you can think of, it can be done.OpenHAB does require quite a bit of research.Do not jump into this platform without doing some digging beforehand.Its documentation is very extensive and is very well made.ExampleHomeKit &amp; Homebridge                                                Below is how my home is set up, which plugins I use (and made myself).The only official HomeKit device I own is my thermostat, all other devices are exposed to HomeKit through Homebridge.Homebridge is a NodeJS server that exposes custom devices to be used inside the HomeKit platform.It can be installed and configured easily, for example on a Raspberry Pi and provides a code or QR code during startup which you can use to add the HomeKit bridge to your setup.A list of all devices I use through Homebridge with their according plugins:  Nest protects - homebridge-nest plugin, only possible now if you already have a nest developer account and the required keys.  Unifi protect cameras homebridge-camera-ffmpeg pluginhomebridge-unifi-protect-motion-sensors plugin  Smartwares smart wifi switch &amp; RF outlets homebridge-homewizard-flamingo plugin  Somfy based shutters homebridge-somfy  LG Airco unit homebridge-lg-aircoThere are many, many more devices and integrations available for use with Homebridge.You can find these by looking on the NPM plugin repository and searching for homebridge-PLUGINNAME.These plugins can be installed by installing the NPM package globally and adding said package to the config.json of the Homebridge instance.The main page of the GitHub repo explains in detail how to set everything up so you too can get started quickly.ConclusionHome automation is a wonderful thing.It can make your live easier in various ways.It is however required to think things through and to do some research before jumping into this.Looking into what devices are supported, out of the box, or via custom integrations/plugins is very important.If you have some programming experience and some time to spare you can very easily make your own integrations for devices that are not supported.In the coming months we’ll be diving a bit deeper into the wonderful world of home automation with more in-depth blog posts about HomeKit with Homebridge and openHAB.Resources  Apple HomeKit  Google Assistant/Google Home  Amazon Alexa  Xiaomi Mi Smart Home  Home Assistant  Home Assistant developer portal  Owntracks  openHAB  Homebridge  Homebridge GitHub  HomeKit catalog, Apple  HomeKit application protocol  Homebridge plugins"
      },
    
      "cloud-2019-09-18-kustomize-html": {
        "title": "Kustomize: Kubernetes configuration management, the easy way",
        "url": "/cloud/2019/09/18/kustomize.html",
        "image": "/img/2019-09-09-kustomize/banner.png",
        "date": "18 Sep 2019",
        "category": "post, blog post, blog",
        "content": "Reading time: 8 min 29 secTable of contents  What problem do we have?  What alternatives are available to solve this problem?  What is Kustomize and how to use it  Extending Kustomize  ConclusionWhat problem do we have?Deploying components to a Kubernetes cluster should be as easy as running kubectl apply -f folder-with-deployment-manifests.This approach works very well for a single environment, but quickly become very hard to do properly when managing multiple environments (Dev, Staging, …, Production).The reason for this is due to the configuration differences in these environments.Every environment needs a different configuration to: connect to different databases, use other secret values, use different deployment configurations (number of replicas for example), …Managing these differences can be very hard to do in a single set of manifests.What alternatives are available to solve this problem?Luckily this problem exists in many organizations and the community already created multiple tools to help solve the problem.In the scope of configuration management for Kubernetes the following tools are in available:  OpenShift Templates  Helm 2  Helm 3  KustomizeOpenShift Templates are part of the OpenShift platform and can be used both to template manifests in a repository and to provide an off-the-shelve experience in the OpenShift platform itself.For example, a cluster administrator can install a template for an Apache Kafka setup.By provisioning this template (through the online UI or the CLI) and providing the required template values, the service can be provisioned like the administrator provides it.This is a very powerful approach to allow developers to provision supporting systems directly in the platform.Helm 2 is a templating and deployment management tool.In Helm 2 a server-side component needs to be installed in the cluster named Tiller.Tiller is the central entrypoint and management component for all deployments using Helm 2.It keeps the state of all deployed manifests and groups them together as a single release.The main disadvantage of Helm 2 is the lack of proper authorization (RBAC) support as Tiller will run with almost root-level privileges on the cluster.Helm 3 is currently still in beta, but the reworked version should resolve a lot of the issues with Helm 2.Like Helm 2 it’s still a templating engine which also manages releases.Unlike Helm 2, it doesn’t use the Tiller component anymore to manage all the state connected to the release.More information about Helm can be found in a future blogpost.Kustomize is a Kubernetes configuration management tool that is provided as part of the kubectl command, behind the -k flag.Kustomize allows a user to use standard Kubernetes manifests and overlay any changes that they want to make using an overlay manifest.Unlike Helm, Kustomize only provides configuration management and doesn’t manage any state about the manifests it adapts.Unlike Helm and OpenShift Templates, the main goal of Kustomize is to allow users to adapt their existing manifests in any thinkable way, instead of templating some parts of the manifest that can be changed.What is Kustomize and how to use itAs briefly discussed, Kustomize is a configuration management tool that has been embedded into kubectl.Originally it was a separate tool and some functionality is still only available in the Kustomize binary and not in Kubectl.The documentation of Kustomize is therefore available in two parts, the core docs and the Kubectl docs.Important to note, especially when considering the usage of this tool, is what it doesn’t do:  It doesn’t manage deployments  It doesn’t package applications in deployable artifacts  It doesn’t manage secrets securelyAs discussed in the Kustomize projects readme a Kustomize manifest exists out of two main structures: a base manifest and overlays.Base manifestA base manifest is, in essence, a set of bare-bones Kubernetes manifests with some Kustomize configuration.For the scope of this blog post, a single base manifest will contain all configuration to deploy a single service.Kustomize doesn’t require this, but it seems like a good fit.Let’s assume the following set of Kubernetes manifests:├── deployment.yaml├── ingress.yaml└── service.yamlThese manifests can deploy one of our apps to a cluster.It will create a deployment, a service exposing the app to the cluster and an ingress object that will allow connections from outside of the cluster.Managing a set of these manifests in separate files or even in one file, can be a bit challenging and often results in violating the Don’t-Repeat-Yourself (DRY) principle.Kustomize can assist in preventing this and allows the user to more generally manage their manifests.Adding Kustomize to a set of manifests is as easy as creating a kustomization.yaml file and running kustomize build.apiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomization# Add labels to all objects created by this kustomize manifest.commonLabels:    app: task-serviceresources:- deployment.yaml- service.yaml- ingress.yamlIn this example, only a single feature of Kustomize is used, namely the commonLabels option.This option makes Kustomize add the label to all managed manifests at build time (when running kubectl apply -k or kustomize build).Other options like image overrides, namespaces overrides and name prefixing are also available.For more information on these features check out the documentation here.Overlay manifestThe second half of the cake in Kustomize are the overlays.Overlays are YAML snippets, Kustomize configuration and/or even full manifests that can be used to adapt a base manifest.In the default setup on the Kustomize homepage, the bases are always local folders.However, a really useful feature is referencing remote locations, including Git repositories, as bases to be used in an overlay.The Git endpoints need to be specified as described in the hashicorp/go-getter URL format.Important to note here is that when using the Git references, the machine that is executing Kustomize builds needs to have a valid Git configuration to access the referenced repositories.As the kustomization file is checked into version control, adding credentials into the link is considered a bad practice.Kustomize supports referencing multiple bases, which again allows for a lot of flexibility.The references work recursively, so multiple levels of manifests are supported.At the time of writing, diamond composition is not possible but being worked on.apiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationresources:- ../../base- git::ssh://git@some-git-provider:some-repo-path.git//folder-in-repo-with-kustomize-config?ref=some-branch- github.com/project/repo//folder-in-repo?ref=branch...The example above uses 3 different ways to include a base manifest in this overlay.The first one, ../../base, references a folder on the machine.A kustomization manifest needs to be present in this folder.The second one, git::ssh://git@some-git-provider:some-repo-path.git//folder-in-repo-with-kustomize-config?ref=some-branch points to a Git repository.More specifically, it points to the Git repository name some-repo-path on git provider some-git-provider.It connects to that Git provider using SSH (ssh://) and it will checkout the branch named some-branch.Finally, it will look for the kustomization manifest in the folder-in-repo-with-kustomize-config folder of the repository instead of in the root directory.The last method is very similar to the second, however it uses the github.com specific syntax.The way Kustomize builds a set of manifests is the following:  Download the remote bases to a temporary folder  Executed kustomize build on all of the bases          This will include executing any generators and transformers that are configured in the bases.        Add any manifests that are listed in the resources section.  Execute the generators.  Apply any patches and execute the transformers against all manifest that are generated or available through the bases.This order of execution is important to remember when creating setups, especially when using overrides for generators in the base.E.g. when using a config map generator in the overlay, a ConfigMap generator needs to be used in the base as well, otherwise Kustomize will not allow the override to be executed.This is because the config map generator adds a random ID to the name of each generated config map and cannot determine whether to change the config maps in the base template as well.Extending Kustomize: pluginsIn some examples the configMapGenerator was used to easily create config maps without having to manually convert files.There are actually two mechanisms to influence YAML files programmatically: generators and transformers.Generators do exactly what their name suggests: they generate YAML files.Generator will use the provided configuration to create new manifests and add them to the set of already available Kubernetes manifests.Transformers however only work on existing Kubernetes manifests.The commonLabels option used in the code snippets above is an example of a transformer.It doesn’t create new manifests, it only changes existing ones.There are a few built-in generators and transformers provided with Kustomize, but the real power lays within the creation of custom plugins that can act as either generators or transformers.PluginsKustomize allows plugins to be created and used during execution.This mechanism allows for a lot of flexibility.Currently, plugins are still an alpha feature and therefore not available through kubectl but only through the kustomize tool itself.Writing a plugin can be done in one of two ways:  Write a plugin in Go and link it as a shared library to the Kustomize tool  Write a plugin based on the exec modelWhile the first way allows the code to be more easily absorbed into the Kustomize binary later on, it requires the plugin to be compiled together with the kustomize binary.The second option is a lot more flexible as it only relies on the plugin being available and providing a very rudimentary interface.More information on support for plugins can be found in the docs and examples can be found here.Real-world exampleIn the last section of this post, a simple example setup will be shown and discussed.Consider the following scenario: a UI, two backends (task service and process service) and a datastore.The components have deployments, services, ConfigMaps ,and ingress manifests.This would result in the following structure:Let’s assume the following set of Kubernetes manifests:frontend/├── deployment.yaml├── ingress.yaml├── configmap.yaml└── service.yamltask-service/├── deployment.yaml├── ingress.yaml├── configmap.yaml└── service.yamlprocess-service/├── deployment.yaml├── ingress.yaml├── configmap.yaml└── service.yamlDeploying this application is as easy as running kubectl apply -f &lt;folder&gt; on each of these folders.A very simple use case for Kustomize is to deploy all of these components at once and group them.The following kustomization.yaml should be added to each folder.apiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationresources:- deployment.yaml- service.yaml- ingress.yaml- configmap.yamlThe following Kustomization manifest could then be used to deploy everything at once by running kubectl apply -k acceptance/acceptance/kustomization.yamlapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationresources:- ../frontend- ../task-service- ../process-serviceDuring the execution of this command, Kustomize will generate a single (giant) file containing all the manifests.Note that the bases are referenced under the resources.The bases key is deprecated and all references should be moved into the resources key.Making variants on the actual bases is super easy.For example, let’s assume that the acceptance environment needs a different configuration.This can be achieved by just overriding the ConfigMap in the acceptance folder.There are three different ways to override a ConfigMap.Using ConfigMap GeneratorGenerators can be used to ease the management of configuration.The ConfigMap generator makes creating ConfigMaps easier by providing a more common way to specify configuration.More information on the ConfigMap generator can be found here.acceptance/kustomization.yamlapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationresources:- ../frontend- ../task-service- ../process-serviceconfigMapGenerator:- name: task-service  env:    - loglevel=warnThe ConfigMap generator will look for the original manifest and apply the override.Important to note here is that the generator only works if the original manifest was generated as well.Using this approach thus requires both the base and overrides to use the generator.A nice bonus to using the generator is that it will add unique IDs to the ConfigMap name every time one is generated.This way, a component is automatically updated when a linked ConfigMap is changed.This provides a nice way to prevent manually triggering a rolling update when configuration changes.Currently only ConfigMap and secret generators are available by default, but as mentioned, there is a very good plugin mechanism available to add more.Using patchesPatches are the last way to override a configuration from a base.Patches are available in two flavors: Json6902 and Strategic Merge.Json6902 is an RFC standard provided by the IETF to describe JSON patches.In a nutshell, operations (patches) can be described using JSON path, operations ,and values.For the example earlier, this would result in the following:acceptance/kustomization.yamlapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationresources:- ../frontend- ../task-service- ../process-servicepatches:- target:    version: v1    kind: ConfigMap    name: task-service  path: configs/acceptance-specific-task-service-configmap-patch.yamlacceptance/configs/acceptance-specific-task-service-configmap-patch.yaml - op: replace   path: /data/loglevel   value: warnMatching is done based on the name (metadata.name), version and kind of the resource.This approach will result in the acceptance-specific config map overriding the base task-service ConfigMap.It will result in the same ConfigMap manifest after the kustomize build.The strategic merge is the final way to provide an override in Kustomize.It merges the existing ConfigMaps with the new configuration provided in the override.It applies the same matching rules as the JSON patch approach to match base manifests with the overrides.Note that the configuration will be added or overridden.acceptance/kustomization.yamlapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationresources:- ../frontend- ../task-service- ../process-servicepatches:- patches/acceptance-specific-task-service-configmap.yamlacceptance/patches/acceptance-specific-task-service-configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  name: task-servicedata:  loglevel: warmBuilding this Kustomize manifest will again result in the same Kubernetes ConfigMap manifest as the other two approaches.Note that none of the three approaches required changing the original manifests.When multiple variants are created, updates to the base will automatically be added to all variants without having to change a single line of code.Conclusion: When to use Kustomize?Kustomize is a great tool to have in your toolbox to simplify configuration management in Kubernetes.DRY principles can be adhered to and managing configuration code can be done in a structured and unified way.Kustomize is a great fit when different environments require different configuration for a microservice.Especially when sensible defaults can be added to the base template and only a small amount of overrides are required per environment.When compared to Helm, both version 2 and 3, Kustomize doesn’t pollute the original manifests with templating code.Kustomize is a configuration management tool for Kubernetes.  Have a lot of configuration code being duplicated across environments?  Have a complex CD setup with manual steps to deploy configuration to a specific environment?  Hate using templating engines?  Really really really like using Kubernetes?If all of the above are true for you, start using Kustomize today and experience it yourself!Useful links  Kubectl-Kustomize docs  Kustomize core docs  Kustomize plugins  Kustomize plugin examples  ConfigMap Generator"
      },
    
      "testing-2019-09-18-js-testing-101-html": {
        "title": "JavaScript Testing 101",
        "url": "/testing/2019/09/18/JS-testing-101.html",
        "image": "/img/2019-07-11-js-testing.png",
        "date": "18 Sep 2019",
        "category": "post, blog post, blog",
        "content": "JavaScript unit testing 101In this blog post I will explain how to write unit tests for JavaScript. For the examples, we will be using jasmine and ts-mockito, but the theory should be applicable to every testing framework. Why am I using two frameworks here instead of only jasmine? Well, in my opinion, jasmine relies too much on magic strings that might give problems during refactoring and makes autocompletion impossible in a lot of cases (depending on how good your IDE is).Ts-mockito is a mocking framework that allows you to create mocks based on a class, so you don’t have to tell it the method names to mock.There are other good mocking frameworks for typescript as well, but my experience is with ts-mockito, so I will be using it for the examples of this blog post.or the code examples I used the Angular CLI to generate a project for me, but I will not be using any of the Angular testing tools to keep these test cases completely framework independent.The Angular CLI was just an easy way to quickly get everything up and running.The basicsThe beginning of a test is naming it. I write every test in the same structure:describe: ‘classname’    ↪ constant values used in the test    ↪ mock declarations    ↪ classUnderTest declaration    ↪ beforeEach to setup the test    ↪ describe: ‘methodname’:        ↪ it ‘should return this when input is so and so’        ↪ it ‘should return this value if input is something else’\t\t    ↪ describe: ‘other method name’:\t        ↪ it‘should return this when input is so and so’\t\t        ↪ it‘should return this value if input is something else’This allows you to quickly see from your testing log, which method from which class has a bug. I personally always try to call the object you are testing something like ‘componentUnderTest’ or ‘serviceUnderTest’.This allows you to very quickly see in a unit test, which object is actually being tested.When you want to mock certain dependencies of your class, you should always declare all the mocks at the top of the test, before your (first) beforeEach call and you should initialize them within the beforeEach. The reason we want to initialize our mocks in the beforeEach and not at declaration, is because you want fresh mocks for every test.If you initialize your mock only once at declaration, the method call count and mock return values will not be removed.This can cause tests to influence each others output, so they can complete successfully or unsuccessfully, depending on the order in which they are run, this can be a headache to debug if you don’t realize this from the beginning.I will explain here with an example:describe('AppComponent', () =&gt; {  const aBrownCar = {color: 'brown', brand: 'Ford'};  const aRedFerrari = {color: 'red', brand: 'Ferrari'};  const aRedPorsche = {color: 'red', brand: 'Porsche'};  const aBlackCadillac = {color: 'black', brand: 'Cadillac'};  const carService: CarService = mock(CarService);  let componentUnderTest: AppComponent;  beforeEach(() =&gt; {    componentUnderTest = new AppComponent(      instance(carService)    );  });  describe('getAllRedCars', () =&gt; {    it('should only return the cars where the color is red', () =&gt; {      when(carService.getCars()).thenReturn([aBrownCar, aRedFerrari, aRedPorsche, aBlackCadillac]);      const actual = componentUnderTest.getAllRedCars();      expect(actual).toEqual([aRedFerrari, aRedPorsche]);      verify(carService.getCars()).once();    });  });  describe('getAllBlackCars', () =&gt; {    it('should only return the cars where the color is black', () =&gt; {      when(carService.getCars()).thenReturn([aBrownCar, aRedFerrari, aRedPorsche, aBlackCadillac]);      const actual = componentUnderTest.getAllBlackCars();      expect(actual).toEqual([aBlackCadillac]);      verify(carService.getCars()).once();    });  });});As you can guess, this test will fail with:Error: Expected \"getCars()\" to be called 1 time(s). But has been called 2 time(s).This happens because the tests don’t start with a fresh mock.So the call to the getCars() from the first test, is still part of the callcount of the mock in the second test.If we reinitialize the mocks every time in the beforeEach, this problem is solved.Writing a unit testEvery unit test has 3 parts.First part is the setup, where you set certain values and mock the necessary method calls.Second part is the actual call of the method you are testing.The last part is the verification, where you will check if the output is correct or the right methods have been called.For readability, leave a blank line between each part, so you can clearly see from a glance what your setup, call and verification part is.A good way to write your unit test is the following:You start with your empty test:it('should only return the cars where the color is black', () =&gt; { });Then you write the method call you want to test: it('should only return the cars where the color is black', () =&gt; {     const actual = componentUnderTest.getAllBlackCars(); });Then you write your verification part:it('should only return the cars where the color is black', () =&gt; {      const actual = componentUnderTest.getAllBlackCars();      expect(actual).toEqual([aBlackCadillac]);      verify(carService.getCars()).once();});After this, you run your test once. It should fail. Then you write the setup you need to get to the expected output:it('should only return the cars where the color is black', () =&gt; {      when(carService.getCars()).thenReturn([aBrownCar, aRedFerrari, aRedPorsche, aBlackCadillac]);      const actual = componentUnderTest.getAllBlackCars();      expect(actual).toEqual([aBlackCadillac]);      verify(carService.getCars()).once();});This is a good way to work when you write unit tests.Writing the verification part before your setup, allows you to see that your test is actually able to fail.This is especially handy when you are testing asynchronous code, where a missing done operator can make your test be successful every time, no matter what your expects check, but more about this in the next part.Asynchronous testingThinking in async code, can be a bit of a challenge, especially when you start out with JavaScript as a new developer or coming from a language that works mostly with synchronous code.In this part of the post, I will show you how to test asynchronous code.Both with promises and observables.For both promises and observables, I will test methods that show the most common usage patterns.One scenario is where you simply get something asynchronously and set its value to a field, the other scenario is a little more complicated, where you get something asynchronously, have some side effects before it resolves, and have some more side effects after it resolves.If that seems complicated, don’t worry, it will become clear very soon.For the promises, I will be using the async / await syntax.If you don’t know what that is yet, I suggest you read up on it, a good article is this one.PromisesScenario AThe method we will be testing will be the following:async getAllRedCarsAsPromise(): Promise&lt;Array&lt;Car&gt;&gt; {    const redCars = await this.carService.getCarsAsPromise();    return redCars.filter((car: Car) =&gt; car.color === 'red');}This function just gets the cars from the carService asynchronously and filters out all the non-red cars before returning the array.The first test, we will simply test the happy scenario, where the carService returns us a nice list of cars and the non-red cars are filtered out. describe('getAllRedCarsAsPromise', () =&gt; {    it('should get the cars where the color is red', async done =&gt; {      when(carService.getCarsAsPromise()).thenReturn(Promise.resolve([aRedPorsche, aBlackCadillac, aBrownCar]));      try {        const actual = await componentUnderTest.getAllRedCarsAsPromise();        expect(actual).toEqual([aRedPorsche]);      } catch (e) {        fail(e);      } finally {        done();      }    });});So what exactly goes on here? First we mock the carService.getCarsAsPromise() method and tell it that it should always return a promise that resolves with a list of cars. Then we call the method that we are testing, we wait for it to resolve (by using the await keyword).Afterward we expect that the returned array only contains the red Porsche.Now, what is this whole try catch thingy? When the promise is rejected, the execution of the code in the try block will be stopped and the code in the catch block will be executed.In this case, if the getCarsAsPromise() should fail, it will go into the catch block and the test will fail.The message that will be shown by jasmine, is the error message that is given to the error that was thrown.For instance, if the getCarsAsPromise() promise return value were to be rejected with the error message ‘Something went wrong.’, the test will fail with the message Failed: Something went wrong..In the finally block, we call the done function, to tell jasmine that our test is done, we call it in the finally block, because the code in the finally block will always be executed, regardless of the result of the promise.Note: the done function should ALWAYS be called, regardless if your test fails or succeeds.The reason for this is that without the done function, the test will only fail after the timeout limit has been reached.When jasmine executes a test, it will wait for 5 seconds before timing out, if the done function has not been called within that time, it will fail.To explain with an example, if we would remove the finally block and call the done function after the expect in the try block, the test would execute as expected in the scenario where the promise is resolved, but if the promise where to be rejected, it would call the fail(e), but since jasmine does not consider the fail call as the end of a test, it would still wait until the timeout for the done function.The test will eventually fail, but it will take 5 seconds to fail instead of 0.5 seconds.In the second test, we will test what happens when the promise is rejected:it('should set retrievalError to true if the promise is rejected', async done =&gt; {      when(carService.getCarsAsPromise()).thenReturn(Promise.reject());      try {        await componentUnderTest.getAllRedCarsAsPromise();        fail('Call should not have succeeded');      } catch (e) {        // nothing to expect here      } finally {        done();      }});As you can see here, we call the fail function in the try block, because the method under test should throw an error.So after calling the method under test, it should go directly to the catch block.Since our method under test does not handle any errors, there is nothing to expect there.In the finally block, we tell jasmine the test is done.Scenario BThe next method we will be testing is the following:async getAllRedCarsAsPromiseWithStuffHappeningInBetween(): Promise&lt;void&gt; {    try {      this.loading = true;      this.cars = await this.getAllRedCarsAsPromise();    } catch (e) {      this.carRetrievalError = true;    } finally {      this.loading = false;    }  }So a few things are happening:  First we set loading to true  Then we get the red cars asynchronously and assign them to our cars field  If getting the red cars throws an error, we set carRetrievalError to true  Afterward we always set loading back to falseIn this case, we don’t just want to test the end result of our test, but also the side effects in between, in this case, the setting of the loading field to true or false. For observables, these types of scenarios are a little easier to test, but with a little creativity, we can also do this for promises, without the use of special testing tools or libraries.it('should set all the red cars to the cars field and set loading field correctly', async done =&gt; {      let promiseResolve: (cars: Array&lt;Car&gt;) =&gt; void;      when(carService.getCarsAsPromise()).thenReturn(new Promise(resolve =&gt; {        promiseResolve = resolve.bind(this);      }));      componentUnderTest        .getAllRedCarsAsPromiseWithStuffHappeningInBetween()        .finally(          () =&gt; {            expect(componentUnderTest.cars).toEqual([aRedPorsche]);            expect(componentUnderTest.loading).toBeFalsy();            done();          }        );      expect(componentUnderTest.loading).toBeTruthy();      promiseResolve([aBlackCadillac, aRedPorsche]);    });So what’s happening here? In the promise callback, we assign the resolve function to a variable outside of that function, so we can use it later on.Then we call our method under test, but notice that we don’t use the await keyword here.Why is this? Well, if we were to use the await keyword, it would simply wait for the promise to resolve, but it would never happen because our resolve function is never called.Instead we just want to call it, let it execute and start expecting the side effects.As we saw in the method under test, the loading field is set to true before the red cars are retrieved, since those cars are not retrieved yet, we can check that the loading field is indeed set to true.Ok, now we have checked that, let’s resolve the carService call. We do this by calling our promiseResolve function.Now the cars are resolved, so we can check that the loading has now been set to false and the cars field has been correctly set to the right value.The finally callback of the returned promise, will execute after we call our promiseResolve function, and will check the final result of our method and subsequently call the done function to tell jasmine our test is over.In the second test we will test the error scenario, where we want to see if the loading value is still set correctly and the carRetrievalError is also set to true.it('should set the retrievalError to true and loading states correctly if the promise is rejected', async done =&gt; {      let promiseReject: (cars: Array&lt;Car&gt;) =&gt; void;      when(carService.getCarsAsPromise()).thenReturn(new Promise((resolve, reject) =&gt; {        promiseReject = reject.bind(this);      }));      componentUnderTest        .getAllRedCarsAsPromiseWithStuffHappeningInBetween()        .finally(          () =&gt; {            expect(componentUnderTest.cars).toEqual([]);            expect(componentUnderTest.loading).toBeFalsy();            expect(componentUnderTest.carRetrievalError).toBeTruthy();            done();          }        );      expect(componentUnderTest.loading).toBeTruthy();      promiseReject(undefined);    });Here we actually do the same thing, except instead of using the resolve function, we use the reject function.ObservablesFor observables, I have created methods that do exactly the same as our previous promises scenarios, except we use observables here.Scenario AFor scenario A we will test a simple observable that returns one value and then completes.  This is a fairly common use case, especially with http calls. Here is our method under test:getAllRedCarsAsObservable(): Observable&lt;Array&lt;Car&gt;&gt; {    return this.carService      .getCarsAsObservable()      .pipe(        map((cars: Array&lt;Car&gt;) =&gt; cars.filter((car: Car) =&gt; car.color === 'red'))       );}We simply get all the cars as an observable and then we call the map operator to filter the list we get.  Pretty simple scenario.For a simple scenario like this, there are two ways to test this.  The first one is this:it('should get the cars where the color is red', done =&gt; {      when(carService.getCarsAsObservable()).thenReturn(of([aRedPorsche, aBlackCadillac, aBrownCar]));    componentUnderTest      .getAllRedCarsAsObservable()      .subscribe(        (actual: Array&lt;Car&gt;) =&gt; {          expect(actual).toEqual([aRedPorsche]);          done();        },        e =&gt; {          fail(e);          done();        }      );});We simply call our method under test, subscribe to the resulting observable, and in the subscribe callback we do our expects and in the error callback we fail the test. This is the easiest way to test this scenario, although, it is a little verbose. To reduce the lines of code, a simple scenario like this, can also be tested by transforming the observable to a promise and test it like we did in the promise scenario:it('should get the cars where the color is red (alternative way)', async done =&gt; {      when(carService.getCarsAsObservable()).thenReturn(of([aRedPorsche, aBlackCadillac, aBrownCar]));      try {        const actual = await componentUnderTest.getAllRedCarsAsObservable().toPromise();        expect(actual).toEqual([aRedPorsche]);        done();      } catch (e) {        fail(e);        done();      }});I guess in this case it depends on what you find the most readable. The async await syntax makes the second version very readable, but comes down to personal preference.Scenario BIn the next scenario we will be testing an observable call with side effects.getAllRedCarsAsObservablewithStuffHappeningInBetween(): void {    this.loading = true;    this.getAllRedCarsAsObservable()      .pipe(        finalize(() =&gt; this.loading = false)      )      .subscribe(        (cars: Array&lt;Car&gt;) =&gt; this.cars = cars,        () =&gt; this.carRetrievalError = true      );  }Same scenario as the promises, we set loading to true and subscribe to an observable, when it resolves, we set the value and when it completes, we set loading to false. If the observable throws an error, we set carRetrievalError to true.So our first test is the happy path, the observable resolves with an array of cars and everything goes as expected:it('should set all the red cars to the cars field and set loading field correctly', async done =&gt; {    const carsSubject = new Subject&lt;Array&lt;Car&gt;&gt;();    when(carService.getCarsAsObservable()).thenReturn(carsSubject);    componentUnderTest.getAllRedCarsAsObservablewithStuffHappeningInBetween();    expect(componentUnderTest.loading).toBeTruthy();    carsSubject.next([aBlackCadillac, aRedPorsche]);    carsSubject.complete();    expect(componentUnderTest.cars).toEqual([aRedPorsche]);    expect(componentUnderTest.loading).toBeFalsy();    done();});This scenario is very easily tested with observables, because of Subjects. What is a subject? A subject can act as both an observable and an observer.This means we can subscribe to it and receive the emitted values, or we can tell it to emit a certain value.In this case, instead of telling the carService to return an Observable that immediately resolves with a value, we tell it to return a subject.asObservable().Now we are in control of when and what value is emitted in this observable.This allows us to check our side effects easily at the right time.Note that I am calling the carsSubject.complete() after I emit the car array value.I do this because I set loading to false in the finalize operator.This finalize operator is only called when the observable is completed.So if I don’t call the carsSubject.complete(), loading will never be set to false and my test will fail.The error scenario is pretty much the same logic, except we call the error method instead of the next on the subject.it('should set retrievalError to true if the observable throws an error', async done =&gt; {    const carsSubject = new Subject&lt;Array&lt;Car&gt;&gt;();    when(carService.getCarsAsObservable()).thenReturn(carsSubject.asObservable());    componentUnderTest.getAllRedCarsAsObservablewithStuffHappeningInBetween();    expect(componentUnderTest.loading).toBeTruthy();    carsSubject.error(undefined);    carsSubject.complete();    expect(componentUnderTest.cars).toEqual([]);    expect(componentUnderTest.carRetrievalError).toBeTruthy();    expect(componentUnderTest.loading).toBeFalsy();    done();});ConclusionI hope these code examples help you to effectively unit test your (asynchronous) code. Readability is everything, so make sure you use blank lines to clearly differentiate between the different parts of a test. And don’t forget that done function, without it, your test might timeout, or even worse, it might succeed without actually having tested anything at all.All the code examples can be checked on the following repository:https://github.com/ivarvh/js-testing-101Simply clone the repo, run npm install and run npm test to execute the tests.Happy testing everyone!"
      },
    
      "architecture-2019-09-13-angular-include-assets-libraries-html": {
        "title": "Lessons learned on including library assets into Angular applications",
        "url": "/architecture/2019/09/13/angular-include-assets-libraries.html",
        "image": "/img/2019-09-09-angular-include-assets-libraries/tpryd.jpg",
        "date": "13 Sep 2019",
        "category": "post, blog post, blog",
        "content": "With Angular 6, which has now been released ages ago, workspaces were introduced.This meant that a repository could include multiple applications and libraries, eventually building a monorepository.I welcomed this change, as it meant that all applications in one repository could be kept up to date easily and more.But I’m not here to talk about all the advantages and disadvantages of a monorepo.Instead I’d like to talk about one specific challenge with this type of repository.How to include assets used by librariesThere are multiple solutions to this problem.But not all solutions match all criteria for a good architecture.These criteria are:  Assets should exist only once, preferably in the project that utilizes them.  An update on an asset should trigger the rebuild of all those applications depending on the library using the asset, and only those.  A dependency on another library should be added with minimal change and with minimal affected projects.I haven’t worked with many monorepo tools, other than Nrwl/nx, so I will base my definition of affected projects on that.Nrwl/nx uses a dependency graph to determine the affected projects.In short: if a library has a changed file, then all projects that import this library (either directly or lazy-loaded) are affected.This works recursively, so the projects that import those projectes are also affected and so on.Some files, like package.json and angular.json have implicitDependencies set to \"*\", which means a change in those files will regard all projects in the workspace as affected.These are the solutions (in order) I’ve gone through to tackle this exact problem for a monorepo I’m currently managing.Note: In that monorepo the focus was on json-files with translations used for @ngx-translate, but in my examples here I will use images.In the example, the application, my-app, only depends on the library neighbourhood-dogs-lib and should only display a picture of my dog and Pete’s dog, but not of Karen’s cat.Solution 1 - Assets in the application source├─ 📂apps│  └─ 📂my-app│     ├─ 📂src│     │  ├─ 📂assets│     │  │  └─ 📂images│     │  │     ├─ 🖼my-dog.jpg│     │  │     └─ 🖼pete-dog.jpg│     │  ├─ 📂app│     │  │  └─ ...│     │  ├─ 📄index.html│     │  └─ ...│     └─ ...└─ 📂libs   ├─ 📂neighbourhood-dogs-lib   │  ├─ 📂src   │  │  ├─ 📂lib   │  │  │  ├─ 📄dog-list.component.ts   │  │  │  └─ 📄dog.component.ts   │  │  └─ ...   │  └─ ...   └─ 📂neighbourhood-cats-lib      ├─ 📂src      │  ├─ 📂lib      │  │  ├─ 📄cat-list.component.ts      │  │  └─ 📄cat.component.ts      │  └─ ...      └─ ...I call this solution the “I’ll figure it out later”-solution.The main idea was to set aside this problem because other tasks had higher priority, the monorepo wasn’t nearly as big as it is now.Basically, the assets used by a library were put in the assets folder of the application’s source directory.This meant that the build of an application would simply include these assets out of the box.This is fine for a single application, but as soon as a second application (let’s say, neighbourhood-animals-app) was to use this library, it, too, would need a copy of those assets in its source directory.A change to one of the assets would also mean that two applications would need this change, which is prone to being forgotten.Moreover, the image of Karen’s cat is nowhere to be found because at this time, no application needs it.As for the criteria:                        Assets should exist only once, preferably in the project that utilizes them.            ❌                            An update on an asset should trigger the rebuild of all those applications depending on the library using the asset, and only those.            ⚠️*                            A dependency on another library should be added with minimal change and with minimal affected projects.            ⚠️**            *Kind of, if I don’t forget to copy our assets**Depends on the amount of applications that need these extra assetsSolution 2 - Shared assets directory├─ 📂apps│  └─ 📂my-app│     ├─ 📂src│     │  ├─ 📂app│     │  │  └─ ...│     │  ├─ 📄index.html│     │  └─ ...│     └─ ...├─ 📂libs│  ├─ 📂neighbourhood-dogs-lib│  │  ├─ 📂src│  │  │  ├─ 📂lib│  │  │  │  ├─ 📄dog-list.component.ts│  │  │  │  └─ 📄dog.component.ts│  │  │  └─ ...│  │  └─ ...│  └─ 📂neighbourhood-cats-lib│     ├─ 📂src│     │  ├─ 📂lib│     │  │  ├─ 📄cat-list.component.ts│     │  │  └─ 📄cat.component.ts│     │  └─ ...│     └─ ...└─ 📂shared-assets   └─ 📂images      ├─ 🖼my-dog.jpg      ├─ 🖼pete-dog.jpg      └─ 🖼karen-cat.jpgThis is the “Share everything”-solution. It was created to solve the main fault of the previous, namely that assets could exist twice.Another important aspect here was to reduce the amount of times that angular.json would be changed when more assets from another library would be added.Because angular.json implicitly affects all projects, this file should be kept untouched as much as possible.So a one-time change was made to let the projects in angular.json include these shared assets:{    \"glob\": \"**/*\",    \"input\": \"./shared-assets/images\",    \"output\": \"./assets/images\"}Unfortunately this meant that all assets from all libraries would be added to those applications, which increases the bundle size significantly.Another disadvantage is that the Nx dependency graph could not link changes in this directory with their corresponding libraries, unless every single file was mentioned in nx.json with an implicit dependency for that library.Ironically, this solves criteria 3 perfectly, because nothing is affected, so that’s the bare minimum.Going back through the criteria:                        Assets should exist only once, preferably in the project that utilizes them.            ⚠️*                            An update on an asset should trigger the rebuild of all those applications depending on the library using the asset, and only those.            ❌                            A dependency on another library should be added with minimal change and with minimal affected projects.            ✅            *There are no longer duplicate assets, but they’re not part of the project that utilizes themSolution 3 - Custom Angular buildersA solution I came across was to copy the assets after a build into the dist-folder using a script in the package.json file, but for obvious reasons that wouldn’t work easily when managing different apps.Neither would it work with the dev-server, so I didn’t even go there.Including assets by adding them to the build target’s assets array is how Angular itself prescribes to solve this problem, so let’s keep that.However, I still wanted to change the angular.json file (and other files) as minimally as possible.Enter Angular 8 and the stable version of the CLI Builder API!├─ 📂apps│  └─ 📂my-app│     ├─ 📂src│     │  ├─ 📂assets│     │  │  └─ 📄include.json│     │  ├─ 📂app│     │  │  └─ ...│     │  ├─ 📄index.html│     │  └─ ...│     └─ ...└─ 📂libs   ├─ 📂neighbourhood-dogs-lib   │  ├─ 📂src   │  │  ├─ 📂assets   │  │  │  └─ 📂images   │  │  │     ├─ 🖼my-dog.jpg   │  │  │     └─ 🖼pete-dog.jpg   │  │  ├─ 📂lib   │  │  │  ├─ 📄dog-list.component.ts   │  │  │  └─ 📄dog.component.ts   │  │  └─ ...   │  └─ ...   └─ 📂neighbourhood-cats-lib      ├─ 📂src      │  ├─ 📂assets      │  │  └─ 📂images      │  │     └─ 🖼karen-cat.jpg      │  ├─ 📂lib      │  │  ├─ 📄cat-list.component.ts      │  │  └─ 📄cat.component.ts      │  └─ ...      └─ ...Using this new API, I was able to construct two new builders to replace the default @angular-devkit/build-angular:browser and @angular-devkit/build-angular:dev-server while still utilizing them.These custom builders take the same options as the ones they replace and work more like a hook than a new builder.The idea is to update the build target in the in-memory workspace before the default builder is actually executed.Simply put, the custom builders read angular.json, update the assets array and pass the updated version to the original builders.A single configuration file (include.json in the application’s assets directory) lets the custom builder read which libraries the application depends on.It then determines its source directory using the workspace configuration file (angular.json) and adds the following to the assets array:{    \"glob\": \"**/*\",    \"input\": \"./libs/neighbourhood-dogs-lib/src/assets\",    \"output\": \"./assets\"}If I wanted my-app to also include the neighbourhood’s cats, then I could change include.json to also include neighbourhood-cats-lib and the next build would add the following to the assets array:{    \"glob\": \"**/*\",    \"input\": \"./libs/neighbourhood-dogs-lib/src/assets\",    \"output\": \"./assets\"},{    \"glob\": \"**/*\",    \"input\": \"./libs/neighbourhood-cats-lib/src/assets\",    \"output\": \"./assets\"}Though I’d rather set the option that these assets are placed in sub folders, so I added that into the builders too.That made the assets array into the following, which prevents libs from overwriting other assets:{    \"glob\": \"**/*\",    \"input\": \"./libs/neighbourhood-dogs-lib/src/assets\",    \"output\": \"./assets/neighbourhood-dogs-lib\"},{    \"glob\": \"**/*\",    \"input\": \"./libs/neighbourhood-cats-lib/src/assets\",    \"output\": \"./assets/neighbourhood-cats-lib\"}Note: The reason this include.json file is in the assets directory was to make a custom HttpTranslateLoader (using a simple RxJS mergeMap and forkJoin) read that same file to determine which translation assets to download.Let’s go through the criteria one more time:                        Assets should exist only once, preferably in the project that utilizes them.            ✅                            An update on an asset should trigger the rebuild of all those applications depending on the library using the asset, and only those.            ✅*                            A dependency on another library should be added with minimal change and with minimal affected projects.            ✅            *Because the assets are inside the library’s directory, the dependency graph can detect a change and determine the affected applicationsThere is one caveat though: the dependency graph will only detect dependencies if the library is also imported in the code of the application, which would be the case mostly.If that’s not the case (for example if the library containing assets has no components/services/modules/…), simply create an empty module in it and import it in the applications that depend on these assets.Sharing is caringBecause I care about the community I packaged this solution and published it to npm.It’s called ngx-library-assets and is available at https://www.npmjs.com/package/ngx-library-assets.Install it as a devDependency.Disclaimer: I do not actually own a dog, or a cat. Neither do Pete and Karen. I don’t even have any neighbours named Pete or Karen."
      },
    
      "testing-2019-08-23-test-design-html": {
        "title": "Avoiding fragile tests with better design",
        "url": "/testing/2019/08/23/test-design.html",
        "image": "/img/fragile5.png",
        "date": "23 Aug 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Why should we design our tests  The most basic test design  Fragile tests example  Implementing the API  Advantages of using an API  ConclusionWhy should we design our testsWe spend quite some time thinking about the design of our production code.We do this because we want our code to be readable and maintainable.The easier our code can be maintained, the easier we can implement new features and perform the necessary refactorings to implement those features.The absurd thing is that we only design our production code and not our tests.Our tests should be equally readable and maintainable as our production code, because if we don’t, we’ll spend too much time fixing and rewriting our tests.If we successfully create readable tests, they will also serve as very good documentation, describing the functionality of our code, and how it is expected to behave.The most basic test designA common practice in writing tests is creating a test class for each production class. The reason why it is such a popular practice, is because:  it’s easy to find tests for the production code you’re looking at,  it’s a quick way to write new tests because you don’t have to think about how and where to write tests.Although this approach does have advantages, it can also be harmful for the maintainability of your application.The disadvantage of this approach becomes clear when you need to refactor some classes.If you move logic from one class to another, or even multiple other classes, you need to create new tests to test each of those classes, if you want to keep your ‘one class means one test class’ strategy.In performing such a refactoring we should not need to change any tests because we are not adding or changing any functionality, only moving logic around. However, if we want to keep our design of having a test class for each production class, we need to refactor our tests as well.Even if we don’t want to keep this design, our tests will have to be modified because chances are big that the API of our production code changed. The parameters of methods might have changed, the fields of objects might have changed, constructors might have changed, etc.If we are lucky, the tests still compile, but they will very likely fail. And the larger your application becomes, the more work it will be to get all tests compiling and green again.This phenomenon is known as fragile tests.Fragile tests exampleAn example of this phenomenon that we encountered on a project is the creation of an instance of an aggregate.A lot of the tests in our project needed an instance of an aggregate. This was not a problem at first, we just created aggregates by using the constructor of the class and passing all the necessary data in it.We created these instances in every test where we needed them, or sometimes created a method in the test class to not duplicate the construction too much in that class.To illustrate the issue we will look at a fictional simplified example about order creation.public class Order {        private final UUID customerId;        public Order(final Customer customer) {        Assert.notNull(customer, \"Customer should not be null\");        Assert.isTrue(customer.isActive(), \"Customer should be active\");        this.customerId = customer.getId();    }        public UUID getCustomerId() {        return this.customerId;    }}public class OrderTest {        @Test    public void given_an_active_customer_then_order_creation_should_be_successful() {        final UUID customerId = UUID.randomUUID();        final Customer customer = mock(Customer.class);        when(customer.getId()).thenReturn(customerId);        when(customer.isActive()).thenReturn(true);                final Order order = new Order(customer);                assertThat(order.getCustomerId()).isEqualTo(customerId);    }        @Test    public void given_an_inactive_customer_then_order_creation_should_result_in_an_illegal_argument_exception() {        final Customer customer = mock(Customer.class);        when(customer.isActive()).thenReturn(false);         assertThrows(IllegalArgumentException.class, () -&gt; new Order(customer));    }}Our problems began when we realised that the number of parameters in the constructor of our aggregate became too large. To resolve this issue we decided to create a class that contains all the data needed to call the constructor.public class Order {        private UUID customerId;        public Order(final CreateOrderData data) {        Assert.notNull(data.getCustomer(), \"Customer should not be null\");        Assert.isTrue(data.getCustomer().isActive(), \"Customer should be active\");        this.customerId = data.getCustomer().getId();    }        public UUID getCustomerId() {        return this.customerId;    }}However when we tried to run all the tests, most of them didn’t compile anymore, which makes sense because we changed the contract.Now we could have made it easier for ourselves by using some IntelliJ refactoring tools, but nevertheless, it’s absurd that so many tests could break by just changing the way we construct our aggregates.When we finally got all our tests green again by just creating the data class parameter, we were so happy and sick of the refactoring that we just stopped there, instead of addressing the underlying issue.public class OrderTest {        @Test    public void createOrder_happyPath() {        final UUID customerId = UUID.randomUUID();        final Customer customer = mock(Customer.class);        when(customer.getId()).thenReturn(customerId);        when(customer.isActive()).thenReturn(true);                final CreateOrderData data = new CreateOrderData();        data.setCustomer(customer);                final Order order = new Order(data);                assertThat(order.getCustomerId()).isEqualTo(customerId);    }        @Test    public void createOrder_customerInactive() {        final Customer customer = mock(Customer.class);        when(customer.isActive()).thenReturn(false);         final CreateOrderData data = new CreateOrderData();        data.setCustomer(customer);                assertThrows(IllegalArgumentException.class, () -&gt; new Order(data));    }}A few months later, after adding some more features, we noticed that there was too much logic inside the constructor of our aggregate. The constructor became too big and complex so we decided to use the factory pattern to create new instances of the aggregate.Since we already used a CreateOrderData as parameter of our constructor, all our tests still compiled and we were happy.That is, until we ran our tests.Because we moved all construction logic from the constructor to the factory, the tests still compiled, but as they relied on this construction logic to create the instance the way we need it, they now failed.Again we were faced with the issue of a large amount of tests that we had to refactor.Not having learned from our previous mistakes and being under time pressure, we decided to use the factory to create instances in all our tests. For the factory we needed some other services, repositories etc. which we all mocked.This was a huge amount of work because of all the mocking we had to do just so we could create a consistent aggregate.And we had to do this, again, in every test that needs an aggregate.After everything worked again, we were happy that the pile of work was done and we could move on with other things.In the weeks that followed, however, we started to notice that every time we changed the logic of the factory, we needed to change all the tests again because we had to add some extra mocks, data, etc. in all the tests.After a few of these iterations where we had to spend too much time fixing tests, we were fed up and decided (way too late of course) to free up some time for a more structural solution.We got some inspiration from a blogpost from Uncle Bob about his opinion on the statement that TDD harms architecture.One of the things he mentions in his post is that we shouldn’t make the mistake of coupling every test to the implementation of our production code.Instead it would be better to put some sort of API in between our tests and the production code.Implementing the APIWe didn’t take Uncle Bob’s solution too literally and gave our own twist to it.For the specific problem of creating aggregate instances we decided to create a class that acts as a scenario builder.In this CreateOrderScenario we have a static factory method that will create a scenario that returns a valid Order when executed.This means that when you need an order that is consistent and it doesn’t matter for your test which data is in the order, you can just use the default scenario when it’s executed.You could also create other default scenarios.For example an order with an invalid customer, or with specific data that triggers a certain flow in the order process.This is very convenient for most tests.However, in some tests we want to influence how the order is constructed, so we can test some custom cases other than a default scenario, specific for certain tests.We implemented this by adding some methods to our scenario class that allows the scenario to be modified to the test’s needs.public class CreateOrderScenario {    public static final UUID customerId = UUID.randomUUID();    private CreateOrderData createOrderData;    private CustomerRepository customerRepository;    public static CreateOrderScenario defaultScenario() {        final CreateOrderScenario scenario = new CreateOrderScenario();        final Customer customer = mock(Customer.class);        when(customer.isActive()).thenReturn(true);        scenario.customerRepository = mock(CustomerRepository.class);        when(scenario.customerRepository.findById(customerId)).thenReturn(Optional.of(customer));        scenario.createOrderData = new CreateOrderData(customerId);        return scenario;    }    public CreateOrderScenario modifyCreateOrderData(Consumer&lt;CreateOrderData&gt; modifier) {        modifier.accept(createOrderData);        return this;    }    public CreateOrderScenario overrideCustomerRepository(Consumer&lt;OrderRepository&gt; modifier) {        customerRepository = mock(CustomerRepository.class);        modifier.accept(customerRepository);        return this;    }    public Order execute() {        final OrderValidator orderValidator = new OrderValidator(customerRepository);        final OrderFactory orderFactory = new OrderFactory(orderValidator);        return orderFactory.createOrder(createOrderData);    }}In the following example we created two tests that verify that the construction of an order works correctly.In the first test, we use the default scenario without modifying anything, meaning, we test the happy path and verify that all data in the created order is correct.In the second test we verify that if we try to create an order for a customer that doesn’t exist, we get a validation exception.We do this by creating a default scenario, then modifying the input data to use a customerId defined in the test, and then overriding the behaviour of the CustomerRepository mock.class CreateOrderTest {        @Test    void createOrder_happyPath() {        final Order order = CreateOrderScenario            .defaultScenario()            .execute();                assertThat(order.getCustomerId()).isEqualTo(CreateOrderScenario.customerId);    }        @Test    void createOrder_customerInactive() {        final UUID customerId = UUID.randomUUID();        final CreateOrderScenario scenario = CreateOrderScenario            .defaultScenario()            .modifyCreateOrderData(orderData -&gt; orderData.setCustomerId(customerId))            .overrideCustomerRepository(repository -&gt;                 when(repository.findById(customerId)).thenReturn(Optional.empty()));                assertThrows(InvalidCustomerException.class, () -&gt; scenario.execute());    }}Advantages of using an APIThe advantage of this design is that our tests are not aware of:  the use of a factory to create orders,  a validator class, used by the factory to validate the input for creating an order,  and how the constructor of the Order aggregate should be called.With this example, it’s easy to see that our new tests are much less likely to break than our original design. There is a clean layer between the implementation/design of our application, and the tests.This lower coupling makes it easier to refactor the application, and implement new features at a higher pace.Also notice that we didn’t create a test class that maps one-to-one to a production code class.Rather than testing our Order object, our OrderFactory, or our OrderValidator, we test the creation of an aggregate instance. We test what we expect our application to do, not what we expect our class to do.Whenever we have to change the logic of how an Order is created, we know that we have to look in the CreateOrderTest class.We don’t have to look at the OrderTest class, the OrderFactoryTest class, or the OrderValidator test class to see where we should add some tests.ConclusionIn no way is this design perfect, nor will it be suitable in every project.However, it is a good starting point to have a lower coupling between tests and production code.And it’s also a good way to take your test design further and make it more applicable and relevant to your specific project.This creates the opportunity to make a higher level language to express your tests, making them more readable, and express your intent of what your test is verifying more clearly.And even if you’re not convinced of this design, think about a design of your own, and start to improve the readability and maintainability of your tests."
      },
    
      "security-2019-08-22-securing-web-applications-with-keycloak-html": {
        "title": "Securing Web Applications With Keycloak Using OAuth 2.0 Authorization Code Flow and PKCE",
        "url": "/security/2019/08/22/Securing-Web-Applications-With-Keycloak.html",
        "image": "/img/securing-web-applications-with-keycloak/keycloak.jpg",
        "date": "22 Aug 2019",
        "category": "post, blog post, blog",
        "content": "  Gone are the days when we had to write our own login mechanisms and permission systems.This article is about how we can hook up our applications to an Identity and Access Management (IAM) solution such as Keycloak in a secure way.Table of contents  Why Keycloak as Authentication Server  Setting up a Keycloak Server  Creating a New Realm  Creating a Client  Creating Roles and Scopes  Creating a user  Setting up the Front End and Back End Applications          Spring Boot Back End      Angular App: Tour of Heroes        Implementing Security          Implicit Flow versus Code Flow + PKCE      JSON Web Token (JWT)      Resource Server in Spring Boot                  Resource Server Imports          Configuration of the Resource Server          Testing the setup                    Securing The Angular application        ConclusionWhy Keycloak as Authentication ServerYou can find several platforms that handle user logins and resource access management such as Keycloak, OKTA, OpenAM, etc. All those platforms have their own features and possibilities that may be useful for your use case. In this article, we choose Keycloak as authentication and authorization server which is an open-source identity and access management platform (IAM) from Red Hat’s Jboss. We have chosen for Keycloak because it is open-source and well-documented.Keycloak comes with several handy features built-in:  Two-factor authentication  Bruteforce detection  Social login (Facebook, Twitter, Google…)  LDAP/AD integration  …We will go over the basics to get you started.Setting up a Keycloak ServerKeycloak supports multiple ways to be set up.For non-production purposes, it’s easiest to just download and run the standalone, which we will do here. For actual deployments that are going to be run in production you’ll need to configure a shared database for Keycloak storage and set up Keycloak to run in a cluster to avoid a single point of failure.At the time of writing, the latest release version was 11.0.3.  Standalone  Docker$ curl https://downloads.jboss.org/keycloak/11.0.3/keycloak-11.0.3.zip --output keycloak-11.0.3.zip$ unzip keycloak-11.0.3.zip$ cd keycloak-11.0.3/bin/$ ./add-user-keycloak.sh -r master -u admin -p admin$ ./standalonedocker run -p 8080:8080 -e KEYCLOAK_USER=admin -e KEYCLOAK_PASSWORD=admin quay.io/keycloak/keycloak:11.0.3  For Windows users, there is also a standalone.bat in the same folder.The Keycloak server is now running on port 8080.  Use -Djboss.socket.binding.port-offset to change the port.-Djboss.socket.binding.port-offset=1000 will run the server on port 8080 + 1000 = 9080Go to http://localhost:8080 and create an administrator account if you haven’t already.You can now click on Administration Console &gt; and log in using the account you’ve just created.You are now on the pre-defined Master realm. A realm manages a set of users, credentials, roles, and groups. A user belongs to and logs into a realm and they are isolated from one another and can only manage and authenticate the users that they control.The master realm is the highest level in the hierarchy. Admin accounts in this realm have permissions to view and manage any other realm. It’s best to create a new realm to manage our application and users.Creating a New Realm    Create a new realm by clicking on the drop-down arrow next to the realm name in the upper left corner.    In this example, heroes is chosen as the name of the realm.Feel free to change this to the name of your organisation if you have one.Creating a ClientEvery application that interacts with Keycloak is considered to be a client.Let’s create one for the Single-Page App (SPA).Look for the Clients tab in the menu and hit Create.Pick a name you think is suitable and choose OpenID Connect (OIDC) as protocol.The Root URL can remain blank.  After saving, we can see all the configuration options of the client.  Valid Redirect URIs should be set to http://localhost:4200/* as this is the address of our SPA  Allow all origins for testing purposes  Single-page apps use a public client because they can not securely hide client credentials  Make sure the Standard Flow is enabled. All other flows can be disabled    Standard flow is another name for the Authorization Code Flow as defined in the OAuth 2.0 specification.  Direct Access Grants Enabled may remain enabled for now. It will be easy to test our configuration later.Don’t forget to hit Save at the bottom of the form!Creating Roles and ScopesRoles and scopes can be used to provide fine-grained access control to resources. We want them to be present when handling requests with our Spring Boot application. This part is optional, but can provide better insight in managing access to resources down the road.Under Roles &gt; Add Role, enter any name you like. Some standard roles are typically user or admin.  A scope is created in a similar way under Client Scopes &gt; Create.We will not show a consent screen, so you can uncheck this option.  Now add the scope to the client we created earlier under Clients &gt; Client Scopes.  Creating a userTo use our application later, we need a user to log in with.In most corporate environments, users are stored in Active Directory (AD) or LDAP.Keycloak can connect to both AD and LDAP but for our example, we will simply create a user in Keycloak itself.Search for the Users tab in the menu on the left and click Add User.  Check Email Verified as we do not have email configured on our Keycloak server.After creation, you still have to set a password.Go to the Credentials tab and enter a new one.    Make sure the password is not a temporary one.If you created a new role in the previous section, you can assign it to your user under the Role Mappings tab.That’s all for the Keycloak part.Now let’s look at some applications to secure.Setting up the Front End and Back End ApplicationsThe demo setup will consist of:  an Angular SPA project  a Spring Boot application to serve some dataSpring Boot Back EndYou can clone the base setup here and switch to the unsecured branch.It is a very simple application which serves some heroes on /api/heroes and /api/heroes/{id} on port 9090.import org.springframework.web.bind.annotation.CrossOrigin;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@CrossOrigin(\"*\")@RestController@RequestMapping(\"/api/heroes\")public class HeroController {    private List&lt;Hero&gt; someHeroes = List.of(            new Hero(1, \"Ken\"),            new Hero(2, \"Yannick\"),            new Hero(3, \"Pieter\"));    @GetMapping    public List&lt;Hero&gt; heroes() {        return someHeroes;    }    @GetMapping(\"/{id}\")    public Hero hero(@PathVariable(\"id\") String id) {        return someHeroes.stream()                .filter(h -&gt; Integer.toString(h.getId()).equals(id))                .findFirst()                .orElse(null);    }}class Hero {    private final int id;    private final String name;    public Hero(int id, String name) {        this.id = id;        this.name = name;    }    public int getId() { return id; }    public String getName() { return name; }}For this example, every application may request these resources, hence the @CrossOrigin(\"*\").Open the project in your favourite IDE and run it. The application will run on port 9090.Angular App: Tour of HeroesTour of Heroes is the Angular tutorial application. It’s pretty simple but has all the basic components which make up a modern Angular application. And most importantly, it’s kept up to date with the latest version of Angular.Use these commands or download the latest version of the Tour Of Heroes application from the website.$ curl -LO https://angular.io/generated/zips/toh-pt6/toh-pt6.zip$ unzip toh-pt6.zip -d toh$ cd toh$ npm iThe example app uses an in-memory service to return some heroes. Since you’ll want to serve them from Spring Boot instead, change the url in hero.service.ts from \"api/heroes\" into \"http://localhost:9090/api/heroes\".export class HeroService {  private heroesUrl = 'http://localhost:9090/api/heroes';  ...}It will still try to fetch preloaded heroes from memory, so delete the HttpClientInMemoryWebApiModule from app.module.ts.Now you can run the application with$ ng serveand browse to localhost:4200 to see if it works.  Please note that not all functionality of the app is working because - for brevity - we only implemented the GET functionality in the Spring Boot back end.Implementing SecurityWe will now dive into the interesting part: setting up the security of the applications.To understand this section, you should have a basic understanding of OAuth 2.0 and OIDC. If this is not yet the case, this section might be more difficult to understand. You can fast-forward to the next section or start your journey by watching this awesome video by Nate Barbettini from Okta.    Implicit Flow versus Code Flow + PKCEIn this example, we will use the authorization code grant flow with Proof Key for Code Exchange (PKCE) to secure the Angular app. It’s a very long name for what could be shortened to “code flow + PKCE” which is more secure than the implicit flow. In fact, the implicit flow was never very secure to begin with. This is well-explained by Nate Barbettini and Aaron Parecki:    The implicit flow was the easiest to understand, since it required one step less than the standard code flow:  PKCE is an addition on top of the standard code flow to make it usable for public clients. It is already in use for native and mobile clients. PKCE boils down to this:  Give hash of random value to authorization server when logging in to ask for code  Hand over the random value to authorization server when exchanging code for access token  Authorization server returns access token after verifying that hash belongs to random value.  If a fraudster were to intercept our authorization grant (the code), he or she would still not have the code_verifier, which is stored in our SPA client. If he/she tries to exchange the stolen authorization grant without this value, the response would not be a token but rather an HTTP 400:{    \"error\": \"invalid_grant\",    \"error_description\": \"PKCE code verifier not specified\"}This is why the code flow + PKCE is more secure than the implicit flow. Even if an attacker manages to obtain the authorization grant, it’s worthless without the code_verifier.  Note that the HTTP 400 will only occur when using PKCE. If no PKCE is used, the client should be confidential (requiring credentials to exchange the authorization grant) rather than be public.So only our Angular client will be able to retrieve the access token in the form of a JSON Web Token.JSON Web Token (JWT)JSON Web Tokens or JWT, often pronounced as ‘jot’, is an open standard for a compact way of representing data to be transferred between two parties. What this means is that it’s a special kind of object which has some data in it. It can also be digitally signed to make sure it is not tampered with.In its compact form, JSON Web Tokens consist of three parts separated by dots (.), which are:  Header:  the type of the token and the signing algorithm being used  Payload: the payload, which contains the claims and additional data  Signature: to verify if the token was not tampered withTherefore, a JWT typically looks like the following:xxxxx.yyyyy.zzzzzThe header (xxx) and payload (yyy) are base64 encoded. An access token is a good example of a JWT:eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJkdE9JZkY2NGZRYnlwWVRFdGV2eV83NUdIWTdQMmNHU1o2a2ZXWDdFblBJIn0.eyJqdGkiOiIxY2EzZTZkYS1kM2Y2LTRiYTMtYjNjZC1iMDExYmRlM2JmNmIiLCJleHAiOjE1NjYzMjk1NTYsIm5iZiI6MCwiaWF0IjoxNTY2MzI5MjU2LCJpc3MiOiJodHRwOi8vbG9jYWxob3N0OjgwODAvYXV0aC9yZWFsbXMvaGVyb2VzIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6IjBkNjg0OWI4LWUyZmEtNGU3My04NjlhLTE1ZDVhOTE1YzdhMiIsInR5cCI6IkJlYXJlciIsImF6cCI6InNwYS1oZXJvZXMiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI4NWRjYTg0Ny00YmQzLTRiOTUtOTNiYy01MjE5ZjUzYWNiMzciLCJhY3IiOiIxIiwiYWxsb3dlZC1vcmlnaW5zIjpbIioiXSwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwidW1hX2F1dGhvcml6YXRpb24iLCJ1c2VyIl19LCJyZXNvdXJjZV9hY2Nlc3MiOnsiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJlbWFpbCBoZXJvZXMgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJuYW1lIjoiSmVyb2VuIE1leXMiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJqZXJvZW4iLCJnaXZlbl9uYW1lIjoiSmVyb2VuIiwiZmFtaWx5X25hbWUiOiJNZXlzIiwiZW1haWwiOiJtZUBhY21lLmJlIn0.cvn79d-n0aFYqDB3p-htDNeeYOdkvqEsmDhGKp9V3a4i6nJx7dU0_r7zicQe26ZgDsM65ILx_X-buWv-e5_eraFo1OOveCGtBbrrLwrQ0Z7SlVMHJrDooJrLEE_m8Qlz_-iLcEC2-ODroEwyLRej_Du626B48QL2bcq-8ADqGSaLf7Y4ZTVMiP_p6dsCi4GDQLq1WOy-g6--z47FKTJVuAl2yY_JNNuEd5aofw0FTE38EoEinIdcy5NXCXDhtGHr_k5lA2Swu4JvK84YB6usECigCb1_zO_c6LhZQkRTCcCojxC6Qn1trQH9epcFEKTkDCHrNf6BLp4X9rH2URWJcAWe can easily decode them using online tools like jwt.io.  The payload section can have different kinds of data. Since our tokens are used for OAuth and OIDC, they have all kinds of claims, which are key-value statements about an entity. You can compare this to something like a coupon. A coupon has claims about how much it is worth, which product(s) it applies to and when it expires.In our example token there is a claim that name is Jeroen Meys and also that scope is email profile heroes. This scope claim means we can use this token to know my email, some profile info and whatever heroes is needed for (hint: keep on reading to find out!). If you are interested in other JWT use-cases, you should definitely give Using JWT for State Transfer a read.  Be careful with online tools to analyze JWT tokens. You are exposing access tokens to the world!Now that we understand what an access token is and looks like, we can pass it along to the back end which will be configured as a stateless OAuth Resource Server.Resource Server in Spring BootResource server is the OAuth 2.0 terminology for API server. It will look at our access token and decide if we are allowed to perform the requested API action.What we really want to secure is the data served by our Spring Boot app.We don’t want the villains out there to be able to access our list of heroes, do we?Let’s take care of that.While Keycloak provides its own libraries to be used with Spring Boot, I personally favour more generic libraries that are provider unaware. This is in line with the Spring framework design philosophy of deferring design decisions as late as possible.This way, when we feel like it, we can more easily switch from one solution (Keycloak) to another (eg. Okta), as long as they support the OIDC protocol. The de facto standard for securing Spring Boot applications is Spring Boot Security. It has resource server support, so this is what we’ll be using.Resource Server ImportsLet’s add the dependecies to our build.gradle or pom.xml file:  Gradle  Mavendependencies {\t...\timplementation 'org.springframework.boot:spring-boot-starter-security'\timplementation 'org.springframework.security:spring-security-oauth2-resource-server'\timplementation 'org.springframework.security:spring-security-oauth2-jose'}&lt;dependencies&gt;\t...\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;\t&lt;/dependency&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.springframework.security&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-security-oauth2-resource-server&lt;/artifactId&gt;\t&lt;/dependency&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.springframework.security&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-security-oauth2-jose&lt;/artifactId&gt;\t&lt;/dependency&gt;&lt;/dependencies&gt;  spring-boot-starter-security: starter dependency for Spring Security  spring-security-oauth2-resource-server: dependency to use our application as a Resource Server  spring-security-oauth2-jose: support for the Javascript Object Signing and Encryption frameworkConfiguration of the Resource ServerThen all we have to do is some configuring.import org.springframework.security.config.annotation.web.builders.HttpSecurity;import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;@EnableWebSecuritypublic class ResourceServerConfig extends WebSecurityConfigurerAdapter {    @Override    protected void configure(HttpSecurity http) throws Exception {        http.cors().and()                .authorizeRequests()                .mvcMatchers(\"/api/heroes/**\").hasAuthority(\"SCOPE_heroes\")                .anyRequest().denyAll()                .and()                .oauth2ResourceServer()                .jwt();    }}With these few lines of code, a lot is happening. We told Spring Security that all endpoints are forbidden, except for /api/heroes/** when the heroes scope is present. This will be checked against the JWT access token.The oauth2ResourceServer() sets up the application as a resource server. It will check if there is an access token on every request and whether it is valid or not. In order to verify that a token hasn’t been tampered with, we need some information from Keycloak, which it exposes via a REST endpoint.We add the endpoint to our application.properties:server.port=9090spring.security.oauth2.resourceserver.jwt.jwk-set-uri=http://localhost:8080/auth/realms/heroes/protocol/openid-connect/certsTesting the setupThat’s all for the resource server part.Don’t forget to restart the Spring Boot application after these changes!We can use curl to verify if our security is working correctly.$ curl -i http://localhost:9090/api/heroesHTTP/1.1 401Set-Cookie: JSESSIONID=A2268A4D12924631929BEBDA57CB2333; Path=/; HttpOnlyWWW-Authenticate: BearerX-Content-Type-Options: nosniffX-XSS-Protection: 1; mode=blockCache-Control: no-cache, no-store, max-age=0, must-revalidatePragma: no-cacheExpires: 0X-Frame-Options: DENYContent-Length: 0Date: Sun, 18 Aug 2019 21:15:14 GMTWithout a token, the server responds with HTTP 401. This means we are not authorized. As we don’t have a login form available just yet, we can use the Direct Access Grants flow to obtain a token. This can come in very handy for testing different scenarios as well.$export TOKEN=$(curl -H \"Content-Type: application/x-www-form-urlencoded\" \\  -d \"client_id=spa-heroes\" \\  -d \"username=jeroen\" \\  -d \"password=1234\" \\  -d \"grant_type=password\" \\  -X POST http://localhost:8080/auth/realms/heroes/protocol/openid-connect/token | jq -r .access_token)$echo $TOKENeyJhbGciOeUNvcWVJVWxNIn0.eyJqdGkiOiI......Hnz5aFdcAiB-5o-yep6rcGP_H6yQoW  Make sure ‘Direct Access Grants Enabled’ is enabled in the Keycloak Client settingsWe can now use this token to request the resource once more:$ curl -i -X GET -H \"Authorization: Bearer $TOKEN\" http://localhost:9090/api/heroesHTTP/1.1 200X-Content-Type-Options: nosniffX-XSS-Protection: 1; mode=blockCache-Control: no-cache, no-store, max-age=0, must-revalidatePragma: no-cacheExpires: 0X-Frame-Options: DENYContent-Type: application/json;charset=UTF-8Transfer-Encoding: chunkedDate: Sun, 18 Aug 2019 21:25:36 GMT[{\"id\":1,\"name\":\"Ken\"},{\"id\":2,\"name\":\"Yannick\"},{\"id\":3,\"name\":\"Pieter\"}]This time we got the requested resource back from the server!Securing The Angular applicationWhen we now look at our Tour Of Heroes application again, it’s complaining a bit:HeroService: getHeroes failed: Http failure response for http://localhost:9090/api/heroes: 401 OKWe didn’t log in (HTTP 401), so we can’t see the content. Time to fix it!There are many packages we could use to secure our Angular app. An easy one to get started with, is angular-oauth2-oidc from Manfred Steyer but you could use any library, as long as it’s certified by OpenID.Add it to the dependencies:$ npm i angular-oauth2-oidc --saveThen add it to the imports of app.module.tsimport { HttpClientModule } from '@angular/common/http';import { OAuthModule } from 'angular-oauth2-oidc';@NgModule({  imports: [    ...    HttpClientModule,    OAuthModule.forRoot({      resourceServer: {          allowedUrls: ['http://localhost:9090/api'],          sendAccessToken: true      }  })Notice how we configure localhost:9090/api to be the only URL where we will send our access token to.Next up, let’s add log in and log out buttons in app.component.html.&lt;button class=\"btn btn-default\" (click)=\"login()\"&gt;  Login&lt;/button&gt;&lt;button class=\"btn btn-default\" (click)=\"logoff()\"&gt;  Logout&lt;/button&gt;Now all there is left to do is to configure the OAuthService. We do this in app.component.ts:import { Component } from '@angular/core';import { OAuthService, AuthConfig } from 'angular-oauth2-oidc';@Component({  selector: 'app-root',  templateUrl: './app.component.html',  styleUrls: ['./app.component.css']})export class AppComponent {  title = 'Tour of Heroes';    constructor(private oauthService: OAuthService) {    this.configure();  }  authConfig: AuthConfig = {    issuer: 'http://localhost:8080/auth/realms/heroes',    redirectUri: window.location.origin + \"/heroes\",    clientId: 'spa-heroes',    scope: 'openid profile email offline_access heroes',    responseType: 'code',    // at_hash is not present in id token in older versions of keycloak.    // use the following property only if needed!    // disableAtHashCheck: true,    showDebugInformation: true  }    public login() {    this.oauthService.initLoginFlow();  }    public logoff() {    this.oauthService.logOut();  }    private configure() {    this.oauthService.configure(this.authConfig);    this.oauthService.loadDiscoveryDocumentAndTryLogin();  }}Make sure that the name of the realm (heroes) and the client id (spa-heroes) correspond to the ones you defined in Keycloak. Remember how we required the heroes scope to be present in our back end? The scope property is how we fix this. If we omit heroes from the scope list, we will be getting a 403 response from our resource server.  Update: Older versions of Keycloak did not include the at_hash claim of the access token in the id token. The client library would crash while parsing it, even with the disableAtHashCheck enabled.This has now been fixed.We can now try to log in and if all went well, we should see our heroes appear again when browsing to /dashboard or /heroes.  Don’t worry if you do not immediatly see the heroes appear. This is because we load the /heroes page before our code was exchanged for an access token.This results in the first /heroes call getting a 401 response. You can create a new endpoint and use it as the redirectUri to get rid of this problem.  ConclusionWe set up a Keycloak server and covered how we can secure a Spring Boot API by turning it into a resource server. We then discussed why the authorization grant flow + PKCE replaces the implicit flow and how to implement it in an Angular application."
      },
    
      "security-2019-08-14-using-lets-encrypt-certificates-in-java-html": {
        "title": "Using Let's Encrypt certificates in Java applications",
        "url": "/security/2019/08/14/Using-Lets-Encrypt-Certificates-In-Java.html",
        "image": "/img/lets-encrypt.png",
        "date": "14 Aug 2019",
        "category": "post, blog post, blog",
        "content": "  At some point in their career, developers come accross the need to work with security certificates.This article describes how to setup Let’s Encrypt, retrieve a certificate, renew it automatically and use the certificate in a Java application for TLS communication.Table of contents  Certificate Authorities and Let’s Encrypt  Installing a Let’s Encrypt certificate  Certificate renewal  Automating the renewal process  Using the certificates in a Java application  ResourcesCertificate Authorities and Let’s EncryptWhen you want to enable HTTPS on your website or need certificates for TLS communication, you’ll need to request this certificate from a Certificate Authority (CA).It acts as a trusted third party between two parties that need to communicate with each other.Let’s Encrypt is such a Certificate Authority.It is their mission to give everyone a secure and privacy-respecting web experience.That’s why they issue certificates free of charge.Installing a Let’s Encrypt certificateAssuming that you have shell access to your server, Let’s Encrypt recommends to use Certbot ACME Client, since it can automate certificate issuance and installation with zero downtime.Certbot is a free, open source software tool for automatically using Let’s Encrypt certificates on manually-administrated websites to enable HTTPS.Clear installation instructions can be found on the Certbot website.Select your web server software (Apache, Nginx, …) and operating system and Certbot provides the installation instructions.  You can check your operating system on Linux by executing cat /etc/os-release.Please note that these instructions also include setting up HTTPS for your website, which for this tutorial isn’t necessary.We’ll use the certificate in another way, for TLS communication in a Java application.For Ubuntu, the following steps are required to install Certbot.See also Apache on Ubuntu 16.04 (xenial).Certbot is installed using APT (Advanced Package Tool), a tool for installing and removing applications on Debian based systems. This tool searches in its repositories for software distributions.Before you can install Certbot, you’ll need to add the Certbot PPA (Personal Package Archive) to your list of available APT repositories.sudo apt-get updatesudo apt-get install software-properties-commonsudo add-apt-repository universesudo add-apt-repository ppa:certbot/certbotsudo apt-get updateRun the following command to install Certbot.sudo apt-get install certbot python-certbot-apacheBy default, certbot retrieves a certificate and installs it immediately on your web server by adding an extra parameter, eg. --apache for Apache HTTP Server.For our situation, it is enough to retrieve a certificate.This is done by adding the certonly parameter to the command as follows:certbot certonlyYou can find the installed Let’s Encrypt certificates in the /etc/letsencrypt/live folder on your file system.Certificate renewalLet’s Encrypt CA issues short-lived certificates of 90 days.Therefore certificates must be renewed at least once in 3 months.Certificate renewal is actually quite simple with Certbot.You can renew the certificates with the following command:certbot renew  Add --dry-run to the command if you want to try it out without consequences.Executing this command multiple times is not a problem.When the certificate is not due for renewal, nothing will happen and you’ll receive an output comparable to this:Saving debug log to /var/log/letsencrypt/letsencrypt.log- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Processing /etc/letsencrypt/renewal/mydomain.be.conf- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Cert not yet due for renewal- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -The following certs are not due for renewal yet:  /etc/letsencrypt/live/mydomain.be/fullchain.pem expires on 2019-09-14 (skipped)No renewals were attempted.- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Automating the renewal processCertbot automatically renews certificates on most operating systems now.Check your operating system’s crontab (typically in /etc/crontab/ and /etc/cron.*/* and systemd timers (systemctl list-timers).On our Ubuntu system we executed systemctl list-timers and found a certbot.timer.NEXT                          LEFT          LAST                          PASSED       UNIT                         ACTIVATESWed 2019-08-14 10:47:41 CEST  1h 19min left Tue 2019-08-13 18:00:03 CEST  15h ago      certbot.timer                certbot.serviceIt basically boils down to the certbot renew command being executed periodically.  If your Linux distribution package didn’t install the cronjob, you can easily set this up yourself.Since we need to automate the keystore and truststore creation as well, you can look at the section Automate the keystore and truststore creation process for more information on creating cronjobs.Using the certificates in a Java applicationAll generated keys and issued Let’s Encrypt certificates can be found in the /etc/letsencrypt/live folder on your file system.We will now see how we can import them in Java keystore files to use them in a Java application.Importing certificates into cacertsThe first way you can use certificates in a JVM is to add them to the cacerts file of your Java distribution.Every JRE has its own keystore, which contains all Certificate Authorities it trusts.This is also referred to as a truststore.This truststore is stored as a file called cacerts.It is typically located in $JAVA_HOME/jre/lib/security assuming $JAVA_HOME is where your JRE or JDK is installed.The default password for this keystore is changeit.The following command imports the certificates into your JRE truststore.keytool -import -alias mydomain.be \\\t-keystore $JAVA_HOME/jre/lib/security/cacerts \\\t-file /etc/letsencrypt/live/mydomain.be/cert.pem \\\t-storepass changeit \\\t-nopromptPlease note that adding certificates to cacerts is not always the best solution.Although technically it is a fully functional keystore file, its purpose is mainly for determining which third-party certificates to trust.On top of this, it is tied to your Java installation and when you install another JRE or JDK, you’ll need to add the certificates again.Our preferred approach is to add your own certificates to a keystore and the third-party certificates to a separate truststore.Continue reading to see how you can do that.Creating a separate .keystore fileTo use a certificate in a Java application, the preferred way is to add it to a separate .keystore file.The Java Runtime Environment (JRE) ships with a tool called keytool to create certificates and manipulate key stores.Adding certificates to a keystore can be done by using OpenSSL and the keytool.You cannot import multiple public and private .pem certificates directly in a keystore, so you’ll first need to add all .pem files to a PKCS 12 archive.We do this with the OpenSSL tool with the following command.openssl pkcs12 -export \\\t -in /etc/letsencrypt/live/mydomain.be/cert.pem \\\t -inkey /etc/letsencrypt/live/mydomain.be/privkey.pem \\\t -out /tmp/mydomain.be.p12 \\\t -name mydomain.be \\\t -CAfile /etc/letsencrypt/live/mydomain.be/fullchain.pem \\\t -caname \"Let's Encrypt Authority X3\" \\\t -password pass:changeitChange mydomain.be with your own DNS name.The next step is to import the certificates into a .keystore file.keytool -importkeystore \\\t-deststorepass changeit \\\t-destkeypass changeit \\\t-deststoretype pkcs12 \\\t-srckeystore /tmp/mydomain.be.p12 \\\t-srcstoretype PKCS12 \\\t-srcstorepass changeit \\\t-destkeystore /tmp/mydomain.be.keystore \\\t-alias mydomain.beYou can now load the keystore at location /tmp/mydomain.be.keystore in your Java application.  Please note that you not only need to create a keystore with your own certificates, but also a truststore with the trusted third-party certificates.However, the approach is exactly the same.Automate the keystore and truststore creation processCreate a shell script /home/&lt;username&gt;/renew-keystore.sh with the following content:#!bin/bash# Create keystoreecho \"Refreshing '~/ssl/mydomain.be.keystore'\"openssl pkcs12 -export \\\t -in /etc/letsencrypt/live/mydomain.be/cert.pem \\\t -inkey /etc/letsencrypt/live/mydomain.be/privkey.pem \\\t -out /tmp/mydomain.be.p12 \\\t -name mydomain.be \\\t -CAfile /etc/letsencrypt/live/mydomain.be/fullchain.pem \\\t -caname \"Let's Encrypt Authority X3\" \\\t -password pass:changeitkeytool -importkeystore \\\t-deststorepass changeit \\\t-destkeypass changeit \\\t-deststoretype pkcs12 \\\t-srckeystore /tmp/mydomain.be.p12 \\\t-srcstoretype PKCS12 \\\t-srcstorepass changeit \\\t-destkeystore /tmp/mydomain.be.keystore \\\t-alias mydomain.be# Move certificates to other serversecho \"Copy '~/ssl/mydomain.be.keystore' to cluster servers\"cp /tmp/mydomain.be.keystore /home/admin_jworks/ssl/mydomain.be.keystorescp  /tmp/mydomain.be.keystore cc-backend-node-02:/home/admin_jworks/ssl/mydomain.be.keystorescp  /tmp/mydomain.be.keystore cc-frontend-node-01:/home/admin_jworks/ssl/mydomain.be.keystore# Create truststoreecho \"Refreshing '~/ssl/theirdomain.be.keystore'\"rm theirdomain.be.keystoreopenssl s_client -connect theirdomain.be:443 -showcerts &lt;/dev/null 2&gt;/dev/null|openssl x509 -outform DER &gt;theirdomain.deropenssl x509 -inform der -in theirdomain.der -out theirdomain.pemkeytool -import \\\t-alias theirdomain \\\t-keystore theirdomain.be.keystore \\\t-file ./theirdomain.pem \\\t-storepass theirdomain \\\t-nopromptecho \"Copy '~/ssl/theirdomain.be.keystore' to cluster servers\"cp theirdomain.be.keystore /home/admin_jworks/ssl/sudo scp ssl/theirdomain.be.keystore cc-backend-node-02:/home/admin_jworks/ssl/sudo scp ssl/theirdomain.be.keystore cc-frontend-node-01:/home/admin_jworks/ssl/You might not need everything from this script.It does more than creating a new keystore:  It creates the keystore mydomain.be.keystore as described in the previous section Creating and using a separate .keystore file  It creates a truststore by connecting to the third-party server, writing their certificate to a file called theirdomain.pem and importing that file in theirdomain.be.keystore  It also copies both keystore and truststore files to other servers in our clusterThe command to execute this shell script is installed in one of the following locations: /etc/crontab/, /etc/cron.*/* or systemctl list-timers.eg. To execute the script once every hour, you can add it to /etc/cron.hourly.We will edit the contents of the crontab file on the system with crontab -e.The line should start with a cron expression telling the system when to execute the task followed by the command to be executed.Example of job definition:.---------------- minute (0 - 59)| .------------- hour (0 - 23)| | .---------- day of month (1 - 31)| | | .------- month (1 - 12) OR jan,feb,mar,apr ...| | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat| | | | |* * * * * command to be executedYou can read more on cron expressions in the Baeldung blog A Guide To Cron Expressions.The following line in crontab makes sure our script is executed every hour.0 * * * * bash /home/&lt;username&gt;/renew-keystore.sh &gt;&gt; /var/log/renew-keystore.log Remember to set execute permissions on the created script to allow the system to run the script.chmod +x /home/&lt;username&gt;/renew-keystore.shThere’s no need to restart something after changing the crontab file.Cron will examine the modification time on all crontabs and reload those which contain changes.Using keystores and truststores in a Java applicationSpring Boot configuration propertiesWe’ll be using Spring Boot to externalize our TLS configuration.First, we add properties to point to our keystore and truststore archives on the filesystem and provide the necessary passwords.  Please note that there are existing Spring Boot properties prefixed with server.ssl to configure TLS.However, these properties are used for securing connections to your Tomcat server.They will not configure HTTP clients used within your application.We also need to configure more information about the service we’re consuming, eg. the endpoint url.We therefore specify our own properties.myprefix:    client:        remote-service-endpoint-url: https://www.theirdomain.be/services/3.0        trust-store: /ssl/theirdomain.be.jks        trust-store-password: changeit        key-store: /ssl/mydomain.be.keystore        key-store-password: changeitThen we load those properties in a @ConfigurationProperties object.@Configuration@ConfigurationProperties(\"myprefix.client\")public class MyClientProperties {    private String remoteServiceEndpointUrl;    private String keyStore;    private String keyStorePassword;    private String trustStore;    private String trustStorePassword;    // Getters and setters}This class is instantiated by Spring and can be autowired in other beans.Java’s SSLContext and HTTP clientsIn Java there are several frameworks you can use to establish an HTTP connection.  Java’s built-in HttpURLConnection  Apache HttpComponents HttpClient – Please note that this is the successor of Commons HttpClient.If you’ll be using this client, make sure you’re importing the org.apache.httpcomponents version.  OkHttpJava supports TLS communication through its javax.net.ssl.SSLContext class.We’ll be using Apache’s HttpClient to setup TLS communication.This library has builder classes with which you can easily create an SSLContext.Add the following Maven dependencies.&lt;dependency&gt;    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;    &lt;artifactId&gt;httpcore&lt;/artifactId&gt;    &lt;version&gt;${httpcore.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;    &lt;artifactId&gt;httpclient&lt;/artifactId&gt;    &lt;version&gt;${httpclient.version}&lt;/version&gt;&lt;/dependency&gt;Now you can use the org.apache.http.ssl.SSLContexts and org.apache.http.ssl.SSLContextBuilder to create an javax.net.ssl.SSLContext.SSLContext sslContext = SSLContexts    .custom()    .loadKeyMaterial(Paths.get(properties.getKeyStore()).toFile(), properties.getKeyStorePassword().toCharArray(), properties.getKeyStorePassword().toCharArray())    .loadTrustMaterial(Paths.get(properties.getTrustStore()).toFile(), properties.getTrustStorePassword().toCharArray())    .build();This SSLContext can be used to create an Apache org.apache.http.impl.client.CloseableHttpClient.We create the CloseableHttpClient with the org.apache.http.impl.client.HttpClients utility class.final CloseableHttpClient client = HttpClients    .custom()    .setSSLContext(sslContext)    .build();If you don’t have the .setSSLContext(sslContext), please check your org.apache.httpcomponents:httpclient version.Each HTTP request executed using this client will be sent over a TLS connection.  Please note that you also have the possibility to set the following Java system properties and ensure all communication uses TLS.  System.setProperty(\"javax.net.ssl.enabled\", \"true\");System.setProperty(\"javax.net.ssl.trustStore\", properties.getTrustStore());System.setProperty(\"javax.net.ssl.trustStorePassword\", properties.getTrustStorePassword());System.setProperty(\"javax.net.ssl.keyPassword\", properties.getKeyStorePassword());System.setProperty(\"javax.net.ssl.keyStore\", properties.getKeyStore());System.setProperty(\"javax.net.ssl.keyStorePassword\", properties.getKeyStorePassword());System.setProperty(\"javax.net.ssl.clientAuth\", \"need\");    Although it’s a valid possibility, these are settings for the entire system.On top of that, when you need to integrate with multiple third-parties and are dealing with multiple trusted parties and multiple public/private keypairs, it can become a mess to add everything to single keystore and truststore files.Using the keystore with Spring RESTIn the previous section we learned how to create an javax.net.ssl.SSLContext and an Apache HttpComponents CloseableHttpClient.Calling REST endpoints with Spring REST is done by using the org.springframework.web.client.RestTemplate class.This class is part of the spring-web module, which is automatically added by adding the spring-boot-starter-web dependency.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;Spring’s RestTemplate is an abstraction of the different HTTP clients we can use in Java.Under the hood, when a request is executed on the RestTemplate, Spring uses the passed org.springframework.web.client.ClientHttpRequestFactory to create a org.springframework.http.client.ClientHttpRequest, executes the request and transforms it to a org.springframework.http.client.ClientHttpResponse.There’s a specific implementation of these classes for each HTTP client.For example, for Apache’s HttpComponents HttpClient, you can find classes with the prefix HttpComponents:  org.springframework.http.client.HttpComponentsClientHttpRequestFactory  org.springframework.http.client.HttpComponentsClientHttpRequest  org.springframework.http.client.HttpComponentsClientHttpResponseWe’ll use HttpComponentsClientHttpRequestFactory to customize the RestTemplate and we register it with the Spring context by annotating the method with @Bean in one of the configuration classes.@Beanpublic RestTemplate restTemplate() throws Exception {    return new RestTemplate(new HttpComponentsClientHttpRequestFactory(client));}Please note that in the above code snippet, client must be an instance of Apache’s HttpComponents HttpClient, eg. the CloseableHttpClient we created in the previous section.Using the keystore with Spring WSFrom time to time you have to integrate with a SOAP web service from the customer or a third party and use TLS communication when doing so.In the following example, we’ll use Spring Web Services to implement this.Spring WS defines an interface org.springframework.ws.client.core.support.WebServiceGatewaySupport of which you can create an instance.It uses an attribute of the type org.springframework.ws.transport.WebServiceMessageSender to do the actual communication.Like with Spring REST, there are specific implementations for each HTTP client library.For Apache HttpComponents HttpClient, this is the org.springframework.ws.transport.http.HttpComponentsMessageSender.It accepts an org.apache.http.client.HttpClient to use for low-level communication.Your application must provide a bean instance of the type WebServiceGatewaySupport.On this object, you can set an instance of a WebServiceMessageSender.@Beanpublic Jaxb2Marshaller marshaller() {    Jaxb2Marshaller marshaller = new Jaxb2Marshaller();    marshaller.setContextPath(\"fully.qualified.package.name.of.generated.sources\");    return marshaller;}@Beanpublic MyClient myClientSecure(Jaxb2Marshaller marshaller) {    MyClient client = new MyClient(properties);    String url = properties.getRemoteServiceEndpointUrl();    client.setDefaultUri(url);    client.setMarshaller(marshaller);    client.setUnmarshaller(marshaller);    client.setMessageSender(new HttpComponentsMessageSender(client));    return client;}Like in the Spring REST example, client must be an instance of Apache’s HttpComponents HttpClient, eg. the CloseableHttpClient we created earlier.Calling a service with the org.springframework.ws.client.core.WebServiceTemplate of our MyClient bean now uses the configured keystore and truststore.You can call a SOAP endpoint with getWebServiceTemplate().marshalSendAndReceive(...).Please note that our MyClient class extends WebServiceGatewaySupport.The code snippet below also includes a sample call.RequestType and ResponseType are classes generated from the wsdl file and typically reside in the target/generated-sources directory of your project.@Componentpublic class MyClient extends WebServiceGatewaySupport {    public ResponseType getMonthlyAlertDetail(BigInteger alertId, String apiKey) {        return JAXBElement&lt;ResponseType&gt; response = (JAXBElement&lt;ResponseType&gt;) getWebServiceTemplate()                .marshalSendAndReceive(new RequestType(...), message -&gt; ((SoapMessage) message).setSoapAction(\"SoapOperationName\"));    }}Using the keystore in KeycloakIf you’re using a product like eg. Keycloak on your server, the way of using the certificate stays the same.It’s even easier, as you don’t need to write code to read the .keystore file.You can point to the .keystore file in the configuration files for that product.This is an example Keycloak configuration in the standalone.xml file:&lt;security-realm name=\"ApplicationRealm\"&gt;    &lt;server-identities&gt;        &lt;ssl&gt;            &lt;keystore path=\"/tmp/mydomain.be.keystore\" relative-to=\"jboss.server.config.dir\" keystore-password=\"changeit\" alias=\"mydomain.be\" key-password=\"changeit\" generate-self-signed-certificate-host=\"localhost\"/&gt;        &lt;/ssl&gt;    &lt;/server-identities&gt;&lt;/security-realm&gt;Resources  Let’s Encrypt Getting Started  Certbot  Certbot CLI Renewing certificates  OpenSSL  Oracle Java keytool documentation  PKCS 12 information  Spring Web Services"
      },
    
      "cloud-2019-08-05-deploy-spring-boot-kubernetes-html": {
        "title": "Deploying your Spring Boot application with Kubernetes",
        "url": "/cloud/2019/08/05/deploy-spring-boot-kubernetes.html",
        "image": "/img/2019-07-11-deploy-spring-boot-kubernetes/banner.jpg",
        "date": "05 Aug 2019",
        "category": "post, blog post, blog",
        "content": "IntroductionToday we are going to look at the features and benefits of using a Kubernetes cluster to deploy your application. As I am mostly focused on Java development, we will use a standard Spring Boot application as an example.Assuming you have already heard of Kubernetes, you are probably aware of the continuing growth of this platform. More and more Kubernetes based platforms are growing in popularity because of the proven record of Kubernetes. Examples are OpenShift, Cloud Foundry, PKS, ….As adaptation is growing, many developers are wondering how to effectively use these platforms to deploy their application in the cloud on a Kubernetes cluster and make full use of its benefits.Many big providers have already picked up Kubernetes and are providing their own (semi) managed implementations. A couple of examples are Amazon Web Services (EKS), Google Cloud Platform (GKE), Azure (AKS), DigitalOcean, ….In this post we will take a look at how you can use Kubernetes to deploy a Spring Boot application.Prerequisites  Docker  kubectl  A Spring Boot project  A Kubernetes cluster.This can be a cluster in the cloud, in an on-premise datacenter or you can use minikube if you want to try this on your local machine.You can use the project from the prerequisites if you want to try it out with a sample project. This blog post will be based on this project. If you are using an other project, then change the names and labels where necessary.First things first: creating a Docker imageKubernetes works with Docker images. This means that your application needs to be dockerized so it can be pushed to a Docker registry. You can find a sample Dockerfile below.FROM openjdk:8-jre-alpineWORKDIR /tmpCOPY target/kubernetesdemo-0.0.1-SNAPSHOT.jar app.jarENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]This is a very basic Dockerfile but it will do for our example.The first line tells us to use the 8-jre-alpine image from the openJDK repository as our base image.The second line tells the image that it should work from the /tmp directory.The third line copies the compiled JAR (which is compiled with the mvn clean install command) file from your target folder to the Docker image (you might have to rename the file depending on the name of your project).Finally, we tell our image to use the java command as entry point, meaning that once the Docker image starts running, it has to execute that command.You can now push this image to your favourite Docker registry, as Kubernetes will need to pull this image from the registry later.If you do not have a Docker registry, I suggest using Docker Hub.If you are using Docker Hub, you can use the following commands to build and push your application to the registry:docker build -t DOCKER_HUB_USERNAME/APPLICATION_NAME .docker push DOCKER_HUB_USERNAME/APPLICATION_NAMEWhere the magic happens: deploying your application on the Kubernetes clusterAll Kubernetes configurations are written in YAML. The reason for this is that Kubernetes configuration files are meant to be easily readable by the human eye and the Kubernetes team decided to use YAML instead of JSON.DeploymentNow that the Docker image is created, we can now deploy it on the Kubernetes cluster. First we need to create a deployment configuration file. This file contains the configuration on how the application should run.apiVersion: apps/v1kind: Deploymentmetadata:  name: kubernetesdemo  labels:    app: kubernetesdemospec:  replicas: 1  selector:    matchLabels:      app: kubernetesdemo    spec:      containers:        - name: kubernetesdemo          image: yolanv/kubernetesdemo          ports:            - containerPort: 8080There’s a lot going on here which I will be explaining step by step.The first two lines are telling which Kubernetes API version is being used and what kind of Kubernetes object is being applied. As we want to create a new Deployment, we use the Deployment object (easy, right?).Lines 3 to 6 are just basic metadata tags so the developer knows which application (s)he is working with. This does not affect the behaviour of the application in any way.Lines 7 to 21 are specifying how the container should be made and which image it has to run. This is the image that we created with the Dockerfile earlier in this post. After that, it describes the port that the container should listen to, which is 8080 in this case. The replica value specifies how many ‘instances’ (also called Pods) that should be running. If the application is expecting a lot of requests, it might be useful to declare a higher number of replicas instead of only one.Lines 13 to EOF are specifying the environment variables that the container uses. They can either be hard-coded like SPRING_PROFILES_ACTIVE or a Secret or ConfigMap can be created, which can then be used in a Deployment configuration, as in the example above.ServiceThe Deployment is up and running, but we need some way to access our pod from the outside world. This is where a Service comes in. A Service provides external access to a set of Pods and decides which pod should handle the request.apiVersion: v1kind: Servicemetadata:  name: kubernetesdemospec:  type: NodePort  selector:    app: kubernetesdemo  ports:    - protocol: TCP      port: 8080      nodePort: 30011The first four lines should be familiar.Instead of a Deployment, we are now declaring a Service.There are three types of services you can declare: ClusterIP, NodePort and LoadBalancer. It is not recommended to use NodePort in a production environment because of the limited options. Instead you might want to use a LoadBalancer. Most big cloud providers can provide a LoadBalancer for you. Another option is to use an Ingress, which is a recommended and popular option. If you want to learn more about this, I suggest you consult the official Kubernetes documentation.The nodePort value can be left out if you want Kubernetes to assign a random NodePort to your service.The selector value is meant to find the Pods with the same value as spec.selector.matchLabels from the Deployment configuration. This is how the Service is able to find our Pods.Applying the configurationNow that we have our YAML files, we can apply them to the cluster. You can even combine the two configurations into one file and separate them with ---.You can use this separator to prevent having too many YAML configuration files.You can apply the configuration by using the following command:kubectl apply -f k8s/kubernetesdemo-deployment.yamlThe output will be something like:deployment.apps/kubernetesdemo createdservice/kubernetesdemo createdThe application is now accessible through http://IP_ADDRESS:NODE_PORT. So if you are using Minikube, the IP should be http://192.168.99.100:30080/.ConclusionThere is a lot of documentation available on the internet if you want to learn more about the power of Kubernetes. Think about the options and features that are available when using this platform. You can integrate it with your CI / CD tools (automated deployments!), autoscaling, … . The options are endless. If you have any questions or feedback, I would love to hear them from you."
      },
    
      "testing-2019-07-18-cypress-html": {
        "title": "Cypress: a new kid on the E2E block",
        "url": "/testing/2019/07/18/Cypress.html",
        "image": "/img/cypress/cypress-logo.png",
        "date": "18 Jul 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Intro  Cypress  Comparison  Setting up Cypress  Amazing features  Continuous Integration  ConclusionIntroThose of us familiar with E2E testing a user interface, we all know the struggle.Not only coding with waits and timeouts but setups that were harder to manage especially when running it on a continuous integration platform.Maintaining versions, network issues, browser support, …For as long as I can remember, there was one constant in this setup!Selenium was always there.No matter what framework you used, Nightwatch, Protractor, Gauge, Robot, …Selenium was the man in the middle.All I can remember from building a Selenium setup a few years ago, are all the difficulties and frustrations.Communication was not working, versions were not matching, timeout issues due to network lag, ….But still, Selenium was a dependency that was needed!  Not that I am not grateful for Selenium and its team of developers and maintainers, because let’s be honest, what would we have done without it?But now, ohh yeah, there’s a new kid in town.Cypress  Fast, easy and reliable testing for anything that runs in a browser~CypressCypress is aiming to provide its users with a bundled experience for writing end-to-end tests for web applications.While lots of other frameworks (as mentioned above) are all interacting over a Selenium server,for remote communication and by definition, running its tests outside of the browser, Cypress is executed inside the browser.Therefore Cypress is executed in the same runtime as your application itself.Because of this, Cypress has native access to every single object. The window, the document, a DOM-element, a service worker, … .Cypress does not need to send commands over-the-wire and can just access everything.Comparison                   Selenium      Cypress                  Debugging:      Hard/Remote      Easy/Access to everything/Nice tool              Speed:      Remote = slow      In browser = Fast              Parallel:      V      V              Headless:      V      V              Language support:      Java, Perl PHP, Python, Ruby, C#, Javascript      Javascript              Browser support:      Everything      Only webkit      Basic ArchitectureCypress consists of a few different building blocks.One of them is its own Nodejs process.You can look at this as being a backend.This backend then launches a browser window, sets up a proxy to this browser window and sets the domain to localhost.The browser window has two iFrames inside.One is for Cypress itself.The second one will hold the application under test.To make it possible to communicate with the application’s iFrame, it injects a &lt;script&gt;-tag that also sets its domain to localhost.Because now, both iFrames are running on ‘localhost’, it is possible to access everything of the application.  The proxy is proxying all requests from the web application itself to its backend.Because the proxy is part of the Cypress-setup, Cypress can act as the man-in-the-middle and spy on, mock or modify the requests and responses.Communication between the Cypress Nodejs-backend and the Cypress iFrame, that is running the tests, is through a websocket.There are a few downsides as seen in the comparison.Because Cypress is running inside of the browser, the language is Javascript.Cypress only supports Chrome so far. Although Cypress is working on supporting other browsers.Today there is no way of testing in Safari, Ìnternet Explorer, Edge, …Cypress tests are written using Mocha and Chai.Setting Up CypressInstalling CypressInstallation of Cypress is really simple.You can install it through Yarn:$ yarn add cypress --devOr plain npm:$ npm install cypress --save-devYou can also install it globally.This way you do not need a local package.json to run Cypress and all Cypress commands can be run straight from the command line.Running CypressDepending on how you just installed Cypress, you can run Cypress by:$ yarn run cypress openof$ $(npm bin)/cypress openOr globally:$ cypress openThe first positive surprise!When opening Cypress for the first time, it notices that you haven’t run it before and it kindly scaffolds a /cypress folder with examples into your project.This way, you already have a configured base to start from.    As you can see, Cypress opens its own application.This is kind of a backend application that will orchestrate the tests.Running one of the tests, means that Cypress will open a second window, which is actually just a new browser window.It will then inject itself into that window in one frame, and load the application under test in another frame.Configuring CypressYou can custom configure Cypress by adding a cypress.json file in the root of your project.{  \"baseUrl\": \"http://demo-app.localtest.me/demo-app\",  \"integrationFolder\": \"src\",  \"testFiles\": \"**.spec.js\",  \"reporter\": \"mochawesome\",  \"reporterOptions\": {    \"overwrite\": false,    \"html\": true,    \"json\": false,    \"reportDir\": \"results\",    \"reportFilename\": \"report.html\" }}You can always override these settings on the command-line:$ cypress run --spec src/** -c baseUrl=http://localhostFor a full overview of the configuration options, just check out the docs.ReportersJust like other frameworks, Cypress lets you add custom reporters for the test results.As you can see above, we’ve added mochawesome.Install it via Yarn:$ yarn add mochawesome --devAnd then manually add it to the cypress.json config file.Take a look at the documentation for the configuration.Cypress and TypeScript  Cypress ships with official type declarations for TypeScript. This allows you to write your tests in TypeScript. All that is required is a little bit of configuration. ~docsThe documentation itself is linking to some different examples.You can read all about the setup here.And you can also take a look at the npm package add-typescript-to-cypress here.Writing your first testsAs shown above, you can configure the path to your spec-files.In our case, we are using /src.Cypress will go through that directory and show all the spec files when using Cypress in development mode.When running Cypress command line to only run the tests, it will just run all those spec-files and then create the report.In our small example we have an Angular demo app that has a material sidenav with three links.Dashboard, clients and products.The latter two both have a material datatable.  Let’s say we want to test our clients navigation and datatable.Create a spec file in the /src directory, in our case, named clients.spec.js:/// &lt;reference types=\"Cypress\" /&gt;context('Clients test', () =&gt; {    beforeEach(() =&gt; {      cy.visit('/clients')    })    it('Clients page should have Clients as a title', () =&gt; {      cy.get('.table-container-header h1').contains('Clients');    });    it('Clients table should initially have 20 rows', () =&gt; {        cy.get('.mat-row').then(($rows) =&gt; {          expect($rows.length).to.be.eq(20);        });    });    it('Clients table should show 10 rows when pagesize is set to 10', () =&gt; {        cy.get('mat-select').click();        cy.get('mat-option').contains('10').then(option =&gt; {            cy.wrap(option).contains('10');            option[0].click();            cy.get('mat-select').contains('10');            cy.get('.mat-row').then(($rows) =&gt; {                expect($rows.length).to.be.eq(10);              });        });    });})Running your first testsIf you would now run $ yarn run cypress open.Cypress will open itself, showing you your new spec-file.You can now run your spec-file by double clicking it, or click the ‘run all’ option on the top right of your Cypress application.Changing the spec-file will trigger a reload/retest in your Cypress-environment.  Amazing featuresThe purpose of this post is not to provide you with some sample code, but trying to convince you to take a look at Cypress.To do so, I’ll quickly go over some really nice features because besides the easy setup and nice main application, Cypress has much more to offer.Debugging with CypressOne of the hardest things when writing E2E tests is debugging.Running tests over and over again, while logging everything to check what is going on, is now history.Cypress injects itself in the same window as the application under test, so it has access to everything.Everything, including the debugger.This means that you can actually debug your test code as you would debug the application itself.Although using the debugger is not that straight forward, it’s a great help.Check out out the documentation here.Snapshots  Take a look at the image above.As you can see, Cypress takes a snapshot at every stage of the test.You can navigate through them later and see the snapshot at a specific time and even see the difference in the state of the application before a request and after its response when running XHR requests.Network accessibilityAs mentioned above, Cypress can take snapshots before and after each XHR request.Cypress knows what is going on under the hood because it is running in the same window.This makes it easy to implement stubs and spies.A simple example for our use case would be intercepting the client-service calls and return mocked data.To do so, Cypress needs to run a server.This can be done by just running cy.server().Next step is to define the route you want to listen on and attach new data to it.Cypress enables this with its route configuration.beforeEach(() =&gt; {  cy.server();  cy.route('GET', '**/*/api/client-service/**/*',    {      \"content\":[        {          \"id\":18,          \"firstName\":\"Tatiana\",          \"lastName\":\"Velez\",          \"email\":\"diam.dictum@Proin.net\",          \"birthday\":\"626286135\",          \"city\":\"Dumfries\",          \"zip\":\"694245\"        }      ],      \"pageable\":{        \"sort\": {          \"sorted\":true,          \"unsorted\":false        },        \"pageSize\":20,        \"pageNumber\":0,        \"offset\":0,        \"paged\":true,        \"unpaged\":false      },      \"last\":false,      \"totalElements\":1,      \"totalPages\":1,      \"first\":true,      \"sort\":{         \"sorted\":true,         \"unsorted\":false      },      \"numberOfElements\":1,      \"size\":1,      \"number\":0    }  ).as('stub-clients');     cy.visit('/clients');})  Screenshots and videosAnother cool embedded feature is the ability to capture screenshots or record videos.Cypress comes with screenshot and video recording out of the box.Screenshots always come in handy when trying to find a bug.Cypress even lets you take a screenshot, manually, from within your code.Before, we were always setting this up using ‘yet another plugin’ (and dependency).Continuous integrationRunning Cypress on a continuous integration platform is also pretty easy.You just want Cypress to run the tests and not to open its Electron test manager for development.$ cypress run just does that.Just append the other options you want to override from the cypress.json and you are good to go.$ yarn run cypress run --spec 'src/**/*' --reporter mochawesome --reporter-options reportDir=results,reportFilename=report.html.Example package.json:scripts: {  ...  \"cypress:ci\": \"cypress run --spec 'src/**/*' --reporter mochawesome --reporter-options reportDir=results,reportFilename=report.html\"  ...}    ConclusionAlthough there are still some downsides to this new player in the E2E testing field, such as only supporting JavaScript and Chrome, there are just too many positives to keep it on the sideline.Cypress provides us with a new amazing test runner and manager. It strips all the hassles of previous setups and provides us with a real robust framework.It is providing us with cool but effective features like the snapshot time travel, easy debugging, headful/headless mode for CI and more.Cypress is too good to not take a look at!"
      },
    
      "agile-2019-07-16-fishbones-for-tomorrow-html": {
        "title": "Fishbones for Tomorrow",
        "url": "/agile/2019/07/16/Fishbones-for-tomorrow.html",
        "image": "/img/fishbones-for-tomorrow/main-image.jpg",
        "date": "16 Jul 2019",
        "category": "post, blog post, blog",
        "content": "For quite a few years now, I have been going steady with Japan, the birthplace of so many philosophies, teachings and practices preceding and gearing up to the Agile movement. This is how I came to discover the well-hidden connection between work and fish. In Eastern Asia, the carp or koi is a deeply symbolic and highly respected animal, the presence and/or representation of which is considered a placeholder for luck, prosperity and a long life. No surprise then, that the fish is sometimes used as a metaphor for good business practice and process improvement.Well, here is another piece of Japanese fish for you to fry.Fish that were yesterdayWithout any doubt, Kaoru Ishikawa 石川馨 (1915-1989) was one of the game changing Japanese organizational theorists of our time. With his near-contemporaries Noriaki Kano (of the famous Kano MVP and customer satisfaction model), W. Edwards Deming (of 14 points and PDCA cycle fame), and Hirotaka Takeuchi and Ikujiro Nonaka (who were the first to come up with SCRUM in ‘The New New Product Development Game’, 1986), Ishikawa sort of laid the foundations of organisational models as we know and use them today. Also, Ishikawa-sama is considered a key figure in the development of quality initiatives in Japan and elsewhere.As a baseline, Ishikawa had in mind to change the way people think about life, including work. His notion of organisation-wide quality control, cross-functional and cross-departmental, implied awareness of everybody always being one another’s supplier and customer, which in turn called for continued, multi-directional customer service. This meant that a customer would continue receiving service even after receiving the product. This service would extend across the organisation itself, infecting not just the work and the output but even the everyday lives of those involved.To this end, Ishikawa thought of a set of useful and relevant methods. One of these, and what Ishikawa is mostly famous for, is the fishbone diagram, a.k.a. the Ishikawa or cause-and-effect diagram, often used in the analysis of industrial processes.Most commonly, Ishikawa diagrams are created to brainstorm over the multiple causes for a specific current event or defect. The current status, default status, or, perhaps in most cases, the defect, is shown as the fish’s head, with the causes extending to the left as fishbones; the ribs branching off the backbone for major causes, with sub-branches for root-causes, up to as many levels as required, brainstormed or identified.An example of a fishbone diagram is shown below:You could try doing fishbones at home with your children. They are a lot of fun to do – and they help your children understand that situations are mere results of a chain of previous events, and that nobody really is to blame for them.Fishbone structures with the head at the right side are beautiful – because they are in some way a reflection of evolution and natural selection. They can also be viewed as a representation of complexity, where multiple minute events, small circumstances and off-chance happenings, all contribute to ‘what is’, in the here and the now.Turning around the arrow of timeOkay. This is all pretty useful of course – but somehow, sometimes, in my work as an Agile Coach, I have been coming across teams more keen on exploring ahead and discovering further possibilities and opportunities, than teams that keep looking backward. And recently, in such a case, I thought I might permit myself and the team I was working with to even meddle with the arrow of time.Kaoru Ishikawa-sama might not have approved, but I decided to use a fishbone diagram to focus on the future rather than digging up causes of a problem from the past.For future orientation, one might keep the fish with its head at the right side, now representing an envisioned state, and work out suggested pre-conditions – or one might turn the fish around altogether and make the head the starting point of an ever widening array of possibilities.In my practice, I have noticed that the cause-and-effect method is in fact a really powerful tool to facilitate teams in parallel thinking. After formulating a goal statement that everyone agrees on, defining the cause areas is a helpful step towards discovering shared and alternative trains of thought. But once these areas are clear, every participant makes a deep dive into their personal knowledge and experience with regard to the goal and tries to gather as many ideas as possible, in silence.Then, one by one, the team members present their ideas while others detect and formulate connections, doubles and insights.This technique helps the team to orderly gather a myriad of ideas, and it really sets things in motion. Because from then onwards, with prioritisation and some concrete actions and owners, the fishbone forms the backbone for team improvement towards a self-defined goal.Collaboration is a whole new piece of fishFor us, Agilists and assorted kindred souls, the virtues of thinking and making stuff together are obvious. After all, there is hardly an approach in the Agile framework where co-creation is not written into the very DNA of who we are and what we do.Does it need saying that being out in the world hunting for appropriate liberating structures permitting cooperation and collaboration has become a second nature for me?It could be for you, too. Think of this: when turning around the arrow of time is permitted, if it facilitates working together - then everything else is permitted too, all the way up to imagining a world upside down.Credit  Artwork in the main image by Heather Fortner  Fishbone example taken from 24point0.com"
      },
    
      "conference-2019-07-10-spring-io-2019-html": {
        "title": "Spring IO 2019",
        "url": "/conference/2019/07/10/Spring-IO-2019.html",
        "image": "/img/2019-05-16-Spring-IO-2019/spring-io.png",
        "date": "10 Jul 2019",
        "category": "post, blog post, blog",
        "content": "Spring IO 2019!Spring I/O has become a yearly tradition for our JWorks consultants. With 21 colleagues we went to the conference in beautiful Barcelona on the 16th and 17th of May.The conference was held at the same location as last year, the Palau de Congressos de Barcelona. As indicated last year, this year there was an overlap at the venue with the Barcelona International Motor Show.This gave us the opportunity to take a look at some beautiful cars during breaks.In this blog post we’ll talk about some of the presentations of this year but it is not a complete list.There were so many interesting talks and all of them are available on Youtube.Let us know if we missed anything by filing an issue or contacting us at our general JWorks email.  Moving from Imperative to Reactive by Paul Harris  Configuration Management with Kubernetes, a Spring Boot use case by Nicolas Frankel  Building better monoliths - Modulithic Applications with Spring Boot by Oliver Drotbohm  Cutting-Edge Continuous Delivery: Automated Canary Analysis through Spring based Spinnaker by Andreas Evers  Using Java Modules in Practice with Spring Boot by Jaap Coomans  Stream Processing with the Spring Framework by Josh Long and Viktor Gamov  How Fast is Spring by Dave Syer  Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by Matthias Haeussler  Migrating a modern spring web application to serverless by Jeroen Sterken and Wim Creuwels  Testing Spring Boot Applications by Andy Wilkinson  How to live in a post-Spring-Cloud-Netflix world by Olga Maciaszek and Marcin Grzejszczak  Event-Driven Microservices with Axon and Spring Boot: excitingly boring by Allard Buijze  How to secure your Spring apps with Keycloak by Thomas Darimont  Zero Downtime Migrations with Spring Boot by Alex SotoDay 1: Talks &amp; WorkshopsMoving from Imperative to Reactive by Paul HarrisWhen development started on the Cloud Foundry Java client, Spring Reactor was also rebooted. Which means that they became their very first customer.Paul Harris says that he made all the mistakes you can make with reactive programming.And now he’ll teach us how to avoid many of those.It al started with the Reactive Manifesto in 2013, which came up with four ideas for reactive applications:  Responsive: it should feel as if the application is progressing, with for example, some feedback.  Resilient: if a particular part of you application fails, the remainder should be able to cope with that.  Elastic: make the most out of the resources available to the application.  Message-Driven: more message-driven than event-driven.A manifesto is nice, but it does not compile.The next step was Reactive Streams which defined a set of interfaces for how we might deal with reactive streaming situations.You can distinguish four interfaces:  Publisher: which emits ‘things’ or signals in other words.  Subscriber: listens to those signals.  Subscription: after a subscriber subscribed to a publication a subscription is obtained.  Processor: is a combination of a publisher and a subscriber that allows you to process data.The intention of Reactive Streams was that more useful real world implementations would follow.One of these is Spring Reactor.For a good introduction to Spring Reactor you can read our blog post about it.Spring Reactor contains various reactive frameworks, the three big ones are:  Reactor-Core: the basic provision.  Reactor-Netty: the reactive implementation of Netty.  Reactor-Test: which is a bunch of really good useful methods for testing reactive streams.Before Paul dove in the code he first explained Mono and Flux.a MonoA Reactive Streams Publisher that emits none or a single element.a FluxA Publisher that emits zero to N elements.Paul showed us a demo of how to make a legacy Maven Spring application reactive.In order to do so the following steps were taken:  Add dependency to Spring Boot Starter WebFlux.This is the reactive variant of Web MVC. You shouldn’t need to change anything to keep it running unless you have used specific server features.  Convert the return of a List to a Flux.  Convert return types for repository methods to Mono or Flux.          Use the static method .justOrEmpty of the Mono type in order to deal with an optional.      Use .switchIfEmpty to return a proper error response.      In order to return a Flux: use fromIterable.      Conclusion is that reactive starts off complicated, but it will become easier when you have used it more often. It doesn’t have that many different methods you can use, so all in all it is quite easy to wrap your head around.You can rewatch his talk here:    Configuration Management with Kubernetes, a Spring Boot use case by Nicolas FrankelNicolas Frankel is a Developer Advocate who works for Exoscale, a European cloud hosting provider.In this session, he explained how to correctly configure each environment with its own parameters and settings.There are traditional configuration management tools such as Chef, Ansible, Puppet, … . But what is the point?Docker images are and should always be immutable.They should be configurable depending on the environment where we want to run our image in.A Docker image should be able to run in different environments without problems, this is where Kubernetes comes in. Kubernetes can easily configure and parameterize each Docker image to run in different environments.One thing to remember is that you should make sure that you are working in the correct environment. Nicolas likes to add banners to the page to know in which environment you are currently working. For example, if you are working in the development environment, then you might want to show a big blue ‘Development’ banner, while in production you would prefer using a big, red, blinking one.This can all be done with the power of Kubernetes and immutable images. You can simply declare your environment variables in Kubernetes, then you can inject your environment in your Spring Boot application.There are three ways to access your environment variables in Spring Boot: profiles, @Value or @ConfigurationProperties.To get started in Kubernetes, you have to create a few Kubernetes objects:  (Optional) A Namespace  A Service  A Pod / DeploymentIn the Deployment, you can give the arguments based on the environment that you want to spin up.With a ConfigMap, you can combine your environment variables that belong to each other (ex. database settings, AWS keys, …).Once done defining the ConfigMap, you can import the ConfigMap in your Deployment declaration.What’s also very interesting is that you can declare your environment variables in a seperate Git repository with the use of an initContainer. Of course, you can already do this with Spring Cloud Config. This is just an alternative on the Kubernetes side of configuration management.If you want to read more about Nicolas Frankel’s work, you can read his blog here.You can watch his talk here:    Building better monoliths - Modulithic Applications with Spring Boot by Oliver DrotbohmThis talk caught our attention because we’re currently working at a client where we see some of the limitations of a microservice architecture. We are currently considering merging multiple of them into a more coarse-grained architecture containing multiple more ‘monolithic’ applications.Microservice architectures also have disadvantages!The talk starts off explaining some of the key differences between microservice and classic monoliths from an architectural point of view.The key takeaway here is that although there’s an advantage in terms of architecture degradation, it is harder to accidentally call another microservice than to call a method on another bounded context in a monolith.This advantage comes at a cost, you lose compile-time safety which makes it harder to refactor than a monolith and it is harder to test the whole system.These are especially disadvantageous in the early stages of the project when it is not clear yet what the correct bounded contexts are.The ModulithIt might thus be useful to consider starting off with a well-structured monolith before considering evolving towards microservices.But how do we avoid having our architecture degrade quickly?Enter the modulith; a modulith is basically a monolith with multiple modules with well-defined dependencies in it.To achieve this, Oliver demonstrated a ‘Moduliths’ tool that he’s in process off developing for the Spring framework.The idea is to use a package structure convention and enforce it with tests using Java’s Reflection.In this package structure convention only (public members of) the root package of each module are accessible to other modules.It’s considered the API package of that module while subpackages are considered internal. There’s more to this tool however; another problem of modularizing your application is that you typically want to do integration testing on the bounds of your modules.The ‘Moduliths’ tool allows to bootstrap your module alone or in various configurations with specific module dependencies for integration testing. To top it off, there’s support to generate PlantUML diagrams for documentation purposes!For more details take a look at https://github.com/odrotbohm/moduliths.Alternative approachesThere are of course other ways to divide your application into modules and maintain the architecture:Multiple artifacts(gradle/maven modules).You might get an explosion of artifacts and it can become kind of verbose with all the configuration(pom.xml or build.gradle) files.Additionally, the artifacts are redundant since we’re planning on deploying everything together anyway.There’s also no support to dynamically compose modules for tests since the dependencies are typically statically defined.This was actually the way we were considering handling it at our client.The big advantage here in our eyes is that reflection can be avoided and the architecture can be verified at compile-time. The good news is that it is possible to combine it with the moduliths approach, which might be useful for integration testing.Java(9+) Module SystemCould be used but it’s certainly not designed for this.It definitely doesn’t have any support for dynamically composing your modules for testing.External toolsJQAssist, Sonargraph, jDepend… . These are powerful tools but usually run during the build making the feedback loop bigger.Wrapping it upThe moduliths approach explained in this talk gives us a nice intermediate step towards a better architecture.It alleviates some of the biggest problems with monoliths without introducing new ones using a more complex architecture like microservices!You can rewatch his talk here:    Cutting-Edge Continuous Delivery: Automated Canary Analysis through Spring based Spinnaker by Andreas Evers  The ultimate goal of continuous delivery is to deploy software quickly and automatically.This can only be achieved if we are able to push new code without fear.Throughout the years, Andreas saw that there are two opposing forces that are battling it out.On the one hand there is the need for speed while on the other hand there is the need for confidence.Like, updating in production without testing will give you great speed, but not much confidence.Microservices using integration tests on an acceptance environment might mean that you test an already obsolete topology because microservices can change that quickly.Contract testing does not cover all the aspects needed to provide confidence as it does not test behaviour.A good alternative that Andreas talked about is Canary Analysis.In order to do so let’s first introduce Spinnaker:Spinnaker is an open source, multi-cloud continuous delivery platform created at Netlfix.It supports a lot of cloud environments like: OpenStack, AWS, Google Cloud, Microsoft Azure, Cloud Foundry, … Major contributors are Netflix, Google, Microsoft, Pivotal, …Under the hood, Spinnaker is composed of a bunch of Spring Boot microservices.Another important component of Spinnaker is Halyard: a bill of materials for the different microservices of Spinnaker, helping you with the deployment of Spinnaker.Spinnaker also integrates well with your CI environments.Cloud deployments are often complex:  Different regions  Different accounts for your environments (production, acceptance, … )Teams want an easy road into the cloud, no complexity to deploy.On the other hand easy rollbacks are important.Spinnaker can help you with this!Various deployment strategies exist:  Red/Black  Rolling Red/Black  Canary analysis You can define a pipeline to deploy into production:For every stage you will have a series of steps.Within every step you can define multiple tasks and every task has some operations which get executed.A lot of these steps are very specific depending on the cloud which Spinnaker tends to abstract away.Spinnakers makes it possible to go fast but still do it safely:  Automated rollbacks  Deployment windows  Cluster locking  Traffic guards which are extra safeguards which can be configured  Manual judgements which makes use of the human “gut” feeling, which a computer does not have  …Andreas had a Rick &amp; Morty demo application of which he had an old, already deployed version and a new version.When doing canary, it is wise to startup a baseline, the old version, so that you have the solid baseline to measure against.Spinnaker will also look at JVM metrics like memory, CPU, etc. But you can also define business metrics like startup time of the app, response times, …When the canary fails, it will just rollback and restore the previous version.Spinnaker will decide if the canary fails by looking at the statistics it gathered.Canary testing allows you to test with real users and real production data.At the same time it reduces the possible impact of your new version on end users.You can rewatch his talk here:    Using Java Modules in Practice with Spring Boot by Jaap CoomansCurrent StateFirst Jaap started with addressing the current state of the module system:  Most tooling support is good (Maven, IDE, …)  In frameworks the adoption is very low  For developers it is even lowerIt can be summarized as: it’s like eating vegetables, we know it is healthy, we know its beneficial, but we don’t do it.Using modules?!What challenges will you face when you want to migrate to modules.  Split packages: packages with the same name exposed by more than one module.  Automatic modules: plain JAR on your module path and thus interpreted as a module.This exports and opens all packages, reads all other modules and it derives its module name from the filenameThe problem with that is that you can only have one module with a certain name on your module path.          In Maven Central, 3.500 collisions are possible.      You can circumvent this with the Automatic-Module-Name in your manifest file.      For the demo application, Jaap used MongoDB.There is a split package issue in the legacy mongo client, not with the new one, but Spring Data Mongo relies heavily on the legacy mongo client instead of the new one.In order to get started with modules, the first steps are just to minimize the problems you might encounter.Step 1 + 2 + 3:  Upgrade your dependencies as this will minimize conflicts.  Use JDK11+.  Compile to  JDK11+.These first three steps are just to reduce the problems you might encounter.Step 4:Prepare the module structure within your code, so you can go from module to module.Don’t start with one big module from the start.Step 5:Add module descriptors bottom-up.Create a new module-info.java.This first module has no external dependencies whatsoever making it very easy to define.You only need to indicate what your are going to expose.module nl.jaapcoomans.boardgame.domain {    exports nl.jaapcoomans.boardgame.domain;    exports nl.jaapcoomans.boardgame.domain.command;}Note: you might need a newer version of the Maven Surefire Plugin; Jaap used version 3.0.0-M3.For a module which needs other modules, you will need to define a little bit more within the module-info.module nl.jaapcoomans.boardgame.bgg {    requires nl.jaapcoomans.boardgame.domain;    //requires com.sun.xml.bind;    requires java.xml.bind;    requires feign.core;    requires feign.jaxb;       exports nl.jaapcoomans.boardgame.bgg.factory;    opens nl.jaapcoomans.boardgame.bgg.xmlapi;}  requires: Defines the modules that you need.  exports: The packages that you expose.  opens: This means that this will make a module available for reflection (i.e. at runtime), that you might need for JAXB in this case.Spring is not yet modular, but they did define automatic module names in all of their JARs.Step 6:Add a module descriptor to the main JAR.Only then you get all the benefits of the module system.At this moment you will also encounter all the hurdles as this will also get you into runtime errors.If you do not execute this step, your main application will still be using the classpath and not the module path.Export the main class and the application module.module nl.jaapcoomans.boardgame.application {    requires nl.jaapcoomans.boardgame.bgg;    ...    requires spring.context;        exports nl.jaapcoomans.boardgame;    exports nl.jaapcoomans.boardgame.application;}You can also define requires transitive. module nl.jaapcoomans.boardgame.persistence{    requires transitive nl.jaapcoommans.boardgame.domain; }This last part means that when you depend on that module, you will also implicitly depend on that transitive module.Runtime Errors.When you encounter runtime errors, you can pretty much copy paste the errors you get about opening the modules.Encountering ClassNotFoundExceptions hints at missing modules for which you should define a requires definition.When you stop getting errors, this means that you have reached the next phase..Spring does use some of the internals of the JDK, which can be fixed by:requires jdk.unsupported.This does help you out for now, but the module name alone screams that you should not use it.Lessons Learned.As a summary here are the lessons learned by Jaap:  Move bottom up.  Test all paths on every step, because you will encounter runtime errors.  The logs have the answer while the JVM gives you a good indication of errors by the module system.  It still involves pioneering.You can rewatch his talk here:    Stream Processing with the Spring Framework by Josh Long and Viktor GamovAll the source code of the live demo can be found on GitHub.Statement: It is dangerous to think of Kafka as a message queue as it tends to become a vine of data within your organization to move data around thus becoming a database.In the demo they made use of Apache Avro.The Avro format will be used as a contract for the messages, it also gives you the option to generate Java classes based on the schema.You can use an Avro Maven plugin for that.Kafka does not care what you put in there.But passing along a schema gives your consumers the option to verify that they can process the message or not.Spring Kafka gives you KafkaTemplates that you can use.The KafkaTemplate wraps a producer and provides you with some extra handy methods to send data to Kafka topics.For more information you can check out the reference guide to use KafkaTemplates.It is important that you think about the type of the key and the type of the value, serializer and deserializer.For this you will need to define a DefaultKafkaProducerFactory which will provide you with some default config options like:  Bootstrap servers: where to find your Kafka.  Schema registry URL: where to find your Avro schema registry.  Key serializer: the class to be used to serialize your key when writing the message to Kafka.  Value serializer: the class to be used to serialize your value.Without those serializers, Kafka will not be able to transform your message.Various other frameworks worth mentioning:  Spring Cloud Stream: allows you to abstract the use of message brokers.It will manage a lot of the bindings for you with Kafka Streams mapping a lot of the configuration automatically for you.  Kafka Streams: a stream processing pipeline that you can use to build processing pipelines.Similar to Spark but less of a hassle to set up.Ktable is the representation of state.Some final notes about Kafka Streams:      Kafka streams allows you to visualize your topology in a Directed Acyclic Graph using TopologyDescroption.For more info see this link.        Kafka Streams allows you to do stateful stream processing in an easy way. Its state store is replicated within Kafka so it can restore it in case of failure.        Do not forget your SerDes when writing out Kafka Streams code.Spring Cloud Stream automatically converts to JSON but your Kafka streams code deals with binary data, so it needs to know how to serialize / deserialize. Some of them are predefined by Kafka: StringSerde, LongSerde and JsonSerdes.  It was a very entertaining live coding session which you can rewatch here:    How Fast is Spring by Dave SyerIn this talk Dave is going over the performance improvements carried out by the Spring team.Cold startup time of the JVM takes up some time but once started it is an awesome place.We went through some measurements.An application started up in 1.300 milliseconds.This went down to 1.200 milliseconds by tuning Spring a bit.By then using Spring functional bean definitions it went down to 600 milliseconds.The overhead of Spring Boot versus no Spring Boot is currently around 15 milliseconds.Thus, a lot of the overhead has already been dealt with.If the classloader has been warmed up, the startup time difference is much smaller.With Spring Devtools you have a warm classloader, which reduces your startup time.Lots of optimizations have happened e.g. heap memory went down from 10MB to 6MB with the move to Spring Boot 2.Async profiler is a tool you can attach to a running Java process.It has little to no impact on the running performance and shows the calls being executed.The width of the flame is the time it took to run.Red and yellow colour means: not in Java user memory and ready for garbage collection.Spring Boot 2.2 has boosted performance.  Classpath exclusion from Spring Boot web starters.  spring-context-indexer: this is marginal but with a lot of beans it will have a bigger impact.  Spring Actuators used to be costly for startup time, but no longer a big impact since the optimizations in Spring Boot 2.0.  Use explicit spring.config.location.  Switch of JMX spring.jmx.enabled = false (in 2.2 this is the default setting).  Make bean definitions lazy by default.In production you might not want this because if a bean is lazy loaded, the application might not fail on startup.It can make sense to do this during development in order to improve development time.  Unpack the fat JAR and run it by specifying the explicit classpath as java --jar is little bit slower compared to using java --cp.  Run the JVM with -noverify and consider -XX:TieredStopAtLevel=1.          All JVM experts will tell you not to do this.      -noverify will gain you 40% time with any app but it does not validate byte code which is less interesting in production as the JVM will just crash and show you no exception whatsoever.      -XX:TieredStopAtLevel=1 this deals with the JIT compiler, will gain you around 10% with any app.        Import auto configuration manually as it is not needed and might give you a small speed gain.A nice list of tools you can use:  Benchmarks: JMH  JUnit and JMH: microbenchmark-runner  Profiling: async-profiler  GC pressure: JMC aka flight controller  Quick metrics for any Spring Boot JAR: Benchmark launcher from dsyer/spring-boot-startup.  Classpath manipulation: Thin Launcher  Profiling with AspectJOther Remarks:  The Hibernate team is pretty aware of the GC issues and have done serious optimizations around it.  Lazy beans: Pay attention to custom beans with an expensive @PostConstruct.It tends to be misused for opening files, accessing database, which tends to block up the startup.  You can try using @ImportAutoConfiguration but then you need to know which AutoConfigurations you need to include.Discovering that is the hard part.  Functional Bean Definitions: If you use @Configuration then you make use of reflection.You can implement an ApplicationContextInitializer which makes you reflection free, but it is a bit harder to implement.  CPU constrained environments benefit from native images built with GraalVM.You can rewatch his talk here:    Kubernetes and/or Cloud Foundry - How to run your Spring Boot Microservices on state-of-the-art cloud platforms by Matthias HaeusslerMatthias Haeussler is a Cloud Advocate at NovaTec Consulting. He gave a presentation about the differences between Kubernetes and Cloud Foundry.He showed us this live with Spring Boot application which was deployed on both Kubernetes and Cloud Foundry.Cloud FoundryTo deploy your application on Cloud Foundry, you simply have to run one command: cf push (under the assumption that you have the CLI installed and configured).This will send your whole codebase to Cloud Foundry, which then builds a container for your application and runs it. Cloud Foundry does not use Docker images, only containers.The thing with Cloud Foundry is that it uses containers behind-the-scenes, but as a CF user, you don’t really notice it.KubernetesWith Kubernetes, it’s a whole different story. You can’t just ‘run’ your application on Kubernetes. You will need a Docker image to run your application, which meansyour application must have a Dockerfile inside it. This Docker image must be pushed to a Docker registry, which is then pulled from the registry by Kubernetes and ran with the specified configuration.ConclusionIn Kubernetes, you can configure way more which is a huge benefit.On the other hand you also need to know more about the platform to do so.Whereas with Cloud Foundry it is just one command and your codebase is pushed, wrapped into a container and ran on the platform.Way more simple but with less configuration options.Thus you can configure less than when using Kubernetes.Kubernetes also requires more dependencies if you want to get more out of it (ex. Helm, Prometheus, Istio, …).This requires additional maintenance of those dependencies.The ideal platform is: the simplicity of Cloud Foundry with the functional features of Kubernetes.You can rewatch his talk here:    Migrating a modern spring web application to serverless by Jeroen Sterken and Wim CreuwelsIs serverless the holy grail? These guys explored the possibilities while migrating an existing monolith to serverless at one of their clients.Serverless will help your developers focus on the code instead of server management and database setup.Wim and Jeroen also mention the flip side of the coin.It’s a new technology and as is the case with every new technology, there is a learning curve.Developers have to get used to the services that the cloud provider supports.They need to “think serverless” and model applications as functions in well-defined steps.Infrastructure has to be modelled using Infrastructure as Code.A topic on which you can find a great resource on our blog here.No, serverless is not the holy grail. It is however a great solution for some typical use cases:  Event-driven architectures  Internet of things  Applications with varying load  Data analysis  …Step functionsJeroen and Wim glued their app together using AWS Step Functions.Step Functions is a serverless orchestration service that lets you model your workflow as a series of steps.Step Functions will keep your Lambda functions free of logic that triggers other Lamba functions.Instead it will use the output of one Lambda function to trigger the next one, thus progressing towards the next step.These steps are made visible by a clear step diagram that shows your workflow.This diagram allows you to monitor your flow by changing color when something goes wrong.In case of an error Step Functions will automatically retry.Spring Cloud FunctionsWe are at SpringIO and we’re talking about Serverless Cloud technology so Spring Cloud Functions cannot be left unmentioned.Spring Cloud Function is a project by the Spring team that allows you to write cloud platform independent code.In the process you can keep using familiar Spring constructs like beans, autowiring and dependency injection.You can find great guides on baeldung.com and spring.io.Using Spring Cloud Functions will lower the stepping stone towards Serverless because most Java developers are already familiar with the Spring Framework.Serverless was already a hot topic.The fact that Spring now has also jumped on the wagon only makes it hotter.Definitely keep your eyes open for Serverless in the near future.You can rewatch their talk here:    Day 2: Talks &amp; WorkshopsTesting Spring Boot Applications by Andy WilkinsonAndy Wilkinson of Pivotal explained us the importance and essence of writing tests in your application to ensure the quality of your services.Of course, having a zero risk functionality is practically impossible but testing helps you to reduce your risk to a minimum.But how do you know if a test is ‘good’? Almost everyone is basing this on the amount of code coverage in their project.This does not determine the quality of your tests.When you write tests, you want to think about mistakes that you make or that can be made by the end user.Unit testingWhen you rewrite your application logic, there’s a high chance that you have to rewrite your unit tests as well. So make sure that you do not have to spend a lot of time on rewriting your tests when you want to refactor your application or write extra features.It’s also very important to use descriptive names for your tests. Make sure that your tests are readable by the human eye.No one wants to read a test that is not clear or creates more confusion (JUnit 5 comes with a display annotation to make a test name more readable).When you are familiar with writing unit tests, you’ve probably also heard of mocking. Unit testing is all round mocking external services and dependencies. After all, in a unit tests we are under the assumption that all our external dependencies are working as they should.Integration TestingAndy gave us a detailed explanation of how the various testing annotations work such as @SpringBootTest, which gives us a more Spring Boot way of testing our application (which means less configuration, hooray!). @SpyBean and @MockBean to create a mock or spy object of a Spring Component, @ActiveProfiles to run your test class with a specific profile, etc.Testing Against DatabasesOne of the more appearing problems in integration testing is working with a database. Typically, when you want to test against data in a database you are going to want to use an in-memory database.Often this is a HSQLDB or H2 database.This is where it gets interesting. You can tell your H2 instance to run in a specific database software mode, such as PostgreSQL.However, it’s not exactly the same as working with a real PostgreSQL server.H2 only interprets the queries that are ran in a PostgreSQL dialect and tries to convert to its own syntax. This can cause lots of problems, because you are not working with a real Postgres server.Even though H2 is ran with PostgreSQL compatibility mode, it can still fail with queries that will run perfectly on a real PostgreSQL server.Andy recommended us to use TestContainers.These have the power to spin up a Docker image of a database of your choice.So you’ll have the full functionality of a database server.What’s next?We are really excited to see what the new Spring Boot versions will have in store to help us write better and clearer tests.Spring Boot 2.2 will come with full JUnit 5 functionality and thus, will leave JUnit 4 behind.You can rewatch his talk here:    How to live in a post-Spring-Cloud-Netflix world by Olga Maciaszek and Marcin GrzejszczakDiscovering the new Spring Cloud stack.That’s what this talk was all about.Olga Maciaszek and Marcin Grzejszczak showed us the new solutions for Gateway proxying, circuit breaking and the whole new Spring Cloud stack.The world is changingSpring Cloud Load BalancerClient side load balancing via the @LoadBalancerClient annotation.Use the @LoadBalanced annotation as a marker annotation to indicate that a RibbonLoadBalancingClient should be used to interact with a service.Spring Cloud GatewayVia routes your requests are processed to downstream services.Spring Cloud Gateway is used as a simple way to achieve this routing to your APIs.You can keep configuring this as code:return builder.routes()        .route(\"users_service_route\",                route -&gt; route.path(\"/user-service/**\")                        .and()                        .method(HttpMethod.POST)                        .filters(filter -&gt; filter.stripPrefix(1))                        .uri(\"lb://user-service\")).build();or in your properties file:spring:  application:    name: proxy  cloud:    gateway:      routes:      - id: fraud        uri: lb://fraud-verifier        predicates:        - Path=/fraud-verifier/**        filters:        - StripPrefix=1        - name: Retry          args:            retries: 3Circuit Breaking and Resilience4JYou need a design that is resilient and fault tolerant.After a number of failed attempts, we can consider a service unavailable.We will then back off and stop flooding it with requests.We can save system resources for calls which are likely to fail.And give the other service some time to get back on their feet.Micrometer and PrometheusPeriodically scraping metrics from your services to monitor health.Spring Cloud Config ServerExternalize your configuration.You don’t have to restart your application to reload your configuration.Just fetch it from the remote service again.ResourcesCheck out a fully working Spring Cloud microservices demo here:https://github.com/OlgaMaciaszek/spring-cloud-netflix-demo.A lot of gratitude to Olga and Marcin for providing a working example that we can play around with to get acquainted with the new services.You can rewatch their talk here:    Event-Driven Microservices with Axon and Spring Boot: excitingly boring by Allard BuijzeIn this presentation, Allard Buijze, Founder and CTO of AxonIQ talks about the advantages of event-driven architectures and shows how easy it is to set up your own event-driven microservices using Axon and Spring Boot.What is Axon?The Axon framework is used for building event-driven microservices using Domain-Driven Design, CQRS and Event Sourcing.It is there to prevent developers from getting lost inside a complex microservice architecture.CQRSCommand and Query Responsibility Segregation is a design pattern where you split the reading and writing of data into seperate models.You use queries for reading the data and commands for updating the data.While for basic CRUD operations having these models combined might be fine but once the amount of business logic and the amount of queries increases it might become increasingly difficult to manage.State Storage vs Event SourcingA big part of this presentation is about the advantages of using event sourcing rather than state storage.Events describe the history of an object rather than just the current state of the object.It is easy to go through the history and generate the current state while also getting a lot of extra information about the object you would otherwise miss out on when just storing its current state. Explicit record that something happened, rather than an implicit record of what happened based on changes that occured.This also makes testing your application easier because you do not have to rely on state but rather on a series of events to take place or an exception to occur.EventsOne of the biggest advantages of events is that they remain valuable over time.They need to be the source of everything in the application and show a true representation of your entities.Once again, you don’t save the state of an aggregate, you can generate the state by replaying the history.The power of not nowThe power of not now basically means that because you save all the events,you can generate reports whenever you want based on the captured data.You don’t have to know in advance what data is important to store for later on, everything is stored.Axon ServerAxon Server is a service that distributes your components, manages routing, stores events and provides high availability and observability.By combining all these otherwise different services into one single easy to configure service,you make your entire architecture a lot easier to manage than if you were to use for example the Netflix Eureka Discovery Service for communication between microservices,the MySQL Event Store for storing events and RabbitMQ to handle messaging.By simply adding the Axon Server dependency and adding some annotations you can use all of these services while keeping the complexity low.TracingAxon can also manage tracing for you by just adding the Axon tracing and Jaeger to your dependencies.Where otherwise setting up tracing would be a lot of work having to deal with all kinds of headers,passing headers along and interpreting them,Axon tracing takes care of all of this for you.You can rewatch his talk here:    How to secure your Spring apps with Keycloak by Thomas DarimontIn this presentation Thomas Darimont talks about what Keycloak is,what you can do with it and gives a demo of how it works and how you can set it up for your own applications.What is Keycloak?Keycloak is a Java based authentication and authorization server. It is developed by Red Hat who use it as a base for their enterprise RH-SSO application on which they provide additional support and documentation.It is also backed by a large open source community providing additional features, documentation and bugfixes.Keycloak FeaturesOne of the most important features of Keycloak is the Single Sign-On and with this the Single logout.Sign into keycloak once to gain access to multiple applications and sign out once to sign out of all applications.Do note though that individual applications can disable this single logout so you might not get logged out of all the applications within a realm.Another great feature of Keycloak is their multi-factor authentication using one of the known authentication apps like the Google Authenticator.Keycloak also supports authentication through social media platforms such as Facebook, Twitter, Google or even Github. Then of course there is the fact that Keycloak is completely customizable and extensible.It comes with a preferred stack on which we will dive into more detail later on.You can get away from this and use your own preferred services albeit with some additional configuration.Keycloak also comes with an easy to use management console for administrators and a user management interface where all users can update their user details.The last feature to discuss are the realms. Sets of applications, users and registered OAuth clients to whom the Keycloak settings will be applied. With these realms you can give users specific roles or just authenticate them across multiple applications using the Single Sign-on feature.The keycloak preferred stackBy default Keycloak is a WildFly based server with a plain JAX-RS application.It uses JPA for storing data and Infinispan for the horizontal scalingof multiple Keycloak nodes that all distribute information like user sessions between eachother.Other than that it uses the Freemarker template engine to render for example the login pagesand Jackson 2.x for everything JSON related like the tokens.Securing your applicationTo add Keycloak to your applications you have to add a dependency and you will have to register your application within a Keycloak realm.After doing some configuration within your application and the Keycloak management console,you will have to authenticate through Keycloak to regain access to your application.The authentication processThe following steps describe the Keycloak authentication process:  Unauthenticated user accesses application  The application redirects to Keycloak for login  When the login is successful, Keycloak will create an SSO session and will emit cookies  Keycloak generates a random code and redirects the user back to application  The application receives the code associated with the sign-on session and sends the code back to Keycloak via a separate channel  If the code sent back is associated with the sign-on session, Keycloak will reply with an access token, a refresh token and an id token  The application verifies the tokens and associate them with a session  The user is now logged in to the applicationFor more information you can refer to Keycloak’s official documentation and you can also watch the original talk itself in the following video:    Zero Downtime Migrations with Spring Boot by Alex SotoIn this talk, Alex Soto, Software Engineer at Red Hat covers the subject of zero downtime migrations of microservices in Spring Boot.He covers some of the different deployment techniques.Some easy to understand ones and some more advanced techniques for when you are dealing with the persistent states of your applications.Dealing with downtime when using microservicesWhile it is easier and faster to take down a single service rather than a single monolith application,deployment or redeployment of services will happen a lot more often when using a microservices architecture. Another thing to consider is that when you have downtime on a service,all the services that have a dependency on the offline service will no longer work either.For this reason it is important to minimise downtime of your applications or even have no downtime at all. This is why you need to deploy and release services at different times.Here are some techniques on how to do this.Blue/Green deploymentBlue/Green deployment is where you will deploy an updated version of the service you want to replaceand release it by changing the routing from the old one to the new one.It is important to keep the old service deployed and monitor the new one so that in case of errorsyou can easily revert the routing back to keep everything up and running.The downside of blue/green deployment is that if something goes wrong,all users are affected if changes happen before reverting the routing.But of course there is a solution to this problem, Canary releases.Canary releasesCanary releases is where you route a small percentage of your traffic through an updated service while the rest keeps using the original service.This limits the amount of users that might be affected by unwanted changes while you monitor your new application.As everything goes well you increase the percentage of users until eventually your entire userbase uses the new service.All while still having the advantage of Blue/Green deployment to fall back on when things go wrong.Mirroring trafficMirroring traffic is another deployment technique where you deploy an updated version of a service next to the original one and send your requests to both services.Only the original service handles requests while the requests to the updated service are just fire and forget requests while you monitor if everything goes according to plan.Before eventually changing the routing to the updated version of the service.But what about sticky sessionsWhen dealing with sticky sessions, which you often see with for example shopping carts on webshops,your session is linked to a specific service by IP.This means that when you get rerouted to an updated service, you will lose your session.To counter this problem you can use an in-memory datagrid using for example Redisand duplicate this across all the services that use these sticky sessions in your cluster.When you do this, your shopping cart will stay, even when the service you are accessing changes.Dealing with persistent dataWhile problems with in-memory data are often fixed quite easily,when dealing with persistent data, zero downtime deployment becomes a little bit more tricky.Take for example the change of a column name in a database.If you were to use different services accessing the same database but using different column names, this would cause issues. This exact problem and how to tackle it is shown in the demo.Together with a more in-depth explanation of the covered topics in a video of the talk below.    The endSpring is trendy as ever.Solid fundamentals and ready for the future of software development.It was great to further extend our Spring expertise.And let’s not forget the amazing time we had amongst colleagues.We’ll be back next year for more!Will you be there too?!"
      },
    
      "conference-2019-07-01-devoxx-fr-html": {
        "title": "Devoxx FR 2019",
        "url": "/conference/2019/07/01/Devoxx-FR.html",
        "image": "/img/devoxx-fr-2019/devoxx-fr.png",
        "date": "01 Jul 2019",
        "category": "post, blog post, blog",
        "content": "  Devoxx France is a yearly conference in Paris, France.Three colleagues of Ordina JWorks: Yannick De Turck, Tim Ysewyn and Tom Van den Bulck attended the conference in April 2019, where all of them were welcomed as speaker.In this blog post we share our impressions and experiences.Table of contents  The Speakers Dinner  Applying (D)DDD and CQ(R)S to Cloud Architectures with Java, Spring Boot, Kafka and Docker by Benjamin Nothdurft, Michael Follmann and Dominik Guhr  Modern Java: Change is the Only Constant by Mark Reinhold  Meet Micronaut: a reactive microservices framework for the JVM by Álvaro Sánchez-Mariscal  Full-Text Search Tips &amp; Tricks by Denis Rosa  MockK, The Idiomatic Mocking Framework For Kotlin by Yannick De Turck  Real Quantum Computing by James Birnie  Agile is a Dirty Word by James Birnie  ConclusionThe Speakers dinnerThe evening of the workshop day, we had the speakers dinner where we were provided with lots yummy snacks to feast upon while enjoying a nice glass wine.It was a great opportunity to both network with the other speakers and connect with other experienced developers, gaining extra valuable insights.Being a speaker and meeting other speakers also gives you a nice icebreaker: “What talk are you giving?”.Very handy to get the conversation going.That is the power of a conference.To speak with other attendees and speakers, gaining valuable knowledge.Most of that knowledge is not presented in the talks but will only bubble up if you speak face to face.Applying (D)DDD and CQ(R)S to Cloud Architectures with Java, Spring Boot, Kafka and Docker by Benjamin NothdurftBenjamin started off by giving a brief introduction on Domain-Driven Design explaining the different building blocks such as domains, domain events, ubiquitous language and Event Storming.He also mentioned the famous two books: Domain-Driven Design by Eric Evans and Implementing Domain-Driven Design by Vaugn Vernon.Benjamin went through all the different steps of Event Storming.The goal is to bring people of different silos together, such as developers, analysts, architects and business experts.Together you want to create a logically ordered sequence of events to document a system using an ubiquitous language i.e. everybody using the same vocabulary and terms.Events describe things that have happened and are thus always in the past tense e.g. product added to cart.In a next step you want to identify commands, which are the triggers of events e.g. add product to cart.There are also aggregates which represent the data that is interacted with.And finally you want to identify the bounded contexts grouping relevant parts together.Benjamin then explained how this all gets translated to your system architecture.Each bounded context can be mapped to a single microservice.He covered different context map patterns such as event publisher, shared kernel, customer/supplier and anti-corruption layer together with detailed code samples.Afterwards he went through a CQRS example with many code samples and the questions you should be asking yourself when determining the right architecture.We really liked how in-depth everything was as many presentations about Domain-Driven Design usually remain rather abstract and high-level.You can check out Benjamin’s slides on slides.com.    Modern Java: Change is the Only Constant by Mark ReinholdMark described how they took the massive monolithic platform, the JDK, and transformed it in three ways:  First, they sliced it up into 26 modules  Second, they removed the Java EE and CORBA modules, together with some other APIs resulting in 19 modules  And third, the grand majestic release model where a new version was shipped every 2, 3 or 5 years was left behind in favour of one where there is a release every 6 monthsJust packaging the modules you need, will make the footprint of your application smaller.The stronger encapsulation also makes Java more secure.A couple of the Zero Day exploits could have been avoided with stronger encapsulation.Three out of six since JDK7.Modules allow you to restrict access to internal packages, making it no longer compile and possibly generating warnings at runtime.Run the following command line: $ jdeps --jdk-internals app.jar to check if internal APIs are used.In theory, if your code only uses standard Java SE APIs then it will most likely work without changes.For more info, see: JEP 291.Lots of methods and APIs have been (and will be) removed but there are four reasons not to worry about it:  There is an advanced notice before removing anything (@Deprecated)  Make the choice yourself between LTS or feature releases  Java is still free  Some things might break but it shouldn’t take much work to fix themAt first there was the fear that with the six month release cadence there would be some releases which would only contains bugfixes.But it has shown that all releases provided new features, see the JEPs for more info.  JDK 9: 90 JEPs, 6 JEPs came from outside Oracle  JDK 10: 12 JEPs, 2 JEPs from other contributors (304, 316)  JDK 11: 17 JEPs, 3 JEPs from others (315, 318, 331), with 2458 issues resolved (1963 solved by Oracle, 169 by SAP and others), this is the latest LTS edition (next one will be JDK 17)  JDK 12: 8 JEPs, 1 JEPs from another contributor (189)Every one of these releases is PRD ready, with the only difference being support time.  Java is still free.  Finally, please let me assure you of one thing: whether by Oracle or Red Hat or someone else, JDK LTS releases will continue to be supportedWe all have a lot invested in Java, and we won’t let it fall - Andrew Haley.Source: http://mail.openjdk.java.net/pipermail/jdk-dev/2018-August/001826.html.Oracle JDK vs OpenJDK:Choose between LTS version and big upgrade migrations or latest release with small migrations every 6 months.Feature release: Make sure your tools and dependencies can keep up!Library maintainers were asked on Twitter if their libraries worked fine on JDK9 and the same question was asked when JDK 11 got released.The result showed that a lot of libraries were already upgraded and functional for JDK 11.If you’re using Java 9 or later, you should upgrade to the latest version of your tools and other dependencies as some libraries might have one thing or another broken.  Everything changes and nothing stands still - Heraclitus.The most important goals when working on a new version are:  Developer productivity, not for one-off scripts but when building and maintaining large reliable programs  Program performance including and not limited to: startup time, memory consumption, …He also touched upon some of the major active projects of the Open JDK community.Amber which is about right-sizing languages ceremony, reducing boilerplate.  Better and improved type inference with a smarter compiler  Less keyboard typing for you  Fix error prone things such as switch case on enums with pattern matching where the compiler detects whether a default is actually needed or not  Multi-line strings (Java 13 JEP although still to be decided)  Value classes for data containers: record Point(double x, double y)Loom brings fibers to the Java platform.  Similar to threads but slightly different          Fibers are simpler than threads      Fibers work well with CompletableFutures        Marc gave a small demo on the performanceValhalla: Value types &amp; specialized generics  Unfortunately, Mark ran a bit out of time so he wasn’t able to cover this topic    Meet Micronaut: a reactive microservices framework for the JVM by Álvaro Sánchez-MariscalThe reality of existing frameworks:Frameworks based on reflection and annotations tend to become fat.But we love the programming model and productivity model so we live with it.So, why should we be more efficient?Spring is an amazing technical achievement but many things are done at runtime:  Reading the byte code of every bean it finds  Synthesizing new annotations on each bean method, constructor,… to support annotation metadata.This leads to high startup time and memory consumption.Jakarta EE also does this.Micronaut is a new framework created by the creators of Grails for which the goal is to have the productivity of either Spring Boot or Grails with the performance of a compile-time, non-blocking framework.It is designed from the ground up with microservices and cloud in mind.Ultra lightweight and reactive, based on Metty instead of Tomcat, its internals are reactive.Provided with:  Integrated AOP and compile-time DI for Java, Groovy and Kotlin  Annotation processors for Java and Jotlin  AST transformations for GroovyMicronaut is fully open source and you can find the source code on GitHub.It was actually announced one year ago, in May 2018 after two years of development behind the scenes.It can be used to build:  Microservices  Serverless applications  Message-driven applications with Kafka/RabbitMQ  CLI applications  Android applications  Simple static void main(String.. args) applicationsMicronaut can be even faster with GraalVM, but this still is experimental.Next, Alvaro showed use a coding demo:A Micronaut project with Consul for service discovery.To give you an example about the performance, the demo greeting application with automated tests took 52ms to start with Consul registration.A second application was created for the gateway.Micronaut has reactive types such as Single, which is non-blocking and similar to a Future.Alvaro wrote a non-blocking API in the gateway which merges two calls to the greeting application.He ended with a demo of an application on Google Cloud Run (serverless based on containers).Google Cloud run was considered to be much easier for you as a developer than AWS Lamba according to Alvaro.The material of the 3 hour long workshop can be found on GitHub.    Full-Text Search Tips &amp; Tricks by Denis RosaDenis Rosa is a developer advocate at Couchbase.The scenario where you have multiple nodes and many clients who want to do full-text search is still pretty hard to do with the JVM. Which is one of the reasons why it was decided to start with Bleve: a framework for full-text search and indexing written in Go.It is important to realize that searching is not only about matching text as it can also be more sales-oriented.For example:  Netflix wants to convince you to watch a movie.  Google want to answer your questions.When building out a search functionality it is important to know how your users are searching.Do they use one word, two words, three words, …User queries should be saved, don’t forget to do so.So that you can learn more about your users and improve your search model.Some quick core concepts:  TD/IDF: aka “Term Frequency–Inverse Document Frequency”, having rare words score higher  Edit distance: minimum number of operations required to transform one string into another: for example, star -&gt; start  Pivoted length norm: matches in small texts will rank higher than in long onesDenis showed an example with Bleve underneath.He made an application called Couchflix allowing you to specify text to search on movies.He showed how “good” his initial search was, when searching on star war vs star wars the first one just showed some zombie movies but nothing about star wars.During the live coding session several search implementations were made to see the difference: match on title alone, match on title and description, …Even after some modifications we still had the situation that when searching on star war, both star and war were matched separately.Something useful for that is a Shingle.A shingle will break your search term in a combination of two, three, four words, … For example: Star wars daisy Ridley gets split out in the following search terms:  Star Wars  Wars Daisy  Daisy RidleyInstead of shingles you can also use Stemming to avoid conjugations.For example, the search terms Fishing, fished and fisher would all be transformed to fish.Another thing is Boosting which allows you to boost up the ranking factors of your search results.Most people tend to first focus on these when searches do not return the results they want but you first need to make sure that everything has been indexed properly. Only after does it make sense that you start thinking about boosting.We also have Decay Functions which allow you to decay your research results.For example: leave out older movies that are less relevant.Things also get more complicated when we want to search on an actor name compared to the movie name for which we now have done all those optimizations. Searches are always biased, so the one we had for titles is less ideal for searching on actors and vice versa.Some other recommendations:  For tests, the Cranfield Evaluation Method is a good strategy to implement tests:for every change you do in your model you need to verify if your query still returns a good result.It is more complicated than the average JUnit test though.  Use Precision and Recall as a metric.  There is a very nice Coursera course available: Text Retrieval and Search Engines, which is really worth checking out.  Be careful with features that are directly influenced by users to avoid being tricked!The Source demo can be found on GitHub.    MockK, the idiomatic mocking framework for Kotlin by Yannick De TurckOur colleague, Yannick, gave a talk about MockK.MockK is a mocking framework specifically made for Kotlin. As a Java developer, he is a huge fan of Mockito for using mocks in his unit tests.When he picked up Kotlin, Mockito was also his first choice to try out.He explained however that using Mockito in a Kotlin project doesn’t go all that well due to how Mockito creates its mocks, which is by overriding classes and functions.And because Kotlin’s classes and functions are final by default, you can see that this poses some challenges.Yannick shared his experiences and mentioned that even though there is an incubating, opt-in feature to allow Mockito to mock final classes and functions, the user experience isn’t all that nice.He looked for other frameworks and stumbled upon MockK, a mocking framework created for Kotlin by Oleksiy Pylypenko.MockK’s main philosophy is offering first-class support for Kotlin features and being able to write idiomatic Kotlin code when using it.He was pretty enthusiastic about MockK and went over all its features and its ease of use.On our tech blog there is also a blog post written by Yannick specifically about his experiences with Mockito and MockK in Kotlin projects.    Real Quantum Computing by James BirnieUsing many worlds to solve the unsolvable.Why quantum computing?James didn’t knew anything about it until he followed a talk of Alasdair Collinson at Voxxed Days Vienna at which point he still found it hard to comprehend.So he decided to make a talk of his exploration of quantum computing in such a way that it would become easier to comprehend for the uninitiated.The last version of the Thoughtworks tech radar has Q# in Assess on the recommendation of James.To understand, first go back to the classical bit:A bit is a unit of binary information, which can hold two states: 0 or 1.This is stored as one of two voltages, logic gates compose this into arithmetic.In quantum computing we have the qubit.This also has a value of either 0 or 1.However, observing the value “collapses” the state to a single value.Any quantum thing can be a qubit, such as an electron, a photon or an ion.The quantum state is somewhat represented by the bloch sphere, a complex values probability amplitude.Probabilities of measuring the different values are derived from the quantum state.This maps to a point on the bloch sphere for a single qubit in a pure state.Quantum logic gates are similar to classical logic gates.IBM Q allows you to make a free account to play with quantum computers.Make circuits by drag and dropping gates and operations.Next, James demonstrated the Schrodingers cat thought experiment.When executed on a real quantum computer there were a lot of error states, showing that quantum computers are not yet production ready.In a simulation the result was balanced in the 00000 and 01111 state.There are problems that classical computing cannot solve but quantum computers can and this is what makes them interesting.Quantum computers cannot meaningfully persist state though.Hardware is not ready yet for quantum computers and they are hard to program.James did an experiment on his own cat and the different states of a cat, culminating in a quantum program on the behaviour of cats.The C# program CatMoodExperiment with multiple outcomes: x people feeding the cat.Four qubits representing four persons, that may feed the cat.Another test represents how the cat is feeling.The program is available on his GitHub for you to try out.What do we need to build a commercial quantum solution?  Algorithms and applications (will happen)  Integration with cloud provider (will happen)  Error correction (hard!)  Quantum classic interface (hard!)  Cryogenic systems (expensive!!)  Scalable qubit foundations (hard!)Two well-known quantum algorithms to look into are:  Deutsch-Josza algorithm, 1992  Grover’s algorithm, 1996The most interesting application for James is quantum chemistry because it should allow us to fully understand complex molecules like iron molybdenum, which exists within beans which fix atmospheric nitrogen. While we humans need 5% of the natural gas supply to make fertilizer with the Haber Bosch process, beans do that with no energy.So if we understand that iron molybdenum complexity, we can potentially do away with this massive energy inefficient process.Then you also have Shor’s algorithm to factorise n, which should make it possible to break RSA encryption.Although this won’t happen for now as quantum computers are still not stable enough.All in all a very interesting talk to give you a heads-up on the current state of affairs around quantum computing, but it also makes it quite clear that we still need to go a long way in order to have practical, production-ready, quantum computing.    Agile is a Dirty Word by James BirnieIn this talk, James explores in which organizations agile became a dirty word.Often it starts with companies who are doing real agile.You can compare the organizations with real democratic countries like the Democratic Republic of Congo and Democratic People’s Republic of Korea.Doing agile does not make you agile, you don’t do agile, you have to be it.Often it is like a cargo cult, just doing the rituals of scrum but loosing the goal and the context out of sight.Combine this with risk management theater, also known as lots of processes and checkboxes to tick off in order to avoid risk taking making it easy for agile to become a dirty word.SAFE is the perfect example of agile gone wrong, too much convention and control.Agile also tends to collide with the current issues with budgeting, which tend to emphasize on short-term goals preventing organizations from doing some real investments.A good paper to read is: How to detect agile bullshit from the US Department of Defence.In order to bring the organization back to track, avoid dirty words which have become contaminated by bad experiences.Read the agile manifesto and talk about the values and principles mentioned.Note, waste is not always obvious.Like code reviews which are often seen als helpful and agile, but too often are just waste.Only work on your biggest problem and only when that has been tackled move over to the next.The goal of agile should be to get autonomous cross functional teams who can deliver themselves and are decoupled as much as possible from the rest of the organization.Also, there does not exist failure within agile, it is better to call it experimenting.    ConclusionDevoxx FR was  an amazing experience for us where we got to see some of our friends again and meet and befriend others.Sharing your experiences should always be the main goal when you attend a conference.It is true that as a speaker, it does is easier to share your experiences and to get in contact with other people.But as an attendee you should never hesitate to go up to a speaker or another attendee and ask a question, to come in contact.Most people happily and willingly share their knowledge.Give it a try.Next time you are at a conference, ask that question that came up in your mind or join an interesting conversation instead of just listening and who knows what interesting discussions will unfold.The date for next year’s edition has already been announced and will happen on 15th until the 17th of April 2020.Be sure to mark it in your agenda so you can enjoy the great atmosphere!    "
      },
    
      "machine-20learning-2019-06-26-tensorflow-in-the-browser-html": {
        "title": "Getting started with TensorFlow in the browser",
        "url": "/machine%20learning/2019/06/26/Tensorflow-in-the-browser.html",
        "image": "/img/2019-06-26-tensorflowjs/banner.jpg",
        "date": "26 Jun 2019",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  TensorFlow  TensorFlow.js  ML5  ResourcesIntroductionMachine learning is a hot topic right now, and rightfully so!It is also something that is very hard to do, if you’re trying to do it right, that is.Since AI/ML is here to stay and not everyone has the time and resources to study every aspect of ML, we need something to help us.We already have TensorFlow which runs in Python, nice if you know Python.But what if you want to experiment with ML in your latest web application?The answer is here: TensorFlow.js!This blogpost serves as a quick introduction to TensorFlow.js and ML5 so you too can get started with some cool AI/ML examples in your web applications!TensorflowIf we are going to implement machine learning into an application, TensorFlow is one of the most used libraries that provides everything we need to get started.It has been around for quite some time and is really mature.It is fully open source and a well-adopted industry standard with great tooling and lots of reference materials.The full documentation on TensorFlow can be found here.Learning TensorFlow is a long process that requires dedication and a lot of trial and error.Experimentation is key!There are also a lot of very good online courses to get started with machine learning that can greatly aid you in understanding the key principles and concepts.        There are many more videos about getting started, and there are also some very good courses on online educational websites like Coursera and Pluralsight.They are a great place to start, but always remember that only with extended trial and error and experimentation you will fully grasp the logic behind it all!TensorFlow.jsEnter TensorFlow.js.This ‘new’ addition to the TensorFlow lineup allows developers to utilize the power of machine learning in the browser or in NodeJS.The biggest advantage of this is that models written in Python can be converted and reused.This allows for some very cool use cases we will go into a bit later.TensorFlow.js is nice because:  GPU accelerated (in the browser), thanks to WebGL/OpenCL, and this even on non CUDA cards!  Lower barrier of entry, no prior Python knowledge required (but can be handy)  Convert/retrain/reuse existing models with your own data  Quickly prototype into existing applications without having to setup a full ML pipeline  In the browser we have direct access to various input devices/sensors like the camera/accelerometer/location/…This sound almost too good to be true!But there are a few limitations though.Browsers are a lot more memory constrained than when training a model ‘offline’ with Python.This means that super complex models pose an issue when training in the browser, keep this in mind!You can however always train a model offline and only use TensorFlow.js to run the model in the browser and make predictions.Also keep in mind that the models you load, tend to be on the larger side, especially when considering web pages.Some models are upwards of a 100 MegaBytes or more, so loading them can take a while, certainly when bandwidth is limited (mobile devices/3G/bad WiFi).Taking the TensorFlow.js variant of TensorFlow into account, we can see that there are many options in the ecosystem to build, train and run models almost everywhere.A small JavaScript exampleThe following example is one how to recognize digits from the MNIST dataset (Modified National Institute of Standards and Technology).This is a large dataset of scanned handwritten digits.It contains 60,000 training images and 10,000 testing images.Each image is black and white spanning 28 by 28 pixels, for a total of 784 pixels.Numbers you’ll get to know by heart.All the API calls used are documented on the TensorFlow.js website.// tf.sequential provides an API where the output from one layer is used as the input to the next layer.const model = tf.sequential();// The first layer of the convolutional neural network plays a dual role:// it is both the input layer of the neural network and a layer that performs the first convolution operation on the input.model.add(tf.layers.conv2d({    inputShape: [28, 28, 1],    kernelSize: 3,    filters: 16,    activation: 'relu'}));// MaxPooling layer for downsampling =&gt; https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networksmodel.add(tf.layers.maxPooling2d({    poolSize: 2,    strides: 2}));// Our third layer is another convolution, this time with 32 filters.model.add(tf.layers.conv2d({    kernelSize: 3,    filters: 32,    activation: 'relu'}));// Max pooling again.model.add(tf.layers.maxPooling2d({    poolSize: 2,    strides: 2}));// Add another conv2d layer.model.add(tf.layers.conv2d({    kernelSize: 3,    filters: 32,    activation: 'relu'}));// Now we flatten the output from the 2D filters into a 1D vector to prepare// it for input into our last layer. This is common practice when feeding// higher dimensional data to a final classification output layer.model.add(tf.layers.flatten({})); model.add(tf.layers.dense({    units: 64,    activation: 'relu'}));// Our last layer is a dense layer which has 10 output units, one for each// We use the softmax function as the activation for the output layer as it// creates a probability distribution over our 10 classes so their output values sum to 1.model.add(tf.layers.dense({    units: 10,    activation: 'softmax'}));This concludes the code for creating the actual layers.Each layer has some comments explaining why it is used and what its function is.As is, this code is not that complex.The complexity stems from knowing what layers to use, what parameters to give them and how to combine them all.This can only be learned up to some degree as a deep understanding is required to figure this out.Thankfully the internet has many websites with resources for machine learning and there are even websites dedicated to providing ready-to-use models, like Model Zoo.Now we will look at code that sets the optimizer, sets up the training and validation, loads the data and then trains and validates the model.A working example of this code can be found here.// An optimizer is an iterative method for minimizing a loss function.// It tries to find the minimum of our loss function with respect to the model's weight parameters.const optimizer = 'rmsprop';// We compile our model by specifying an optimizer, a loss function, and a// list of metrics that we will use for model evaluation. Here we're using a// categorical crossentropy loss.model.compile({    optimizer,    loss: 'categoricalCrossentropy',    metrics: ['accuracy'],});// Batch size is another important hyperparameter. It defines the number of// examples we group together, or batch, between updates to the model's// weights during training.const batchSize = 320;// Leave out the last 15% of the training data for validation, to monitor// overfitting during training.const validationSplit = 0.15;let trainBatchCount = 0;const trainData = data.getTrainData();const testData = data.getTestData();const totalNumBatches = Math.ceil(trainData.xs.shape[0] * (1 - validationSplit) / batchSize) * trainEpochs;// During the long-running fit() call for model training, we include callbacks.  let valAcc;await model.fit(trainData.xs, trainData.labels, {    batchSize,    validationSplit,    epochs: trainEpochs,    callbacks: {        onBatchEnd: async (batch, logs) =&gt; {            trainBatchCount++;            if (onIteration &amp;&amp; batch % 10 === 0) {                onIteration('onBatchEnd', batch, logs);            }            await tf.nextFrame();        },        onEpochEnd: async (epoch, logs) =&gt; {            valAcc = logs.val_acc;            if (onIteration) {                onIteration('onEpochEnd', epoch, logs);            }            await tf.nextFrame();        }    }});const testResult = model.evaluate(testData.xs, testData.labels);const testAccPercent = testResult[1].dataSync()[0] * 100;const finalValAccPercent = valAcc * 100;const testExamples = 100;const examples = data.getTestData(testExamples);// Code wrapped in a tf.tidy() function callback will have their tensors freed// from GPU memory after execution without having to call dispose().tf.tidy(() =&gt; {    const output = model.predict(examples.xs);    // tf.argMax() returns the indices of the maximum values in the tensor along    // a specific axis. Categorical classification tasks like this one often    // represent classes as one-hot vectors. One-hot vectors are 1D vectors with    // one element for each output class. All values in the vector are 0    // except for one, which has a value of 1 (e.g. [0, 0, 0, 1, 0]).     const axis = 1;    const labels = Array.from(examples.labels.argMax(axis).dataSync());    const predictions = Array.from(output.argMax(axis).dataSync());});This is still not all that complicated code, but it is a lot to figure out all by yourself.There are pre-trained models available which can make your life easier, but what if there was an even easier way.What if there exists a library (or more than one) that allows you to do some commonly used machine learning techniques with very little code.It exists, read on below to find out all about it!ML5Enter ML5!This library provides ready to use building blocks, as their website describes:  ML5.js aims to make machine learning approachable for a broad audience of artists, creative coders, and students.The library provides access to machine learning algorithms and models in the browser, building on top of TensorFlow.js with no other external dependencies.This gives us the ability to quickly use these machine learning techniques in prototypes, to experiment with and test viability in real world conditions without requiring a full model to be built up front.This is provided the machine learning technique is available within the ML5 library.Classifying an image becomes really simple:// Initialize the Image Classifier method with MobileNet. A callback needs to be passed.const classifier = ml5.imageClassifier('MobileNet', () =&gt; {    console.log(‘ready’);});let img;function setup() {    noCanvas();    img = createImg('images/bird.jpg', () =&gt; {        classifier.predict(img, gotResult);    });    img.size(400, 400);}function gotResult(err, results) {    if (err) {        console.error(err);    }    select('#result').html(results[0].className);    select('#probability').html(nf(results[0].probability, 0, 2));}This small example also uses p5.js for image handling/drawing.It has many different available machine learning techniques available:  Image classification  Pose estimation  Person segmentation  Biomedical image segmentation  Style transfer  Image to image translation  Feature extraction  Text sentiment detection  …Detailed information and reference documentation can be found on their website.The project is also fully open source and in active development.Some ML5 examplesWe have written some small examples ourselves with ML5.You can run the examples by checking out the repo and switching to any of the solution branches.Be sure to run it from a local webserver or the demos will not work correctly!The first example uses YOLO (You Only Look Once)  and ImageNet to detect the contents of an image and classify it.The second example uses Style transfer to transfer styles from a base image to a provided target image.                                 These are really simple to make and provide a real world value for quickly prototyping an application to include a machine learning technique.While not everything is perfect (the cat is a Holy Birman not a Siamese, but I’ll excuse that one) it can give you a rough view on how well it will work and if it merits further development effort.Be sure to give it a try and see for yourself how easy it really is to get started with TensorFlow in the browser!Resources  TensorFlow.js  Model Zoo  ML5  ML5 Demos  YOLO  ML5 Custom Examples"
      },
    
      "java-2019-05-22-jdk-distributions-html": {
        "title": "JDK distributions",
        "url": "/java/2019/05/22/JDK-distributions.html",
        "image": "/img/2019-05-20-JDK-distributions/openjdk.png",
        "date": "22 May 2019",
        "category": "post, blog post, blog",
        "content": "  Around February we had a discussion in our chat group of developers which made us realise how much confusion there is on the new update cycle for the JVM.We decided it would be good idea to write a blog post that should clear up the confusion for our developers and clients.Table of content  Oracle’s JDK distributions  OpenJDK updates  OpenJDK providers  Java Desktop, Java Web Start and JavaFXOracle’s JDK distributionsFirst we have to explain what changed to Oracle’s JDK distributions recently, which has brought along a lot of uncertainty within the Java community.This blog post is going to try to explain these changes and more importantly, tell you what you should know about the licenses and options you have.At the end of this article you should know more about the free updates and commercial support options from  various JDK vendors and understand that OpenJDK 8 and 11 are still being updated, even though Oracle is going to stop leading those projects, to focus on the new versions and providing commercial support.Six month release cycleJava SE now has new major versions released every six months since Java version 9.Prior to this version, updates where provided by Oracle and other OpenJDK contributors.These releases, such as 8u91, 8u111 and 8u131, were released every six months.You would not get further updates on 8u91 once 8u111 was released and no longer get updates for 8u111 once 8u131 was released.Since Java version 9 however, there is a new six month release cycle for major versions.Similar to the old update cycle.In the new version system you will no longer get updates for Java 12 once Java 13 is released.Exceptions being long-term support versions which will get updates beyond six months.More about that later.Below is a table for free updates.Each vendor can release their own OpenJDK distributions and decide for themselves how long they provide free updates.If you want pick a vendor, make sure you research how long they will be providing free updates/support because this may vary from vendor to vendor.Further in the article we provide a table with most of this information.            Version      Release date      Free updates ended or superseded                  OpenJDK 6      December 2006      Supported primarily by Azul systems              OpenJDK 7      July 2011      Supported primarily by Red Hat until at least June 2020              OpenJDK 8      March 2014      Supported by Red Hat, Amazon, Azul Systems, BellSoft, Google, IBM, jClarity, SAP, and others.              OpenJDK 9      Sept 2017      Superseded by OpenJDK 10              OpenJDK 10      March 2018      Superseded by OpenJDK 11              OpenJDK 11      Sept 2018      Supported by Red hat, Amazon, Azul Systems, BellSoft, Google, IBM, jClarity, SAP, and others.              OpenJDK 12      March 2019      Superseded by OpenJDK 13              OpenJDK 13      Sept 2019      Will be superseded by OpenJDK 14      Support and licensesBefore Java 9, Oracle would provide updates for their JDK for three years.The license allowed you to use these updates personally and commercially.The updates had no support so if you wanted support you had to buy a license from Oracle and those licenses also came with longer update cycles.Starting April 2019, the new Oracle JDK 8 updates will be restricted and will remain free for personal desktop use, development, testing, prototyping, demonstrating and for use with certain types of applications.Commercial use of these updates requires you to get a license for a paid support plan.Using older versions is allowed, but will not contain important security patches.Java 9 brings new update cycles which allows for new implementations like TLS 1.3 to be added to Java faster.Oracle also started producing Oracle OpenJDK builds which use the GNU General Public License, version 2, with the Classpath Exception.Every major Java version has free updates for six months until the next major version is released.If you need new updates and support for these older versions, you will have to buy a license from Oracle which will extend support and updates although they only provide this for LTS releases.SupportSo what if I want support from Oracle?Support isn’t free because Oracle and other companies have to hire employees and developers to provide these support services.If you need the reassurance of bug fixes and somebody to answer your questions then you have to get commercial support.You aren’t limited to Oracle’s support.Which vendor you choose to get this support from is up to you.But each vendor has their own JDK binaries so make sure you use the binaries of the vendor you want to get commercial support from and don’t forget to check how long they will support your OpenJDK version.Each vendor can decide for themselves how long they provide support for their OpenJDK libraries, so this isn’t a decision you should make on a whim.You might also be thinking: “Why would I pay for commercial support for (open source) JDK versions?”.Don’t forget that Oracle and other vendors pour money in development of these JDK versions and this money has to come from somewhere.You are not obliged to get paid support with one of the vendors providing OpenJDK versions, but we think it’s worth thinking about supporting the Java ecosystem to ensure its long-lasting future.If there is no money to develop the JDK further then there won’t be any progression to the future of the JDK ecosystem.Long-Term Support (LTS)In the OpenJDK, LTS is an understanding between various contributors which are mainly led by Oracle and Red Hat which means that the code line for Java SE 11, 17, 23, etc will be maintained for a longer period of time than six months.Oracle leads the first six months of the OpenJDK LTS code line, providing updates and producing the Oracle OpenJDK builds, but afterwards as mentioned before, provide updates under a paid support plan.There’s a but however: Oracle will work with other OpenJDK vendors to hand over the OpenJDK LTS code line and allow those vendors to continue working on these updates together.Handing over the OpenJDK code line has already occurred for both Java 8 and Java 11 with Red Hat taking over those update projects.This doesn’t mean that they are the only one contributing to the OpenJDK project.Various people can still provide patches and add new features to new versions of the OpenJDK.You can read more about JDK 11 updates on the OpenJDK Wiki.You’ll see that there are updates planned until at least mid October 2019.This means that the Oracle JDK could differ from the OpenJDK based binaries and the JDKs provided by other vendors.Most of the major vendors have continued to take efforts to keep them in sync as much as possible, but this does mean you should develop, test, … your applications on the binaries you plan on using in production, or you might end up with some unexpected results during production.To prevent big differences, a TCK is  provided by Oracle for OpenJDK distributions.More about that later in the article.OpenJDK updatesThe OpenJDK community works on free, open-source implementations of the Java SE standard.Oracle contributes a lot to these projects and forms the basis for both the Oracle JDK and OpenJDK builds.OpenJDK 11+ versions are interchangeable with Oracle’s JDK for applications.Oracle will continue to contribute to OpenJDK while they provide updates for the corresponding Oracle OpenJDK build version.Once that version is superseded, Oracle will cease contributing to that version and start updating the next one.Updates from other vendorsOracle is very receptive of the idea on having community maintenance and will continue to support handover of the OpenJDK to the community to a qualified volunteering entity once they have moved on to work on the next version.Red Hat is currently globally leading and updating OpenJDK 6 and OpenJDK 7 projects after Oracle ended updates for them.After Red Hat stopped updating OpenJDK 6, Azul Systems took over leading the project and continued to provide updates for the project to this day of writing.Red Hat is currently leading OpenJDK 8 and OpenJDK 11 since April 2019.This does not mean that they are the sole contributors to the project.Other vendors are providing patches and updates as well. The biggest contributions are happening to the OpenJDK 8 project with contributions from not just Red Hat, but Amazon, Azul Systems, BellSoft, IBM, jClarity, Google, Sap and many other vendors.For consistency, these vendors provide extended update cycles for their OpenJDK for the same versions that are deemed LTS for Oracle’s JDK.OpenJDK providersBuild yourself from source providersOne of the options you have is to build a JDK from source code meaning OpenJDK, no commercial support and you need to build it yourself and keep it updated.This is not something we would suggest doing since this requires you to put resources in, checking for updates and applying patches if needed.There is also no way of getting any commercial support if you ever need it.Source providers  Mercurial  Tarballs (Java 7+)  AdoptOpenJDKUsing binaries from providersThe most convenient option is to use binary distributions from other providers that are providing public updates.Free binary distributions &amp; commercial support            Distribution      Versions      TCK      Public updates      Arch(*)      Commercial Support      Commercial Support ended                  AdoptOpenJDK      8, 11      No      Until at least Sep 2023      Major + Minor      IBM, jClarity      Indefinitely (IBM), Until at least 2025 (jClarity)              Amazon Corretto      8, 11      Yes      Until at least June 2023      Major      -      ?              Azul Zulu      8, 11      Yes             Major + Minor      Azul      March 2025 (8), September 2026 (11)              Bellsoft Liberica      8, 11      Yes      Until at least 2023      Major + Minor      BellSoft      Until 2026              Oracle OpenJDK      11      Yes      Until Mar 2019      Major      Oracle (through the Oracle JDK)      September 2023, September 2026 (extended support)              SapMachine      11      Yes      September 2022      Major      SAP      September 2022      * Major = Linux x86, MacOS, Windows x64,Minor = various other platforms.Notes:As a general philosophy, AdoptOpenJDK will continue to build binaries for LTS releases as long as the corresponding upstream source (Oracle OpenJDK) is actively maintained.The Eclipse OpenJ9 Support Document covers extra support info for that VM.This information might change overtime and was gathered from the vendors pages and support.Contact vendors for the most recent information.jClarity will support their JDK binaries as long as produced which likely means 2028Technology Compatibility Kit for Java (TCK)The Java Compatibility Kit (a.k.a., the JCK or TCK for Java SE) is an extensive test suite used by Oracle and licensees to ensure compatible implementations of its platform.This ensures that the OpenJDK implementation does not have major differences from the Oracle’s JDK, but it is still possible for there to be minor differences in the binary distribution.Sun released a specific license to permit running the TCK in the OpenJDK context for any GPL implementation deriving substantially from OpenJDK.This also means to be TCK compliant.The JDK distribution is required to use the same GPL license.Otherwise you cannot obtain legal access the TCK.It is available at no charge to developers who are planning to deploy a compatible Java implementation based on code derived from the OpenJDK or who are participating in OpenJDK research, bug fixes, code enhancement and/or ports to other hardware or software architectures.Using distributions provided by your linux distributionMany Linux distributions will continue to provide OpenJDK binaries for their distributions through their package managers including and not limited to Debian, Ubuntu, CentOS, Fedora, Alpine, …Java Desktop, Java Web Start and JavaFXThere are various changes with Desktop Java SE starting with the Oracle JDK 11 that you should be aware of.JavaFX and OpenJFXAs of Java version 11, both Oracle’s JDK and Oracle’s OpenJDK will no longer contain the JavaFX or OpenJFX libraries.You will have to add these libraries yourself or through build tools.The update cycle is the same as OpenJDK: if OpenJFX 12 is released, public updates are dropped for OpenJFX 11.Java PackagerThe Java packager, which allows you to bundle applications and their dependencies with the JVM, is no longer part of the OpenJDK and has been removed from all of Oracle’s JDK versions starting from version 11.There is a JEP to add a new packaging tool to OpenJDK but this is not yet ready for Java 11.Java WebStartJava WebStart has been removed from Oracle’s JDK versions starting from version 11.  Alternatively you can use IcedTea-Web  AdoptOpenJDK will be supporting OpenJDK binaries with IcedTea-Web  IBM will be supporting AdoptOpenJDK builds of OpenJDK with IcedTea-Web  Builds from Red hat include a simplified IcedTea-Web installer (ojdkbuild)  Karukun is working on an OSS replacement for Web StartSourcesThe information in this blog post comes from various sources which will be listed below.A huge thanks goes out to the creators of the “Java Is Still Free” document who granted us permissions to use their post for this blog post.We used these sources either because we were granted permissions to use them or the terms allowed us to use them.  Java Is Still Free  The jdk-updates-dev Archives"
      },
    
      "agile-2019-05-15-ba-and-beyond-html": {
        "title": "BA &amp; Beyond - an inspiring conference for keen business analysts",
        "url": "/agile/2019/05/15/BA-and-Beyond.html",
        "image": "/img/ba-and-beyond-2019/main-image.png",
        "date": "15 May 2019",
        "category": "post, blog post, blog",
        "content": "  The BA &amp; Beyond conference is a two day programme enabling business analysts to share their experiences and find inspiration in their daily work.The conference was an intense mix of talks and workshops, providing space for inspiring people to share their stories. After each talk, the attendees had the possibility to engage in conversation during a Q&amp;A. The sessions were very interactive and really empowering.This second edition of the BA &amp; Beyond conference was held during two days in Brussels and Amsterdam. Various topics were discussed, such as visual facilitation techniques, «How to Make an Agile Project Succeed», business analyst behaviours, or «The Inner Architecture of a Performer». What follows below is an overview of some of these talks.Table of Contents  When BAs go BAD - Christina Lovelock  The 7 hidden layers behind agile techniques: Ever wondered why we use post-its during retrospectives? – Pieter Van Driessche  You are doing it wrong: The truth about user stories – Pieter Hans  ConclusionWhen BAs go BAD - Christina LovelockChristina Lovelock has built BA teams in several public sector organisations including the NHS, ranging in size from 5 to 120 Business Analysts. She currently leads the BA practice at the University of Leeds.She is active in the BA professional community in the UK, attending and regularly speaking at local and national events and conferences.Christina’s talk was purely soft skills related and depicts the sort of behaviours shown by business analysts when they face difficult situations.Some typical BA behavioural mistakes are triggered by the way they see themselves in the team. For instance, a BA could feel as if he or she is not part of the team: there are de facto less analysts than developers in a team which can lead to an «Us VS Them» mentality.It is also sometimes difficult to build a BA community, and being sandwiched between IT and the business can sometimes lead to negative self perceptions such as «I am just a BA» or «Neither the PO nor the developers understand me».Understanding that the BA is as much a member of the team as the others and that he or she is not the only one accountable for the communication between IT and Business, leads to better collaboration and is a great step towards agility, allowing all team members to be more confident and positive.Whatever your role in a project team; collaboration, transparency and confidence - in yourself and in your team mates - as well as clear communication are key factors to succeed together.Another difficulty in the BA role is providing the right level of detail in your analysis. Indeed it is sometimes difficult to have the right level of focus on the right things. Moreover, being a good BA is also about sharing knowledge, tools and techniques. Talking with other analysts and contributing to the practice are important aspects of the job.Of course all those behaviours and feelings don’t just apply to business analysts but also to every team role: scrum master, developer, product owner, tester.This talk gave us the opportunity to identify and name some feelings we can face as business analysts while giving us the keys to find solutions to the difficult situations we could face.The 7 hidden layers behind agile techniques: Ever wondered why we use post-its during retrospectives? – Pieter Van DriesschePieter Van Driessche has worked in IT for over 20 years and did most of the common software jobs: developer, analyst, team lead and project manager. He experimented with cultural change management for 2 years, after which he returned to IT because of the faster speed of change. He has been an agile coach for almost 15 years and coaches all kinds of teams in all kinds of organisations: software, operation or marketing teams, in Belgium and abroad, in small and large organisations.This talk was a great eye-opener about a simple thing we all do every day in our scrum teams: sticking post-its! Although this has become a very common habit, we might not know why post-its are used during scrum meetings and especially during retrospectives. Below are some insights.Psychological safetyThanks to post-its, all participants get the same space. Post-its give everybody a voice, everybody can express themselves without fear. Moreover, post-its give a real chance to introvert people.Team objectives and peer pressuresBy giving post-its to everybody attending the retrospective, peer pressure encourages everybody to participate and everybody will stick at least one post-it. And the more post-its we have, the greater the outcome we build.This fact is closely linked to our comfort zone. Although it is sometimes difficult to say what we really think, the post-its and the peer pressure force us to get out of our comfort zone by expressing ourselves, finding solutions and being confronted with our team mates.Getting out of our comfort zone promotes continuous improvement, and that’s what we all want.The physical connectionMoving post-its on a white board allows us to make abstract things tangible. As a consequence, we are more involved in our daily work as we, and the others, can see our progress in a physical way.You are doing it wrong: The truth about user stories – Pieter HensPieter Hens has been working for over 15 years in a software product development and analysis setting. He has taken up roles as a business and functional analyst, project manager, coach and team lead in projects of various sizes. He currently specialises in the full range of software product management: product strategy, discovery and development.User stories are part of the root of our projects, we use them on a daily basis to achieve our goals, however the user stories are sometimes wrongly used, leading to counterproductive effects.Here are some myths about user stories.The analyst writes the user stories and the dev team implements themWriting a user story is not the BA responsibility, elaborating a user story is a team effort and everybody in the team is eligible to write user stories.Writing the user stories together reduces risk of misunderstanding. Before implementing a user story, all team members should agree on and understand the user story content.We don’t need documents or models anymoreUser stories have to be backed up by documents and models. A user story alone is not enough, we need a context and the big picture.User stories are not meant to replace documents, they are meant to tell stories.If it does not fit the ‘As a [role] I can [do something]’ - template, it is not a user storyHaving a template for the user story is a good way to ease the understanding of the common goal. Having our requirements standardised simplifies the processes.However, not all user stories fit the same strict template. Even though a template is important, we should focus less on the template and more on the content itself.ConclusionI learned a lot during these two days and the conference gave me new insights and tools to deal with my work every day.This great experience also gave me the opportunity to talk and share knowledge and information with other business analysts. It was really inspiring.The slides for these presentations can be found on the website of BA&amp;Beyond."
      },
    
      "cloud-2019-05-03-istio-service-mesh-s2s-html": {
        "title": "Istio Service Mesh: service to service communication",
        "url": "/cloud/2019/05/03/istio-service-mesh-s2s.html",
        "image": "/img/2019-04-14-istio-service-mesh-s2s/istio.jpeg",
        "date": "03 May 2019",
        "category": "post, blog post, blog",
        "content": "  This post will describe how to use the Istio service mesh to provide service to service authentication and authorization in a Kubernetes cluster.It will show how ServiceRoles, ServiceRoleBindings and Identities in Istio can be used to achieve this.Table of content  What is Istio?  Istio concepts  Show me the code  ConclusionWhat is Istio?Istio is a service mesh created by Google, Lyft and IBM. It aims to simplify some security and management aspects of a microservices software architecture.More information on Istio and its features can be found in its docs.In this blogpost we will highlight one of the key security features of Istio: service to service authentication and authorization.For the sake of simplicity, this post will focus on an Istio setup in Kubernetes.In a microservices architecture, managing access to services can be a challenging operation. For end-user facing services, JWTs are used to add authorization information to a request.They are used by the service to determine which end-user is making the request.These tokens can be generated based on information that the end-user provides to an identity provider.In most cases this information is a username and password, with some additional 2FA if possible.This setup can be achieved by using OpenID Connect as a protocol with the authorization code grant flow and an identity provider like Keycloak.When services communicate with each other, they also need to provide an identity to each other.A common option to do this is by using client credentials grant flow of OpenID Connect.In this flow a service provides its client credentials to authenticate against the identity provider, and to be able to generate an access token once authenticated.This token will be used to communicate to a service.These are types of authorization flows on application level.They allow services to determine what resources an end-user or service can access.Istio’s service to service role based acccess control (RBAC) is not on application level but on communication level.It specifies which services can connect and communicate with each other. In order to achieve this, Istio connects an identity to each service in the mesh and allows it to authenticate itself.The requested service can use this identity to determine if the service is allowed to connect or not.Istio makes use of proxies to handle all traffic (into and out of services) and using mutual trusted certificates to secure the connection and provide an identity to these proxies.When using the automatic proxy injection, enabling Istio’s service to service RBAC mechanism is almost as easy as flipping a switch.There are five main components responsible for making this possible in Istio: Citadel, Pilot, Galley, Mixer and Envoy.Citadel is Istio’s fortress of trust.It manages all certificates and acts as a Root CA in the Istio setup.Galley is the main configuration manager.It is responsible for gathering all required information from the underlying platform.Pilot manages all routing information and manages all the information for the proxies.It will initialise the proxies during start-up with their configuration and the certificates from Citadel.Mixer is responsible for all monitoring, logging and authorization information.Whenever a proxy performs an action, Mixer knows about it. This allows it to both monitor and log connections, but also provide authorization information to the proxies.The final piece to the puzzle is Envoy. Envoy is the sidecar proxy responsible for handling the actual traffic between services in the service mesh.It will setup and manage the required mTLS connections and perform all required check with regards to the routing. Envoy is managed as a separate project and in theory an other proxy could be used, but Envoy is most common.A final, optional component is the sidecar injector.This component is not mandatory for the service mesh to work, but makes using it a lot easier.The injector is set up as a mutating webhook admission controller. In a nutshell, this allows the injector to inspect and update some specific objects in the Kubernetes API.It will automatically inject the Envoy sidecar proxy into every pod which needs it.Istio conceptsIstio stores all its configuration directly in the Kubernetes API through the use of Custom Resource Definitions (CRDs).Next, a small description of the ones relevant for our blog are explained.PoliciesPolicies are at the heart of the mTLS setup in Istio.They define when mTLS should be used and how.Policies can be scoped in two levels: mesh wide (Mesh Policies) and namespace wide.Destination RulesDestination rules are a set of rules that are evaluated when a service is called.They define multiple different routing options.For the scope of this blogpost, they will only be used to define which services require to be accessed using mTLS.Service RolesService Roles are used in Istio to describe which access a role provides. It specifies which endpoints of a specific service can be used.Currently this is described by specifying the full internal DNS name of the service, the methods and the paths that the role can access.Service Role BindingsService Role Bindings are used to connect identities (service accounts) or identity properties (namespaces) to actual roles. When a binding is created, the identities connected to it are allowed the access specified in the referenced service role.Show me the codeThe Istio service to service authentication and authorization will now be explained by using an example setup.Note that the code snippets have been shortened in this blogpost.This is denoted with three dots ....The full examples can be found in the accompanying repository on GithubPrerequisitesThis demo assumes that Istio is already installed in the cluster with the demo profile enabled. See Install Istio for more information on the installation of Istio.In the demo repository, a small script can be found that can assist in setting up the demo environmentSetupThe setup of our application is a very simple service with a database backend. Our service exposes one HTTP GET endpoint which will be accessed by the outside world. Our database is an Apache CouchDB instance.Both the database and the service run inside Kubernetes.The setup is shown in the image below.Create namespaceFirst, a new namespace is created.The service and database will both be added to this namespace.kubectl create namespace with-istiokubectl label namespace with-istio istio-injection=enabledInstall CouchDBNext, the database is installed.This looks like a normal stateful set for a CouchDB database.There are some important changes.First, a specific service account is created for CouchDB. This is needed as Istio will use the service accounts in Kubernetes as its identities.The service account is linked to the podspec in the stateful set definition. This way, it can be used by the Istio proxy later on.Secondly, the probes have been adapted to work in Istio.Since Istio intercepts all traffic in the pod, it will also intercept requests from the Kube API to the service.Since the demo setup requires mTLS to be used, the probes would fail because the Kube API doesn’t use mTLS.Instead of manually changing the probes, Istio now has the option to rewrite the probes during the automatic proxy injection. More information on the probes can be found in the Istio Docs.Note that no Istio specific configuration is required in the service manifests.This is possible because the demo profile automatically enables the sidecar injector and we enabled the injection on the with-istio namespace using the istio-injection=enabled label.The automatic sidecar injector will inject the Envoy sidecar into all pods.---# Source: couchdb/templates/serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: couchdb---# Source: couchdb/templates/service.yamlapiVersion: v1kind: Servicemetadata:  name: couchdb  labels:   ...spec:  type: ClusterIP  ports:    - port: 80      targetPort: http      protocol: TCP      name: http  selector:    app.kubernetes.io/name: couchdb    app.kubernetes.io/instance: couchdb---# Source: couchdb/templates/statefulset.yamlapiVersion: apps/v1kind: StatefulSetmetadata:  name: couchdb  labels:    ...spec:  replicas: 1  serviceName: couchdb  selector:    matchLabels:      app.kubernetes.io/name: couchdb      app.kubernetes.io/instance: couchdb  template:    metadata:      labels:        app.kubernetes.io/name: couchdb        app.kubernetes.io/instance: couchdb    spec:      serviceAccountName: couchdb      containers:        - name: couchdb          image: \"couchdb:2.3.0\"          imagePullPolicy: IfNotPresent          ports:            - name: http              containerPort: 5984              protocol: TCP          livenessProbe:            exec:              command:              - curl              - http://localhost:5984/_up          readinessProbe:            exec:              command:              - curl              - http://localhost:5984/_up          resources:            ...Install the service: test-appA small NodeJS application was created for this demo.It exposes an HTTP GET endpoint which connects to the CouchDB database.The manifests are very similar to the CouchDB versions.As with CouchDB, note that no Istio specific configuration is required on the manifests.A service account is created and linked to provide the service with a unique identity in Kubernetes and Istio.---# Source: test-app-chart/templates/serviceaccount.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: test-app-test-app-chart---# Source: test-app-chart/templates/service.yamlapiVersion: v1kind: Servicemetadata:  name: test-app-test-app-chart  ...---# Source: test-app-chart/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: test-app-test-app-chart  labels:   ...spec:  replicas: 1  selector:    matchLabels:      app.kubernetes.io/name: test-app-chart      app.kubernetes.io/instance: test-app  template:    metadata:      labels:        app.kubernetes.io/name: test-app-chart        app.kubernetes.io/instance: test-app    spec:      serviceAccountName: test-app-test-app-chart      containers:        - name: test-app-chart          image: \"test-app:latest\"          imagePullPolicy: IfNotPresent          ports:            - name: http              containerPort: 8080              protocol: TCP          livenessProbe:            ...          resources:            ...The current setup is displayed in the following drawing.Enabling mutual TLS (mTLS)Currently the service can connect to the backend just fine. TLS is currently not used to communicate between the service.The following manifest defines a policy which changes this. It is a namespace scoped policy telling Istio that all services in the with-istio namespace should ONLY accept mTLS connections.This configuration will be picked up by Pilot and distributed to all Envoy proxies in the with-istio namespace.When this policy is applied, Envoy will drop any requests it gets that don’t use mTLS.apiVersion: \"authentication.istio.io/v1alpha1\"kind: \"Policy\"metadata:  name: \"default\"  namespace: with-istiospec:  peers:  - mtls:       mode: STRICTNote, this policy only affects the incoming connections on the Envoy proxy.When a request would be sent to the test-app service now, it would be rejected with an HTTP 503 error code.This is shown in the following drawing.Next, the outgoing (client) connections needs to be configured to use mTLS.This can be done by specifying a destination rule for the services.A destination rule defines a set of rules that are evaluated for every outgoing request from a proxy.This rules defines that every proxy in with-istio namespace needs to use mutual TLS for every service that ends with .local.By applying this rule, the requests will succeed again on the test-app service.apiVersion: \"networking.istio.io/v1alpha3\"kind: \"DestinationRule\"metadata:  name: \"default\"  namespace: with-istiospec:  host: \"*.local\"  trafficPolicy:    tls:      mode: ISTIO_MUTUALEnabling Role Based Access Control (RBAC) on the servicesServices can now communicate securely over mTLS.To increase the security even further, RBAC can be added to the services.RBAC allows for roles to be defined that specify access to specific services in the cluster. By attaching these roles to service accounts (which are connected to services) services can be permitted to access specific other services. This limits the reach a single service has in the cluster and therefor adheres to the least privileges principle.The following manifest defines a cluster RBAC configuration.Such configuration can only exist once in the entire service mesh and it needs to have the name default.The mode ON_WITH_INCLUSION specifies that all subjects that are listed in the inclusion section need to have RBAC enabled. These subjects can be namespaces and/or specific services.Specifying the namespace with-istio in the inclusion section, enables RBAC for all services in that namespace.By default the RBAC configuration will reject all requests which don’t have the proper access defined with an HTTP 403 error code.apiVersion: \"rbac.istio.io/v1alpha1\"kind: ClusterRbacConfigmetadata:  name: defaultspec:  mode: 'ON_WITH_INCLUSION'  inclusion:    namespaces: ['with-istio']After this RBAC config is applied, requests to the test-app instance will start failing again. The test-app currently doesn’t have a role attached to its service account that allows it to access the CouchDB database. Therefor all requests to the service will be rejected with an HTTP error code of 403.This is shown in the following drawing.The following manifest creates a role that allows access to the CouchDB service for all GET requests on any given path.Note that the full service name is used in the services specification, this is currently required by Istio.This is only needed for Istio to identify the traffic, short names can still be used to access the service.By applying this service role, nothing will change to the requests to the test-app since the role is not yet connected to the service account of the test-app service.apiVersion: \"rbac.istio.io/v1alpha1\"kind: ServiceRolemetadata:  name: couchdb-role  namespace: with-istiospec:  rules:  - services: [\"couchdb.with-istio.svc.cluster.local\"]    methods: [\"GET\"]    paths: ['*']So next, we link the new role to the service account of the test-app service.This is done through a service role binding.There are two sections to this binding: the role and the subjects.The role is the one that was created using the previous manifest. The subjects can be any identity known to Istio. In the demo scenario, only the service accounts are known.Istio defines a service account as a user identity. As with the service names, the service account reference needs to be the full reference scoped towards the cluster.This allows services from outside of the namespaces to be specified as well.apiVersion: \"rbac.istio.io/v1alpha1\"kind: ServiceRoleBindingmetadata:  name: bind-test-app-service-couchdb-role  namespace: with-istiospec:  subjects:  - user: \"cluster.local/ns/with-istio/sa/test-app-test-app-chart\"  roleRef:    kind: ServiceRole    name: \"couchdb-role\"After applying the last manifest, requests should again be authorized and allowed to connect to the CouchDB instance.ConclusionThis demo showed how Istio can be used to secure communication between services using mTLS.Moreover it showed how the service mesh level authentication can be used to grant or deny access to services in the mesh.A role can be connected to a service account to allow access. Important to note is that the service mesh only allowes or denies traffic.It doesn’t influence the application level access.In a nutshell, Istio allows cluster admins to enable secure communication, and strong authentication and authorization mechanisms on their Kubernetes cluster without having to manage all kinds of certificates, usernames and passwords. The application developers don’t need to adopt their application in order to communicate securely in the cluster, nor do they have to change their deployment configuration to enable the service mesh.This blogpost only highlighted a portion of the features of Istio. Security is only a part of the feature set. Istio also allows advanced traffic management, monitoring and logging.Maybe something for a future blogpost."
      },
    
      "development-2019-04-29-keep-your-rxjs-streams-from-bursting-their-banks-html": {
        "title": "How to keep your RxJS streams from bursting their banks",
        "url": "/development/2019/04/29/Keep-your-RxJS-streams-from-bursting-their-banks.html",
        "image": "/img/2019-04-15-Keep-your-RxJS-streams-from-bursting-their-banks/speedlimit.jpg",
        "date": "29 Apr 2019",
        "category": "post, blog post, blog",
        "content": "In my current role as Angular coach I try to help my colleagues as much as possible to make their applications in a logical and comprehensible way.I’m a huge supporter of RxJS for reactive programming and I ask others to consider using the technology to help them create declarative code and prevent them from going to Callback Hell.But as people start to embrace RxJS streams, the difficulties paired with using reactive programming start to surface and I’ve concluded that there also exists something I’d like to call Observable Purgatory.At the moment of writing this, I’m staring at an Angular component file having 548 lines - no inline template or styles - of which 64 lines are imports and 411 lines either are a declaration or instantiation of a stream or are a function in which a stream is used.Some of these functions are more than 20 lines, and 3 are even more than 40 lines.Holy nightmare, Batman!Imagine the unit tests needed to test all possible cases.Apparently those aren’t needed, because the biggest function - 48 lines - is tested in 2 unit tests and of course “everything works, doesn’t it?”So without further ado, here are the most common bad practices and their solutions.Note: Unless explicitly mentioned, these are NOT Angular specific.Consuming all the RAM  Developers who are new to RxJS will often forget that a subscription can live on during the whole life time of your application.Most of the time they have only worked with simple XHR requests and those will by default emit only one value and then complete.But reactive programming allows to create streams that emit multiple values and might never complete.This will cause memory leaks, unexpected behaviour and therefor bugs.The solution is quite simple: just unsubscribe or take only the needed events!You can cancel a subscription by calling its unsubscribe function, but then you’re skimming the power of reactive programming.Operators like - but not limited to - first, take, takeWhile and takeUntil will close a subscription as soon as the condition is met.interval(1000)       // --0--1--2--3--4--5--6--7--8--9--...--&gt;  .pipe(take(5))     // --0--1--2--3--(4|)  .subscribe(tick =&gt; console.log(tick));Note: When using mergeMap or another variant in combination with takeUntil, make sure you add the takeUntil pipe at the end.  RULE: CANCEL SUBSCRIPTIONS THAT DO NOT COMPLETE BY THEMSELVESAngular specific: Use the AsyncPipeAngular provides an AsyncPipe, which lets you subscribe to a stream from inside your component’s template.The great advantage is that this subscription will automatically be openend when the component or the element in which it’s used is created and cancelled when the component is destroyed.A quick example of a counter of seconds:@Component({  selector: 'my-app',  template: `&lt;p&gt;      Tick: {{ tick }}    &lt;/p&gt;`})export class AppComponent implements OnInit, OnDestroy {  myStream$ = interval(1000);  subscription: Subscription;  tick: number;  ngOnInit() {    this.subscription = this.myStream$.subscribe(tick =&gt; {this.tick = tick})  }  ngOnDestroy() {    this.subscription.unsubscribe();  }}This can be written a lot shorter and maintainable using the AsyncPipe:@Component({  selector: 'my-app',  template: `&lt;p&gt;      Tick: {{ myStream$ | async }}    &lt;/p&gt;`})export class AppComponent  {  myStream$ = interval(1000);}  RULE: USE ANGULAR’S ASYNCPIPE WHERE POSSIBLEYo dawg, I heard you like Subscriptions  This should be basic knowledge by anyone using the Observable pattern.Apart from being extremely ugly to look at, it’s near impossible to keep track of what subscription is open at which moment.Consider the following example:subscription1 = param$.subscribe(param =&gt; {  subscription2 = apiService.getObject(param).subscribe(obj =&gt; {    this.obj = obj;    subscription3 = anotherApiService.getEvents(obj.busId).subscribe(events =&gt; {      this.events = events;    });  });});onDestroy() {  subscription1.unsubscribe();  if (subscription2 &amp;&amp; !subscription2.closed) subscription2.unsubscribe();  if (subscription3 &amp;&amp; !subscription3.closed) subscription3.unsubscribe();}In this case, when the param$ stream emits a value, the obj is requested from an API endpoint and when received, it will in its turn request events occurring on a specific busId.But what happens when param$ emits a new value? The objproperty will be replaced eventually, but the event stream for the first object will still exist.So you could start keeping track of subscriptions and cancel them at the ‘right’ moment:subscription1 = param$.subscribe(param =&gt; {  if (subscription2 &amp;&amp; !subscription2.closed) subscription2.unsubscribe();  subscription2 = apiService.getObject(param).subscribe(obj =&gt; {    this.obj = obj;    if (subscription3 &amp;&amp; !subscription3.closed) subscription3.unsubscribe();    subscription3 = anotherApiService.getEvents(obj.busId).subscribe(events =&gt; {      this.events = events;    });  });});onDestroy() {  subscription1.unsubscribe();  if (subscription2 &amp;&amp; !subscription2.closed) subscription2.unsubscribe();  if (subscription3 &amp;&amp; !subscription3.closed) subscription3.unsubscribe();}Wow! That’s amazing!Except it isn’t.There are still 3 subscriptions, while it could be done with just 1 when using the correct operators:subscription1 = param$.pipe(  switchMap(param =&gt; apiService.getObject(param)),  tap(obj =&gt; this.obj = obj),  switchMap(obj =&gt; anotherApiService.getEvents(obj.busId))).subscribe(events =&gt; {  this.events = events;});onDestroy() {  subscription1.unsubscribe();}This way, only 1 subscription will exist, canceling it will automatically cancel any inner streams.And when param$ would emit a new value, the switchMap operator will automatically cancel its inner stream and create a new one.  RULE: NEVER SUBSCRIBE WITHIN A SUBSCRIPTIONThis also applies for calling a function inside a subscription when that function has a subscription.Sharing is caring  When using a stream on multiple locations, remember that there is a difference between a cold and a hot Observable.In short: a cold Observable will restart its stream for each subscription, while a hot Observable will reuse an existing stream when a subscription is added.Think of a cold Observable as a HTTP request, which fires each time again, and a hot Observable as a keyPress stream which emits the same event no matter how many subscriptions exist on the stream.If you need more details, you can read this article by Ben Lesh Hot vs Cold Observables.Sometimes you’ll need to react on a cold Observable in multiple separate streams.For example, you need to display the response, but you also need to initiate a new stream to get other data.So you’ll create two subscriptions and use a mergeMap, because subscriptions inside subscriptions are not allowed:stream1 = httpGet('userCarMake');stream2 = stream1.pipe(mergeMap(make =&gt; httpGet('userCarMake', make)));stream1.subscribe(make =&gt; console.log('The car make is', make));stream2.subscribe(models =&gt; console.log('The models for this make are', models.join()));But in your developer tools’ network tab, you notice that userCarMake has been requested twice.The answer to why should be obvious by now: there are two subscriptions.To solve this with minimal changes, make the source stream a hot Observable using shareReplay.stream1 = httpGet('userCarMake').pipe(shareReplay(1));stream2 = stream1.pipe(mergeMap(make =&gt; httpGet('userCarMake', make)));stream1.subscribe(make =&gt; console.log('The car make is', make));stream2.subscribe(models =&gt; console.log('The models for this make are', models.join()));  RULE: MAKE YOUR OBSERVABLES HOT WHEN NECESSARYThough be careful when using this operator.If the source doesn’t complete by itself, it will keep emitting values.To prevent this behaviour, use an options object in the shareReplay operator and set the required property refCount to true:sharedStream = source.pipe(shareReplay({bufferSize: 1, refCount: true}));See this stackblitz for an example of the difference.You can set a tslint rule to make sure you always use shareReplay with options.Here you can find the configuration: rxjs-tslint-rules: rxjs-no-sharereplay.Angular specific: use shareReplay with AsyncPipeA point of note for Angular’s AsyncPipe is that each AsyncPipe will create a new subscription.Use shareReplay when multiple AsyncPipes are used on the same stream or a stream depending on it.@Component({  selector: 'my-app',  template: `&lt;ng-container *ngIf=\"hasItems$ | async\"&gt;    &lt;p *ngFor=\"let item of items$ | async\"&gt;      Tick: {{ item }}    &lt;/p&gt;    &lt;/ng-container&gt;`})export class AppComponent  {  items$ = getItems().pipe(shareReplay(1));  hasItems$ = items$.pipe(map(items =&gt; items.length &gt; 0));}My stream is too big  One of the major advantages of using streams is that you can write declarative code.Declarative code means that the code can explain itself without the need of comments.Great!Now developers don’t need to worry about writing comments anymore.RxJS provides a lot of operators that are self-explanatory: map, filter, withLatestFrom, catchError, …So what’s the problem?It’s something what I’ve noticed a lot lately.A stream with so many piped operators that it isn’t even funny anymore.Let’s look at the following stream: (Note: this is a slightly modified stream from the file spoken of earlier, because I just can’t come up with this stuff.)this.childObject$ = merge(    this.startFetch$.pipe(        switchMap(_ =&gt; this.parentObject$),        withLatestFrom(this.cancelled$),        filter(([parentObject, cancelled]) =&gt; cancelled !== true),        map(([parentObject, _]) =&gt; parentObject),        filter(            parentObject =&gt;            !isNullOrUndefined(parentObject) &amp;&amp;            parentObject.subObjects.length &gt; 0 &amp;&amp;            parentObject.errors.findIndex(                (error: parentObjectError) =&gt;                error.errorCause === parentObjectErrorCauseEnum.HEIGHT_EXCEEDED            ) === -1        ),        withLatestFrom(this.collectionId$),        switchMap(([parentObject, collectionId]) =&gt;            this.service.getChildObjectList(parentObject.subObjects[0].id, collectionId).pipe(                catchError(error =&gt; {                    console.log('error while retrieving second object list', error);                    return EMPTY;                }),                take(1)            )        ),        switchMap(childObjectList =&gt; {            if (childObjectList &amp;&amp; childObjectList.length &gt; 0) {                this.isLastChildObject$.next(childObjectList[0].isLast);                if (isValidChildObject(childObjectList[0])) {                    this.isFormValid$.next(true);                } else {                    this.isFormValid$.next(false);                }                return of(childObjectList);            } else {                this.startFetch$.next(null);                return EMPTY;            }        }),        map(childObjectList =&gt; childObjectList[0])    ),    this.startFetch$.pipe(mapTo(null))).pipe(shareReplay({bufferSize: 1, refCount: true}));this.childObject$.subscribe(childObject =&gt; console.log('Second object is', childObject);this.isFormValid$.subscribe(isFormValid =&gt; console.log('Form valid?', isFormValid);this.isLastChildObject$.subscribe(lastChildObject =&gt; console.log('Second object is last from list', lastChildObject);Can anyone explain what is going on here?If you’re reading this blog than you probably have some experience with RxJS and probably understand the meaning of the operators used in this example.And while it might be readable for an insightful developer, try to imagine a newbie getting thrown into a project where this is presented.I bet they won’t be very inspired or motivated to work on this.When that happens, you have to split up the stream, just like you’d split up functions that get too big.This might mean a bit more lines, but it would make the code a lot more readable.isParentObjectValid(parentObject) {    return !isNullOrUndefined(parentObject) &amp;&amp;        parentObject.subObjects.length &gt; 0 &amp;&amp;        parentObject.errors.findIndex(            (error: parentObjectError) =&gt;            error.errorCause === parentObjectErrorCauseEnum.HEIGHT_EXCEEDED        ) === -1;}getChildObjectList(subObjectId) {    return this.collectionId$.pipe(        take(1),        switchMap(collectionId =&gt; this.service.getChildObjectList(subObjectId, collectionId)),        catchError(error =&gt; {            console.log('error while retrieving second object list', error);            return EMPTY;        }),    )}getFirstItemFromChildObjectList(childObjectList) {    if (childObjectList &amp;&amp; childObjectList.length &gt; 0) {        this.isLastChildObject$.next(childObjectList[0].isLast);        if (isValidChildObject(childObjectList[0])) {            this.isFormValid$.next(true);        } else {            this.isFormValid$.next(false);        }        return of(childObjectList[0]);    } else {        this.startFetch$.next(null);        return EMPTY;    }}this.childObject$ = merge(    this.startFetch$.pipe(        switchMap(_ =&gt; this.parentObject$),        withLatestFrom(this.cancelled$),        filter(([parentObject, cancelled]) =&gt; !cancelled &amp;&amp; isParentObjectValid(parentObject)),        switchMap(([parentObject]) =&gt; getChildObjectList(parentObject.subObjects[0].id)),        switchMap(childObjectList =&gt; getFirstItemFromChildObjectList(childObjectList))    ),    this.startFetch$.pipe(mapTo(null))).pipe(shareReplay({bufferSize: 1, refCount: true}));this.childObject$.subscribe(childObject =&gt; console.log('Second object is', childObject);this.isFormValid$.subscribe(isFormValid =&gt; console.log('Form valid?', isFormValid);this.isLastChildObject$.subscribe(lastChildObject =&gt; console.log('Second object is last from list', lastChildObject);Now it’s more possible to test various situations for each inner stream and mock the individual outcomes to be used in a test for the outer stream.We can even move parts to separate files or classes, but for the sake of this example we’ll keep everything together.It’s not yet perfect, but it gives some room to breathe.At least we can now read quickly what the outer stream is supposed to do.Notice that there are no longer pipe functions within other pipe functions.This makes a stream more streamlined (pun intended, really).  RULE: SPLIT UP YOUR STREAMS AND USE DECLARATIVE FUNCTIONS INSIDE THE OPERATORS  RULE: TRY TO REDUCE THE USE OF PIPE INSIDE PIPE, UNLESS IT’S IN A SEPARATE FUNCTIONImpurity and side effects  Pure functions are functions that will always return the same value for the same input parameters, keep state local and do not alter the input parameters.In the previous example the created function isParentObjectValid is pure, but getChildObjectList and getFirstItemFromChildObjectList aren’t.Of course not everything can be written in only pure functions, but we should at least try as much as possible.The same can apply for streams.A “pure” stream is a stream which produces the same value for the same source, does not alter the source and produces no side effects.This means that it should not set events on a different stream and should not set or read a value from outside its scope.For example, the following blocks will do the same, but one keeps its state inside the Observable scope, which is safer.let _score = 0const score$ = goals$.pipe(    map(goalPoints =&gt; {        _score += goalPoints;        return _score;    }));// Keeping state within the stream is better:const betterScore$ = goals$.pipe(    scan((totalScore, goalPoints) =&gt; totalScore + goalPoints, 0));Notice the use of the scan operator instead of a map.If you have side effects, you might be using the wrong operators.More about that a bit later in this post.I believe that a stream should always be a constant and never be reinitialized.Luckily it’s possible to create custom pipe operators for these situations.Let’s continue based on the example from the previous chapter:this.nonCancelledParentObject$ = this.parentObject$.pipe(this.onlyNonCancelledParentObject(this.cancelled$));this.childObjectList$ = this.nonCancelledParentObject$.pipe(this.childObjectListForCollection(this.collectionId$));this.firstItemOfChildObjectList$ = this.childObjectList$.pipe(this.selectFirstItemOfList);this.isLastChildObject$ = this.firstItemOfChildObjectList$.pipe(this.isChildObjectLastInList);this.isFormValid$ = this.firstItemOfChildObjectList$.pipe(this.isChildObjectValid);this.startFetch$ = this.childObjectList$.pipe(this.isListEmpty);this.childObject$ = this.firstItemOfChildObjectList$.pipe(this.createStreamOnTrigger(this.startFetch$));onlyNonCancelledParentObject = (cancelled$) =&gt; (parentObject$) =&gt; {    return parentObject$.pipe(        withLatestFrom(cancelled$),        filter(([parentObject, cancelled]) =&gt; !cancelled &amp;&amp; isParentObjectValid(parentObject)),        map(([parentObject]) =&gt; parentObject)    );}childObjectListForCollection = (collectionId$) =&gt; (nonCancelledParentObject$) =&gt; {    return nonCancelledParentObject$.pipe(        take(1),        map(parentObject =&gt; parentObject.subObjects[0].id),        withLatestFrom(collectionId$)        switchMap(([subObjectId, collectionId]) =&gt; this.service.getChildObjectList(subObjectId, collectionId)),        catchError(error =&gt; {            console.log('error while retrieving second object list', error);            return EMPTY;        })    );}isListEmpty = filter(list =&gt; list &amp;&amp; list.length &gt; 0);selectFirstItemOfList = (childObjectList$) =&gt; {    return childObjectList$.pipe(        filter(childObjectList =&gt; childObjectList &amp;&amp; childObjectList.length &gt; 0),        map(childObjectList =&gt; childObjectList[0]),        shareReplay({bufferSize: 1, refCount: true})    );}isChildObjectLastInList = map(childObject =&gt; childObject.isLast);isChildObjectValid = map(childObject =&gt; isValidChildObject(childObject));createStreamOnTrigger = (trigger$) =&gt; (source$) =&gt; {    return trigger$.pipe(        switchMap(_ =&gt; source$.pipe(            take(1),            startWith(null))        ),        shareReplay({bufferSize: 1, refCount: true})    );}this.childObject$.subscribe(childObject =&gt; console.log('Second object is', childObject);this.isFormValid$.subscribe(isFormValid =&gt; console.log('Form valid?', isFormValid);this.isLastChildObject$.subscribe(lastChildObject =&gt; console.log('Second object is last from list', lastChildObject);Now almost every part of the stream is created using a pure function as pipe operator.Just count the number of times the keyword this is used inside the functions (hint: we went from 10 times to only 1).This could get even better if we pass the service’s function as a parameter too.Each of these custom operators can easily be tested with marble testing.It’s more readable, because now you can know for each stream how its values are determined.For example, in line 5, we can already read that the isFormValid will be changed by a change of the first item in the list of ChildObjects and that it will react on the validity of that object.We no longer need to sift through the code to find that out.You’ll notice that there are a lot of streams now.Most of them are intermediary, so they could be scoped inside a different block.Or they can be moved to separate files to keep the main file clean and the streams grouped by logical unit, but always keep subscriptions in your main file.For the sake of this example I kept all streams together.  RULE: INSTANTIATE YOUR STREAMS WITH PURE FUNCTIONS SO THEY CAN BE TESTED AND MOVED EASILY  RULE: TRY TO ELIMINATE THE USE OF SUBJECTSAngular specific: handling @Input() propertiesA common question I get with this approach is that some streams can not be defined until an @Input() property is set.Ridiculous, there is no such thing as a “can not” in programming!Consider this example:@Component({  selector: 'greet',  template: `&lt;h1&gt;{{ message }}&lt;/h1&gt;`})export class GreetingComponent implements OnInit, OnChanges {  @Input() name: string;  message: string;  constructor(private service: APIService) { }  ngOnInit() {      this.service.getMessage(name)).subscribe(name =&gt; this.message = name);  }  ngOnChanges(changes: SimpleChanges) {      if (changes['name'] &amp;&amp; changes['name'].previousValue !== changes['name'].currentValue) {          this.service.getMessage(changes['name'].currentValue).subscribe(name =&gt; this.message = name);      }  }}This can be also be written as following:@Component({  selector: 'greet',  template: `&lt;h1&gt;{{ message$ | async }}&lt;/h1&gt;`})export class GreetingComponent  {  private name$ = new ReplaySubject&lt;string&gt;(1);  @Input() set name(name: string) {    this.name$.next(name);  };  message$ = this.createMessageStream(this.name$);  constructor(private service: APIService) { }  private createMessageStream(name$: Observable&lt;string&gt;) {      return name$.pipe(        distinctUntilChanged(),        switchMap(name =&gt; this.service.getMessage(name))      );  }}  RULE: EVERYTHING CAN BE A STREAMThough it doesn’t always have to.I have no idea what I’m doing  Last but not least, make sure you understand the flow of your streams.If it gets too confusing, it will be a pain in the behind to find bugs or make changes without breaking something.Before creating a stream, like anything in programming, analyse what your stream should exactly be doing.Do this by drawing marble diagrams.If multiple streams are needed, make a diagram for each stream and place them under each other.Use marble testing to write easy to understand unit tests for streams.In my experience, the package rxjs-marbles can help a lot with that.it('should emit parentObject only if not cancelled', marbles((m) =&gt; {    isParentObjectValidSpy.mockImplementation((parentObj) =&gt; parentObj !== 'd');    const parentObject  = m.hot('--^-a--b--c--d-|');    const cancelled  =    m.hot('--^f-t---f-----|', {t: true, f: false});    const subs =                  '^------------!';    const expected  =             '--a-----c----|';    const result = onlyNonCancelledParentObject(parentObject, cancelled);    m.expect(result).toBeObservable(expected);    m.expect(parentObject).toHaveSubscriptions(subs);    m.expect(cancelled).toHaveSubscriptions(subs);}));  RULE: ANALYSE THE NEEDS OF YOUR STREAMS USING MARBLE DIAGRAMSWhen you know what your stream should do, then it’s time to build it.There is a big amount of operators available in RxJS.The most common are displayed in an interactive diagram at rxmarbles.com.It’s important that you know the differences between them.Some examples:There’s a difference between using mergeMap, switchMap, concatMap and exhaustMap when making substreams.There is a difference between using combineLatest, withLatestFrom, zip and forkJoin when combining streams.There is also a subtle difference between first and take(1) when a stream would never emit a value and just complete:// This will output 'error: no elements in sequence'EMPTY.pipe(first())  .subscribe(    event =&gt; console.log('event', event),    error =&gt; console.log('error:', error.message),    () =&gt; console.log('completed')  );// This will output 'completed'EMPTY.pipe(take(1))  .subscribe(    event =&gt; console.log('event', event),    error =&gt; console.log('error:', error.message),    () =&gt; console.log('completed')  );The following guides can help to find which operators to use:  Which Operator to Use? - Creation Operators  Which Operator to Use? - Instance OperatorsOnce you know which operators to use, you can easily try out your stream using StackBlitz or Rx Visualizer.Don’t forget you can build your own operators easily to combine or quickhand other operators.  RULE: USE THE CORRECT PIPE OPERATORSEndingThat’s all folks.Some of these rules are opinionated, but I hope they will help some of you to write better RxJS streams.Just remember: It’s not because you know what you’re doing that everyone knows what you’re doing."
      },
    
      "agile-2019-04-04-the-scrum-framework-is-a-liberating-structure-html": {
        "title": "The Scrum framework is a Liberating Structure",
        "url": "/agile/2019/04/04/The-Scrum-framework-is-a-liberating-structure.html",
        "image": "/img/scrum-is-a-liberating-structure/main-image.png",
        "date": "04 Apr 2019",
        "category": "post, blog post, blog",
        "content": "Scrum is an Agile framework.What does that actually mean?Are we supposed to fill it? Could we grab it, put it on the wall and paint our image in it, the way we see it, the way it works for us?Probably, we can. Seemingly, many organizations attempt to connect the numbers and dots in precisely that way, trying to get a hold on their portfolio planning and utilising Scrum for metrics to fill their project reports - solely focusing on accelerating output and pushing velocity through the roof. But let’s be careful, since we can easily get trapped in this. A framework isn’t there to just fill it and use it. Did we take the time or the effort to ponder over further possibilities that the frame might entail?The framework is not meant to contain just our own painting, our own image, whatever it is that is known to us. It is there to draw attention to something beyond… like the frame on the photo. Let it be a window. A transparent artifact, always representing a mere part of reality, depending on the spot from where we are and from where we are looking, not at it but through it, giving us an opportunity to inspect what we are spending our efforts on, or maybe… what we are wasting them on?The power of the Scrum framework is in its simplicity. The predictively recurring Scrum events form a synchronized pulse creating a steady heartbeat for the members of the Scrum team. The drone is persistently present, but only discretely so, not drawing attention, because the frame is not what it’s about. It’s a minimalist frame, that vanishes after a while, and is merely setting the scene… trying to create a fertile setting for a space in which, as soon as the Scrum team is accustomed to the pattern of the heartbeat, all team members can focus - as one organism - on collaboration, innovation and co-creation.Interhuman friction due to role shifts, adapting to new responsibilities and accountabilities, as well as difficulties with alignment of expectations are well-known characteristics of a team’s storming phase. But Tuckman, with his forming, storming, norming and performing model of team dynamics was only partly right. We now know that storming will always be there to some degree, accompanying the complexity and quickly-changing environment of today. A team, in fact, is constantly hovering over its storming arrhythmia, longing for it to clear up, using the heartbeat of the framework to gradually pave the road to high performance, focusing on value and gradual improvement.Indeed, the Scrum framework is a ‘liberating structure’ in every sense. Agreeing as a team on the structure to work with - and taking up one of the roles and its respective responsibilities therein, provides a clarity on what to expect and how to cooperate within that constraint. The frame and heartbeat might appear to be limiting structures at first, but once applied as intended, prove to provide liberation – within the safety of knowing that the entire team is referencing through the same framework.Metaphorically: Imagine a team looking at the sea and the framework is not there. What are we looking at? What are we seeing? What area of the horizon are we exploring?It will be hard to be aligned unless someone or something tells us where to look and what to look for. Then put the frame, the window, on the beach. All of us are looking through it – together, exploring a far bigger wedge of the sea than we ever could imagine doing alone.At the same time it is enabling us to jointly aim for what could be beyond the horizon, when we all look in the same direction.Mind you, it is just a framework. “Metaphors are all nice and well, but what about the real world?” - you might dryly remark. Some teams indeed never really achieve this ‘selfless’ wavelength of high performance. And Scrum as a mere facilitative tool is certainly not to blame for that. Some teams tap into it rather effortlessly. But it requires a common understanding of the framework - and the discipline to jointly be accountable for its success. Furthermore, and most dauntingly, it builds on human trust, as an unconditional recipe to create psychological safety.Just like the Scrum master serves the team to enable every team member to be at her/his best, the Scrum framework is there to create the best possible circumstances for generating flexible value creation in a transparent and predictive way.Browsing through the Liberating Structures list of Henri Lipmanowicz and Keith McCandless1, it is easy to notice a parallel and detect some clear common attributes:  A Liberating Structure is simple to introduce. Just like Scrum they are easy to learn but can be hard to master. Having a good facilitator is a must.  They are result focused. Not used for the sake of it, only for the extra value it generates.  They involve rapid cycling, meaning fast iterative rounds generating input and feedback.  They are inclusive, asking everyone’s opinion to make informed decisions on the way forward.  They are seriously fun and boost a sense of freedom and responsibility within a group.These attributes help us to:  Share and spread vital knowledge  Cope with complexity  Include every member of the team and unleash their potential  Adopt a habit of creative adaptability  Promote anti-fragilityAnd on top of that, Scrum - being an Agile practice strongly rooted in Lean principles - reintroduces systems thinking and the routine of improvement in the process of co-creation.If there is one principle in the world that should never change, let it be this one.  “Stay agile, never change” - Adam Weisbart2References  1 For a full but ever growing list of these liberating structures, visit http://www.liberatingstructures.com/ls-menu/ and http://www.liberatingstructures.com/ls-in-development/  2 The epic quote by Adam Weisbart, concluding every podcast: see https://weisbart.com/agile-answers/"
      },
    
      "kickstarters-2019-04-02-kickstarter-trajectory-2019-light-html": {
        "title": "Kickstarter Trajectory 2019 Light Edition",
        "url": "/kickstarters/2019/04/02/Kickstarter-Trajectory-2019-Light.html",
        "image": "/img/kicks.png",
        "date": "02 Apr 2019",
        "category": "post, blog post, blog",
        "content": "IntroductionWe started this kickstarter trajectory with four kickstarters.Jago was freshly graduated from school, where as Giel and Yolan already had working experience in IT.Seppe had multiple years of working experience in Digital Signage but made a career change and was also new to IT.The main goals of the kickstarter course was to give every kickstarter a knowledge foundation of the best practices within JWorks and to introduce them to the IT world.First dayMorningOn the first day there, we were welcomed by Robbe Struys and Angela Gallo.They gave us the basic information about the HR working of Ordina.After receiving the keys to our new car and our laptop they showed us how to work with Ordina Connect.We made our first CV and filled in our first timesheet entry.They toured us around the office and introduced us to our future colleagues.They were very friendly and they all said that we made the right choice.This was of course very nice to hear and put us at ease.We had brunch together and then we had a group picture as well as our profile pictures taken.AfternoonWith every developer comes his/her personal development environment.To help us pick the best tools to suite our needs, we had help from Kevin Van den Abeele.He showed us the best IDEs for each language and best practices as to what we can do to improve our development experience.GitA tool all developers use is Version Control.At Ordina we prefer to use Git (this is preferred almost everywhere, who even uses SVN anymore?).So we learned to use Git, the best practices to get the best out of it and all this is done in the terminal of course.If you want to use a GUI for Git, they recommended GitKraken.Over the whole kickstarter traject, we would use Git to get our code examples and presentations.We went over good practices and learned by doing this hands-on on our own machines.Creating our own repositories, branching, merging, … .Yannick,our teacher for this course, was very clear to avoid spaghetti history by rebasing and squashing your commits to give a clean linear overview that is readable by your co-workers.DockerAs the era of containerization is rising, it only feels right to teach us the fundamentals about it and the importance of Docker in a project.That’s why they asked Tom Verelst to give us a detailed presentation about the mystical power of Docker.After the first introduction, we were soon ‘dockerizing’ our first full-stack application.We also combined everything together with Docker Compose, which made us start our whole full-stack application with just one command!The session gave us an overview as to how Docker is used in the real world, and we couldn’t wait to use an orchestration framework to deploy our containers into the cloud!DatabasesOn day 3, Tom Van den Bulck, Competence Lead in Big and Fast Data, gave us a course on SQL and NoSQL database systems.As some of us were not familiar with NoSQL, this was very interesting to see the difference in usage and possibilities between normal SQL systems which we were all used to using before.SQLFor SQL database systems we had a look at PostgreSQL, an open-source object-relational database management system that is increasing in popularity across bigger enterprises for reasons such as high scalability, extensive features and as it works cross-platform.NoSQL      Redis    Redis is an open-source key-value store that runs in-memory.Used where consistency and durability is less important than speed.        Cassandra    Cassandra is an open-source wide column store.Distributed across different nodes for high accessibility and low chance of downtime.        MongoDB  MongoDB is a document-oriented database system. Data in MongoDB does not need to be consistent and the data structure can change over time.      Neo4j  Neo4j is a graph database management system. No index is required and data with a lot of relations to other data can be accessed faster when dealing with higher amounts.Reactive programming with RxJSA course given by Orjan De Smet covering reactive programming, the advantages it brings and how and where to use it and how to use it in combination with unit testing.In short, reactive programming offers a solution to handling asynchronous calls with multiple events.Which means it offers more than one channel of communication so multi-step events can be handled efficiently.When coding in a traditional way you will often end up with a lot more code, could run into problems when for example a promise clogs a thread or you could end up with a mess of callbacks making your code extremely hard to read.DevOps and Continuous IntegrationAn introduction to DevOps &amp; CI given by Tim Vierbergen explaining this way of working and how it increases the productivity of a team.We also covered some best practices considering version control, building, testing and deploying with an example project to get a bit more familiar with the software used to do this.Software such as Git for version control, Jenkins for building, Jasmine for testing and Docker/Spinnaker for deploying.Security PrinciplesIn this presentation we went over the basics on how to protect your application and the user data it stores from malicious intent.We went over some good practices regarding the storage of data and the verification of your users.For example the hashing of passwords, enabling 2-factor authentication and deciding on the amount of allowed invalid login attempts before issuing a timeout.All of these things should be decided using a mix of guidelines and common sense.Clean CodePieter Van Hees gave us a course of clean code, this course was not focussed on writing new code but improving the way you write the code.Improvements:  Readability  Maintainability  Avoid rewritesThe biggest enemy of clean code is pressure, so Pieter advised us to take our time to write clean code.During this course we also did some exercises through public code katas available on the internet.This course only gave an introduction and he recommended us to read the book Clean Code by Robert Cecil Martin.Frontend Build tools, testing, package managers and moreThis course was led by Michael Vervloet, who is a full stack JavaScript/TypeScript developer at Ordina.He gave us the know-how on the building process, serving your application and doing this in an optimized way.He also showed us to use generators and build tools to create a whole lot of code and files in the terminal.The main topics of this course were Node.js, package managers and ways to build &amp; generate your code (gulp, webpack and Angular CLI).We went over them one by one and got the chance to test and install them on our machines to get a hands-on experience.In the end, we created an Angular application from scratch and played around with the generator to make some components and serving them to look at our work.Java Language FeaturesJava is a pretty popular language in the backend development world, and is our preferred backend language here at JWorks.That’s why Yannick De Turck explained us the newest features of Java versions 7, 8, 9, 10 and 11.Java 8 is currently the most used Java version.Yannick prepared some exercises for us so we could focus on the newest Java 8 features (lambdas, streams, optionals, …).One of the most useful features that Java 10 introduced is the ‘var’ keyword.How great is it that you don’t have to specify the type twice during the initialization of an object!?Java 11 is the newest LTS version, so it was important for us to get a detailed explanation about its newest changes and features.Other than that, there were a lot of extra useful features that will certainly be nice to have once we can use them.It was very entertaining to get a quick overview as to what is new, what is being removed or deprecated and what we can or should expect in the coming Java versions.Spring FrameworkFor a framework this big and popular, we followed a three-day course given by Ken Coenen.On the first day, we received a brief explanation as to how the Spring magic works behind the scenes (dependency injection, beans, …).We saw the basics of the most common components of the full Spring Framework such as Cloud, Security, … .On the second day, we dived into the magic behind Spring Boot.It’s remarkable how much Spring Boot does for you without any configuration needed, although you can fully configure Spring Boot to your needs and satisfactions.On the third day, Ken did a live coding session and created a Spring Boot application from scratch and explained how to fully initialize your Spring Boot project and get the most out of it through various steps and always showing the best practices for each implementation.Full House during the third dayOf course, afterwards we had some time to relax after three days of exploring the Spring Framework.We closed our three-day session on Friday with the best combination: pizza and beer!Pizza &amp; beer!Unit Testing and mocking in JavaWe got an introduction to Unit Testing in Java from Maarten Casteels.The red line:  Goals of Testing  What to test  Fixtures  Mocks  AssertionsIn the morning we got a very interactive theory session where we learned how important testing really is, the basics and what it all stands for.In the afternoon we learned to unit test our code, the best way to do this, how to mock dependencies, use fixtures and a whole lot more.Maarten also showed us the most common pitfalls to avoid, and some best practices like test-driven development (TDD) and how writing tests can help you with refactoring your code and lastly look at it with a different vision.For lunch we went to a place called Meals On Wheels were we were introduced to a whole other world of sandwiches.Once you’ve been there you will know what we mean by that, don’t go too often though.KubernetesKubernetes is an open source container orchestration framework which was first introduced to us by Tom Verelst during the kickstarter traject.It is made by Google and is now maintained by the Cloud Native Computing Foundation.First they introduced us to all the features that Kubernetes possesses (service discovery, horizontal scaling, load balancing, …).Soon we learned how to deploy Docker containers in the cloud by using Kubernetes, and afterwards we had an hands-on exercise where we could deploy a full-stack application to a Kubernetes cluster using Minikube.It’s wonderful how you can deploy a full-stack application through Kubernetes with just one configuration file needed.Of course, it takes some time to get used to it, but once you get the hang of it, you can do outstanding stuff with this platform!Cloud Providers &amp; PlatformsTo get a bigger picture of all the cloud providers and platforms that are out there conquering the IT world, we had a dedicated session about this topic given by Bas Moorkens and Dieter Hubau.Bas was focusing on Amazon Web Services and all its features that it has to offer.We quickly learned that AWS was very advanced and had lots of components to facilitate the life of a developer.It was a very interesting session and made me realise that AWS was a big part of the development scene.We are eager to use it and learn more of what is has to offer.As for cloud platforms, we got a very detailed explanation of how OpenShift (made by RedHat) works and what its features and options are.We also got a high-level explanation as to how an application in the cloud works and what the best practices are to achieve deploying your application in the cloud.Overall, it was a very interesting session for cloud-enthusiasts and we definitely want to learn more about it!TypeScriptAfter the session HTLML5, CSS3, JavaScript, Dimitri De Kerf learned us the benefits of TypeScript. He told us some benefits of using TypeScript instead of JavaScript.TypeScript is a wrapper around JavaScript, which means it has more methods to make your daily programming more pleasant.It also adds optional static typing for richer IDE autocomplete support.Dimitri De Kerf showed us how to configure our project to use TypeScript and to use these features.He explained us that it is important to know how to use TypeScript because it is used in popular frameworks like Angular and React.AngularRyan De Gruyter was our teacher for today.He quickly introduced us to Angular, a platform that is designed to easily create browser applications built by Google. The first version of Angular was AngularJS.It was very popular and used by many companies.Google decided to update Angular and created Angular 2 which was not welcomed by the industry at that time because it removed all the AngularJS concepts.It took some time for the industry to adapt and see the positive stuff of Angular 2: Open Source community, improved dependency injection, better performance, etc..Angular 2 is not the holy grail of frameworks. It still has some downsides like lots of ceremony and boilerplate thanks to the use of Angular CLI.After the information session, he showed us how easy it was to create an Angular project where we learned how to create an Angular application using small reusable pieces called components.Frontend hands-onJan De Wilde asked if we still had question about the Angular session. Because in this session we would create an Angular application using all the techniques we learned from the previous course and he wanted to be sure we understood everything before we started.So he went a bit deeper on some topics and showed us on how to execute calls to an API and to structure the project properly.After the lunch break, when we were still digesting our food, we started to write a complete Angular application. Jan De Wilde guided us through the process, showing us possible issues we could encounter and explained how we could solve those issues.Intro to Cloud-Friendly DevelopmentKevin Van Houtte introduced us to contract testing.It is a tool to write an exact input and output of an API call.After we run our project, our contract will generate tests for our controller, checking if the controller output is the same as we expected in the contract.The files, generated by the contract, can be imported into Javadoc for documentation.Afterwards we had some exercises where we could use all the skills we had learned in these courses.  API driven programming with contract tests.  Loading the API docs into our Java docs  Attaching a database to our Spring Boot application  Creating migration scripts and using these to populate the database with FlyWay  Creating a config server and connecting our Spring Boot application to it  Enabling actuator and using it to generate metrics dataAll these exercises help us prepare for a real project in the future.Agile DevelopmentTogether with Michaëla Broeckx, Practice Manager Agile, we saw different approaches to work as a non-agile team.Like the waterfall system that has some downsides such as getting late feedback from the business or end user.The feedback is only in the end of the life cycle of the project or when the project got tested.Applying an Agile approach offers a lot of benefits:  Quicker risk reduction  limit handovers  shorter term plans          to improve predictability, accuracy &amp; reliability.      to redone stress and unleash innovative emergent ideas        and so on!She proved her theory by doing a live exercise which involved folding paper airplanes as a team.At the end we would analyze the outcome.After this we learned some other Agile practices: we got introduced into the SCRUM framework and the practice of Extreme Programming, plus its benefits.The new JWorks colleagues  "
      },
    
      "iot-20smart-20tech-20smart-20glasses-20augmented-20reality-2019-04-01-vuzix-blade-html": {
        "title": "The Vuzix Blade",
        "url": "/iot,%20smart%20tech,%20smart%20glasses,%20augmented%20reality/2019/04/01/vuzix-blade.html",
        "image": "/img/2019-03-31-vuzix/banner.jpg",
        "date": "01 Apr 2019",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  The hardware  The software  Using the Vuzix  Developing for the Vuzix  Looking forward  ResourcesIntroductionAs we are strong believers about the impact that Augmented Reality will have in a business setting, we were eager to get started building things. But this required getting our hands on some actual hardware!We acquired some budget and went looking for ‘affordable’ smart glasses to experiment with.We were following the Vuzix for quite some time as they seemed market leader in wearable head-mounted technology. The type of devices they were producing didn’t have the coolness factor we were hoping for.And along came the Vuzix Blade… This looked like a real game changer. Something someone would actually wear on their face!    It was a video like this one that had won us over to give the Vuzix Blade a try since the displayed features look very nice, if they all worked as promised…After a long wait we finally got our hands on a pre-production hand-built Vuzix Blade and joined the Edge Developer Program.We got these glasses to analyze the wearers experience and see how we could integrate it into the numerous business cases:  Assistance for field technicians  Order picking  Communications platformsIn this blogpost we’ll go a bit into detail what makes the Vuzix Blade tick and how our experience with it has been so far.Read on ahead for all the juicy details!The hardwareThe Vuzix Blade is essentially an Android smartphone you can wear on your face.Well actually, it’s really more like an Android smartwatch you can strap to your face, but you get the idea.The device we received was a pre-production build, which was assembled by hand.This means we can’t really say much about what the final hardware will look like, if there will be any changes or if the build quality, which was very solid, will change.During our testing it has been through some light and heavy action: like daily commute use, office use, running, biking, etc, and still hasn’t shown any faults or cracks.We’ve always found the idea of computing devices in the form factor of glasses quite intriguing as some of us have been cursed with nearsightedness and already have to wear prescription glasses.If we have to wear the bloody things every day, might as well put some intelligence into them.Below you can find some specs about the device, but for us these are quite irrelevant for the moment.This device is all about showcasing innovation in two areas: form factor and display technology.And boy are we impressed.The glasses actually feel comfortable enough to wear for longer periods and the display technology is quite amazing! It’s nowhere near the HoloLens, but they serve a completely different purpose.Let’s get down the mandatory spec overview!The internals inside the glasses are alright, maybe a bit underwhelming.Knowing it’s always a fine line to balance between power consumption and battery life, the internals inside the glasses are alright, maybe a bit underwhelming.  Projected display resolution of 480 by 853 pixels  Quad core ARM A53 CPU  WiFi, Bluetooth  8MP camera up to 1080p video recording  470mAh batteryThe amount of RAM is not specified but seems to be just the right amount to get the job done.Overall the device works fine for normal apps but the speed and fluidity could be better for some high-end apps (1080p video capture, TensorFlow Lite, …)There is no audio on the device as no regular or bone conducting speaker is present.Audio can be provided through either Bluetooth or USB audio, but an included speaker would have been nicer.Initially the video recording only supported up to 720p at a lower frame rate, which with the lack of OIS was not very usable in high motion scenarios.However, the latest software update added support for 1080p recording and as you can see in one of the videos down below is actually acceptable.All of this is actually quite irrelevant to us.There is no innovation in fitting a better camera or having oodles of computing power on the device.The technological marvel in this device is the display technology, named the Cobra Display Engine.It’s difficult to explain how well this works.So we’ll just rip off the movie “Contact” and say:  No words to describe, they should have sent a poet.So beautiful! I had no idea.The best description we could think of so far is:It’s like someone is following you from behind with a projector and is projecting the user interface on an invisible screen in front of you.Hold a smartwatch right in front of you in a readable position.It’s kinda like that, but transparent and without losing the functionality of one of your arms.So instead of describing it to people, we just put it on their head and they were just immediately captivated by what they’re experiencing.It takes a moment to learn how to switch your eyes’ focus on the heads up display and back to your surroundings.Once you master this it becomes very natural to interact with the display, however staring at it for prolonged periods is not what’s it’s meant for.A lot of people have difficulty in wrapping their heads around the idea of that transparent interface when trying out the glasses for the first time.After this we show them some pretty pictures with a variety of colors.This really shows off the unexpectedly good visual qualities of the display and brings everything to life!For a concept device it really shows what the technology is capable of.We do hope that the final hardware specs will be a bit more beefy. Imagine running TensorFlow lite object detection in full force on the device. So many cool things we could do with computer vision!Adding a sim card slot and a GPS chip would also be awesome, since this would allow us to autonomously use the glasses without a companion smartphone. This would allow us completely sever the link to the companion smart phone.The softwareThe Vuzix Blade runs on Android 5.1.Due to the limited screen real estate of the device, the look and feel of the apps reminds us a lot of smartwatch apps.There aren’t many out-of-the-box apps on the device installed:  Welcome dashboard  Camera  Gallery  Music control  SettingsOne of the most important features of wearables is notification mirroring, which works out-of-the-box.With the Vuzix Blade also comes a companion app for your Android or iOS Smartphone.This companion app allows you to configure settings, fetch images and videos from the device, manage installed apps and explore the Blade app store.As this device doesn’t run the Google Play Store, a specific app store is needed.This app store allows Vuzix specific apps to be installed on the device.                                                                  Using the VuzixThe thing we like about the Blade is how comfortable it is to wear compared to other head-mounted wearable solutions like the HoloLens.The HoloLens is quite heavy and in our opinion not meant to be worn all day long. The Blade however is light enough to stay comfortable for long time wearing.Although Vuzix targets the Blade partially at the consumer market, we believe that there is much more potential in the enterprise market.Let’s hope they don’t make the same mistake Google made with Google Glass!But because they also target the consumer market, they thought about very important things like ergonomics and making it look appealing enough for non techies.                                                Our colleague, Frederick tested the device for a longer period of time:  Sometimes I wear this device for a full day to get deeply immersed in the experience.As it is comfortable to wear, this wasn’t much of an issue.  My first experiment was to check how many would look funny at me during my morning commute.The good news is that during my train ride and walk around the office, not many people were or kept staring at me.However, the people that knew me asked what I had on my face.    The interaction models are quite straightforward.It’s a good platform to consume push content.Your screen lights up, you get your info, the screen dims.If you want to actually interact with the app, you can use the touchpad located near your right temple.Using gestures like:  Swipes          Up      Down      Left      Right        Two finger swipes  Tap  Double tap  Long tap  etcAgain, very similar to smartwatches.Support for Amazon Alexa is currently in a Beta program for which we’ve signed up.Really wondering how natural this voice interaction will be!As we said before, some of us wear glasses and the Blade display is readable when you have only minor nearsightedness, but the display is much sharper when you put the Blade on top of your regular glasses.For an additional markup it is possible to get prescription lenses with the Blade so people who wear glasses on daily basis can also use this device.Battery life is very much inline with smart watches: it all depends on the usage.We can easily keep an app running with the screen on for almost two hours.If you are only consuming (push) notifications it’s possible to stretch this to a full day.For longer and more intensive usage an external battery pack is a must.Luckily it’s quite non-intrusive to equip a battery pack by using the USB port located on the side.Once you do this, battery life is not an issue anymore.We did some testing and actually went running and cycling while wearing an external battery pack and did not experience any hinder at all.Developing for the VuzixDeveloping for the Blade is just like developing for any Android device.You just develop in Android Studio, like you would normally do.This means Vuzix can leverage the huge amount of Android devs out there.Our Android devs found the learning curve to be relatively low.You do need to take into account that the Blade comes with its own design guidelines and UI components.The interaction model and how apps are structured is quite elegant and straightforward, no surprises here!Just import two Blade specific libraries with the components and you’re good to go.No other dependencies are needed!There is no Blade emulator available, but Vuzix has added support for the Android Studio design view.Although the layout of most screens will be very basic, it was still very handy to quickly prototype UIs.We brainstormed a bit about what would be a good app to leverage the innovative aspects of the Blade.As Frederick was recently training to regain his once athletic body, he bought a Polar H10 heart rate sensor which can connect to a smartphone using Bluetooth Low Energy (BLE).A lot of runners already have smartwatches to monitor their heart rate. Some of these watches even vibrate when you’re not running in the correct heart rate zone.More info on heart rate zones can be found here.A lot of runners already have smartwatches to monitor their heart rate. Some of these watches even vibrate when you’re not running in the correct heart rate zone.Although runners already have access to this information on their smart watch, it’s not the best form factor to consume the data. Ever tried reading your watch while running and bouncing around at 10+ km/h? Having to shift your focus like this just completely gets you out of “the zone”.We thought this was a good showcase of the capabilities of the Blade: easily consume the information you need, enabling you to make the best decisions, while being as non-intrusive as possible.Because Polar implements the official Heart Rate device specification it was very straightforward to set up a BLE connection between the sensor and the Blade.Every second or so the BLE device pushes an update of the current heart rate to the BLE client.After tapping into this stream of sensor data, it wasn’t too difficult to build the app.Currently we only display the current time, heart rate and heart rate zone.The video below showcases the app.The user interface is still very minimalistic and the app itself is still a work in progress.However, it’s already very functional.    The video doesn’t do the app justice as you don’t get to experience the transparent display, allowing you to see the world around you.Seeing those numbers and letters float in open air is always a joy to see.While experimenting with new technologies, we prefer to use the Minimal Viable Product (MVP) approach: focus on what brings most value and then validate this as soon as possible.This also means field testing the concept in the most representative and harsh environment you can think off.So, Frederick ventured forth to a place where not many developer dare venture: outdoor in the sun.Everyone who has ever worked with a laptop, smartphone or tablet outside can agree that the readability of these screen drops to zero as on bright and sunny days.The Blade solves this by having a very bright display. Apps also use the following two tricks to optimize readability:  Use high contrast colors, like green.  Dynamically make the transparent part of display white to increase the contrast even more.Frederick took the Blade on a 10km run to validate if the app was usable, readable and useful… And it sure was!Seeing your live heart rate gives you a lot of insight into your performance. It also turns everything into a game: Can I do better? How long can I keep going at this pace?The glasses were comfortable enough to wear for the full run. And most importantly: the batteries didn’t run out!All in all, it was such a positive experience that Frederick found it difficult to go running without these glasses.We have sent a demo to the people of Vuzix and they were also very enthusiastic about the concept.We will now polish the app a bit more to make it consumer friendly and then publish it to the Vuzix app store.In a future version of the app, we would like to add things like:  Average heart rate  Max heart rate  Calories burnedWith the latest software upgrade we can also tap into the GPS data from the smartphone via the companion app.This will allow us to also display things like: current speed, max speed, average speed, distance travelled, etc.It will be an interesting challenge getting all this data on the rather small display.This is something we will probably outsource to our UX / UI wizkids over at ClockWork.Looking forwardWhat we got with the Vuzix Blade looks already very promising even though there are a few small rough edges.Vuzix keeps rolling out significant software updates for the device that open up new possibilities.It is not unthinkable that devices like this will become common consumer electronics if you see what Focals by North are.But certain hurdles still have to be taken such as making them look even more appealing to everyday users.Focals by North is already taking a nice step in this direction. They however do this by sacrificing certain features (no camera, integrated touchpad, SD-card slot, …) which we think are important for enterprise user.As such, we still see the Vuzix Blade as best in breed.The better battery technology that is just around the corner could also prove to be a total game changer for wearable devices. Imagine your glasses as an all day long companion, augmenting all your senses and feeding you with on the spot contextual information.We do not believe this device will ever be direct competition to the Microsoft HoloLens as they serve 2 different purposes at a completely different price point.Would €500 be a good enough price point to appeal to the general public? Would this cost be low enough to have companies build positive business cases to equip their technicians with Augmented Reality / Assisted Reality devices?Let’s hope so!Resources  Vuzix Blade Smart Glasses  Vuzix product videos  Vuzix app store  Microsoft HoloLens 2"
      },
    
      "cloud-2019-03-28-building-with-google-cloud-build-html": {
        "title": "Building with Google Cloud Build",
        "url": "/cloud/2019/03/28/Building-With-Google-Cloud-Build.html",
        "image": "/img/2019-03-28-cloudbuild/cloudbuild.png",
        "date": "28 Mar 2019",
        "category": "post, blog post, blog",
        "content": "In this post, we will have a quick overview on what is possible with Google Cloud Build.Google Cloud Build is a fully managed solution for building containers or other artifacts.It can integrate with Google Storage, Cloud Source Repositories, GitHub and BitBucket.A simple YAML fileWe can easily set up a build pipeline using a YAML file which we store in our source code repository.Each build step is defined using a container image and passing arguments to it.Here is an example:steps:  # Test Helm templates  - name: 'quay.io/helmpack/chart-testing:v2.2.0'    id: 'Helm Lint'    args: ['ct', 'lint', '--all', '--chart-dirs', '/workspace/helm', '--validate-maintainers=false']  # Build image  - name: 'gcr.io/cloud-builders/docker'    id: 'Building image'    args: ['build', '-t', 'eu.gcr.io/$PROJECT_ID/cloud-build-demo:$COMMIT_SHA', '.']  # Create custom image tag and write to file /workspace/_TAG  - name: 'ubuntu'    id: 'Setup'    args: ['bash', '-c', \"echo `echo $BRANCH_NAME | sed 's,/,-,g' | awk '{print tolower($0)}'`_$(date -u +%Y%m%dT%H%M)_$SHORT_SHA &gt; _TAG; echo $(cat _TAG)\"]  # Tag image with custom tag  - name: 'gcr.io/cloud-builders/docker'    id: 'Tagging image'    entrypoint: '/bin/bash'    args: ['-c', \"docker tag eu.gcr.io/$PROJECT_ID/cloud-build-demo:$COMMIT_SHA eu.gcr.io/$PROJECT_ID/ms-map-report:$(cat _TAG)\"]images: ['eu.gcr.io/$PROJECT_ID/cloud-build-demo']timeout: 15moptions:  machineType: 'N1_HIGHCPU_8'We are free to use any image that we like.Cloud Build already provides a set of base images (called Cloud Builders),including images for Maven, Git, Docker, Bazel, npm, gcloud, kubectl, etc.We can also customise some build options like the timeout of the build,or on which kind of node the build runs.Pricing is done based on the amount of build minutes. However, if we use the default node, the first 120 build minutes are free every day!If the build finishes successfully,Cloud Build will automatically upload the built images to the container registry.This is based on the images defined in the images array.Data usually needs be shared between steps.We might want to download dependencies in one step,and build your artifact in another step,or run tests in a separate step.Google has provided a simple solution for this.Each build step has access to the /workspace folder, which is mounted on the container of each step.Each build has access to its own workspace folder,which is deleted automatically after the build finishes.In the above example, a custom Docker tag is created and saved to the /workspace/_TAG file,and then read from again in the next step.To start the build, we can use the gcloud builds submit command,or create an automatic trigger on the Google Cloud console that triggers the build on new commits in the Git repository.After adding a trigger, we can also trigger the build manually in the Google Cloud console.Build parameters (substitutions)It is possible to pass in parameters (called substitutions) to our build.We can override substitutions when submitting a build: $ gcloud builds submit --config=cloudbuild.yaml \\     --substitutions=TAG_NAME=\"test\"Cloud Build provides the following default substitutions:  $PROJECT_ID: build.ProjectId  $BUILD_ID: build.BuildId  $COMMIT_SHA: build.SourceProvenance.ResolvedRepoSource.Revision.CommitSha (only available for triggered builds)  $SHORT_SHA : The first seven characters of COMMIT_SHA (only available for triggered builds)  $REPO_NAME: build.Source.RepoSource.RepoName (only available for triggered builds)  $BRANCH_NAME: build.Source.RepoSource.Revision.BranchName (only available for triggered builds)  $TAG_NAME: build.Source.RepoSource.Revision.TagName (only available for triggered builds)  $REVISION_ID: build.SourceProvenance.ResolvedRepoSource.Revision.CommitSha (only available for triggered builds)We can use substitions to define our own custom parameters.Note that the name of the substitution must start with an underscore (_),and can only use uppercase alphanumeric characters. Example:substitutions:    _CUSTOM_PARAM_1: foo # default value    _CUSTOM_PARAM_2: bar # default valueimages: [    'gcr.io/$PROJECT_ID/myapp-${_CUSTOM_PARAM_1}',    'gcr.io/$PROJECT_ID/myapp-${_CUSTOM_PARAM_2}']Securing your buildIf we require to use credentials in our builds,it is possible to do this securely using Google Cloud Key Management Service (KMS).We will not go into how to use and to setup KMS,but once we have set it up,we can start encrypting our build secrets.First, we will need to give Cloud Build access to KMS by adding the Cloud KMS CryptoKey Decrypter roleto our ...@cloudbuild.gserviceaccount.com service account.Encrypt our secret with KMS:$ gcloud kms encrypt \\  --plaintext-file=secrets.json \\  --ciphertext-file=secrets.json.enc \\  --location=global \\  --keyring=[KEYRING-NAME] \\  --key=[KEY-NAME]This will create an encrypted file which we can add to our application’s source code.Using KMS, we can decrypt this secret in our Cloud Build pipeline:steps:- name: gcr.io/cloud-builders/gcloud  args:  - kms  - decrypt  - --ciphertext-file=secrets.json.enc  - --plaintext-file=secrets.json  - --location=global  - --keyring=[KEYRING-NAME]  - --key=[KEY-NAME]This will decrypt the secret into a file in our workspace folder,which then can be used in subsequent steps.Debugging and running your build locallyWhen creating a build pipeline, we do not need to keep pushing our code to the source repository to trigger a build.We can use the cloud-build-local tool to run our build locally,using the Google Cloud SDK and Docker.If we are using the Cloud Builder images (gcr.io/cloud-builders/...),we must first configure our Google Cloud SDK to be able to pull the images:# Configure Docker$ gcloud components install docker-credential-gcr$ docker-credential-gcr configure-dockerThen install the cloud-build-local tool:$ gcloud components install cloud-build-localNow we can use the tool to test our build pipeline locally!To build locally, we run the following command:$ cloud-build-local --config=[CONFIG FILE] \\  --dryrun=false \\  --push \\  [SOURCE_CODE]  CONFIG FILE is our Cloud Build YAML config file  SOURCE_CODE is the path to our source code  --dryrun=false will cause our build to actually run. This is true by default and we must enable this explicitly to cause the containers to execute.  --push will cause the built images defined in images to be pushed to the registry.If we use some of the default substitions like $COMMIT_SHA in our build,we must pass these in with the --substitions flag in key=value pairs,separated by commas.Example: $ cloud-build-local --config=cloud-build.yaml \\   --dryrun=false \\   --substitutions COMMIT_SHA=$(git rev-parse HEAD),BRANCH_NAME=$(git rev-parse  --abbrev-ref HEAD) \\    /path/to/sourceCloud Build stores intermediary artifacts in the workspace folder.This workspace folder, as mentioned before,will be removed after the build finishes.If we want to debug our build and check what happened in the workspace folder,then we can copy the artifacts to a path on our computer,using the --write-workspace flag.Note that this path must reside outside of our source folder!$ cloud-build-local --config=cloud-build.yaml \\   --dryrun=false \\   --write-workspace=/path/on/computer \\   /path/to/sourceBuild eventsIt is possible to trigger other actions when a build starts, finishes, or fails.Notifications to our team’s chat,triggering a deployment pipeline,monitoring our build. These are just a few examples.Cloud Build pushes build events to Pub/Sub on the cloud-builds topic.This topic is created automatically when Cloud Build is used.We can easily create a subscription on this topic. There are two kinds of subscriptions we can use.The first one is a push subscription, which pushes the message to a HTTP endpoint you define.In this case messages are delivered the moment the event is published on the topic.{  \"message\": {    \"attributes\": {      \"buildId\": \"abcd-efgh...\",      \"status\": \"SUCCESS\"    },    \"data\": \"SGVsbG8gQ2xvdWQgUHViL1N1YiEgSGVyZSBpcyBteSBtZXNzYWdlIQ==\",    \"message_id\": \"136969346945\"  },  \"subscription\": \"projects/myproject/subscriptions/mysubscription\"}Messages that are received using a pull subscription have the following format:{  \"receivedMessages\": [    {      \"ackId\": \"dQNNHlAbEGEIBERNK0EPKVgUWQYyODM2LwgRHFEZDDsLRk1SK...\",      \"message\": {        \"attributes\": {          \"buildId\": \"abcd-efgh-...\",          \"status\": \"SUCCESS\"        },        \"data\": \"SGVsbG8gQ2xvdWQgUHViL1N1YiEgSGVyZSBpcyBteSBtZXNzYWdlIQ==\",        \"messageId\": \"19917247034\"      }    }  ]}Each message contains the Base64 encoded event of the Build resource.Here is an example:{  \"id\": \"a0e322f2-5d8d-4d56-a2b5-05cc18a350af\",  \"projectId\": \"myproject\",  \"status\": \"SUCCESS\",  \"source\": {    \"repoSource\": {      \"projectId\": \"myproject\",      \"repoName\": \"mygitrepo\",      \"branchName\": \"feature/my-branch\"    }  },  \"steps\": [    {      \"name\": \"gcr.io/cloud-builders/mvn\",      \"args\": [        \"mvn\",        \"clean\",        \"--batch-mode\"      ],      \"id\": \"Clean\",      \"timing\": {        \"startTime\": \"2019-03-23T15:01:25.421160679Z\",        \"endTime\": \"2019-03-23T15:02:04.363792008Z\"      },      \"pullTiming\": {        \"startTime\": \"2019-03-23T15:01:25.421160679Z\",        \"endTime\": \"2019-03-23T15:01:59.834114283Z\"      },      \"status\": \"SUCCESS\"    },    ... More steps  ],  \"results\": {    \"images\": [      {        \"name\": \"eu.gcr.io/myproject/myapp:d76cce6d732e6edc01e65a547997caf107411468\",        \"digest\": \"sha256:0bb2f72d3d267c6bfebee8478d06dbf553d5932e01a0b86b7fc298c3a9b4a1f2\",        \"pushTiming\": {          \"startTime\": \"2019-03-23T15:15:58.377229824Z\",          \"endTime\": \"2019-03-23T15:16:01.908997933Z\"        }      }    ],    \"buildStepImages\": [      \"\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:d30ca59f3315232f539955a6179f2b287445ec56db41e7d7a41a622c9faee575\",      \"sha256:d30ca59f3315232f539955a6179f2b287445ec56db41e7d7a41a622c9faee575\",      \"sha256:d30ca59f3315232f539955a6179f2b287445ec56db41e7d7a41a622c9faee575\"    ],    \"buildStepOutputs\": []  },  \"createTime\": \"2019-03-23T15:01:16.591984806Z\",  \"startTime\": \"2019-03-23T15:01:17.438509785Z\",  \"finishTime\": \"2019-03-23T15:16:02.968224Z\",  \"timeout\": \"1800s\",  \"images\": [    \"eu.gcr.io/myproject/myapp:d76cce6d732e6edc01e65a547997caf107411468\"  ],  \"artifacts\": {    \"images\": [      \"eu.gcr.io/myproject/myapp:d76cce6d732e6edc01e65a547997caf107411468\"    ]  },  \"logsBucket\": \"gs://199957373521.cloudbuild-logs.googleusercontent.com\",  \"sourceProvenance\": {    \"resolvedRepoSource\": {      \"projectId\": \"mateco-map\",      \"repoName\": \"bitbucket_matecocloud_myapp\",      \"commitSha\": \"d76cce6d732e6edc01e65a547997caf107411468\"    }  },  \"buildTriggerId\": \"9bd093c7-9de4-4eae-bfea-ce8e46afafa8\",  \"options\": {    \"substitutionOption\": \"ALLOW_LOOSE\",    \"logging\": \"LEGACY\"  },  \"logUrl\": \"https://console.cloud.google.com/gcr/builds/a0e322f2-5c8d-4e56-a2b5-05cc18a350af?project=199957373521\",  \"substitutions\": {    \"_MOD_BRANCH_NAME\": \"$_tmpvar\"  },  \"tags\": [    \"event-f2d96d7b-22f5-41d7-9ded-a98a2a6f43ca\",    \"trigger-9bd093c7-9de4-4eae-bfea-ce8e46afafa8\"  ],  \"timing\": {    \"BUILD\": {      \"startTime\": \"2019-03-23T15:01:25.421114358Z\",      \"endTime\": \"2019-03-23T15:15:58.377209942Z\"    },    \"FETCHSOURCE\": {      \"startTime\": \"2019-03-23T15:01:20.519103589Z\",      \"endTime\": \"2019-03-23T15:01:25.368505523Z\"    },    \"PUSH\": {      \"startTime\": \"2019-03-23T15:15:58.377226850Z\",      \"endTime\": \"2019-03-23T15:16:01.909032379Z\"    }  }}Note that Cloud Build does not publish events between steps, but only when the build is queued, starts or ends.            Event      Build status                  The build is queued      QUEUED              The build starts      WORKING              The build is successful      SUCCESS              Build is cancelled      CANCELLED              Build times out      TIMEOUT              Step times out      TIMEOUT              Build failed      FAILURE              Internal error by Google Cloud Build      INTERNAL_ERROR      Using Google Cloud Function, we can easily trigger other actions based on these build events.Here is a small, redacted snippet of a Google Cloud Functionwhich sends build updates to a Slack webhook.It receives the build event, reads the Base64 encoded data,converts it into a Slack message and triggers the webhook with the created message.const IncomingWebhook = require('@slack/client').IncomingWebhook;const SLACK_WEBHOOK_URL = \"https://hooks.slack.com/services/XXXXXXXXXXXXXX\";const WEBHOOK = new IncomingWebHook(SLACK_WEBHOOK_URL);// Main function called by Cloud Functions.module.exports.cloudBuildSlack = (event, callback) =&gt; {    const build = eventToBuild(event.data.data);    WEBHOOK.send(createSlackMessage(build), callback);};    const createSlackMessage = (build) =&gt; {    const app = getApplicationName(build);    const branch = build.source.repoSource.branchName;    const subject = createSubject(build);    const tag = getImagetag(build);    return {        attachments: [{            fallback: `${subject} - ${app} - ${branch} - &lt;${build.logUrl}|Logs&gt;`,            title: subject,            title_link: build.logUrl,            fields: getFields(app, branch, tag),            color: getMessageColor(build)        }],        mrkdwn: true    };};// eventToBuild transforms pubsub event message to a build object.const eventToBuild = (data) =&gt; {    return JSON.parse(new Buffer(data, 'base64').toString());};...more functions    ConclusionCloud Build offers a simple solution and utilises the power of containers to offer a lot of possibilities.A build pipeline is set up in a few minutes, and your Docker images are uploaded automatically!It saves you a lot of time and trouble in setting up build infrastructure, because, well, you do not have to!If you wish to try it yourself,we have provided a demo application on GitHub.Enjoy Cloud Building!"
      },
    
      "streaming-2019-03-25-streaming-traffic-data-html": {
        "title": "Streaming Traffic Data with Spring Kafka &amp; Apache Storm",
        "url": "/streaming/2019/03/25/streaming-traffic-data.html",
        "image": "/img/2018-08-08-streaming-traffic-data/traffic.png",
        "date": "25 Mar 2019",
        "category": "post, blog post, blog",
        "content": "  Earlier I did a workshop at Ordina in order to introduce my colleagues to the wonderful world of stream processing.For that workshop I used traffic data, since especially in Belgium, traffic data is something everybody can easily relate to as we all have to endure it every workday.Table of content  Introduction  The Data  Native Java Stream Processing  Kafka Streams with Spring Kafka  Apache Storm  ConclusionIntroductionIn this blog post we will use traffic data made available by the Flemish government.Several examples will be provided about how this data can be processed in various ways:  Transform the data into events with Spring Cloud Stream  Do some stream processing using some plain old Java, the native way  Process these events with Kafka Streams via Spring Kafka  Do similar processing with Apache StormThe DataThe traffic data is registered on fixed sensors installed in the road itself.General information about the sensors can be retrieved from http://miv.opendata.belfla.be/miv/configuratie/xml.    &lt;meetpunt unieke_id=\"3640\"&gt;        &lt;beschrijvende_id&gt;H291L10&lt;/beschrijvende_id&gt;        &lt;volledige_naam&gt;Parking Kruibeke&lt;/volledige_naam&gt;        &lt;Ident_8&gt;A0140002&lt;/Ident_8&gt;        &lt;lve_nr&gt;437&lt;/lve_nr&gt;        &lt;Kmp_Rsys&gt;94,695&lt;/Kmp_Rsys&gt;        &lt;Rijstrook&gt;R10&lt;/Rijstrook&gt;        &lt;X_coord_EPSG_31370&gt;144477,0917&lt;/X_coord_EPSG_31370&gt;        &lt;Y_coord_EPSG_31370&gt;208290,6237&lt;/Y_coord_EPSG_31370&gt;        &lt;lengtegraad_EPSG_4326&gt;4,289767347&lt;/lengtegraad_EPSG_4326&gt;        &lt;breedtegraad_EPSG_4326&gt;51,18458196&lt;/breedtegraad_EPSG_4326&gt;    &lt;/meetpunt&gt;It is pretty static as these sensors do not tend to move themselves.Every minute the latest sensor output is published on http://miv.opendata.belfla.be/miv/verkeersdata.This is one big XML file containing all the aggregated data of every sensor of the last minute.    &lt;meetpunt beschrijvende_id=\"H211L10\" unieke_id=\"1152\"&gt;        &lt;lve_nr&gt;177&lt;/lve_nr&gt;        &lt;tijd_waarneming&gt;2017-11-20T16:08:00+01:00&lt;/tijd_waarneming&gt;        &lt;tijd_laatst_gewijzigd&gt;2017-11-20T16:09:28+01:00&lt;/tijd_laatst_gewijzigd&gt;        &lt;actueel_publicatie&gt;1&lt;/actueel_publicatie&gt;        &lt;beschikbaar&gt;1&lt;/beschikbaar&gt;        &lt;defect&gt;0&lt;/defect&gt;        &lt;geldig&gt;0&lt;/geldig&gt;        &lt;meetdata klasse_id=\"1\"&gt;            &lt;verkeersintensiteit&gt;0&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;0&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;252&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"2\"&gt;            &lt;verkeersintensiteit&gt;6&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;116&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;113&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"3\"&gt;            &lt;verkeersintensiteit&gt;1&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;118&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;118&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"4\"&gt;            &lt;verkeersintensiteit&gt;3&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;84&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;84&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"5\"&gt;            &lt;verkeersintensiteit&gt;5&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;84&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;84&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;rekendata&gt;            &lt;bezettingsgraad&gt;9&lt;/bezettingsgraad&gt;            &lt;beschikbaarheidsgraad&gt;100&lt;/beschikbaarheidsgraad&gt;            &lt;onrustigheid&gt;366&lt;/onrustigheid&gt;        &lt;/rekendata&gt;For more information (in Dutch) about this dataset you can go to https://data.gov.be/nl/dataset/7a4c24dc-d3db-460a-b73b-cf748ecb25dc.Over there you will also find the XSD files describing the XML structure.Transform to EventsSince I am using Spring Boot to kickstart the application, you can go to https://start.spring.io/ to get started.Some handy baseline dependencies to get started are: Web, Actuator and DevTools.Because the data is provided in a single XML file, we will transform it into separate events per sensor.This brings it also inline with how true sensory events would arrive within our system if we would not be dealing with a big XML file.A small Spring Cloud Stream application will be built to read in the XML, transform it to events and push these events to a Kafka topic.You might wonder, why would we use Spring Cloud Stream for this?It makes it very easy to read/write messages to Kafka with it.Add the appropriate starter:    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;    &lt;/dependency&gt;Define a Spring Boot application - make sure to enable scheduling.    @SpringBootApplication    @EnableScheduling    @EnableBinding({Channels.class})    public class OpenDataTrafficApplication {        public static void main(String[] args) {            SpringApplication.run(OpenDataTrafficApplication.class, args);        }    }Define some input and output topics.    public interface Channels {        @Input        SubscribableChannel trafficEvents();        @Output        MessageChannel trafficEventsOutput();        @Output        MessageChannel sensorDataOutput();    }Create a bean to read in the events.    public List&lt;TrafficEvent&gt; readInData() throws Exception {        log.info(\"Will read in data from \" + url);        JAXBContext jc = JAXBContext.newInstance(\"generated.traffic\");        Unmarshaller um = jc.createUnmarshaller();        Miv miv = (Miv) um.unmarshal(new URL(url).openStream());        log.info(\" This data is from \" + miv.getTijdPublicatie().toGregorianCalendar().getTime());        List&lt;TrafficEvent&gt; trafficEventList = convertXmlToDomain.trafficMeasurements(miv.getMeetpunt());        lastReadInDate = miv.getTijdPublicatie().toGregorianCalendar().getTime();        log.info(\"retrieved {} events \", trafficEventList.size()) ;        return trafficEventList;    }Next we will retrieve the data out of the XML and split it out into something more event like.For every sensor point per vehicle we will extract one TrafficEvent.    @Data    public class TrafficEvent {        private VehicleClass vehicleClass;        private Integer trafficIntensity;        private Integer vehicleSpeedCalculated;        private Integer vehicleSpeedHarmonical;        private String sensorId;        private String sensorDescriptiveId;        private Integer lveNumber;        private Date timeRegistration;        private Date lastUpdated;        /*        actueel_publicatie: 1 = data is less then 3 minutes old.         */        private Boolean recentData;        /*         Indicate if the sensor (meetPunt) was available when trying to retrieve the data        */        private Boolean available;        private Integer sensorDefect;        private Integer sensorValid;    }The VehicleClass is just an enum with the vehicle type.    MOTO(1),    CAR(2),    VAN(3),    RIGGID_LORRIES(4),    TRUCK_OR_BUS(5),    UNKNOWN(0);We will also retrieve the detailed sensor information from the XML containing the sensor descriptions.    @Data    public class SensorData {        private String uniekeId;        /*        MeetpuntId        */        private Integer sensorId;        /*        Meetpunt beschrijvende Id         */        private String sensorDescriptiveId;        private String name;        /*        Unique road number.            More info in the dataset of numbered roads in the \"Wegenregister\" (Roads registry), field: locatieide,            http://opendata.vlaanderen.be/dataset/wegenregister-15-09-2016            Or the dataset \"De beheersegmenten van de genummerde wegen\" by AWV, field ident8,            http://www.geopunt.be/catalogus/datasetfolder/12b65bc0-8c71-447a-8285-3334ca1769d8        */        private String ident8;        /*        Reference to the lane of the measurement point.          The character indicates the lane type.            R: Regular lane            B: Bus lane or similar            TR: measurement of the traffic in the opposite direction (p.e. in or near tunnels) on the corresponding R-lane.            P: Hard shoulder lane            W: parking or other road            S: Lane for hard shoulder running            A: Hatched area          Counting starts at R10 for the first regular lane of the main road. Lane numbers increase from right/slower to left/faster lanes.          Lanes 09, 08, 07, ... are positioned right of this first lane, and mainly indicate access/merging lanes, deceleration lanes, recently added lanes, lanes for hard shoulder running, bus lanes          Lanes 11, 12, 13, ... are positioned left of lane R10.          The lane number 00 is used for measurement points on the hard shoulder (P00).          The TR-lane is identical to the corresponding R-lane (TR10=R10,TR11=R11,TR12=R12,...), but returns the data of the \"ghost traffic\" instead.          (The data for TR10 and R10 are provided by the same detection loops.)         */        private String trafficLane;    }Write these events to a topic.    public void sendMessage(TrafficEvent trafficEvent) {        outputChannels.trafficEvents().send(MessageBuilder.withPayload(trafficEvent).build());        log.info(\"Send message to the trafficEventOutput channel\");        outputChannels.trafficEventsOutput().send(MessageBuilder.withPayload(trafficEvent).build());    }    public void sendSensorData(SensorData sensorData) {        outputChannels.sensorDataOutput().send(MessageBuilder.withPayload(sensorData).build());    }The events will be sent to Kafka as JSON messages.With the @Scheduled annotation Spring Boot will read in the events every 60 seconds.    @Scheduled(fixedRate = 60000)    public void run() throws Exception {        putAllEventsInKafka();    }When you are taking your data in, it is important to decide what you want to send in.You do not want to remove too much information nor do you want the events becoming too bloated.Meaning, that they contain too much information and you needing to spend a lot of time extracting information when analysing your data.Keep them as close to the actual event as possible, only adding in data if this is required.In our current example the sensor location does not need to be part of the traffic events as it is pretty static.If in your situation, you have another data entry where your sensor specific data changes every few events, it might be worthwhile to add it to your event when taking it in.So that later on you do not have to spend time joining that data together.Sometimes your intake data is also too large, it is not wrong to ignore certain properties when taking in data in your stream.In our case we ignore a lot of the properties within the XML, as they do not serve our example.Having less properties to analyze can make your life easier, but if that raw data is no longer available you have lost that information for good.Be wise with what you remove as time travel is not something we can code in, ignored data is lost forever.Takeaways  Think in events  Keep the data structure as flat as possible  Do not optimize your data too soonNative Java Stream ProcessingDo not forgetDo not forget that you can also process your events in native Java.You will not have a lot of fancy features available but it might get the job done.Especially when you take into consideration the extra cost involved in introducing a streaming framework.For both Kafka and Storm you not only need to set up a cluster of the framework itself, but also of Zookeeper.That setup does not come for free and will need to be maintained in the future.Easy to get startedWith Spring Cloud Stream it is easy to start processing your stream of data in native Java.First define a SubscribableChannel.    @Input    SubscribableChannel trafficEvents();Then you will need to define a MessageHandler which will describe what you will do with every message you process.    MessageHandler messageHandler = (message -&gt; {            log.info(\"retrieved message with header \" + message.getHeaders().toString());            log.info(\"retrieved message \" + message.getPayload().toString());            TrafficEvent event = (TrafficEvent) message.getPayload();            log.info(\" the sensor id is \" + event.getSensorId());            if (event.getTrafficIntensity() &gt; 0) {                log.info(\"We now have {} vehicles on the road {}\", event.getTrafficIntensity(), event.getSensorId());                int vehicleCountForEvent = event.getTrafficIntensity();                if (vehicleCount.get(event.getSensorId()) != null) {                    vehicleCountForEvent += vehicleCount.get(event.getSensorId());                }                log.info(\"We now had total: {} vehicles\", vehicleCountForEvent);                vehicleCount.put(event.getSensorId(), vehicleCountForEvent);            }            if (event.getVehicleSpeedCalculated() &gt; 0) {                if (lowestWithTraffic.get(event.getSensorId()) == null || lowestWithTraffic.get(event.getSensorId()).getVehicleSpeedCalculated() &gt; event.getVehicleSpeedCalculated()) {                    lowestWithTraffic.put(event.getSensorId(), event);                }                if (highestWithTraffic.get(event.getSensorId()) == null || highestWithTraffic.get(event.getSensorId()).getVehicleSpeedCalculated() &lt; event.getVehicleSpeedCalculated()) {                    highestWithTraffic.put(event.getSensorId(), event);                }                messages.add(event);            }        });Finally, link that MessageHandler to an InputChannel.    inputChannels.trafficEvents().subscribe(messageHandler);There you go, you are now processing your stream of data in native Java.It does become obvious that doing something more fancy, like windowing and aggregation, will require you to write all of that logic yourself.This can get out of hand pretty quickly, so do watch out for that.But for simple data processing, nothing beats some native Java.Takeaways Native Java  Can easily handle 1000 events per second  Easy to get started  You will lack advanced features like windowing, aggregation, …Kafka Streams with Spring KafkaKafkaSpring Kafka allows us to easily make use of Apache Kafka.Kafka is designed to handle large streams of data.Messages are published into topics and can be stored for mere minutes or indefinitely.It is highly scalable allowing topics to be distributed over multiple brokers.Kafka Streams allows us to write stream processing applications within the Kafka cluster itself.For this reason, Kafka Streams will use topics for both input and output allowing it to store intermediate results within Kafka itself.What “topics” does Kafka Streams useKStreamA KStream records a stream of key/value pairs and can be defined from one or more topics.It does not matter if a key exists multiple times within the KStream, when you read in the data of a KStream every record will be sent to you.KTableA KTable is a changelog stream of a primary keyed table, meaning that whenever a key exists multiple times within the KTable you will receive only the most recent record.GlobalKTableLike a KTable, but it is replicated over all Kafka Streams instances, so do be careful.KGroupedStreamThis is an intermediate format based on a regrouped stream of records based on a KStream, with usually, a different key than the original primary key.It is derived from a groupBy() or a groupByKey() on a KStream.Via aggregate(), count() or reduce() it can be converted to a KTable.KGroupedTableThis is pretty similar to a KGroupedStream, but a KGroupedTable is derived from a KTable via groupBy().It can be reconverted to a KTable via aggregate(), count() or reduce().Coding with Spring KafkaWe still have the Spring Cloud Stream topics to which we send in some data.Let’s use these but now using Kafka.First we are going to take in the static data of the sensors into a KTable.    KStream&lt;String, SensorData&gt; sensorDescriptionsStream =        streamsBuilder.stream(\"sensorDataOutput\", Consumed.with(Serdes.String(), new SensorDataSerde()));    KStream&lt;String, SensorData&gt; sensorDescriptionsWithKey =        sensorDescriptionsStream.selectKey((key, value) -&gt; value.getUniekeId());    sensorDescriptionsWithKey.to(\"dummy-topic\");    KTable&lt;String, SensorData&gt; sensorDataKTable =        streamsBuilder.table(\"dummy-topic\", Consumed.with(Serdes.String(), new SensorDataSerde()));The main reason we are using a KTable is that it makes it easy to be sure to only get the most recent state of that sensor, as a KTable will only return one result per key.dummy-topic is just the name I chose.For my example it is not that important to have a well defined topic name.But do realize that Kafka Streams will persist the state of a Ktable within Kafka topics.Subsequently we are going to enrich the traffic event with the sensor data.    KStream&lt;String, TrafficEvent&gt; stream =            streamsBuilder.stream(\"trafficEventsOutput\", Consumed.with(Serdes.String()                    , new TrafficEventSerde()));    stream.selectKey((key,value) -&gt; value.getSensorId())            .join(sensorDataKTable,((TrafficEvent trafficEvent, SensorData sensorData) -&gt; {                trafficEvent.setSensorData(sensorData);                return trafficEvent;            }), Joined.with(Serdes.String(), new TrafficEventSerde(), null))            .to(\"enriched-trafficEventsOutput\");Resulting in a new KStream with enriched TrafficEvents.The .stream(String topic, Consumed&lt;K,V&gt; consumed) will consume all entries from a topic and transform these into a stream. Mapping these to topic records with a key and a value.In our case the key is just a string, while the body of the topic will be a JSON message which gets converted into a TrafficEvent.With join(), full definition:    &lt;VT, VR&gt; KStream&lt;K, VR&gt; join(final KTable&lt;K, VT&gt; table,         final ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner,         final Joined&lt;K, V, VT&gt; joined);We join our KTable with our TrafficEvent records using the ValueJoiner we pass along which will result in a new Joined result.The ValueJoiner is just a function in which we indicate what needs to be done with both records the function receives. In our case a TrafficEvent and a SensorData.The Joined describes the new record structure we will write towards Kafka using .to(String topic) sending the newly generated records to that Kafka topic.Once this stream has started, it will continue processing these events whenever a new record is inserted into the intake topic.For some of our further processing we do not care for all traffic events, so let’s filter out some.    KStream&lt;String, TrafficEvent&gt; streamToProcessData =         streamsBuilder.stream(\"enriched-trafficEventsOutput\", Consumed.with(Serdes.String(), new TrafficEventSerde()));    streamToProcessData.selectKey((key,value) -&gt; value.getSensorId())        .filter((key, value) -&gt; canProcessSensor(key));Filtering happens on the key of the records, so first we will use selectKey() passing along a KeyMapper to map to the new key.The KeyMapper is a function to which you pass along the field which you want to become the new key.    private boolean canProcessSensor(String key) {        return this.sensorIdsToProcess.contains(key);    }Then we will use filter() to filter out the keys we want to retain which match the given Predicate.In our case the predicate just verifies if a key appears within a List:For every record we will now do some simple processing with updateStats():    streamToProcessData        .selectKey((key,value) -&gt; value.getSensorId())        .filter((key, value) -&gt; canProcessSensor(key))        .foreach((key, value) -&gt; updateStats(value));The updateStats() method just updates some basic counters to track how much traffic has been processed since we started with the data intake to a hashtable.So that we know how many vehicles have passed, the highest speed detected, …WindowingIn an ideal world all events arrive in a perfect and timely fashion within our Kafka system.In an ideal world we can also process all the events we want to process.In the real world however, this does not compute.Events tend to arrive out of order and too late.If you want to get a count of all the vehicles which ran over your road network from 21:00 to 21:05 but one of your sensors sends its events too late, the count you have generated will not be correct.Windowing allows you to mitigate these risk by  Limiting the scope of your stream processing  Allowing you to catch some “late” events within a windowFor adding windows you use .windowedBy, in this example we define a window of 5 minutes which gets every 10 minutes.Then you will need to aggregate the results per window with .aggregate.Do not forget to provide the correct Materialized parameters so Kafka knows what type of key and value is used as input by the aggregation.    private void createWindowStreamForAverageSpeedPerSensor(KStream&lt;String, TrafficEvent&gt; streamToProcessData) {        Initializer initializer = () -&gt; new SensorCount();        streamToProcessData            .groupByKey()            .windowedBy(TimeWindows.of(300000).advanceBy(60000))            .aggregate(initializer, (key, value, aggregate) -&gt; aggregate.addValue(value.getVehicleSpeedCalculated()),                    Materialized.with(Serdes.String(), new JsonSerde&lt;&gt;(SensorCount.class)))                    .mapValues(SensorCount::average, Materialized.with(new WindowedSerde&lt;&gt;(Serdes.String()), Serdes.Double()))                    .toStream()                    .map(((key, average) -&gt; new KeyValue&lt;&gt;(key.key(), average)))                    .through(\"average-speed-per-sensor\", Produced.with(Serdes.String(), Serdes.Double()))                    .foreach((key, average) -&gt; log.info((String.format(\" =======&gt; average speed for the sensor %s is now %s\", key, average))));    }    streamToProcessData.filter((key, value) -&gt; canProcessSensor(key))                .selectKey((key,value) -&gt; value.getSensorData().getName().replaceAll(\"\\\\s\",\"\").replaceAll(\"-\", \"\"))        .to(\"traffic-per-lane\");    KStream&lt;String, TrafficEvent&gt; streamPerHighwayLaneToProcess =             streamsBuilder.stream(\"traffic-per-lane\", Consumed.with(Serdes.String(), new TrafficEventSerde()));    this.createWindowStreamForAverageSpeedPerHighwaySection(streamPerHighwayLaneToProcess);Takeaways Kafka Streams and Spring Kafka  When you have a Kafka cluster lying around, using Kafka Streams is a no-brainer  Excellent support within Spring  Easy to get started  Using the Kafka Streams DSL feels quite naturalApache StormTwitterIt was first created at Twitter who open sourced it as an Apache Project.One of the first streaming frameworks that got widely adopted.Spouts &amp; Bolts  When you work with Storm you need to think in Spouts, Bolts and Tuples.A Spout is the origin of your streams.It will read in Tuples from an external source and can be either reliable or unreliable.Reliable just means that when something goes wrong within your stream processing, the spout can replay the Tuple.While an unreliable spout will go for the good old fire-and-forget approach.Spouts can also emmit to more than one stream.Spouts will generate Tuples, the main data structure within Storm.A Tuple is a named list of values, where a value can be of any type.It is however important that Storm will serialize all the values within a Tuple, so for a more exotic type you will need to implement a serializer yourself.Bolts do all the processing of your streams.A Bolt can send out to more then 1 stream.It is also possible to define a Stream Grouping on your Bolts allowing you to tailor the distribution of your workload over the various Bolts of your Storm topology.Multiple instances of a Bolt will run as tasks.You have the following Stream Groupings:  Shuffle Grouping: completely random  Fields Grouping: based on the value of certain fields, Storm will make sure that all the Tuples with the same “key” will be processed by the same Bolt, handy for word counts for example - great business value  Partial Key Grouping: pretty similar to fields grouping, but with some extra load balancing  All grouping: the entire stream will go to all the tasks of a Bolt, use this with care  None Grouping: implies that you don’t care how it gets processed - which corresponds with a shuffle grouping  Direct Grouping: here the producer of the Tuple will decide which task of the Bolt will receive the Tuple for processing  Local or Shuffle Grouping: this will also take a look at the worker processes running the Bolt’s tasks, this in order to make the flow somewhat more efficient.Now let’s get started with some code.First take in some necessary dependencies:    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-core&lt;/artifactId&gt;        &lt;version&gt;1.2.2&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-kafka-client&lt;/artifactId&gt;        &lt;version&gt;1.2.2&lt;/version&gt;    &lt;/dependency&gt;The idea is to get to a Storm topology with one Spout and two Bolts.    final TopologyBuilder tp = new TopologyBuilder();        tp.setSpout(\"kafka_spout\", new KafkaSpout&lt;&gt;(spoutConfig), 1).setDebug(false);        tp.setBolt(\"trafficEvent_Bolt\", new TrafficEventBolt(sensorIdsToProcess)).setDebug(false)                .globalGrouping(\"kafka_spout\");        tp.setBolt(\"updateTrafficEventStats_bolt\", new TrafficCountBolt()).setDebug(true)                .fieldsGrouping(\"trafficEvent_Bolt\", new Fields(\"sensorId\"));        return tp.createTopology();First we will define a KafkaSpout which will take in the data of a Kafka topic.    protected KafkaSpoutConfig&lt;String, String&gt; getKafkaSpoutConfig(String bootstrapServers) {        ByTopicRecordTranslator&lt;String, String&gt; trans = new ByTopicRecordTranslator&lt;&gt;(                (r) -&gt; new Values(r.topic(), r.partition(), r.offset(), r.key(), r.value()),                new Fields(\"topic\", \"partition\", \"offset\", \"key\", \"value\"));        trans.forTopic(\"trafficEventsOutput\",                (r) -&gt; new Values(r.topic(), r.partition(), r.offset(), r.key(), r.value()),                new Fields(\"topic\", \"partition\", \"offset\", \"key\", \"value\"));        return KafkaSpoutConfig.builder(bootstrapServers, new String[]{\"trafficEventsOutput\"})                .setProp(ConsumerConfig.GROUP_ID_CONFIG, \"kafkaSpoutTestGroup\")                .setRetry(getRetryService())                .setRecordTranslator(trans)                .setOffsetCommitPeriodMs(10_000)                .setFirstPollOffsetStrategy(EARLIEST)                .setMaxUncommittedOffsets(1050)                .build();    }For completeness this is the retryService which just handles some retrying whenever your Kafka cluster is behaving naughty:    protected KafkaSpoutRetryService getRetryService() {            return new KafkaSpoutRetryExponentialBackoff(KafkaSpoutRetryExponentialBackoff.TimeInterval.microSeconds(500),                    KafkaSpoutRetryExponentialBackoff.TimeInterval.milliSeconds(2), Integer.MAX_VALUE, KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(10));    }Then we will emmit that data to a TrafficEventBolt which will filter out the events we want to process further.    public class TrafficEventBolt extends BaseRichBolt {        private OutputCollector collector;        private final List&lt;String&gt; sensorIds;        public TrafficEventBolt(List&lt;String&gt; sensorIdsToProcess) {            this.sensorIds = sensorIdsToProcess;        }        @Override        public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {            this.collector = outputCollector;        }        @Override        public void execute(Tuple input) {            log.info(\"input = [\" + input + \"]\");            input.getValues();            TrafficEvent trafficEvent = new Gson().fromJson((String)input.getValueByField(\"value\"), TrafficEvent.class);            if (sensorIds.contains(trafficEvent.getSensorId())) {                collector.emit(input, new Values(trafficEvent.getSensorId(), trafficEvent.getVehicleSpeedCalculated(), trafficEvent.getTrafficIntensity()));            } else {                collector.ack(input);            }        }        @Override        public void declareOutputFields(OutputFieldsDeclarer declarer) {            declarer.declare(new Fields(\"sensorId\", \"speed\", \"trafficIntensity\"));        }    }Finally we will send out the tuples to a TrafficCountBolt which will gather some general statistics.    public class TrafficCountBolt extends BaseRichBolt {        private OutputCollector collector;        private final HashMap&lt;String, Integer&gt; countPerSensors = new HashMap&lt;&gt;();        @Override        public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {            this.collector = outputCollector;        }        @Override        public void execute(Tuple input) {            log.info(\"input = [\" + input + \"]\");            Integer count = countPerSensors.get((String)input.getValueByField(\"sensorId\"));            if (count == null) {                count = 0;            }            count = count+ (Integer) input.getValueByField(\"trafficIntensity\");            countPerSensors.put(input.getString(0), count);            collector.emit(new Values(input.getString(0), count));            collector.ack(input);        }        @Override        public void declareOutputFields(OutputFieldsDeclarer declarer) {            declarer.declare(new Fields(\"sensorId\", \"count\"));        }    }WindowingStorm also knows about the concept of windowing.    public class CountPerSensorIdBolt extends BaseWindowedBolt {            private OutputCollector collector;        private final HashMap&lt;String, Integer&gt; countPerSensors = new HashMap&lt;&gt;();        @Override        public void execute(TupleWindow tupleWindow) {            for (Tuple input : tupleWindow.get()) {                Integer count = countPerSensors.get((String)input.getValueByField(\"sensorId\"));                if (count == null) {                    count = 0;                }                count = count+ (Integer) input.getValueByField(\"trafficIntensity\");                countPerSensors.put(input.getString(0), count);                collector.emit(new Values(input.getString(0), count));                collector.ack(input);            }        }    }Subsequently you can define this bolt within a topology at which moment you will also define the size or duration of the window:In this example we are just using windows with a fixed duration of five seconds.    private StormTopology getTopologyKafkaSpout(KafkaSpoutConfig&lt;String, String&gt; spoutConfig) {        final TopologyBuilder tp = new TopologyBuilder();        tp.setSpout(\"kafka_spout\", new KafkaSpout&lt;&gt;(spoutConfig), 1).setDebug(false);        tp.setBolt(\"trafficEvent_Bolt\", new TrafficEventBolt(sensorIdsToProcess)).setDebug(false)                .globalGrouping(\"kafka_spout\");        tp.setBolt(\"updateTrafficEventStats_bolt\", new TrafficCountBolt()).setDebug(true)                .fieldsGrouping(\"trafficEvent_Bolt\", new Fields(\"sensorId\"));        tp.setBolt(\"windowedProcessBolt\", new CountPerSensorIdBolt().withWindow(BaseWindowedBolt.Duration.seconds(5)))                .setDebug(true)                .globalGrouping(\"trafficEvent_Bolt\");        return tp.createTopology();    }You can also pass in an extra parameter slidingInterval to define a sliding window.    withWindow(Duration windowLength, Duration slidingInterval)Both the windowLength and the slidingInterval can also be represented by a Count, which will base the window duration on the amount of tuples being processed.Either determining the length of the window by the tuples, or when to slide.    withWindow(Count windowLength, Duration slidingInterval)    withWindow(Duration windowLength, Count slidingInterval)Even tumbling windows are possible:    withTumblingWindow(BaseWindowedBolt.Count count)    withTumblingWindow(BaseWindowedBolt.Duration duration)Please note that a tuple belongs to only one of the tumbling windows, while with a sliding window it is very much possible that a single tuple is processed within multiple windows.Stream APIThe Storm Streams API is pretty new.It tends to provide a DSL which corresponds more with other streaming DSLs, making your data processing feel more natural and less clunky, as compared to be thinking in spouts and bolts.In the background it will convert the DSL to spouts and bolts though, so knowing how Storm works internally is still pretty important.Takeaways Apache Storm  It is pretty mature  Low latency / high throughput  It does tend to feel pretty clunky thinking in Spouts and Bolts - for a developer it is not that big of a hassle, but for a data scientist I can imagine that at times it will be harder to wrap your head around itConclusionIn order to get started with basic stream processing you do not need Kafka or Apache Storm, native Java is good enough for you to take your very first steps when processing a stream of data.It is easy, everybody understands it and you will have less moving parts within your software landscape which can cause issues.Using a dedicated streaming platform will become necessary when you want to do more advanced streaming operations or when performance becomes more and more important.The existing platforms can easily scale up to the processing of thousands of messages a second, something which is going to be much harder to achieve when building your solution yourself.Do not make the mistake of re-inventing the wheel by writing your own streaming platform, others have done that hard work for you.Kafka Streams is a no-brainer to use when you have a Kafka cluster lying around, stream processing there feels natural and it is easy to get going.If however you do not have a Kafka cluster available, it will come with an extra cost of setting it up and maintaining it.There do exist managed solutions in order to make your life easier.Apache Storm is a pretty robust framework which has been around for some time and is used by many.However, writing the processing logic feels quite clunky and I can imagine that for a non-developer it also might feel quite unnatural. They are currently working on a new streaming API which should alleviate that issue though.According to their GitHub, a release of version 2.0 has already happened, but their website does not reflect it yet.When doing stream processing always think about how messages will be handled as most streaming or messaging platforms use an at-least-once approach, meaning that the same message can be processed more than once by the streaming pipeline. Both Kafka Streams and Apache Storm can be configured to provide exactly-once processing within their streaming pipelines.For Kafka Streams it means using Kafka transactions while for Storm this can be achieved by Trident.Even then, it is only within the streaming pipeline itself meaning that as soon as your processed results leave the streaming platform, you will be back to at-least-once guarantees."
      },
    
      "conference-2019-03-05-ddd-europe-html": {
        "title": "DDD Europe 2019",
        "url": "/conference/2019/03/05/ddd-europe.html",
        "image": "/img/2019-02-16-ddd-europe/ddd-europe.jpg",
        "date": "05 Mar 2019",
        "category": "post, blog post, blog",
        "content": "  This year, Pieter Van Hees and Kristof Eekhaut attended the Domain-Driven Design Europe  conference in Amsterdam.The conference was all about Domain-Driven Design and related topics, with loads of interesting talks from beginners and experts in their field.In this post you can read about some of the talks and workshops we attended.Table of content  When we lose sight of our domain by Carola Lilienthal  Make your tests tell the story of your domain by Anne Landro and Mads Opheim  Domain modelling towards First Principles by Cyrille Martaire  Collaborative Modelling hands on session by Marijn Huizendveld  Lost in transaction? Strategies to manage consistency across boundaries by Bernd Ruecker  Estimates or No Estimates, Let’s explore the possibilities by Woody ZuillWhen we lose sight of our domain by Carola LilienthalCarola discusses nine traps that developers fall into, and which prevent us to focus on the important aspect of developing software, the domain.Trap 1: Model monopoly  “In order for developers to learn about the domain, they have to talk to the users, in a language that the users understand.”The first thing to understand is that developers need to talk to the users, because if they don’t they will lose a lot of information. However, in a lot of companies, it is the analyst alone who talks to users when he/she gathers requirements.By having one or more analysts who communicate with users, they have the monopoly of the domain.When developers do communicate with users, they should do so in a language and/or model that the users understand.Sharing class diagrams or database models with users is counterproductive.The users will not understand this complex model and think it took a lot of effort to create.As a consequence they either cannot give relevant feedback because they don’t understand it, or they won’t dare to because they don’t want to discourage you.A better way to communicate the model between users and developers is to use e.g. a schema with icons and descriptive names for actions.Trap 2: Only look at the future without taking into account the presentLook at how they are working today instead of only looking what you want to achieve in the future.Ask yourself: “Who is doing what wherewith and what for?”Avoid using requirements documents without concrete examples.Trap 3: Forget about reuse in your domainFirst think about something being usable, and then see if it can be reused.The Don’t Repeat Yourself (DRY) principle should not be applied rigorously and blindly. If you apply DRY too often and too soon it often leads to leaky abstractions.Trap 4: Don’t try to be too generic, DDD is about being as concrete as possibleBy being concrete in your domain and your code you will have explicit and understandable code.Trap 5: if your components are too dependent on each other, you cannot scale them independentlyHigh coupling between components prevents you from splitting them into different services that could scale separately.Another disadvantage of high coupling is that it becomes difficult to let you software evolve, because changes in one component force changes in others.Trap 6: Large business classesFor example when modeling containers that move through different stages in a harbour.The large business class could be the container that manages all stages the container goes through.It would be better to model these stage as separate components.This is called functional decomposition.  “Don’t create big business classes that serve everybody.”Trap 7: How do we know what to buildHow do we split a big elephant into pieces?Let’s say we have four different types of elephants in our business domain.A common mistake would be to split elephant by different parts of the body. Where one component would be all four types of feet, another would be all four types of heads, etc.This might not necessarily be the best approach to split the four elephant types.The better approach would be to build one small elephant that is fully functioning, and then let it grow each iteration.This approach lets you learn from each iteration and allows for incremental growth and refactoring.Trap 8: The expert trapThe people who developed the elephant will start to think they are experts, and know everything there is to know about the elephant, because they built it from scratch.This assumption is false, because even the developers who built the elephant from scratch have assumptions, and assumptions can be false.The real experts are, and will remain, the users.Trap 9: Everything is new, and therefore betterPeople tend to believe that this new system they are building will be way better than the old system they’re replacing, because it looks better.What they forget is that the users know the old system very well and are often very productive in it.When the users will start to use the new system, they will feel like beginners again.They will be less productive than with the old application, at least for a little while.Make your tests tell the story of your domain by Anne Landro and Mads OpheimAnne and Mads tell us how they drastically reformed the testing approach for the Norwegian Court Case Management system from constant repetitive manual verification to automated testing.They explain that Value Chain tests have helped their team document the domain:PersonIsRegisteredAsDeadAfterStartOfACaseOfDeath {   createACaseOfDeath()   registerTheDiseased()   registerTheHeirs()   notifyTheNationalRegistrtyOfTheDeath()   assertThatThePersionIsRegisteredAsDead()}Each of these tests runs through a workflow of the domain and verifies the state at the end of it. They are high level tests that can be understood by all stakeholders, so that anyone - including domain experts and users - can look at a test and verify whether the result is what they expect.From a  quick glance at this code you can learn a lot about how the domain works. Their team also uses this technique to document special cases that they discover in the domain, so that bugs caused by these quirks don’t happen again.Domain modelling towards First Principles by Cyrille MartraireIn this great talk Cyrille explains us why he thinks that the Domain-Driven mind set of most teams is “too gentile” and we aim to “raise the waterline”.With DDD we learned to immerse ourselves in the domain, use our domain-driven skills to understand the domain and conceptualise the domain into conceptual models. But we should go further by defining theories for our models and spot the First Principles that the theory consists of. Then we can challenge them, so that we can suggest changes to the business instead of reproducing the domain as it is. This way we get more involved and get to the next step, which is:  Innovation!He points out a number of common problems that many teams have and suggest how we can improve them:The Human Compiler effectOne thing we often see is that we are given requirements piece by piece: the first sprint we get one case, then the next sprint another case and so on.But most of the time it turns out that all of these cases are actually special cases of some general case that we haven’t been told about.The reason for this is what Cyrill calls the Human Compiler effect:  someone behaving like a compiler, by taking the general problem and splitting it up and dumbing it down in separate simple solutions for every single consequence, so that a developer can implement them.This is obviously a bad approach, because by dumbing down the domain for the developers, we keep them “dumb” and unaware of how the domain actually works. This leads to a dumb - and often wrong - implementation of the domain.We should instead first describe the problem to the entire team. Then the team should build a theory upon it and challenge it by asking critical questions (Why? Why? Why? …).This leads to a better understanding of the domain and thus to building better solutions.Technical complexityOn the other hand, sometimes we are given an explanation about a problem and some developers turn it in something even more laborious.This increases technical depth and make the code unmaintainable.The solution for this: refactoring and using Test-Driven Development.Theory vs Residual MessWhen we start creating theories about the regular world of our domain, often someone from the teams asks: “But what about ALL the other business rules?”. We have an obsession for the “big bag of business rules”. As if every business is a bunch of data with a bunch of if-statements on top.We should realize that there is always some order in this mess and that a lot of things are more regular that irregular. We have to find out these regularities, find out the theories behind them and then we can create our domain model.Of course any business also has irregularities that do not fit into our theories and we can not just ignore these. We call this the Residual Mess.However we should not allow this mess to affect our beautiful theories. Instead we should - as Eric Evans explained before - define a Spill Zone in the application where we can put all the messy parts of the application.ConclusionCyrill advices us to:  Raise the waterline  Expect untold regularities  Practice TDD  Practice DDD  Build theories, not just lists of business rules  Learn to think based on First Principles. Disrupt and become innovative!Collaborative Modelling hands-on session by Marijn HuizendveldWe are divided in groups of five with one team leader.The goal: to model an application for the maintenance team of a car rental company in Amsterdam.The application must determine when a car is due and available for maintenance.New requirements are provided step by step on “requirement” cards, so that we have to adapt and reshape our model each time we discover a bit more about the domain. We learn the importance of visualising the solution (model) and talking about the problem based on what we have visualized in front of us. Putting notes on the board with the different concepts that we identify, sparks interesting discussions that make use think further about the problem:  Is the given name correct and clear?  Do we mean the same thing when we talk about …?  Do two words on the board actually mean the same thing?After each requirement card follows a card for the team leader to consider making changes to the way we work.One card tells the leader to look for someone who has been a bit quiet or outside of the discussion and move the group around so that he is next to the board.This immediately make this person more involved in the discussion and we also start paying attention to his view.Another card suggests to let someone go through the entire process that is modelled on the board and explain it step by step.We immediately find out that some definitions on the board are hard to explain and not as clear as we thought they were.With this excellent workshop Marijn shows us how easy it can be to come up with a great domain model that is understood and agreed upon by everyone involved.Lost in transaction? Strategies to manage consistency across boundaries by Bernd RueckerIn this talk Bernd explains the challenges we face when using transactions in big applications and distributed systems.He starts by reminding us that our Aggregates in DDD are usually our transactional/consistency boundaries. Meaning that within an Aggregate, you have an ACID transaction.If you were to have a transaction over multiple Aggregates, you would have a stronger coupling between them.For example you can’t split them easily into multiple separate microservices.What you could do is use two-phase commits to have you transaction over multiple Aggregates in separate services.But the problem is that two-phase commits don’t scale.  Grown ups don’t use distributed transactionsAn alternative solution is the alternative to ACID: BASE.  Basically  Available  Soft-state  Eventual consistencyBy applying Eventual Consistency you update one aggregate in one transaction and the other in a different transaction.This means that the system will be in an inconsistent state for a short time, but eventually it will be consistent.After that, Bernd explains different strategies how to implement this eventual consistency with an example.Let’s say that we have an credit card payment aggregate that charges a credit card aggregate, and that this communication happens through an asynchronous message. This communication can go wrong in multiple ways: the message might never arrive at the credit card service, it might arrive but the payment service doesn’t receive the feedback, etc.Strategy 1: CleanupIf the payment service can’t send the message, or if it doesn’t receive feedback that the message was received, it can send a payment failed event.The problem with this strategy is that this ‘payment failed’ event also might not arrive at the credit card service. Which means that it won’t be able to do his cleanup.Strategy 2: Keep stateStateful retryBy using a stateful retry the payment service would keep the state of whether or not the message was delivered to the credit card service.As long as the credit card service does not acknowledge that it processed the message, the payment service will keep sending the message.Stateful retry and cleanupWith this strategy the payment service keeps retrying to send the message until a timeout has passed or after X retries.After that it will send a payment failed event for which the retry policy might also apply.Strategy 3: Compensation/SagasChoreographyCompensation means that if something in the asynchronous process fails, a compensating process will be triggered.A classic example is a system where you book a hotel in one service which will trigger a car booking.If the car booking fails, it will emit an event that will be picked up by the hotel service which will cancel the hotel room related to the car booking.This system of services responding on events from each other is called orchestration. We don’t define in one place how the whole process works, but services know themselves on what to react on.However, this compensation saga implemented with choreography might become complex because it could be a trainwreck of cascading cancellations.E.g. a hotel booking triggers a car booking, which triggers a flight booking.If you have complex processes with a lot of services involved, this might become chaotic.OrchestrationBy using an orchestration approach there would be one service responsible for managing the whole process.In the hotel/car/flight example a trip service could be this orchestrating service that calls the other services and tells them to book or cancel.Bernd then argues that if you choose an orchestration strategy that BPMN tools and libraries can help a lot in defining these processes.You could for example define your business process and all compensating activities.Some libraries even provide quite nice DSLs where you can make your business process quite explicit.And the good thing is that this business process or saga is even part of your domain logic.Estimates or No Estimates, Let’s explore the possibilities by Woody ZuillWoody starts by pointing out that his workshop does not give answers, but does ask critical questions.His goal is to share some experiences he had, and he realizes that what works in some companies, does not work in others.After this disclaimer he talks about a big project he worked on where they experienced sprint after sprint that their estimates were always plain wrong.Every retrospective this frustration was mentioned and every time the solution management came up with was that they just had to get better at making estimations.  “Trying the same thing over and over again expecting different results is the definition of insanity” - EinsteinIn fact, Woody said, a constant in his 35 year career was that estimations were always off, and people were always trying to solve this by “getting better at estimates”.Wrong estimates are often not the problem itself, but a symptom of something else.It could be that they are off because the requirements were unclear, or that requirements keep changing.#NoEstimates#NoEstimates was originally used to refer to reference a blog post Woody had written on a project where they did not use or make estimates.But actually ‘No estimates’ is a placeholder for a larger conversation.Woody mentions that for some things in life we want estimates, but we never do because we know it’s impossible.E.g. how long will this clinical trial take?How long till we find a cure for cancer?How long till you finish this work of art?In fact if we have enough data to definitively say how long something will take to develop, we already built it and we don’t need to do it again.Next he asked the audience to explain in a single word what an estimate means. Quite some different answers were given, but in general it came down to this list:  Guess  Expectation  Lies  Misunderstanding  ApproximationFrom these answers the following working definition could be extracted:  An estimate is a guess of the amount of work time to create a project, a feature or some bit of work in developing software.Why do we estimate?Some reasons why we make estimates:  Planning / budget  Which approach do we choose / in what order do we do things  Dependencies on other teamsIn software development, estimates are often used to attempt to predict the future.When will it be done?How much will it cost?What can we get done this sprint?What can we get done for this amount of money?Basically we use estimates to help us make decisions.If we have to choose between making project A or project B, we would make an estimation of how long it would take to do either of them.But do we really want to choose between project A or B based on a guess?Wouldn’t it be better to do an MVP of both and see which is working best?Is on time or on budget a good measure of the results of our decision?No, because you cut features, make it unmaintainable, etc.Isn’t it better to measure customer satisfaction as a metric for success?ConclusionAfter this workshop the audience was left with even more questions.But what we did realize is that often people make estimations without any good reason. And sometimes it would be better to reflect on why we do estimations, and see if it really provides us value, and if there is no alternative solution for the problem we’re actually trying to solve with estimations.Summing it all upDomain-Driven Design Europe was a great conference where we got to learn more about software design and techniques that help us do what we love to do the most: creating great software for users.The organizers did an excellent job in creating a conference with great speakers.Next year’s conference will take place in Amsterdam on the 6th and 7th of February 2020."
      },
    
      "frontend-2019-03-04-vue-with-typescript-html": {
        "title": "Vue with TypeScript",
        "url": "/frontend/2019/03/04/vue-with-typescript.html",
        "image": "/img/vue-with-typescript/vue-plus-typescript.png",
        "date": "04 Mar 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Vue with TypeScript, an introduction  Creating a Vue project with TypeScript  A look into the files created by the Vue CLI  How to write your first component  Using your first plugin  Your first deployment  Conclusion  Resources and further reading1. Vue with TypeScript, an introductionWith the release of the Vue CLI 3 in August 2018, Vue now officially supports development in TypeScript.In September 2018, Evan You even announced that the next version of Vue will be rewritten in TypeScript.This does not mean that you are forced to use TypeScript, it will still be an option.TypeScript has numerous advantages such as static typing and transpiling of the latest ECMAScript features for full compatibility with older browsers.Especially the static typing is a very interesting feature for projects in a professional environment as it helps define more strict interfaces.With the use of types, you inherently provide documentation to other developers on how to use your code as it offers guidance on how to use your functions, components and so on.In this tutorial we will make a really simple blog system to showcase how you create a project, create a component, install a plugin and do calls via HTTP.Our little project will be called wordvue.At the same time we will explain some tips and tricks and give some background information about Vue with TypeScript so that you fully understand what the purpose is of each line of code.The project can be found in a GitHub repository so you can see a working version.  2. Creating a Vue project with TypeScriptUsing the Vue CLIThanks to the Vue CLI, it is the easiest way to create a new Vue project.First make sure you have the latest version of the CLI installed with NPM:$ npm i -g @vue/cliAfter that we create our project:$ vue create wordvueThe CLI knows some presets but we will go through the manual mode to be sure we select the TypeScript version.At the time of writing the current default language for Vue is JavaScript.  Babel + TypeScript  We check the TypeScript option and for the purpose of this article we will not look in detail at the other features of this screen and fall back on the default values.I also checked CSS Pre-processors just because I like SCSS.Make sure you have Babel selected, Babel will automatically add multiple polyfills.The polyfills will help with having backwards compatibility of ECMAScript features.Using the class-style component syntax  In the next screen we will get the question if we want class-style component syntax, for which we answer Yes.With this, we actually install the decorators that can be found in the Vue Class Component package.We will now explain the difference between using the class-style component syntax and using the classic Vue syntax.The classic Vue SyntaxIf you do not use the class-style component syntax, your components will look exactly as if you have rendered them with Vue with JavaScript, but with the addition of types:import Vue, { VNode } from 'vue'export const HelloComponent = Vue.extend({    data () {        return {            message: 'Hello',        }    },    methods: {        greet (): string {            return this.message + ' world';        }    },    computed: {        greeting(): string {            return this.greet() + '!';        }    },    render (createElement): VNode {        return createElement('div', this.greeting);    }});As you can see, you’ll still have the data function, the methods, computed properties and the render function that you can use in regular Vue.Class-style component syntaxWith the class-style component syntax, we would write the same component like this:import Vue from 'vue'import Component from 'vue-class-component'@Component({    template: '&lt;div&gt;&lt;/div&gt;'})export default class HelloComponent extends Vue {    message: string = 'Hello'    greet(): string {        return this.message + ' world';    }    get greeting() {        return this.greet() + '!';    }}When you compare it to the previous version, the data property message is now a regular property in our component class. Methods are present as class methods. And the computed properties can be defined as a getter.The other settingsAfter you’ve replied Yes to the “Use class-style component syntax?” question, you can continue with the default options.For the CSS pre-processor, you either choose between Stylus, Less and SCSS. We choose the default Sass/SCSS (with node-sass).  As for the linter, you can either choose between TSLint or ESLint with a bunch of configurations.I opt for the default TSLint option as the support for TypeScript in ESLint is (at the time of writing) fairly recent.But ESLint is certainly a valid option as the TypeScript has announced in their January to June 2019 roadmap that ESLint will be their own focus.  We can choose for the Lint on saveoption as we want to see immediately the effects of our linter.  Finally we have to choose if we want the configurations in dedicated files or all bundled together in our package.json.We opt for In dedicated config files as we prefer to not clutter the package.json with a lot of configurations.  The Vue CLI will now create the project with a Git repository, perform an NPM install and generate a README.  Launching your first Vue projectAfter the Vue CLI has created your Vue project, you can go with the terminal to the root folder of the project and launch it with:$ npm run serve  As you will see, the CLI starts the development server, starts the type checking and linting service.By default the project runs on http://localhost:8080 but if there’s already something running on port 8080, it will pick port 8081 or the next one available.This way you don’t need to specify a free port.A default Vue project looks like this:  3. A look into the files created by the Vue CLI  The files that the Vue CLI generated are mainly all the configuration files that we wanted separately.So we have a configuration file for Babel with babel.config.js, PostCSS (which contains the configuration for SCSS) with postcss.config.js, TypeScript with tsconfig.json and TSLint with tslint.json.You will also find a node_modules folder for all your NPM packages with a package.json in which we define all the NPM packages that we need in our project.If we would have opted for the In package.json option, we would have had a large package.json file.The main folders in which you will work are public and src. We will look at these more in detail later so you fully understand what their purpose is.PublicPublic is meant for static assets like images, favicons and more.It also contains your index.html which is very basic:&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\t&lt;meta charset=\"utf-8\"&gt;\t&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\t&lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0\"&gt;\t&lt;link rel=\"icon\" href=\"&lt;%= BASE_URL %&gt;favicon.ico\"&gt;\t&lt;title&gt;wordvue&lt;/title&gt;&lt;/head&gt;&lt;body&gt;\t&lt;noscript&gt;\t\t&lt;strong&gt;We're sorry but wordvue doesn't work properly without JavaScript enabled. Please enable it to continue.&lt;/strong&gt;\t&lt;/noscript&gt;\t&lt;div id=\"app\"&gt;&lt;/div&gt;\t&lt;!-- built files will be auto injected --&gt;&lt;/body&gt;&lt;/html&gt;It is only in rare cases that you should adapt the index.html.One example situation would be to add Google Analytics, add more meta tags or adapt the title tag.Vue will automatically inject the necessary generated JavaScript files right before the closing body tag.This will include the transpiled version of your own code as well as vendor code.The most important thing is the div tag with id app.This should always be present in your index.html as this is the tag on which Vue will bootstrap the entire application.Srcmain.tsimport Vue from 'vue';import App from './App.vue';Vue.config.productionTip = false;new Vue({\trender: (h) =&gt; h(App),}).$mount('#app');When you look in the src folder, you will find a main.ts file.This is the one that Vue will execute first.As you can see this creates a new instance of Vue in which we only define a render function.Vue will pass along h which is of type CreateElement.h has been chosen by the creator of Vue as it is short for Hyperscript, a term that is used in several virtual DOM implementations.A hyperscript is a script that will generate HTML structures.It takes one parameter: App.Thus in a Vue project, h will generate the HTML for our App component.App is our main component that was generated by the Vue CLI and we will dive into that after this section.main.ts should only be adapted to plug in new core functionalities of your application.For example, a main.ts of one of my own projects is this:import Vue, { CreateElement, VNode } from 'vue';import App from './App.vue';import i18n from './i18n';import './registerServiceWorker';import router from './router';import store from './store';Vue.config.productionTip = false;new Vue({    router, // custom router configuration    store, // custom store implementation    i18n, // my translations    render: (h: CreateElement): VNode =&gt; h(App),}).$mount('#app');As you can see, I have added three core functionalities: a router, a store and an i18n library.In each component that you make, these functionalities will be available.The reason why these will be available is because the Vue type gets extended by each of these libraries.For example in the typings of VueI18n (the i18n library that I use), we find:declare module 'vue/types/vue' {\tinterface Vue {\t\treadonly $i18n: VueI18n &amp; IVueI18n;\t\t$t: typeof VueI18n.prototype.t;\t\t$tc: typeof VueI18n.prototype.tc;\t\t$te: typeof VueI18n.prototype.te;\t\t$d: typeof VueI18n.prototype.d;\t\t$n: typeof VueI18n.prototype.n;\t}}This means that we will have a $i18n property available and five different functions.If you would use a different i18n library, you will have most of these things also readily available.For example vue-i18next defines $i18n as:declare module \"vue/types/vue\" {    interface Vue {        readonly $i18n: VueI18Next;        $t: TranslationFunction;    }}Vue itself does not provide an i18n implementation nor a store nor a router nor does it even support HTTP calls by default.Vue is designed to be as light as possible so that developers can keep a project as lightweight as possible.Vue does officially support specific NPM packages for these core functionalities.Other packages will follow the same naming conventions as the official supported libraries for convenience sake.Like said before we will only focus on the basic Vue functionalities and HTTP calls.The other topics will be for a future article.App.vueThe App.vue file is our first component that Vue bootstraps through our main.ts file.It is considered to be the root component.The Vue CLI generates the App component with one child component.&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;img alt=\"Vue logo\" src=\"./assets/logo.png\"&gt;\t\t&lt;HelloWorld msg=\"Welcome to Your Vue.js + TypeScript App\"/&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import HelloWorld from './components/HelloWorld.vue';@Component({\tcomponents: {\t\tHelloWorld,\t},})export default class App extends Vue {}&lt;/script&gt;&lt;style&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;Each page and part of a page that you will create, is a component that is a child of the root component.Together, your whole application should have a component structure which should look like a tree:App\t- HomePage\t\t- HelloWorld\t- NewsPage\t\t- NewsArticle\t\t\t- Reaction\t- ContactPageEach node of the tree is a component.With the use of the @Component(...) decorator, we define which components can be child components of the component that we’re defining.For example in our App component, we want the HelloWorld component (through the HelloWorld tag), thus we add the components option with HelloWorld in there.These components are local components.If you would want to write a component that is global, you have to register it like this:Vue.component('my-component-name', {\t// ... options ...})A global component can be accessed anywhere.Try to avoid this as much as possible as it fills up the global namespace.An example of a use case that is justified would be an icon library like Font Awesome:library.add(    faBars,    ...    faCameraRetro,);Vue.component('font-awesome-icon', FontAwesomeIcon);After this we can access the font-awesome-icon tag from everywhere.&lt;font-awesome-icon icon=\"arrow-down\" /&gt;4. How to write your first componentWe will keep the project as simple as possible for now.Firstly I will explain the basics of a Vue Component so that you will fully understand what happens when we write our first real component.The structure of a .vue file&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t...\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import HelloWorld from './components/HelloWorld.vue';@Component({\tcomponents: {\t\tHelloWorld,\t},})export default class App extends Vue {}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\t...}&lt;/style&gt;The standard way to write a Vue component is by using the .vue file extension.In a .vue file, we define three optional tags: template, script and style.According to the Vue documentation, you should always order the tags consistently with style being the last one.&lt;template&gt;…&lt;/template&gt;This is the visual part of your component, in here you define the HTML that will be used to display your component.Note that your custom HTML should always be surrounded by a div tag.The reason for this is that it allows Vue to encapsulate your custom CSS without unknowingly affecting the styling of your whole site.You can use this to add a custom id or class to the tag to help you identify the component in for example your e2e tests.Note that a .vue file can contain at most one template tag.&lt;script lang=”ts”&gt;…&lt;/script&gt;In the script tag, you can add your custom TypeScript code.The lang attribute is not required but if you do not add it, the default language will be JavaScript.In order for TypeScript to be available, you need to add lang=\"ts\".All of our TypeScript code should be present in this script tag, even the import statements.Note that a .vue file can contain at most one script tag.&lt;style&gt;…&lt;/style&gt;In the style tag we can define our own SCSS specific for this component.By default, all the styles you define in a style tag are global.By adding the scoped attribute to our style tag, our custom SCSS will be specific for that component.&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;p&gt;Hello World!&lt;/p&gt;\t\t&lt;ChildComponent&gt;&lt;/ChildComponent&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;...&lt;/script&gt;&lt;style scoped&gt;#app {\tp {\t\ttext-style: italic;\t}}&lt;/style&gt;&lt;style&gt;#app {\tp {\t\tcolor: red;\t}}&lt;/style&gt;In the example above, the style tag with the scoped attribute will only affect the p tag in our component and not the one in our child component.The color: red styling however will affect also the styling of p tags in ChildComponent.So it is best to be aware of the implications as it can have unwanted side effects.For a good way to structure your CSS, check out the BEM methodology as it will help with avoiding conflicts and will guide you in having clean CSS selectors.Be careful though as the scoped attribute leads to a certain performance hit.Simply by adding the same class or id that we used in the template, we avoid this performance hit and still can have some scoped SCSS.Note that this will also style the child components.A .vue file can contain more than one style tag.If you want to use global styles, you can either put them in App.vue or create your own CSS file that you import either directly in index.html or in App.vue.Should you put everything in a .vue file?For small components, a .vue file will be very interesting as you have all the elements that make up your component into one specific file.But what if you have for example lots of lines in the template tag?Or what if you just want to split up the file?One tactic you can use is the src attribute on the template, script and/or style tags.&lt;template src=\"./mycomponent.html\"&gt;&lt;/template&gt;&lt;script lang=\"ts\" src=\"./mycomponent.ts\"&gt;&lt;/script&gt;&lt;style src=\"./mycomponent.scss\"&gt;&lt;/style&gt;Personally I avoid using the src attribute as I like to force myself to keep my .vue files as small as possible.There’s also no performance difference between putting the HTML/SCSS separately or in the same .vue file.The Vue CLI will generate the same compiled code.How to organise your filesThe basic structureThe basic project structure in Vue is very simple:/public\t/index.html/src\t/assets\t/components\tApp.vueThe idea behind this is that you add all your custom components into /components and any assets that also need to be transpiled/compiled into /assets.For example a global stylesheet or an icon library that you want to treeshake fits perfectly into /assets.In the public folder we put all things static that don’t need to be parsed and treeshaked: the logo, the favicons, images, …A more advanced structureThe basic structure is enough for a simple single page application.But once you start to have complex pages, things will quickly need to be changed to accommodate the amount of files that you will create.In a more advanced project (like a personal project of mine), the structure could be like this:/public\t/index.html/src\t/assets\t/components\t/i18n\t/models\t/store\t/views\tApp.vueIn this project structure, both /public and /assets provide the same purpose.I use /assets for some local .json files that contain some of my data that is needed in the application.In /components I keep all my generic components that can be used by pages and other components.This is what you would put in a SharedModule in Angular for example./i18n contains all my translations of my website as well as the initialisation of an i18n library.Same goes for the /store that contains my implementation of a store.I had based myself on the structure proposed in the Vuex library where they group everything store related into /store and applied the same principle for other libraries.ModulesVue is designed to be as lightweight as possible and this can be seen in how the basic project is structured: no modules are present.Vue does support modules but not in the way like we know them from other frameworks like Angular.Vue modules are simply ES6 modules.BlogPost componentThe basic fileOur first component we will write is a BlogPost component in components/BlogPost.vue.In a first stage of our little project we will just hardcode a blogpost.The BlogPost component is small:&lt;template&gt;\t&lt;div class=\"blogpost\"&gt;\t\t&lt;h2&gt;{{ post.title }}&lt;/h2&gt;\t\t&lt;p&gt;{{ post.body }}&lt;/p&gt;\t\t&lt;p class=\"meta\"&gt;Written by {{ post.author }} on {{ date }}&lt;/p&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Prop, Vue } from 'vue-property-decorator';export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;}@Componentexport default class BlogPost extends Vue {\t@Prop() private post!: Post;\tget date() {\t\treturn `${this.post.datePosted.getDate()}/${this.post.datePosted.getMonth()}/${this.post.datePosted.getFullYear()}`;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;div.blogpost {\th2 {\t\ttext-decoration: underline;\t}\tp.meta {\t\tfont-style: italic;\t}}&lt;/style&gt;As you can see the template is rather small.We’ve grouped all the elements in a div with class blogpost.Vue expects us to wrap the content in one tag and by convention, they advise to use a div tag.Within the script tag you’ll notice that we have created a small Post interface to wrap our data.On the component itself, we have a member that is decorated with @Prop().With the decorator, we allow the use of the BlogPost component with the attribute post that should have type Post.You’ll notice we’ve added a ! behind post so we end up with post!.The exclamation mark is the non-null assertion operator which tells the browser that post will eventually be filled in with a value and that it shall never be null or undefined.&lt;BlogPost :post=\"blogPost\" /&gt;Where blogPost is an instance of Post in our component.After that we have a date member which is a computed property.Sadly there is no full type checking going on at the moment.If we were to use the BlogPost component, we can always pass along another object into the post attribute.We can pass along a type attribute in the Props decorator but even that is not that stable.@Prop({type: Object as () =&gt; Post})We end the component with our styling in which we make use of SCSS to nest all our styling.Using the BlogPost componentSo we have created the BlogPost component but how are we going to actually use it?We adapt App.vue as follows:&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;h1&gt;Elke's fantastic blog&lt;/h1&gt;\t\t&lt;BlogPost v-for=\"blogPost in blogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import BlogPost, { Post } from './components/BlogPost.vue';@Component({\tcomponents: {\t\tBlogPost,\t},})export default class App extends Vue {\tprivate blogPosts: Post[] = [\t\t{\t\t\ttitle: 'My first blogpost ever!',\t\t\tbody: 'Lorem ipsum dolor sit amet.',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 18),\t\t},\t\t{\t\t\ttitle: 'Look I am blogging!',\t\t\tbody: 'Hurray for me, this is my second post!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 19),\t\t},\t\t{\t\t\ttitle: 'Another one?!',\t\t\tbody: 'Another one!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 20),\t\t},\t];}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;As you can see we define a property on the App component that contains our blog posts, we add the components property to the Component decorator and add the BlogPost tag in the template.We simply loop over the blog posts with the v-for directive.We pass each blog post to the BlogPost component by binding it to the correct data attribute.This can be done through v-bind:post=\"blogPost\" but we use the shorthand method of :post=\"blogPost\".Vue transforms :post to v-bind:post behind the screens.Note that we also pass :key which we bind to the title of our blog post.The reason for this is that it allows Vue to keep track of the state of the list by only looking at the :key attribute instead of having to deep compare objects.Try to have an unique key of type number or string that can be used for actions such as identifying, ordering and searching.A blog post title is a good start but when ordering, updating or other modifying operations you might not have the wanted result if we have blog posts with the same title.It’s best to use something of type number or string as the key.Vue will tell you this in the console of your browser if you would take for example the datePosted as your key:  The reason behind this is that Vue relies on the built-in sort and find functionalities of JavaScript.For objects, Vue can not do this natively.When we serve our app with$ npm run serveWe see in our browser:  Great, you’ve written your first working component!Now it is time to extend it with some functionalities.Adding conditional elements to the componentAn important part of a component is to have some dynamic behaviour.For example what if we want to show a highlighted blog post? We could create a new component called HighlightedBlogPost but we could also extend our existing component.We can add a new paragraph with a v-if statement:&lt;p v-if=\"post.highlighted\"&gt;This post is highlighted!&lt;/p&gt;The contents of the v-if is a TypeScript statement that should return true or false.We extend our Post interface to accomodate this:export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;\thighlighted?: boolean;}After that we add highlighted: true, to the second blog post in App.vue.In our browser it looks like this:  We end up with this as our BlogPost component:&lt;template&gt;\t&lt;div class=\"blogpost\"&gt;\t\t&lt;h2&gt;{{ post.title }}&lt;/h2&gt;\t\t&lt;p v-if=\"post.highlighted\"&gt;This post is highlighted!&lt;/p&gt;\t\t&lt;p&gt;{{ post.body }}&lt;/p&gt;\t\t&lt;p class=\"meta\"&gt;Written by {{ post.author }} on {{ date }}&lt;/p&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Prop, Vue } from 'vue-property-decorator';export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;\thighlighted?: boolean;}@Componentexport default class BlogPost extends Vue {\t@Prop() private post!: Post;\tget date() {\t\treturn `${this.post.datePosted.getDate()}/${this.post.datePosted.getMonth()}/${this.post.datePosted.getFullYear()}`;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;div.blogpost {\th2 {\t\ttext-decoration: underline;\t}\tp.meta {\t\tfont-style: italic;\t}}&lt;/style&gt;Adding conditional CSS to our componentWe now know how to add a conditional element to our component, but we can also have conditional CSS.We will use this conditional CSS so our highlighted blog post is also visually highlighted.We can add in our div with class blogpost an extra v-bind directive:&lt;div class=\"blogpost\" v-bind:class=\"{ highlighted: post.highlighted }\"&gt;...&lt;/div&gt;With v-bind we define to which attribute we want to bind after the colon.So in our case, v-bind:class results in a binding with the class attribute in our HTML.v-bind:class accepts an object as parameter in which each key should be mapped to a boolean.For each key that is mapped to a truthy value, that key is added as a class to the HTML tag on which the v-bind is located.You will notice that we use v-bind to bind to the class attribute but that this attribute already exists on our HTML element.This is no problem as Vue will simply concatenate all the values.In the case that post.highlighted is truthy, we will thus end up with:&lt;div class=\"blogpost highlighted\"&gt;...&lt;/div&gt;And when it is falsy, we end up with:&lt;div class=\"blogpost\"&gt;...&lt;/div&gt;We extend our .blogpost to give the blog posts a width, center them and add a border with a background:div.blogpost {\twidth: 400px;\tmargin: 0 auto;\t&amp;.highlighted {\t\tborder: 1px solid #f4d942;\t\tbackground: #fff3b2;\t}\t...}In our browser it looks like this:  Note that we also have a shorter version of v-bind:attributename which is :attributename.So we can shorten v-bind:class to this:&lt;div class=\"blogpost\" :class=\"{ highlighted: post.highlighted }\"&gt;...&lt;/div&gt;We end up with this as our BlogPost component:&lt;template&gt;\t&lt;div class=\"blogpost\" :class=\"{ highlighted: post.highlighted }\"&gt;\t\t&lt;h2&gt;{{ post.title }}&lt;/h2&gt;\t\t&lt;p v-if=\"post.highlighted\"&gt;This post is highlighted!&lt;/p&gt;\t\t&lt;p&gt;{{ post.body }}&lt;/p&gt;\t\t&lt;p class=\"meta\"&gt;Written by {{ post.author }} on {{ date }}&lt;/p&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Prop, Vue } from 'vue-property-decorator';export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;\thighlighted?: boolean;}@Componentexport default class BlogPost extends Vue {\t@Prop() private post!: Post;\tget date() {\t\treturn `${this.post.datePosted.getDate()}/${this.post.datePosted.getMonth()}/${this.post.datePosted.getFullYear()}`;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;div.blogpost {\twidth: 400px;\tmargin: 0 auto;\t&amp;.highlighted {\t\tborder: 1px solid #f4d942;\t\tbackground: #fff3b2;\t}\th2 {\t\ttext-decoration: underline;\t}\tp.meta {\t\tfont-style: italic;\t}}&lt;/style&gt;Using events in a componentAs a final extension to our blog, we also want to add some dynamic behaviour by reacting to events.For our example, we will bind a button to the click event in our App component with the v-on:click directive.&lt;button v-on:click=\"toggleHighlightedPostsVisibility\"&gt;Show/hide highlighted posts&lt;/button&gt;The syntax to bind to events is v-on:eventname.We can also use the shorthand version which is @eventname:&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;Show/hide highlighted posts&lt;/button&gt;After that we write the event handler along with some variables in our component.The code block is followed by an explanation about what we have done exactly:export default class App extends Vue {\t// ...\tpublic showHighlighted: boolean = true;\tprivate blogPosts: Post[] = [];\tget visibleBlogPosts() {\t\treturn this.blogPosts.filter((post: Post) =&gt; post.highlighted === undefined ||  post.highlighted === this.showHighlighted);\t}\tpublic toggleHighlightedPostsVisibility() {\t\tthis.showHighlighted = !this.showHighlighted;\t}\t// ...}First what we did was add the showHighlighted boolean.This is to keep track whether we should show or hide the highlighted blog posts.We also wrote a getter to only show the blog posts that are allowed to be shown.In our filter, we check if the highlighted member is defined and if so, we check if it equals our showHighlighted variable.The reason why we write this in a getter, is that we want to avoid putting business logic in our template.Thus we opt for writing a getter which is the equivalent of a computed property in Vue JavaScript.After this we have to adapt the v-for in our template so that we use the new getter:&lt;BlogPost v-for=\"blogPost in visibleBlogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;As a small bonus, we will make the text in our button dynamic.Currently we have Show/hide highlighted posts as the text but it would be cleaner if we showed Show highlighted posts and Hide highlighted posts depending on the state of the component.We update the button to the following code:&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;{{ showHighlighted ? 'Hide' : 'Show' }} highlighted posts&lt;/button&gt;In the end, we end up visually with this:  And our App component looks like this:&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;h1&gt;Elke's fantastic blog&lt;/h1&gt;\t\t&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;{{ showHighlighted ? 'Hide' : 'Show' }} highlighted posts&lt;/button&gt;\t\t&lt;BlogPost v-for=\"blogPost in visibleBlogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import BlogPost, { Post } from './components/BlogPost.vue';@Component({\tcomponents: {\t\tBlogPost,\t},})export default class App extends Vue {\tpublic showHighlighted: boolean = true;\tprivate blogPosts: Post[] = [\t\t{\t\t\ttitle: 'My first blogpost ever!',\t\t\tbody: 'Lorem ipsum dolor sit amet.',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 18),\t\t},\t\t{\t\t\ttitle: 'Look I am blogging!',\t\t\tbody: 'Hurray for me, this is my second post!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 19),\t\t\thighlighted: true,\t\t},\t\t{\t\t\ttitle: 'Another one?!',\t\t\tbody: 'Another one!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 20),\t\t},\t];\tget visibleBlogPosts() {\t\treturn this.blogPosts.filter((post: Post) =&gt; post.highlighted === undefined ||  post.highlighted === this.showHighlighted);\t}\tpublic toggleHighlightedPostsVisibility() {\t\tthis.showHighlighted = !this.showHighlighted;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;5. Using your first pluginVue comes without any libraries, it is a super clean and lean framework where even functionality for HTTP calls is not present.However, every component is a Vue object and can be extended with a $http member that you can use in your code to perform HTTP calls.To provide this $http member, we have to add the correct plugin to our code.In the awesome-vue project on GitHub, we can find an extensive list of HTTP plugins.We will use axios as our HTTP library but we will use vue-axios for the bindings with Vue in TypeScript as axios does not provide the necessary typings for axios in Vue.Installing a pluginWe follow the installation instructions for vue-axios which are pretty straightforward:$ npm i axios vue-axiosAs you noticed we also installed axios.This is because vue-axios only focuses on the TypeScript bindings for Vue and does not include the actual axios library.Vue-axios basically turns the axios library into a plugin compatible for Vue.After that, we have to signal to Vue that we want to use this plugin.We add a Vue.use(plugin, options) statement in our main.ts so it looks like this:import axios from 'axios';import Vue from 'vue';import VueAxios from 'vue-axios';import App from './App.vue';Vue.config.productionTip = false;Vue.use(VueAxios, axios);new Vue({\trender: (h) =&gt; h(App),}).$mount('#app');The important part is that we put the Vue.use(...) statement before we actually bootstrap the application with new Vue(...).The effect of adding a pluginSo we have added a plugin, but what does that actually mean?What is the effect on our Vue code?The main effect is that we now have $http accessible in every Vue component.This means that we can now have this.$http in our classes in which a unique instance of the axios library for the whole application will be plugged.When we check the typings  from axios, we find that we now have methods like get(...), post(...) and many more default REST methods available in our code through the $http member in which an instance of axios is present.Methods like get(...) and post(...) will also exist on other HTTP libraries that we can add to Vue.It is not obliged by Vue to provide these same functionalities in another HTTP library.But it makes sense for library creators to comply to the standard set by the HTTP library of preference as chosen by Vue, in this case axios.Otherwise it would not be easy to change certain libraries for another one.Using the axios plugin for performing HTTP callsA Vue component has multiple lifecycle hooks with the most interesting ones for what we want to do: created() and mounted().Created is called by Vue when the object is created: reactive data is set up, event callbacks are ready and the object is not yet mounted on the DOM.The Vue object will thus be ready to go but it will not yet be visible to the user.The mounted hook is used for when the element is mounted into the HTML DOM, which means the rendering is performed by the browser.&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t...\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';@Component({})export default class App extends Vue {\tprivate created() {\t\tconsole.log('The app is created!');\t}\tprivate mounted() {\t\tconsole.log('The app is mounted!');\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {  ...}&lt;/style&gt;There are two reasons why we want to start our HTTP calls in the created method.The first reason is that we can limit the amount of time the user has to wait for data to be loaded and shown on the screen.The second one is that the mounted hook is not called when we would use serverside rendering.To ensure that our code is compatible with all use cases, we place the HTTP calls in the created method of our App.vue which results in this component:&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;h1&gt;Elke's fantastic blog&lt;/h1&gt;\t\t&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;{{ showHighlighted ? 'Hide' : 'Show' }} highlighted posts&lt;/button&gt;\t\t&lt;BlogPost v-for=\"blogPost in visibleBlogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import BlogPost, { Post } from './components/BlogPost.vue';import { AxiosResponse } from 'axios';@Component({\tcomponents: {\t\tBlogPost,\t},})export default class App extends Vue {\tpublic showHighlighted: boolean = true;\tprivate blogPosts: Post[] = [];\tget visibleBlogPosts() {\t\treturn this.blogPosts.filter((post: Post) =&gt; post.highlighted === undefined ||  post.highlighted === this.showHighlighted);\t}\tpublic toggleHighlightedPostsVisibility() {\t\tthis.showHighlighted = !this.showHighlighted;\t}\tprivate created() {\t\tthis.$http.get('http://localhost:3000/blogposts').then((response: AxiosResponse) =&gt; {\t\t\tthis.blogPosts = response.data.map((val: any) =&gt; ({\t\t\t\ttitle: val.title,\t\t\t\tbody: val.body,\t\t\t\tauthor: val.author,\t\t\t\tdatePosted: new Date(val.datePosted),\t\t\t\thighlighted: val.highlighted,\t\t\t}));\t\t});\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;As you can see we have added a private created method since this should not be publicly available to other components.We call an API and map the response into our Post array.Now we need to set up our API.How we have set up a local APITo simulate a real API call, we set up json-server, a small tool that launches a web server with a REST API that serves a JSON file which we call db.json present in our assets folder:{\t\"blogposts\": [\t\t{\t\t\t\"title\": \"My first blogpost ever!\",\t\t\t\"body\": \"Lorem ipsum dolor sit amet.\",\t\t\t\"author\": \"Elke\",\t\t\t\"datePosted\": \"2019-01-18\"\t\t},\t\t{\t\t\t\"title\": \"Look I am blogging!\",\t\t\t\"body\": \"Hurray for me, this is my second post!\",\t\t\t\"author\": \"Elke\",\t\t\t\"datePosted\": \"2019-01-19\",\t\t\t\"highlighted\": true\t\t},\t\t{\t\t\t\"title\": \"Another one?!\",\t\t\t\"body\": \"Another one!\",\t\t\t\"author\": \"Elke\",\t\t\t\"datePosted\": \"2019-01-20\"\t\t}\t]}We install json-server with NPM and then we launch it with:$ npm i json-server$ json-server src/assets/db.jsonBy default, json-server will launch on port 3000.When we launch wordvue and open it in the browser, we will see that the blog posts are now coming from our local API.Now you know how to install a plugin and retrieve data with axios over HTTP.6. Your first deploymentDevelopment mode versus production modeJust like other frontend frameworks, Vue has its development mode and production mode. The development mode is available with:$ npm run serveWhile production mode is available with:$ npm run buildNow what is the difference between both modes?            Development mode      Production mode                  CSS &amp; HTML bundled into JS      CSS separately, HTML bundled into JS              Warnings in console      No warnings in console              Additional checks to identify warnings      No additional checks, ignores any situation that would trigger warnings              Everything in one app.js      Separate app.js and vendor.js              Heavy use of eval() for hot reload      No use of eval(), no hot reloading necessary              Basic bundling of all code, use of minified libraries only when available      Bundling &amp; maximum minification              No minification of index.html      Minification of index.html        vendor.js: Contains all the node_modules code that your project uses  eval(): JavaScript function that executes strings as if it’s a line of code and should never be used in productionAll the minification, avoiding the use of eval(), removing of warning checks and so on results in a much smaller size of the code.If we do a npm run serve and check our Developer Tools in Chrome, we see the size of our application:  In development mode, our application is more than 2MB large. We see the index.html alongside a generated app.js that contains all our own code and all the node_modules that we are using.While when we build with the production mode via npm run build, we get an application that is in total less than 125KB:  The only thing we changed to get a dist folder that is so small, was adding a vue.config.js file in the root of our folder which exports an object with the settings we want:module.exports = {    productionSourceMap: false};We only had to add the productionSourceMap set to false to disable the creation of source maps.More configurations can be found at cli.vuejs.org but most of the configuration is already done for a maximum optimised production build.Building for productionWhen running the npm run build command, you’ll get the following output:  So what the Vue CLI does is take all the SCSS out of the components and minifies it, compiles all the components into an app.js file and treeshakes  all used libraries into a chunk-vendors.js file.After that, it Gzips all those files to ensure that everything is as light as possible.If you have any assets, it will also clone those into the dist folder.The result is a dist folder which contents you can directly deploy onto your favourite server.7. ConclusionCongratulations, you have built your very first Vue application with TypeScript!The end result can be found in my GitHub repository  so you can see the working version.You now know how to write a basic component with the use of decorators, create a component structure and fill it with data coming from an API.After that you can also deploy it onto a server.A next step would be to add routing, add a store or an i18n library.Vue is a lightweight framework that primarily focuses on visualisation.If you want to add more functionality, you will have to rely on plugins who either support Vue integration directly or you can use a plugin like vue-axios that will facilitate the integration of another library like axios.8. Resources and further reading  Vue CLI: cli.vuejs.org  Awesome-vue, overview of Vue plugins: github.com/vuejs/awesome-vue  Axios, HTTP library: github.com/axios/axios  Vue-axios, typings for using Axios in Vue: github.com/imcvampire/vue-axios  Vue styleguide: vuejs.org/v2/style-guide  Vue-class-component: github.com/vuejs/vue-class-component  JSON server: github.com/typicode/json-server  Wordvue repository: github.com/ordina-jworks/vue-typescript-wordvue"
      },
    
      "frontend-2019-02-21-observables-html": {
        "title": "Observables: The right way",
        "url": "/frontend/2019/02/21/observables.html",
        "image": "/img/observables-the-right-way/cover.png",
        "date": "21 Feb 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Intro  Setup  Refactoring  Example  ConclusionIntroDuring my consultancy projects, I often come across the same implementations and problems when colleagues are trying to implement an observable strategy.A lot of frameworks are offering observables out of the box for their communication layer.Almost all Promises are replaced by Observables nowadays.Angular 2+ HttpModule for example, is using the rxjs library.Each http.get() is returning an Observable&lt;HttpResponse&gt;.The setup is almost always the same.A (visual) component needs to render some data.So next to the component (HTML/template for view, and a JavaScript(TypeScript) component as the controller), a service gets created.This service’s purpose is to provide data to the component’s controller by calling the HttpClient’s functions (POST, GET, DELETE, PATCH, …) and returning the Observable to the component.Sometimes they are remapping the Observable&lt;HttpReponse&gt;to a more defined type, for example Observable&lt;MyData&gt;, by using one of the rxjs operators such as flatmap, map, … .All of this works pretty well, as long as only one component is in need of the data and its changes.With changes, I refer to refreshing the data, or requerying it with another filter, paging, or …  Every time the query parameters change, the component is just executing the same call in the service, which in turn is calling the right HttpClient-function.Again an (new) Observable is returned.Again the result can get remapped before throttling it back.The subscriber, the component controller in this case (or the HTML if you are using Angular’s async pipe), receives the remapped data.What happens when we have another component that is in need of this data (or maybe just a part of it)?Let’s say we have a header and a datatable.And we are NOT using push events, but simple REST calls (for the sake of this explanation).The datatable is the component we were talking about earlier.It needs to display messages in a simple datatable.The header is the second component that needs this data.It needs to display the number of unread or critical messages.datatable.component.tsthis._dataService.get(this.filter).subscribe((page:Page) =&gt; {\tthis.data = page.data;})header.component.tsthis._dataService.get(this.filter).subscribe((page:Page) =&gt; {\tthis.count = page.number;});data.service.tspublic get(filter?: Filter): Observable&lt;Page&gt; {\treturn this._http.get('urlToData' + this.parseFilterToString(filter)).pipe(\t\ttap((data) =&gt; this.logService.log(data)),\t\tmap((response) =&gt; {\t\t\tlet page: Page = new Page();\t\t\tpage.data = response.body;\t\t\tpage.number = response.headers.get('x-count');\t\t})\t);}In the setup we have so far, both components will use the same service for requesting the data.They will both subscribe to an Observable, however, it will be a different one.When you refresh the data in the datatable, the header will not receive a new value in its subscription and therefore will still show the old number of messages.In some use cases this might be the desired outcome, but in most cases you want more components to be able to subscribe to the same Observable.SetupIn our new setup, we want both, header and datatable, to subscribe to the same Observable, so a call for new data will result in an update in the datatable and the header.To make this happen, we will use some kind of layer in between them.This new layer will provide our components with one and only one and the same Observable and will mask the communication layer from the view’s controller.Both components will subscribe to this service, so they both get updated with the same result.We can do this by creating a simple Subject in our service and returning it as an Observable to our components.We can then implement other calls for this service that will trigger an update of the data, and send it through the subject to both components.Because we are not providing a filter when we call the getter for the Observable (Subject) we should also find a way of providing the filter to the service, before requerying the data.This means we are going to use one shared filter, for both components, which makes sense in this case, but not in all use cases.  RefactoringWe actually don’t need to refactor any of our components.They will still subscribe to an Observable of the service, and react on the incoming data.The service however will get refactored.Start by defining the Subject as a local property, and because we are going to implement a getter, we can make it private.data.service.tsprivate _data$: Subject&lt;Page&gt; = new BehaviorSubject&lt;&gt;({});...public data(): Observable&lt;Page&gt; {\treturn this._data$ as Observable&lt;Page&gt;;}...public reload(): void {\tthis._http.get('urlToData' + this.parseFilterToString(this._filter)).pipe(\t\ttap((data) =&gt; this.logService.log(data)),\t\tmap((response) =&gt; {\t\t\tlet page: Page = new Page();\t\t\tpage.data = response.body;\t\t\tpage.number = response.headers.get('x-count');\t\t}).subscribe((page: Page) =&gt; {\t\t\tthis._data$.next(page);\t\t})\t);}This way, every component or service that is subscribing on this subject, is getting data when some other component or service triggers the reload.There are even a lot more options to this setup:  Clearing data  Resetting to default filter  Refreshing the current filter  Caching data  Manipulating data through other services or components  Adding an event consumer that also updates the datatable  …ExampleI’ve build a simple example to demonstrate this behaviour.A header that is displaying an alert icon when there are unread, critical messages.A sidebar that is displaying the amount of unread messages next to its navigation link, and an overview of the messages, with a basic paging implementation.  A simple backend that is written in Node.js with Express provides a few endpoints:  api/message (with paging and filter, although the filter isn’t implemented in the frontend example.)  api/message/:id (not used in the example)  api/stream  api/refreshThe service is not reloading data as long as the page or the filter hasn’t changed.While the service is still loading the data, a new reload will not fetch again the data.You can find the code on GitLab.  Server-Sent Events are added to update the read status of a message when the envelope gets clicked.This will also trigger the observable.To run front- and backend together, execute the following command in the root of the project:$ npm run startThis way, a proxy is added to the serve command to overcome CORS blocking going from localhost:4200 to localhost:3000Don’t mind the backend server, it’s a quick and dirty solution and is not implemented as it should.ConclusionAlthough observables are a great feature, and are easy to use, it’s always better to have your own layer of control.Especially when it comes to using observables from frameworks.I can accept, for simple applications, that you don’t want to ‘over-architect’.But in most cases, you want to control the distribution yourself.For those of you that know Redux (RxJS), you can compare this implementation with effects and store-subscriptions.If you trigger an effect, you will only see the result when you have subscribed to the ‘key’ that is responsible for providing you with the data,and not to the ‘key’ that is responsible for triggering the effect."
      },
    
      "cloud-2019-01-14-infrastructure-as-code-with-terraform-and-aws-serverless-html": {
        "title": "Infrastructure as code: Terraform and AWS Serverless",
        "url": "/cloud/2019/01/14/Infrastructure-as-code-with-terraform-and-aws-serverless.html",
        "image": "/img/2019-01-14-Infrastructure-as-code-with-terraform-and-aws-serverless/featured-image.png",
        "date": "14 Jan 2019",
        "category": "post, blog post, blog",
        "content": "Table of content  Infrastructure as Code  Introduction and demo  Creating the application  Prerequisites  Terraform: the basics  General  Database: DynamoDB  IAM  Lambda Functions  API Gateway  Endgame  Resources and further readingInfrastructure as CodeInfrastructure as Code (IaC) is a way of managing your devices and servers through machine-readable definition files. Basically, you write down how you want your infrastructure to look like and what code should be run on that infrastructure. Then, with the push of a button you say “Deploy my infrastructure”. BAM, there is your application, running on a server, against a database, available through an API, ready to be used!And you just defined all of that infrastructure using IaC.  IaC is a key practice of DEVOPS teams and integrates as part of the CI/CD pipeline.A great Infrastructure as Code tool is Terraform by HashiCorp.(https://www.terraform.io/)Personally I use it to provide and maintain infrastructure on AWS.And I’ve had a great experience doing that.  Introduction and demoI will demonstrate IaC by working out an example. We are going to set up an application on AWS.I provisioned the code on GitLab: https://gitlab.com/nxtra/codingtips-blog.A user can enter a coding tip and see all the coding tips that other users have entered.The tips are stored in a NoSQL database which is AWS DynamoDB.Storing and retrieving these tips is done by the Lambda Functions which fetch or put the tips from and to the database.For the application to be useful, users have to be able to call these Lambda Functions.So we expose the Lambda Functions through AWS API Gateway. Here is an architectural overview of the application:  You could couple these functions to a web page where users can enter tips and see all tips that have been given.Below you see the final result:  Let’s dive in!Creating the application  I will now go over the steps to set up the application you see in the demo above.IaC is the main focus.I will show the code and AWS CLI commands that are necessary but I will not explain them in detail since that is not the purpose of this blog.I’ll focus on the Terraform definitions instead.You are welcome to follow along by cloning the repository that I linked to in this blog post.Prerequisites  Install Terraform  Install AWS CLI  Checkout the repository on GitLab: https://gitlab.com/nxtra/codingtips-blog  Be ready to get your mind blown by IaCTerraform: the basicsThe main things you’ll be configuring with Terraform are resources.Resources are the components of your application infrastructure.E.g: a Lambda Function, an API Gateway Deployment, a DynamoDB database, …A resource is defined by using the keyword resource followed by the type and the name.The name can be arbitrarily chosen.The type is fixed.For example:resource \"aws_dynamodb_table\" \"codingtips-dynamodb-table\"To follow along with this blog post you have to know two basic Terraform commands.  terraform applyTerraform apply will start provisioning all the infrastructure you defined.Your databases will be created.Your Lambda Functions will be set up.The API Gateway will be set in place.  terraform destroyTerraform destroy will remove all the infrastructure that you have set up in the cloud.If you are using Terraform correctly you should not have to use this command.However should you want to start over, you can remove all the existing infrastructure with this command.No worries, you will still have all the infrastructure neatly described on your machine because you are using Infrastructure as Code.We’ll put all infrastructure that is defined using Terraform in the same folder.The files need to have a .tf extension.GeneralLet’s start out by creating a file general.tf.provider \"aws\" {  region = \"eu-west-1\"}# variablesvariable \"lambda_version\"     { default = \"1.0.0\"}variable \"s3_bucket\"          { default = \"codingtips-node-bucket\"}The provider block specifies that we are deploying on AWS.You also have the possibility to mention credentials that will be used for deploying here.If you have correctly set up the AWS CLI on your machine there will be default credentials in your .aws folder.If no credentials are specified, Terraform will use these default credentials.Variables have a name which we can reference from anywhere in our Terraform configuration. For example we could reference the s3_bucket variable with ${var.s3_bucket).This is handy when you are using the same variable in multiple places.I will not use too many variables throughout this blog post since that will add more references to your Terraform configuration and I want it to be as clear as possible.Database: DynamoDB  Let’s start with the basis.Where will all our coding tips be stored? That’s right, in the database.This database is part of our infrastructure and will be defined in a file I named dynamo.tf.resource \"aws_dynamodb_table\" \"codingtips-dynamodb-table\" {  name = \"CodingTips\"  read_capacity = 5  write_capacity = 5  hash_key = \"Author\"  range_key = \"Date\"  attribute = [    {      name = \"Author\"      type = \"S\"    },    {      name = \"Date\"      type = \"N\"    }]}Since Dynamo is a NoSQL database, we don’t have to specify all attributes upfront.The only thing we have to provide are the elements that AWS will use to build the partition key with.When you provide a hash key as well as a sort key, AWS will combine these to make a unique partition key.Mind the word UNIQUE.Make sure this combination is unique.  DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. All items with the same partition key value are stored together, in sorted order by sort key value.– from AWS docs: DynamoDB Core ComponentsFrom the attribute definitions in dynamo.tf it is clear that Author (S) is a string and Date (N) should be a number.IAM  Before specifying the Lambda Functions we have to create permissions for our functions to use.This makes sure that our functions have permissions to access other resources (like DynamoDB).Without going too deep into it, the AWS permission model works as follows:  Provide a resource with a role  Add permissions to this role  These allow the role to access other resources:          permissions for triggering another resource (eg. Lambda Function forwards logs to CloudWatch)      permissions for being triggered by another resource (eg. Lambda Function may be triggered by API Gateway)      # ROLES# IAM role which dictates what other AWS services the Lambda function# may access.resource \"aws_iam_role\" \"lambda-iam-role\" {  name = \"codingtips_lambda_role\"  assume_role_policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": \"sts:AssumeRole\",      \"Principal\": {        \"Service\": \"lambda.amazonaws.com\"      },      \"Effect\": \"Allow\",      \"Sid\": \"\"    }  ]}EOF}# POLICIESresource \"aws_iam_role_policy\" \"dynamodb-lambda-policy\"{  name = \"dynamodb_lambda_policy\"  role = \"${aws_iam_role.lambda-iam-role.id}\"  policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Action\": [        \"dynamodb:*\"      ],      \"Resource\": \"${aws_dynamodb_table.codingtips-dynamodb-table.arn}\"    }  ]}EOF}In the example above, the first resource that is defined is an aws_iam_role.This is the role that we will later give to our Lambda Functions.We then create the aws_iam_role_policy resource which we link to the aws_iam_role.The first aws_iam_role_policy is giving this role permission to invoke any action on the specified DynamoDB resource.The second role_policy allows a resource with this role to send logs to CloudWatch.A couple of things to notice:  The aws_iam_role and the aws_iam_role_policy are connected by the role argument of the role_policy resource  In the statement attribute of the aws_iam_role_policy we grant (Effect attr.) permission to do some actions (Action attr.) on a certain resource (Resource attr.)  A resource is referenced by its ARN or Amazon Resource Name which uniquely identifies this resource on AWS  There are two ways to specify an aws_iam_role_policy:          using the until EOF syntax (like I did here)      using a separate Terraform aws_iam_policy_document element that is coupled to the aws_iam_role_policy        The dynamodb-lambda-policy allows all actions on the specified DynamoDB resource because under the Action attribute it states dynamodb:*You could make this more restricted and mention actions like\"dynamodb:Scan\", \"dynamodb:BatchWriteItem\",\"dynamodb:PutItem\"Lambda Functions  There are two Lambda Functions that are part of this application.The first Lambda is used to get or retrieve the coding tips from the database further referenced as the getLambda.The second Lambda is used to post or send the coding tips to the database further referenced as the postlambda.I am not going to copy paste the code of the Lambda Functions in here.You can check it out in the repository linked to this blog (GitLab repository: https://gitlab.com/nxtra/codingtips-blog).Here I will demonstrate the example of the getLambda function.The postLambda is deployed in the same way and you can find the Terraform definitions in the Git repository.A Lambda Function is a little different from the other infrastructure we defined here.Not only do we need a Lambda Function as infrastructure.We also need to specify the code that runs in this Lambda Function.But where will AWS find that specific code when deploying the Lambda Function?They don’t have access to your local machine, have they?That is why you first need to ship your code to a S3 Bucket on AWS where it can be found when your Function is being deployed.That also means creating an S3 Bucket, which you can do with this command when you want it in region eu-west-1 (Ireland):aws s3api create-bucket --bucket codingtips-node-bucket --region eu-west-1 --create-bucket-configuration LocationConstraint=eu-west-1Now you have to zip the code of your Lambda Functions:zip -r getLambda.zip index.jsAnd upload that file to s3:aws s3 cp getLambda.zip s3://codingtips-node-bucket/v1.0.0/getLambda.zipMind that I am sending it to a bucket named codingtips-node-bucket in a folder v1.0.0 with filename getLambda.zip.Okay, the code is where it needs to be.Now let’s see how we specify these functions using Terraform.resource \"aws_lambda_function\" \"get-tips-lambda\" {  function_name = \"codingTips-get\"  # The bucket name as created earlier with \"aws s3api create-bucket\"  s3_bucket = \"${var.s3_bucket}\"  s3_key = \"v${var.lambda_version}/getLambda.zip\"  # \"main\" is the filename within the zip file (index.js) and \"handler\"  # is the name of the property under which the handler function was  # exported in that file.  handler = \"index.handler\"  runtime = \"nodejs8.10\"  memory_size = 128  role = \"${aws_iam_role.lambda-iam-role.arn}\"}resource \"aws_lambda_permission\" \"api-gateway-invoke-get-lambda\" {  statement_id  = \"AllowAPIGatewayInvoke\"  action        = \"lambda:InvokeFunction\"  function_name = \"${aws_lambda_function.get-tips-lambda.arn}\"  principal     = \"apigateway.amazonaws.com\"  # The /*/* portion grants access from any method on any resource  # within the specified API Gateway.  source_arn = \"${aws_api_gateway_deployment.codingtips-api-gateway-deployment.execution_arn}/*/*\"}  Notice that we tell Terraform the S3 Bucket and directory to look for the code  We specify the runtime and memory for this Lambda Function  index.handler points to the file and function where to enter the code  The aws_lambda_permission resource is the permission that states that this Lambda Function may be invoked by the API Gateway that we createdAPI Gateway  I kept the most difficult one for last.On the other hand, it is also the most interesting.I hand Terraform a Swagger definition of my API.You can also do this without Swagger, but then you will have to specify a lot more resources.The Swagger API definition looks as follows:swagger: '2.0'info:  version: '1.0'  title: \"CodingTips\"schemes:  - httpspaths:  \"/api\":    get:      description: \"Get coding tips\"      produces:        - application/json      responses:        200:          description: \"The codingtips request successful.\"          schema:            type: array            items:              $ref: \"#/definitions/CodingTip\"      x-amazon-apigateway-integration:        uri: ${get_lambda_arn}        passthroughBehavior: \"when_no_match\"        httpMethod: \"POST\"        type: \"aws_proxy\"    post:      description: \"post a coding tip\"      consumes:        - application/json      responses:        200:          description: \"The codingtip was added successfully\"      x-amazon-apigateway-integration:        uri: ${post_lambda_arn}        passthroughBehavior: \"when_no_match\"        httpMethod: \"POST\"        type: \"aws_proxy\"definitions:  CodingTip:    type: object    description: \"A coding tip\"    properties:      tip:        type: string        description: \"The coding tip\"      date:        type: number        description: \"date in millis when tip was entered\"      author:        type: string        description: \"Author of the coding tip\"      category:        type: string        description: \"category of the coding tip\"    required:      - tipIf you do not know Swagger yet, copy the above and paste it in the online (Swagger Editor).This will grant you a nice visual overview of the API definition.  There is only one AWS specific thing in the Swagger specification above and that is x-amazon-apigateway-integration.This is specifying the details of how the API is integrating with the backend.  Remark that this is always a POST even if the HTTP method of the resource path is a GET  aws_proxy means that the request is passed to the Lambda Function without manipulation  when_no_match passes the request body to the backend without tranforming it when no requestTemplate is specified for the Content-Type  uri is referencing a variable eg. ${get_lambda_arn} that Terraform passes to the Swagger definition.We’ll see this in a minute.As I already mentioned, using Swagger to define your API Gateway has some advantages:  It keeps your Terraform more concise  You can use this Swagger to get a nice representation of your APIresource \"aws_api_gateway_rest_api\" \"codingtips-api-gateway\" {  name        = \"CodingTipsAPI\"  description = \"API to access codingtips application\"  body        = \"${data.template_file.codingtips_api_swagger.rendered}\"}data \"template_file\" codingtips_api_swagger{  template = \"${file(\"swagger.yaml\")}\"  vars {    get_lambda_arn = \"${aws_lambda_function.get-tips-lambda.invoke_arn}\"    post_lambda_arn = \"${aws_lambda_function.post-tips-lambda.invoke_arn}\"  }}resource \"aws_api_gateway_deployment\" \"codingtips-api-gateway-deployment\" {  rest_api_id = \"${aws_api_gateway_rest_api.codingtips-api-gateway.id}\"  stage_name  = \"default\"}output \"url\" {  value = \"${aws_api_gateway_deployment.codingtips-api-gateway-deployment.invoke_url}/api\"}  We start by mentioning the aws_api_gateway_rest_api resource.It does what is says and provides an API Gateway REST API.          body references the Swagger file        The template_file datasource allows Terraform to use information that is not defined in Terraform (Swagger in our case)          Variables are passed to this template_file to fill the file        For a given rest-api to be usable, it has to be deployed          This is done by the aws_api_gateway_deployment resource      It references the REST API      It needs a stage which is like a ‘version’ or ‘snapshot’ of your APIThe stage name will be in the URL to invoke this API.        At last the URL on which the API can be invoked is outputted to the terminal/api is appended to have the correct resource pathEndgameAll right, let’s see it now.Does this actually work?Here I am running terraform apply within the repository linked to this blog.  Nice, it worked.And I only told Terraform about the infrastructure I wanted.The whole setup process goes automatically!You can now use the outputted URL to GET and POST coding tips.The body of the POST should look like:{  \"author\": \"Nick\",  \"tip\": \"Short sessions with frequent brakes\",  \"category\": \"Empowerment\"}When you need to couple the API endpoints to a frontend of your own design, you need to set the CORS headers correctly.If you want this challenge, there is another branch in the repository (cors-enabled) where I worked this out.Happy coding folks, Code that Infrastructure!Resources and further reading  Terraform website: Terraform.io  Terraform-Lambda-APIGateway: learn.hashicorp.com  Swagger editor: editor.swagger.io  Swagger official website: swagger.io"
      },
    
      "development-2019-01-10-flutter-html": {
        "title": "Flutter: Hybrid apps for mobile &amp; beyond.",
        "url": "/development/2019/01/10/Flutter.html",
        "image": "/img/2019-01-10-Flutter/Flutter.jpg",
        "date": "10 Jan 2019",
        "category": "post, blog post, blog",
        "content": "IntroMobile development has always intrigued me.Bringing data to life by visualizing it with different components and being able to carry your app along inside your pocket is something special.I still remember the excitement I felt when working on my first mobile app in university,even with the lack of good resources back then and the sluggish emulator which was available at that time.The mobile world has kept expanding with big improvements in resources, frameworks, tools and designs.From the first iPhone to having your refrigerator running your favorite apps, more and more possibilities and challenges have become available to keep you going.Nowadays, there are different paths that you can follow to create those apps.Native, hybrid or web apps, it all depends on what you want to achieve and how many resources are available.Each has its benefits and pitfalls, which doesn’t make the decision any easier.Do you want to give your audience the best native experience with great performance?Or does a hybrid app suffices where you might compromise in speed and in look &amp; feel of a native app?Flutter might be the answer, by providing you the best parts of both worlds.Beautiful native apps in record time  Flutter allows you to build beautiful native apps on iOS and Android from a single codebase.A promising statement which is presented to you when you browse to Flutter.io.A statement that explains perfectly what Flutter is in fact.Flutter is an open source mobile UI framework made by Google.With Flutter, you can build beautiful apps that run at native speed.Given the single codebase, you don’t have to develop the same app twice for both iOS and Android.Flutter is even the first-class citizen for Fuchsia, an upcoming mobile OS that is currently being developed by Google.Flutter apps follow platform conventions and interface details, so the scrolling, fonts, navigations, etc. will look natively respecting the specific platform.All of this results in beautiful apps that require less time and resources to develop and this without compromising on quality, features, performance or design.Performance at its coreHow did the Flutter team achieve all of this?Why aren’t there any downgrades in performance while being a hybrid app?The reason for this is that Flutter is built with performance in mind from the beginning.In fact, being performant was the main reason why the Flutter team started with the framework.Being hybrid was more of a side-effect because of the way the implementation is set up.Because Flutter is so performant, you are guaranteed that Flutter apps will run at 60 FPS and more.This leads to smooth animations and an instant responsive app which results in a great user experience.Your app will also render perfectly on older devices, while even some native apps may have trouble to keep running at a constant refresh rate.To achieve this performance, Flutter does something different than other hybrid solutions.The framework avoids having a JavaScript bridge between the app and the platform by using a language that compiles to native code.This bridge is typically the bottleneck when it comes to performance in hybrid solutions.The lack of the JavaScript bridge allows Flutter to communicate directly with the platform.Which language are we talking about?Well, all of this is made possible with Dart.‘Dart’ you say?Dart is a programming language that was also developed by Google.Its purpose is to build web, server and mobile apps.You can develop your app with Dart, which will either be compiled to JavaScript or into native code.The Flutter team considered different languages, but many of them had drawbacks in one of the four dimensions for evaluation that they used.Since Google had its own language ready to be used, the team also took a look at Dart.Dart scored high on all the requirements and criteria that the team had predefined, which is why the decision was obvious.There are some huge benefits when using Dart compared to other languages.One of them is the Dart runtime and compiler.This enables Dart code to be compiled both AOT (Ahead Of Time) and JIT (Just In Time).With AOT, Dart is compiled to native code which ensures that the execution is fast, high-performing and predictable.Your app will start up faster and it will feel smoother while running it.JIT enables stateful hot-reloading, which gives developers an extremely fast development cycle.Once you hit Save in your editor, the code changes are applied to your running app within a second without losing state.This gives productivity an enormous boost and helps you to reach your goals faster than before.No more refilling the same form to test some validation or navigating to a specific screen for you.It is really impressive to see hot reload in action, especially when you’re used to the development cycle within native mobile development.To me, it kind of feels like magic.You might be asking yourself if it is worth learning a new language for a new framework.From my experience, Dart is really easy to adopt, especially if you’re used to Java or JavaScript/TypeScript.Dart feels natural to use and is a powerful language.Most of the time when I’m playing around with Flutter I even don’t realize that I’m writing in another language.In fact, it just works writes.Everything is a widgetThe native performance of Flutter apps is great, but how can you use Flutter to build your app?The answer is widgets.They are the building blocks that Flutter uses to build up your interface.Widgets are responsible for the native look and feel that you want to create, so they are really important.With widgets you’ll be able to create beautiful apps, exactly how your design team imagined them to be.In Flutter, everything is a widget!Ranging from a button, an image, to the app itself.Even the padding, positioning or navigation are all defined by the use of widgets.You combine widgets to build up the interface to your liking.Flutter will generate a widget tree out of it and uses that tree to render the layout on the screen of the device.Flutter uses its own set of widgets, which assures you have a pixel perfect layout on every device.You’re not dependent, nor limited by the widgets provided by the platform.The only thing that Flutter needs is a canvas to draw on.You can compare Flutter with a game engine, or in this case rather an app engine.By providing its own set of widgets, you can customize all of them to your liking.This enables you to include your company branding through your app, ranging from colors to the shape of buttons.All the widgets are written in Dart.Because Flutter is an open source project, you can use the source code as a reference while applying the widgets.With Flutter you don’t have to worry anymore about support libraries to render your apps on old devices or about OEMs that decide to alter the platform widgets because they can.You can even enjoy the beauty of Material Design on devices that were released years before Material Design was introduced.Your app will also be future proof, as new design implementations of platform widgets won’t affect or break the layout of your app.If there are any breaking changes with future OS versions, then it’s a bug for Flutter to resolve instead for you.Flutter even added notch-support for the iPhone X before the phone was released.Here you can find a quick example how you can build up your layout by combining different widgets together.The JWorks widget is rendered inside a default Material app template on the iPhone XS.import 'package:flutter/material.dart';class JWorksWidget extends StatelessWidget { // I'm a widget  @override  Widget build(BuildContext context) {    return Card( // I'm a widget too      elevation: 4.0,      child: Padding( // Yep, another widget        padding: const EdgeInsets.all(16.0),        child: Image.network( // We're all widgets!          'https://ordina-jworks.github.io/img/jworks/jworks-400x400.png',        ),      ),    );  }}One hybrid framework to rule them allOn the 4th of December 2018 the first Flutter Live event was hosted.A lot of exciting announcements were made, which definitely shows that the Flutter team is determined to keep improving.The first stable version was released almost two years after the first Alpha version was released.Another huge announcement was revealed about the future plans of Flutter.The first step is Hummingbird, or Flutter for the web.Since Dart can also compile to JavaScript, this was a logical step to take.Being able to run on the web, you can also create a Progressive Web App with Flutter, so your mobile app which runs in the browsers becomes a web app.Appception right there.Google is also working to bring Flutter to desktop with Flutter Desktop Embedding.To prove this concept, the presentation of Flutter Live was running in a Flutter app on a laptop.This means that Flutter won’t be a mobile SDK solely, but it might become the way to go SDK for hybrid apps across mobile, web and desktop.Maybe later on, any device that can render pixels will be able to run Flutter apps.Furthermore, the Flutter team is working to provide integration between Flutter and your existing native apps.Not everyone can start from scratch, so having a way to gradually move your app to Flutter is a very welcome addition if you’re planning to do so.This project, which contains all the APIs and tooling, is named Add2App and is currently in a preview state.With Add2App, you can launch a view containing your new Flutter app from your existing native app.You can also work in the opposite way with the introduction of Platform Views.These views allow you to add native content inside your Flutter apps.Platform Views unlocks Flutter to render Google Maps and WebViews inside the Flutter app.ConclusionFlutter keeps getting better and the community keeps on growing.More and more companies start to embrace Flutterand developers are excited and positive when using Flutter in their apps.It surely looks promising that Flutter can become a big player in the mobile world.While I was getting in touch with Flutter and digging through the documentation and examples,I got more and more fascinated about all the possibilities that you can achieve with this new mobile SDK.The Flutter journey reminded me back of the feeling I had when I was working on my first mobile app,discovering a new mobile world full of possibilities, this time built out of widgets."
      },
    
      "conference-2018-12-17-devoxx-ma-html": {
        "title": "Devoxx MA 2018",
        "url": "/conference/2018/12/17/Devoxx-MA.html",
        "image": "/img/devoxx-ma-2018/devoxx-ma.png",
        "date": "17 Dec 2018",
        "category": "post, blog post, blog",
        "content": "  Devoxx MA is a yearly conference in Morocco.Previously it was held in Casablanca but for their 4th edition, xHub, the organisation behind Devoxx MA, decided to hold it in Marrakesh, in the lovely Palm Plaza Hotel on the 27th, 28th and 29th of November 2018.Aside from the conference, the speakers were also offered an exclusive trip.Four colleagues of Ordina JWorks: Yannick De Turck, Tim Ysewyn, Tom Van den Bulck and Maarten Casteels attended both the conference and the speakers trip.Three of them were also featured as speaker.In this blog post we share our impressions and experiences.Table of contents  The Speakers Dinner  VueJS Animation In Action by Charles-Philippe Bernard  MockK, The Idiomatic Mocking Framework For Kotlin by Yannick De Turck  Stream Processing Live Traffic Data with Kafka Streams by Tom Van den Bulck and Tim Ysewyn  Applying (D)DDD and CQ(R)S to Cloud Architectures with Spring Boot and Docker by Benjamin Nothdurft  Resiliency for 140 PB Cluster by Meriam Lachkar  La Keynote De La Nouvelle Generation by Saskia and Lois  The Speakers Trip in EssaouiraThe Speakers dinnerThe evening before the conference started, we had the speakers dinner sponsored by Lightbend.The dinner was held at restaurant Kasar El Hamra in the center of Marrakesh.On our way to the center we got to experience the Moroccan traffic which seemed to be pretty chaotic with all the cars and motorcycles zipping around, evading each other on the streets.Once arrived, we got treated with Moroccan dishes.One after the other, each of them truly delicious.VueJS animation in action by Charles-Philippe BernardCharles-Philippe explained why he loves VueJS so much.He criticised Angular and React because of the companies behind the frameworks, as well as the multiple variations of utilities and plugins. Instead, he prefers a community that stands as one behind the framework. Which is why he really likes VueJS as it is community-driven.He explained that for each functionality, there is exactly one solution promoted by the community.During his session, Charles-Philippe went over several libraries that he often uses for animated websites:  CSS3  Velocity.js  Animate.css  anime.js  Vanilla-tilt.jsA convenient trick that we learned during the session is how to make a JPG transparent using CSS.Note that this will only work if the image has a white background.img {    mix-blend-mode: multiply;}Be sure to check out Charles-Philippe’s amazingly animated slides.MockK, the idiomatic mocking framework for Kotlin by Yannick De TurckOur colleague, Yannick, gave a talk about MockK.MockK is a mocking framework specifically made for Kotlin. As a Java developer, he is a huge fan of Mockito for using mocks in his unit tests.When he picked up Kotlin, Mockito was also his first choice to try out.He explained however that using Mockito in a Kotlin project doesn’t go all that well due to how Mockito creates its mocks, which is by overriding classes and functions.And because Kotlin’s classes and functions are final by default, you can see that this poses some challenges.Yannick shared his experiences and mentioned that even though there is an incubating, opt-in feature to allow Mockito to mock final classes and functions, the user experience isn’t all that nice.He looked for other frameworks and stumbled upon MockK, a mocking framework created for Kotlin by Oleksiy Pylypenko.MockK’s main philosophy is offering first-class support for Kotlin features and being able to write idiomatic Kotlin code when using it.He was pretty enthusiastic about MockK and went over all its features and its ease of use.There is also a blog post written by Yannick specifically about his experiences with Mockito and MockK in Kotlin projects.Stream Processing Live Traffic Data with Kafka Streams by Tom Van den Bulck and Tim YsewynOur colleagues, Tim Ysewyn and Tom Van den Bulck, gave a talk about stream processing live traffic data with Kafka Streams.Tom presented the theoretical part of the talk starting with the bigger picture. He explained the stream processing concept which is basically computing data directly as it is produced or received.In the image above we can see an example without stream processing.In this case, the data is stored in databases and file storages.Using a scheduler, applications can retrieve and process the stored data.With stream processing the data will be processed directly as streams of events, creating other event streams for other applications if needed.The applications will react on events instead of scheduling jobs to retrieve and process data stored in databases and file storages.Following up the theoretical part, it was time for the demo.Key part of the demo was of course the data that was going to get processed.The Flemish government offers XML documents with live traffic data.These documents are created every minute so by using a scheduled job, an event could be created out of it.The theory behind all of this can be a bit abstract if you are not familiar with the concept which is why Tim first did some live coding to demonstrate how the events can be handled properly.Afterwards we learned how Spring can be used to help us with Kafka and how we can periodically fetch the data.We did this by first using pure code and secondly with the help of some convenient annotations to do the same thing with less code.At the end of the demo we created a small application that calculates the average speed for a specific sensor during a time frame of two minutes.The slides are available on SlideShare. The demo code can be found on GitHub.Applying (D)DDD and CQ(R)S to Cloud Architectures with Spring Boot and Docker by Benjamin NothdurftBenjamin started off by giving a brief introduction on Domain-Driven Design explaining the different building blocks such as domains, domain events, ubiquitous language and Event Storming.He also mentioned the famous two books: Domain-Driven Design by Eric Evans and Implementing Domain-Driven Design by Vaugn Vernon.Benjamin went through all the different steps of Event Storming.The goal is to bring people of different silos together, such as developers, analysts, architects and business experts.Together you want to create a logically ordered sequence of events to document a system using an ubiquitous language i.e. everybody using the same vocabulary and terms.Events describe things that have happened and are thus always in the past tense e.g. product added to cart.In a next step you want to identify commands, which are the triggers of events e.g. add product to cart.There are also aggregates which represent the data that is interacted with.And finally you want to identify the bounded contexts grouping relevant parts together.Benjamin then explained how this all gets translated to your system architecture.Each bounded context can be mapped to a single microservice.He covered different context map patterns such as event publisher, shared kernel, customer/supplier and anti-corruption layer together with detailed code samples.Afterwards he went through a CQRS example with many code samples and the questions you should be asking yourself when determining the right architecture.We really liked how in-depth everything was as many presentations about Domain-Driven Design usually remain rather abstract and high-level.You can check out Benjamin’s slides on slides.com.Resiliency for 140 PB Cluster by Meriam LachkarMeriam works at Criterio which is a marketing company managing Europe’s largest Hadoop cluster.Criterio uses various technologies:  Batch Processing: Map/Reduce and Apache Spark.  Stream Processing: Apache Flink and Apache Kafka.  Machine Learning: Spark ML and Tensorflow.Her talk focused on the Hadoop setup of Criterio.The current cluster in Paris has 220 PB of hard disk, 550 TB of memory and 100.000 cores.But since it is almost “full”, with 160 PB used, a new cluster has been set up in Amsterdam.Between both clusters there is a dedicated 400 Gbit fiber installed.Every day, 1 PB of data is generated.Meriam currently works at a project in which they want to synchronise the data between both clusters.This was not a trivial thing to do and the main question was how they were going to sync the data between both clusters, as copying over all data would fill the existing line for an entire day.RSync was just too slow, also some jobs are non-deterministic which means that executing the job a second time will yield a different result compared to the first time.Various options were considered:  Double run: this means that the Paris data center would still become the bottleneck and would not yield real business value  Producer push  Consumer push  Dedicated central service which will determine where the jobs will run, on which data center.A dedicated central service was the chosen solution.Codenamed “Mumak”, as it is the convention to name everything in the Hadoop ecosystem to an elephant.Dataset by dataset will be progressively moved to Amsterdam, so that jobs will eventually be distributed between both data centers.La keynote de la nouvelle generation by Saskia and LoisThe closing keynote was presented by Saskia (13) and Loïs (10) Blanc,the children of Sébastien Blanc. They started the closing keynote by asking the audience to close their eyes for a few seconds and to think about the near future and how we would imagine it.The audience had to explain what they were thinking of.Most people shared the same things such as self-driving-cars and robots to help in the household. Saskia and Loïs in turn, shared their vision of the future which pretty much aligned with ours.Saskia started the first demo by giving an introduction on the Logo programming language while Loïs was doing some live coding.A simple square was drawn first but it got more interesting as they added rotations to draw more complex figures.Loïs on the other hand explained Scratch, a program where you can create your own games, animations and interactive stories. He showed the community around the program and gave a really entertaining demo about two figures walking around.Saskia explained us that she became more interested in what her dad was doing and that she wanted to get into more real programming.Her father being a Java developer, told her about the Groovy programming language.Saskia gave a demo in Groovy and explained some basics while live coding a small program.The program consisted of an Animal interface and a Cat class that could meow.Saskia and Loïs ended their keynote mentioning that they both want to become developers of the new generation.Given their impressive presentation, we definitely see that happening!The Speakers Trip in EssaouiraAfter three interesting days of Devoxx MA, we went on the speakers trip to Essaouira, considered as one of the best anchorages of the Moroccan coast.We started off by visiting Chez Ali in Marrakesh on the last evening of the conference.As we arrived we were heartily greeted by horsemen and Moroccans playing authentic music.We got to explore Ali Baba’s cave before passing by all kinds of folkloric groups.Dinner was served in big tents and consisted of multiple courses of Moroccan dishes.After the dinner we got to watch an amazing spectacle of stuntmen on horses, belly dancers and cavaliers.The morning after, we set off to Essaouira with three buses filled of speakers and people part of the Devoxx MA organisation.During the three-hour long ride we stopped by the magnificent “goat tree” and Arganomade, where they manufactured organic argan oil all by hand and where we got to see the whole manufacturing process.In the early afternoon we arrived at Essaouira Lodge where we would be staying during the trip.After checking in and unpacking our luggage, we set off to the Essaouira beach where we had delicious sea food at restaurant Fanatic.With our hunger satisfied, we set off with our buses a bit further along the beach where we got to ride camels in caravans.With our newly allocated trusty steeds, we rode around the coast with the sun setting off in the background.After a 30-minute ride in the sand we arrived at a big tent where we would spend the rest of the evening.As dinner was being prepared in many tajines, we were entertained by live Moroccan music and dancers with a big cozy bonfire blazing about.This was truly an amazing and memorable day!The day after we went back to Essaouira beach and visited the fortress city with the walls still intact.As we strolled through the alley streets we got to visit all kinds of shops and markets.We had lunch on a roof terrace at Il Mare with an amazing view on the sea.Some more exploring was done around the city before we returned to our lodges.We spent the rest of the evening in our lodges where we invited all speakers in our lodge for a nice last evening together with a couple of Casablanca beers.As we would not have been able to catch our flight back to Belgium in time, the four of us booked a beautiful riad right in the center of Marrakesh.We explored the Djemaa el Fna together with its back alleys and visited an authentic tannery.Summing it all upDevoxx MA was amazing experience for us where we got to meet and befriend a lot of great people.We would like to thank the organisers for organising both Devoxx MA and offering us the chance to see more of Morocco with the speakers trip!The date for next year’s edition has already been announced and will happen on the 12th, 13th and 14th of November 2019.Be sure to mark it in your agenda!"
      },
    
      "agile-2018-12-13-catching-the-waves-of-servant-leadership-html": {
        "title": "Catching the waves of servant leadership",
        "url": "/agile/2018/12/13/Catching-the-waves-of-servant-leadership.html",
        "image": "/img/catching-the-waves-of-servant-leadership/main-image.png",
        "date": "13 Dec 2018",
        "category": "post, blog post, blog",
        "content": "As human beings, we’ve always had a complicated relationship with change.On the one hand, we see it all around us and try to cope with the complexity of it.Attempting to avoid or even sabotage it in order to hang on for dear life, to what we have always known to be normal, and therefore believe it to be what’s best.On the other hand, it’s what allows us to hope and strive, what drives us towards a dream for a better future.And so we look for ways to form it, to mould it, even manipulate it at times, thinking that whatever comes out of the change will be better and lasting.But does change have an outcome? Or is it endlessly moving forward? This continuous tension between coping and moulding makes our relationship with change a very personal one. Every day we strive, all of us, in this global community, to find a moment of peace with ourselves and with the ways we try to manage this uncertainty and complexity.And sometimes, when the odds are right, in a cursory moment of clear thoughts, we discover that precious balance, and see the bigger picture. That’s when we embrace change and sense the opportunity…When projecting this mechanism from the personal into the organisational area, the same elements are at play. But the personal aspect is still the crucial driver (and the key to unlock it), that can push corporate change forward or go against it like a stubborn forceful undercurrent.Tapping into that current, though, is the key to unlocking the potential of an engaged community of professionals, and so helping the organisational culture grow organically. And doing that needs a ‘feel’ for what our present corporate culture is about. But more so it requires an understanding of what the employees building our brand - and the people creating the customer value – really strive for, personally and professionally.Easier said than done, you think? Maybe so, especially when you’re working in a traditional corporate world, with siloed processes, with lots of gates and handovers, often distorting communication and slowing down value delivery. It does indeed call for a veritable paradigm shift, not just new management practices, but a management revolution; a shift towards a new approach, focused on what these value creators actually require in order to thrive in your organisation, for the benefit of your brand. Enabling them to become better, more inventive, more efficient, and allowing them to listen closely and collaborate with the customer, is the most successful way forward in our age of agile.This is of course not a one (wo)man show. Complexity is not something we can tackle with a single mind solution. Agile transformation, like cultural transformation, can only be set in motion by natural leaders, and by this I don’t necessarily mean ‘managers’. They can be emergent leaders from any level in the organisation. People who spread vibes, generate positive energy, think beyond the borders of their own roles and put their brains to work to reimagine how value is created. What defines them is that they ‘service’ their community, and that service is exactly one of their personal goals.As a manager, make sure these emergent leaders experience the freedom to act. Each one comes with a natural community, call it a ‘tribe’. Explore every nook and cranny of the company to trace them. Then help them to understand the agile strategy, and support them to instigate an agile mindset and a zest for growth, by taking small but consistent steps, clear and aligned.The organisation will be surfing their powerful waves. And with the help of these servant leaders throughout the company, you’ll be able to blow the wind in the right direction, towards the customer.  “Not one thing ever does it, it’s a series of consistent things that makes people say, ‘Aha, it’s time for change’” - Oprah WinfreyRecommended reading  Stephen Denning - The Age of Agile: How smart companies are transforming the way work gets done  Sunil Mundra - Enterprise agility: Being agile in a changing world  Seth Godin - Tribes  Em Campbell-Pretty - Tribal Unity: Getting from teams to tribes by creating a one team culture"
      },
    
      "conference-2018-12-12-dot-css-dot-js-2018-html": {
        "title": "dotCSS and dotJS",
        "url": "/conference/2018/12/12/dot-css-dot-js-2018.html",
        "image": "/img/2018-11-08-dot-css-dot-js-2018/dotjs.jpeg",
        "date": "12 Dec 2018",
        "category": "post, blog post, blog",
        "content": "Paris!This year for the first time Ordina JWorks travelled to Paris to attend a two day conference on CSS and JS, more exactly dotCSS and dotJS which are part of the dotConferences. dotConferences is a series of developer events in Paris that started in 2012.We only included talks on which we could elaborate. You can find all talks on the YouYube channel of dotConferences.Table of contents  Day 1: dotCSS          ‘Power of SVG’ by Sara Soueidan      ‘Reading Hex codes’ by David DeSandro      ‘Variable fonts’ by Mandy Michael      ‘CSS taught me…’ by Dan Cederholm      ‘Breaking the norm with creative CSS’ by Aga Naplocha        Day 2: dotJS          ‘The State of JS’ by Sacha Greif      ‘Minecraft is getting a JavaScript runtime’ by Tobias Ahlin      ‘Learning to Love Type Systems’ by Lauren Tan      ‘Choosing Your JavaScript Framework’ by John Papa      ‘JavaScript on the Desktop, Fast and Slow’ by Felix Rieseberg        ConclusionDay 1: dotCSS‘Power of SVG’ by Sara SoueidanSome websites really have nice visuals that contribute to a pleasant user experience when visiting them. Think of brands that incorporate their logos in images for instance, or cover pictures that look like some Instagram filters were applied to them. You would think that such assets are created by designers and that they are applied directly to the webpage, but there is also a way to get those Photoshop effects straight into your browser.Sara Soueidan showed us a few possibilities to apply some stunning visual effects on images and text with code. The cool thing is that when using plain text, it’s still searchable! First she discussed some techniques that designers use to create templates in Photoshop and afterwards she applied the same techniques by only using SVG. It was very impressive to see how you can manipulate images by applying a set of SVG filters. Hopefully using those SVG techniques will become more common in the future so that more visual appealing websites will be created with the power of SVG.            View talk      View slides      ‘Reading Hex codes’ by David DeSandroThis was one of the most mind blowing talks of dotCSS by far.David DeSandro begins the talk with saying he has a special ability.He can read color hex codes. At that moment the most of us were like “What?”“You can tell the color based on the hexcode?”“Is that even possible?”And well, he proved it to us that you really can tell the color based on the hex code.He even showed us how to do so.And so he told us that it requires 5 steps to read color hex codes.  3-digit shorthand  Line graph  Hue from shape  Lightness from total  Saturation from rangeJust like this I bet you have no idea what these steps mean.But let us teach you the magic of reading color hex codes as well thanks to David.To get started he taught us that the best way to describe colors is with the HSL color model.HSL stands for Hue, Saturation and Lightness. Hue is the pure pigment of a color and can be described with 12 color names.We’ll show you the color names later in Step 3.Saturation is how vibrant or muted the hue is and can be described as saturated, washed, muted or gray.Lightness speaks for itself and describes how light or dark the color is and this can be done with light, middle or dark.Now that you know how you can describe a color all you have to do is to follow the 5 steps.And deterimine the hue, lightness and saturation.So let’s get started!Step 1: 3-digit shorthandTo better understand the process let’s take #D49B25 as an example.The first step is to retrieve the 3-digit shorthand of the hexcode.This can be done easily by breaking up the hexcode in 3 pairs (D4 | 9B | 25).Keep in mind that each pair represents a value of the RGB color channel.Now drop the second number of each pair and that gives us the shorthand code #D92.Step 2: Line graphWith the shorthand from step one we have to create a linegraph based on the numbers of the hexcode.With basic understanding of hexadecimal numbers we can visualize a little line graph for the channel values.D is high, 9 is around the middle, 2 is low.And that’s how we get our litle line graph.Step 3: Hue from shapeThis is what we think is the most tricky part about reading color hex codes.With the line graph we got from the previous step you’ll have to find a matching color on the color wheel.So you’ll have to remember this one by heart if you really want to show off.For our example color, it matches best with the color orange.Step 4: Lightness from totalTo determine the lightness you should look either at the total sum of the channel values (pairs) or at the values in the line graph we created.If the values are higher to the top, the color is closer to white and thus lighter.If the values are closer to the bottom, the color is closer to black and thus darker.For our example color #D92, the values are both high and low, so it has middle lightness.Step 5: SaturationSaturation is a measure of how vibrant or rich the hue/color is.To measure the saturation whe need to look at the difference between the highest and the lowest value in our shorthand code.The wider the range, the higher the saturation. Colors with small range have low saturation, appearing faded.A color with no saturation is a pure gray.With our color, #D92, D is the highest value, 2 is the lowest.D is high. 2 is low.That’s a wide range, but not completely wide.So our color has moderate saturation, thus making it a washed color.Now we have all three attributes for our colorSo we can say #D49B25 is Middle Washed Orange.Pretty amazing right?Since dotCss puts most of its talks online we definitely recommend watching this talk.He just explains it so well and goes a little bit deeper than we did here.            View talk      View slides      ‘Variable fonts’ by Mandy MichaelA variable font is an OpenType font format that includes new technology called OpenType Font Variations. Jointly developed by four of the most influential technology companies — Google, Apple, Microsoft, and Adobe — these font files contain a huge amount of extra data, compared to your average OpenType font. A variable font can contain a font’s entire glyph set, or individual glyphs with up to 64,000 axes of variation, including weight, width, slant, and, in some cases, specific styles, such as Condensed, Bold, etc.There are two main advantages when using variable fonts:  The average file size is smaller than separate font files  Only one request is necessary to load the necessary font variationsA good example is the font Source Sans Variable or the font Decovar.She also showed us the possibilities of variable fonts and what kind of awesome stuff you can do with them.Did you know that you can make text animations with them?Well you can, so check it out on her codepen.io collection.You can easily check for browser support using the @supports CSS rule: @supports (font-variation-settings).‘CSS taught me…’ by Dan CederholmDan talked about all the things he learned in his lifelong journey of working with CSS.But in the end it all came down to this:  Have side projects to keep your skills sharp  It’s OK not to use the latest and greatest in business‘Breaking the norm with creative CSS’ by Aga NaplochaThe last presentation of the dotCSS conference was about using other inspirations and tools to build webpages. Aga made a bold remark that most of the websites out there have the same structure and even look and feel the same. As a big fan of Brutalist Design she showed some examples to demonstrate what is possible when you think out of the box.With her talk she wanted to encourage developers to try and use other CSS Properties for building and designing websites. The three CSS properties she mentioned were clipping, masking and shape-outside (all in combination with SVG — using masking with images is resource intensive since it operates pixel per pixel). She showed each property with a clear example and discussed the differences between them, and mentioned the compatibility of each property in the different browsers.Saying that most of the websites have the same look and feel is a bold statement but after you look around for a while you notice that she actually has a point. Most of the websites are using frameworks that give you a uniform, recognizable look and feel but are also really easy to use. With the properties she mentioned you can certainly build a beautiful, well-designed website but it would take more time to make sure you have a responsive website that has the same look and feel over the different browsers and devices.Day 2: dotJS‘The State of JS’ by Sacha GreifAs we all know, the JavaScript ecosystem is richer than ever, and even the most experienced developers become victims of the amount of choices they have to make.It’s always changing. New libraries, frameworks, languagues…For this reason Sacha Greif, Raphaël Benitte and Michael Rambeau decided to create a global survey they called the ‘State of JavaScript’.The survey contains data from over 20.000 developers from all around the globe and you can find the results on their website.The survey tries to figure out what these developers are using these days, what they like and what they would love to learn.When they published the State of JS results of 2018 there was a lot of commotion regarding the results.More specifically regarding the front end frameworks Angular, React and Vue.The State of JS survey declares that Angular is suffering a lot in comparison with React and Vue.The Angular results are very disappointing and show us a high rate of dissatisfaction within its userbase.On the other side there is only love for both React and Vue.This lead to a war on Twitter and other channels saying the survey is flawed.Olivier Combe who is a member of the core team called out to Sasha why they didn’t make the distinction between AngularJS and Angular.It makes total sense a lot of people are using Angular (read v2+) now and are not using AngularJS any longer.There even is a YouTube Video that takes a closer look at the results.In any case, we believe this survey can help lots of developers make choices and it gives at least some insights on the State of JavaScript.Besides the front end frameworks, the State of JS also offers lots of data of other subjects regarding JavaScript which are definitely worth checking out!‘Minecraft is getting a JavaScript runtime’ by Tobias AhlinTobias blew our mind with the message that Minecraft has a HTML based UI that is using a JavaScript runtime.Just think about it: a JavaScript runtime running on top of your games.The JS runtime provides access to UI elements and an API to give developers enough freedom to work with the interface.Worth mentioning is that this is completely mobile optimized and is based on flexbox.Using floats is just too expensive on processing power.‘Learning to Love Type Systems’ by Lauren TanSince the introduction of TypeScript in 2012, web development has consistently looked more towards building software with the use of static typing. Lauren discussed why we should embrace the use of optional static typing that TypeScript provides.By using types, we add more constraints to our code and how other developers can use our code. Thus this decreases the amount of possible bugs during the development phases of a project. According to Lauren, types are mathematical propositions on how a program should work and the written code that complies to the types is a proof of the type system.Lauren explained that while using types we should be as strict as possible. The lower cardinality we have in our types, the less bugs will occur as we limit the possible inputs to our functions.With TypeScript it’s convenient to use a type such as any but we’re better off avoiding using the any type. By consistently being as strict as possible in our type usage, we facilitate better integration between different developers and teams. It is definitely true that stricter typing helps with defining the limits of your code while also documenting your code indirectly.            View talk      View slides      ‘Choosing Your JavaScript Framework’ by John PapaWith all the current JavaScript frameworks out there it’s hard to pick one to work with. During his talk John Papa took a closer look to the three most popular frameworks at the moment: Angular, React and Vue.When choosing the framework for your project you could ask yourself questions like “Does it have all the features that I need?”, “What about the documentation?”, “Is it backed by a strong community?” or “How fast is the framework?”.In this case all three frameworks would apply. But they all have a different way to work with components, lazy loading, state management and other stuff.Take for example the language it’s written in. With Angular comes TypeScript, but React and Vue are by default JavaScript. Do you fancy TypeScript but you don’t like to work with Angular? No issue, you can perfectly use TypeScript in React and Vue.They are all perfect for building successful applications and you can go on for hours on which one is the best.But in the end it comes down to one question: “How does the framework make you feel when you use it?”.The only way to find out is by trying each one. That’s why John Papa created a Tour of Heroes project for each framework.  Tour of Heroes with Angular  Tour of Heroes with React  Tour of Heroes with VueDo you wanna know which one he prefers? Well, he did not want to reveal that during the talk…            View talk      View slides      ‘JavaScript on the Desktop, Fast and Slow’ by Felix RiesebergJavascript is everywhere today, even in desktop applications e.g. Battlefield 1, Nvidia GeForce Experience and Adobe Creative Suite. Felix reveals his four tricks that can make your application more efficient.Before importing a specific module, really consider if you need all of its functionalities as it might bring in some additional items that you don’t need. When you do require a specific module, make sure you embed it at the right place.When using Node.js - which uses the V8 Javascript engine - the engine runs every time you build and compiles your code into something your machine can execute. You can easily cache this with the module v8-compile-cache.Repainting the screen is an expensive operation. You can check with Chrome Developer Tools how much this operation costs and maybe tweak some code.His third trick is that not all code is equal. Sometimes there is a more efficient way to structure your code or use different function calls with the same end result that could make your application faster. Using getElementByClassName instead of querySelectorAll for example would be 99% faster.And to close off, application lifecycle.If your application is minimized, pause the network requests and stop refreshing every couple of seconds. You can check this with document.hidden.            View talk      View slides      ConclusionWe had an interesting two days at dotCSS and dotJS, and had a lot of fun engaging with the wonderful people at the conference.One thing we noticed is that the talks at dotCSS were way more technical than the ones given at dotJS and somehow that made us feel a little bit disappointed.Nonetheless we learned a lot of things and had a great time visiting the city of love."
      },
    
      "development-2018-12-11-stairway-to-health-html": {
        "title": "Visualising IoT data with tableau",
        "url": "/development/2018/12/11/stairway-to-health.html",
        "image": "/img/2018-11-16-stairway-to-health/stairway-to-health.png",
        "date": "11 Dec 2018",
        "category": "post, blog post, blog",
        "content": "VisionWorks meets JWorks: StairwayToHealthWe, from VisionWorks, were asked to rebuild the visualisation dashboard JWorks used in the application they built as a result of the internal Stairway to Health project (you can find more information about that project here). We decided to use Tableau, a popular BI Visualisation tool we largely use at our clients.We developed the dashboard working around some key questions while keeping the appearance of the dashboard in line with the dashboard JWorks developed.In the following section we will explain how the dashboard is currently set up and how to use it properly.Next, we will go over the features we can add in future releases to allow the user to go even deeper in their analysis.Dashboard overviewThe dashboard is built to answer the following questions:  What is the percentage of people taking the stairs or elevator at Ordina today?  How is the same metric on weekly, monthly or yearly basis? What is it in absolute numbers?  How does it evolve over time based on each day, week, month or year?  Are people taking the stairs more this week compared to last week?The dashboard will try to provide answers to these questions using the following three main parts.We will go over these parts and highlight which question(s) they try to answer.Part one: the TitleThe title is what the user sees first and answers the first question.By using the colors in the title, the dashboard shows the user - in a subtle way - what the colors in the next visuals represent.There is also an option to select another day as illustrated below.Part two: the horizontal bar comparisonIn part two the user can find an answer to questions two and three.The visual uses the selected day to show the division between people taking the stairs / elevator on a daily / weekly / monthly and yearly basis.When the user hovers over the chart he can also see the evolution of people taking the stairs / elevator within that day / week / month / year.Next to the chart, the total absolute number of all the observations measured is reported per period.Part three: the more detailed area chartThe third part visualises how the division stairs / elevator is evolving over time expressed in daily, weekly, monthly or yearly basis.This gives the user the possibility to look at trends and to see how the situation of today compares itself to past situations.In the title the user has the option to change the appearance of the data (absolute or shares).The amount of periods shown (starting from the most recent period) can also be changed.When the user hovers over the chart the same horizontal bar comparison can be seen. Comparisons can be made with the period selected above.Last feature to discuss here is how the user can change which period the chart is showing.This can be done by clicking the chart above.When you click on the day bar on the top chart the bottom chart is expressed on a day level.This also applies to the other period bases in the chart.What can we do next?While this dashboard already gives an answer to the most important questions and gives the user the possibility to explore the data over time, there are still some extra things that can be developed.The dashboard is currently built within a Tableau workbook which is using the data of the MongoDB database JWorks set up as an extract.This means we don’t have a live connection to the actual database JWorks has in their app.This brings us to the first thing we can still explore: deployment.In order to integrate the dashboard in the original application, we could publish the dashboard on the Tableau server of Ordina which is running on Microsoft Azure.Running this instance is not free so when taking a decision we should also take the user relevance in consideration: does the user really need to have a live connection to the data or does a nightly update cover the load?Secondly we can still do a lot on the analysis part. What are the reasons why some patterns in the data exist? Do people take the stairs less when it is hot outside?JWorks recently tracked on which floor the observation is measured, allowing us to look into difference by floor. Do people take the elevator more when they need to go from floor 1 to floor 3?We will keep you posted on further progress related to Stairway to Health. Thank you for reading and don’t forget: always take the stairs!"
      },
    
      "iot-20machine-20learning-2018-12-10-10-cool-ai-examples-html": {
        "title": "10 Cool AI/ML Examples",
        "url": "/iot,%20machine%20learning/2018/12/10/10-cool-ai-examples.html",
        "image": "/img/2018-12-10-AI10EX/banner.jpg",
        "date": "10 Dec 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  10 Cool examples  ResourcesIntroductionTo end the year on a lighter more inspirational note we’ll go into 10 cool examples achieved with artificial intelligence.These are all short videos with some additional information, most of these already have or will soon have an impact on our lives one way or another.The 10 examples can be divided in to three categories:How the models work and perceive the world  How neural networks see the world  Attack on human vision systemAudiovisual models for rendering, photography and impersonation  Style transfer for videos  Amazing night time photographs  Nvidia AI based image restoration  Noise reduction for path traced GI  Isolate speech signals  Impersonate anyoneModels used for and helped by gaming  Deepmind becomes superhuman in quake 3  Using games for deep learning researchFor now, there is still no need to fear Skynet becoming a reality.While progress in the artificial intelligence world proceeds at a staggering pace, we are no where near having a general ‘super’ AI.This however does not mean precautions do not need to be taken to prevent this from happening in the long run.Some people, like Elon Musk, are very vocal about this and question if we should even pursue the goal to create a ‘super’ AI.10 Cool examplesBelow are the ten selected examples we think you should see!In the resources section underneath all of them you can find more useful resources to use and watch about AI/ML.Example 1 - How neural networks see the world    Understanding a neural network is difficult, we don’t actually know what is happening inside of it.We need ways of visualizing and understanding what happens inside to help debug and improve these networks.For convolutional neural networks this helps us see what the network sees and how it identifies and uses parts of the input to get to the desired output.Example 2 - Attack on human vision system    Not only artificial neural networks are vulnerable to attacks to fool them.Our very own brain, a neural network as well, is also able to be tricked by some of these techniques.This video shows how such an attack works.    This video shows how neural networks can be fooled by changes to the input as small as a single pixel!It shows that caution needs to be taken in neural network based image recognition because a sufficiently witty/crafty attacker could fool the system by employing such an attack.Example 3 - Style transfer for videos    Style transfer is when the style of a given input image is transferred to a secondary input image while maintaining the content of that image but with the style of the first input.This gives you the option to apply the style of certain great works of art to regular images or even works with a totally different style.In this video the technique is applied to video content, but it is not just as simple as running the earlier technique on each frame of the video since it does not provide a result that is temporally coherent.The video is styled based on a given input and produces quite amazing results.Example 4 - Amazing night time photographs    Very noisy night time images might soon be a thing of the past.The technique in this video can turn unusably noisy photos into perfectly viewable photos.Something like this has been implemented in the google pixel phones recently.In a few years all cameras will have a mode like this implemented making unusable night shots a thing of the past!Example 5 - Nvidia AI based image restoration    Like the example above this is about denoising and is similar yet different.This AI has been trained without ever having been shown what noise is, so no before vs after comparison.It can remove noise from images, restore images that are almost only visible noise and even remove lots of text from a given image.This technique will make cleaning up images much easier and allow us to preserve and restore imagery that might otherwise be lost or unusable!Example 6 - Noise reduction for path traced GI    This video shows that denoising techniques can have other great benefits in the visual/gaming industry.Path traced global illumination (casting light in a 3D scene to determine lighting from a global source like the sun) is a very resource intensive task.Current solutions use all sorts of tricks to mimic this but they are not the real deal.This technique allows for path traced GI with a very low sample count and denoises the output whilst being temporally stable.Something like the new cards from Nvidia are now capable of!Example 7 - Isolate speech signals    Having an audio or video file with multiple people speaking at once or when there is a lot of background noise can be annoying for various reasons.It makes it harder to understand any of the speaking parties.This technique allows each speaker’s audio to be isolated and listened to without hearing the other sources of interference.It is helpful to clean up conversations or remove background noise.Example 8 - Impersonate anyone    Soon you will not be able to tell that what you see is actually what happened.This advanced technique improves on older versions, and allows you to transfer your facial and torso movements onto a target.Techniques like this make it clear that fake news and fake sources of media will become an even bigger problem in the future as this technology becomes even better.It might not be such a bad idea to invest in that blockchain backed media repository after all so the validity of media files can be tracked…Example 9 - Deepmind becomes superhuman in quake 3    In games you normally play against the AI.These AI’s are mostly cheaters though, they know more because they are fed insider information from the game itself.The AI’s in this video are actual players that only get the video output of the game and learn to play accordingly.This in the long run will allow games to have decent real AI in-game.Other sectors can also benefit from this as it can be applied to different fields where complex behavior with tactics and long term planning is required.Example 10 - Using games for deep learning research    Self driving cars are all the rage these days.Getting cars to drive themselves is an immensely complex task, requiring truly vast amounts of correctly classified data in a dataset.Classifying this data is a very time consuming process.This technique can use games like GTAV to create a dataset with imagery from the game.The game already knows what all the types of objects are in the scene, so classification can be simplified and automated.It also provides an easy way to simulate hard to recreate situations in real life.Time of day and scene composition can be easily changed which results in a vastly more extensive dataset.ResourcesA very good video to watch is How machines learn by CGP Gray.It generally explains how machine learning works and what some of the implied dangers are.All the videos used in this blogpost are from the the Two minute papers YouTube channel.This channel has short videos that showcase some scientific research in a visual and compelling way whilst not going too technical but still providing all the technical resources for those who want it.Lastly is the playlist about neural networks by 3Blue1Brown.It goes into how neural networks work and is very visual which helps greatly with understanding the subject matter.  How machines learn - CGP Gray  Two minute papers youtube channel  Neural networks playlist - 3blue1brownAll these videos and the accompanying channels on YouTube are from amazing content creators, all rights for the content goes to them.Do like I do and subscribe to these awesome channels to support them!"
      },
    
      "development-2018-12-06-mongodb-transactions-html": {
        "title": "Transactions in MongoDB 4.0",
        "url": "/development/2018/12/06/mongodb-transactions.html",
        "image": "/img/2018-11-08-mongodb-europe-2018/mongodb-acid-logo-thumb.png",
        "date": "06 Dec 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Relational vs. document database  When to use transactions?  Technical details  Practical  Best Practices  ConclusionIntroduction  How and When to Use Multi-Document Distributed Transactions, Aly Cabral  MongoDB: Building a New Transactional Model, Keith BosticThe sessions I was looking forward to the most at MongoDB Europe 2018 were the two sessions about multi-document transactions, the most talked about feature of the MongoDB 4.0 release.In the morning I attended a session by Aly Cabral which was very practically oriented.In the afternoon there was a more esoteric yet very interesting session by Keith Bostic who provided some insight into the inner workings of the WiredTiger storage engine and the difficulties the MongoDB team had to overcome to implement the new transaction model.To give some background, as a longtime Oracle DBA, I always found it odd that a database would lack what I had always considered a crucial database feature, so I was naturally curious to know more about MongoDB’s implementation and how it would compare to a typical relational database.In this post we will explore how multi-document transactions are implemented in MongoDB, how the implementation is similar to a relational database system and where they differ.Relational vs. document databaseAs I learned through working with MongoDB the past two years, there is less need for multi-document transactions in document databases.For the majority of use cases single document transactions suffice.This is because the data model you use with a document database is quite different from what you would use with an RDBMS.In a relational database system you typically normalize data in order to avoid duplication.A single entity more often than not has data spanning multiple tables, so when you perform updates to a single entity you have to update multiple rows in multiple tables concurrently, which necessitates transactions.In a document database like MongoDB though, you typically embed all data that represents one entity within a single document, in which case updates to that entity are also limited to a single document.You can see that there’s less need for transactions.In essence, the RDBMS approach prioritizes disk space efficiency above everything else whereas the MongoDB approach prioritizes ease of development and simplicity.Nevertheless, there are some scenarios where you may want to use multi-document transactions in a document database.Let’s see what some of those scenarios might be…When to use transactions?RelationshipsIn the case that your datamodel does have relationships between separate entitites, you may want to use transactions to update both of them at the same time.An example of this could be a customer and a car that the customer owns.They are distinct entities, but there is a relationship between them in the form of ownership information.If you update the ownership information on the customer document, you probably need to update it on the car document (or documents) as well.The only way to do this with guaranteed consistency is through a multi-document transaction.Event processingAnother use case of transactions is event processing.When a certain event occurs, it may need to atomically create, update or delete several entities at the same time.The example that was given in Aly’s presentation was the creation or invalidation of a customer’s account, which would require an update to all of the customer’s entities.Event logging or auditingConsider the case where, for logging or auditing purposes, you want to create an event trail of all changes that happen to a certain document or collection and you want to store this event trail in another collection.The event trail should be representative of what really occurred, so events that never occurred should not be logged nor should events that actually happened be lost.The only way to achieve this is to put the update and the logging of the update inside the same transaction.Technical detailsNow let’s explore some of the more technical features of multi-document transactions.AtomicityThis is pretty straightforward.In MongoDB transactions are atomic, which means that execution of multiple changes inside a transaction is an all-or-nothing deal: either all updates get committed, or none.Snapshot isolationWhen you start a transaction, MongoDB creates a snapshot of the current state of the database.During your transaction you will not see any updates made by other sessions.You are isolated from them.This guarantees that throughout the transaction your session will see one consistent version of the data.Internally MongoDB uses an update structure inside the WiredTiger cache to maintain this consistent view on the database.This structure grows as writes occur to the database and is only evicted from the database once the transaction is committed or aborted.The implication of this is that long running transactions or a high write volume can put pressure on the cache.It’s therefore recommended to keep the duration of any transaction as low as possible.To minimize cache pressure, you can use the server parameter transactionLifeTimeLimitSeconds to set a sensible maximum transaction time.If a transaction runs for a longer time than this value, it will be aborted.The default value of transactionLifeTimeLimitSeconds is 60 seconds.A sidenote for those DBAs or developers who are already familiar with Oracle: the update structure in the WiredTiger cache is similar to how rollback segments and undo tablespaces work in Oracle.The differences are that the snapshot information is kept entirely in memory instead of on disk, and that you don’t have to actively manage it by allocating a tablespace for it.Read your own writesMongoDB guarantees that you can read any writes you make inside your transaction, even before they are committed.It also guarantees that no other session can read your writes before they are committed.Again, these writes are handled by the snapshot structure inside the WiredTiger cache.Write locksWhen two sessions are trying to update the same document at the same time, you get a write conflict.In MongoDB this conflict is handled by write locks.There are basically two conflict scenarios:      Before a transaction updates a document, it will try to acquire a write lock.If the document is already locked the transaction will fail.        Before a non-transactional operation tries to update a document, it will try to acquire a write lock.If the document is already locked, the operation will back off and retry until MaxTimeMS is reached.  Note that reads never block writes.MongoDB is also smart enough to recognize a so called no-op write: if the document you are trying to update was not changed by the update, it will not attempt to acquire a write lock.LimitationsCurrently you can only use multi-document transactions with replica sets.Sharded clusters are not supported yet, though this feature is planned for a future release (4.1 perhaps?).Due to the WiredTiger cache pressure, long running transactions can be problematic. The MongoDB developers are working on improving this for future releases and plan to support transactions running for several hours or even days.PracticalAs for semantics, the MongoDB developers thankfully chose not to reinvent the wheel.Using transactions is similar to what most developers are used to on relational database systems.The precise syntax varies per programming language, so you will have to do some RTFM to learn it, but it always comes down to the following steps:  Open session  Start transaction  Update multiple documents  Commit or abort transactionFor example, in Mongo Shell syntax a transaction typically looks like this:mySession = db.getMongo().startSession();mySession.startTransaction();mySession.getDatabase(\"mydb\").coll1.insert({\"foo\" : \"bar\"});mySession.getDatabase(\"mydb\").coll2.insert({\"hello\" : \"world\"});mySession.commitTransaction();One important thing to note here is that once we open the session on the first line, every subsequent action must use the session variable (“mySession” in this case), otherwise they will be simple update operations not belonging to the transaction.Best practicesFinally, here are some best practices we learned in the session:  Don’t change your data modeling rules because of transactions.For example: don’t start normalizing data  Transactions shouldn’t be the most common operation.If they are, you’re doing it wrong.  Pass session information to all statements inside your transaction.  Implement retry logic.MongoDB returns errorcodes that tell you if a transaction has failed and if it failed with a retryable error or not.  To reduce WiredTiger cache pressure, keep transactions short and don’t leave them open, even read only transactions.  Take into account that long running DDL operations (e.g. createIndex() ) block transactions and vice versa.ConclusionMulti-document transactions are a useful and easy to use addition to MongoDB.They make MongoDB a better general purpose database and a stronger alternative for applications where you would traditionally have to choose a relational database."
      },
    
      "testing-2018-11-21-sse-spring-node-dev-ci-html": {
        "title": "Mocking server sent events: Development and CI",
        "url": "/testing/2018/11/21/sse-spring-node-dev-ci.html",
        "image": "/img/2018-11-21-sse-spring-node-dev-ci/sse-front.png",
        "date": "21 Nov 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Intro  What are Server-Sent Events  Java  Nodejs  Angular  Continuous Integration  ConclusionIntroI came across this topic during some consultancy a few months ago, and again a few weeks ago.As I stated in my previous blogpost about mocking a backend (Node-RED: Development and CI), we don’t live in an ideal world.Backends are not always finished before frontend development starts and personally I hate it when I have to include mock data into my frontend code.And again, even if that backend feature is finished and deployed somewhere so we don’t need to run it locally, sometimes you have less control over messages sent from the backend that need to trigger events in the frontend.For both of those projects, a use case arose where the system was in need of messages sent from the backend to the frontend, based on purely frontend and backend events.On older technologies and systems, these problems were solved with a polling mechanism.Every few seconds, the frontend is querying the backend for updates.The first technology that comes to mind when reading the specifications are Websockets.A websocket is a bidirectional TCP connection opened between 2 entities, in our case a frontend and our backend.Messages can get sent by a client to the backend, or the other way around.For more information about websockets a simple Google search will overload you with information and frameworks for Java, Javascript and others.For Javascript, take a look at  Socket.io.In our use case, we were only in need of unidirectional streaming, Server-Sent Events or in short SSE.Again, the goal was not to implement the backend, but to come up with an easy to implement mock that can be used during development by our frontend developers, and could get reused in testing the frontend against this mock backend.Ideally, this demo code could get reused by our backend developers as an example.Although Node-RED has add-ons for SSE, I decided to start writing one myself.Note: In real systems, multiple clients can connect to the backend and open a channel.What are Server-Sent Events  Server-Sent Events is a technology for enabling unidirectional messaging over HTTP. The EventSource API is standardized and part of HTML5.In our use case, the backend should be able to send messages to its clients at any time.These messages can get triggered by client-side events (over REST) or even triggers from external resources and queues or database changes.  To make SSE work, we need to keep some things in mind.The logical flow behind it is pretty straight forward.A client requests a channel by GET-ting a resource over REST.In Javascript you can make use of the EventSource API.A backend should respond with some specific headers:  Content-Type -&gt; ‘text/event-stream’  Cache-Control -&gt; ‘no-cache’  Connection -&gt; ‘keep-alive’This way, the connection between the client and backend is kept open.At any time, the backend can send a message (event) through this tunnel to the client.We will go a bit deeper into each section later.You can read more about the specs on W3schools and W3.JavaAround a year ago, Dieter Hubau wrote a blogpost about Spring Cloud Stream and ‘a’ microverse of Rick and Morty. He implemented SSE using org.springframework.web.servlet.mvc.method.annotation.SseEmitter.I figured, that’s a place to start.SpringStart by generating a Spring Boot application with some dependencies.Navigate to Spring initializr.Add data-repository, flyway and h2.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;    &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.flywaydb&lt;/groupId&gt;    &lt;artifactId&gt;flyway-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.h2database&lt;/groupId&gt;    &lt;artifactId&gt;h2&lt;/artifactId&gt;    &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;I’ve added the Flyway and H2 dependencies because I’ve generated test data online (sql).I’ve created an easy model which represents a message (notification), and maps to a database table, which can be sent to the frontend.@Entity@Table(name = \"notification\")public class Notification {\t@Id\t@GeneratedValue\t@Column(name = \"id\")\tprivate Long id;\t@Column(name = \"title\")\tprivate String title;\t@Column(name = \"message\")\tprivate String message;\tpublic Long getId() {return id;}\tpublic void setId(Long id) {this.id = id;}\tpublic String getTitle() {return title;}\tpublic void setTitle(String title) {this.title = title;}\tpublic String getMessage() {return message;}\tpublic void setMessage(String message) {this.message = message;}}I’ve created a custom CrudRepository&lt;Notification, Long&gt;:public interface NotificationRepository extends CrudRepository&lt;Notification, Long&gt; {    ArrayList&lt;Notification&gt; findAll();    Optional&lt;Notification&gt; findById(Long id);}And a basic service:@Servicepublic class NotificationService {    @Autowired    private NotificationRepository notificationRepository;    public ArrayList&lt;Notification&gt; getAll() {        return this.notificationRepository.findAll();    }    public Notification get(Long id) throws EntityNotFoundException {        Optional&lt;Notification&gt; notification = this.notificationRepository.findById(id);        if (notification.isPresent()) {            return notification.get();        } else {            throw new EntityNotFoundException();        }    }}Most logic is implemented in the Controller:@RestController@RequestMapping(\"/notification\")public class NotificationController {    private final List&lt;SseEmitter&gt; emitters = new ArrayList&lt;&gt;();    @Autowired    private NotificationService notificationService;    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)    public SseEmitter events() {        SseEmitter emitter = new SseEmitter();        emitters.add(emitter);        emitter.onCompletion(() -&gt; {            emitters.remove(emitter);        });        emitter.onError(throwable -&gt; {            emitters.remove(emitter);        });        emitter.onTimeout(() -&gt; {            emitters.remove(emitter);        });        return emitter;    }    private void handleNotification(Notification notification) {        emitters.parallelStream().forEach(emitter -&gt; {            try {                emitter.send(notification);            } catch (IOException e) {                emitter.complete();            }        });    }    @Scheduled(fixedDelay = 2000)    public void receiveNotification() {        this.handleNotification(this.notificationService.get((long) (Math.random() * (100 - 1)) + 1));    }}The logic behind the code is again pretty straightforward.Querying this resource will respond with the correct headers (Content-Type -&gt; MediaType.TEXT_EVENT_STREAM_VALUE == ‘text/event-stream’) en open an event stream.This resource will create an SseEmitter for each request and add that emitter to a list.When an event needs to be sent out to the clients, you can then just loop over that list of emitters and send that event.If you loop at the example code, you can see that the emitter itself has some callbacks (completion, error, timeout, …).You can use those function for implementing a specific error strategy, monitoring and logging.For development purposes, I’ve added a @Scheduled-function that will fire every two seconds and send a random notification from the database through each emitter.For one of my clients, it wasn’t possible to work with Spring.A Google search resulted in a lot of other solutions for Java implementations of sse.  SseEventSink, SseEventSource  SseFeature  EventSourceServlet  …NodejsAlthough the Java implementation wasn’t finished yet, another problem arose.Not all of our frontend developers where happy with this approach.They still needed to run a simple Java backend, even if it was a simple Docker container.So I switched to a Nodejs implementation using Express as a webserver.Express doesn’t come with an SSE-feature out of the box, but there are plugins you can use:  sse-express  express-sse  …But instead of using a library, I’ve implemented my own middleware.Writing custom middleware is very easy and well documented in the docs.sse-middleware.js:sse_middleware = function (req, res, next) {    res.sseSetup = function() {      req.socket.setTimeout(0);      req.socket.setNoDelay(true);      req.socket.setKeepAlive(true);      res.setHeader('Content-Type', 'text/event-stream');      res.setHeader('Cache-Control', 'no-cache');      res.setHeader('Connection', 'keep-alive');      res.statusCode = 200;       }    res.sseSend = function(data) {      res.write(JSON.stringify(data));    }    res.sseOnClose = function(callback) {      req.on(\"onClose\", callback);    }    next()}module.exports = sse_middleware;As mentioned before, to make SSE work, you need to set the right headers (cfr. MediaType.TEXT_EVENT_STREAM_VALUE).I’ve implemented this in the setup of the custom middleware.Besides this initialization, I’ve also implemented an sseSend-function, for sending messages over the channel, and an onClose-callback that will fire whenever the connection closes.Instead of using an in-memory database, like I did in the Java part of this post, I decided to go with a basic Javascript file that I can switch later to a simple json-file with test data.database.js:var database = {    notifications: [        {type: 'test', title: \"TEST\", message: \"testmessage\"},        ...,        {type: 'test', title: \"TEST2\", message: \"testmessag2\"}    ],    updates: [        {              entity: 'contact',            data: {                id:'123456'                email:'contact123456@gmail.com'            }         },         ...,         {               entity: 'company',             data: {                 id:'123456'                 tel1:'+3234457645'             }         }    ]}module.exports = database;This time, I added different kinds of data lists to my mock data.Depending on specific parameters, you can then choose to send back a different type of event.Now, let us take a look at the server implementation.server.js:var express = require('express');var sse_middleware = require('./sse-middleware');var database = require('./database');var DATA_LENGTH = 10;var app = express();app.use(sse_middleware);var channels = [];var interval;function start() {  interval = setInterval(() =&gt; {    let data = this.createMockEvent(); // to implement yourself    for(let key in channels) {      if(channels.hasOwnProperty(key)) {        channels[key].sseSend(data); // console.log('Emitting to ' + key);      }    }  }, 2000);}app.get('/stream', function(req, res) {  console.log(\"New subscriber request\");  res.sseSetup();  channels.push(res);  res.sseSend(\"Connection open\"); // if you want to send feedback for opening connection  // res.sendStatus(200);  res.sseOnClose(()=&gt; {     // implement your own strategy for removing a channel  })})app.listen(8080, function() {  console.log('Listening on port 8080...');  start();})In the first lines, I just import my mock database and the middleware.I then initialize the express-app and tell it to use the middleware, app.use(sse_middleware);.When the server is started, the app also starts a simple interval that will produce a random (or fixed order for testing purposes) event each two seconds.To start this service:$ node server.jsTo test it, you can just open your browser and navigate to http://localhost:8080/stream.You should be able to see events appearing now.However, there is a catch, and it took me some time to figure out what was going wrong.In your browser you can see the content of the events, but if you run $ curl -X GET http://localhost:8080/stream you won’t see anything.However, if you would start the Java app, you’ll see the events appearing in your browser, and during your curl-session.The reason for this, lays in the specs of Server-Sent Events.  As you can see, a message expects a data field.Adjusting the send-method in the middleware will fix this problem:res.write('data:' + JSON.stringify(data) + \"\\n\\n\"));You can also add the other fields, just separate them with \\n\\n;For development purposes, it isn’t a bad idea to add a start en stop action for managing the interval.Just add the following to your server:app.get('/start', function(req, res) {  console.log(\"Starting stream\");  start();  res.sendStatus(200);});app.get('/stop', function(req, res) {  console.log(\"Stopping stream\");  clearInterval(interval);  res.sendStatus(200);});So you can start and stop the stream by triggering a REST-endpoint.$ curl -X GET http://localhost:8080/start to start the stream of events.`$ curl -X GET http://localhost:8080/stop to stop the stream of events.`AngularFrontend SSEThe frontend is an Angular 7 app, created with the angular-cli.Because of reusability the server-sent event receiver feature is bundled in a separate module that can get moved to a shared library later.In the most simple implementation, you only need a service to handle the connection and forward events to other components.In this service, you can make use of the EventSource API of plain javascript.  The API comes with an easy constructor and 3 callbacks:  EventSource.onerror  EventSource.onmessage  EventSource.onopensse.service.ts:import ...@Injectable({  providedIn: 'root'})export class SseService {  readonly url = 'api/stream';  private _eventSource: EventSource;  private _open: boolean;  constructor(private _http: HttpClient) {    this.init();  }  public init(): void {    this._eventSource = new EventSource(this.url);    this._eventSource.onmessage = (evt) =&gt; this._onMessage(evt);    this._eventSource.onerror   = (evt) =&gt; this._onError(evt);    this._eventSource.onopen = (evt) =&gt; this._onOpen(evt);  }  private _onMessage(message: MessageEvent): void {    this._handleEvent(JSON.parse(message.data));  }  private _onError(evt: MessageEvent): void {    console.log(\"Error:\");    console.log(evt);    // implement your own strategy for reconnection  }  private _onOpen(evt: MessageEvent): void {    console.log(\"Open:\");    console.log(evt);  }  private _handleEvent(event: MessageEvent): void {      // e.g. dispatch to ngrx store  }}You’ll notice that the url used is not mapping on the mock backend.For local development and testing, this doesn’t matter.Even if both paths would match, the user interface and backend can’t run both on the same port (http://localhost:8080 vs http://localhost:4200 (standard cli port for $ ng serve)).Requesting resource cross domain will result in CORS issues. A proxy to the rescue!ProxyTo overcome the CORS problems, angular-cli, the serve-command to be more precise, comes with an optional parameter to add a proxy configuration.In our production ready setup, all calls to /api to the same (sub)domain as where the user interface is getting served, get routed to the REST-API.Because we don’t want to add dev or test specific code in the app itself, we proxy the /api to our mock backend.Example given:proxy.config.json{    \"/api/*\": {        \"target\": \"http://localhost:8080/\",        \"secure\": false,        \"logLevel\": \"debug\",        \"changeOrigin\": true,        \"pathRewrite\": {\"^/api\": \"\"}    }}To use this proxy, serve the app with:$ ng serve --proxy-config proxy.config.jsonIf you take a look at the logs, you can see the system is logging the routes in the console.Frontend + BackendIf you want to run the mock backend (Nodejs) along with the frontend, you need to be able to run concurrent tasks.You can do this in a node environment using the concurrently-package.Just install it by running $ npm i --save-dev concurrently.Add an entry in the package.json scripts section:\"start:proxy\": \"concurrently \\\"ng serve --proxy-config proxy.config.json\\\" \\\"node path/to/your/server.js \\\"\"Because they are both starting at the same time, it might happen your backend is not ready while your frontend starts connecting to the stream.A good retry strategy will help you overcome this problem, that can also happen in real life systems as well.Continuous IntegrationAs mentioned before, this whole approach should result in a mock that can be used for testing as well.In one of our systems, we have a lot of different event types.Some only need to show a notification on screen, while others need to refresh data in a cached object, or even change permissions of the logged in user.To mock this behavior, you can just put all these events in an array and just loop over it.You can even define different delays for each event if that is what you need.If you are using my Node-RED setup from one of my previous posts you should give one of the add-ons a try, however, you can also run both mocks next to each other.In most approaches, you don’t run the application itself thought the dev environment ($ ng serve --proxy-config proxy.config.json).You should run your packaged app like you would do in production.In our case, we are running everything Dockerized.This means, we build our frontend application and wrap it into a Docker image (tag it, and push it to our registry).In a next stage, we run (deploy) an environment where we can run our tests against.In this case we are also not going to use the proxy from our development setup.An easy setup would be using a docker-compose (e.g.):version: \"3\"services:  nginx:    image: \"nginx:mainline-alpine\"    container_name: proxy    restart: always    volumes:      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro    ports:      - 80:80    links:      - your-web-app      - node-red  node-red:    image: \"nodered/node-red-docker\"    restart: always    container_name: node-red    volumes:      - ${userDirPath}/node-red:/data      - ${userDirPath}/data:/usr/src/node-red/data    ports:      - 1880:1880  your-web-app:    image: \"registry.your-domain.com/your-web-app:${TAG}\"    container_name: your-web-app    restart: always    links:      - node-red    ports:      - 9080:80      - 9081:8080  selenium:    container_name: selenium-grid    image: selenium/standalone-chrome-debug    ports:      - 4444:4444      - 5900:5900    volumes:      - /dev/shm:/dev/shm    network_mode: hostWe now need to include our own ss-mock backend into this compose.You can do this by easily adding a plain Nodejs service, map your folder to your server.js and overwrite the CMD.sse-service:  image: \"node\"  restart: always  container_name: sse-mock  volumes:    - ${pathToYourServer}:/sse-mock  ports:    - 8080:8080    command: node /sse-mock/service.jsDon’t forget to add the service to the links section of your nginx and to add the proxy rules in the nginx.conf.location /api/stream {    proxy_pass http://sse-mock:8080/api/stream ;}  As mentioned before, you could/should use the /start and /stop for the sse-mock.In this setup, this means adding extra rules in your nginx config.You want all your api calls to go to the other mock (Node-RED in this case) while proxying /stream, /start and /stop to your sse-mock.The advantage of implementing the start/stop functionality, is that you can tell your test framework to start the sse-mock events stream and then start watching the response in the UI.e.g. (protractor, jasmine):beforeAll(async () =&gt; {    await browser.get('/api/start'); // depending on the host/address});If you’ve build your test data/setup in a specific order, you know what to expect and test for in the user interface.ConclusionSetting up Server-Sent Events is very easy.It is a powerful tool for unidirectional streams to you clients.The hardest part is defining a strategy for your connections and event type differentiation.Setting up the CI part is easy as well.Although you can test a lot in your unit tests, implementing End 2 End testing, mock and real, is recommended."
      },
    
      "development-2018-11-20-mongodb-europe-018-html": {
        "title": "MongoDB Europe 2018",
        "url": "/development/2018/11/20/mongodb-europe-018.html",
        "image": "/img/2018-11-08-mongodb-europe-2018/main-image-mdbe.png",
        "date": "20 Nov 2018",
        "category": "post, blog post, blog",
        "content": "  MongoDB Europe is a yearly conference where MongoDB shows off their latest features and new products.This year the venue took place in Old Billingsgate Walk, London.Jan, Nick and Chris wrote this blog post to summarize several of the given sessions.Table of contents  Atlas  MongoDB University  Compass Aggregation Pipeline Builder  Common query mistakes  Stitch  Meet the experts  Streaming data pipelines with MongoDB and Kafka at AO  MongoDB Enterprise Operator for Kubernetes at Amadeus  MongoDB Charts  ClosingAtlas(MongoDB Atlas for your Enterprise, Vladislava Stevanovic &amp; Veronica Tudor)Atlas is the database as a service offering (DBaaS) by MongoDB itself.You can run your MongoDB in the cloud with the cloud provider of your choice, secured by default and automatically backed up.Getting startedIt is very easy to get started since a free tier is provided for everyone and you can deploy with the cloud provider of your choice (Azure , GCP, AWS).A cluster can be started for free in just a few clicks.  Start your own cluster: https://cloud.mongodb.comCloudMongoDB Atlas is a cross-platform database which you can run on Amazon Web Services, Google Cloud or Microsoft Azure. It provides you an availability map that shows you which users are served from where and what the expected latency is.  ScalabilityWhen your database grows it is easy to scale up or out.You can scale up by increasing the size of the instance on which your database runs.Scaling out is done by the process of sharding.Here we are storing data of the same collection across multiple smaller partitions so we can distribute these partitions over multiple machines and increase read-write performance.This way you do not run into the limitations of a single server.To ensure that MongoDB stores data equally across shards you need the right strategy of choosing a partition key.High availabilityWhen a primary node goes down, a new primary is chosen immediately by a system of voting.All nodes vote on who should become the new primary. The node with the majority of votes becomes the new primary.A general guideline is to have a replica set that consists of one primary node and at least two secondary nodes.To ensure maximum uptime the procedure to recover from instance failures is completely automated.MongoDB UniversityWith MongoDB University, Mongo has its own platform for online courses.A lot of them are available for free.You can pick out courses according to your needs or profession.There are training tracks for Developers, DBA’s and everyone else…The courses are ideal to get you started or to deepen your knowledge when you want to be more advanced.If you want you can even get certified!I speak from my own experience when I say that the University platform is great to work with, and the courses are very well taught.Find all available resources here: https://university.mongodb.com/.Compass Aggregation Pipeline BuilderRecently the aggregation pipeline builder was introduced in MongoDB Compass.This allows you to create an aggregation pipeline step by step and that makes it easy to debug the pipeline along the way.Let’s see an example: Suppose I have a collection which contains documents that represent a person, like this :Here are two examples of elements in the collections:{\t\"_id\" : ObjectId(\"5be40f6e7047ead15753d073\"),\t\"firstName\" : \"Didi\",\t\"lastName\" : \"Verhoft\",\t\"birthYear\" : 1996},{\t\"_id\" : ObjectId(\"5be40f6e7047ead15753d074\"),\t\"firstName\" : \"Nick\",\t\"lastName\" : \"Van Hoof\",\t\"birthYear\" : 1992}A person has the fields lastName, firstName and birthYear (and of course for some people more info could be stored).I want to build a pipeline with the following functionality:  I want to filter out all people that share my lastname “Van Hoof”  Then I want to count how many times these people also share the same firstname and birthyear  Next I want to group them by birthYear so that I can see how many people were named “Nick Van Hoof “ (my fullname) in 1992.  Finally, I want them sorted on year in ascending orderFilter all with last name  “Van Hoof” and group by lastName, firstName and year:  Group by year and sort in ascending order:  As you can see from the images above, Compass will show all the intermediary results. With one push of a button you can generate a command line query or the translation to a programming language.Compass tells me the full aggregate will look like :db.people.aggregate([{$match: {                      lastName : \"Van Hoof\"                    }}, {$group: {                      _id: {                         \"lastName\" : \"$lastName\",                        \"firstName\": \"$firstName\",                        \"year\" : \"$birthYear\"                      },                      count: {                         $sum: 1                       }                    }}, {$group: {                            \"_id\": {                                \"year\": \"$_id.year\",                            },                            \"occurences\": {                              \"$push\" : {                                  \"identity\": \"$_id\",                                  \"count\": \"$count\"                              }                            }                    }}, {$sort: {                      \"_id.year\": 1                    }}])It would have been a lot harder to write this query manually, without the pipeline builder.Common query mistakes(Tips And Tricks for Avoiding Common Query Pitfalls, Christian Kurze)Key takeaways from this sessionGenerally speaking, there are three major causes of query slowness:  Blocking operations  Using the $or operator  Case insensitive searchesIt’s not uncommon that a properly tuned query delivers a factor 1000 speed-up.So it’s definitely worth investigating.Problem 1: blocking operationsThis happens when you use an operator that needs all the data before producing results, so results can’t be streamed.The most common culprits are the aggregation operators such as $sort, $group, $bucket, $count and $facet.Possible solutions:  Create a compound index to support your query and make sure that the sort order in the index is the same as in your query.  Offload the query to a secondary member.  Work around the issue by using a precalculated count.Problem 2: $and is fast, $or is slowSometimes a query is fast when you use the $and operator but slow when you use the $or operator.Solution:  Use a compound index to support $and queries.  Use separate single field indexes to support $or queries.Problem 3: case insensitive searches are slow!It is much harder for MongoDB to perform case insensitive searches because it has to match all possible permutations of the search string. For example, if you do a case insensitive search for the string “ace”, it has to match “ace”,”Ace”,”aCe”,”ACe”, and so on…Solution:  (3.4 and higher) Support the query with a case insensitive index.  Alternatively, store a toLower() copy of the string in another field and index and query that field instead.General tips and tricks  Create an index on an element you are interested in instead of scanning the whole table.  When you query on a combination of fields create a compound index for these fields and not separate indices on each field.  …but be careful with the usage of $or!  Build indices in the background instead of making it a blocking operation.  Do not index all fields as this will negatively impact write performance. Investigate what you really need!  Use .explain() to analyze queries.  Ops Manager and Atlas have a Performance Advisor to help you identify problematic queries.  Train your people.  Work smarter, not harder!Stitch(Ch-Ch-Ch-Ch-Changes: Taking Your MongoDB Stitch Application to the Next Level With Triggers, Andrew Morgan)  Write less code and build apps faster!Stitch is the ‘Serverless platform from MongoDB’, and it comes with a free tier to play around!It provides a very easy way to create an application without having to write lots of code in a separate backend.The functionalities of Stitch are provided through an SDK.Currently there are SDK’s for JavaScript, React Native, IOS and Android.There is even an Electric Imp Library for IoT devices.Stitch has four main services :  Stitch QueryAnywhere  Stitch Functions  Stitch Triggers  Stitch Mobile SyncStitch QueryAnywhereQueryAnywhere enables you to query the database directly instead of going through a REST api.The benefit here is that as a client application you are not restricted to what a REST api would expose but you can use all the power of the MongoDB Query Language directly:const employees = mongodb.db(\"HR\").collection(\"employees\");    return employees.find({}, {    limit: 3,    sort: { \"salary\": -1 }  })    .asArray();Of course, all of this is secured with authentication and fine grained authorization based on the logged in user or the contents of the documents.Stitch FunctionsYou can write JavaScript functions in the Stitch serverless platform and combine database calls with cloud services.For example, send a message with Twilio to all users:exports = function (message) { var mongodb = context.services.get('mongodb-atlas'); var twilio = context.services.get('twilio'); var coll = mongodb.db('db').collection('users'); return coll.find().toArray().then(users =&gt; {  users.forEach(function (user) {   twilio.send({    to: user.phone,    from: context.values.get('twilioNumber'),    body: message   });  }); });};​// Then call callFunction from the client sidestitchClient.callFunction('sendMessage', ['Hello from Stitch!']);Stitch TriggersMongoDB does not provide triggers, as known in the RDBMS world.With MongoDB change streams you can build your own triggers in your application.This comes with the cost of handling the complexity of change streams yourself. For example: how to resume the change stream after a network issue?So that’s why there is Stitch Triggers to make this easier.Stitch triggers combines change streams with Stitch Functions.So when the inventory of an article goes up, Stitch Trigger calls a function that uses Twilio to send a text message to your client.Stitch Mobile SyncSince 4.0, MongoDB provides a Mobile version for IOS and Android.With Stitch, you can sync your data in your mobile application with your database.So now you can use the full MongoDB Query Language, including aggregations, on your mobile device and sync it with your database.Built-in external IntegrationsThe fun with Stitch really starts when you combine all the goodness of Stitch with its integrations with cloud services like Twilio, AWS, Google, etc…You can authenticate with Google, store files on S3 or spin up a cluster on Redshift after you send a text message with Twilio.All of this can be hidden behind a simple function call for your application, or a trigger on your Atlas cluster.Limited to Stitch UI?Luckily MongoDB builds its products with developers in mind.So you can import and export your Stitch applications and put them in a source control of your choice.Meet the expertsAt the conference you had the chance to book a 20 minute session with a MongoDB expert.This was of great help in getting to know the new MongoDB Aggregation Pipeline builder.The expert also gave some more tips in “thinking noSQL”.  When data is shown together it is stored together – MongoDB expert  Data should be stored in the same way it is used – MongoDB expertStreaming data pipelines with MongoDB and Kafka at AOAppliances Online, AO, is an international online appliances retailer.They wanted to solve the issue of having data locked in different places so they wanted a Single Customer View.The idea was to get the data from all the different places and consolidate this in MongoDB.We are talking here about data stored in legacy databases or messages going through queues.The data could be the usual customer data and phone calls with customer care.But also about parcels moving through the warehouse and delivery tracking.They wanted to get the data while it’s hot, not in hourly or daily (or worse…) batches.It was decided to use MongoDB to build up this materialised view of all different data streams, and Atlas to be able to focus on the application and not the database administration.The vast majority of the data resides in MsSql databases.Extraction happens with Kafka Connect SQL CDC to generate a stream of all create, update and delete operations into a stream, and push it to Kafka.All with a simple piece of configuration like:{  \"name\" : \"msSqlSourceConnector1\",  \"config\" : {    \"connector.class\" : \"io.confluent.connect.cdc.mssql.MsSqlSourceConnector\",    \"tasks.max\" : \"1\",    \"initial.database\" : \"testing\",    \"username\" : \"cdc\",    \"password\" : \"secret\",    \"server.name\" : \"db-01.example.com\",    \"server.port\" : \"1433\"  }}They use Avro for the schema definition in combination with a schema-registry.Interested clients can then read the data off the topics and do their single-view-thing on the data and save it to MongoDB.The view is being built up, message per message.Afterwards this view in  MongoDB is then pushed back to Kafka as another stream to provide this data to interested parties.This avoids locking the data in one place.To finish it of they shared some lessons learned :  Watch out for frameworks generating queries. They can create bad performing aggregations or queries. For them it was better to write some queries explicitly.  Use custom _id for unique determination of your model, it saves an index and RAM  Watch out for unbounded document growth.MongoDB Enterprise Operator for Kubernetes at AmadeusAmadeus is the world’s largest technology company dedicated to the travel industry.They have developed an impressive MongoDB farm, a large environment with 100 clusters.Some of these clusters run more than 100 shards, while others have 100TB MongoDB databases.Amadeus processes more than 1 trillion flight availability requests per day. For each single search you do on a website they receive 500.000 availability requests.So search responsibly ;-)The number of requests per day grows by 50% each year.The second of January has by far the most requests, due to new years resolutions!If this day is in the weekend all systems are pushed to their limits.The airline fare database for one of their big clients, Kayak, is 100TB in size and changes daily.That’s some pretty big numbers there.No wonder that Amadeus is a happy user of the MongoDB Enterprise Operator for Kubernetes.Starting with the MongoDB Ops Manager v4.0, MongoDB officially supports the management and deployment of MongoDB in Kubernetes with Backup, Automation, Alerting and Monitoring.A MongoDB Enterprise Kubernetes Operator has app-specific awareness about stateful applications, so it knows how to deploy them.This operator helps automating scripted tasks and enables MongoDB-as-a-service for developers.It talks to Ops Manager and delegates the creation of clusters, shards, backups and automation to Ops Manager.The underlying necessary Kubernetes infrastructure is orchestrated by the operator itself and so they work in conjunction.This provides for clusters to be setup, scaled up/down/out/in, with a single simple yaml file.And kubernetes provides the self-healing capabilities, how nice is that!?The following yaml file is all you need to spin up a 3 node replica set :apiVersion: mongodb.com/v1kind: MongoDbReplicaSetmetadata:  name: myReplicaSet  namespace: mongodbspec:  members: 3  version: 4.0.1  project: projectName  credentials: myUberSecretCredentialsI kid you not, that’s it.Scale out or back in with a simple change in the config yaml andcubectl apply -f file.yamlMongoDB Charts  Bringing Data to Life with MongoDB Charts, Guillaume Meister.Currently, if you want to visualize data in a MongoDB database you either have to code it yourself, or rely on a 3rd party tool and migrate your data to a different platform (for example: Kibana with Elasticsearch is very popular).Needless to say that this can be quite cumbersome.MongoDB Charts intends to solve this.So what is it? MongoDB Charts is a visualization tool that runs as a standalone webserver so you can access it via a web browser.In Charts you define data sources by pointing to a collection or view on a MongoDB instance.Then you can create all kinds of visualizations based on the data source, using various aggregation and filter functions.Finally, you can combine charts into dashboards with a customized layout and you can share these with other users.A picture is worth a thousand words, so to give you a better idea of what it is all about, let’s look at an animation of Charts in action:  Charts is still in beta but you can already try it out. MongoDB provides a docker image that you can download via the MongoDB download center.ClosingIt was a great day being submerged in MongoDB-knowledge. This conference gave us plenty of opportunity to talk to other experts and learn about the new and upcoming features.Keep an eye on this space for more MongoDB goodness."
      },
    
      "conference-2018-11-16-join-2018-html": {
        "title": "JOIN 2018",
        "url": "/conference/2018/11/16/JOIN-2018.html",
        "image": "/img/join-2018/join-2018.png",
        "date": "16 Nov 2018",
        "category": "post, blog post, blog",
        "content": "JOIN is back!As has become tradition, Ordina JWorks organised the annual JOIN conference on Oct 4th.This edition was the 6th and once again it was bigger and better than the previous editions.JOIN is a free conference hosted every year by Ordina JWorks, by and for our own employees.External colleagues, and basically anyone interested in Ordina JWorks, is also invited to come over and indulge themselves in the JWorks atmosphere.To learn about new technologies, trends and best practices in our domain.Talks &amp; Workshops  Frederick Bousson: Unbeatable at Connect 4 with Wearables and Computer Vision  Tom Verelst: Testimonial TVH MyAssetPlanner  Dieter Hubau: Visualizing Mandelbrot fractals using Riff and Spring Cloud Function  Pieter Van Hees: Testimonial HZIV  Julia Müller: The 10 worst mistakes your product owner can make  Remco Runge: Deep learning in practice  Bart Blommaerts: Innovation without asking permissionUnbeatable at Connect 4 with Wearables and Computer Vision by Frederick BoussonFrederick Bousson kicked off the technology track with his talk about wearables.Wearables are becoming more mainstream every day as proven by the prevalence of fitness trackers and smart watches.Smart glasses on the other hand are still very rare.Mainly because they still look awful.But improvement is on its way here as well: better looking sets are starting to appear like the Vuzix Blade.This is important as humans are very vision-oriented: 90% of the information on the internet is processed via the eyes.Connect 4To really demonstrate the power of smart glasses, Frederick was looking for an interesting case he could use as a demonstration.The result of this quest was a way to help you become unbeatable at Connect 4.Connect 4 is a “solved game”, which means that there’s an algorithm that can’t be defeated (provided you get to go first).Frederick went hunting in the Open Source community and discovered a program written by Brian Borowski which contained this algorithm.4 years ago, Scott Bouloutian added visual processing code on top of that and yet another person ported the result to Google Glass.Nice to see how, with open source, you have people who build upon each others’ work.Image processingIf you want to have your smart glasses solve your game of Connect 4, you’ll need to do some image processing.And since the game revolves around colour which means your program doesn’t just need to see, it has to reason about these colours as well.Reasoning in colour is a pain in the ass though.OpenCV, Open Computer Vision, is a great library for image processing and it’s available for a lot of programming languages.Unfortunately it doesn’t use RGB (Red-Green-Blue) as a color space, it uses BGR (Blue-Green-Red) instead.If your brain is conditioned to think in RGB, switching to BGR is not an easy task.Before you can start solving the game you need to know the position of the pieces that were already played.And before you can find those pieces you need to know where the board is.In order to find the board you’ll need to look for a large, blue rectangle.Then you draw lines around it and find the corners.Now your application “sees” the board.  The next step is to look at the location of the played pieces.For this you need to look inside the board for concentrations of the right color.If you’re using a board that has a tray for the game pieces this tray is also part of the board and you’ll need to cut off a bit from the bottom.Of course if you’re using a different light source, colours are perceived differently by the camera (e.g. the same colour in fluorescent lighting is green while it will be matched to yellow when using a light bulb). This means that the colours found by openCV might no longer match the ones you programmed in so you’ll also have to take the white balance into account.The resulting data is then mapped into a 2-dimensional array which then can be fed to an Minimax AI solver.This solver will use a decision tree and you can define the amount of positions it needs to think ahead.This process also takes advantage of alpha-beta pinning. It uses a binary tree to decide which option is the best, but as soon as it sees that one branch scores too low it will no long check that part of the tree.All in all it’s a pretty performant bit of code: OpenCV can take the image, process it and spit out the results in about 50ms.Everything is possibleAR/VR can do anything, but that makes you wonder why it is not everywhere yet.There are quite a few reasons for this: first of all you need a business case.Companies won’t invest a lot of money into something that doesn’t have a return on investment.And even if there’s money to be made in it you still need customers.People actually have to want your product.TakeawayThe key takeaways of Frederick around image processing are:  Everything is possible  Think before you act  Just do it  You can rewatch his talk on our channel on Youtube.Testimonial TVH MyAssetPlanner by Tom VerelstTom spoke about the application MyAssetPlanner he has been working on at TVH.TVH is a company which rents out equipment and can provide you with spare parts for a lot of tools and vehicles.MyAssetPlannerTomorrowland is quite a big event and for setting up and tearing down they rent up to 160 machines (or “assets”).For every asset a contractor will need to schedule it to be available at a certain time, resulting in more than 300 phone calls which need to be handled by the asset planner.TVH wanted to improve the customer experience and decided they were going to develop a new application to help with the planning of these assets: MyAssetPlanner was born.First there was a design sprint by Clockwork in order to come up with a prototype and the most important business needs which needed to be implemented. This part took about two weeks.After this prototype, TVH started with the implementation of the first version which was put into production two months later.This first version has currently been in use for four months including for the last iteration of Tomorrowland.For development, the Spotify model was used which is focussed around:  autonomous squads  do it yourselfThe architecture is driven by events and uses Kafka as a backbone allowing you to just consume what you need, the events in which you are interested.The entire CI/CD pipeline is fully automated thanks to Spinnaker.This allows a developer to commit a feature which gets automatically deployed to an acceptance environment.As soon as the product owner approves the feature it will be rolled out to production.The teams work with Scrum Agile, it was interesting to hear about how they handle the sprint review.After every sprint, different business stakeholders receive a scenario and one person of the team is present for assistance.All the stakeholders must go through the scenario themselves.This forces your stakeholders to work with the application instead of just watching a demo, resuling in a ton of valuable feedback.What is nextFor the moment there are a 17 customers using the tool. Very soon this will scale up to 5% of all TVH customers which amounts to 3.000.After that there will be a gradual ramp up towards 20.000 customers.  You can rewatch his talk on our YouTube channel.Visualizing Mandelbrot fractals using Riff and Spring Cloud Function by Dieter HubauFunction as a ServicePivotal Function Service (PFS) is a pretty new addition to the Pivotal landscape.Riff, an open source project, is the base of PFS.The idea behind Riff is to easily create functions and move these to the cloud.Its name is derived from the guitar world as you can see here, the project lead is Mark Fisher.Initially Zookeeper and Kafka were used to send messages between functions but this did mean a lot of extra overhead to set up.Google found out about Riff and came to Pivotal with Knative which runs on Kubernetes.You can find its repository on GitHub.Knative allowed Pivotal to remove a lot of the boilerplate needed to run with Kafka and Zookeeper.Knative has four components:  Knative Serving: request/reply messages.  Knative Build: auto-detect your code and create the required containers and sidecars.  Knative Eventing: replaces Kafka and Zookeeper providing channels and subscriptions.  Fourth Knative block: makes it run really well on Google Cloud.RiffA new version was released a couple of weeks ago.Since this is a very fresh and new project, this did mean a lot of late nighters to migrate the code of the demo.Experimental projects like this do tend to make frequent API changes during their initial development phase as backwards compatibility is currently not (yet) a requirement.MandelbrotOne of the simplest mathematical functions which creates the most complex objects is the Mandelbrot set.You can keep zooming in, resulting in more calculations the deeper you go.It also tends to create pretty graphics which is nice for a demo.The codeFor a 400 x 400 image we will send a request to the backend to calculate the result for every pixel.This means that around 160.000 calls will be sent to do the calculation.It is not the most efficient way to do the calculation but this demo is about using functions and not about calculating a Mandelbrot really fast.Riff uses containers to package your business logic.There are two kinds:  UserContainer: actual container contains the actual function  InitContainer: they run before your container.You can use these to instantiate a database.For one function you need about 200MB of memory as it spins up a slimmed down Spring Boot in the backend.All in all it is still pretty performant.Dieter mentioned a talk of Dave Syer about performance in Spring which is very interesting to watch: link.It was a nice talk showing what you can do with Riff but also a warning that it still is under development and using it might have you encounter bugs.If that does happen do not hesitate to reach out to the Riff team.When working on his talk, Dieter also raised several issues.  You can rewatch his talk on our channel.Testimonial: HZIV by Pieter Van HeesPieter is currently on a project at HZIV, a government health agency which offers its members legal health and disability insurance.The old situationA lot of old applications still run with the almighty Cobol.10 years ago, HZIV wanted to rewrite these using Java 5 and Swing but that rewrite didn’t go so well.So they are still using these old beasts.A new startWith more time comes more insight and it was decided to do a new rewrite and also to give the teams more freedom in how to set up their development environment and choosing the technologies that are best suited.Chosen technologies are, most notably:  Java 8 &amp; 10  MongoDB  ELKCurrently the teams are working in an agile way, delivering fancy new features.Working @ HZIVWhat is great?You have great freedom to choose and test out new stuff. At the same time the atmosphere is relaxed, allowing you to explore without pressure.What is bad?There is the risk of Developer Anarchy, where developers just introduce fancy new stuff without properly ironing out the edges.Some managers like to micro-manage.The same managers also tend to introduce frequent scope changes which does not combine well.Why should you want to work here?Appearantly working at HZIV feels like being in a spy HQ.The combination of some old furniture and modern technology create a very particular atmosphere.Very interesting to hear about an organization who has embraced a new way of working and is currently in the process of finding their way around it.To see what goes well and what does not.It is also surprising that this can happen in a government agency as most of them in Belgium are not known to be very innovative.  You can rewatch his talk on our channel.The 10 worst mistakes your Product Owner can make by Julia MüllerA product ownerIs responsible for maximizing the value of the product and the work of the development team.1. PO doesn’t know his/her productHaving no knowledge of your product is fatal, because you are responsible and accountable for the success of your product.A good tip is that as a product owner you should be able to explain your product in 3 minutes.Use a product canvas to help you understand your product.2. Team doesn’t have a productIf a team does a lot of little tasks, it will miss context and will have to endure frequent scope changes.This means that the team will have very little focus which is not very effective.3. There is no vision for the productIt is bad when user stories describe solutions like: The user needs a button to be able to ... This causes the team to stop thinking about what would be the best possible solution.Do not forget that a team of people has more knowledge than one individual.Scrum is developing towards a goal and is about autonomous teams.The vision which you as production owner provide must provide direction and guidance for the team.A helpful tool is a “postcard from the future”.4. The non-economic dreamerEvery feature adds complexity, increases the chance of failure, and makes future changes more expensive.A product owner must be able to reason economically as every feature should add a clear benefit.A product owner should also not hide behind ‘something’ the tech team does.As those ‘things’ also determine the cost and business value of your product.5. Tech Debt not my Problem.A good technical design is important for the success of your product. For example: Investing in delivery speed might not add a direct business feature but will reduce development cost and allow you to provide new features faster in the future.Customer value is different to business value.An investment can also lead to business value.6. The YES-sayerIf you always say ‘yes’ to every feature request then your product may become a Frankenstein monster.It is better to discuss more upfront instead of implementing features you might need to remove later.Wishful thinkingDon’t fall in the trap of wishful thinking.Do not just assume that your product will be a success.Know how to validate your assumptions and make sure that you have a good feedback loop so that you know that the features you provided fulfil the business requirements.Ignoring the factsIgnoring the obvious signs that you are not going to meet your goal.So many demos are made with Powerpoint, a real sprint review should be the user starting the application and using it themselves.If they get stuck you need to ask them why, as these situations will give you great feedback.7. Obsessing over detailsThe product owner who used to be an analyst.A huge backlog is not good, a good exercise is to take the top 50 and delete the rest.In a huge backlog there is a lot ‘relevant’ information in the stories, however this information tends to become stale.Later on it will also become increasingly difficult to implement these stories as there is no certainty that the information they contain is still correct.Another big no-no is having a ‘definition of done’ being more than one page long for a user story.A nice concept is the three C’s:  Card (post it - fat pen): a story is just a single card. This is a reminder that we need to talk about this.  Criteria: Only Acceptance Criteria on the story.  Conversation: During refinement there will be a conversation with the team and then you will expand the story.Also be careful not to be a perfectionist.No first release is going to be perfect.Being fast in the market beats having a complete product.8. No presence, no communicationCommunication is one of the most important factors of failure in an organization.A product owner must be with the team and not communicate from the sidelines.9. Crossing competencesThis is not a 13, but it is much smallerIf your team says it is a 13, it is a 13.They have the knowledge to estimate and implement a story.10. Product owner is the only one filling the backlogLet the team bring their own ideas to the backlog.This will help them focus and deliver a better solution.Focus on what truly matters for your product.QuestionsThe product owner has a lot of hats, how can he manage all this?The product owner is indeed responsible for a lot of stuff.But being responsible does not mean that you need to deliver everything yourself. Learn to delegate.The product owner should also not be the only one who meets the stakeholders and not everybody should have to be a stakeholder.Learn to remove stakeholders who do not contribute to the business value.Remember the Frankenstein monster.Can a product owner ignore stakeholders wishes?A Product Owner should be able to take his own decisions and decide to run some tests on customers and collect the resulting data.It is sometimes better to ask for forgiveness later.If the product owner has no trust of the organization making him/her unable to take decisions, then that person should not be the product owner.  You can rewatch her talk on our channel.Deep learning in practice by Remco RungeMachine learning: you try to get the good algorithm to find the stuff you want to find.Deep Learning is based on biology, about how people thought the brain functioned. The assumption was proven wrong but the basic principles still work.Tensorflow has a nice playground where you can see these effects live in action.Deep learning has two flows:  Forward propagation: feed data through it.  Backward propagation: update weights so the next iteration will be better.You can define weights on input in order to make certain inputs more important.Deep learning requires a very large network with a large amount of nodes and a lot of hidden layers.The early layers allow your network to distinguish basic structures within your data.Further down in the layers the network will learn more detailed features like; eyes, ears, …It is cool that the network learns these features about how to recoginize a person just by providing it with pictures of faces. We did not provide it with the concept eyes but eventually, in one of the iterations the network started to take eyes into consideration.Deep Learning at OrdinaWorkshop: hack a drone.A small drone was equiped with deep learning to detect and identify objects by using the Deep Learning for Java.In a short demo we saw it could identify a notebook.Tic Tac ToeIn contrary to Go, Tic Tac Toe just has 360.000 possible moves making it easier to use for a demo.The network for this demo learned by playing against some random opponents without any real strategy, this made it quite dumb. You could improve this by letting it play against itself.Digital Railway SurveyThe idea is to recognize signs next to the railway and verify if these have been installed at the correct place.You Only Look Once is a very cool algorithm which gives you very nice bounded boxes around the objects on the images making it easier for your network to know where it needs to look and thus preventing you from processing unrelevant pictures.It was also nice to see that with deep learning the system can distinguish signs from a passing train even when we would be unable to see these signs, let alone see what has been written on it.QuestionsHow long does it take to get started?If you work with existing data sets and known algorithms, 15 lines of code is enough.Remco mentioned that for a brand new project it can easily take up to 55 days just to show that deep learning is possible for a specific business case.It takes a long time to gather the required data.You need to label it and make sure it is diverse enough so your model is not too focussed on one specific subset of input.  You can rewatch his talk on our channel.Innovation without asking permission by Bart BlommaertsService decompositionWhen you want to split up a monolith into smaller applications there are three patterns you can follow.  Split: define vertical functional boundaries.This does not happen to be possible in all old applications.  Strangle: extract and re-implement logic in new components.  Extend: new functionality in new microservices. Do pay attention to not create a distributed micro-**** fest, also known as a distributed monolith.Generally you will not just use one pattern but all three.Anti-corruption layerThe anti-corruption layer translates to and from different models.It allows you to enforce loose coupling between contexts.You can use different patterns for it:  Shared repository: just share the same repository between various services.  Data-synchronized repository: each microservice has one database asynchronously synchronize the data.  Event-driven synchronised repository: the main idea behind events, do not wait for data to be asked but just publish it.An event happened in the past and it contains three types of data:  Data it owns: this is data tied owned by the publisher in the event  Data it needs: this is data which can originate from other services but which is necessary to handle the event  Referenced Data: data which might be relevant for the event. For example when booking a holiday, the reference temperatures of the location to where you want to travel to.The event should contain enough information so that consumers do not need to query for additional data.Otherwise you will not think properly about your bounded contexts and you will not be able to build a loosely coupled system.In an ideal world you can talk with your business stakeholders about how these events should look like.Bounded contextsBart indicated that it is very interesting to look at the DDD lite movie from Greg Young.It may look complex and after watching it the first time you might decide that you don’t ever want to do Domain-Driven Design, but make sure to watch it a second time and things will definitely become more clear. ;-)Identify the domainWhere does your microservice start?Identify domains and subdomains.These subdomains tend to correspond with your bounded contexts.How?Look at the data model:  Talk to business stakeholders.Make the list of subdomains explicit.  Look at the data.  Look at the code, both existing and historically.If two files always get committed together, they most likely belong to the same bounded context.Integration patternsBetween bounded contexts there can be lots of integration patterns. A great source of information about these patterns can be found on the site of Gregor Hohpe: Enterprise Integration Patterns.DemoThe previous concepts were then shown in a short live demo providing a real implementation.What was interesting to see was that Apache Avro was used as a schema for the events.This shared schema repository is then the only coupling between your services.Using a schema allows your consumer services to see if they can process an event or not.APIsThere exist a lot of API guidelines which you can use as baseline for your own, for example: Paypal, Zalando, …This talk is not about API design but when you design an API please keep Postel’s law in the back of your head.Postel’s Law: Be conservative in what you do, be liberal in what you accept from others.Do make sure that you have documented your API using: Swagger, Avro, HAL, RAML.Without asking permission?Continuous experimentation is the first thing to introduce.You should really try to introduce that culture.Microservices are small and thus can be easily thrown away.Use feature toggles and monitor your users in order to gather user feedback about which business features are the most interesting for your stakeholders.With traffic routing you can make sure that only certain users get access to certain features allowing you to experiment with a very low risk.Use this data, these metrics you gathered, to convince your business stakeholders.Distributed systemsAre hard, see the eight fallacies of distributed computing.MonitoringSee the hard nature of distributed systems as a way to introduce monitoring and logging (Grafana, Prometheus)You can also provide this monitoring data to your users to give them more insights in the application, giving them direct feedback of their actions.Which can give you new opportunities.For example: They will see failures and they will want this to be resolved.MaintenanceDo not forget to take maintenance into consideration before adopting some new technology.Because you should not forget that you will still need to maintain it.  You can rewatch his talk on our channel.  You can find the slides of his talk on Slideshare.Thanks KevinMany thanks to our colleague Kevin for organizing the JOIN event.Without all of his work it would not have been as great as it was.There is much more  This was just summary of some of the talks we had on JOIN, there were many more as you can see on YouTube.Next YearHope to see you around next year. Keep an eye out for future updates on our JOIN site."
      },
    
      "development-2018-11-05-managing-translations-with-crowdin-html": {
        "title": "Managing translations with Crowdin",
        "url": "/development/2018/11/05/managing-translations-with-crowdin.html",
        "image": "/img/2018-11-05-managing-translations-with-crowdin/main-image.png",
        "date": "05 Nov 2018",
        "category": "post, blog post, blog",
        "content": "  As your application continues to grow, you may want to support more and more languages.Managing all that in the source code will become very hard and prone to errors.Luckily there’s an easy solution to that and it’s called Crowdin!Table of contents  What is Crowdin  Key features  Getting started  In app translations  ConclusionWhat is CrowdinCrowdin is an online platform that allows you to do translations through a visual interface.You can appoint people as translators for any natural language.Either an integration with your Git repository or a CLI tool can be used to get the source strings into Crowdin.After uploading the source strings, the translators can get to work.They’ll be notified whenever there’s something new to translate.When they’re done, approvers can start proof-reading with the help of some thorough quality assurance functionalities.You’ll never miss a translation again because it will clearly be marked as not being translated.Strings that are removed from the source file will also be removed from all translations.The translated files can be downloaded at any moment and will be in the exact same format as the source file.In other words, clean and up-to-date translation files!Translator view:Approver view:Key features  A large number of file formats are supported, ranging from json files to csv and properties files.It can thus be used for Angular, React, Vue … applications as well as a Spring Boot service and so on.  Crowdin has a very clever parser and detects thing such as HTML tags and variables.It will warn you if they are translated and no longer match the original tags/names.  Quality assurance checks will help the approver in verifying the translations.  Extra context can be provided for each source string to help translators understand what exactly they need to translate.  Integration with source control systems is very easy and will make sure that the source strings are up-to-date with your Git repository. Translations will automatically be pushed to your Git repository.  There’s even the possibility to integrate it with your website and have people do the translations on your website directly.That way the translators get a lot of context of what to translate.  If you’re unable to do the translation for a certain language, Crowdin allows you to easily hire professional translators!Getting startedAccountTo start using Crowdin, you of course need an account at Crowdin.com. There’s a limited free trial. After that you can choose from a variety of payment plans depending on the number of source strings and projects you’ll have.  The number of projects you need on Crowdin depends on your setup.However, it’s recommended to create a project for each Git repository.So if your front- and backend application live in the same repository, you can configure multiple source files within one project.If not, you’ll need to create two separate projects on Crowdin.Once logged in, you can create a new project. You can choose the source language and the languages you wish to translate to. Then go to the project’s settings and then to API.There you’ll find a project identifier and a API key. We’ll need those later when setting up Crowdin in our project.Setup in your projectFirst we need to install the Crowdin CLI. For Mac you can run brew install crowdin. For Windows an installer is available.Once installed, open a terminal and go to the root of your project and run crowdin generate.This will generate a crowdin.yml file that is used to configure Crowdin in your project.In that file four things need to be changed:  project_identifier: the one you find under API on the Crowdin site  api_key: also under API  source: this should point to the source translation file (e.g. in an Angular app /src/assets/i18n/en.json)  translation: the translated files will end up here (e.g. /src/assets/i18n/%two_letters_code%.json)  I don’t recommend selecting the source language as a target language, because it will then be overwritten when downloading the translations.You could put the source language file in a separate directory if you prefer the source language being editable on Crowdin as well.However, you would have to update the source file manually when there are changes.  There are more options and regexes available, but these are the only ones required to set up Crowdin in an Angular app for example.Upload and download translationsThere are two options here: either you integrate Crowdin with your Git repository or you up and download the translations manually (or automate it using CLI commands).I personally prefer integrating it with Git so we’ll get to that first.  It’s not possible to use both at the same time because you’ll end up with multiple source files on Crowdin.com.Git integrationOn the Crowdin website, go to the project’s settings again, then to Integration.Choose the source control system you’re using and click on Set Up Integration (or choose Enterprise if you’re on a self-hosted version).For GitHub and GitLab you’ll be redirected to their website to authorize Crowdin to use your account.If you chose Enterprise you’ll need to generate a token yourself and paste it into the correct field together with the URL to your self-hosted source control system.Next you can choose which branches you want to watch for changes to the source language file and choose a name of the branch to where translations should be pushed.  Which source branch you wish to use depends on your workflow. In my case, we work with short-lived feature branches and once they are done, we merge them to the master branch.So, the master branch is the one being watched by Crowdin, since it will have all the correct source strings.With this setup, each change to the source language file will be reflected on the Crowdin site.When a source string is translated and approved, Crowdin will push the translation to the target branch (e.g. i18n_master) and even open up a merge request for your review!A great benefit of this is that your stories don’t have to wait for all translations to be done.Translations are now the responsibility of the people that you appointed to do the translations.When they do, you’ll be notified with a merge request!Manual up- and downloadIf you don’t want to integrate with your Git repo, you can use the Crowdin CLI.There are two simple commands that let you upload the source files and download the translations.Upload:crowdin upload sourcesDownload:crowdin downloadIn app translationsAnother nifty feature is that you can do translations directly in you webapp.All you need to do is add a JavaScript snippet to your index.html and select a pseudo language called Acholi.Crowdin will give unique identifiers to all of your source strings and use those as translations.Because of the snippet, these IDs will be located and replaced with translated strings for the language you’re translating to.You will also be able to edit the translations inline in your application and they will immediately be reflected on Crowdin.Now, this isn’t something you want to do in your production environment.Ideally you’d only have a specific environment setup that loads the JavaScript snippet.If the snippet is loaded you’ll always see the Crowdin tools in your webapp,so it’s up to you whether you want to use it and where.More info here.ConclusionCrowdin has really helped me and my team in the way we manage our translations.At first we were sending out emails for every label to get it translated.Of course we missed quite some translations in various languages.The variables in the translated string would sometimes be translated as well which meant that they wouldn’t work.With Crowdin we now have a clear overview of what is translated and what isn’t.Instead of sending emails, translators get notified and can do the translations through a nice UI.Because of the thorough quality assurance tools, less mistakes are made and those translated variables can be fixed before approving the translation and bringing it to production.In short, it’s a must-have tool when you need to manage translations, especially for languages you do not speak yourself!"
      },
    
      "microservices-2018-11-02-inter-service-communication-html": {
        "title": "Communication in a distributed system with OpenFeign: Tips &amp; Tricks",
        "url": "/microservices/2018/11/02/Inter-service-communication.html",
        "image": "/img/intercommunication/intercomm_header.jpg",
        "date": "02 Nov 2018",
        "category": "post, blog post, blog",
        "content": "In contrast to monolithic applications, services in a distributed system are running on multiple machines. To let these services interact with each other, we need some kind of inter-process communication mechanism.With the help of OpenFeign, I will explain how we can fire off synchronous calls to another service.Table of contents  Setup  Different kinds of HTTP clients  Enabling Mutual SSL  Intercepting requests  Give it a (re)try  Securing your API  Creating SOAP clients  Handling errors with the error decoder  ConclusionCommunication with OpenFeignTo understand the basics of inter-process communication, we need to look at what kind of interactions we can do.OpenFeign, a declarative HTTP client by Netflix simplifies our way of interacting with other services. When we decide that it is time to decompose our modulith because of numerous reasons, we would have to look for a way to handle our inter-process communication.SetupTo use OpenFeign we need to add it to our classpath&lt;dependency&gt;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;When we inspect the dependency module, we see that there is a lot coming out-of-the-box with the Spring Cloud Starter.If you are providing your own resilience or load balancing library you can just add the necessary dependencies you need.Be aware that the syntax is different between using the Spring wrapper or OpenFeign itself.To let your Spring context know that it has to search for OpenFeign clients, we just add @EnableFeignClients. You can add this annotation to any class annotated with @Configuration, @SpringBootApplication or @SpringCloudApplicationAfter we’ve enabled OpenFeign on our classpath, we can start adding OpenFeign clients. When defining these clients, we have two solutions you can choose from. The OpenFeign library, which provides us with the basics but very customizable OpenFeign clients, and the Spring library, that adds a few extra libraries to it for cloud tooling.Spring@FeignClient(value = \"auth\", fallback = FallbackAuthClient.class, configuration = FeignConfig.class)public interface AuthClient {     @RequestMapping(method = RequestMethod.GET, value = \"checkToken\")    boolean isTokenValid(@RequestHeader(\"Authorization\") String token);}  @FeignClient: is the annotation for Spring to recognize OpenFeign clients, OpenFeign clients have to be interfaces as it is self-declarative.  value/name: is the name of the Feign client that will be used to create a Ribbon load balancer which can then be linked to the target application using service discovery or a fixed list of servers. You could also use the url attribute to point your client to the target application when you’re not using Ribbon.  fallback: if Hystrix is enabled, you can implement a fallback method.@Componentpublic class FallbackAuthClient implements AuthClient {    @Override    public boolean isTokenValid(@RequestHeader(\"Authorization\") String token) {        return false;    }}  configuration: is for extra configuration like logging, interceptors, etc… more on that below.  @RequestMapping: Spring Cloud adds support for Spring MVC annotations and for using the same HttpMessageConverter's used by default in Spring Web.OpenFeignTo create an OpenFeign client we need an interface and a Feign builder that tells the interface it is an OpenFeign client.public interface AuthClient {    @RequestLine(\"GET /auth\")    @Headers({\"Authorization: {token}\", \"Accept: application/hal+json\"})    boolean isValid(@Param(\"token\") String token);}  @RequestLine: is defining which verb and which URI path you are communicating to.  @Headers: is defining the request headers that come with the request.The builderOpenFeign provides us with a builder-like pattern for our clients.When we want to customize, we just add our own customization to the builder. To see the builder at work, let’s create a bean of our client and return a Feign builder.It’s important to let the builder know which interface he has to target for communication. The second parameter is most likely the base url where all the requests begin. Get your URLs from the yml or properties file with the help of @Value.@Value(\"${base.url}\")private String baseServerUrl;@BeanAuthClient authClient() {    return Feign.builder()            .target(AuthClient.class, baseServerUrl);}Different kinds of HTTP clientsThe default HTTP client of OpenFeign uses HttpUrlConnection to execute its HTTP requests.You can configure another client (ApacheHttpClient, OkHttpClient, …) as follows:@BeanAuthClient authClient() {    return Feign.builder()            .client(new ApacheHttpClient())            .target(AuthClient.class, baseServerUrl);}OkHttpClientOkHttp is an HTTP client that’s efficient by default:  HTTP/2 support allows all requests to the same host to share a socket.  Connection pooling reduces request latency (if HTTP/2 isn’t available).  Transparent GZIP shrinks download sizes.  Response caching avoids the network completely for repeat requests.ApacheHttpClientThe advantage of using ApacheHttpClient over the default client is that ApacheHttpClient sends more headers with the request, eg. Content-Length, which some servers expect.  Aside from these clients, there are a few more to research if you want : OpenFeign clientsEnabling Mutual SSLMutual SSL is supported in all of these clients.To achieve this in an ApacheHttpClient, we have to create an HttpClient that builds the SSL context.When the SSL context is valid, we wrap this inside an ApacheHttpClient for being compliant with OpenFeign.public ApacheHttpClient createHttpClient() throws SSLException {    HttpClient httpClient = HttpClients.custom()            .setSSLSocketFactory(createSSLContext())            .build();    return new ApacheHttpClient(httpClient);}Add it to the builder.@BeanAuthClient authClient() {    return Feign.builder()            .client(createHttpClient())            .target(AuthClient.class, baseServerUrl);}Give it a (re)tryWhen we want to build some resilience in our communication, we can setup a retry mechanism in our OpenFeign client. If the other service is unreachable, we will try again until it is healthy or until the max attempts you have set in your configuration has been reached. When we want to use the retryer of OpenFeign, we got three properties we can set.      period: How long it takes before the retry is triggered        maxPeriod: That’s what the maximum is of how long it can take before a retry is triggered        maxAttempts: How many retries may the client trigger before it fails  Example:@Value(\"${retry.period:3000}\")private int period;@Value(\"${retry.maxPeriod:30000}\")private int maxPeriod;@Value(\"${retry.maxAttempts:5}\")private int maxAttempts;@BeanAuthClient authClient() {    return Feign.builder()            .retryer(new Retryer.Default(period, maxPeriod, maxAttempts))            .target(AuthClient.class, baseServerUrl);}Intercepting requestsIf you need some basic authorization, custom headers or some extra information in every request of the client, we can use interceptors. This becomes very useful in situations where every request needs this extra information.To add an interceptor, we just add an extra method that returns the OpenFeign interceptor.private RequestInterceptor requestInterceptor() {    return requestTemplate -&gt; {        requestTemplate.header(\"user\", username);        requestTemplate.header(\"password\", password);        requestTemplate.header(\"Accept\", ContentType.APPLICATION_JSON.getMimeType());    };}To enable the customization, we add the interceptor to the builder.@BeanAuthClient authClient() {    return Feign.builder()            .requestInterceptor(requestInterceptor())            .target(AuthClient.class, baseServerUrl);}Securing your APIWhen we want to add the security layer between our services, there are a couple solutions to look at. Here are a few that can be handled by OpenFeign.BasicWhen you want to send basic credentials you can just add an interceptor for the OpenFeign client and add the username and password.BearerFor only Bearer token communication, you can just pass it down in the request header of your method call.//Spring@RequestMapping(method = RequestMethod.GET, value = \"checkToken\")boolean isTokenValid(@RequestHeader(\"Authorization\") String token);//OpenFeign@RequestLine(\"GET /auth\")@Headers({\"Authorization: {token}\", \"Accept: application/hal+json\"})boolean isValid(@Param(\"token\") String token);OAuth2This link provides a good explanation about the use of OAuth2 with OpenFeign: OAuth 2 interceptor.Creating SOAP clientsBesides JSON encoders and decoders, you can also enable support for XML.If you ever have to integrate with SOAP third party APIs, OpenFeign supports it.There is a very detailed explanation on how to use it in the documentation of OpenFeign.Handling errors with the error decoderThe OpenFeign API provides an ErrorDecoder to handle erroneous responses from servers.Since there are many kind of errors we can get, we want a place where we can handle each one of them accordingly. An OpenFeign ErrorDecoder must be added to the configuration of the client object as you can see in the code below.@BeanMyClient myClient() {   return Feign.builder()           .errorDecoder(errorDecoder())           .target(MyClient.class, &lt;url&gt;);}Rather than throwing an exception in the decode method of the ErrorDecoder, you return an exception to Feign and Feign will throw it for you.The default error decoder ErrorDecoder.Default always throws a FeignException.The problem with ending up with a FeignException is that it does not contain a lot of structure. It is a plain RuntimeException which only contains a message with a stringified response body. No way of interpreting that exception to rethrow a more functional exception eg. UserNotFoundException.Error decoderTo handle the errors, we have to look at the structure of these errors. From that structure, we build up our own exception and throw it so the ControllerAdvice class can handle our exception.public class CustomErrorDecoder implements ErrorDecoder {    @Override    public Exception decode(String methodKey, Response response) {        CustomException ex = null;        try {            if (response.body() != null) {                ex = createException(response);            }        } catch (IOException e) {            log.warn(\"Failed to decode CustomException\", e);        }        ex = ex != null ? ex : new CustomException(\"Failed to decode CustomException\", errorStatus(methodKey, response).getMessage());        return ex;    }        private CustomExceptionException createException(Response response) throws IOException {        String body = Util.toString(response.body().asReader());        List&lt;ErrorResource&gt; errorMessages = createMessage(body);        return createCustomException(body, errorMessages);    }        private List&lt;ErrorResource&gt; createMessage(String body) {        return read(body, \"$.errors\");    }        private CustomException createCustomException(String body, List&lt;ErrorResource&gt; errors) {        CustomException ex = new CustomException();        ex.setErrors(errors);        int status = read(body, \"$.status\");        ex.setStatus(Integer.toString(status));        ex.setTitle(read(body, \"$.title\"));        return ex;    }} Warning: Working with checked exceptions and Feign is a bit tricky for several reasons.Returning a checked exception is possible in the ErrorDecoder, but to avoid Java’s UndeclaredThrowableException, you’ll have to add it to the method signature in the Feign interface. Doing this however, causes Sonar to complain because there’s no actual code which throws that exception.ConclusionThese were my experiences with OpenFeign and I like the simplicity of it. If you choose for the Spring wrapper or OpenFeign, the client is an advanced tool for enabling inter-service communication.As of now, they just released a new version that is compliant with Java 11.So go experiment and learn on the way!Sources  OpenFeign Documentation  Spring Cloud OpenFeign Documentation  OAuth 2 interceptor  SOAP integration  Other HTTP clients  Ribbon  Hystrix"
      },
    
      "kickstarters-2018-10-29-kickstarter-trajectory-2018-summer-html": {
        "title": "Kickstarter Trajectory 2018 Summer Edition",
        "url": "/kickstarters/2018/10/29/Kickstarter-Trajectory-2018-Summer.html",
        "image": "/img/kicks.png",
        "date": "29 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Introduction  54 young professionals started the Ordina kickstarter trajectory this summer, on the 1st of August. JWorks counted 5 kickstarters: Sander, Steven, Ken, Wout en Michaël. All looking for a challenge and a fresh start. Some freshly graduated from school, others having multiple years of experience, IT related or not. The goal was to broaden every kickstarter’s knowledge of the fast evolving IT world. We learned the basics of every hot topic which will definitely give us enough credits for our first project.The kickstarter trajectory consisted of two parts:  One month covering all kinds of trainings: technical, non-technical, backend, frontend, DevOps…  After our minds were filled with all this information, there was a DevCase where we could put everything we learned into practice.First DayOn the first day of the kickstarter trajectory we were welcomed into Ordina and got an introduction about the structure of the company. After that we took a tour around the building, and we were told what the different workspaces are and where the different teams work. It was nice to notice that everyone we met was very friendly and helpful. This made us feel directly at ease.On the first day we also received the keys of our car and a laptop, so we were equipped to begin our journey at Ordina.SecurityIn the beginning of the trajectory we got an introduction by Tim De Grande of the most important security principles like GDPR. This is very important to Ordina because all its consultants need to keep this information in the back of their minds when working at a customer.We also followed a more technical security course which explained some of the most common attacks and how to avoid them.BackendJavaIn this lecture given by Yannick De Turck, we were introduced to all new features of Java 7, 8 and 9 aswell as Java 10.We started off with Java 7 where we learned:  Switch-statement with String values  Automatic Resource Management  Diamond Syntax  Better exception handling with multi-catch  Literal Enhancements  The new IO API  Fork Join Framework  JVM EnhancementsJava 8 also introduced some useful new features:  Lambda Expressions  Extension Methods  Functional Interfaces  Method and Constructor References  Streams and Bulk Data Operations for Collections  Removal of PermGen  New Date &amp; Time API  New Default API for Base64 Encoding  Improvements for Annotations  Performance ImprovementsJava 9 introduces:  Project Jigsaw: Modules  Project Kulla: JShell  Factory Methods for collections  Diamond operator for anonymous inner classes  Try-with-resources enhancement  CompletableFuture API improvements  Private methods in interfaces  HTTP 2.0 Client  Process API Improvements  Reactive Streams  Optional Improvements  Collectors Improvements  Stream ImprovementsLast but not least we had an introduction of Java 10 which delivers:  Local Variable Type Inference  Unmodifiable Collections  New Optional.orElseThrow() method  Performance Improvements  Container awareness  Root CA Certificates includedIn the afternoon we made a few exercises on these new features and improvements which gave us a brighter view on the possibilities within present Java development.Spring and Spring BootThe lectures of Spring and Spring Boot were given by Ken Coenen. These were spread over two days where the first day was an overal introduction to Spring fundamentals, followed by a second day where we have put everything into practice.Day 1:The first day we got an introduction to Spring and Spring Boot about the core fundamentals and concepts of JPA, Beans and application contexts. After that we went further into the features of the framework where we were introduced to Spring Web Services and Spring Security.Day 2:On the second day we made a small project where we created a backend application to fetch all information of different digital coins (cryptocurrencies). We learned how to read data from an API using DTOs and storing them into a database. At the end of the day we had a fully working backend application which fetched all information automatically and exposed it to different endpoints.MicroservicesTim Ysewyn and Kevin Van Houtte gave a brief overview of the microservices architecture. We learned when this is best applicable. This can be applied when there is a monolith that is responsible for multiple different tasks. It could be better in that case to split off these different tasks into multiple microservices. One of the advantages of doing that is the possibility to deploy the different microservices separately and the possibility to upscale the resources of the microservices that receive the biggest loads.Clean CodeDuring this course of clean code, Pieter Van Hees taught us the best practices of how to write clean code. This improves the readability and performance of our applications which is of great importance to Ordina.FrontendThe frontend courses started off with HTML/CSS/JavaScript given by Yannick Vergeylen after which we went more in-depth of other topics.Build tools:In this course given by Michael Vervloet, we started off with Node.js and its features like asynchronous programming and event emitters.Later, we learned about NPM and other package managers and how to use them inside a project like an Angular app. At the end, Angular-CLI was covered, the start of one of the most important frameworks we use at JWorks.TypeScript:In the TypeScript course given by Kevin Van Houtte, we built further on Node.js and NPM. We did an exercise about school management where we used OO-programming and CRUD in TypeScript. This was pretty challenging but with each other’s help, we managed to get the final assessment done.Angular:Angular was the last frontend course given by Ryan De Gruyter. This helped us to quickly create a frontend that is connected to a backend project. Here, we went more in-depth on the SPA framework and how different components interact with each other.DevopsThe trajectory also included courses about the DevOps culture. We got some introductions to Docker, Kubernetes and CI/CD given by Tim Vierbergen.Docker &amp; KubernetesThese courses were given by Tom Verelst. He explained us the basics of containerization, more specifically how this is done by Docker.During this hands-on session we learned how to work with containers and images. We learned how to use, create and delete them. There was also an explanation of the theory behind containerization and what the advantages are of using this, especially when compared to using virtual machines.To orchestrate the containers, we received a course on Kubernetes. There we learned about the concepts of pods, secrets, and more.We practiced this in a small exercise where we needed to configure a Minikube and run a simple application.CloudThe last technical session we followed was about different cloud technologies. This was given by Dieter Hubau and Bas Moorkens.We learned about the advantages of running applications in the cloud and what the differences are between the different operation models.To make this more tangible, we went into multiple cloud platforms to see what the possibilities were. At the end we focussed on OpenShift Origin as this is one of the preferred container management packages inside JWorks.Soft SkillsBesides sharpening our technical skills we worked on our soft skills as well.In the 2-day session ‘Essentials of Communication’ we learned how to present ourselves by means of role playing games and cases that reflect real world scenarios.After an additional ‘Brand Yourself’ session we were ready to prove and present ourselves as worthy consultants to the management of Ordina.All these techniques are also useful in the Agile &amp; Scrum methodologies where we learned the importance of being prepared for change.DevCaseIntroductionDuring the second month of the kickstarter trajectory we were assigned to develop an event planner.The purpose was to have more of an overview and control of the upcoming JWorks events.In short, JWorks employees can create and approve events depending on their rights.In addition, a weekly digest of the events is sent to the JWorks Telegram chat group and a Telegram bot is made available with some defined commands.Technology &amp; MethodologiesTogether with our coaches Orjan De Smet, Axel Bergmans and Haroun Pacquée we started off with an introduction to the project.The user stories were presented on the Scrum board.After defining the sprint goal for the first upcoming two weeks we divided ourselves into a frontend and a backend group. Using Scrum methodology we held our daily stand-up meetings and as soon as a new functionality was developed a pull request was made and reviewed by our coaches.Every two weeks, at the end of the sprint, a demo was shown to our coaches followed by a retrospective and a sprint planning.By making use of Continuous Integration, code changes in Github were automatically deployed to OpenShift where a Jenkins pipeline went through different stages ranging from testing the code and code quality, to building the Docker image.Frontend tools that were used in the project:  Angular  Angular-CLI  Angular Material  JestIn the backend we made use of the following technologies:  Java 8  Spring Boot  Mockito and JUnit  Telegram API  Keycloak SecurityFor more technical details of the used tools and technologies of the DevCase, a separate blog post will follow!Personal ExperiencesKen Keymolen  The kickstarter trajectory provides the means to learn new evolutions &amp; technologies within the IT world.The DevCase gave us a good understanding on how to incorporate new technologies within an IT project. These trainings provide a solid base to continue to build our skills &amp; expertise in the different areas IT has to offer,making sure we are positioned to provide the best solutions for our customers.Sander Smets  Before the kickstarter trajectory, I did not really have an in-depth view on deployment and cloud automation. Our DevCase and trainings made sure that all of us have a complete understanding of frontend and backend technologies, cloud automation and new architecture strategies like DDD and microservices. Now I feel like a more complete developer and ready to tackle day-to-day problems at customers.Wout Meskens  I am very happy that I have been given the opportunity to follow the kickstarter trajectory. The first month updated my knowledge about a lot of interesting topics. It was especially interesting to learn about the DevOps technologies. The DevCase was very helpful to put all this new knowledge into practice. It was fun to see that we could make an exciting application with all these technologies. The kickstarter trajectory made me excited to use these technologies to help customers.Steven Deleye  The kickstarter trajectory was the chance for me to learn a lot about new technologies in a short amount of time.Putting this information into practice during the DevCase gave me more understanding in how and when we use these technologies.Michael De Wree  The kickstarter trajectory was not easy, but achievable. This made me so much more excited. Especially the DevCase was a good way to gain technical experience. Besides the possibility to learn and grow, Ordina makes me feel at home. I look forward to the next couple of years!"
      },
    
      "conference-2018-10-26-lead-dev-london-2018-html": {
        "title": "The Lead Developer London 2018",
        "url": "/conference/2018/10/26/lead-dev-london-2018.html",
        "image": "/img/2018-10-26-Lead-Dev-London-2018/lead-dev.png",
        "date": "26 Oct 2018",
        "category": "post, blog post, blog",
        "content": "London!For the fourth year in a row White October Events and Meri Williams organised The Lead Developer: a two-day conference covering topics from leadership to self-improvement and team dynamics, mixed with some technical talks. This year Ordina JWorks travelled to the Barbican Centre in London to attend this amazing and inspiring conference for the very first time and returned home energized and full of ideas!  As tech lead you have to be a great developer. But you also need to be a good team leader, be commercially astute, and stay on top of all the tech developments and trends coming over the horizon. With a balance of content across team, tools and tech, The Lead Developer conference is designed with the complexities of the job in mind.In this blogpost we will summarize some of the talks that are mainly focused on personal development of a team lead. All talks were recorded and can be found on The Lead Developer London 2018 YouTube channel. Highly recommended!‘First steps as a lead’ by Dan PersaWhy become a lead in the first place?According to Dan Persa, dealing with people problems is much more interesting than dealing with computers, but each company interprets this in a slightly different way. Some call it “tech lead”, others call it “people manager”, but in either case it is your job to understand what the role is all about and check that it fits your expectations.Becoming a lead is not a natural evolution for a developer. It’s a career switch and not a promotion; because a different skillset is required.The Don’t-Repeat-Yourself principle, which is a key principle for each software developer, does not apply when working with people. Communicate things over and over again.Repetition is key.Getting into leadership is not trivial, so ask people around you for help. How do you learn these leadership skills very fast?  Shadow a mentor to ensure a smooth transition and not feeling lost.  Don’t just read books, but put them in practice as well.  Reflect and remember good and not-so-good behaviour from your former leads.As a lead you need to create an open and safe environment with mutual trust.But how you do earn the trust and respect of your team?  Open yourself up to be vulnerable.  Have regular one-to-ones.  Be transparent in decision-making, especially around performance evaluations.  The best solutions to problems come from the team, let them take ownership of the problem.  Create a feedback culture: give feedback as soon as possible after the action in need of feedback happened, decouple development feedback from performance evaluations, and organise health checks meetings.  Celebrate successes!  “Not taking a decision is a decision: we are accountable for the decisions we take as well as for the ones we decide not to take.” @danpersa #LeadDevLondon‘Building sustainable teams to handle uncertainty’ by Jenny DuckettWhat can disrupt a team?Because we frequently make deliberate changes about how we’re working in our team, change is often a good thing. However, there’s uncertainty involved.  People can leave or join the team; team members can feel un(der)valued. The dynamics of personalities and opinions change whenever people change.  It can be hard for teams to make decisions and handle roadmaps while senior leaders move. Revisiting decisions that you thought you’d settled and making progress can be hard.  Shifting wider priorities can make a team feel frustrated and disempowered.  Re-organising teams out of existence can make people feel undervalued as individuals.  Even things like desk moves can have a significant impact on people. More about this on Lara Hogan’s blogpost.Putting people through these things more than occasionally, will result in a retention problem.The way changes make people feel is at least as important as the change itself.Emotions are critical.  “An empowered, open team who understands its context can adapt to change.” @jenny_duckett #LeadDevLondonWhat can you do to prepare?First of all, work on yourself so you can sustain through these disruptive changes and be able to support your team better as a result.You don’t need to do it all yourself: let go of details, give people your trust and assume positive intent.Set yourself up to lead sustainably.Make your team’s work ownable by stopping to split efforts and starting to define a single and clear goal. Communicate this goal over and over again.It will feel like you’re just repeating yourself and you’re boring people, but it’s reassuring just to know that the goal hasn’t changed.Give your team the background they need for each piece of work by adding context to user stories, organizing story kickoffs and running workshops.Empower your team to take ownership by embracing opportunities for positive change.Also become great at integrating new people by assigning them a mentor. Let them improve your onboarding guide and technical documentation.Make it clear that you value learning as part of everyday work, recognise that everyone is always learning.Share understanding of your work in the team, the whole context is important.This can be done by writing good commit messages (also the why not just the what), but also by documenting decisions and the reasons for it, and keeping your documentation up-to-date.Taking ownership helps people handle change.Support and grow individuals. Use every piece of work to help someone grow and start with individual needs.Effective delegation can be accomplished by being clear and explicit, explain what’s happening and why.Also set the scope and boundaries, what’s the problem that they’re trying to solve, who’s involved and when should they bring questions to you.Grow the next generation of leaders by teaching people to do your job and make yourself dispensable.Show your team where they fit into the wider organisation and how they relate to its goals.A wider view helps people adapt when things change.Don’t over-insulate your team because when something gets through your shield, it will be very disruptive.Grow people for a resilient team and organisation, and encourage your team to show off their work, to be proud of it.Make sure that your manager understands how much work it takes from everyone to build a team that works well and the time it takes to become familiar with the domain they’re working in.Ask them for support to build a sustainable team and show why all your team’s work matters.Good communication about change is vital.  “When stressful things happen, it’s easy to default back to not communicating. But that’s when it’s even more important to empower people during tough times.” @jenny_duckett #LeadDevLondon‘The hardest scaling challenge of all - yourself’ by Christian McCarrickCommunication  Be an effective communicator: as you grow, you’ll get better at getting your point across to larger and larger groups.  Communicate up to the manager, across peers and down to your team.  Personal branding and self-promotion are important: have confidence and show the value of your work.  Create a safe environment by genuinely praising other people and giving honest feedback.  Seek out for negative feedback and ask specific questions for better feedback.  The most important communication skill to learn is how to say “no” without feeling guilty.Prioritization &amp; Time management  If you don’t prioritize your life, someone else will.  The Eisenhower Matrix: How to make decisions on what’s urgent and important.  Multitasking is an anti-pattern, so write down all the things you need to do and keep a journal.  Turn off email notifications. Instead, schedule short blocks during the day when you quickly check emails and schedule longer blocks at the end of the day where you tidy things up.  Get into the zone by blocking your calendar: dedicate time to specific things.  Make yourself a priority once in a while. It’s not selfish, it’s necessary.  Obsess with the things that matter!Delegation  Move away from “How can I get things done?” to “How can this task/decision/goal get done?”.  Not delegating properly is one of the biggest anti-patterns and limits the scaling your entire team because you become the bottleneck around decision making.  Let things go!Personal development &amp; mental health  Leading takes energy and often gets lonely.  If athletes get injured, they sit on the bench. Mental fatigue and burnout are issues that affects us more than we might think.  Excercise, meditate (tip: Headspace), and read every day.  “Your job as a manager is to make your team more badass.” @cmccarrick #LeadDevLondon‘Go slow to go fast: building strong foundations for leadership’ by Alicia LiuAlicia talked about how her rapid increase in job responsibilities in a rapidly growing startup led to an increase in stress, difficulty sleeping, and ultimately depression. She wrote a great blogpost and gave a very inspiring talk about the tools she used to recover from depression and insomnia, and how she became a better leader and manager.A lot of engineers are promoted into management because they’re good engineers, regardless of their management skills.Some people are naturally good at both, but if you’re good at engineering, it might actually be harder to develop leadership skills. Coding drains a lot of mental energy, while managing others requires lots of emotional energy.Being a good engineer doesn’t make you a good leader because information is not the same as knowledge. You can’t study to be a leader, you’ll have to change and embrace discomfort to become a good leader. Be humble by listening to others, being present with each person and regularly switching context is a hard thing to do.Focus first on form then speed.Mastering others is power, mastering yourself is true strength.  “A gardener doesn’t tell plants how to grow. A gardener creates the best environment for the plants to flourish. But you still need to know how to garden, and you need to know what to weed out. Leadership is about dealing with the unknowns.” @aliciatweet #LeadDevLondon"
      },
    
      "kafka-2018-10-23-kafka-stream-introduction-html": {
        "title": "Kafka Streams introduction",
        "url": "/kafka/2018/10/23/kafka-stream-introduction.html",
        "image": "/img/kafka/kafka-logo-thumb.png",
        "date": "23 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Kafka Stream - A practical introductionThe GoalThe aim of this article is to give an introduction to Streaming API, and more specifically to the Kafka streaming API.Since I’m not really into writing huge loads of theory, I’m going to try and keep the theory to the minimum to understand the basics and dive directly into the code by using an example.That left the task to find a useful example, for which I got inspired by the work of some collegues. They created an IOT system that measures the usage of the staircase in big buildings with Lora IOT sensors (Stairway To Health).So I thought that this is indeed streaming data, the people that open the doors of the staircase are considered as being the stream.With that done let’s go to the theory …The theoryEase of UseKafka Streams is a simple client library which only depends on Kafka.So if you have Kafka, there is nothing else you will need to do in order to be able to work with Kafka Streams.GuaranteesKafka Streams provides you with guarantees making it safer for you to process records.The (intermediate) state of the stream processors is stored within replicated changelog topics allowing Kafka Streams to recover from failures and resume processing after replaying the changelog topics.A Kafka Streams stream processor will receive one input record at a time.It will apply its operation to it, like: map, filter, join and send out one or more output records to the downstream processors.Since 0.11.0, Kafka is able to process deliver messages exactly once, the same logic can be used within Kafka Streams so a record will only be processed exactly once.Just set processing.guarantee to exactly_oncewith the default being at_least_once.DSLKafka Streams provides a Domain Specific Language which is recommended for most users, especially beginners.  KStream: a KStream is created from a specified Kafka input topic and interprets the data as a record stream.   It will only receive records of a subset of the topic partitions.  All of the topics’ available partitions will be processed by Kafka Streams instances.  KTable: a KTable is also based on a Kafka topic, but is interpreted as a changelog stream.   So that for every record key only the most recent value will be returned, it will also handle a subset of partitions.  GlobalKTable: a special type of KTable, as its data will be populated with records from all the partitions of the input topic.After you have created your KStream or KTable you can start with a variety of transformations on your record stream, like: filter, map, flatMap, groupBy, …As you will see later in the example we provide, it is all quite easy to grasp.TopologySource ProcessorA Source Processor does not have any upstream processors and it will produce an input stream from one or more Kafka topics.Stream ProcessorA node within the processor topology representing a single processing step, it is used to transform data.Sink ProcessorA Sink Processor will act like a sink, it will send any of the received records to a specific Kafka topic without any further processing.StreamThis corresponds to an unbounded, continuously updating data set. Like a Kafka topic it will consist of one or more stream partitions.Local StateEvery stream task in a Kafka Streams topology can use one or more local state stores.These state stores can be a RocksDB database or an in-memory hash map.When data is persisted to a local state store Kafka Streams provides automatic recovery in the case of some failure allowing the processing to continue.WindowingTime is pretty important when dealing with streams, we distinguish the following notions of time within streams:  Event time: when the event occured.  Processing time: the time when the event was processed by the stream processing application.  Ingestion time: the time when the event was stored within a topic by Kafka.Windows will allow you to group your records with the same record key towards that time.We have the following types of windows:Tumbling time windowsThese feature fixed-size, non-overlapping, gap-less windows.Since the windows do not overlap, a data record will belong to only one window.Hopping time windowsHopping time windows feature a fixed size, but the advance interval (aka “hop”) can be different to that fixed size.These windows can also overlap, so that a data record may belong to more then one window.Session windowsThese represent a period of activity separated by a defined gap of inactivity.All events within that gap will be merged with an existing session.If the gap is too large, a new session window will be created, the size of the window itself will thus vary.The Practical PartDisclaimerThis project is intended as a first step into the world of streaming, so some shortcuts were taken, and not all design decisions are production ready. A good example is the use of strings as the content of the messages, this should be done in a more structured way (with Avro for example){:target=”_blank” rel=”noopener noreferrer”}.Setup of the projectThis is the really easy part, to use the streaming api from Kafka only 1 dependency must be added. Here is the example to do it in Maven.    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;            &lt;version&gt;1.1.0&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;And that is it. Sometimes life can be simple. :)Creation of the inputIn the real world this would be done by the IOT devices that send their data through the network to the central system. But since it is not easy for demo purposes to have a sensor and a door nearby, and even less handy to open and close it a couple of hundred times to test it out,I created a simulator that just sends data to the Kafka cluster.This simulator creates two kinds of messages:key = 0E7E346406100585, value = T_7configuration information about on which floor a certain device is located.key = 0E7E346406100585, value = pulse@1496309915each time a person opens the door, the key is the unique id of the device, and then a pulse and the time at which it occurred    List&lt;String&gt; devices = new ArrayList&lt;&gt;();    devices.add(UUID.randomUUID().toString());    devices.add(UUID.randomUUID().toString());      devices.add(UUID.randomUUID().toString());    KafkaProducer producer = new KafkaProducer(props);    //send the device information    for (int i = 0; i &lt; devices.size(); i++) {        String val = String.format(\"%s@%s@%s\", \"T\", \"\" + (i + 1), System.currentTimeMillis());        producer.send(new ProducerRecord(\"stream_in_dev\", devices.get(i), val));    }    try {        Thread.sleep(1000);    } catch (InterruptedException e) {    }    new Random().ints(10, 0, devices.size()).forEach(i -&gt; {        producer.send(new ProducerRecord(\"stream_in\", devices.get(i), \"pulse@\" + System.currentTimeMillis()));        try {            Thread.sleep(250);        } catch (InterruptedException e) {}    });    producer.close();This wil create three floors with a random device id, and afterwards it will send an event for a random door being opened 10 times.Reading of the outputFor checking what happens in the system a data dumper was created that outputs all the messages on all the topics of interest (the input, the output and the intermediate queues included).public class DumpData {    private static Logger log = LoggerFactory.getLogger(DumpData.class);    public static void main(String... args) {        ExecutorService executorService = Executors.newFixedThreadPool(2);        executorService.submit(() -&gt; {            KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(defaultProperties(\"your_client_id\"));            List&lt;String&gt; topics = consumer.listTopics().keySet().stream()                    .filter(streamName -&gt; streamName.startsWith(\"stream_\"))                    .peek(log::info)                    .collect(Collectors.toList());            consumer.subscribe(topics);            while (true) {                ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);                records.forEach(record -&gt; {                            log.info(\"topic = {}, partition = {}, offset = {}, key = {}, value = {}\",                                    record.topic(), record.partition(), record.offset(), record.key(), record.value());                        }                );            }        });    }}This will subscribe to all the topics that start with ‘stream_’ on the Kafka.The main partSo we finally arrived at the part where it all happens.Just as a recap, the goal of this stream is to transform both input streams into a stream that gives how many people took the stairs at each floor.As a start we must create a new StreamBuilder from the Kafka library    final StreamsBuilder builder = new StreamsBuilder();We need to get the data somewhere, here we get it from the same topics our data simulater writes to.    KStream&lt;String, String&gt; sourceDev = builder.stream(\"stream_in_dev\"); // which device is where    KStream&lt;String, String&gt; stream_in = builder.stream(\"stream_in\"); // the pulse messagesThe first stream (stream_in_dev) will be converted into a lookup table that is used when handling the second stream. This lookup table will contain which device is installed on which floor.    KTable&lt;String, String&gt; streamKtableDev = sourceDev        .groupByKey()        .reduce((val1, val2) -&gt; val1, Materialized.as(\"stream_ktable_dev\"));Since one of the shortcuts we took is creating all the topics with only one partition, we don’t have any problems with streams not having the data it needs.If using multiple partitions, then we should have used either the KGlobalTable or we should have made sure that the partitioning is done in such a way that we get the corresponding data from both partitions on this node.The second stream contains the pulses. Each time a person takes the stair, a message is sent, and this must be added to the counter of the people taking the stairs at that specific minute.        stream_in                .filter((key, value) -&gt; value.startsWith(\"pulse\"))                .leftJoin(streamKtableDev, (pulse, device) -&gt; device.substring(0, device.lastIndexOf('@')).replace('@', '_') + pulse.substring(pulse.indexOf('@')))                .map((k, v) -&gt; new KeyValue&lt;&gt;(v.substring(0, v.indexOf('@')), v.substring(v.indexOf('@') + 1)))                .groupByKey()                .windowedBy(TimeWindows.of(TimeUnit.MINUTES.toMillis(1)))                .count()                .toStream((k, v) -&gt; String.format(\"%s - %s\", k.key(), Date.from(Instant.ofEpochMilli(k.window().start()))))                .mapValues(v -&gt; \"\" + v)                .to(\"stream_out\");This seems to do a lot of things and this is indeed the case. But the API makes a clean chain that is not hard to follow.  .filter() We only want the inputs that start with a pulse, the real IOT devices also send battery information and so on, on the same topic.This could also be solved by sending them to different topics but it shows that filtering is possible  .leftJoin()  we join with the devices lookup table created with the previous KTable statement. This allows us to translate the device id into the location.key is ‘0E7E346406100585’ and value is ‘pulse@1496309915’ will be translated to the same key but with value ‘T_7@1496309915’  .map() we map the message into something more useful. in stead of the key ‘0E7E346406100585’ and value ‘T_7@1496309915’ format we now get a key of ‘T_7’ and a value of ‘1496309915’.  .groupByKey() we want to group these by key (which is now the floor number and not the device id like it was in the beginning)  .windowedBy() and create a tumbling window for each minute  .count() and within the window count the number of items  toStream() This means that the last three lines together change the stream into a stream that gives the number of messages per minute for a certain floor  mapValues() map the result of this into a new stream that gives the amount per minute where the key is the floor (T_7) and the value is a combination of the amount and the when (5 - Thu Oct 10 16:28:04 CEST 2018)  to() send it to the output stream"
      },
    
      "agile-2018-10-23-team-under-high-voltage-html": {
        "title": "Team under High Voltage",
        "url": "/agile/2018/10/23/Team-under-High-Voltage.html",
        "image": "/img/teams-under-high-voltage/main-image.png",
        "date": "23 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Recently a client called on us to help bring motivation back to a development team.Although the final deadline for delivery was nearing rapidly, team members started to leave the project, worn out and fed up.Need it be said that this didn’t exactly promote work joy and team spirit – or, for that matter, diminish the already unrealistic workload.How had matters come to these dire straits?It surely wasn’t the case that a dedicated multi-skilled dev team was lacking.Only, apart from this, the project seemed to be tainted with all the stains one can think of to make being Agile and rolling out Scrum difficult or impossible.When teams are immersed in a waterfall culture and stuck in the iron triangle of project management, there is little chance for Agility to be a success.Making matters worse, a belated start of the project (due to a delayed go) had gradually created a growing requirement/necessity to work quickly rather than focusing on quality.The easy answer would be… just fix Scrum and all the rest will follow.That makes sense, for indeed, Scrum can fix a whole lot, with effort, time and a great deal of customer convincing.Creating customer satisfaction by maximizing the business value, is of course the ultimate goal of every software project.But the belated launch of the project and the sheer impossibility of still attaining the definition of success – made me realize that it would be very wrong to talk about story points or speeding up velocity.It wouldn’t even be right to keep focusing on minimizing the MVP – which in this case is at a bare minimum already.We all have to choose our battles wisely.The first thing to do, I thought, is enable the team to find a common denominator, a list of preferred behaviours and shared interests that would make them feel better as a team.Primary focus is ensuring that the project mood board switches from taking to giving energy to the people working on it.This exercise makes it possible to disconnect the team members’ degree of happiness and motivation from the looming deadlines and the ever-changing backlog.This makes success somehow achievable again, albeit a more personal sort of success, focused on growth and team dynamics.I know this sounds like a long shot, but at least it could possibly lead to a solution with a long-term outcome.In working together the Agile way, we must always remember that people and interactions come first.Read the Manifesto carefully – and you will see that the psychological safety of a team is key to the whole lot – enabling flow away from the items on the right and making room for the items on the left.From there, all the rest is due to follow.Burning up story points should never be accompanied by burning up energy.What is burning down at one side should be building up at the other.Once this is ensured, progress comes at a steady pace.Details about the exerciseAre you interested in knowing how I helped the team detect these common denominators?For that I used the liberating structure 1-2-4-All, which goes like this:  Everybody gets a card and a thin marker.  In a time box of one minute, everyone by themselves writes down what comes after “I hate it when people…”(Somehow it’s easier for people to find examples of unwanted behaviour, than asking them for what they appreciate.)  Next two minutes they pair up and share their examples, thinking the value and good behaviour that could counterbalance the negative one.This value should be written on the other side of each card.  Team up to make a group of four and share your good behaviours and values during a time box of four minutes.Start clustering them when doubles or connections start to emerge.  For ten to fifteen minutes (depending on how big the group is), everybody explains to the group their preferred behaviour and the related value.Try to cluster them into groups and define a common denominator for each cluster, to make it more manageable.Make sure that everyone understands them and that nobody perceives any contradictory combinations.  Ask for commitment of the group to try and stick to these behaviours and values.Anyone who doesn’t respect them, can be made accountable and should be addressed by the others.Don’t hesitate to reach out if you have any questions regarding this topic!"
      },
    
      "development-2018-10-20-make-your-own-cli-with-golang-and-cobra-html": {
        "title": "Make your own CLI with golang and cobra",
        "url": "/development/2018/10/20/make-your-own-cli-with-golang-and-cobra.html",
        "image": "/img/make-your-own-cli-with-golang-and-cobra/banner.jpg",
        "date": "20 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Getting started  Adding functionality  Adding flags  Using environment variables instead of flags  Taking a file as input  That’s it!  There are lots of command line interfaces (CLIs) available these days, some of which are written in Golang.There’s even a big chance that you’re using one of them.Docker or Kubernetes for example, have a CLI written in Go.These were written using a framework called Cobra.And that’s exactly what I’m going to use to show you how to write your own simple CLI with Golang.I’m also going to show you ways to provide parameters like environment variables and config files to your CLI, so you can do more than just the basics.Getting startedThe framework Cobra provides a generator that adds some boilerplate code for you.This is handy because now you can focus more on the logic of your CLI instead of figuring out how to parse flags.Assuming you have Golang correctly installed, you can get the generator by doing the following:$ go get github.com/spf13/cobra/cobraThis creates an executable which you can run from anywhere, because it is located in the $GOPATH/bin directory, which is in turn added to your PATH variable if you installed Go correctly.You can go ahead and create a new folder for your Go code.The name of the folder will be used as the name of your CLI.Navigate inside this folder with your terminal and execute $ cobra init to start a new project.It should generate a main.go file and a cmd package.Sidenote: by default Cobra will add an Apache License.If you don’t want this, you can add the flag -l none to all the generator commands.It will however add a copyright claim at the top of every file (// Copyright © 2018 NAME HERE &lt;EMAIL ADDRESS&gt;).If you pass the flag -a YOUR NAME the claim will contain your name. These flags are optional though.When you look inside the main.go file, there’s not much going on. It just calls the execute function of the cmd package.This function resides in the root.go file, which is doing a lot more. For now, just focus on this part:// rootCmd represents the base command when called without any subcommandsvar rootCmd = &amp;cobra.Command{   Use:   \"hello-cobra\",   Short: \"A brief description of your application\",   Long: `A longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.`,   // Uncomment the following line if your bare application   // has an action associated with it:   //  Run: func(cmd *cobra.Command, args []string) { },}Here you can define how to use the CLI, together with a short and a long description.Adding functionalityIn the next step we need to think about which actions we want the CLI to perform.A good practice is to work with verbs like get, post, describe,… For our example we want the CLI to say hello, so I’m going to construct it as follows:$ hello-cli say helloTo do this we can leverage the generator again to add a new command for us:$ cobra add sayIt will generate a file inside the cmd package called say.go.In this file you can again specify the way you want the command to be used and describe its function.You’ll also see the execute function which gets executed every time you call hello-cli say.You’re probably never going to use it like that, except with a --help flag. If a user calls it like that, we want the user to know he needs to provide additional items to the say command.So we’re going to return an error if that happens.The Run function of the cobra command doesn’t return anything by default.You can however change Run to RunE, which expects the function to return an error if there is any:RunE: func(cmd *cobra.Command, args []string) error {    return errors.New(\"Provide item to the say command\")},The RunE function also shows the help output if there’s an error.This is to show the user how to properly use your command.At the bottom of the file you’ll see a function called init:func init() {   rootCmd.AddCommand(sayCmd)   // Here you will define your flags and configuration settings.   // Cobra supports Persistent Flags which will work for this command   // and all subcommands, e.g.:   // sayCmd.PersistentFlags().String(\"foo\", \"\", \"A help for foo\")   // Cobra supports local flags which will only run when this command   // is called directly, e.g.:   // sayCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\")}Here you can add subcommands and flags.By default, the say command is added to the root command, which is exactly what we want.Let’s add a sub command to say hello:$ cobra add sayhelloLike the say command, sayhello is added to the root command.In this case we want it to be added to the say command instead:func init() {    sayCmd.AddCommand(sayhelloCmd)}To make the sayhello print “Hello World”, edit the Run function:Run: func(cmd *cobra.Command, args []string) {    fmt.Println(\"Hello World!\")},Now execute the following inside your projectfolder:$ go installFollowed by:$ hello-cli say helloIf everything worked correctly, your CLI should output “Hello World!”.Adding flagsTo make your CLI a bit more interesting, we are going to add some flags.You can choose between local and persistent ones.Local flags are available only for that command, whereas persistent flags are also available for the subcommands of that command.For this example we want to greet a person by name.We’re going to do this by making a --name flag.Navigate to the init function of the sayhello.go file, and add a flag:func init() {    rootCmd.AddCommand(sayCmd)    sayhelloCmd.Flags().StringP(\"name\", \"n\", \"\", \"Set your name\")}The first string is the full name of the flag, and can be executed with two dashes like --name.The second string is the short notation, which can be executed with one dash.The third one is the default value, and the fourth is a description.To make the flag do something we need to add some logic to the Run function:Run: func(cmd *cobra.Command, args []string) {    name, _:= cmd.Flags().GetString(\"name\")   \tif name == \"\" {       \t\tname = \"World\"   \t}  \tfmt.Println(\"Hallo \"+name)},Do a quick go install, and check if it works:hello-cli say hello -n NickThe output should be “Hello Nick”.Using environment variables instead of flagsIf you don’t want to pollute your command line, or if you’re working with sensitive data which you don’t want to show up in the history, it’s a good idea to work with environment variables.To do this, you can use Viper. Viper is another dependency from Steve Francia.Cobra already uses Viper in the generated code, so why not use it as well.You can however achieve the same result by using the os package from the Go standard library.We want to make the environment variable the default value.If you remember, the default value is set in the init() function.:func init() {    sayCmd.AddCommand(sayhelloCmd)    sayhelloCmd.Flags().StringP(\"name\", \"n\", viper.GetString(\"ENVNAME\"), \"Set your name\")}That’s all you need to do to parse environment variables.Taking a file as inputFor this next part,  we want to provide multiple parameters to our CLI.You can provide flags for every single one of them, but when you have a lot of parameters, a config file can be a better option.So create a new file with a .yaml extension, and add the following contents:name: \"Billy\"greeting: \"Howdy\"If you used the generator, There will already be a flag configured that expects the path to an initial config file.You can find this flag in the root.go file under the initConfig() function.As you can see, it uses Viper again to do this.Good news! We don’t have to do it ourselves!The only thing we need to do is to extract the variables from the file:Run: func(cmd *cobra.Command, args []string) {    greeting := \"Hello\"    name, _ := cmd.Flags().GetString(\"name\")    if name == \"\" {        name = \"World\"    }    if viper.GetString(\"name\")!=\"\"{        name = viper.GetString(\"name\")    }    if viper.GetString(\"greeting\")!=\"\"{        greeting = viper.GetString(\"greeting\")    }    fmt.Println(greeting + \" \" + name)},Again use Viper in the same way as you did before with the environment variables.Try it out by doing another install and entering the following command:$ hello-cli say hello --config config.ymlThat’s it!  You have successfully written a CLI in Golang that can parse about any variable around the block! All sample code can be found on GitHub."
      },
    
      "architecture-2018-10-12-spring-boot-angular-gradle-html": {
        "title": "Integrate Angular in Spring Boot using Gradle",
        "url": "/architecture/2018/10/12/spring-boot-angular-gradle.html",
        "image": "/img/2018-10-12-spring-boot-angular-gradle/angular-spring-boot-gradle.jpg",
        "date": "12 Oct 2018",
        "category": "post, blog post, blog",
        "content": "I often found myself struggling when I had to integrate Angular into a Spring Boot project. This is something that shouldnot take a lot of your valuable development time. That’s why I want to share how I did it with Gradle in a very fast and easy way.I set up an example repo which you can find on GitHub.Application structureThis guide assumes you have a root directory that contains two child directories. One with the Angular code, and another one with the Spring Boot code. By keeping these apart from each otherit will be easier to develop within the application.We’ll make use of Gradle’s multi-project buildsto split the application into multiple modules.Because I generated my Spring Boot project with Spring InitializrI already have a Gradle Wrapper, gradlew file, gradle.bat file and settings.gradle file available. We want to move those to our root directory. Keep the build.gradle file within the Spring Boot directory.We should also add a new build.gradle file to the root directory and another one to the child directory with our Angular code.Your application structure should look like this:.├── build.gradle├── gradle│   └── wrapper│       ├── gradle-wrapper.jar│       └── gradle-wrapper.properties├── gradlew├── gradle.bat├── todo-api│   └── build.gradle├── todo-ui│   └── build.gradle└── settings.gradleYou should have three build.gradle files, we’ll check their content within a minute. But we first have to tell Gradle the name of our project and make sure it will recognize the two modules. This can be done in settings.gradle.rootProject.name = 'todo'include 'todo-api', 'todo-ui'We’ll use todo as the project name and include both the backend and frontend module by specifying their directory name.It’s important that this name matches the path of the directory, otherwise Gradle cannot find these modules.Gradle will now recognize both child directories as a subproject.AngularFor the Angular part we want to create a jar with a static directory that contains the result of our Angular build. By doing this we can include the jar in our backend module. And because Spring Boot will automatically add static web resources located within static, the Angular application will be visible when we launch the application.This can be done by using the com.moowork.node plugin.Let’s take a look at the build.gradle file in our todo-ui project.// 1plugins {  id 'java'  id \"com.moowork.node\" version \"1.2.0\"}// 2node {  version = '9.2.0'  npmVersion = '6.4.1'  download = true}// 3jar.dependsOn 'npm_run_build'// 4jar {  from 'dist/todo-ui' into 'static'}Let me explain step-by-step what we’re doing here:  We need the java plugin to have the jar task available and the com.moowork.node plugin to execute node scripts like npm_run_build.  We have to specify which node and NPM version we want to use.  Before creating the jar with the jar task we want to build our Angular project, otherwise we don’t have any static files to serve. This can be done by using the npm_run_build task.1  When we build the Angular project our static files will become available in dist/todo-ui. We want those files into static. The from 'dist/todo-ui' into 'static' command in the jar task will simply copy everything from dist/todo-ui into static.2When we build the subproject it will run npm run build and create a new jar with the build result in the static directory.We can now setup Spring Boot to include the jar.Spring BootOur Spring Boot project already has a build.gradle file generated. We just have to add one line within our dependencies to include the todo-ui module.dependencies {\timplementation(project(':todo-ui'))}Build the project, execute the generated Spring Boot jar and go to localhost:8080, you should see your Angular web application. That’s all folks!NoteThere are several ways to integrate Angular in Spring Boot using Gradle (or Maven). To me, this is the easiest and fastest way to do it. Because we’ve separated the backend from the frontend it will be a lot easier to find your way in both modules.Now go and apply this on your own projects! Don’t hesitate to contact me if you have any questions.1 by default, npm run build will execute ng build (specified in package.json)2 by default, Angular will output the build result in dist/{project-name} (specified in angular.json)"
      },
    
      "angular-2018-10-08-angular-state-management-comparison-html": {
        "title": "NGRX vs. NGXS vs. Akita vs. RxJS: Fight!",
        "url": "/angular/2018/10/08/angular-state-management-comparison.html",
        "image": "/img/2018-10-08-battle-of-the-state-managers/battle-of-the-state-managers.jpg",
        "date": "08 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Reason for comparingWhen creating a web application, one of the questions to ask is how data should be managed.If the application must be reactive, it’s best to use ReactiveX (or Rx for short) to create streams of data.The next question is how this could work performant and reliable.The current trend is to use a Redux-based storage solution, which consists of a Store, Selectors to get data from the store in the form of Observables and Actions to modify the store.This allows for a single source of truth, a read-only state and the flow of data going in one direction.There are a couple of different solutions for Angular.NGRX is by far the most popular, leaving the new kids in town, NGXS and Akita, far behind in popularity.It is, however, not always needed to have a storage framework solution.Very small applications are easy to create with plain RxJS, if you are quite skilled.In this post, I’ve stacked each of these solutions against each other to see what can be learned.Table of content  The fighting ring  The competitors  Fight          Available Tooling      Features      Boilerplate code      Community      Dependencies and size        Final scoreThe fighting ringTo compare the four competitors, I’ve set up a simple To Do-application (GitHub) with Angular CLI.The master branch holds a base of the application, which cannot run on its own.It needs a state management solution from one of the other branches.To make the comparison easier, the base application is written in such a way that each solution only adds files to a statemanagement folder and loads a service and zero or more modules into the AppModule.No other files (except package.json, package-lock.json and logo.png) are to be changed.From an end-user perspective, the application would appear and behave the exact same, no matter which state management solution is used.The logo is added to be able to differentiate which solution is running.A To Do-application is perfect to demonstrate CRUD.A FakeBackendService is provided to simulate a RESTful API backend.The idea is to load the list only once in the application’s lifetime and then update the state, without needing to fetch everything from the backend again.As such, the FakeBackendService logs its calls to the console for monitoring.The competitorsNGRX(v6.1.x) Docs  RxJS powered state management for Angular applications, inspired by Redux  @ngrx/store is a controlled state container designed to help write performant, consistent applications on top of Angular.NGXS(v3.2.x) Docs  NGXS is modeled after the CQRS pattern popularly implemented in libraries like Redux and NGRX but reduces boilerplate by using modern TypeScript features such as classes and decorators.Akita(v1.7.x) Docs  Akita is a state management pattern, built on top of RxJS, which takes the idea of multiple data stores from Flux and the immutable updates from Redux, along with the concept of streaming data, to create the Observable Data Stores model.  Akita encourages simplicity. It saves you the hassle of creating boilerplate code and offers powerful tools with a moderate learning curve, suitable for both experienced and inexperienced developers alike.Plain RxJS(v6.0.x) DocsOne of the aims of a (progressive) web application is to minimize loading time by reducing the package size.In that light, some developers opt to not use a framework, but instead use plain RxJS.To simulate a store as much as possible, I’ve used BehaviorSubjects to hold the state and pipeable operators to modify the state.Fight1. Available toolingSince this post is aimed at developers, it might be best to first evaluate the tools available for developers.A Redux Devtools plugin exists for Chrome and Firefox, or it can be run as a standalone application.It allows developers to see the impact of a Redux action and time travel between these actions.Another useful feature available to Angular developers is Angular Schematics, which allow to create pieces of code through Angular CLI.None of the solutions have these tools in their default packages and they need to be installed separately.Redux DevToolsDevTools in NGRXNGRX provides @ngrx/store-devtools for DevTools.It works as expected, displaying the latest actions with their impact and the resulting state of the store.It’s possible to jump to specific actions and even skip actions.It also allows devs to dispatch an action directly from the DevTools itself, but does not verify that action’s payload.Implementing the tools is as easy as importing the following line to the AppModule:StoreDevToolsModule.instrument()NGRX DevTools provide options for displaying a maximum age of actions, displaying a name, logging only to console, sanitizing state and actions and serializing the state.DevTools in NGXSAlthough NGXS is also modeled after CQRS, it behaves a bit differently.It provides @ngxs/devtools-plugin for DevTools.It does, however, not support all functionalities.The latest actions can be viewed with their impact and resulting state.But while it’s possible to jump to specific actions, it’s not possible to skip actions or dispatch new ones using the DevTools.Implementing the tools is just as easy as with NGRX, importing the following line to the AppModule:NgxsReduxDevtoolsPluginModule.forRoot()NGXS also provides some options for displaying a maximum age of actions, displaying a name and sanitizing state and actions.DevTools in AkitaAkita is the only solution not powered by a Redux-like pattern.That is why it also has limited functionality in the DevTools.The DevTools are available through @datorama/akita-ngdevtools.Similar to NGXS, the latest actions can be viewed with their impact and the resulting state.And similar to NGXS, it’s possible to jump to specific actions in the timeline, but impossible to skip actions or dispatch new ones using the DevTools.What’s more is that the raw action does not present the actual payload.When adding custom actions, you also have to name them with the @action decorator.Implementing the tools is, as ever, possiblie by importing the following line to the AppModule:AkitaNgDevtools.forRoot()Akita’s DevTools plugin also provides some options for displaying a maximum age of actions, a blacklist and a whitelist.DevTools in plain RxJSSince RxJS itself is no Redux-based storage solution, it obviously does not provide any support for Redux DevTools at all.SchematicsSchematics in NGRXNGRX has quite a lot of schematics available through @ngrx/schematics.It allows to create stores, feature stores, reducers, actions, container components, effects, entity stores all with a lot of options.In my To Do-applications, most of the work was done using two simple commands:ng g @ngrx/schematics:store AppState --module app.module.ts --root --statePath statemanagementng g @ngrx/schematics:entity statemanagement/TodoItem --reducers index.tsThe first command added the StoreModule and StoreDevToolsModule into AppModule and created a reducer in statemanegement/index.ts.The latter command created the following files:  statemanagement/todo-item.actions.ts, with a lot of premade actions for inserting, updating, upserting, removing one or multiple entities.  statemanagement/todo-item.model.ts, with a premade model interface I changed to just export the TodoItem interface which I created for the base application.  statemanagement/todo-item.reducer.ts, and its corresponding spec file handling the generated actions (the spec file did however only test ‘unknown action’) and providing several basic selectors, though I had to modify the following code for the selectors to work:export const selectTodoItemState = createFeatureSelector&lt;State&gt;('todoItem');export const {  selectIds,  selectEntities,  selectAll,  selectTotal,} = adapter.getSelectors(selectTodoItemState);The attempt to create extra actions using the following command was not as easy as it seemed.ng g @ngrx/schematics:action statemanagement/FilterOnly one action was created called LoadFilters, with the type [Filter] Load Filters.It would’ve been easier if one could specify the name of the action some more.I ended up extending the generated Entity store with the filter and sorting properties for which I had to add extra reducers and selectors manually.These properties could have been part of a separate state, but I opted to keep the store as simple as possible.Schematics in NGXSNGXS does not offer any schematics extensions.It does, however, offer a CLI through ngxs-cli.Although the documentation makes mention of the package @ngxs/ngxs-cli, this package was at the time of writing not available.Using the CLI, a state file todo-items.state is created, along with todo-items.actions.ts with 1 example action (add), to add an item to an array.Other than that, everything must be done by yourself, including adding importing the module into AppModule.The CLI offers some options for a name, whether or not to create a spec-file, the path and the name of the folder to create.Schematics in AkitaAkita does offer schematics through akita-schematics.It allows to separately create a store, model, query and service (meant for http), or everything together in what they call a feature.I used the following command to create a feature store:ng g akita-schematics:feature statemanagement/todoItemsThis created the following files:  statemanagement/state/todo-item.query.ts, which is used for selecting items from the state.  statemanagement/state/todo-item.model.ts, with a premade model interface I changed to just export the TodoItem interface which I created for the base application.  statemanagement/state/todo-item.store.ts, which contains the actions that can be performed.Since I didn’t use the --plain option, the state created by that command was an extension of EntityState, and already has some actions available for setting, inserting, updating and removing entities.Similar to NGRX, it was easy to extend the state with a filter and a sort property.Akita also provides a CLI tool, but I didn’t test this.Schematics in plain RxJSSince RxJS is based on operator functions, it’s nearly impossible to have useful schematics for this use case.This means that a developer must write everything by hand.Tooling summary            Tooling      Redux DevTools      Schematics                  NGRX      Yes      Yes              NGXS      Yes, limited      No, but limited CLI              Akita      Yes, limited      Yes, also CLI              Plain RxJS      No      No      2. FeaturesThis is a tough one.I didn’t have use cases in a To Do-application to research every possible feature.But, nevertheless, let’s cover the most useful features and their solutions.Feature: Asynchronous actionsWhat is meant with asynchronous actions, is that an action is dispatched to the store and the store is updated in an asynchronous way.An example of this is the use of a FetchItems action, which performs a request to the backend and dispatches one or multiple different actions when that request completes.This is especially useful when using a realtime database or Google Cloud Firestore, which opens a socket and can emit multiple events.In the example application, I’ve implemented this for a one-time fetch of items where possible.NGRX can handle this with @ngrx/effects, a separate package to be installed.Effects can be added to the root module or to a feature module for lazy loading.They can react on any Observable (not only emitted actions) and must emit a new action.If multiple actions should be emitted, these must be flatMapped.There is also a schematics extension available to generate an Effects class.NGXS allows actions to be handled asynchronously out-of-the-box.These actions can dispatch different actions, but can also modify the state directly.An Observable or Promise must be returned to notify the dispatcher that the action has been completed.Akita does not have support for asynchronous actions.The subscription to an asynchronous stream of data must be handled by yourself.While RxJS is effectively the reason asynchronous actions can exist in Angular, it is quite difficult for novices to update the store from a stream.Feature: Memoized SelectorsNGRX offers support for Selectors as constants.These can be easily chained in other selectors, making them ideal when the store is being refactored.NGXS works similar, but uses functions inside the State class.They can be chained, but it’s not as clear to understand as the NGRX solution.A neat feature within NGXS, is the so-called shared selector, which allows to create a selector that can be used with different states.Akita takes a different approach.A Query class is created, in which functions and constants can be defined.These return Observables, which can be used to obtain a part of the store and can be combined using RxJS operators.Unlike NGRX and NGXS, Akita does not easily offer selecting queries across different states in the store, without creating substates.As ever, RxJS, must throw in the towel for this. When you need something from the store, you’ll need to use some operators to get that specific item.Feature: PersistenceNGRX does not offer any persistence logic itself.There is however a 3rd party package available, ngrx-store-localstorage, which works through a meta reducer.It offers some options, one of which is setting the Storage interface and which keys to sync and how to (de)serialize those items.NGXS does have its official plugin, @ngxs/storage-plugin, a separate module that can be imported into the AppModule.It also has options for setting the Storage interface and which keys to sync and how to (de)serialize those items, but also offers migration strategies.This allows for a version with a radically changed store to not meet with synchronization errors.Akita’s main package includes a persistState() function.Including this function in the main.ts file allows the state to be stored in either localStorage or sessionStorage.Other options include, setting the key by which the state is saved, and including/excluding several aspects of the store and how to (de)serialize those items.When using plain RxJS, you’re on your own again.Other featuresThe frameworks offer even more features.I’m not going into detail for each of them.Most of them are included in the following summary.Features summary            Features      NGRX      NGXS      Akita      Plain RxJS                  Async actions      Yes, through effects      Yes      No      No              (Memoized) selectors      Yes      Yes      Yes, as queries      No              Cross-state selectors      Yes      Yes      No      No              Offline persistence      3rd party package      1st party package      Main package      No              Snapshot selection without first()      No      Yes      Yes      No              Forms synchronization      3rd party packages      1st party package      Main package      No              Router synchronization      1st party package      1st party package      No      No              WebSocket      3rd party package      1st party package      No      No              Angular ErrorHandler      No      Yes      No      No              Meta Reducers      Yes      Yes      No      No              Lazy loading      Yes      Yes      Yes      No              Cancellation      No      Yes      No      No              Side effects      Yes      Yes      No      No              Web workers      No      No      Yes      No              Transactions      No      No      Yes      No      3. Boilerplate codeIn this round the boilerplate code is evaluated.This is code that is needed for each part of the state, but differs only a little per state.I opted not to use immer for immutable state changes to give each competitor the same chances.Also within this section, there is the amount of files needed or generated for the To Do-application.Starting with NGRX, which generated 9 files through schematics.These files include the reducer file.Even though I created an Entity store, to ease the use of an entity collection, the reducer file still contained a lot of code through the adapter.A lot of which were the reducer cases for all the adapter’s actions, like set, insert, upsert and delete one or many items.Even though these cases only call the adapter’s functions, most of these methods won’t change and it would be nicer if these could have been part of the @ngrx/entity package, like the generated selectors.The same argument holds for the actions created in todo-item.actions.tsNGXS fairs a little better in this aspect.It generated only 3 files, though each action had to be written out myself and I created extra files for other actions.And even though the action functions can refer to Generic functions, it’s a shame an Entity State with specialized functions is not included in the package.Akita generated just 4 files using the schematics.Because I used an EntityState, a lot of Query functions and Actions were readily available, without them taking extra space.With RxJS I managed to create an operator function for each ‘action’ very simply.The application is quite simple, but the method is scalable enough without much overhead.Summary            Boilerplate      Files generated      Total files (*)      Boilerplate code                  NGRX      9      12      Heavy              NGXS      3      7      Medium              Akita      4      6      Low              Plain RxJS      0      6      Medium      4. CommunityBased on Google Trends of the past 12 months, NGRX is obviously the most searched for state manager.The reason is likely that it was the first Redux implementation available for Angular.When looking at the GitHub repositories, NGRX has the most stars (at over 3.5K), followed by NGXS (at 1.4K) and Akita (at around 480).Again this indicates NGRX is the most popular framework.But what about contributors?Looking at the repositories’ insights, it’s clear that the same sequence is followed.NGRX takes the lead, NGXS a solid second and Akita last.There it’s also visible that NGRX is still under very active development, looking at the commits.NGXS meanwhile stagnated and Akita has a steady pace.Community summary            feat.      Google Trends      GitHub stars      Contributors      Commits                  NGRX      1st      1st      1st      1st              NGXS      2nd      2nd      2nd      3rd              Akita      3rd      3rd      3rd      2nd      5. Dependencies and sizeState management does not come out-of-the-box with Angular. There is a need to install extra dependencies.Luckily all these dependencies are available through npm.To make the different implementations as feature-equal as possible, I’ve decided to create entity stores where possible and include dev-tools if available.Furthermore, all implementations were built with and without production mode.For comparison purposes, the base application measured in at 14.6MB without production mode, and a mere 754KB with production mode.NGRX is a heavy hitter.It included multiple dependencies for different features. @ngrx/store is the basis.@ngrx/store-devtools, @ngrx/entity, @ngrx/effects and @ngrx/schematics complement this, although the schematics are dev-only.All this gives the packages a weight of 14.9MB without production mode and 786KB with production mode.NGXS fairs a little better.It only includes @ngxs/store and @ngxs/devtools-plugin.This makes the packages weigh in at 14.8MB without production mode and 778KB with production mode.Akita also has an all-in-one package for the store.@datorama/akita holds all functionality, while @datorama/akita-ngdevtools and akita-schematics provide some development tools.Despite this, Akita overthrows NGRX with 15.4MB without production mode and matches NGXS with 778KB with production mode.The difference between NGXS and Akita in production mode was a mere 24B.RxJS is the clear winner here.It needs no extra dependencies whatsoever as RxJS already is a dependency of Angular, making the packages 14.6MB without production mode and 762KB with production mode.Dependencies and size summary            Size      Non-production (MB)      Production (KB)                  Base      14.6      754              NGRX      14.9      786              NGXS      14.8      778              Akita      15.4      778              Plain RxJS      14.6      762      Final scoreIt’s not easy to just say which solution is the all-time champion.Each of the competitors has its advantages and disadvantages.These are the podium places for each round:            Round      NGRX      NGXS      Akita      Plain RxJS                  Tooling      1st      3rd      2nd                     Features      2nd      1st      3rd                     Boilerplate code      3rd      2nd      1st      2nd              Community      1st      2nd      3rd                     Dependencies and size      3rd      2nd      2nd      1st      "
      },
    
      "cloud-2018-10-01-how-to-build-a-serverless-application-with-aws-lambda-and-dynamodb-html": {
        "title": "Create a Serverless Application with AWS Lambda and DynamoDB",
        "url": "/cloud/2018/10/01/How-to-build-a-Serverless-Application-with-AWS-Lambda-and-DynamoDB.html",
        "image": "/img/2018-10-01-How-to-Build-a-Serverless-Application/AWS-Lambda-and-DynamoDB.png",
        "date": "01 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Table of content  Introduction  Serverless: What &amp; Why  What we will build  Prerequisites  DynamoDB  Lambda: scan DynamoDB  API Gateway: Access the scan Lambda  Lambda: Write to DynamoDB  API Gateway: Access the write Lambda  What is next?  Extra resourcesIntroductionReady to create a serverless application?New to AWS and curious to learn more?Read on and learn more about the AWS services by building a serverless app!  Serverless: What &amp; Why  A serverless architecture is a way to build and run your applications without having to think about infrastructure.You no longer have to maintain servers to run your applications, databases and storage systems.And most of all, it is so easy!  Yes, once you get the hang of it, it really is mind-blowingly easy.However, first you need to know the basic infrastructure to set up a serverless application.Let’s do this!What we will buildJoin me in building a serverless application in which users can give great coding tips to each other. To keep it as simple as possible we will build everything through the AWS Console and focus on the infrastructure.No need to deploy any code from your computer to AWS.DemoI could show you a frontend that uses our serverless backend to give and get coding tips.But that would be an extra layer between you and our serverless application.Here, I am triggering the app with Curl.Post a new Coding Tip to the database:curl -X POST \\    https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips \\    -H 'Content-Type: application/json' \\    -d '{    \"author\": \"Nick\",    \"tip\": \"Learn by doing\",    \"category\": \"General\"  }'A new item was added to the CodingTips database.I added a few already and can retrieve them too.View all the Coding Tips that are currently in the database:curl -X GET https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips  Architecture  The coding tip items are stored in a NoSQL database AWS DynamoDB.There are two Lambda Function in play.One to GET the coding tip items from the database and one to POST a new coding tip item to the database.The user can access these Lambda Functions through an API provided by the AWS API Gateway service.This Gateway will redirect to the right Lambda Function based on the HTTP method (POST or GET).Both Lambda Functions are connected to CloudWatch where you can view the logs of your functions.AWS IAM is used to give the services the right permissions to connect to each other.PrerequisitesTo follow along you need:  an AWS account.If you do not have one already, you can create one by following these steps from the official guidelines:Create an AWS account.You have to provide a credit card number to create an account.Don’t worry! The AWS-Free-Tier provides plenty of resources that widely exceed what you will use for this tutorial.If you ask me, AWS is really offering a fantastic amount of stuff for free.You should be grateful for this, it will give you plenty of time to get to know the AWS Services.  coding enthusiasmDynamoDB  Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.– from AWS docs: https://docs.aws.amazon.com/amazondynamodbCreate a database to store your items.Login to the AWS Console and under Services go to DynamoDB.Click on Create table.Name the table CodingTips. As primary key make a Partition key  author, type String’.Check the Add sort key checkbox and choose date, type Number as a sort key for your table.Leave the default settings checked and hit Create.  Notice the Amazon Resource Name ARN property.We will use this later to point to this DynamoDB table.You just created the DynamoDB table that the application will use.Awesome!Add elements to CodingTips tableManually add some elements to the CodingTips table.Go to the CodingTips table, open the Items tab and click Create item.Add a couple of random items to the table as shown in the image below.Notice that date is in milliseconds.These are the milliseconds that have past since the Unix Epoch 1970-01-01.For example, 1538368878527 equals Mon 1 October 2018 06:41:18.Hit Save to store the item in the database.  I added a couple of items as you see in the image below.  Notice that I did not add a coding tip yet.We will do this later by using a Lambda Function!Lambda: Scan DynamoDB  AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.– from AWS docs: https://docs.aws.amazon.com/lambdaLet’s build a Lambda that scans the DynamoDB table for our items.In the AWS Console under Services navigate to Lambda.Click the Create Function button to start creating a Lambda.Choose Author from Scratch and start configuring it with the following parameters:  Name: CodingTips_Scan  Runtime: Node.js 8.10  Role: Create a custom role  Selecting Create a custom role will take you to another page to create this new role.The role is used to give the Lambda Function the right permissions.Configure the role as shown in the image below.If everything went well you should only have to adapt the name of the role.Name it lambda_dynamodb_codingtips.The rest will be automatically generated for you.  Click Allow.Hit Create function to create the Lambda.This will open the designer view of your Lambda Function.  One thing is missing here.The Lambda Function has the authority to send its logs to CloudWatch.This authority is given by the role we just gave it.However, it is mentioned nowhere that it has the right to access the CodingTips table.We should arrange this too.Configuring the Role for the Lambda Function  AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.– from AWS docs: https://docs.aws.amazon.com/IAMUnder AWS Services navigate to IAM (Identity and Access Management).Under roles find the lambda_dynamodb_codingtips role and click it.It has one policy (for the CloudWatch logs) attached to it already.Click Add inline policy and go to the JSON tab. In the JSON tab add the following JSON to configure this new policy.Add the arn that points to your own CodingTips table!You can find this in the Overview tab of your table which we showed above.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": \"dynamodb:*\",            \"Resource\": \"arn:aws:dynamodb:eu-west-1:389795768041:table/CodingTips\"        }    ]}Click Review policy and name it Lambda-DynamoDB-CodingTips-Access.Hit Create policy.You now attached a new policy to the existing lambda_dynamodb_codingtips role.The role summary looks like this:  Go back to the designer view of the CodingTips_Scan Lambda.Now you see that the Lambda Function has the right to connect to the DynamoDB table.  Function CodeYeah, finally it is time for some code!In the configuration window of the lambda add the code in the Function code block.‘index.js has to contain the following code:console.log('function starts');const AWS = require('aws-sdk');const docClient = new AWS.DynamoDB.DocumentClient({region: 'eu-west-1'});exports.handler = function(event, context, callback){    console.log('processing event: %j', event);    let scanningParameters = {        TableName: 'CodingTips',        Limit: 100 //maximum result of 100 items    };    //In dynamoDB scan looks through your entire table and fetches all data    docClient.scan(scanningParameters, function(err,data){        if(err){            callback(err, null);        }else{            callback(null,data);        }    });}Save the Lambda Function to persist the changes.  The handler function is the function where the Lambda execution starts when the Lambda is triggered.  The event parameter contains the data from the event that triggered the function.  The scanningParameters are used to configure the scan of the table.  This function scans the DynamoDB table for the first 100 items it finds.  docClient.scan(scanningParameters, function(err,data) executes the scan and returns either the result or the error that occurred.Test write-LambdaAll right! Let’s test this thing..On the Lambda Function configuration page you see a dropdown and test button in the upper right corner.Click the dropdown and configure a new test event.I called mine Test and added an empty test event {}.  Save it and you are ready to test the Lambda.From the dropdown select your test event and hit the Test button!Nice one, this returns the items in your table:  API Gateway: Access the scan LambdaMmmh, fine.. We can trigger the Lambda Function with a test event.But we want to be able to trigger it from anywhere using a URL.In the designer view of the lambda you can still see add triggers from the left.Well, let’s add that trigger!To expose a Lambda Function AWS provides the API Gateway.Under Services navigate to API Gateway.  Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a “front door” for applications – from AWS docs: https://aws.amazon.com/api-gatewayBasically this is the Service you use to create all of your API’s.  Click Create API and name your api CodingTips  Add a description if you like  Leave the Endpoint Type to regional and Create API  The API has been created.Configure it by adding a resource.  Under Actions click Create Resource and name it tips with /tips as Resource Path  Check Enable CORS to make your API accessible from anywhere  Hit Create ResourceTime to configure the HTTP GET request.  Select the /tips endpoint  Under Actions select Create Method and select GET.  Integration type is Lambda Function  As Lambda Function provide the name of the lambda.In this case that is Codingtips_Scan.  Save the configuration.  Only one thing left: select the API and under Actions click Deploy API. You will be asked to provide a name for the stage. Name it default.In the Stages tab click the GET method and copy the Invoke URL.  This is your gateway to trigger the lambda.Since we just created a HTTP GET request you can use either your browser, Curl or Postman to do this.In a browser tab past the Invoke URL.  From the command line with Curl execute this command with your own Invoke URL:curl -X GET https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tipsEither of the above actions will return the items in the CodingTips table!Congratulations, you just created your first serverless app!Did you know:  performing a scan on a DynamoDB table will return the items in a random order  you just joined the club of serverless application developers  you should be proud of yourselfCommon errors  Missing Authentication Token:          Check the URL you are trying to invoke.  Does it have the format ‘https://{domain}/{stage}/{method}’.  Stage and method were set when creating the API Gateway.      Enable CORS for your API Gateway      Made changes to the API Gateway? Make sure to redeploy the API.        Lambda Exceptions:          Check CloudWatch for logs.  Under Services go to CloudWatch.  In the Logs tab access the Log Group /aws/lambda/CodingTips_Scan to view the logs of the Lambda.        Trouble with API Gateway:          Enable API Gateway logging      Lambda: Write to DynamoDBUsers should be able to POST new items to the table.This is possible when we create a Lambda Function to write to the database.Create write-LambdaIn the AWS Console under Services navigate to Lambda.Click the Create Function button to start creating a Lambda.Choose Author from Scratch and start configuring it with the following parameters:  Name: CodingTips_Write  Runtime: Node.js 8.10  Role: Choose an existing role  Existing role: lambda_dynamodb_codingtipsCreate the function.The CodingTips_Write Lambda Function already has access to CloudWatch and DynamoDB.This is because we gave it the existing lambda_dynamodb_codingtips role that has policies which allow these access.The designer view of the Lambda Function now looks like this:  Function CodeLet’s add the code of this function!In the configuration window of the lambda add the code in the Function code block.Enter the following code in the index.js file:console.log('function starts')const AWS = require('aws-sdk')const docClient = new AWS.DynamoDB.DocumentClient({region: 'eu-west-1'})exports.handler = function(event, context, callback){    console.log('processing event: ' + JSON.stringify(event, null, 2))    let currentMonth = new Date().getMonth() + 1     let currentYear = new Date().getFullYear()    let params =  {        Item: {            Date: Date.now(),            Author: event.author ? event.author : \"Anonymous\",            Tip: event.tip,            Category: event.category,            MonthAttribute: currentMonth,            YearAttribute: currentYear,            YearMonthAttribute: currentYear + \"-\" + currentMonth        },        TableName: 'CodingTips'    };    docClient.put(params, function(err,data){        if(err) {            callback(err, null)        }else{            callback(null, data)        }    });}Save the Lambda Function to persist the changes.What happens in this Lambda Function:  The event parameter of the handler function contains the data from the event that triggered the function.  The params are used to configure the scan of the table.  The Item object contains the data that has to be put into the table.  The Item not only contains the Date and Author, but also other attributes like the Tip itself and Category..That’s allowed because it is a NoSQL database. The MonthAtrribute, YearAttribute and YearMonthAttribute are added automatically.  docClient.put(params, function(err,data) executes the write and returns either the result or the error that occurred.Test write-LambdaConfigure a new test event called test and add the following JSON attributes:    {      \"author\": \"Nick\",      \"tip\": \"Don't hesitate to ask for help when you need it\",      \"category\": \"General\"    }Save it and test the lambda by hitting the test button.Execution result: succeeded? Go to the CodingTips table and you will see a new item that was added into your table.  API Gateway: Access the write LambdaAgain we need to expose our Lambda Function via an API Gateway so that users can post messages to it.  Under Services navigate to API Gateway.  Click the CodingTips API that we created already for the GET Request.  You need to add a HTTP POST Method to this API.Under Resources click /tips, Actions, Create Method and select POST.The Integration Type is Lambda Function.The name of that Lambda function is CodingTips_Write, which we just created.    Hit Save to create the new method.  When AWS asks you, add the permission to the Lambda FunctionWe want to pass a JSON object to this API.The API in turn has to pass on the JSON to the Lambda.To enable this, click on Integration Request and under Mapping Templates check When there are no templates defined (recommended)  Add mapping template with Content-Type application/json.Add this template and save:{  \"author\": $input.json('$.author'),  \"tip\": $input.json('$.tip'),  \"category\": $input.json('$.category')}  Under Actions click Deploy API. You will be asked to provide a name for the stage. Select the default stage and Deploy.In the Stages tab there is an Invoke URL.  This is your gateway to trigger the lambda.Since we just created a HTTP POST request you can use either Curl or Postman to do this.From the command line with Curl execute this command with your own Invoke URL:curl -X POST \\    https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips \\    -H 'Content-Type: application/json' \\    -d '{    \"author\": \"Nick\",    \"tip\": \"Learn by doing\",    \"category\": \"General\"  }'You just added a tip to the CodingTips table!This can be checked by invoking the GET method of the API Gateway we designed in the beginning of this article.Use your browser or curl to check the items in the table.curl -X GET https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips  Common errors  Missing Authentication Token:          Check whether the Mapping Template under the Integration Request of your API Gateway is correct      Check the URL you are trying to invoke.      Made changes to the API Gateway? Make sure to redeploy the API.        Lambda Exceptions:          Check CloudWatch for logs.  Under Services go to CloudWatch.  In the Logs tab access the Log Group /aws/lambda/CodingTips_Write to view the logs of the Lambda.        Trouble with API Gateway:          Enable API Gateway logging      What is nextSome suggestions to keep you busy:  Query DynamoDB instead of scanning  Create GSI (Global Secondary Index) to query and sort  Create a frontend that uses this serverless infrastructure as backend  Deploy this infrastructure with AWS Cloudformation  Deploy using a Jenkins pipeline  Run locally with SAM LocalExtra resources  AWS Lambda: AWS Lambda Introduction  AWS DynamoDB: AWS DynamoDB Introduction  AWS API Gateway: AWS API Gateway  AWS IAM: AWS IAM Introduction  AWS CloudWatch: Getting started with AWS CloudWatch  Using the DynamoDB docClient: AWS DocClient Example"
      },
    
      "ionic-2018-09-30-e2e-testing-ionic-protractor-appium-e2e-testing-html": {
        "title": "Automated E2E (End-to-End) testing on Android and iOS with Ionic, Protractor and Appium.",
        "url": "/ionic/2018/09/30/e2e-testing-ionic-protractor-appium-e2e-testing.html",
        "image": "/img/ionic-protractor-appium.jpg",
        "date": "30 Sep 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Automated testing  Appium  Getting started with Appium  Configuring Protractor and the Ionic project  Running Tests  Conclusion  Example repositoryIntroductionMany articles related to E2E testing Cordova/Ionic applications are about applications which only run in the browser.But what if your application run’s on native mobile devices?This article will cover how to get started with E2E testing your Ionic application on native Android and iOS devices.To follow along, I recommend having a basic understanding of Javascript, TypeScript, Jasmine and automated testing in general.Automated testingWhen releasing an application, we need to make sure it was thoroughly tested.Making sure of discovering any bugs before reaching production.These tests or test scenarios can be done manually, but this would consume a lot of time and resources.The more cost-effective solution would be to automatically run these test scenario’s entirely by a programmable agent.Thanks to a few technologies, we can script a bot that can perform most user interface interactions, such as clicking on a button, performing touch gestures (f.e. swiping), etc.The most popular solution for automated E2E tests is called Selenium which is based on the WebDriver protocol.While Selenium is a great solution for browsers, there is a better solution for native mobile apps called Appium.AppiumAppium is a tool for automating mobile applications and writing cross-platform UI tests. It is very similar to Selenium. The difference is that Selenium is a tool for automating browsers and web applications, whereas Appium is a tool for automating Native / Hybrid mobile applications.Appium allows developers to write UI tests for mobile applications in any number of programming language (Javascript, Java, .NET, etc.), as it uses a superset of the Selenium WebDriver specification, called the MJSONWP protocol.Cross-platform UI testingBecause we are using Ionic with Cordova, we can write our codebase using only web technologies but still build, deploy and run on multiple platforms.Our mobile application can be packaged and deployed as a native application for both iOS and Android.We can achieve the same cost-savings strategy “Write once, run anywhere” for our UI tests using Appium.To automate our UI tests, there needs to be an agent that programmatically drives the UI of your mobile application.For each platform there are different agents:  iOS: XCUITest  Android: UIAutomator, Selendroid, EspressoControlling these agents requires a developer to write platform-specific code.Appium with MJSONWP (Webdriver spec) provides an abstraction layer to drive these agents programmatically in a platform agnostic way.We will explain how to set up your Appium server and run automated UI tests in your Ionic application on Android and iOS mobile devices.Getting started with AppiumEnvironmentThe first step is to setup your environment.Because we are targeting Android and iOS we will only describe the setup for macOS, but it shouldn’t be too different compared to other platforms once you have followed the official ionic resources guide below, they have guides for all platforms.Again: On the developer resources page of the official ionic documentation, you wil find guides on how to setup your machine depending on the OS you are working on.Next:  Install appium-doctor using npm.  Run appium-doctor –ios and fix any issues  Run appium-doctor –android and fix any issuesAppium serverThere are multiple ways to start an Appium server:  Appium Desktop  webdriver-manager  npm install -g appium &amp;&amp; npm run appiumAppium desktopAppium Desktop is a graphical user interface for running an Appium server and starting sessions to inspect your applications.Note: For macOS make sure to drop the downloaded package in the /Applications folder.Otherwise you will encounter write permission issues.Appium desktop has two advantages:  It comes with an inspector to show and inspect all elements of your application  Record user actionsThe drawback is inspecting and recording user actions only supports the Native context.You cannot record actions for the Webview context.Cordova applications always run in the webview context.https://appium.io/docs/en/writing-running-appium/web/hybrid/index.htmlwebdriver-managerSelenium and appium server managerWebdriver-manager is officially supported by Angular and works well together with Protractor,the official E2E testing framework for Angular applications.Ionic up until version 3.x is built on top of Angular,from version 4 and on Ionic has decoupled from the Angular framework and recreated all of their components using StencilJS.NPMWe will be using this package to start up our Appium server.LanguageDecide in which language you want to write your tests in.You need to have a client library that can send MJSONWP / JSONWP HTTP Requests to the Appium server.For our application, we will write our tests in TypeScript using Protractor since it has added support for Angular and type safety.Other webdriver javascript clients:  http://webdriver.io/guide/getstarted/modes.html  https://github.com/admc/wdClient libraries for different languagesProject setupWe are going to use Ionic 4 and the super template as our application to run our tests against.First, make sure your development machine has been set up correctly.On the developer resources page of the official ionic documentation,you will find guides on how to set up your machine depending on the OS you are working on.Once your machine is set up, install the Ionic CLI.npm i -g ionicNext, generate the Ionic Cordova application using the Ionic CLI.ionic start superApp super --type=ionic-angular --cordovaionic cordova platform add androidionic cordova platform add iosTest if you can build the application by entering the following commandsAndroidionic cordova build androidiOSNote: You will have to open your ios project in xcode first to configure your code signing identity and provision profile.ionic cordova build iosIf you were able to run these commands successfully, we can start E2E testing our application on both iOS and Android.Before continuing, make a folder /e2e in the root of your project.Configure the E2E testing tools in your Ionic projectAppium  Install Appium as a local dependency  Add the correct chrome driver  Create an NPM task in your package.json  Boot up the appium service1. Install Appium as a local dependencyJust run the following command to add Appium as a local dependency, this will allow us to work with Appium using NPM scripts.npm i -D appium2. Add the correct chrome driverTo be able to run your tests on Android devices, you need to match the correct chrome driver with the Chrome version running on the Android test devices.Here is an overview of all the chrome drivers and their respective Chrome versions.To download a chrome driver, go to the Chrome Driver Downloads page.Once you have selected your chrome driver, download it and put in the /e2e folder.3. Create an NPM task in your package.jsonBefore running Appium, you can provide the downloaded chrome driver as a cli argument:\"appium\": \"appium --chromedriver-executable e2e/chromedriver\"4. Start your Appium serverNow you should have everything configured correctly to start your Appium server.Simply run:npm run appiumProtractorProtractor will be our test runner and testing framework.Visit their website for more information on Protractor.  Install protractor as a local NPM dependency  Configure typescript configs  Create your protractor config  Create NPM script for running your e2e tests1. Install Protractor as a local NPM dependencyInstall the test runner with the following command:npm install -D protractor1. Configure TypeScriptWe require a few extra tools to be able run and write our tests in TypeScript.npm install -D ts-node @types/jasmine @types/nodeNext, in your /e2e folder, create a tsconfig.json file with the following configuration:{  \"compilerOptions\": {    \"sourceMap\": true,    \"declaration\": false,    \"moduleResolution\": \"node\",    \"emitDecoratorMetadata\": true,    \"experimentalDecorators\": true,    \"lib\": [      \"es2016\",      \"esnext.asynciterable\"    ],    \"outDir\": \".\",    \"module\": \"commonjs\",    \"target\": \"es5\",    \"types\": [      \"jasmine\",      \"node\"    ]  }}This will be our tsconfig for our e2e test scripts.It’s also a good idea to write your configuration files in TypeScript.For our protractor configuration, we will use a different typescript configuration file.In your /e2e folder, create a file called /e2e/protractor.tsconfig.jsonThis configuration file will extend the one we created earlier, we want to overwrite the include and exclude parameters to make sure it only matchesand transpiles the protractor.config.ts file.{  \"extends\": \"./tsconfig.json\",  \"include\": [    \"**/*.config.ts\"  ],  \"exclude\": [    \"./test\"  ]}3. Configure protractorNow one of the more exciting parts, configuring Protractor!Create a file called /e2e/protractor.config.ts with the following contents:import {Config} from 'protractor';import * as tsNode from 'ts-node';const serverAddress = 'http://localhost:4723/wd/hub';const testFilePAtterns: Array&lt;string&gt; = [  '**/*/*.e2e-spec.ts'];const iPhoneXCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  app: '/Users/${user}/ordina/e2e/superApp/platforms/ios/build/emulator/superApp.app',  version: '11.4',  platform: 'iOS',  deviceName: 'iPhone X',  platformName: 'iOS',  name: 'My First Mobile Test',  automationName: 'XCUITest',  nativeWebTap: 'true'};const androidPixel2XLCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  platformName: 'Android',  deviceName: 'pixel2xl',  app: '/Users/${user}/ordina/e2e/superApp/platforms/android/build/outputs/apk/android-debug.apk',  'app-package': 'be.ryan.superApp',  'app-activity': 'MainActivity',  autoAcceptAlerts: 'true',  autoGrantPermissions: 'true',  newCommandTimeout: 300000};export let config: Config = {  allScriptsTimeout: 11000,  specs: testFilePAtterns,  baseUrl: '',  multiCapabilities: [    androidPixel2XLCapability,    iPhoneXCapability  ],  framework: 'jasmine',  jasmineNodeOpts: {    showColors: true,    defaultTimeoutInterval: 30000  },  seleniumAddress: serverAddress,  onPrepare: () =&gt; {    tsNode.register({      project: 'e2e/tsconfig.json'    });  }};To get an idea of all the configuration parameters and their description, visitThe official Protractor Github repoI will go over the points that took me the most effort to configure correctly.CapabilitiesRefers to the capabilities of a single E2E session, it describes which features a particular session should have, for example:  Platform (Android / iOS / …)  Device Name  Automation framework  Location of the Application build (.apk, .ipa)  etc.If you want to spin up multiple E2E testing settings, you need to configure the multiCapabilities property.Android Capability  Run ionic cordova build android and configure the output path in the app property  app-package should match the package name in your config.xml  app-activity is always MainActivity by default unless you have changed this in your config.xmlconst androidPixel2XLCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  platformName: 'Android',  deviceName: 'pixel2xl',  app: '/Users/${user}/ordina/e2e/superApp/platforms/android/build/outputs/apk/android-debug.apk',  'app-package': 'be.ryan.superApp',  'app-activity': 'MainActivity',  autoAcceptAlerts: 'true',  autoGrantPermissions: 'true',  newCommandTimeout: 300000};iOS Capability  Run ionic cordova build ios and configure the output path in the app property  Point to the .app file and not the .ipa if you are using simulators.  Set automationName to XCUITest instead of the deprecated UIAutomator  browserName is a mandatory parameter, but since we’re targeting Native apps, we can leave this as an empty stringconst iPhoneXCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  app: '/Users/${user}/ordina/e2e/superApp/platforms/ios/build/emulator/superApp.app',  version: '11.4',  platform: 'iOS',  deviceName: 'iPhone X',  platformName: 'iOS',  name: 'My First Mobile Test',  automationName: 'XCUITest',  nativeWebTap: 'true'};4. Create an NPM script for running e2e testsIn your package.json, add the following task:\"e2e\": \"tsc --p e2e/pro.tsconfig.json &amp;&amp; protractor e2e/protractor.config.js --verbose\"Running and Writing UI testsFor Protractor to know which tests to run, you need to configure the specs property, in our case all the files that end with .e2e-spec.tsconst testFilePAtterns: Array&lt;string&gt; = [  '**/*/*.e2e-spec.ts'];export let config: Config = {  ...  specs: testFilePAtterns  ...};If you followed along, you should be able to run your tests by entering the following command:npm run e2eWriting protractor tests is out of scope in this post, but here is an example test script that you should be able to run on both iOS and Android.import {browser, by, element, ElementFinder, protractor} from 'protractor';describe('App', () =&gt; {  describe('Tutorial Screen', () =&gt; {    it('should skip to the welcome screen and have the correct button labels', async () =&gt; {      const skipButton: ElementFinder = element(by.id('skip'));      await browser.wait(protractor.ExpectedConditions.elementToBeClickable(skipButton));      const skipButtonLabel: string = await skipButton.getText();      expect(skipButtonLabel).toEqual('SKIP');      skipButton.click();      const loginBtn: ElementFinder = await element(by.id('btn-login'));      await browser.wait(protractor.ExpectedConditions.elementToBeClickable(loginBtn));      const loginBtnLabel: string = await loginBtn.getText();      expect(loginBtnLabel).toEqual('SIGN IN');      loginBtn.click();    });  });});There are seven basic steps in creating an Appium test script.  Set the location of the application to test in the desired capabilities of the test script.  Create an Appium driver instance which points to a running Appium server  Locate an element within the mobile application.  Perform an action on the element.  Anticipate the application response to the action.  Run tests and record test results using a test framework.  Conclude the test.Webview And Native contextOur example application is a hybrid application.Meaning it will be packaged and deployed as native app so we can access Native API’s.But it will acually run inside a webview. By using Cordova our webview can communicate with Native API’s (f.e. Camera).When the Camera is launched, we enter a Native Context, if we exit the Camera and go back to our Hybrid application we return to the Webview Context.Appium helps us to easily switch between these contexts since locating and interacting with UI elements are very different in both contexts.For example, there are no DOM elements in the Native Context.To locate a native UI element you need to use an Accessibility ID. At the same time, AccessibilityID’s are not available in a Webview context.TouchEvents like Tap / Swipe / Drag ‘n Drop are only supported in the Native context.You can not use them in the Webview Context.Behaviour-driven development with CucumberCucumber is a tool for BDD.You can easily integrate Cucumber with Appium using Protractor cucumber framework on NPM.A typical workflow looks like this:Describe an app feature and corresponding scenarios in a .feature file. The contents are written in GherkinFeature: As an employee, I want to access the application@AuthenticationScenario: Authenticate with AzureADGiven I am on the Login pageWhen I click on \"Login\"When I provide my credentialsWhen I click on the \"Submit\" buttonThen I should see the Dashboard page@Authentication FailedScenario: Authenticate with AzureAD failsGiven I am on the Login pageWhen I click on \"Login\"When I provide incorrect credentialsWhen I click on the \"Submit\" buttonThen I should see the Login failed pageDevelopers write an implementation for the feature in a step definitions file:Given(/^I am on the Login page$/, () =&gt; {    expect(app.getTitle()).to.equal('Login');});When(/^I click on \"Login\", () =&gt; {    LoginPage.loginButton.click();});When(/^I provide my credentials$/, () =&gt; {    WindowsAuthPage.enterCredentials(usr,pass);});When(/^I click on the \"Submit\" button$/, () =&gt; {    WindowsAuthPage.submitButton.click();});Then(/^I should see the Dashboard page$/, () =&gt; {    expect(app.getTitle()).to.equal('Dashboard');});Let protractor and Appium run the step definitions in an Automated way.The advantage of using cucumber is that non-developers can easily write their own .feature files in plain English (Gherkin).This offers:  Better collaboration between Business &amp; Developers  Feature files can act as contracts for acceptance criteria  Better reporting and readability of the UI testsCloud testing providersThe following providers offer great support for Appium tests in the cloud:  Saucelabs  TestObject, only real devices, has been purchased by Saucelabs.  Kobiton, only real devices, allows you to connect your local mobile device farm.  ExperitestConclusionWhile investigating and hands-on experiencing Appium, I noticed the following trade-offs:  Tests can be flaky (Simply rerunning a failing test can make it succeed)  Tests on iOS take a while to run  Appium is slower than for example running tests directly with Espresso or XCUITest  Documentation can be outdated and is scattered around the web  Setting up an environment for iOS and Android takes a lot of time initially  UI tests can differ for each platform  Sending key events can be very slow, which make the tests run very slow  You need a good knowledge of WebDriver API’s to write good tests  Debugging is hard, I mostly relied on console.log statements.  Testing on Android needs to happen with Chrome Browser version 54+.For Android we are limited to recent Android API’s and devices with Chrome browser version 54+, this means we can not test older devices or devices with older Android versions.Setting up a local Appium server also takes a lot of setup and configuration, but this can be circumvented if you decide to go for a cloud testing provider like Saucelabs.Still, I believe Appium offers a lot of value because  We can write UI tests both for Android and iOS using a single programming language  You can automate manual testing for multiple platforms  There are quality cloud testing providers out there to help you with all your testing needsAnd once you have everything set up, it works quite well.Example repositoryhttps://github.com/ryandegruyter/ordina-ionic-appium-protractor"
      },
    
      "spring-2018-09-28-springone-fun-with-the-functional-web-framework-html": {
        "title": "SpringOne Platform - Fun With The Functional Web Framework",
        "url": "/spring/2018/09/28/SpringOne-fun-with-the-functional-web-framework.html",
        "image": "/img/2018-09-27-SpringOne-Platform/post-image.jpg",
        "date": "28 Sep 2018",
        "category": "post, blog post, blog",
        "content": "Fun with the Functional Web Frameworkby Arjen PoutsmaThis talk is a follow-up of a talk that Arjen Poutsma has been giving a few years now, called ‘New in Spring 5: Functional Web Framework’. In his new talk he goes more in depth in some of the features that are offered by the framework.What is it?The Spring functional web framework (called WebFlux.fn) is an alternative to the annotational style web framework, Web MVC.It was introduced in Spring 5.0 and for spring 5.1 they did some refinements in the API after feedback from developers.Design goalsThe WebFlux.fn framework had three main goals.The first one was to create a web framework with a functional style.By this they mean that they wanted to leverage the new functional concepts introduced in Java 8, like Function and Stream.The second goal was to make the framework fully reactive by using the functionality from Reactor.The third goal was to act more like a library and less like a framework.The reason for this is that many people don’t like the “automagic” things the Web MVC (annotational style) framework does.Web MVC does a lot of things behind the scenes that you as a client of the framework don’t know about, unless you read up on how the framework works internally.So acting more like a library instead of a framework means that a lot of things will be more explicit, so you as a client of the library will see more clearly what is going to happen.A fourth goal, that was more a side effect than intention, is that there is no more reflection in WebFlux.fn.By not using annotations anymore to map HTTP requests to controller methods, there is no more reflection.This has the great effect that your application will take less time to start up because Spring has to do less classpath scanning.This is also useful for when you want to use GraalVM.How does it workThere are three main concepts in the WebFlux.fn framework:  The HandlerFunction  The RouterFunction  The HandlerFilterFunctionWe’ll discuss these in the following sections.The HandlerFunctionIs a function that maps a ServerRequest to a Mono&lt;ServerResponse&gt;.public Mono&lt;ServerResponse&gt; showPet(ServerRequest request) {    String id = request.pathVariable(\"id\");    return this.petRepository.findById(id)            .flatMap(pet -&gt; ServerResponse.ok().contentType(APPLICATION_JSON).body(fromObject(                    pet)))            .switchIfEmpty(Mono.defer(() -&gt; ServerResponse.notFound().build()));}You can see in this example that there are some differences with a Web MVC controller method.A big difference is that we can only get a ServerRequest as a parameter. So if we want a path variable, body or anything else from the HTTP request, we have to get it from the ServerRequest variable.Spring does not inject this information as method parameters in WebFlux.fn.The second difference is that the object we return has to be a Mono&lt;ServerResponse&gt;.In Web MVC the return type could be a lot of different things like any type of Object, a ResponseEntity, etc.The RouterFunctionIs a function that takes a ServerRequest and returns a HandlerFunction using a RequestPredicate.@Beanpublic RouterFunction&lt;ServerResponse&gt; routerFunction(PetHandler petHandler) {    RouterFunction&lt;ServerResponse&gt; html = route()            .GET(\"/pets/{id}\", accept(TEXT_HTML), petHandler::renderPet)            .GET(\"/pets\", accept(TEXT_HTML), petHandler::renderPets)            .build();    RouterFunction&lt;ServerResponse&gt; json = route()            .GET(\"/pets/{id}\", accept(APPLICATION_JSON), petHandler::showPet)            .GET(\"/pets\", accept(APPLICATION_JSON), petHandler::showPets)            .build();    return html.and(json);}The order in which you define these router functions matters.The first router function’s  handler that matches your HTTP request will be the one that is executed.This makes it a lot clearer when you read the router functions to know which one will be executed, it’s the first one that you define and matches.An advantage of the RouterFunction over the annotational style is that you can map multiple endpoints to the same HandlerFunction.This is not possible in Web MVC because you can only put one @RequestMapping on a controller method.In the WebFlux.fn framework however, you can refer to one HandlerFunction in as many RouterFunction matchers as you want.Improvements in the RouterFunction spring framework 5.1:  A router DSL with less static imports:     //5.0 version route(GET(\"/people\"), personHandler::getPeople) //5.1 version route()   .GET(\"/people\"), personHandler::getPeople)        And a new pattern matcher to resolve which HandlerFunction to call, which is a lot faster than the previous one.RequestPredicatesIs a function that maps a ServerRequest to a boolean.This is used to match your HandlerFunction to a HTTP request.Spring provides a lot of default predicates for paths, accept headers, etc.But you can also create your own very easily, with lambdas, methods, or classes.// lambdaroute().GET(\"/people\", serverRequest -&gt; serverRequest.path().endsWith(\".json\"), personHandler::getPeople)// methodroute().GET(\"/people\", this::pathEndsWithJson, personHandler::getPeople)private boolean pathEndsWithJson(ServerRequest request) {    return request.path().endsWith(\".json\");}// classroute().GET(\"/people\", new PathEndsWithJsonPredicate(), personHandler::getPeople)public class PathEndsWithJsonPredicate implements RequestPredicate {    @Override    public boolean test(final ServerRequest request) {        return request.path().endsWith(\".json\");    }}nested RouterFunctionSimilar to the class level @RequestMapping, but a lot more powerful.@Beanpublic RouterFunction&lt;ServerResponse&gt; petsRouter(PetJsonHandler petJsonHandler, PetHtmlHandler petHtmlHandler) {    RouterFunction&lt;ServerResponse&gt; html = route()            .nest(accept(TEXT_HTML), builder -&gt; { builder                .GET(\"/{id}\", petHtmlHandler::renderPet)                .GET(\"\", petHtmlHandler::renderPets);            }).build();    RouterFunction&lt;ServerResponse&gt; json = route()            .nest(accept(APPLICATION_JSON), builder -&gt; { builder                .GET(\"/{id}\", accept(APPLICATION_JSON), petJsonHandler::showPet)                .GET(\"\", accept(APPLICATION_JSON), petJsonHandler::showPets);            }).build();    return route()            .path(\"/pets\", () -&gt; html.and(json))            .build();}You can choose on what you nest, depending on the needs of your software.In WebFlux.fn you can couple HTTP requests for the same path, but different accept headers to different classes, as in the example above.Here you only define your path once which means no duplication, and it’s easier to change the path to for example /animals, because there is only one place where you have to change it.In Web MVC it would look like this.@RestController@RequestMapping(value = \"/pets\", produces = MediaType.APPLICATION_JSON_VALUE)public class PetJsonController {...}@RestController@RequestMapping(value = \"/pets\", produces = MediaType.TEXT_HTML_VALUE)public class PetHtmlController {...}The HandlerFilterFunctionIs a function that takes a ServerRequest and a HandlerFunction and returns a ServerResponse.@BeanRouterFunction&lt;ServerResponse&gt; mainRouter(PetHandler petHandler, OwnerHandler ownerHandler) {    RouterFunction&lt;ServerResponse&gt; petsRouter = petsRouter(petHandler);    RouterFunction&lt;ServerResponse&gt; ownerRouter = ownerRouter(ownerHandler);    return petsRouter.and(ownerRouter)            .filter(this::performanceLogging);}public Mono&lt;ServerResponse&gt; performanceLogging(ServerRequest request, HandlerFunction&lt;ServerResponse&gt; next) {    Instant start = Instant.now();    Mono&lt;ServerResponse&gt; response = next.handle(request);    Duration duration = Duration.between(start, Instant.now());    LOGGER.info(\"Processing request {} took {} ms \", request, duration.toMillis());    return response;}The HandlerFilterFunction is more flexible than Servlet filters because you can put a HandlerFilterFunction on a RouterFunction.This means that you can apply this filter to a subset of your routes instead of on all routes.It can be used for example for security, logging, timing, etc.Future evolutionsCurrently the functional web framework does not work with Servlets but only with Spring’s self made ServerRequest and ServerResponse.They are however looking at creating a functional web framework that works with Servlets and without Reactor.ConclusionThe functional web framework is a lot better  in what properties of the HTTP request you can match on to choose a controller function.  in reducing duplication of your matching logic  in providing a clean way to separate controller logic and routing logic  in explicitness of routing so you can easily see how your HTTP request will be bound to a controller methodIt is a very good alternative to the more common annotational style web framework Web MVC. The advantages mentioned definitely make it worth trying it out for yourself!#References  Code from the talk  Documentation"
      },
    
      "iot-2018-09-28-3d-printing-intro-html": {
        "title": "3D Printing: An introduction",
        "url": "/iot/2018/09/28/3D-Printing-Intro.html",
        "image": "/img/2018-09-28-3D-Printing/banner.jpg",
        "date": "28 Sep 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  What is 3D printing  Types of 3D printing  Getting started with 3D printing  The futureIntroduction3D printing is a term that has been hyped for a long time.It’s a technology that is in essence not that new, but now more than ever is getting better and accessible to everyone.Today we take a dive into the world of 3D printing and what it really has in store for us and the world in the coming years.What is 3D printingBasically 3D printing can be described as: Producing 3D modeled objects by printing them with a 3D printer.It offers a new way to do fast prototyping without the need to create very expensive molds or stencils.It has long been hyped to be the next big thing for:  Everyday use: recreating objects, replacing broken parts  Medical use: create patient specific casts or prostheses that fit better  Weapons: In the news, 3D print a gun which can pass through metal detector, and is not registered with the authorities.Although 3D printing has changed how prototyping works, a lot of misconceptions exist:  3D printing is slow, very slow          Thus not usable to create batches of the same object        Limited available materials (for hobby use)  3D printed parts are strong but not as strong as molded or cast parts (mainly for plastics)Types of 3D printing3D printing is not one technology, there are many different methods how a 3D print can be created.Wherein the method the print is created varies depending on the technology used.Different technologies use different methods and materials, each with their distinct advantages and disadvantages.Some of these methods include (but are not limited to):  Fused deposition modeling (FDM)  Stereolithography (SLA)  Digital Light Processing (DLP)  Masked Stereolithography (MSLA)  Direct Metal Laser Sintering (DMLS)  Selective laser melting (SLM)  Electronic Beam Melting (EBM)Below we will go into detail for some of the more used types:  FDM  SLA, DLP &amp; MSLA  SLS &amp; SLMDifferent printing methodsFDM: Fused Deposition ModelingWith Fused Deposition Modeling the printed model is created by melting a compound (this being mostly PLA/ABS/PETG) and tracing the model layer by layer, each layer thickening the model as the printer deposits more material on the model.In the image above all the main pieces for an FDM printer are visible. The nozzle is moved on the Z-axis, and the build plate is moved on the X/Y-axes by stepper motors which control the movement up to one tenth or even one twentieth of a millimeter.A spool of material is fed into the nozzle which melts it and deposits it onto the model by use of extrusion wheels/stepper motors.FDM printers are widely available both ready to use and as kits that require assembly, these are also the most affordable type of printers on the market.You can always extends your printer and even print replacements for broken components.                                SLA &amp; DLP: Stereolithography &amp; Digital Light ProcessingThese types of printers work by using either a laser or a projected image to cure a UV-reactive resin.With Stereolithography a powerful UV laser traces the object layer by layer, like the FDM printer does, however it does not deposit material itself, rather it cures the resin in the tank at the point where the laser is.Because SLA uses a laser that is moved by mirrors, it can have a very high resolution, the disadvantage is that it is as slow as an FDM printer since each layer needs to be traced.With Digital Light Processing an UV projector is used, it projects an entire layer at once, this results in a lower printable resolution for the object, but yields a significant speed increase since an entire layer is printed at once, there is no need to trace the entire layer.Masked SLA is a cheaper version of the DLP method where a LCD based photomask is used in front of an UV-LED array instead of an UV based projector.The images above show the distinct difference between these two (three) methods.The SLA method has much better quality compared to the DLP or MSLA methods.                                DMLS &amp; SLM: Direct Metal Laser Sintering &amp; Selective Laser MeltingDirect Metal Laser Sintering and Selective Laser Melting are two of the more industrial methods of 3D printing. They are used, as the name suggests, to 3D print object in metal.The main procedure for both methods is similar.A moving arm pushes metal particles across the printing area, after which a powerful laser will trace the current layer of the object being printed.This will fuse the particles together and is also where the biggest difference between the two methods exists.Once the current layer is finished, the arm moves back and forth again, bringing in a new layer of metal particles.With DMLS the particles are sintered together but the metal itself has not melted completely, is hasn’t flowed.When using SLM instead, the metal particles are actually fully molten and they fuse together much more firmly creating extremely though objects!Objects created with these methods are free of internal stresses and defects that are common and hard to prevent with other production methods.The parts can also be printed as a whole rather than needing assembly of different parts, which further reduces the possibility for failures or errors during assembly.This however comes at great cost, literally, since these methods are very expensive and require state of the art equipment!Because of this high cost they are used in industries where the fault tolerance is very low, such as aerospace.Different printing materialsAs you might have noticed in the previous section different printing methods require different printing materials.In the section below we’ll go more into detail about these materials.Polymers/plasticsFDM and DLP printers require polymer based materials.These are plastics or plastic like materials that are easy to print.PLA (PolyLactic Acid):  Biologically degradable  Melting (printing) temperature: 170 ⇔ 230 °C  Can be used to print medical supplies  Many colors available + glow in the dark  Not UV stable  Cannot withstand high temperatures, do not leave it in the sun inside a car!ABS (Acrylonitrile Butadiene Styrene):  Not biologically degradable  Melting (printing) temperature: 220 ⇔ 260 °C  Many colors available + translucent  UV semi stable  Can withstand higher temperatures than PLA!  Harder to print than PLA and requires venting since the fumes are toxic!PETG (PolyEthylene Terephthalate Glycol):  Not biologically degradable  Combines best of PLA and ABS  Melting (printing) temperature: 220 ⇔ 250 °C  Many colors available + translucent  UV semi stable  Can withstand higher temperatures than PLA!  No toxic fumes, odorlessThese three materials are commonly used in FDM printers. The most and safest to use is PLA, which is fully bio degradable (over a long long time), it also is the easiest material to print with.ABS is stronger but not biodegradable and produces toxic fumes when printing, it also requires very precise cooling during printing or the print will warp and the layer will detach, ruining the print.PETG is the best of both worlds, it is stronger than PLA and does not produce toxic fumes.There are a lot more materials available, oftentimes with very specific properties to make it act more like rubber, be more flexible, glow in the dark,…All these different materials require different print settings and some can be quite hard to get right, experimentation is key!Some materials also exist that can be used in a medical context, these are however not printed with the average FDM printer since they need to match very high standards for medical use for both internal and external use.DLP materials:  Liquid polymer that undergoes photopolymerization, meaning it is cured/hardened by exposure to UV light.  Many different types of resin are available          Biodegradable ⇔ Non biodegradable      Flexible ⇔ Sturdy      Polymer ⇔ Ceramic      Low temp ⇔ High temp        There are also resins available that cure with regular “daylight” and do not require special UV lights to cure the printed objectMetalFor the moment metal printing is not for the mere mortal, however larger companies/industries are already using 3D printers capable of printing metal.Not all methods can print all metal materials, some like SLM are limited in the metals it can ‘print’.Most printed metals:  Titanium  Aluminium  Cobalt chrome  Gold  Silver  Copper  Bras (alloy)Aerospace industries nowadays print both external and internal parts of rockets and their engines, these objects are stronger and have less internal defects and stresses.The medical industry can also print patient specific prostheses in titanium allowing for better patient recovery after injuries.Concrete/constructionIn more recent years concrete printing has caught some attention and people are experimenting with it.It has some nice advantages:  Allow cheaper construction of small buildings  Faster construction  Small area mini homes for the developing world -or- after natural disastersThe video below shows a working concept of a small 3D printed ‘home’, it is constructed in place and can be finished in very little time.    FOODFood printing has been a hype for a long time.Culinary experts have been experimenting with the concept, but also companies like NASA for astronauts.Simple items like chocolate and dough can already be 3D printed, based on the FDM method, the edible object is printed layer by layer.There are ‘robots’ that can print more complex items, but these are not actual 3D printers and more automated assembly machines, the boundary between 3D printing and assembly can become a bit blurry.At this point in time it is not yet possible to print entire meals from raw base ingredients.This will no doubt be developed further as time progresses.Getting started with 3D printingGetting started with 3D printing is not easy, it can be very daunting.There are a lot of things you need to take into accountSince it is still relatively expensive, you want to ask yourself these questions:  What type of 3D printing do I want?  What is the best (price/quality) printer available?  Will I use it enough to justify the purchase?  You could also just order a printed model online!Getting things right is hard:  Printer calibration  Model slicing &amp; print settings⇒ 3D printing remains an intensive trial and error process!Only begin if you are willing to invest the necessary time into it.If at first you don’t succeed, try again and again and again and again!Thankfully the online hobby community is very large and generally very helpful.You will need to model your object or download it from a ‘makers’ website.You will need to slice your model with settings for your specific printer, and adjust these settings based on the quality and speed you want.Online resources:  Youtube channels:          Maker’s Muse      3D Printing Nerd      Make Anything        Forums:          3D Hubs      Reddit 3D printing        These YouTube channels and forums are an invaluable resource when getting into 3D printing, they contain loads of information, do’s and don’ts.By watching videos and reading articles you can prevent mistakes gaining insights faster in how 3D printing works and what is possible.  Maker websites:  Thingiverse  Tinkercad  MyMiniFactoryThese websites provide vast libraries of models and some even offer an online modeling tool.Before diving deep into your modeling software check if someone else had the same idea and created the object you want.⇒ good artists copy, great artists steal - PicassoModeling software:  Modeling software:          3DS Max      Maya      Blender        CAD software:          Fusion 360      Autocad      Inventor      These software suites let you create and export models.All of these programs are easy to pickup (except for the CAD software) but are extremely hard to master!There are also many alternatives available, but these are the most common ones.Slicing software:  Ultimaker Cura  Slic3r  Simplify 3DThese software suites let you convert your 3D models into GCODE.GCODE is the ‘language’ that 3D printers use to print an object.It contains a set of instructions for stepper motors, extruders, fans and other peripherals that make up a 3D printer.This code is generated from the 3D model and is layer based hence the name ‘slicer’, since it slices the model into layers and translates this to the GCODE required to print.The slicer software also takes into account overhangs and infill.Overhangs are parts of the object that have no support beneath them, they float, but since material cannot just float in midair supports are generated that are removed (by the user) when the print is finished.Infill is a way to speed up printing, the inside of and object would normally be 100% filled with material, this is slow and uses a lot of print material.By setting the infill percentage a structure is generated inside sealed of parts of the model that contains a lot less material while maintaining most of its strength.By playing with this setting a lot of time can be saved!The futureThe future is now!3D printers are advancing at an extremely rapid pace, new printer models arrive on the market almost weekly.Chinese firms are producing very high quality printers and production quality is going up.The future will bring a lot more:  Better printers  Finer resolution  Lower cost  Lower maintenance  More and more versatile materials  A 3D printer in every home?  Repair stuff yourself ⇔ Throw away stuff that is broken, built to break…ResourcesGetting into 3D printing requires a lot of research, A LOT!Since the technology is advancing so fast I have not listed any specific printers, they are reviewed on youtube and forums where experienced members of the community test them and give their verdict on them.Printing methods &amp; materials:  SLA vs DLP vs MSLA  DMLS vs SLM  Types of FDM polymers/plastics  Types of DLP resinsOnline resources:  Maker’s Muse  3D Printing Nerd  Make Anything  3D Hubs  Reddit 3D printingMaker hubs:  Thingiverse  Tinkercad  MyMiniFactoryModeling software:  3DS Max  Maya  Blender  Fusion 360  Autocad  InventorSlicers:  Ultimaker Cura  Slic3r  Simplify 3D"
      },
    
      "spring-2018-09-27-springone-platform-html": {
        "title": "SpringOne Platform - A birds-eye view",
        "url": "/spring/2018/09/27/SpringOne-Platform.html",
        "image": "/img/2018-09-27-SpringOne-Platform/post-image.jpg",
        "date": "27 Sep 2018",
        "category": "post, blog post, blog",
        "content": "This year we went again to SpringOne Platform to check out the latest changes and what’s to come.In this post I’ll try to give an overview of everything that’s just been released or coming in the future.Because there were A LOT of sessions, I wasn’t able to gather or note down everything.All sessions except for the workshops were recorded and I’ll update this post to get you directed straight to the videos on YouTube when they come online!AnnouncementsI listed some of the major announcements that were made during SpringOne.You can find a link to the corresponding video at the end of each topic.AWS Service broker for PCF, open betaAmazon will bring the AWS Service Broker to PCF, making it easier for you to connect your applications to its service catalog of over 18 services.Watch the talk here.Pivotal Function ServiceNext to PAS (Pivotal Application Service) and PKS (Pivotal Container Service), Pivotal will expand PCF with PFS (Pivotal Function Service) which will be build on top of Knative.Pivotal Greenplum, which is its “Postgres for Petabytes”, will also be coming to PCF in the near future so that the same easy-to-use experience that developers nowadays already have with the platform will be extended to the data scientists and data engineers.Watch the talk here.First-timers-only issuesBecause it can be daunting for people to enter a community they are creating “first-timers-only” issues on the Spring Boot project, making it easier for newcomers to get connected.While working on these issues developers of the Spring Boot project will come alongside you to teach you to contribute back, help you and coach you.This way you will get to become part of the Spring community.Watch the talk here.Pivotal Tracker: Maestro  What happens when your organization scales, and you end up with lots of fast-moving developers?It’s really, really easy to get misaligned.And when you get misaligned, it’s really easy to ship the wrong thing.That’s why Pivotal has built a tool to help teams articulate and align around business outcomes to give organizations the high-level view so they can assure that everyone is aligned and delivering value.Watch the talk here.Pivotal Act  “Pivotal Act is a program that partners with humanitarian organizations and charities to identify, design, and develop practical solutions to pressing challenges around the world.”Together with developers, designers, and engineers from Pivotal Labs, Pivotal is going to apply the same methodology they use in other engagements with clients but tailored to the needs of the humanitarian and social impact sectors instead of just donating technology or funds.This way charitable and nonprofit organisations can use the present-day technologies and put them directly to use through the partnership with Pivotal, while building up the organisation’s technology capabilities so they can continue their work after the engagement ends.If you want to find out more, just visit https://pivotal.io/act.You can watch the announcement here.R2DBCWhen using the reactive model, people were limited to NoSQL databases because those were the only ones that had a driver which supports things like streams and backpressure.During the keynote Oliver Gierke announced the R2DBC project.This project currently consists out of a client, an SPI and a PostgreSQL implementation to bring the reactive capabilities to SQL databases.It also has an adapter to support the OpenJDK incubator project “ADBA”, a non-blocking database access API that Oracle is proposing as a Java standard.RSocketOn stage Stéphane Maldini announced the RSocket project.RSocket, built by Facebook, Netifi and Pivotal, is a binary application protocol that provides reactive streams semantics.This protocol is payload, transport AND language agnostic making it easy to send eg. JSON or protobuf payloads over TCP, UDP, WebSockets or HTTP/2 using Java, Kotlin, C++, JavaScript,…Using this protocol we can eg. easily resume our stream of events where we left off in case the connection got interrupted.To get more in-depth knowledge of the reasoning behind the protocol visit https://rsocket.io/ and look up the document that explains the motivation in detail.Spring Cloud AzureTo quickly integrate with Azure services, Microsoft has already created a couple of starters to easily connect to eg. Azure Active Directory or Azure Key Vault.Not all starters can be found on start.spring.io, to get a complete overview of all their starters and modules visit their GitHub repository.Watch the talk here.Cloud Native BuildpacksCloud Native Buildpacks is a new effort initiated by Pivotal and Heroku which aims to unify the buildpack ecosystems with a platform-to-buildpack contract.They embrace modern container standards, such as the OCI image format and take advantage of the latest capabilities of these standards.This way buildpacks can be used cross-platform.You can watch the announcement here.ReleasesSpring Framework 5.1The highlights of this release are the support for JDK 11, initial refinements for GraalVM compatibility and Reactor &amp; Hibernate got an upgrade to respectively Californium and 5.3.Improved startup times and less heap memory consumption are also some benefits you get when upgrading to this release.Check out this post by Juergen Hoeller to find out more!Spring Boot 2.1.0.M4The fourth milestone of Spring Boot 2.1 got released to incorporate the 5.1 release of the Spring framework and closes over 40 issues and pull requests.Madhura Bhave wrote this post announcing the milestone release and added some useful links to the release notes and updated reference documentation.Make sure to check them out!Spring Batch 4.1.0.RC1Mahmoud Ben Hassine announced yesterday the first release candidate of 4.1.0.This release was mainly focused on running Spring Batch correctly on Java 8, 9, 10 and 11.Their plan is to release this version by the end of October so it can be shipped with Spring Boot 2.1.Spring Data LovelaceWith the release of Spring Framework 5.1 comes a new release of Spring Data.This release notable topics are:  Support for immutable objects  Deferred JPA repository initialization  Support for MongoDB 4.0 Client Sessions and Transactions  New Spring Data JDBC module  Apache Cassandra mapping improvements for Map and tuple types, Lifecycle Callbacks, and Kotlin Extensions  Replica Reads with Spring Data RedisTo see what else has changed, check out this post by Mark Paluch.Spring Security 5.1In this new release a lot has been added regarding WebFlux support: OAuth 2, CORS and HTTPS redirection  are just a few to sum up.More than 50 issues have been resolved too!To see what else has been added, check out what’s new in the reference documentation.Spring Fu 0.0.1Spring Fu is an experimental micro-framework that makes it easy to create lightweight Spring-powered applications with functional APIs instead of annotations.It introduces Kofu (Kotlin and functional) and Jafu (Java and functional, still a proof of concept) configuration for configuring Spring Boot in a functional way and makes use of the functional bean registration.It also ships with coroutines support, GraalVM native images support and various other features.To learn more about this interesting project, visit this link!Spring Tools 4Spring Tools 4 is completely re-built from scatch after a decade of updates and improvements of Spring Tool Suite (STS).It’s a new set of IDE agnostic tools that can be installed in your favorite IDEs and editors.Currently they support Eclipse, Visual Studio Code and Atom IDE.With this release also comes the end of Spring Tools 3, but not until mid 2019!STS 3.9.x will still receive updates and will be shipped as a full distribution, and the distribution will be updated to the upcoming Eclipse releases (2018-09, 2018-12, and beyond).After mid 2019, it will no longer receive any maintenance updates."
      },
    
      "docker-2018-09-24-docker-networking-with-weave-html": {
        "title": "Docker multihost networking with weave",
        "url": "/docker/2018/09/24/docker-networking-with-weave.html",
        "image": "/img/docker-basic-networking/docker-basic-networking.png",
        "date": "24 Sep 2018",
        "category": "post, blog post, blog",
        "content": "  In my last blogpost we talked about setting up a Docker network on a single host. We talked about a very basic 3 tier application which was packaged into 3 containers.An Angular frontend, a spring boot backend and a mysql database container.  No person in their right mind would ever run an application on a single host in production. ( If you are doing this please give us a call and we will gladly come help you out ;-) )So today I will talk you through the process of setting up multi host networks with the help of Weave.Table of contents  Why multihost networking?  Weave  Basic example  Application example  Onprem cloud application example  ConclusionWhy multihost networking?In my last blogpost we ran our three containers on one single host network. This has a few obvious disadvantages which I will address first:      If you would run all your applications as Docker containers on one machine, your environment would still be very prone to outages.If something happens to the physical machine that you use, then you are pretty much done for and you have an outage.Depending on how critical your applications are, this could have disastrous results.That’s why in practice you want to spread your applications over several hosts and connect them to each other over a network.        Not every application that you package in a Docker container is the same.In our example we have a web frontend, a Spring Boot backend and a MySQL database server.A database server has fundamental different needs than a webserver.For example, you want to make sure your database container is running on a system with large hard disks with much faster access times.On the other side, a webserver has little use for large hard drives and has much more benefit from increased memory to allow for many concurrent connections.        If you use a microservice architecture and you want to scale up a certain microservice, it is recommended to run the new instance of that microservice on a different machine than the first instance.This is done for resilience of the application but also to spread the application workload over several different physical machines.This improves the scalability of your system.  So in short: multihost networking is pretty important to build resilient, robust applications that scale well.It also allows you to deploy your containers on hardware that makes most sense for each container.Now that you are convinced that we need multi host networks, at least for our production environments, let’s talk about what we can do to set this up with Docker and Weave.WeaveThere are several options to do multihost networking with Docker.Nowadays Docker itself even supports a basic form of this but we want to use all the good stuff like DNS lookups and service discovery.That’s why we are going to use Weave.NET.Now, what is Weave.NET?Weave.NET is software that is build by the company Weaveworks.It allows you to create a virtual network across multiple hosts and enables automatic discovery of hosts and containers within the network.The following features are the most important and useful to me:      It’s easy to setup. As I will show in the hands-on part of this post, the setup of Weave is done within minutes and is pretty straightforward.        It provides a virtual network on top of your existing network.In big organisations, the network setup can be quite complex with multiple VLANs and firewalls.Using Weave, your network people only need to open one port.Everything that stays within the Weave virtual network can communicate over that same port.        The virtual network is very flexible.You can use Weave to build one virtual network between your on-prem and cloud environment.Within this network, all your containers can communicate with each other as if they were living on the same machine.        An added benefit of the virtual network is that it’s pretty easy to secure.You can encrypt all the traffic on the Weave virtual network which adds a layer of security ontop of the existing security you have in place.This is especially useful if you wish to build a network which spans your on-prem and cloud environment.Since all traffic is encrypted you are not at risk when containers in the cloud communicate with containers in your on-prem environment.        The Weave network comes with its own DNS server.This allows you to do service discovery within the Weave network.This has huge benefits over addressing applications with their IP addresses.Service discovery allows you to do easy loadbalancing and provides your applications with high availablity.        Last but not least I feel obligated to point out that Weave works very well with Kubernetes.Kubernetes is basically the orchestration tool you want to be using to run your containers.A deeper tour of Kubernetes is for another time, but for now I’d like to point out that the default network provider within Kubernetes is flanel but you can replace that with Weave if you want.You then get all the benefits of Weave plus the added value that you can make a virtual network that spans several Kubernetes clusters and/or your regular Docker applications.  The main point I’m trying to make here, is that Weave.NET takes care of a lot of low level networking stuff for you.This allows you to build a more robust and scalable environment to run your applications on without having to worry much about the lower levels in the networking stack.Basic exampleIn this example, I will be using Ubuntu machines on which I have already installed Docker.Following picture shows the setup we will be build in this example:    Installing WeaveTo install Weave on an Ubuntu system you can run the following commands:sudo curl -L git.io/weave -o /usr/local/bin/weavesudo chmod a+x /usr/local/bin/weaveIf you want to install Weave on another system take a look on the weave install page.On systems which don’t support native Docker, you will have to set up something like Docker-machine to get everything to work.We will need to run the Weave install on both of the machines that we are going to use in this example.Launching WeaveNow that we have installed Weave on both machines, let’s start it up.Setting up host 1On the first host, type the following commands:weave launcheval $(weave env)The first command launches Weave. Weave runs as a set of Docker containers on your system, you can see this when you run the launch command:    If you run the docker images command you can take a look at all the images that Weave downloaded:    The second command configures the Weave environment so that containers that get launched will automatically attach to the Weave network.This means that you have to provide no additional arguments to your Docker run commands to use the Weave network.Behind the scenes Weave has setup a Docker network for you on your machine, you can take a look at this with the command:docker network ls     You can recognise the Weave network by the name Weave and as you can see it uses the weavemesh driver instead of one of the standard Docker network options.When our network setup is done we will test it out with a test container.So for now let’s start this container on the first host:docker run -d --name app_on_host1 weaveworks/ubuntuThis is just a stripped down Ubuntu Docker image.We will use it later to test our network setup.If you look at your machine’s network stack you will see that Weave has setup several different networks on your host machine in order to make it’s magic work.When I run the:ifconfigcommand on my machine, I can see that Weave added following network stacks to my machine:    What all these networks do is out of scope for this post as that operates on a pretty low level in the TCP/IP stack and we are not really interested in it in our usecase.We just want it to work, right?Setting up host 2In the last screenshot of the previous section we looked at the network stack of host 1.This is important because we need the physical IP address from that host to tell our second host to connect to host 1.Host 1 has IP address 192.168.1.18. This gives us following command to run on host 2:weave launch 192.168.1.18eval $(weave env)The syntax of this command is pretty straightforward.You tell Weave to launch and to connect to every IP you supply as an argument to the launch command.We will also setup a test container on this host:docker run -d --name app_on_host2 weaveworks/ubuntuThese are all the setup steps you need to do in order to setup a simple Weave network.Now let’s see how we can verify that everything is working.Verifying our setupThere are a few things we can do to verify that our setup is working.To start off let’s look at the status off our Weave service:weave statusThis produces following result:    You can run this command on either host, it will provide you with basically the same results.All sorts of useful stuff is available in the status overview including the range of your Weave subnet, the connected peers, the connections.As a first step to verify that our network is working, let us examine the connections and peers in more detail with following commands:weave status peersweave status connections    As you can see from the output of the commands there are now two hosts in our network and there is one connection between them.So far everything is looking good!Now let’s do the real test and see if the containers can find each other by the name we gave each container.SSH into the container on host 1 and ping the application on the other host:docker exec -ti app_on_host1 bashping app_on_host2    As we can see from the results, our container was able to resolve the other container over the Weave network by making use of the Weave DNS service and the Weavemesh network.If we reverse our test and run it on host 2 we get this result:docker exec -ti app_on_host2 bashping app_on_host1As you can see from the example outputs, both hosts are now connected, and thanks to the Weave network and DNS service, they are able to resolve each other by hostname.Another noteworthy thing is that Weave has setup a class A network (10.xxx.xxx.xxx) range for us.These are all IP addresses within the Weave network so you don’t have to worry about any IP conflicts with your existing network.If you would have the need for a specific subnet you can force Weave to use whatever subnet you like (192,168.1.XXX for example).Application exampleFor this example we will use the Weave setup we created in the previous example.I will be running the application from the basic Docker networking blogpost on our two hosts in the Weave network.You can checkout the code for this example at my github account.For this example I will be using the weave-basic-example branch.Setup databaseIn the first step, we setup our database container on one of the Weave hosts.I will be using my desktop to run the database container.First we will run the run_db.sh script to start up a new MySQL container and assign a root password to it.When that container is up and running we can initialise it with a database and some data.To initialise the database run the init-db.sh script.We can run following commands to verify that our database is up and running in the Weave network:docker psweave psThis should give you a similar output to the screenshot below:    As you can see the mydb container was automatically added to the Weave network because of the eval $(weave env) command we ran earlier in this demo.Note that the Weave ps command shows you the container id and the IP address it has allocated to that container.Now that our database is up and running, let’s switch over to our other machine and start the other services over there.Setup backend applicationLet’s install our backend service on our second host.In the backend folder we will run the following commands to compile our application and start up our Docker container.Let’s start our backend container with the test_run_backend.sh script.This script runs the container and exposes its webservice to the outside world through port 8080.We can then use our browser on our host to verify that the service is running correctly:mvn clean installdocker build -t rest-backend ../test_run_backend.shTo verify that out application has started successfully we can run the following commands:docker psweave psThis shows us that the container wass started correctly and it got an IP addresss within the Weave network.    Now that we know our backend and our database are running within the Weave network, we can query our REST service and see if everything is working as expected:    When I do an HTTP GET to my REST backend, it comes back with a greeting it got from the database container on the other host.Now that we have verified that our backend runs fine, we will stop it and start it up again without exposing a port to the outside world:docker stop rest-backenddocker rm rest-backend./run_backend.shThe Docker ps command on the backend now runs without exposing a port.Since our frontend application is calling our REST backend through the Weave network, we don’t have to expose a port to the outside world:    Setup frontend applicationOur frontend application is a bit different from the one I used in my previous demo.In the previous demo we used an Angular frontend, but I replaced it with a simple webserver I wrote in Golang.I want to demonstrate that applications can access each other through the internal Weave network by their container name.I could not demonstrate this last time with Angular code since that’s rendered on the browser at client side.Our client’s browser is not inside the Weave network so it has no way to resolve the webservice call from our frontend application to our REST service.In this example, our Go frontend app renders the HTML response on the server side.This means the code that calls the REST service is running inside the Docker container and is thus a part of the Weave network.We can also access our REST backend via following url:    The source code of the Go application is on Github in the frontend folder.Please note: I also included a compiled binary so that you can use the program even if you don’t have a working Golang installation.To start our frontend container, run following commands inside the frontend folder.Only execute the first command if you wish to compile the Go program yourself:go builddocker build -t frontend ../run_frontend.shOur Go frontend application should be up and running now so let’s check if everything is working as expected:    We can access our frontend application through port 8080 (which we exposed to the outside world with the Docker run command).In this example I gave the name “john” which has no entry in our database so our REST backend returned the standard hardcoded message “hello friend”.    In this example I gave “bas” as a parameter and since we have a message in our database for this name, we got the message “hello master” as a result.Since no Docker ports are exposed to the outside world (except for our frontend application to make it accessible from our host), this is definitive proof that our Go application can access our REST backend application through the Weave network.Likewise, our REST backend application accesses our mydb container through the Weave network as well.Oh, and by the way: You can run your containers on any host which is connected to our Weave network. Cool, huh?Now, let’s take this one step further. In this next example we will extend our Weave network from our home network into the cloud.Onprem cloud application exampleIn the final example of this blogpost I will show a setup where we have 1 Weave network that spans our onprem environment and connects to a cloud environment (AWS).In this example we will run each application container on a different host. It looks like this.    In the end result I will be running the mydb container on my desktop (database server). The rest-backend will run on my laptop (backend server) and our frontend container will run on an EC2 instance on AWS.Everything will be connected through the Weave network that we will reconfigure to encrypt our data between the cloud and onprem environment.Let’s get started.Setup EC2 serverThe first thing we have to do is create a new EC2 server on AWS that we can use for this demonstration.I’m not going to go into full detail here about how you can do that but it’s quite easy and there are a lot of tutorials out there.After our EC2 server is up and running we can login to it via ssh and install docker and weave.We will start up Weave on our EC2 server with encryption enabled.weave launch --password weavetesteval $(weave env)The password weavetest part enables the encryption for this peer. If another peer wants to connect to this peer it has to provide this password before a connection can be established.After the connection is established all traffic over the connection will be encrypted.Another thing we have to take care of is make sure that Weave on the EC2 server can connect to it’s peers.Remember, an EC2 is running in a VPC behind a firewall so we have to open certain ports for this to work.In the case of Weave we have to open the port 6783 – which is the control port.And we also have to open the port 6784 – which is the data port.You can do this by editing the launch group of your EC2 and add following inbound rules to it.    As you can see I added rules to allow TCP and UDP traffic on port 6783 from all sources. This enables the Weave control process to connect to its peers.Afterwards I added a rule to allow UDP traffic on port 6784 from all sources. This port is used by Weave to send it’s data.Note, I also opened port 8080 because our frontend uses this port to listen for web requests.    We will use the public ip4 address and DNS name later in this demo.Reconfigure the existing onprem hostsBefore you can connect Weave from your homenetwork to the cloud you have to make sure that your router ports 6783(TCP/UDP) and 6784(UDP) are opened.First lets reconfigure our database host.weave resetweave launch --password weavetesteval $(weave env)The weave status command on this host now shows that encryption is enabled.    As you can see Weave is now reporting that encryption is turned on but this Weave peer is not yet connected to any others.Let’s take care off that now. Switch over to the backend host.Now let’s run the magic Weave command that will make our setup run.weave launch 34.247.178.145 192.168.1.99 --password weavetestThis command launches Weave on our backend host and tells it to connect to our EC2 instance in the cloud and to our database server onprem.    As you can see our backend host is now connected to both our EC2 server and our database server. Let’s check our EC2 server as well.    As you can see our EC2 server now also has encrypted connections to both the backend server and the database server.Weave will automatically connect hosts on your network as long as they can be connected though 1 common peer.In this case our backend server has a connection to our EC2 server and a connection to our database server.So Weave will automatically create a connection between those 2 as well.    As you can see in the screenshot I browsed to the frontend application on the AWS server and it generated a correct response which means our setup works!ConclusionIn this blogpost we started off with a simple onprem Weave network.After testing that the network works we deployed a distributed application on that network.After this we extended our private Weave network with a peer on AWS in the cloud.The end result we achieved was that our distributed application was running spread over a cloud and onprem environment.In our final example we also enabled encryption on the Weave network so all traffic between your onprem environment and the cloud is encrypted.This can be a huge benefit for enterprises who want to move their applications into the cloud. With Weave you can setup your network in a hybrid cloud/onprem model and be sure that all communication is safely encrypted.This allows the enterprise to do a gradual move to the cloud instead of having to do a big bang approach."
      },
    
      "testing-2018-08-15-node-red-dev-ci-html": {
        "title": "Node-RED: Development and CI",
        "url": "/testing/2018/08/15/node-red-dev-ci.html",
        "image": "/img/node-red-dev-ci/node-red-dev-ci-white.png",
        "date": "15 Aug 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  What is Node-RED  Why we are using Node-RED (or an alternative)  Node-RED to the rescue  Configuration components  Running an instance  Creating your first flow  Spicing things up  JSON  Node-RED persistent config  Node-RED and Docker  Node-RED and CI  ConclusionWhat is Node-RED  Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways.” – from docs https://nodered.org/And yes, that’s all true.But we’re not using Node-RED for those things.There are two use cases for which we use Node-RED, but before we go into those, we’ll take a quick look at some other Node-RED features.Some great pros:  Node-RED is built on Node.js, taking full advantage of its event-driven, non-blocking model.This makes it ideal to run at the edge of the network on low-cost hardware such as the Raspberry Pi as well as in the cloud. (from docs)  Node-RED comes with a web based graphical user interface, where you can manage your API flows with drag and drop functionality.  Because Node-RED is built on Node.js you can just run it locally on a lot of systems (Windows, Mac OSX, Linux,…), on a lot of devices and in the cloud.The graphical user interface lets you create your endpoints and flows in an easy way.Just drag and drop your components in your flowchart and connect them by drawing a line between them.Double clicking each component will open the detail screen where you can set some variables for that component.When your flows are ready, there is an import and an export function available for storing and sharing your configs.Why we are using Node-RED (or an alternative)Node-RED and DevelopmentComing from a frontend dev background, I can confirm that nothing is more frustrating than running backends, or at least part of the backend, locally, before you can start coding and testing.Yes, Docker and Docker Compose are helping a lot but still…In some cases, backend development, for a specific feature, is planned for the same sprint as the frontend development.So even running a backend locally, will not help you.Mocking can be an option.You can start mocking the data inside the communication layer or mocking an external backend with test data.In an ideal world, teams should do some analysis of the specifications coming from business and their analysts.Based on those specification, developers can start estimating tasks (or stories and epics), and think about the architecture and design.At this point, technical specifications could already get written down, such as model designs and API contracts.It’s almost impossible to do a good job in the frontend if those things are not available, or at least specified.Why do you want good test data?If you know the characteristics, it’s easier to do some layout stuff.You would think that mockups and designs would be accurate, but most of the time they aren’t or they are not inline with the data.So what about mocking?Yes, you can mock stuff yourself and maybe skip the communication layer.Or you can mock the communication part in the communication layer by means of interceptors for example.But at some point in time, you’ll need an API and test data that is close to production data when it comes to live data specs.When mocking inside your components, would you include everything in your code base?Or would you exclude it, but still make it more accessible for your colleagues?Some frameworks include this kind of functionality such as MockBackend from Angular (More info here).That said, I really like and prefer external tools for mocking a backend.That way it’s separated from your app and code base, so it can easily get replaced or modified without triggering stuff in your frontend pipelines.You can even choose to run a tool on your local machine.Or on-premise and make it accessible for all your (frontend) developers.Node-RED and Testing (CI)Even if you have implemented your mocks inside your communication layer and it’s doing the job you expect it to do, how are you going to test your communication itself?You can use those mocks to run unit tests, but it’s harder to run integration tests that cover your communication layer as well.Using an external tool, gives you the possibility to reuse this part in your test setup.You can integrate this external mock backend in your setup and redirect your frontend calls to the mock instead of a real backend.Why you should do this, is explained in an earlier post on our tech blog.If you’re interested, you can read all about it here.Node-RED to the rescueIn both cases, Node-RED can help us providing a real API that responds with test data.For the test data, I prefer test data getting delivered by the business or the client, but if that is not an option, you can create your json data based on the API contracts and model designs.Again, I’m mentioning the API contracts and model design, because often, organisations fail at this.Let’s setup a real easy API.PrerequisitesBefore starting this tutorial, you should download or install some things:  Make sure you have a Node.js (incl npm) environment  sudo npm install -g --unsafe-perm node-red  A running Docker daemon  docker pull nodered/node-red-dockerNOTE: To be honest, you only need one of those 2 environments.For the other part, you can just read through it and then try it in the other setup.Configuration componentsAlthough we are only using Node-RED for HTTP(S) (REST) and MQTT backend mocking, Node-RED provides more options.Node-RED can even be used as a real backend, using external databases for example.So before we start our setup, let’s take a quick look at some basic features.InputsHTTP, TCP, UDP are probably the most straightforward and known by all of you.I guess no explanation is needed.WebSockets are also used a lot nowadays.WebSockets are an advanced technology that makes it possible to open an interactive communication channel/session for pushing messages between a client(s) and a server(s).MQTT is one of the standards for messaging for mobile devices.It is a lightweight messaging protocol for small sensors and mobile devices, optimized for high-latency or unreliable networks.Inject, catch, status and link are internal inputs.Inject provides a way of injecting a message(s) into a flow.This can be done manually or at a regular interval.Catch can be used to handle an error from another node.The normal from of that node will get terminated, but with catch you can handle and catch the error providing an alternative route.The status node is just reporting messages from other nodes and link lets the user create virtual links between flows.OutputsSame as above for HTTPS, TCP, UDP, WebSockets, MQTT and link but now for outputs.Debug lets you show messages in the sidebar.The debug sidebar provides a structured view of the messages it has sent, making it easier to understand their structure.FunctionsNode-RED comes with a lot of predefined function blocks.The ones we are using most are the regular function-block itself and the json- or xml-function to parse json and xml.Besides those, there are function blocks that lets you run calls to external platforms, delay, reroute or redefine messages, etc.If you need more information about one of them, Node-RED show an information screen when selecting one of them.SocialThis section provides us with functionality to intercept (input/output) email- and twitters messages.I’ve personally never used one of them, but I can imagine they can come in handy.StorageFile (input/output) is self explanatory.The tail function, lets you add a watcher to a file.As the docs describe, this cannot be used on Windows systems.It’s based on the $ tail -f fileName command on Unix systems and watches for changes.AnalysisThis component lets you analyse a received message based on a sentiment score.You can read all about it here.AdvancedLike the tail function, the watch-block watches for directory or file changes.The feedparse can be used to monitor an RSS/atom feed for new entries.The last one, the exec-block lets you run a system command and returns its output.Running an instanceFirst things first, let’s fire up a Node-RED instance.(We are going to start with the npm package version)To start using or initiating an instance of Node-RED by using the npm repositories, just run:$ node-redIf installed correctly, you should see the following log:  There are four interesting lines in this log.The first three of them are the settings file, the user dir and the flows file.The last line shows us where the service is available.We’ll come back to the config later, and we are going to take a look at the User Interface based on the uri in the last line.As mentioned before, Node-RED offers a user interface, to configure the service.Open your browser and navigate to http://127.0.0.1:1880 (default setup config).You’ll see all the components mentioned above on the left side of the screen.In the middle you have a tabbed structure for your flow chart(s), and on the right you have debug, info and detail windows.On the top right of your screen, you have a settings dropdown and a menu with deploy options.Creating your first flowLet’s use the Node-RED user interface to create a simple flow.In this case a simple http-input connected to a function that returns its data through an http-output.Start by dragging the http from the input list onto the chart.Then, drag and drop a function block from the function list and the http from the output list onto the chart.Connect the http-input with the function block by dragging a line starting from the input node, to the function block.Repeat this for the function-block to the output.  Of course this isn’t ready for being deployed just yet, cause we did not add any logic yet.Let’s configure our flow!InputDouble click the http-input block.This action will open a window on the right where you can set some properties.  Method: GET  URL: /data  Name: Get all data  Confirm these settings by clicking ‘Done’.Function blockDouble click the function-block.Again, a window will open!  Name: Retrieve data  Function:msg.payload = [  {id: 1, title: \"Title 1\"},  {id: 2, title: \"Title 2\"},  {id: 3, title: \"Title 3\"},  {id: 4, title: \"Title 4\"},  {id: 5, title: \"Title 5\"}];return msg;NOTE:The information section on the right comes in handy while configuring the function-block  A JavaScript function block to run against the messages being received by the node.The messages are passed in as a JavaScript object called msg.By convention it will have a msg.payload property containing the body of the message.This explains why we are setting the msg.payload property.  Again, confirm these settings by clicking ‘done’.OutputOpen the config for the http-response-output block.  Name: return all  Status Code: 200  As you can see, the output lets you choose the return status code.This can be used later if you would like to implement failed REST-calls for mocking purposes.Confirm these settings by clicking ‘Done’.DeployOn the top right of your screen, you have the deploy-dropdown.Clicking deploy will deploy this flow onto the running server, but you can also use the arrow to open up the menu and choose a more accurate scenario.Since this is our first deploy and we only have one flow, we don’t have a choice.  Test setupTo test our deployment and of course our flow, just navigate to http://localhost:1880/data in your browser or use curl to GET the data:  $ curl -X GET http://localhost:1880/data  Spicing things upLet’s try and implement a DELETE.We would first drag and drop an http-input, a function-block and an http-response-output onto the flow chart.  The input would look like:  Notice the :id.This way we tell Node-RED that this is a path parameter.The output would look like:  But what about the function-block ? We could come up with something like:let data = [ {id: 1, title: \"Title 1\"}, {id: 2, title: \"Title 2\"}, {id: 3, title: \"Title 3\"}, {id: 4, title: \"Title 4\"}, {id: 5, title: \"Title 5\"}];if(msg.req.params &amp;&amp; msg.req.params.id) { // the id   data = data.filter((item) =&gt; {       return item.id != msg.req.params.id;   })}msg.payload = undefined;return msg;So we’ve filtered out an element, but that’s it.Everytime we call this function block, it will start by initialising the data array.Since we are not persisting the data, requesting all the records, will still return all the data records.Persisting dataThere is a way to connect to a database, but that is outside of the scope of this post.First, let’s show some ways of persisting data.  Node context  Flow context  Global contextThe node context lets the user persist variables for that node.Whenever that node gets redeployed, the context is gone.The flow context is a context shared over all nodes in that flow chart while the Global context lets you share a context over the whole app.You can set and get a context variable with the getters and setters from the api.For a flow context this will look like:let data = flow.get('data');// do stuffflow.set('data', data);Initialising dataNow that we know how we can store data, we need to find a way to initialise the data.We could for example launch a call to trigger a function that would store data in the flow context.We could build this by using an http-input, a function-block and an http-response-output.But this doesn’t feel right.Luckily, Node-RED provides us with an inject-input.  This input will get triggered periodically, or just one time. inject-input is made for injecting messages of almost any type into the next component.This message can be a string, a number, the current timestamp, …We are going to use it as a trigger on startup to initialise our data into our flow.inject-input:  inject-function:  RefactoringNow we know how we can inject and persist data in a context, let’s refactor our GET and DELETE.And while we are at it, add a second GET so we can query one record by ‘id’.GETOur two http-inputs are exactly the same except for the parameter (id).The function-block on the other hand, will have some build-in logic:let data = flow.get('data');if(msg.req.params &amp;&amp; msg.req.params.id) {    for(let itx in data) {        if(data[itx].id == msg.req.params.id) {            msg.payload = data[itx];            break;        }    }} else {    msg.payload = data;}return msg;We are retrieve the data from the context and storing it in a local variable.Based on the existence of the id parameter, we are going to search for a single record, or return all records.Notice the == instead of ===.This is because the id in the data is a number, where the id from the params is a string.Based on the logic, we could add a query parameter to the endpoint that let’s us filter the data on the title property for example, but that is out of scope.DELETEAs shown earlier, we are going to use a simple filter function to filter out the record with the given id.We start by retrieving the data from the flow context, and then filtering this data.Don’t forget to rewrite the new data to the flow context, so other components will get updated context data.let data = flow.get('data');if(msg.req.params &amp;&amp; msg.req.params.id) {    data = data.filter((item) =&gt; {        return item.id != msg.req.params.id;    });    flow.set('data', data);}msg.payload = undefined;return msg;All togetherNow we should have an inject-input and 3 http-inputs (2 GET’s and 1 DELETE).Our flow should look like:  Let’s deploy and test this setup.First click the deploy button and wait a second.Then navigate with your browser to http://localhost:1880/data.This should display your data in your browser.  Now, let’s try to delete a record.$ curl -X DELETE http://localhost:1880/data/3And refresh your browser:  Seems like one big success! No?Not exactly.Try and delete records 1, 2, 4 and 5.Now what?Our data is gone and we don’t have a POST nor a PUT implemented.Implementing a POST and a PUT is really straightforward.POST and PUTFor the example, we are going to work with an upsert. POST and PUT will trigger the same function block that will be smart enough to update (if exists) or insert.  The upsert function-block can look like:let data = flow.get('data');let body = msg.payload;body.id = Number(body.id); // quick fix making sure it's a numberlet id = msg.req.params &amp;&amp; msg.req.params.id || body.id;let found = false;for(let itx in data) {    if(data[itx].id == id) {        data[itx] = body;        found = true;        break;    }}if(!found) {    data.push(body);}msg.payload = body; // or adjusted body if neededreturn msg;So now we have a GET, POST, PUT and DELETE.But still, we are missing something.Assume we are developing delete functionality in the frontend while using this setup as a backend.Since we only have 5 records, I should inject new data to test after my 5 deletes.Or assume we are testing a frontend, connected to this mock backend.Wouldn’t it be nice that we can refresh the data to its initial state before running a new test suite?The point I’m getting to is, we should come up with a way of refreshing our data to its initial state.Resetting dataWhen you look at your flow chart, you’ll see there is something that looks like a button on the left side of our inject-input.When you click it, you’ll notice a toast message appearing on the top of your user interface:  But what if we don’t have access to the user interface, or we don’t have control over it what so ever? For example, when running automated tests against this mock backend, somewhere on a dynamic Jenkins Docker slave? To overcome this, we are using an http-input to trigger the reset of our data.Let’s take a look at our implementation:  The http-input is listening on /rest.As you can see, we’ve added an http-response-output.REST-calls are expecting a response! Don’t forget to deploy your new setupMaking sure this works:$ curl -X GET http://localhost:1880/data   // should respond with the initial data$ curl -X DELETE http://localhost:1880/data/2   // should delete record with id 2$ curl --header \"Content-Type: application/json\" \\  --request POST \\  --data '{\"id\":\"7\",\"Title\":\"Title 7\"}' \\  http://localhost:1880/data   // should add record with id 7$ curl -X GET http://localhost:1880/data   // should give the expected result$ curl -X GET http://localhost:1880/reset  // should reset the data$ curl -X GET http://localhost:1880/data   // should show the reset is working  JSONThe last refactoring we are going to do in this tutorial is switching from an hardcoded data json object to an external json-file.As mentioned before, Node-RED provides ways of integrating storage and databases, but that is out of the scope of this post.Still, we don’t want to include real data in this setup.We should loosely couple our data provider and the backend mock implementation.If some analysts, for example, provide us with new test data, we don’t want to change anything in this setup.One way to overcome this, is to work with json-files.Assume we have a json-file, called data.json with content:[  {\"id\": 1, \"title\": \"Title 1\"},  {\"id\": 2, \"title\": \"Title 2\"},  {\"id\": 3, \"title\": \"Title 3\"},  {\"id\": 4, \"title\": \"Title 4\"},  {\"id\": 5, \"title\": \"Title 5\"}]Remember when I mentioned some important lines in the console at startup?One of those lines was referring to the User directory:User directory : /Path/To/Your/Home/.node-red (which is the default)Because we didn’t change anything in the settings.js file (or didn’t point to a custom one), Node-RED creates a context folder for the current instance.Copy your data.json to this folder.I will explain later why we are doing this.  Let’s read data from this json file instead of hardcoding the data:Add a file component from the storage sectionOn the left of the user interface, there is a storage section with an input file option.Drag this onto the flow chart.Unlink the connections from the inject-input and the reset http-input with the ‘store’ function-block and connect both with the file-storage component.Open the details and set the properties:  Do read the text on the bottom about the patch to the file! And keep in mind we have selected the ‘single UTF-8 string’ option.Add a Json parse functionIn the function section on the left, you have a JSON component.Depending on its input, it will parse or stringify a string or json object.Because we will be sending this component a ‘single UTF-8 string’, it will parse it to a JSON object.  Integrate this component in between the file-storage component and the store function-block.Refactor the store functionWe are now ready to refactor the store function-block.Remove the data and replace it with the content of the json-parse function.  Result [result]When finished, our setup should look like the following flow chart:  When the json-file gets updated, a data reset call will reload the new data into the flow context without needing to change the setup itself!Node-RED persistent configAs mentioned before, Node-RED uses a user directory, a flow.json file and a settings.js file.If those do not exist, it will create default ones for you in the default node-red path %USERPROFILE/.node-red.Going through the settings file is also out of the scope of this post.Advanced users can just read all about it in the docs or just open it and modify it.Why I’m mentioning this directory, is because of the config persistence of Node-RED.Go ahead and kill your Node-RED server (CTRL-C in terminal).And now just restart it:$ node-redAs you can see, Node-RED will just pick up its previous config because it’s available in the default directory.When more people need to work with this config, we need to share it.The easiest way to share this config, is to wrap it in a Git repository.Keep in mind that this may not be best practice to wrap the whole directory, but this way, your colleagues and continuous integration platform can just checkout the repository and run the server, including the data files.When running $ node-red --help it shows the command line params.  Look at the -s and -u option.Node-RED provides us with cli parameters to run custom configs.Let’s assume we clone our newly created Git repository in ~/repositories/node-red.$ node-red -s ~/repositories/node-red/settings.js -u ~/repositories/node-redNode-RED and DockerNode-RED also provides a Docker image at Docker Hub.This way you do not even need a Node.js environment preinstalled.You can just run it with:$ docker run -it -p 1880:1880 --name mynodered nodered/node-red-dockerWhat about our persistent config?  Node-RED is started using NPM start from this /usr/src/node-red, with the –userDirparameter pointing to the /data directory on the container.The flows configuration file is set using an environment parameter (FLOWS),which defaults to ‘flows.json’.This can be changed at runtime using thefollowing command-line flag. – from docs https://hub.docker.com/r/nodered/node-red-docker/$ docker run -it -p 1880:1880 -e FLOWS=my_flows.json nodered/node-red-dockerSo we can choose only to provide the flows.json file or we could map our user directory from our repository as a volume to the /data in the container, providing the container our context.An other option is to bake your userDir into your image.This way you can provide your CI with a ready to go Docker image for this particular case.This is probably not the best solution, but in some cases it can be very helpful.Node-RED and CIAs we now know how we can configure, dockerize and run our mock backend, we could easily include it in one of our testing stages during our Continuous Delivery pipelines setup.For those of you whom read the article about the different stages of API testing (here), the following setup will need no further explanation.  In this setup, the task at hand is running the automated tests defined in and run by our Gauge, Protractor or Nightwatch.js framework.To be able to do this, it would be nice to launch an environment at once.We can provide our Continuous Integration environment with a compose file that will launch and destroy our frontend and Node-RED mock backend in one environment.ConclusionIt’s nice to have an external tool available that can get reused for multiple purposes, in our case development and testing.Node-RED provides us with cool features and an easy to use User Interface to do so!"
      },
    
      "testing-2018-08-03-testing-angular-with-jest-html": {
        "title": "Testing Angular with jest",
        "url": "/testing/2018/08/03/testing-angular-with-jest.html",
        "image": "/img/2018-08-03-testing-angular-with-jest/jest.png",
        "date": "03 Aug 2018",
        "category": "post, blog post, blog",
        "content": "Last year I learned about Jest, a testing framework. ‘Yet another one’ was my first thought.Later that year my colleague and I were looking to extend our test coverage. We where using Jasmine to write the tests we had and Karma to run them.It worked for sure and we had a lot of tests but it was like a punishment to write them every time, repeating the same code to mock things and when it finally worked and we pushed them to the CI they would sometimes fail randomly.So we were eager to find a better way to test.Pretty quickly we started looking into Jest. It differentiated itself by not using Karma as a test runner.We liked the idea because Karma actually has some points of failure which we encountered often.Karma vs JestKarmaLet me quickly give you an overview of what it is that Karma does:  it spawns a webserver to serve your source and test code;  it starts a browser from your machine and connects to the webserver;  it spawns a reporter which has a socket connection with the webserver;  it runs every test, awaits its result and sends it to the reporter.In the end we have 3 components communicating with each other. Which components exactly, dependends on the environment Karma is running on.Our CI was a Linux machine, I had an Ubuntu to work on, my colleague was on MacOS, and other guys in the team were on Windows.So we all had different Chrome versions which gave us some issues. And our CI used PhantomJS, which is outdated, so here we also had some issues.JestHow are these issues fixed in Jest?As I mentioned before Jest does not use Karma to run the tests, it will just start a single NodeJS process which does all the work by itself:  it compiles your code;  it runs your tests with NodeJs (or JSDOM for DOM testing);  it creates a report.Just plain and simple without too many interconnected processes to break.Also, no real browser is needed on the machine since NodeJs and JSDOM are used.Therefore the only tool to keep up to date is Jest, which is managed automatically via the Yarn lockfileSet upSo how can you set it up and quickly replace all your tests (if you’re coming from Jasmine)?To make Jest available in an Angular project, you first need to install Jest and jest-preset-angular.Since Jest is made for React (backed by Facebook, remember) we need jest-preset-angular to fix some things for us.$ yarn add -D @types/jest jest jest-preset-angularSome configuration is always needed so let’s add some lines to the package.json (or export the config in a jest.config.js).First we point to the preset that we will use. Next we provide a setup-jest.ts script in which we import some necessary files (we’ll create it later on).Then we provide information about how Jest should transpile our code under the transform property. Therefore we point to the preprocessor from jest-preset-angular for our typescript and html files.And for the Javascript files we’ll point to babel-jest (which ships with Jest).The transformIgnorePatterns point to some libraries that don’t need to be transpiled for our tests. (If you get Unexpected token import issues, you might need to add some packages here)\"jest\": {  \"preset\": \"jest-preset-angular\",  \"setupTestFrameworkScriptFile\": \"&lt;rootDir&gt;/src/setup-jest.ts\",  \"transform\": {     '^.+\\\\.(ts|html)$': '&lt;rootDir&gt;/node_modules/jest-preset-angular/preprocessor.js',     '^.+\\\\.js$': 'babel-jest'   },  \"transformIgnorePatterns\": ['node_modules/(?!@ngrx|ng2-translate|@ionic|lodash|ionic-angular)'],}As mentioned previously, we create a setup-jest.ts file in which we import some code from jest-preset-angular and a global mocks file.import 'jest-preset-angular';  import './jest-global-mocks';  In the jest-global-mocks.ts we provide functionality that is not found in JSDOM but that we use in our code (and thus is found in our preferred browser).So we mock things that are globally accessible, if you use certain browser API’s you should also mock them here.For our example we needed the following code:const mock = () =&gt; {  let storage = {};  return {    getItem: key =&gt; key in storage ? storage[key] : null,    setItem: (key, value) =&gt; storage[key] = value || '',    removeItem: key =&gt; delete storage[key],    clear: () =&gt; storage = {},  };};Object.defineProperty(window, 'localStorage', {value: mock()});Object.defineProperty(window, 'sessionStorage', {value: mock()});Object.defineProperty(window, 'getComputedStyle', {  value: () =&gt; ['-webkit-appearance']});Object.defineProperty(window, '__env', {value: {env: {backendUrl: 'mocked URl'}}});As you can see, our tests use localStorage, sessionStorage, getComputedStyle and an environment property (__env) on the window.With everything set up we could run our test by running the Jest commandjestOf course not much is running yet since all our tests use Jasmine, and ‘jasmine’ (as a keyword) is unknown to Jest.To fix this we need to replace some Jasmine specific code by Jest specific code.Jasmine to jestIn Jasmine we would create a mock object using Jasmine’s createSpyObj-function and passing some parameters in it.// Jasmineconst serviceMock = createSpyObj('service', ['methods', 'you', 'want', 'to', 'mock']);In Jest we just create an object with the expected properties, and jest.fn() creates a mock function for us.This is a great improvement since you get better code hints and you can easily create more advanced mocks.// Jestconst serviceMock = { methods: jest.fn(), you: jest.fn(), want: jest.fn(), to: jest.fn(), mock: jest.fn()};Also to mock return values it is a bit different (for the better):// JasmineserviceMock.you.mockReturnValue(serviceMock.you as Spy).and.returnValue('yannick vergeylen');// JestserviceMock.you.mockReturnValueOnce('yannick vergeylen');// And you can chain multiple return values if you need itserviceMock.you.mockReturnValueOnce('yannick vergeylen')               .mockReturnValueOnce('bob')               .mockReturnValue('everyone');// Or even with a function which can execute simple logic.// But you shouldn't be implementing to much logic, since you don't want to test the tests.serviceMock.you.mockImplementation((firstname,lastname) =&gt; `${firstname} ${lastname}`);// Or provide it at initialisation which saves you a line of codeconst serviceMock = { methods: jest.fn(), you: jest.fn((firstname,lastname) =&gt; `${firstname} ${lastname}`), want: jest.fn(), to: jest.fn(), mock: jest.fn()};For the assertions you shouldn’t have to change much, since Jest uses almost the same assertion functions as Jasmine.expect(serviceMock.methods).toHaveBeenCalled();expect(serviceMock.methods).toHaveBeenCalledWith('value');// Jasmine(serviceMock.you as Spy).calls.mostRecent()[0]// JestserviceMock.you.mock.calls[0][0] // to get the first argument of the first call (firstname)serviceMock.you.mock.calls[0][1] // to get the second argument of the first call (lastname)I changed all our tests with some regexes, it is possible with some creativity, but today there are codemods which should do the hard work for you.Checkout the jest documentation to find out more.Jest really gets interesting when you use libraries and need to mock them:// Jestimport {HttpClient} from '@angular/common/http';import {CompaniesService} from './companies.service';import {Observable} from 'rxjs/Observable';jest.mock('@angular/common/http');const httpClient = new HttpClient(null);let companiesService= new CompaniesService(httpClient);test('the service should map the return value to an array of companies', () =&gt; {  httpClient.get.mockReturnValueOnce(Observable.of({companies:[{name:'C1',code:'C1'}],page:6,total:51}))  companiesService.getPage(6)    .subscribe((value)=&gt;expect(value).toEqual([{name:'C1',code:'C1'}]));  expect(httpClient.get.mock.calls[0][0]).toEqual('backendUrl/companies?page=6')});Instead of mocking HttpClient we can just import it, provide the return value we know backend will give and focus on testing the output of our getPage method.In the above example you see I have to create a instance of httpClient to get around dependency injection in Angular, but other imports can also be mocked in the same way.ConclusionSo one year later we are still using Jest and testing is still a lot more enjoyable than it was before.Not painless as Jest claims it to be, but that’s just the nature of testing I guess."
      },
    
      "agile-2018-07-31-agile-leadership-game-html": {
        "title": "Agile Leadership Game",
        "url": "/agile/2018/07/31/Agile-Leadership-Game.html",
        "image": "/img/agile-leadership-game/How-Agile-Are-You.jpg",
        "date": "31 Jul 2018",
        "category": "post, blog post, blog",
        "content": "Last week we, as the guild of agile coaches from Ordina, tried and tested the Agile Leadership Game developed by the Agile Consortium called ‘How Agile Are You?’.In this blog post, we will provide you with some more information about the game.Details  2-6 players (even though more is possible)  Intended audience is management  Takes 1.5 hours (on average)  Needs a skilled facilitatorGame PlayThis game has been designed to get discussions on agile leadership going and to ensure that people start observing behaviour that belongs (or does not belong) to an agile organisation.It is specifically intended for management teams.The game is focused on agile mindset and agile leadership: what do people and organisations need from management when engaged in an agile transformation?Participants are asked to very rapidly (! ten seconds per card on average :) !) divide fourty-two cards with ‘characteristics’ of organisations, over one of the quadrants on the board.As a participant you are to decide how the characteristic manifests itself in your organisation at this moment.The game board consists of the quadrants “LET GO, IGNORE, CREATE and KEEP” as you can see in the image below.If the characteristic on the card is behaviour that the organisation DOES HAVE at that time but that you DON’T WANT, the card gets sorted in the quadrant “LET GO”.If the participant perceives the characteristic on the card as a behaviour the organisation DOES HAVE and that you DO WANT, it ends up at “KEEP” and so on.An example of a characteristic is “management spends a lot of time putting out fires”.There is no right or wrong: the game provides insight in the current state of affairs.The characteristics are labelled in types of behaviour belonging to types of organisations.So you could end up with lots of characteristics belonging to agile organisations in the “CREATE” quadrant. :)After the time box for sorting the cards ends, the participants will discuss the characteristics that seem most relevant to them and their organisation.Each participant gets the chance to enter at least one card/characteristic into the discussion.This needs to be (well) facilitated by an experienced coach/agile master.You can take as long as you like for the discussion and could do several discussion rounds, but we would recommend not to have it last for too long (1.5 hours max).After the discussion round it is useful to add an extra step in the game as to make the outcome more actionable:the participants prioritise the top three of characteristics/topics they would like to let go or create.These are the topics they can start working with the very next day!This way you also prevent the session being perceived as just fun, irrelevant and/or without consequences.Because there are fourty-two cards in the game and you won’t discuss them all, the game is most suitable to be played again at a later time – it won’t lose its value.Outcomes retrospective:TIP:  Participants need some time to figure out how to play the game and how to work with the quadrants on the board.  An ice breaker to start with and a small warm up round increases effectiveness and actionable outcome of the game.  Making sure the topics in the discussion round are precisely those that matter most to the organisation and the participants, adds greatly to the usefulness of the session.  You need a skilled facilitator.TOP:  Very nice game to play, offering lots of insight and actionable openings to start your transformation at leadership level.  The game doesn’t take too long, can easily be played several times over time and can easily be fitted into a leadership programme, workshop, curriculum or “heidag”.  It’s a perfect self-assessment game for management teams that are wondering how agile they actually are as a team. :)If you would like to try this game but don’t yet have an experienced facilitator available, don’t hesitate to reach out to us! We’d love to help!More information about the game can be found here."
      },
    
      "culture-2018-07-03-jworks-a-culture-of-learning-html": {
        "title": "JWorks: A Culture of Learning",
        "url": "/culture/2018/07/03/JWorks-a-Culture-of-Learning.html",
        "image": "/img/jworks-culture.jpg",
        "date": "03 Jul 2018",
        "category": "post, blog post, blog",
        "content": "  People often wonder, why should we invest in people, what if they leave?Better ask yourself, what if you don’t, and they stay?The Java unit at Ordina Belgium has always been amongst the most profitable and respected units of the entire group.Yet, we’ve seen a big change in management style a couple years back.While during the financial crisis the primary focus had to be on continuity and securing revenue, the last couple of years have been revolving around the people and their careers.The Java unit has been rebranded to Ordina JWorks, a separate identity with a well-defined mission.We want to help our customer strategically, by co-creating solutions that help them disrupt their respective industry.To do that, we had to become thought-leaders in our field, recognised for our expertise by both the tech community and our customers.In today’s market, hiring only the very best profiles doesn’t scale well.Hence, allowing people to grow is the only way forward.Aside from books and online courses, internal trainings and competence centers are a great way to get people to the next level.JWorks organises at least one training or knowledge sharing session a week.During winter and summer, entire training programs are being organised both for junior, medior and senior profiles.This begs the question though, how do we keep these trainings relevant?The gap between repeating what others have said and found out, and actually performing new qualitative research, is what makes all the difference.Being able to experience learnings firsthand allows people to really dig into subjects and deeply understand the trade-offs and decisions that led to solutions we use today.Part of researching a given topic consists of talking to international subject matter experts.Thanks to the open source community we get to work in, people are very approachable.It’s quite straightforward to get to know core committers, well-known architects or developer evangelists.Yet, the best way to really pick someone’s brain is to meet them face to face.JWorks facilitates this by sending our people to conferences and events around the world.San Francisco, New York, Seattle, London, Barcelona, Las Vegas, Austin, Berlin, Kiev, Riga, Paris, Amsterdam, Budapest, Casablanca, Frankfurt, Washington DC; only a few of the cities we’ve frequented during the past couple of years.We encourage people to take a couple days extra to enjoy the city and do some sight-seeing.Conveniently labelled “learning holidays”, this type of trips really create a community feeling inside the company.If people are having fun, they’ll be much more motivated to go the extra mile.Upon return, the accumulated knowledge gets put into good use by giving input in training material, creating blogposts with differentiating content or by helping to shape customers’ vision and facilitating technical decision-making.We also encourage people to be vocal about the things they love, by giving a workshop, speaking at internal events, or becoming a rock star speaker at international conferences.By becoming public speakers ourselves, we really get to know the nitty gritty details of whatever subject we’re talking about.Taking a seat at conferences’ speaker dinners also helps to network and enter the world of the greatest minds of our industry.This is how JWorks builds a community of highly motivated thought-leaders."
      },
    
      "conference-2018-07-03-agile-devops-summit-brussels-html": {
        "title": "Agile &amp; DevOps Summit Brussels",
        "url": "/conference/2018/07/03/Agile-DevOps-Summit-Brussels.html",
        "image": "/img/agile-devops-brussels-2018/agile-devops.png",
        "date": "03 Jul 2018",
        "category": "post, blog post, blog",
        "content": "  Agile &amp; DevOps Summit Brussels is a one day conference on agile practices and DevOps.In this post, I will take you along to the various talks that were attended.Table of Contents  Agile is (not) only for IT  Agile Maturity  How to bring Agile out of software sector  What we wished we knew before signing up for Agile and DevOps Transformation  Is Spotify becoming the new Xerox?Agile is (not) only for ITThe talk started with explaining that all 12 principles of the Agile Manifesto can be applied to all layers of the organization, it’s not only for the IT department (hence the title). The Agile Manifesto enables everyone to focus on the same objective and towards the same goals.In order to get everyone to focus, it’s important to know in what situation you are. To find out, you can use the Cynefin framework. This framework explains where you belong by differentiating between 4 different domains.  Complex: non-lineair, never complete.  Complicated: analysis is needed, a lot of possibilities, multiple possible answers.  Chaotic: take action, dynamic, nothing is the same.  Obvious: predictable and repeatable, one or few answers.To take a real example: ‘Planning a birthday party for your child’:  If you leave the children all alone the entire afternoon, you have no idea what happens during your absence or what the state of your house will be when you return: Chaos.  If you plan the entire afternoon, with a timetable, to ensure that everything happens as planned: Complicated.  If the children are obedient and don’t do anything wrong at all (not exactly realistic): Simple.  If you set boundaries, rules, adjust continuously: Complex.When you’re in a complex situation, you can apply the agile principles.The other part of the talk was about the Agile Wave and how it affects other parts of your organization other than IT. If the IT department starts working according to the agile principles, there is always an impact on the other domains of your organization.For instance, if you have a team that goes agile, there’s an impact on items like budget, bonuses, costs, collaboration, estimation and so on. So the other domains need to adapt as well.So agile is not only for IT because:  Agile values and principles are applicable in all domains.  Agile helps dealing with complexity, which is present in all domains.  The roots of the agile principles originate from more domains than just the IT.  The agile journey of an organization impacts all layers, not just IT.Agile MaturityThe next talk’s subject was the issue of measuring agile maturity in an organization. Agile maturity can be answered with 3 questions:  Why am I doing it?  What am I doing?  I am convinced about its outcome?If you can answer those 3 questions as an organization, you’re already far ahead of others in terms of agile maturity.To determine the maturity, discuss the following items:  Context: You should know the context of what you’re doing.  Team: What is your motto? Know your team to avoid turnover. Important values for a team: respect, trust and humility.  Discipline: Agile requires much discipline, such as timeboxing, estimating, communicating with the customer, collaborating and so on.  Infrastructure: Must be fast to react to changes. If it’s not, it blocks various agile processes. DevOps is obviously crucial as it will enable faster reaction times.  Ownership: Taking ownership of what you’re doing. Never shift responsibility to someone else.Bring Agile out of the software sectorThis talk touched the same subject as the first talk, but had a bigger emphasis on why an organization should shift towards agile.There are some organizations that don’t like change, they’re not comfortable with it. Yet inevitably, everyone needs to adapt. In general, IT departments are used to change and quickly embrace it, but other departments might not.The biggest investment that will be needed when transitioning is people’s moods and behavior.  So it’s important to state why agile is useful and what the advantages are:  Time to volume: It’s easier to respond to changes in requirements.  Efficiency: People share and collaborate more, which decreases the amount of bottlenecks.  Engagement: You can retain and attract more talent.The process of change traditionally follows the Kübler-Ross model.What we wished we knew before signing up for Agile and DevOps TransformationUnlike the other talks, this one featured a panel discussion about what you should do if you want your Agile and DevOps transformation to fail. A discussion was had with 3 panel members and the rest of the audience.  Just rebrand your middle management          For example, instead of having a regular project manager, now you can have agile project managers.        Ignore resistance and feedback          It doesn’t matter what your employees think, the management knows better.        Discourage experimentation and fire bearers of bad news          Employees shouldn’t be allowed to try new things and if it’s bad news, everyone should ignore it.        Don’t plan for productivity loss with new people          If you add new people to a team, the team should be more productive right away, right?        Add all agile ceremonies on top of existing meetings          If we have more meetings, it means we’re discussing items and are productive.        Do no take minutes, assign no actions          Everyone remembers what was discussed and knows what to do, many days later.        Don’t take metrics          If we don’t see the problems, there aren’t any.        Don’t measure anything apart from badging hours          Obviously, there’s a direct correlation between badged hours and productivity.        Do DevOps with Ops          It’s a fancy name.      Infrastructure should still be separate so it can be controlled.        Create DevOps department to coordinate between Dev and Ops          More control and coordination is better        Ban teams from using their own tools          Management knows best what tools their teams need, all teams need the exact same things.      To be clear, this list contains a fair bit of sarcasm.Spotify becoming the new XeroxThe last talk I attended was a discussion about the agile model of Spotify that is becoming the new Xerox. The term ‘new Xerox’ is a reference to the generic trademark that Xerox became. When you talked about photocopies in the past, you would talk about Xerox as they were the leading organization in that area.Other examples are Bic, Sharpie, Croc and so on.The speaker then started talking about the agile model that Spotify uses in The Good, The Bad and The Ugly terms.The Good  It helps to maintain a team’s autonomy and keep the various teams aligned.  It helps to scale agile by using the concept of tribes, chapters, guilds, squads and so on.The Bad  It’s a mistake that it’s called a model. It’s an engineering culture which typically cannot be mapped one-on-one to another organization.  It was released 6 years ago, which is ancient in agile terms. It has evolved and might be completely different now.  It doesn’t talk about management nor budgets. While agile is less concerned about this, it should still be mentioned if you want the management to follow.  The context that was used by Spotify while creating this model is not the same as the context that is present in your organization.The UglyThe terminology that Spotify uses has some issues:  Tribes: it sounds primitive.  Squad: it sounds military.  Guild: it’s a blocked evolution from medieval times.To complete the talk, he explained the acronym CALMS that is used for DevOps transformations:  C: Culture          Empower your teams.      Roles should be used instead of functions.      T-profiles are the way to go.      Failure should be allowed as long as you learn from your mistakes.      Work together with the business to achieve the desired product.        A: Automation          Everything is code (config, code, tests and so on) and in the pipeline.      Pay attention to things that can go wrong.        L: Lean principles          Constant flow of small changes.        M: Measure to improve          Inspect and Adapt.        S: Sharing          Learning from your peers.      "
      },
    
      "iot-2018-06-27-viroreact-html": {
        "title": "ViroReact: Build cross-platform AR/VR applications for Android and iOS using React",
        "url": "/iot/2018/06/27/viroreact.html",
        "image": "/img/2018-06-26-viroreact/viroreact.jpg",
        "date": "27 Jun 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Basics of ViroReact  Demo app  Lessons learned and conclusion  Extra resourcesIntroductionAugmented Reality is a very interesting space so naturally, we wanted to do something with it.There are many frameworks available for building cross-platform AR/VR applications.We came across ViroReact which uses React and React Native to create immersive VR and AR applications using a single codebase for Android and iOS.This persuaded us to give it a try.In one day, with some tweaking afterwards, we were able to create a simple app to show the status of a meeting room.Basics of ViroReactComponentsThe main building blocks of a ViroReact app are the components.The most important component is a SceneThe scene is the digital environment the user interacts with.All other components live in this scene.&lt;ViroScene&gt;&lt;ViroARScene&gt;Just like many other components, the Scene comes in two variations: ViroScene and ViroARScene.As the name suggests ViroARScene is used for Augmented Reality applicationswhile the ViroScene is meant for Virtual Reality.A ViroSceneNavigator is used to navigate between these scenes.&lt;ViroSceneNavigator&gt;&lt;ViroARSceneNavigator&gt;The SceneNavigator holds a stack of scenes to enable simple transitions from one scene to the next or previous scene.These scenes can be populated with all kinds of componentsFor example a ViroText.&lt;ViroText\ttext=\"Hello World\"\tposition={[0,0,-5]}\tcolor=\"#ff0000\"\twidth={2} height={2}\tstyle=/&gt;Or perhaps a ViroBox, a ViroQuad or a ViroSphere.These components can be modified by setting their properties.One of the most important properties is the position.It takes an array of three numbers as the x, y and z coordinates.There are also more specialised componentsLike a Viro3DObject which lets you include custom made 3D objects in your scene.&lt;Viro3DObject\tsource={require(\"./res/spaceship.obj\")} type=\"OBJ\"\tresources={[require('./res/spaceship.mtl'),\t\trequire('./res/texture1.jpg')]}\tposition={[1, 3, -5]} scale={[2, 2, 2]}\trotation={[45, 0, 0]}\ttransformBehaviors={[\"billboard\"]}/&gt;These 3D objects can be created in your favourite 3D modelling tools.ViroReact supports OBJ or VRX (converted from FBX) formats.The billboard transform behaviour is an interesting feature to make sure the object is always facing the user.A Viro360Image can be set as the background of a scene.A ViroPortal lets the user take a peek into a different scene.It basically is a window to another world.The Viro3DObject included in it acts like the window frame.A ViroARImageMarker reacts when one of the ViroARTrackingTargets is scanned.It will show all the components inside the ViroARImageMarker tag.We have used this in our little app, more on that below.&lt;ViroARImageMarker target={\"targetOne\"}&gt;    &lt;ViroBox position={[0, 0.25, 0]} scale={[0.5, 0.5, 0.5]} /&gt;&lt;/ViroARImageMarker&gt;ViroARTrackingTargets.createTargets({  \"targetOne\" : {\tsource : require('./res/targetOne.jpg'),\torientation : \"Up\",\tphysicalWidth : 0.1 // real world width in meters  },});We wouldn’t see anything without eyes and lightThe Camera component is our eyes.&lt;ViroCamera position={[0, 0, 0]} rotation={[45, 0, 0]) active={true} /&gt;A default camera is provided at the origin position={[0, 0, 0]}.The camera always looks in the negative Z direction.So if you want an object to be visible as soon as the scene loads up, make sure to set its position with a negative Z value position={[0, 0, -5]}. If the object would have a positive Z value it would be placed behind you when you load up the scene.There is also a ViroOrbitCamera where the camera orbits around a certain position, always keeping it in view.Lights are very important components in a scene.Without any light, the user wouldn’t see anything.Luckily a default light is provided when none is defined.We didn’t need lights in our setup but if you want a more realistic or visually stunning experience, I highly recommend you to look into the different lights in the documentation.There are four lighting models: Phong, Blinn, Lambert and Constant.These are the algorithms that calculate what your objects will look like when influenced by light.By default, elements use the Constant lighting model, which means lights will be ignored and the object will show its full color.ViroReact supports four types of light.The ViroAmbientLight is the simplest light.&lt;ViroAmbientLight color=\"#ffffff\"/&gt;It lights up all the objects in the scene equally from every direction.Only the color needs to be set.A ViroOmniLight is comparable to a light bulb.&lt;ViroOmniLight    color=\"#ffffff\"    attenuationStartDistance={2}    attenuationEndDistance={6}    position={[0,-5,5]} /&gt;The light originates from a specified position and moves in all directions.The light slowly fades out if the distance is between 2 and 6, set in the attenuation properties.A ViroSpotLight is comparable to a flashlight.&lt;ViroSpotLight    color=\"#ffffff\"    attenuationStartDistance={2}    attenuationEndDistance={6}    position={[0, -5, 5]}    direction={[0 -1, 0]}    innerAngle={0}    outerAngle={45} /&gt;The light originates from a single point and shines in a cone.The direction and angle of the cone can be set as well as the attenuation start and end distances.Some other properties are available to create the shadows you want.The last type of light is the ViroDirectionalLight.&lt;ViroDirectionalLight    color=\"#ffffff\"    direction={[0, -1, 0]} /&gt;The sun would be the prime example of a ViroDirectionalLight.It shines over the whole scene in a specific direction.To make these objects look good we need MaterialsMaterials are used to place texture images on 3d objects.They can make a long box look like a brick wall or a sphere look like our planet.ViroMaterials.createMaterials({  earth: {\tshininess: 2.0,\tlightingModel: \"Lambert\",\tdiffuseTexture: require('./res/earth_texture.jpg'),  }});There are many properties available for materials.Discussing all of these is outside the scope of this article.The diffuseColor and diffuseTexture are the main color and texture of the material.These can be placed on basic 3D objects like ViroBox, ViroQuad and ViroSphere.Complex Viro3DObjects can have multiple materials.Move them around with AnimationsOur scenes are still lacking motion.That’s where animations come in handy.ViroAnimations.registerAnimations({\tanimateImage:{properties:{scaleX:1.0, scaleY:0.6},\t\t\t\t  easing:\"Bounce\", duration: 5000}});&lt;ViroImage source={require('./res/myimage.jpg')}\t\t   position={[0, -1, -2]}\t\t   animation=/&gt;Animations change the numerical value of properties over time.Typically this is used for the position and scale properties but it can just as well be used for color values or other numerical values.There are five easing types: Linear, EaseIn, EaseOut, EaseInEaseOut and Bounce.These represent how the values change over time.Control the scene with a ViroControllerWith all of the above, we can create an awesome scene but we can’t interact with it yet. The ViroController provides us the ability to interact with our scene.&lt;ViroController reticleVisibility={true} controllerVisibility={true} onClick={this._onClickListenerForAllEvents} /&gt; With the reticle (the blue circle in the middle of the view) the user can select and point to objects and call the function in the onClick property.It’s also possible to use other events like onHover, onTouch, onSwipe, onPinch, onScroll, etc…These events are very useful for UI.There are many more fun componentsApart from the ones we mentioned here, there are many more fun components to include video, sound and particles in your scene.They follow the same principles but each have their own properties.This being built on React Native we can also declare our own components of course!export default class CustomComponent extends Component {\tstatic propTypes = {customProperty: PropTypes.number}    constructor() {super() ;}\trender() {return (\t\t&lt;ViroText text={this.props.customProperty}&gt; );}\t}&lt;CustomComponent customProperty={42}&gt;Just extend from Component, define your properties in propTypes and use it in your scenes.In our demo application, we used this to create our scene but you can create small reusable components too of course!Demo applicationBefore you can build a ViroReact application you need to request an API key from ViroMedia.This should not take more than a few minutes.For our demo application, we decided to use meeting rooms as a use case.Since ViroReact is advertised as a framework for rapidly building AR / VR applications, we wanted to put this statement to the test and try to create an application in one day with no knowledge of the framework at all.Use Case: Meeting room status viewer    Sometimes people want to have a quick meeting or Skype call. They might be standing or passing by a meeting room, and have the ability to immediately check if the meeting room is available for the next 30 minutes.We created an application where the user can view the status of a meeting room in Augmented Reality by scanning the room name/picture.We used Image Recognition in ViroReact to achieve this.Aside from the Image Recognition, we were also curious how ViroReact handles  HTTP requests  UI Updates  User interactionEventually, I created three applications:  A backend application for holding meeting room state (NestJS)  A client application for changing the state of a meeting room (Ionic PWA)  A client application for viewing the state of a meeting room (ViroReact)For this blogpost, I will only discuss how we created the ViroReact application.The AR Meeting room viewer (ViroReact)    ViroReact is built on top of React, so basic knowledge of React is necessary. To get started, you can follow this great free beginner guide by Kent C. Dodds at egghead.io The beginner’s guide to ReactStep 1: Project setupWe began by following the Official Quick Start guide from ViroMedia.Using the react-viro CLI, we generated a ViroSample project.react-viro init ARMeetingRoomViewerStep 2: Create an AR scene for viewing the status of a meeting room.To be able to scan the meeting room we needed a picture of the meeting room nameplate, this will act as the marker to scan the meeting room information.    I placed the marker image inside of the **/js/res** folder.Because the file extension was in capital letters (.JPG), I had to configure this extension in the rn-cli.config.js file inside of the root folder.  getAssetExts() {    return [\"obj\", \"mtl\", \"JPG\", \"vrx\", \"hdr\"];  },Next, I created the actual scene in a file called markerscene.js in the /js/ folder.To be able to scan the image marker, we need two important APIs:  ViroARTrackingTargets  ViroARImageMarker componentWhen the scene initialises we need to setup the Tracking Target(our image marker)mr7 refers to a meeting room name.We call this method inside of the constructor.    setMarker() {        ViroARTrackingTargets.createTargets({            \"mr7\": {                source: require('./res/mr7.JPG'),                orientation: \"Up\",                physicalWidth: 1            },        });    }Then we need to define what to render. The root component will be an AR Scene: &lt;ViroARScene onAnchorFound={this._getInfo}              onClick={this._getInfo}&gt;  ... &lt;/ViroARScene&gt;We bind two events on the ViroARScene component, onAnchorFound and onClick. Every time one of these events occurs, we want to fetch the latest meeting room state.onAnchorFound gets called when the Image Marker has been detected.getInfo() {    fetch('https://rooms.meeting/rm7')        .then((response) =&gt; response.json())        .then((res) =&gt; {            const isTrueSet = (res.isAvailable === 'true');            this.setState({                isAvailable: isTrueSet,                nextMeeting: isTrueSet ? `Next meeting: ${res.nextMeeting}h` : `Free @ ${res.nextMeeting}h`,                text: isTrueSet ? `Available` : 'Not Available'            })        })        .catch((error) =&gt; {            console.error(error);        });}Inside the scene, we want to display the meeting room data when the Image Marker is scanned.We need to use the ViroReact ImageMarker component for this.&lt;ViroARImageMarker target={\"mr7\"}&gt;    &lt;ViroFlexView style={this.state.isAvailable ? styles.containerAvail : styles.containerNotAvail}                  width={3}                  height={3}                  position={[0, 0, 1.25]}                  rotation={[-100, 0, 0]}&gt;        &lt;ViroText text={this.state.text}                  width={2}                  height={2}                  style={styles.text}/&gt;        &lt;ViroText text={this.state.nextMeeting}                  position={[0, -1, 0]}                  width={2.5}                  height={2}                  style={styles.nextMeeting}/&gt;    &lt;/ViroFlexView&gt;&lt;/ViroARImageMarker&gt;The ViroARImageMarker component has a target “mr7” assigned.This refers to the ViroARTrackingTarget we defined in the setMarker() method above.When the target is successfully scanned, all the content of the ViroARImageMarker component will be rendered.In our case, two TextViews positioned with a FlexView.We bind the data we fetched from in our getInfo() method to the ViroText and ViroFlexView components.And these are the styles we defined for the ViroText and ViroFlexView.var styles = StyleSheet.create({    text: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    nextMeeting: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    containerAvail: {        flexDirection: 'column',        backgroundColor: \"#E98300\",        padding: .2,    },    containerNotAvail: {        flexDirection: 'column',        backgroundColor: \"#e91530\",        padding: .2,    }});Our final scene'use strict';import React, {Component} from 'react';import {StyleSheet} from 'react-native';import {    ViroARScene,    ViroText,    ViroConstants,    ViroARTrackingTargets,    ViroARImageMarker,    ViroFlexView} from 'react-viro';export default class MarkerScene extends Component {    constructor() {        super();        // Set initial state here        this.state = {            text: \"Initializing AR...\",            nextMeeting: \"\",            isAvailable: true        };        this.setMarker();        this._getInfo = this._getInfo.bind(this);    }    setMarker() {        ViroARTrackingTargets.createTargets({            \"mr7\": {                source: require('./res/mr7.JPG'),                orientation: \"Up\",                physicalWidth: 1            },        });    }    render() {        return (            &lt;ViroARScene onAnchorFound={this._getInfo}                         onClick={this._getInfo}&gt;                &lt;ViroARImageMarker target={\"mr7\"}&gt;                    &lt;ViroFlexView style={this.state.isAvailable ? styles.containerAvail : styles.containerNotAvail}                                  width={3}                                  height={3}                                  position={[0, 0, 1.25]}                                  rotation={[-100, 0, 0]}&gt;                        &lt;ViroText text={this.state.text}                                  width={2}                                  height={2}                                  style={styles.text}/&gt;                        &lt;ViroText text={this.state.nextMeeting}                                  position={[0, -1, 0]}                                  width={2.5}                                  height={2}                                  style={styles.nextMeeting}/&gt;                    &lt;/ViroFlexView&gt;                &lt;/ViroARImageMarker&gt;            &lt;/ViroARScene&gt;        );    }    _getInfo() {        fetch('https://rooms.meeting/rm7')            .then((response) =&gt; response.json())            .then((res) =&gt; {                const isTrueSet = (res.isAvailable === 'true');                this.setState({                    isAvailable: isTrueSet,                    nextMeeting: isTrueSet ? `Next meeting: ${res.nextMeeting}h` : `Free @ ${res.nextMeeting}h`,                    text: isTrueSet ? `Available` : 'Not Available'                })            })            .catch((error) =&gt; {                console.error(error);            });    }}var styles = StyleSheet.create({    text: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    nextMeeting: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    containerAvail: {        flexDirection: 'column',        backgroundColor: \"#E98300\",        padding: .2,    },    containerNotAvail: {        flexDirection: 'column',        backgroundColor: \"#e91530\",        padding: .2,    }});module.exports = MarkerScene;Step 3: Load your marker scene on application startupNow that we have our scene, we can load it on start-up.In the root folder you can find the app.js file.Here we can define which scene to load when starting up the application.Assign your API key from ViroMedia.const sharedProps = {    apiKey: \"6E2805CC-xxxx-4Ex0-8xx0-02xxxxxxx\",};Import your marker scene.const MarkerScene = require('./js/MarkerScene');Render your AR Scene.Final result:import React, {Component} from 'react';import {ViroARSceneNavigator} from 'react-viro';const sharedProps = {    apiKey: \"6E2805CC-xxxx-4Ex0-8xx0-02xxxxxxx\",};const MarkerScene = require('./js/MarkerScene');export default class ViroSample extends Component {    constructor() {        super();        this.state = {            sharedProps: sharedProps        };        this._getARNavigator = this._getARNavigator.bind(this);    }    render() {        return this._getARNavigator();    }    _getARNavigator() {        return (            &lt;ViroARSceneNavigator {...this.state.sharedProps}                                  initialScene=/&gt;        );    }}module.exports = ViroSample;Demo        Lessons learned and conclusionWithout prior knowledge it was a bit challenging for us to get our development environment set up correctly.We had a lot of issues with debugging and cached builds. When we had issues, it was hard to tell if the problem was with React Native or ViroReact.Debugging was a big challenge for us and the react native development tools don’t seem to work well with ViroReact.The documentation is quite expansive but it was not always up-to-date.But aside from that, once we were aware of which parts of the dev tools that worked and which ones that didn’t, we were able to quickly build an AR application.Extra resources  An introduction to virtual and alternate reality  Documentation  Sample applications  Beginner’s guide to React"
      },
    
      "conference-2018-06-27-craft-conf-html": {
        "title": "Craft Conf 2018",
        "url": "/conference/2018/06/27/Craft-Conf.html",
        "image": "/img/craft-conf-2018/Craft-Conf-2018.png",
        "date": "27 Jun 2018",
        "category": "post, blog post, blog",
        "content": "  Craft Conf is a two day conference in Budapest, aimed at talks surrounding the ‘Software as a craftsmanship’ idea.JWorks was present this year on the 10th and 11th of May and we would love to give you an overview of some of the talks we attended.Table of Contents  Power Games for High-performance Team Culture, Psychological Safety, and EI - Richard Kasperowski  Perceived performance: The only kind that really matters - Eli Fitch  SWARMing: Scaling Without A Religious Methodology - Dan North  Seven (plus/minus two) ways your brain screws you up - Joseph Pelrine &amp; Jasmine Zahno  Designing a high-performing team - Alison Coward  Estimates or NoEstimates: Let’s explore the possibilities - Woody ZuillPower Games for High-performance Team Culture, Psychological Safety, and EI - Richard KasperowskiRichard Kasperowski is a speaker, trainer, coach and author focused on high performance teams.Richard is the author of The Core Protocols: A Guide to Greatness.He leads clients in building great teams that get great results using the Core Protocols, Agile, and Open Space Technology.During this talk, Richard provides an overview of some of the conditions that need to be met for the team to become a high performance team.In order for this to happen, it’s very important that companies realize that they need to focus on getting the right people together instead of focusing on achievements.Hierarchy and power distanceA company needs to achieve the best possible team culture.But what does this mean?What is culture?  The collective programming of the mind which distinguishes the members of one group or category of people from another. - Geert HofstedeRichard talks about six different culture dimensions, researched by Geert Hofstede:  Power distance:Expresses the degree to which the less powerful members of a society accept and expect that power is distributed unequally.  Individualism versus collectivism:Individualism can be defined as a preference for a loosely-knit social framework in which individuals are expected to take care of only themselves and their immediate families.Collectivism, however, represents a preference for a tightly-knit social framework in which individuals can expect their relatives or members of a particular group to look after them in exchange for loyalty.  Masculinity versus femininity:The masculinity side of this dimension represents a preference in society for achievement, heroism, assertiveness and material rewards for success.This society is more competitive.Its opposite, femininity, stands for a preference for cooperation, modesty, caring for the weak and quality of life.This society is more consensus-oriented.  Uncertainty avoidance:This dimension expresses the degree to which the members of a society feel uncomfortable with uncertainty and ambiguity.The big issue here is how a society deals with the fact that the future can never be known: should we try to control the future or just let it happen?  Long term orientation:Every society has to maintain some links with its own past while dealing with the challenges of the present and the future.Societies who score low on this dimension prefer to maintain time-honoured traditions while viewing change within the society with suspicion.Those with a culture which scores high, on the other hand, take a more pragmatic approach: they encourage efforts in modern education as a way to prepare for the future.  Indulgence versus restraint:Indulgence stands for a society that allows enjoyment of basic and natural human drives related to enjoying life and having fun.Restraint stands for a society that suppresses enjoyment of needs and regulates it by means of strict social norms.The talk further focuses specifically on the power distance dimension.Power distancePower distance entails hierarchy, the gaps between people in the company.A lot of power distance means a lot of hierarchy.Next, we did some activities that let us experience hierarchy.The first one was with a partner of our choice where one of us was the ‘leader’ and the other was the mirror.Naturally, the mirror had to imitate everything the leader did.For the next exercise one of the two persons was again the leader, but now the other person had to keep his or her nose at a distance of 10 centimeters from the palm of the hand of the leader at all times.The last exercise was with a few volunteers from the audience where one person played the CEO, a few people played the Senior Vice Presidents and some people played the Vice Presidents.The CEO stood in the middle of the room and directed the SVPs, the SVPs directed the VPs.The people who were being directed were again said to keep their noses at a distance of 10 centimeters from the palm of the hand of the person that directed them.These were very interesting activities that showed that having too much hierarchy and people who direct other people can lead to chaos when for example the leaders moved too fast for the others to be able to follow the directions.Directions are much easier to handle when there are less people involved and more trust is created.What do we need to build a high performance team?At the core, there are six building blocks for high performance teams:  Positive bias: Non-negativity, accept a positive mindset  Freedom: Every team member can choose and make decisions by themselves  Self awareness: Listen to yourself, use check-ins to discover, articulate and achieve what you want  Connection: Listen to others, connect great people into a great team and support each other towards a common goal  Productivity: Focus on results  Error handling: Ensure we are maintaining the positive bias, freedom, self-awareness, connection and productivityHierarchy and power culture erode high performance so when setting up and trying to maintain a high performance team, these building blocks are essential.To achieve high performance, increase the team’s emotional intelligence and psychological safety.This will decrease power distance and hierarchy.The Core Protocols are a way to reach this goal.More info on the Core Protocols can be found here.Perceived performance: The only kind that really matters - Eli FitchEli Fitch is a frontend developer with a passion for web performance, animation and all things 3D.When talking about performance, we need to account for both objective time and subjective time.Usually, we tend to optimize for objective time by using tools like lazy loading.But how feasible is this?Even a 20% increase in objective speed is unnoticeable to the user.So we have to aim for an increase of 30% or even more.Of course this is not easy at all, especially when taking into account that when working with multiple teams (possibly over different time zones), all the teams have to align on this.So how do we get the user to perceive an increase of performance?We focus on the subjective time!Active vs passive phase:What does a passive phase entail?Our passive phase kicks in when we are waiting for something to happen, say for our water to boil.Time spent in passive phases feels ~36% slower!There are two ways to prevent users from getting bored during a passive phase and keeping them interested enough to wait:  Keep users in an active state  Make passive states feel fasterHow do we keep users in an active state?There are three ways to achieve this:  Don’t tell users they’re waiting: Use loaders wisely because they can prematurely put users in a passive state  Respond to users immediately, even if nothing is happening  Keep users in flow with more responsive event listenersTo respond to the users immediately, we can implement an optimistic UI.99% of requests succeed so why not take advantage of this by first updating the UI and only then doing the actual request.We can also do our best to react as soon as the user signals intent.For example, why use the onclick event when the onmousedown event exists as it shows intent much earlier?This will provide you with a nice 100-150 millisecond head start.This is also usable on touch devices.Using :active animations also buy you more time.An animation that lasts ~200 milliseconds will provide you with 50 milliseconds extra time keeping the user in an active state.Onto topic two: how to ‘unsuck’ passive states.A wait of 1-4 seconds is long enough to lose people’s interest.There are three ways of preventive loss of interest:  Use the right loading animation  Adapt the loading scheme to the user  Distract with shiny objects!Uncertain waits feel longer so make sure to use progress bars and loading animations when appropriate.For example: bands with accelerating bars feel 12% faster!What about spinners?  “Meh” - Eli Fitch  Less than ~1 second wait: Don’t indicate loading  More than ~2 second wait: Use progress bars!Spinners are only useful between a 1 second and 2 second wait.Don’t forget that most progress bars are fake!This is due to the connection differences between the user’s connection and the backend.However, we can use adaptive loading.Measure the actual requests done!You do need baseline times to know how long to expect it to run.Of course this needs to be normalized for the resource or payload that needs to load.Again, adapt the loading scheme to the user that is requesting data.For example, you can check the user’s connection to give him a personally optimized experience.We can also learn a lot from game developers.Remember FIFA who made you play a mini football game while loading the game in the background?Predictive preloading:What if we could predict the user’s intent?One easy to setup option is to start loading the app and data in the background when a user has just started entering credentials in a login form.This quick win will give you a 4 second head start!Another option is to exploit behavioural quirks:  People tend to watch hover animations: Fancy hovers buy you ± 600 millisecond extra time  People slow down when approaching the target: Load on mouse deceleration!When combining above two techniques (hover animation + futurelink) we can get a ± 1.5 second head start.But: Use predictive preloading wisely!You will get it wrong some of the time. Do some dry runs first and try to mitigate risk by using metrics to improve.A quick summary  Perceived performance is low hanging fruit since you can provide the user with immediate, accurate and live feedback  Tailor your loading solution to individual users  React as soon as users signal intent  Introduce predictive loading bit by bit, implement it at the places in your app where it has the most impact  At the end of the day, what matters is how it feels.Eli also covers this talk on his website.SWARMing: Scaling Without A Religious Methodology - Dan NorthDan North has been coaching, coding and consulting for over 25 years, with a focus on applying systems thinking and simple technology to solve complex business problems.Business stakeholders, developers, infrastructure, the project management office and change groups don’t understand each other. What are they doing wrong?They are aiming at the wrong target!Wrong target: Cost accounting!Large businesses tend to look only at costs and profits since they are too big to be able to keep an eye on everything in the organization.Local performance targets are viewed as extremely valuable, such as reducing head count or sticking to the budget.Right target: Throughput accounting!The whole business creates value.People in the organization are either directly creating value, or they are at least enabling others to do so.The business tries to identify and resolve bottlenecks.They care about lead time and throughput: How quickly from identifying a need can they meet that need and how much value can be pushed through the organization?How can they reach the right target?  “Agile will save us!” - EveryoneThe two people still waiting for Godot might help you with this one.Agile is no holy grail and people desperately holding on to the agile transformation will realize this soon enough.Q: How do we always end up at, “Whatever we did was okay, but somehow it stopped working”?A: Some things are just inevitable:  Degradation: things start to wear out because of time  Dysfunction: once things wear out, they stop working  Expiry: after they stop working, things start to decay and fall awayHowever, these also stimulate positive change: degradation stimulates the idea of maintaining things and transforming them.Dysfunction drives innovation and challenging things and expiry stimulates creating and starting over.Why does this happen?  Change drives the need to adapt. If we’re not adapting, things are going to fail.  Interdependency drives the need to collaborate.  Imperfection drives the need to iterate.Surprise, surprise: adaption, collaboration and iteration are the drivers of Agile and Lean methods!What are our options?SWARM!ScalingWithoutAReligiousMethodology  religion (n): The structures, constraints and rituals we humans accrete around a kernel of faith because we crave answers and we fear uncertainty. - Dan NorthThe table stakes are:  Education: we need to learn some new tricks.  Practice: it’s not enough to learn them, we also need to implement them (possibly with a few failures in the process and trying again and again).  Time: minimum 3-5 years to have an impact.  Investment: time costs money.  Influence: we need to reach up and down in the organization. Everybody needs to participate.  Communication: needs to be in the plan from the get-go or it will not work.  External help: both Amazon Prime and Netflix run on AWS. They don’t block each other, they learn from each other.  Leadership: needs to be consistent, they need to be invested and resilient.If you don’t have these factors in place, SWARM won’t work.These are based on simple principles:  People are basically good. “Everyone is trying to help”. Assume this is always true.  Sustainable flow of value is the goal. We need to learn new metrics and techniques.  Theory of Constraints: only tackle one constraint at a time.Underlying principles:      Visualize -&gt; stabilize -&gt; optimize!You cannot change a system if you don’t know what that system looks like, so visualize!Use event storming, value stream mapping or whatever you need.Stabilizing means that even if the system is horribly bad, you want it to be consistently horribly bad because only then can you observe what impact your changes actually have.        Start small, get data.    “If we have data, let’s look at data. If all we have are opinions, let’s go with mine!” - Jim Barksdale  Learn from mistakes, iterate.Summing it up  Don’t be fooled! It’s easy to believe that this time will be different.  You can’t defeat the universe.Mastery is understanding how to work with the grain. Don’t try to fight all the universal, inevitable things.That means adapting, iterating and combining techniques for your context and the changes around you.  There is no magic formula! But there definitely is hope.  This all takes education, time, practice and other things like investment and leadership.Seven (plus/minus two) ways your brain screws you up - Joseph Pelrine &amp; Jasmine ZahnoJoseph Pelrine is a senior certified Scrum Master Practitioner and Trainer as well as one of Europe’s leading experts on eXtreme Programming.Jasmine Zahno is an agile coach who is passionate about the people side of product development. Her master’s degree in organisational psychology uniquely qualifies her to deal with the human issues that arise when the agile paradigm collides with traditional organisational structures.Joseph and Jasmine talk about a few subjects that they found to be powerful in understanding the complexity of ourselves and software craftmanship.The first of these subjects is that men find women who wear red to be more attractive.Men actually don’t realize this.A woman in red also triggers an exceptional reaction in other women.These women tend to react more aggressive towards women who wear red.Also, men who carry a guitar are found more attractive than men who carry a tennis racket.The next part of the talk will cover some subjects about our psychology that are relevant to use in our field, but also just fun and interesting topics.FACT: We use more than 10% of our brain. Even when we sleep, we use more.Willpower?Willpower is the ability to resist short-term temptation in order to meet long-term goals.Each day, we make 227 choices.These are all chances to follow your long-term goals.This has absolutely nothing to do with your intelligence and everything with willpower.Willpower determines academic successes over intelligence.Willpower deteriorates!After multiple choices that required willpower, your willpower will start deteriorating and you will start making worse decisions.This is actually something that supermarkets use.The fruit and vegetables aisles are always presented to you first whilst you pass the sugary items right before checkout, when your willpower is at its lowest.How to boost your and your team’s willpower?  Establish motivation - for example, align the action items in a retrospective to the team’s motivations  Focus on one goal at a time  Be authentic  Express your emotions - hiding your emotions deteriorates your willpower  Physical exercise - it will actually increase your willpower!  Eat regularly - for example, foresee fruit instead of Snickers to keep up the willpower since some will try very hard to not eat that Snickers bar  Mindfulness practicesRelative estimating =/= estimating!Social compliance is very important to keep in mind during planning poker.Once the numbers are out there, it will influence the people who have not yet voted.See the example of the five subjects to say which drawn lines are the same length.Among these five people there is one actual test subject while the other four are deliberately giving the wrong answer.The test subject will actually adjust his answer to match the others’ answers even though it is clearly the wrong answer.There are multiple factors that influence our estimations.Let’s discuss a few.Seven plus/minus twoThere’s actually a limit on our capacity to process information.This means that the number of objects an average human can hold in working memory is 7 ± 2.This is frequently referred to as Miller’s law.It has nothing at all to do with memory.It has everything to do with the way we make decisions.Linguistic primingThe way the Product Owner formulates the story has a big impact on how it is estimated.Let’s say for example that two cars had an accident.The following five words that were used in the experiment gave a different result:  Collided  Bumped  Contacted  Hit  SmashedThe question here is which word would give the highest estimate of speed?The answer is the following, where 1 represents the highest estimate and 5 the lowest:  Collided - 2  Bumped - 3  Contacted - 5  Hit - 4  Smashed - 1We are not as good with numbers as we think we are  We tend to ignore the base rate:If, for example, the yellow taxis take up 80% of all the taxis in Budapest and the red ones take up 20%, when we see 10 yellow taxis pass by we will assume that the next one will be red even though the actual chance is still 20%.  The law of small numbers:We tend to believe small numbers more than large numbers because it’s easier for us to process  We overlook the statistics when a story is involved because we tend to be more emotional than analyticalJoseph and Jasmine also refer to the Monty Hall problem:  Suppose you’re on a game show, and you’re given the choice of three doors:Behind one door is a car; behind the others, goats.You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat.He then says to you, “Do you want to pick door No. 2?”Is it to your advantage to switch your choice?The answer here is actually ‘Yes!’, however counter intuitive that may be.We are not good psychologistsThe Dunning-Kruger effect: The less intelligent a student was, the smarter they thought they actually were.Reduced intelligence leads to a reduction in the ability to self-reflect on their own intelligence.Working in teamsWe have the tendency to overemphasize personal characteristics and ignore situational factors when judging others’ behaviour.  When we are late, it’s because we have a good excuse  When a team member is late, he was probably just lazyIKEA effectWe have the tendency to overvalue the things we build ourselves.One example here is using pancake mix.These mixes didn’t sell properly until people were actually required to add their own set of eggs to the mix.To finish, Joseph and Jasmine showed us and discussed some more interesting research papers about the human psychology.Their talk is available on the Craft Conf website.Designing a high-performing team - Alison CowardAlison Coward is the founder of Bracket and author of “A Pocket Guide to Effective Workshops”. She is a strategist, trainer and workshop facilitator. With over 15 years of experience of working in, leading and facilitating creative teams, Alison is passionate about finding the perfect balance between creativity and productivity.Team work today is cross-functional and self-organizing.AirBnB for example has core teams and expand the team with the necessary expertise from within the organization when necessary.A team is made up of multiple individuals which creates a new team culture when putting these teams together.The team’s dynamics also tend to change on different projects.So we cannot apply a preset way of working to a new team or a familiar team working on a new project.Three principles for creating High Performance Teams  Can your team learn to work better together?With a fixed mindset you believe that talent is fixed.However, with a growth mindset, you believe that anyone can improve with practice and persistence.See the value of continuous learning.These teams were more successful in the past and tend to perform much better.They challenge each other and learn together.Don’t forget that this can actually be developed.Each team has the potential to improve.   What new ways of working can you create together?Apply a design approach and a design mindset. Design a way of working that works for that team!   How will you and your team start to work differently?Behaviour change and building new team habits. What team habits, what rituals can we create to help our teams work better together?Two factors for designing effective teamsCommunication &amp; trust.  How you work together is more important than what you’re working on and who you are working with.MIT Human Dynamics LabThe research conducted by the lab concluded the following:The most effective teams  Communicate frequently  Talk and listen in equal measure  Engage in frequent informal communication  The conversations are dynamicGoogle’s project AristotleA few years ago, Google was very interested in finding out what the factors were that made up the most effective teams.They concluded the following:  The people could see the impact of the work they were doing  Meaning of work, they could see why they were doing the work  Structure and clarity, everybody knew who was doing what and what they were working on  Dependability, they knew they could depend on each other  Psychological safety, the individuals in the team were able to make mistakes and take risks in front of each otherIt had nothing to do with IQ and individual talents of the people in the team and everything to do with how they worked together.Self-awareness95% of us think we are self-aware, while actually only 10-15% is!Self-awareness exercise:  What time do you naturally wake up?  When are your most productive hours?  When do you get your best ideas?  What does your ideal work day look like?We can use these answers to set up new ways of working within the team.We can do this during the project kickoff:  Share team expertise, share what your role is and what you can bring to the table  Clarify the roles, everyone needs to know who can do what work  Talk about how you will work together, how you will meet, when you will meet, how to communicate, etc.Organize better meetings and workshopsMake your meetings count!  What is the purpose of the meeting?  What is the best format of the meeting? How long do we need? Do we need to split up the meeting?  Set meeting rhythmsAmazon’s two pizza teams:If a team is too large to be fed by two pizza’s, then the team is just too large.Google Ventures Anxiety PartiesEvery three or four months they would get together with a list of all the things that were bothering them about their performance and would share that with their team.An example of this might be that they weren’t sure if they were spending too much time going to conferences or if they’re working too slow.They would share these concerns during the anxiety party and the team members could respond whether they felt this actually was an issue or not to them.Someone could then find out that what they saw as an issue wasn’t being considered an issue at all by the rest of the team, or perhaps even vice versa.These conversations build trust and better connections within the team.Take inspiration from workshopsGreat workshops have the following in common:  Collaboration  Creativity  Equal contribution  Good content  Clarity (on what needs to be done)  MotivationThe next paperclip exercise shows you how to make your meetings more effective:The first question Alison asked us was, ‘Come up with as many ideas as possible on what you can do with a paperclip.’This question was followed with, ‘Which idea would make a good business proposition?’Two different activities came up during this exercise:  Divergent thinking: Trying to come up with as many ideas as possible, the focus was on the quantity.  Convergent thinking: Applying criteria to your ideas and being critical.It’s very important to keep these two activities separate during your meetings.Our goal is to have productive conflict and constructive discussions.Watch out for group thinking by making sure everybody gets heard, introverts and extroverts alike.Equal contributions are very important!Don’t forget about check-in rounds! Use the first five minutes of a meeting to ask a personal question to encourage equal contributions and build trust in the team.Team habitsHow do you change behaviour?  Start small!One step at a time works best instead of a big bang!  “Implementation intention”: Make a plan!When you have these team habits in place, it still needs a constant review to keep improving.We need the following loop:  Design -&gt; Test -&gt; Iterate -&gt; RepeatStarting a new High Performance Team  Determine how you meet  Determine how you share ideas and how you can learn from each other  Social times, find ways to get to know each other  Alone time, don’t only think about collaborationSummary  Team work is changing  Having a growth mindset, design approach and behaviour change  Make your meetings count  Build a ‘workshop culture’  Create better team habitsMore information on how to run effective workshops can be found here.Estimates or NoEstimates: Let’s explore the possibilities - Woody ZuillWoody Zuill has been programming for 30+ years and works as an Agile Coach and Application Development Manager. He and his team are the originators of the Mob Programming approach to teamwork in software development and he is considered one of the founders of the “#NoEstimates” discussion.The first part of Woody’s talk is about his own personal experiences.He was involved in a project that included over 200 developers.He concluded that the same ‘lesson learned’ kept popping up after each iteration: ‘Our estimates need to be better!’It was always the same lesson learned, there was never any improvement. Woody calls this the ‘Cycle of Continuous No-Improvement’.How does one get out of this cycle? Is it safe to question the status quo?There is a bigger idea behind these questions:  “The object isn’t to make art, it’s to be in that wonderful state which makes art inevitable.” - Robert HenriIf you set up the environment for good things, then good things will happen.Woody’s suggestion was very simple:How about working with no estimates?The tweet that started it all in 2012:What is an estimate?Working definition: An estimate is a guess of the amount of time (usually work time) to create a project, a feature or some bit of work in developing software.We use estimates to help us make decisions and spark a conversation (to ultimately make a decision).Afraid of changeWhy is it that we want control and certainty over time, cost and schedule?We don’t need help to make decisions, we need help to make good decisions.Are estimates then really the only way to make them?Are we even able to draw the right conclusions?Perhaps ‘on time’ and ‘on budget’ are actually not a good measure of the results of our decision if we had to cut in our feature to be able to deliver?But why hold on to estimates?  “Fear of losing control is a big barrier for change.” - Bjarte BogsnesWe tend to hold on to what we know.A few quotes in favor of breaking the cycle:  “Alternative to estimates: do the most important thing until either it ships or it is no longer the most important thing.” - Kent Beck  “As teams progress they first struggle with estimation, then can get quite good at it, and then reach a point where they often don’t need it.” - Martin FowlerWhen starting a project, requirements are handed over to the development teams in an orderly fashion, very neatly organized and containing everything that needs to happen.Or so the stakeholders like to think.The project itself, however, usually ends up being a lot more disorganized with features being added, removed or changed with no way to tell early on what the end result will look like.Then why try to get estimates for the project if most of the requirements tend to change anyway?  “It’s in the doing of the work that we discover the work that we must do.” - Woody ZuillRather than size and smallness, look for the following qualities in a story:  Potentially valuable  Understandable  Cohesive - does it belong together  Distinct - is it clearly separate from other stories-&gt; These don’t require estimates! When these qualities are found, we have something we can work on.The Twelve Calculations example  80% of the use of the app comes from 20% of the features  80% of the use of a feature comes from 20% of its implementationThis means that with only 4% of the implementation effort (20% of 20%), we can cover the 20% of the features being actively used.Let’s try to have many, small, inexpensive attempts at value.Let’s do, discover, validate and steer.This is what being agile is all about!What do you think would have more payoff?Turning up the goods on getting our estimates better?Or turning up the goods on our ability to rapidly deliver potentially useful software?Let’s learn to control our urge to control things.Let’s quit worrying about whether we will get done in three months for now.Let’s get good at being done everyday.On Woody’s website, you can find more info on his take on estimates.Interested in more?As we were not able to attend all the talks at CraftConf, we only covered the ones we’ve attended in the blogpost above! Hope you find this blog post as interesting as we did the conference!If you’re interested in more details; all the talks were recorded and can be found on the CraftConf website.Thanks for reading, everyone!"
      },
    
      "conference-2018-06-26-spring-io-2018-html": {
        "title": "Spring IO 2018",
        "url": "/conference/2018/06/26/Spring-IO-2018.html",
        "image": "/img/spring-io-2018/spring-io.jpg",
        "date": "26 Jun 2018",
        "category": "post, blog post, blog",
        "content": "Spring IO is back!Marked in red on the calendar of every JWorks consultant: the yearly edition of Spring I/O.This year, we weren’t going to wait for the explicit approval of our manager and we ordered 27 early bird tickets as soon as we could and booked our flights to sunny Barcelona!It promised to be a special edition, since everything was gonna be bigger and better: the venue, the speaker roster, the food, the atmosphere.  The Palau de Congressos de Barcelona is a much bigger venue than the one we’re used to from previous years.This is why the organizer Sergi Almar was able to accomodate 1000 attendees this year (twice as much as the year before!), which shows how much interest there is in the Spring community.It was a really good location, it has ample space to grow in the coming years and the catering was also of good quality.Next year we’ll most likely get the same venue, but the event will probably overlap with the nearby Barcelona International Motor Show, which takes place every two years.Free test drives during a conference? Yes, please!We’ll talk about some of the presentations this year, but it is definitely not a complete list.There were so many interesting talks, we’re actually going to need quite some time to rewatch them all on Youtube!Let us know if we missed anything by filing an issue or contacting us at our general JWorks email.We will probably still update this blogpost with other talks.Links to videos and resources might still be added as they become available on the Spring I/O Youtube channel.  Michael Plöd: Implementing DDD with the Spring ecosystem  Mark Heckler: Migrating legacy enterprise Java applications to Spring Boot  Andreas Falk: Spring Security 5 Workshop  Juergen Hoeller: Spring Framework 5 - Hidden Gems  Ray Tsang: Google Cloud Native with Spring Boot  Simon Baslé: Flight of the Flux  Nakul Mishra: Spring Kafka - One more arsenal in a distributed toolbox  Oliver Gierke: Breaking Down Monoliths Into System of Systems  Tommy Ludwig: Observability with Spring based distributed systems  James Weaver: Machine Learning exposed: The fundamentals  Jeroen Sterken &amp; Kristof Van Sever: Testing every level of Spring microservices application Day 1: Talks &amp; WorkshopsImplementing DDD with the Spring ecosystem by Michael PlödAfter the keynote session, one of the first talks was given by Michael Plöd.He talked about implementing Domain-Driven Design (DDD) using the Spring ecosystem, leveraging various Spring technologies such as Spring Boot, Spring Data and Spring Cloud.Michael attributed the inspiration for and ideas around DDD to the following two books:  Domain-Driven Design: Tackling Complexity in the Heart of Software, by Eric Evans.  Implementing Domain-Driven Design, by Vaughn Vernon.Domain-Driven Design is currently a very popular way of implementing and looking at microservices.However, he immediately made an important disclaimer:  Everyone should be aware that DDD is not a silver bullet to be used in all projectsOne should not force DDD on problems that aren’t suited for it.Another important thing to remember is to model your microservices along business capabilities.If your microservices are highly coupled on a business level, all that fancy technology in Spring Boot won’t help you.We will use Strategic Design to find a solution that takes into account business capabilities.Bounded ContextEvery sophisticated business (sub-) domain consists of a bunch of Bounded Contexts.We can, for example, create linguistic boundaries using Bounded Contexts if the solution has two types of “accounts”: a BankAccount and a UserAccount.Each Bounded Context contains a domain model and is also a boundary for the meaning of a given model. We don’t nest Bounded Contexts.Inside of a Bounded Context, it’s important to not repeat yourself.On the other hand, between several Bounded Contexts, repeating yourself is allowed for the sake of decoupling.Tactical DesignSystems, and this applies to both monolithic and microservice architectures, should be evolvable.DDD offers a set of patterns, which are the internal building blocks of the Tactical Design part of DDD, that helps us in this regard.Michael now talked us through each of these concepts:  Aggregates:          Entities      Value Objects        Factories  Repositories  ServicesEntitiesEntities represent the core business objects (not data objects) of a Bounded Context’s model.Each of these has a constant identity which should not be your primary database key but rather a business key.Each Entity also has its own lifecycle.Value ObjectsValue Objects derive their identity from a combination of various attributes.As an example, Michael brought up the representer object he was holding: it costs 80 euros so this representer object could be identified by the value of 80 and the currency Euros. We do not care about which ‘80 euros’.Value Objects do not have their own lifecycle: they inherit it from Entities that are referencing them.It’s also important to note that for example a Customer can be an Entity in one Boundary Context but be a Value Object in a totally different Boundary Context.Take note that your DDD Entity is not your JPA Entity.Because the JPA Entity is a data entity while the DDD Entity is a business entity. Don’t mix these types.AggregatesAggregates group Entities and Value Objects together.The Root Entity is the entry point in terms of access to the object graph and for the lifecycle.For example, you aren’t allowed to enter a loan application form through the loan for instance: you would also have to go through the loan application form.Best Practices for architecting AggregatesSmallPrefer small Aggregates that usually only contain an Entity and some Value Objects. Don’t build big reference graphs between Aggregates.Reference by identityDo not implement direct references to other Root Entities. Prefer referencing to identity Value Objects.One transaction per AggregateAggregates should be updated in separate transactions which leads to eventual consistency.Consistency BoundariesTake a look which parts of your model must be updated in an atomically consistent manner.Best practices for implementing AggregatesCode can be found on the author’s DDD-with-Spring GitHub project where he implemented a credit loan application consisting of three Spring Boot applications.VisibilityDon’t just make everything private and expose everything with public getters and setters.This is, in Michael’s words, the “shortcut from hell” because you aren’t doing information hiding and are exposing everything to the outside world.ReferencesHow do we hook these up?  Referencing them from one Value Object to the other.  Create intermediary Value Objects to bind them together.Michael prefers Aggregates that do not reference themselves.They are hooked together with a few shared Value Objects which leads to more decoupling.There are 4 Aggregates in the application and we add a Value Object, personID, to hook AgencyResult and Applicant together.The ApplicationNumber object brings Applicant, Financial Situation and ScoringResult together.Keep your Aggregates Spring free.Aggregates should be plain old Java.PackagesWhen working with Aggregates, place each Aggregate in its own package and work with package level visibility in terms of information hiding.Creation of Aggregates: there are two options  Use the Root Entity directly.  Explicitly create an aggregate concept around your Entities and Value Objects.Make up an educated decision of your own.Builder pattern.The Builder pattern works very well with Aggregates as a substitute to the DDD factory. All Aggregates have Builders in this author’s project.Use an annotation @Aggregate and @AggregateBuilder.Why?To have a code review system in place that checks whether Aggregates are publicly visible and other non-Aggregate classes are packaged protected.Michael recommends ArchUnit, a unit testing tool for software architectures to verify visibility of classes and other architectural rules.Application ServicesThe ScoringApplicationService class holds a service that orchestrates between a lot of Aggregates.RepositoriesIn Spring Data, one uses Spring Data JPA repositories with JPA Entities. But remember these JPA entities shouldn’t be your DDD Entities.ArchitecturesThe hexagonal onion architecture is not your only option and is not suitable for everything.CRUDIf you use a CRUD architecture, Spring Data REST or a context that doesn’t run on business Entities or Aggregates may be suitable.Query Driven contextsAll the logic resides within queries.Domain EventsFor communication between Bounded Contexts there are two possible differentiations:  Orchestration.  Choreography.Orchestration is about synchronous calls going somewhere.Choreography is about events: domain events, event sourcing and event storming.Choreography turns around the call flow so for example: the credit application submits a Credit Application Submitted Event and the scoring component reacts on that Event.You model the information about your activity as a flow of discrete events.Options for Event PayloadFull Payload.Put everything we filled out in the credit application in there and work with it.REST URL.Use RESTful URLs to REST resources for the event; not the Spring Data REST repository.  Empty.  Mix.InfrastructureApache Kafka or message brokers are not the only options for infrastructure. You can also work with:  Brokers (for example Kafka or RabbitMQ): use Spring Cloud Stream.  HTTP Feeds (for example Atom): use Spring MVC with Quartz and Rome libraries.  Internal Event Bus: use Spring Application Events. Eventing within same JVM, so not using an external system.The credit application offers a HTTP feed using Atom that provides new credit agency ratings.Feed polling happens by a combination of REST with Atom: using Spring MVC and the Rome library (to create Atom feeds).At the end of the talk, Michael referenced the ddd-by-examples GitHub project as a great resource.Michael is currently writing a book, Hands-on Domain-Driven Design by example, for which you can get notified upon release by signing up on Leanpub.The slides of this talk can be found on Speakerdeck.Migrating legacy enterprise Java applications to Spring Boot by Mark HecklerMark explained how easy it can be to migrate an existing legacy Enterprise Java application to a modern, state-of-the-art Spring Boot app.Many people think that migrating these kinds of applications is impossible or very hard without rewriting the whole thing, but Mark gave us some very good pointers on how to do it quickly and efficiently:  Generate a new skeleton project from start.spring.io  Use schema.sql and data.sql data sheets to migrate and test your database  Use Kotlin to vastly simplify your code by using data classes to simplify access to members and constructors, and by moving the constructor definition in the same line as the class definition  Using Spring Data, no more need to use PersistenceContext or EntityManager  Using Spring MVC with @RestController, no more need to declare @Produces or @ConsumesBenefits  Less code: your code vastly diminishes using Kotlin and data classes  The Spring Boot + Kotlin combination greatly reduces the amount of boilerplate code  Business logic and Service Layer of the old application remains the same and is better encapsulated  Code becomes easier to maintain, easier to test  Spring Boot offers more and better deployment optionsJWorks consultants have done these kinds of migrations at multiple clients, with great success rates.Non-technical people like functional analysts, product owners and business experts are continuously amazed at the speed with which we are able to do this.Technical people that have been doing JEE development for years ask us how they can learn Spring Boot.Spring Security 5 Workshop by Andreas FalkThe target of this workshop was to learn how to make an initially unsecured (reactive) web application more and more secure step-by-step.It was a very well prepared workshop and I really enjoyed the interactivity with Andreas, he answered questions on the fly and helped us understand some of the finer details and changes in Spring Security 5, especially when using it with Spring Boot.I don’t want to diminish the excellent Spring Security workshop from Andreas Falk by copying anything from him.He deserves all the credit for his amazing work so I’m just gonna link to it here:https://andifalk.github.io/spring-security-5-workshop/Thank you Andreas for this great resource!Spring Framework 5 - Hidden Gems by Juergen HoellerSince almost every feature was backported to 4.3, most of them are already known to the general public.Though there are 7 areas of refinement within 5.0 that aren’t widely known to the public.Commons Logging BridgeSo the Spring team came up with a new dependency called spring-jcl which is actually a reimplementation of a logging bridge.It is a required dependency and is here to help streamline the logging functionality.The main difference with this way of working is that you don’t need to go through a dependency hell where you would manually add exclusions to ignore certain logging dependencies.Just add the logging library to your classpath and everything will switch to the logging implementation of your choice.It now has first class support for Log4J 2 (version 1 has reached its end of life), SLF4J and JUL.Build-Time Components IndexerThe file system traversal for classpath scanning of all packages within the specified base packages using either &lt;context:component-scan&gt; or @ComponentScan might be slow on startup.This is especially true if your application is started for a small period of time or where I/O is very expensive.Think short-running batch processes and functions, or applications being started and stopped on Google App Engine every 2 minutes.The common solution was to narrow your base packages, or even to fully enumerate your component classes so you would skip scanning all together.Starting with 5.0 there is a new build-time annotation processor that will generate a META-INF/spring.components file per JAR containing all the classes which in turn will be used automatically at runtime for compatible component-scan declarations.NullabilityThe new version contains comprehensive nullability declarations across the codebase.Fields, method parameters and method return values are still by default non-null, but now there are individual @Nullable declarations for actually nullable return values for example.For Java this means that we have nullability validation in IntelliJ IDEA and Eclipse.This allows the Spring Team to find subtle bugs or gaps within the framework’s codebase.It will also allow us, as developers, to validate our interactions with the Spring APIs.When you’re writing code in Kotlin it will give you straightforward assignments to non-null variables because the Kotlin compiler will only allow assignments for APIs with clear nullability.Data Class BindingSpring Data can now work with immutable classes.No need for setters anymore since it can work with named constructor arguments!The property names are matched against the constructor parameter names.You can do this by explicitly using @ConstructorProperties or they are simply inferred from the class bytecode (if you pass -parameters or -debug as compilation argument).This is a perfect match with Kotlin and Lombok data classes where the getter and setters are generated at compile time.Programmatic Lookup via ObjectProviderThe ObjectProvider is a variant of ObjectFactory, which is designed specifically for injection points, allowing for programmatic optionality and lenient not-unique handling.This class had the following original methods: @Nullable getIfAvailable() and @Nullable getIfUnique().With the new version of Spring these methods have been overloaded with java.util.function callbacks which empowers the developer to return a default value instead of returning null.Refined Resource InteractionSpring’s Resource abstraction in core.io has been overhauled to expose the NIO.2 API at application level, eg. Resource.getReadableChannel() or WritableResource.getWritableChannel().They are also using the NIO.2 API internally wherever possible, eg. FileSystemResource.getInput/OutputStream() or FileCopyUtils.copy(File, File).Asynchronous ExecutionSpring 5.0 comes with a couple of interface changes that will help you with asynchrous execution:  The ListenableFuture now has a completable() method which exposes the instance as a JDK CompletableFuture.  The TaskScheduler interface has new methods as an alternative to Date and long arguments: scheduleAtFixedRate(Runnable, Instant, Duration) and scheduleWithFixedDelay(Runnable, Instant, Duration).  The new ScheduledTaskHolder interface for monitoring the current tasks, eg. ScheduledTaskRegistrar.getScheduledTasks() and ScheduledAnnotationBeanPostProcessor.getScheduledTasks().Google Cloud Native with Spring Boot by Ray TsangOn one hand this workshop lowered the entry threshold for newbies.On the other hand it provided insight about what services Google Cloud has to offer.Google Spanner, Pub/Sub messaging system, CloudSQL, Runtime Config.They were all addressed.It’s quite cool to see how the team at Google managed to create decent Spring Boot Starters for all these services.They basically remove all the boilerplate code for you and offer you easy connectivity to all its cloud services.The sensible auto-configuration pre-fills most of the settings required to use:  PubSub as messaging middleware  CloudSQL as a managed relational database  Runtime Config as the backing store for your application configuration  Google Spanner as a horizontally scalable, strongly consistent, relational databaseDuring the workshop, we created a guestbook application which consisted of a frontend and some backend microservices.The workshop builds this up neatly by adding features to the application step by step.Each step introduces you to another Google Cloud service.Those of you who want to make the workshop yourself, check out the link below.  Workshop: http://bit.ly/spring-gcp-lab  Code: https://github.com/saturnism/spring-cloud-gcp-guestbook  Cloud console: https://console.cloud.google.com/Google PubSubWhat stayed with me is Google’s Pub/Sub message-oriented middleware.A publisher that creates the messages sends them to a topic.Consumers can subscribe to this topic to obtain the messages.Publishers and subscribers are decoupled. Neither of them is required to know the other one.Subscribers will either pull messages or get messages pushed from the topic.PubSub messages will be delivered at least once, but can be processed multiple times by different subscribers.Unprocessed PubSub messages are only kept for 7 days.  Ray told us a fun story about how he wanted to really explore the capabilities of PubSub and see how many messages it could handle.They created this website called https://pi.delivery/ which calculates the numbers of pi.It’s really interesting to read how hard they were able to stress PubSub (hint: BILLIONS? TRILLIONS!)Flight of the Flux by Simon BasléIn this talk Simon went deeper into the inner workings of Spring Reactor.The session started off giving a brief recap of reactive programming and reactive streamsbefore delving deeper into the machinery behind Reactor 3.Assembly time vs Execution timeWhen programming with Reactor 3 (and other functional reactive libraries like RxJava)the programming model is quite different compared to the classic imperative style.Basically your whole chain is lazy, you describe a sequence of operations and no actualprocessing happens until someone subscribes.To give an example:Assembly:    this.myFlux = Flux.just(\"foo\").map(String::length);As stated above, all that really happens when this code is called is creating a chain of operators(under the hood this phase is also used for things like operator fusion, see below).Execution:    this.myFlux.subscribe(System.out::println);When the subscribe method is called the actual chain is executed and the length of the string is printed in the console.For those familiar with Java 8 it’s basically the same as Java 8 streams,nothing really happens until a terminal operation is used (collect, reduce, count, …).One of the drawbacks of this is error handling.Since the error doesn’t happen until the subscription, it’s harder to see where the error actually happens.To alleviate this, Reactor provides a feature called assembly tracing which can be enabled with the checkpoint() operator.  Nothing happens until you subscribe.Cold and hot observablesPrevious statement holds for most observables typically encountered in a project.HTTP calls, data lookups, etc. No data is actually being produced until someone subscribes.Sometimes however an observable can be a constantly emitting event stream.When multiple subscribers subscribe on a cold observable, each of these subscriptions will trigger the whole chain from the start.A hot observable is constantly producing data and will only give the elements to the subscriber emitted after subscription time.SchedulingReactor is concurrency agnostic which means it doesn’t impose a concurrency model while it does give the developer the tools to change Reactor’s executor behavior.This is done using the scheduler abstraction: a scheduler defines the execution context, this can be the same thread, another thread or using a threadpool.The special operators publishOn() and subscribeOn() allow you to change the execution context of the current chain.The publishOn() changes the execution context of the downstream operators.For example:flux.op1().op2().publishOn(scheduler1).op3().subscribe((result) -&gt; doSomethingWith(result));op1 and op2 will execute in the original execution context(usually the thread in which the subscribe is called).op3 and the action defined in the subscribe method itself will execute in scheduler1’s execution context.The subscribeOn() changes the execution context of the subscription, meaning of the start of the execution of the chain.For example:flux.op1().op2().subscribeOn(scheduler1).op3().subscribe((result) -&gt; doSomethingWith(result));The whole chain will execute in scheduler1’s execution context.Work stealingAlthough previous topics surface easily and are simpler to demonstrate, this was perhaps the most abstract one of the session along with operator fusion.When using schedulers supporting parallel execution, Reactor uses so called ‘work stealing’ algorithms to balance the load on the different threads.If a thread is idle it can take over execution of tasks that were originally scheduled to be executed by a different thread.Under the hood, this is achieved by using a shared queue for the tasks and a drain loop.Operator fusionOne of the big advantages of having a chain of tasks defined and split up in multiple steps is that it allows the engine to identify possible optimizations in the chain.Since each individual operator also has an overhead (eg. queue for work stealing), it’s sometimes more efficient to combine some operators and execute them as one.For example:map(a).map(b).map(c) =&gt; map(abc)Reactor tries to achieve this by using a negotiation process between the operators.ConclusionThis talk gave us more insight into the more advanced parts of Reactor, arming us with knowledge to tackle potential problems in a reactive environment and helping us understand Reactor’s deeper mechanisms.For more information, Simon uploaded his presentation on Speaker Deck.(Spring) Kafka - One more arsenal in a distributed toolbox by Nakul MishraNakul started by describing Apache Kafka,a very potent messaging system which allows you very easily to act as a throughput between your applications,as long as you stay away from recreating ESB anti-patterns with Kafka.Kafka is more than a messaging queue, combining speed, scalability and stronger ordering guarantees then traditional messaging keys.In order to benefit from this ordering, it is important to choose a correct partition key.Kafka puts more emphasis on smart consumers, meaning a more client centric approach vs the broker centric approach used by Message Oriented Middleware.By designing for retention and scale, Kafka gives consumers (clients) the time to process the messages they want to process whenever they want to.It is also possible to use Kafka as a database, by having it process a stream of data in real-time using KSQL of which the results can very easily be pushed to external systems (HDFS, S3, JDBC).To be scaleable, Kafka is kept simple at its core, all data is stored as a partitioned log.This means that writes are append-only and reads are a single seek-and-scan allowing the underlying filesystem to very easily handle the storing and caching of messages.Also when reading, data is directly copied over from the disk buffer into the network buffer bypassing the JVM, ideally for flooding your network.Spring Kafka integrates Kafka with Spring giving you all the benefits of the Spring ecosystem.It also supports Kafka Streams since a few months.Testing is made easier by providing an @EmbeddedKafka and a TestUtils class:@EmbeddedKafka(partitions = 1,         topics = {                 KafkaStreamsTests.STREAMING_TOPIC1,                 KafkaStreamsTests.STREAMING_TOPIC2 })Spring Kafka also has a starter available for Spring Boot which makes it very easy to get started with Kafka and start playing around.As always, just go to start.spring.io and get the Kafka dependency.The slides of this presentation can be found at slideshare.Breaking down monoliths into system of systems by Oliver GierkeThe goal of this workshop is not to provide a clear architecture of the perfect application, but more to make you think.To let you reflect about your existing applications.There is a shorter version of this talk here while this one consumed a full two hours.This gave us the possibility to have a more in-depth look at the code that Oliver prepared and look into potential problems, which would be much harder were it a one hour session.The workshop basically is a summary of observations about the workings of monoliths and microservices.It all tends to boil down to the correct definition of bounded contexts within applications, how you can divide your application in logical modules and how these can communicate with each other.First we will observe what happens when a monolith is transformed into a microlith AKA a distributed monolith.Subsequently we will improve the design of the monolith with these bounded contexts and end up with a modulith.This modulith is still a single application, a monolith, but with different bounded contexts each having clearly defined borders allowing us to easier divide the work over various teams.From a modulith, one can go to a system of systems, a true microservice architecture.In a system of systems there are two ways you can implement the communication, either via messaging or via REST.The sample code of this workshop can be found on Github.MonolithThe monolith is reasonably ordered and the bounded contexts have been split in various packages.When building your Java application, you should make optimal use of the package options provided by Java as mentioned in this blogpost of 2013.It stated:  Make your code package protected whenever it does not need to be accessed from the outside, a good starting point is to make your repositories no longer public.  Whenever there is leakage over the bounded contexts, for example when LineItems contains Products, try to use IDs and not the actual objects of another bounded context, because whenever you update an object used within another bounded context, you will also leak into that context.It is also noted that badly structured applications tend to be built from the bottom up, from DB to the top.This means that your design is going to be way too data-centric instead of focusing on the real business interactions you are supposed to handle.Try to prevent using methods which update two bounded contexts simultaneously, as these methods have the reflex of drawing more and more code in, and they tend to grow like a cancer, killing your application from the inside out.To summarize:  Move bounded contexts into packages  Inter-context interaction is processed locally and resulting in either success or an exception (method calls in the JVM are very efficient and executed exactly once)  Avoid referencing two domain classes over bounded contexts, it’s convenient but results in problems  When you leak into other bounded contexts, there is a great risk of creating circular dependencies  When there are no clear boundaries, adding a new feature often requires you to touch other parts of the system  A monolith is easy to refactor  By its nature, it has strong consistency but this is also a disadvantage as transactions become more brittle when they fail because of related business functionalityThe monolith example code can be found here.MicrolithCreating a microlith means splitting up your systems into various smaller systems.This doesn’t mean that suddenly all your problems have been solved.If you do not correctly define your bounded contexts in order to minimize your communication, the chance is great that you have made a microlith with the following problems:  You are no longer able to use local transaction consistency  Local method invocation is transformed into RPC-style HTTP calls  You have translated the transactions of your monolith into a distributed system, needing HTTP to update each other  Remote calls are executed while serving user requests and this over multiple services  Running and testing requires the other services to be available  There is a strong focus on API contracts, which tend to be very CRUD-looking with a lack of business abstraction and hypermedia  Detecting breaking API changes is prioritized over making evolvable APIs  One tends to add more technology in order to solve issues: bulkheads, retries, circuit breakers, asynchronous calls, more monitoring systems, etcIt tends to minimize the risks of a rollback, but it does not really solve any issue, it just distributes your problems.  The first rule of distributed systems is: don’t distribute your system until you have an observable reason to.If you did not define your bounded contexts properly, it is very difficult for you to observe how to distribute your system.Example code of the microlith can be found here.ModulithWe will start using events inside our modulith, as well as more domain specific methods, like an add() on an Order.This makes everything more abstract, making your domain objects much more than glorified getters and setters.  We don’t do CQRS or event sourcing, but we just use eventing as a way to signal events over bounded contexts.These events make it relatively easy to split up the work, they can serve as either input or output for different services within the applications.Your units of work will have clear boundaries making testing and design easier.The differences with a monolith are:  Focus of domain logic has moved to aggregates  Integration between bounded contexts is event based  The dependency between bounded contexts is inverted  Side Step: Application Events with Spring DataThis is a very powerful mechanism to publish events in a Spring application.Whenever you need to send data to another bounded context, you trigger events.This has the advantage that your business services no longer needs to know about each other, they just need to trigger an event which gets picked up by the services which are interested in this event.Transactional semantics are still retained because the eventing is synchronous, by default.This also applies for JEE eventing.The @TransactionalEventListener annotation allows you to delay the execution of events,so for example, you can send out an email when an Order has truly been completed.  Side Step: Error ScenariosWhen a synchronous event listener fails, this will be handled by the transaction, so no worries.But when an asynchronous event listener fails, the transaction does not get rolled back and you will need to deal with retries.You can make use of an Event Publication Registry when you use TransactionalEventListeners as these event listeners are decorated with a log before the commit, since the system needs to know the events need to be sent out to.When the event has been processed, the log will be cleared.If it doesn’t get cleared the system can keep retrying, so you don’t lose events.Example code of the modulith can be found here.System of SystemsMessagingWhenever you make use of a message broker, you introduce a potential single point of failure, like with Apache Kafka or RabbitMQ.These brokers know about all the messages of all the systems and decide how long these messages will be retained.Coupling does exist, although not explicit, but the message format will decide which version of a service can process these messages, just as with REST.Especially if you keep your events for a long time, which is possible with Kafka, you might need to think about transforming existing events.But these messaging systems tend to be designed for scale.Pro tip: make use of JsonPath annotations for the message payload in order to make it more robust.Example code of a system of systems with messaging can be found here.REST MessagingIf you use REST you will have to deal with caching, pagination and conditional requests.Messages do not tend to be stored for long periods of time and most communication tends to be synchronous.One does have to pay attention on not to lose events as your application will immediately know if the message was processed correctly, incorrectly or timed out.REST PollingWhen using a polling mechanism, your producers do not send out messages to your consumers, but the consumers will poll the producers for new events to process.This means that:  You do not need any additional infrastructure like an Apache Kafka or a Message Bus  Event publication is part of the local transaction  The publishing system (producer), controls the lifecycle of the events and can transform these if necessary  The events never leave the publishing system  There might be a bigger consistency gap, depending on how frequently the consumers poll  It does not scale that wellThe example code with REST can be found here.ConclusionThis was a great workshop which makes you think about the design decisions you have made for your applications.If you ever get the opportunity to participate in one of these workshops, do not hesitate to join as they are much more valuable than regular talks that can be viewed online as well.Observability with Spring based distributed systems by Tommy LudwigIntroductionTommy’s talk introduced three main pillars of observability: logging, metrics, and tracing.Tommy explained that observability is achieved through a set of tools and practices that aim to turn data points and contexts into insights.Observability is something you should care about as it provides a great experience for the users of your system and it builds confidence in production where failure will happen.You ought to give yourself the tools you need in order to be a good owner in order to detect these failures as early as possible.Mean time to recovery is key here.He also quoted Werner Vogels’, the CEO of Amazon, “You build it, you run it” while also adding to it that you need to monitor it.Within a Spring Boot project, we have access to Actuator and it is awesome.It comes with a lot of goodies out of the box.There is also Spring Boot Admin that makes it easy to access and use each instance’s Actuator endpoints.Distributed systems make observing them hard by design as a request spans multiple processes.You therefore need to stitch these together in order to fully make sense of it.There are also more points of failure and adding multiple instances of the same service, for scaling reasons, will only increase the monitoring complexity.Tommy named three sides to observability:  Logging  Metrics  TracingLoggingLogs are request scoped, arbitrary messages that you want to find back later.They are formatted to give you context via things such as logging levels and the timestamp.The issue with logs is that they do not scale, concurrent requests intermingle logs, and searching through them can be cumbersome.In order to tackle these issues you can make use of centralized logging while also adding a query capability to retrieve a collection of matching logs.Within Spring Boot we can configure the logging via Spring Environment and via Actuator at runtime.Spring Cloud Sleuth is useful to add a trace ID for request correlation.MetricsMetrics aggregate time series data and have a bounded size.You can slice these based on dimensions, tags and labels.The main goal of metrics is to visualize and identify trends and deviations, and to raise alerts based on metric queries.Some examples of metrics are: response time, the response’s body size and memory consumed.In order to properly measure all this, you need to set up a metrics backend to which all applications publish their metrics data.In Spring Boot 2, Micrometer is introduced as its native metrics library.Micrometer supports many metrics backends such as Atlas Datadog, Prometheus, SignalFX and Wavefront.A lot of the instrumentation is auto-configured by Spring Boot and custom metrics are added easily.These are configurable via properties and common tags such as the application name, the instance, region, zone, and more.TracingLocal tracing happens via the Actuator /httptrace endpoint and displays the latency data.With distributed tracing you can go across process boundaries which is useful as metrics lack request context and as logs have a local context but limited distributed info.You define the sample size of the request to trace yourself as you don’t want to trace everything especially if you have a high load.This sample size is configurable at runtime, especially handy to debug errors in production.Zipkin with its UI helps you to see the timing information visually and is a good tracing backend for Spring applications.Using Spring Cloud Sleuth, the tracing instrumentation via Zipkin’s Brave is auto-configured.Via properties you can configure things such as the sampling probability and whether certain endpoints should to be skipped.It is also compatible with the OpenTracing standard that is being developed under the wings of the CNCF.Correlation everywhereHaving set up all of these, you now have correlated logging, metrics and tracing across your system, and you can find the data from each based on identifiers.Observability cycleIf an issue produces itself we can take the following steps to troubleshoot and bandage the situation:  The issue should have been reported via an alert or report  We check the metrics of our system  If needed, we check the tracing data  If needed, we check the logs  Based on the gathered information we can triage the issue and make adjustments to prevent a recurrenceKey takeawaysSystem wide observability is crucial in distributed architectures.The tools to help you with this exist and Spring makes it easy to integrate them in your system as the most common cases are covered out-of-the-box or are easily configurable.Use the right tool for the job and synergise across the different tools.Day 2: Talks &amp; WorkshopsMachine learning exposed: The fundamentals by James WeaverMachine Learning is a hot topic in tech land with all kinds of applications like predicting property prices, forecasting weather, self-driving cars, plants classification and so on. James gave a brief overview about the fundamentals of Machine Learning and its applications.But how can we define Machine Learning? Andrew Ng, Co-founder of Coursera and Adjunct Professor of Stanford University defined Machine learning in his introduction course “Welcome To Machine Learning”1 as “Machine Learning is the science of getting computers to learn, without being explicitly programmed”. An example that Andrew gave was a cleaning robot that can tidy your house. Instead you program the algorithm explicitly on how it should clean. You can for instance let the robot watch you while you demonstrate the tasks on how it should clean and learn from it.Later on he gave examples of different categories of machine learning.Categories of Machine LearningSupervised learningThis was the category where James gave the most examples of during his talk.Supervised learning is where you train your model with a dataset which contains the initial data and its correct answers.The more training data you have, the more accurate your predictions will be.Regression exampleAn example he showed us was the prediction of housing prices using regression.(From Andrew NG’s Machine learning course)In this example, the dataset consists of instances with a square footage (input) and price (output).With a regression, we can predict a continuous valued price.Classification exampleSource:Nicoguaro's Wikipedia media gallery (CC BY 4.0) An example of Supervised Learning using classification Another example of Supervised Learning is to determine a certain species of an Iris flower.The algorithm tries to determine the species of the flower with the Sepal and Petal size as input.Unsupervised LearningFor Unsupervised Learning on the other hand, you don’t give the right answers with your dataset.Your learning algorithm will try to find a structure in the given data.A method to try to find a structure, is to do it by clustering.This means the data is ‘grouped’ in clusters together with data that more or less belongs to each other.Market segment discovery and social media analysis are examples of Unsupervised Learning.Reinforcement LearningBy Reinforcement Learning, you give your algorithm rewards when it did something well.This type of learning is very popular in game playing.AlphaGo for example from Google Deepmind was taught by Reinforcement Learning.Neural networksThe second part of his talk was about Neural Networks.(Artificial) Neural Networks are computing systems that are inspired by biological neural networks.It’s made up of highly interconnected processing elements or ‘nodes’ that can process information.A Neural Network consists of different layers.An input layer, one or more hidden layers and an output layer.We can visually demonstrate how Neural Networks work with the help of deeplearning4j.You can clone and try out his example on https://github.com/JavaFXpert/visual-neural-net-server.Let’s use the flower classification example with our neural network. 1: Welcome to Machine Learning (Andrew Ng)  Testing every level of your Spring Microservices application (Workshop) by Jeroen Sterken &amp; Kristof Van SeverIntroductionThis workshop focused on testing the different levels of a microservices application.It was split up into two parts:  Testing within a single microservice  Testing the relationships between microservices with Spring Cloud contractTesting a single microservice with Cucumber and JUnitThe presentation started off with some of the new features that JUnit 5 has to offer.Since JUnit 5 supports Java 8, it allows you to use lambdas in assertions, as well as using group assertions with the assertAll() method.It’s also possible to run tests multiple times with different parameters, by annotating them with the @ParameterizedTest and @ValueSource for the arguments.Behaviour driven testing with CucumberUnit tests alone are not enough of course, you also need to test how different components work together.Usually it’s the developer who writes such tests, but it’s also possible for non-technicals to write such tests with Cucumber.How does this work exactly?Cucumber achieves this by using Gherkin, an English plain text language.It has .feature files where the different scenarios for a certain feature are described.Feature: A new empty basket can be created and filled with TapasScenario: Client creates a new Basket, and verifies it's empty    When the user creates a new Basket    Then the total number of items in the Basket with id 1 equals 0The above is a simple example of how to describe a feature and scenario.The words in bold represent Cucumber keywords, called step definitions.It’s also possible to substitute Scenario with Scenario outline in case you need to test the same scenario with different values.You can put parameters inside angle brackets (&lt;&gt;), which are substituted with values that you define in an Examples data-table.The next step is to annotate your methods with the Cucumber step definitions (e.g. @Given @When).The text that you provide the annotations with, should match the text in your .feature file so that Cucumber can glue the two together.In this case, the When the user creates a new Basket of the example above matches with:@When(\"^the user creates a new Basket$\")The annotated methods should execute what you described in the feature files, so in this case the method looks something like this:public void theUserCreatesANewBasket() {    userBasketManagement.createNewBasket();  }To try it out for yourself, go to the Workshop repo.There’s a solution branch in case you’re stuck or wish to compare your code.Spring Cloud ContractOne of the challenges of testing chained microservices is making sure that a microservice stub reflects the actual service at all times.One way this can be achieved is by using Spring Cloud Contract.Spring Cloud Contract enables Consumer Driven Contract development, where one service (consumer) defines its expectations of another service (producer) through a contract.The first step is for the consumer to write the test for the new feature, following the TDD approach.Next, add the Spring Cloud Starter Contract Verifier dependency and maven plugin to your producer.Create a base test class to the test package that loads the Spring Context.Make sure to annotate it with @AutoConfigureMessageVerifier.We should also add the contract to our resources on the producer-side:Contract.make{    description \"should return a list of all tapas\"    request{        method GET()        url \"/tapas\"    }    response{        status 200        headers {            contentType applicationJson()        }        body (            [                [                        id: 0,                        name: \"All i oli\",                        price: 1.5                ],                [                        id: 1,                        name: \"Banderillas\",                        price: 3                ]             ]        )    }}The above is an example of how a contract is defined, written in Groovy (although YAML is a possibility as well).It simply specifies that a GET request to /tapas should return the provided body as application/json.Now it’s time to create the stub.Since you’ve already added the dependency and plugin to your producer, simply run your build for the plugin to generate the stubs.The built stub artifact will be stored in your local maven repository.The plugin will also create a test class that extends the base test class we created earlier containing the necessary setup to run your tests.The next step is to add the Spring Cloud Contract Stub Runner dependency to the consumer and annotate your test class with @AutoConfigureStubRunner.By annotating your class with @AutoConfigureStubRunner and providing the groupId, artifactId and port on which the stub will run, your test class is configured to use the producer’s generated stub.Interested to try out this Spring Cloud Contract workshop?You can find the Github repo here.Got triggered?All talks were recorded by the Spring IO team. You can view them on YouTube."
      },
    
      "cloud-2018-06-01-automated-canary-analysis-using-spinnaker-html": {
        "title": "Automated Canary Analysis using Spinnaker - Codelab",
        "url": "/cloud/2018/06/01/Automated-Canary-Analysis-using-Spinnaker.html",
        "image": "/img/spinnaker/spinnaker-logo.png",
        "date": "01 Jun 2018",
        "category": "post, blog post, blog",
        "content": "IntroSpinnaker is a multi-cloud, multi-region automated deployment tool.Open sourced by Netflix and heavily contributed to by Google, it supports all major cloud providers including Kubernetes.Last month, Kayenta was open sourced, a canary analysis engine.Canary analysis is a technique to reduce the risk from deploying a new version of software into production.A new version of the software, referred to as the canary, is deployed to a small subset of users alongside the stable running version.Traffic is split between these two versions such that a portion of incoming requests is diverted to the canary.This approach can quickly uncover any problems with the new version without impacting the majority of users.The quality of the canary version is assessed by comparing key metrics that describe the behavior of the old and new versions.If there is a significant degradation in these metrics, the canary is aborted and all of the traffic is routed to the stable version in an effort to minimize the impact of unexpected behavior.        PrefaceOrdina helps companies through digital transformation using three main focus areas:      Embracing a DevOps culture and corresponding practices allows teams to focus on delivering value for the business, by changing the communication structures of the organization.Through automation, teams are empowered and capable of delivering applications much faster to production.        Having a modular decoupled architecture, our second focus area, fits well with this model.Making these changes to our architecture in combination with a culture of automation, results in a lot more moving parts in our application landscape.        Naturally, the next step is tackling the underlying infrastructure accomodate this new architecture and way of working.Cloud automation is therefore our final focus area in digital transformations.  Releasing more often doesn’t only allow new features reaching the user faster, it also fastens the feedback loops, improves reliability and availability, developer productivity and efficiency. Spinnaker plays a crucial part in all of this, as it allows more frequent and faster deployments, without sacrificing safety.Automated canary analysis, demonstrated in this codelab, is a powerful tool in that sense.Overview  Goal  Prerequisites  Introducing our Rick &amp; Morty demo  Installation  Configuration  Running the demo scenario  ConclusionGoalThe purpose of this codelab is to simplify getting up-and-running with automated canary analysis using Spinnaker on Kubernetes.PrerequisitesWe’re using Google Cloud Platform for this demonstration.Monitoring and logging will be handled by Stackdriver, which is integrated completely with GCP.The canary functionality we’re going to use in this setup requires the use of a specific cluster version with full rights:  You must be an Owner of the project containing your cluster.  You must use Kubernetes v1.10.2-gke.0 or later.Introducing our Rick &amp; Morty demoRick &amp; Morty is a television show following the misadventures of cynical mad scientist Rick Sanchez and his good-hearted but fretful grandson Morty Smith, who split their time between domestic life and interdimensional adventures.        Our demo application is a Java Spring Boot application, running on an Apache Tomcat server, packaged inside a docker container.The docker container runs on Kubernetes managed by Google Cloud Platform (GKE).The application exposes an endpoint on http://localhost:8080 and can be run locally by executing ./mvnw spring-boot:run, assuming you have a JRE or JDK (v8+) installed.The endpoint returns an HTML with a background of Pickle Rick.In season three episode three, Rick turns himself into a pickle in an attempt at escaping family therapy.        Pickle Rick will act as our initial green deployment, running stabily on production.We will try to replace it with a blue deployment.Mr. Meeseeks, featured in season one episode five, will be the protagonist of that deployment.Meeseeks are creatures who are created to serve a singular purpose for which they will go to any length to fulfill.After they serve their purpose, they expire and vanish into the air.Their motivation to help others comes from the fact that existence is painful to a Meeseeks, and the only way to be removed from existence is to complete the task they were called to perform.Meeseeks can however summon other Meeseeks to help, which could spiral out of control if the task at hand is unsolvable.        Therefore, Meeseeks are quite dangerous and a good candidate for our misbehaving blue deployment.Aside from the HTTP endpoint, our demo application also prints out a number of character names from the series.Blue Green DifferencesAside from the leading character in our two versions, there are two specific differences in the code between both versions.The following commit shows moving from the green version to the blue version: 24cc45cfFirst of all, the background image changes, which gives a clear visual indication of which version is currently deployed.Since using Meeseeks could get out of hand quickly, keeping track of how many times the Meeseeks HTTP endpoint has been hit makes a lot of sense.Hence, the blue version prints an extra Meeseeks in the logs, for every request to the endpoint.Using this setup, we should be able to consider logs as a source of information for judging the canary healthiness.Note that the Github repository can constantly switch between Pickle Rick and Meeseeks.Before starting a build and making deployments, make sure your fork is aligned with the green version.If this isn’t the case, switching to green is demonstrated in the following commit: 784e616aSetup Continuous IntegrationMaking changes to our application will be the trigger for our pipelines.Therefore, we should have a simple continuous integration flow set up.We could use Jenkins or any other build server that uses webhooks, but since our entire demo is being deployed on GCP, we can use the build server from GCP instead.First of all, fork the demo application repository.In the GCP console, open build triggers underneath the Container Registry (GCR) tab.Select Github as repository hosting, and select the forked repository to create a trigger for.Configure the trigger to activate on any branch, using a cloudbuild.yaml file located in the root of the repository.        This will run a maven build and docker build, and push the created docker image into the GCR.Installation  Throughout this guide we refer to the official documentation for individual parts of the installation already covered by the Spinnaker team.However, as reference we also compiled an exhaustive list of commands to execute based on the commands found in those articles.This means you could skip the official documentation and simply execute those commands.However, we still recommend going through the docs to get more context.The list of commands to execute can be found at the end of this chapter.Follow the guide on Spinnaker’s website: https://www.spinnaker.io/setup/quickstart/halyard-gkeCreate a cluster as mentioned here: https://cloud.google.com/monitoring/kubernetes-engine/installinggcloud components updategcloud auth logingcloud config set project &lt;PROJECT_NAME&gt;Find out the latest supported cluster version with the following command:gcloud container get-server-config --zone=$ZONECreate a cluster for your specific zone (e.g. europe-west1-d) and preferred cluster version (v1.10.2-gke.0 or later):CLUSTER_VERSION=1.10.2-gke.1GCP_PROJECT=$(gcloud info --format='value(config.project)')ZONE=europe-west1-dCLOUDSDK_CONTAINER_USE_V1_API=falseCLOUDSDK_API_CLIENT_OVERRIDES_CONTAINER=v1beta1gcloud beta container clusters create spinnaker \\  --zone=$ZONE \\  --project=$GCP_PROJECT \\  --cluster-version=$CLUSTER_VERSION \\  --enable-stackdriver-kubernetes \\  --enable-legacy-authorizationMake sure --enable-stackdriver-kubernetes and --enable-legacy-authorization are passed.Enable APIsNavigate to the Google Cloud Console and enable the following APIs:  Google Identity and Access Management (IAM) API  Google Cloud Resource Manager APIHalyard setupThis section complements official documentation with some recommendations and extras.Postpone running the hal deploy apply command until the end of this chapter.Basic SpinnakerDuring the Halyard on GKE guide on Spinnaker’s website, remember to use the right zone when creating the Halyard VM.gcloud compute instances create $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --scopes=cloud-platform \\    --service-account=$HALYARD_SA_EMAIL \\    --image-project=ubuntu-os-cloud \\    --image-family=ubuntu-1404-lts \\    --machine-type=n1-standard-4When SSH’ing into the Halyard VM, also remember to use the right zone.gcloud compute ssh $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --ssh-flag=\"-L 9000:localhost:9000\" \\    --ssh-flag=\"-L 8084:localhost:8084\"Before you perform hal deploy apply, add the Docker registry corresponding to your region. In case your project is located in Europe, add the eu.gcr.io registry as illustrated below.hal config provider docker-registry account add gcr-eu \\    --address eu.gcr.io \\    --password-file ~/.gcp/gcp.json \\    --username _json_keyhal config provider kubernetes account edit my-k8s-account --docker-registries my-gcr-account gcr-euIAM ConfigurationEnable Stackdriver access for Spinnaker in GCP’s IAM settings.Add the following roles to the member with name gcs-service-account:  Logging Admin  Monitoring AdminAutomated Canary AnalysisBefore you perform hal deploy apply, enable automated canary analysis.Follow the guide further down, but first of all, set some variables while still SSH’d in the Halyard VM.One of these variables is the Spinnaker bucket automatically created when installing Halyard.Look for the right bucket identifier in the GCP GKE buckets dashboard.PROJECT_ID=$(gcloud info --format='value(config.project)')JSON_PATH=~/.gcp/gcp.jsonMY_SPINNAKER_BUCKET=spin-48b89b5e-dd67-446a-ad9f-66e8783e9822Follow the official canary quickstart documentation.Configure the default metrics store.hal config canary edit --default-metrics-store stackdriverAnd finally execute the rollout.hal deploy applyTroubleshootingAuthenticationSometimes an issue might occur with credentials on the Halyard VM:! ERROR Unable to communicate with your Kubernetes cluster: Failure  executing: GET at: https://35.205.113.166/api/v1/namespaces. Message: Forbidden!  User gke_spinnaker-demo-184310_europe-west1-d_spinnaker-alpha doesn't have  permission. namespaces is forbidden: User \"client\" cannot list namespaces at the  cluster scope: Unknown user \"client\"..? Unable to authenticate with your Kubernetes cluster. Try using  kubectl to verify your credentials.In this case, enable legacy authentication in the GKE UI for the cluster.Cluster DebuggingYou can monitor deployment locally on your own PC by running kubectl get pods -w --all-namespaces.For this to work, kubectl needs permissions to talk to the cluster.You can use gcloud to populate your kubeconfig file with credentials to access the cluster.This can help you to look into specific logs of each Spinnaker pod or follow up on deployments handled by Spinnaker.Audit LoggingYou can find out which commands are sent to GCP by enabling audit logging.Turn on audit logging: https://cloud.google.com/monitoring/audit-logging &amp; https://cloud.google.com/logging/docs/audit/configure-data-access#exampleComprehensive list of commandsThese are all the commands we have executed in order to get everything set up.Fill in the &lt;PLACEHOLDER&gt; placeholders according to your preferences.ZONE=&lt;ZONE_NAME&gt;CLUSTER_VERSION=&lt;CLUSTER_VERSION&gt;GCP_PROJECT=&lt;PROJECT_NAME&gt;gcloud components updategcloud auth logingcloud config set project $PROJECT_NAMEgcloud container get-server-config --zone=$ZONECLOUDSDK_CONTAINER_USE_V1_API=falseCLOUDSDK_API_CLIENT_OVERRIDES_CONTAINER=v1beta1gcloud beta container clusters create spinnaker \\  --zone=$ZONE \\  --project=$GCP_PROJECT \\  --cluster-version=$CLUSTER_VERSION \\  --enable-stackdriver-kubernetes \\  --enable-legacy-authorizationHALYARD_SA=halyard-service-accountgcloud iam service-accounts create $HALYARD_SA \\    --project=$GCP_PROJECT \\    --display-name $HALYARD_SAHALYARD_SA_EMAIL=$(gcloud iam service-accounts list \\    --project=$GCP_PROJECT \\    --filter=\"displayName:$HALYARD_SA\" \\    --format='value(email)')gcloud projects add-iam-policy-binding $GCP_PROJECT \\    --role roles/iam.serviceAccountKeyAdmin \\    --member serviceAccount:$HALYARD_SA_EMAILgcloud projects add-iam-policy-binding $GCP_PROJECT \\    --role roles/container.admin \\    --member serviceAccount:$HALYARD_SA_EMAILGCS_SA=gcs-service-accountgcloud iam service-accounts create $GCS_SA \\    --project=$GCP_PROJECT \\    --display-name $GCS_SAGCS_SA_EMAIL=$(gcloud iam service-accounts list \\    --project=$GCP_PROJECT \\    --filter=\"displayName:$GCS_SA\" \\    --format='value(email)')gcloud projects add-iam-policy-binding $GCP_PROJECT \\    --role roles/storage.admin \\    --member serviceAccount:$GCS_SA_EMAILgcloud projects add-iam-policy-binding $GCP_PROJECT \\    --member serviceAccount:$GCS_SA_EMAIL \\    --role roles/browserHALYARD_HOST=$(echo $USER-halyard-`date +%m%d` | tr '_.' '-')gcloud compute instances create $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --scopes=cloud-platform \\    --service-account=$HALYARD_SA_EMAIL \\    --image-project=ubuntu-os-cloud \\    --image-family=ubuntu-1404-lts \\    --machine-type=n1-standard-4gcloud compute ssh $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --ssh-flag=\"-L 9000:localhost:9000\" \\    --ssh-flag=\"-L 8084:localhost:8084\"Inside the Halyard VM:KUBECTL_LATEST=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)curl -LO https://storage.googleapis.com/kubernetes-release/release/$KUBECTL_LATEST/bin/linux/amd64/kubectlchmod +x kubectlsudo mv kubectl /usr/local/bin/kubectlcurl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/debian/InstallHalyard.shsudo bash InstallHalyard.sh. ~/.bashrcGKE_CLUSTER_NAME=spinnakerGKE_CLUSTER_ZONE=europe-west1-dPROJECT_ID=$(gcloud info --format='value(config.project)')gcloud config set container/use_client_certificate truegcloud container clusters get-credentials $GKE_CLUSTER_NAME \\    --zone=$GKE_CLUSTER_ZONEGCS_SA=gcs-service-accountGCS_SA_DEST=~/.gcp/gcp.jsonmkdir -p $(dirname $GCS_SA_DEST)GCS_SA_EMAIL=$(gcloud iam service-accounts list \\    --filter=\"displayName:$GCS_SA\" \\    --format='value(email)')gcloud iam service-accounts keys create $GCS_SA_DEST \\    --iam-account $GCS_SA_EMAILhal config version edit --version $(hal version latest -q)hal config storage gcs edit \\    --project $PROJECT_ID \\    --json-path $GCS_SA_DESThal config storage edit --type gcshal config provider docker-registry enablehal config provider docker-registry account add my-gcr-account \\    --address gcr.io \\    --password-file $GCS_SA_DEST \\    --username _json_keyhal config provider kubernetes enablehal config provider kubernetes account add my-k8s-account \\    --docker-registries my-gcr-account \\    --context $(kubectl config current-context)# Only required in case you want to use eu.gcr.iohal config provider docker-registry account add gcr-eu \\    --address eu.gcr.io \\    --password-file $GCS_SA_DEST \\    --username _json_keyhal config provider kubernetes account edit my-k8s-account --docker-registries my-gcr-account gcr-euhal config deploy edit --account-name my-k8s-accounthal config deploy edit --type distributedMY_SPINNAKER_BUCKET=&lt;SPINNAKER_BUCKET_ID&gt;hal config canary enablehal config canary google enablehal config canary google account add my-google-account \\  --project $PROJECT_ID \\  --json-path $GCS_SA_DEST \\  --bucket $MY_SPINNAKER_BUCKEThal config canary google edit --gcs-enabled true \\  --stackdriver-enabled truehal config canary edit --default-metrics-store stackdriverhal deploy applyhal deploy connectConfigurationApplication ConfigurationThis guide uses the Kubernetes V1 provider, but you can use V2 just as well.Follow the official documentation to enable the V2 provider.Visit localhost:9000 to open the Spinnaker UI.In the applications page, create a new application:        Under Infrastructure, the Clusters view should normally be opened automatically.Click the Config link on the top right and enable Canary for this project.        This should enable Canary Analysis for the project.The result should be that the Spinnaker menu for this project should be changed.Pipelines are now nested underneath Delivery, which also now boasts Canary Configs and Canary Reports.In case this is not visualised directly, you can refresh the cache by clicking on the Spinnaker logo on the top left of the page, and clicking the Refresh all caches link in the Actions drop down.        Initial ProvisioningUnder Infrastructure, switch to the Load Balancers view and create a load balancer.Fill in the stack, port, target port and type LoadBalancer.        Under Infrastructure, switch to the Clusters view and create a Server Group.                Once the server group is created, it will show up like this:        By clicking on the little load balancer icon on the right-hand side, we can now visit the Ingress IP through the load balancer view on the side of the page.        Back in the server group section, clicking on the little green chicklet will display container information on the side of the page, including logs of the application.Let’s do this for PROD as well.Follow exactly the same steps as for DEV, except use prod as Stack instead of dev.Once the PROD load balancer and server group are deployed, we’d like to make sure we never have downtime on PROD.We can set up a Traffic Guard, responsible for making sure our production cluster always has active instances.Go to the Config link on the top right of the page, and add a Traffic Guard.        Staging PipelineNow that we’ve deployed a single version of our application to DEV and PROD, it’s time to create a pipeline.This will enable us to continuously deploy new versions of our application without having to manually create new server groups every time.Head over to the pipelines view and create a new pipeline called Deploy to DEV.Under the first “Configuration” stage, configure an automated trigger.        Now add a stage to deploy our application.        We now have to add a server group as deployment configuration.We can reuse the existing deployment as a template.        Change the strategy to Highlander.It’s important to change the image being deployed, otherwise, we’d always deploy the image of our existing server group.Go down to the Container section and select the Image from Trigger.        This will automatically change the container image at the top of the dialog box under Basic Settings.        Keep all other settings as they are.Save the server group configuration, and save the pipeline.When we now select the pipelines view, we can see the newly created Deploy to DEV pipeline.We can test this by either starting a manual execution, or committing a change to our application GIT repository.Production PipelineCreate new pipeline Deploy to PROD.        Add a new Find Image from Cluster stage.This stage will allow us to look for the image we deployed to DEV, and pass that information on to upcoming stages.        Add a new Deploy stage to deploy the new DEV version into production.Under deploy configuration, add a server group based on the one in DEV.Make sure to set the right load balancer, i.e. spinnakedemo-prod.        Scrolling down to the Container section, select the image found in DEV by the Find Image stage.        Since this is a new version we’d like to push to production, it would be a good idea to build in some safety measures to protect us from unexpected failure.Using a canary release strategy allows us to limit the blast radius of potential issues that might arrise.In the Basic Settings section, set the stack as prod and the detail as canary to indicate that this deployment is our canary deployment. Also use the None strategy, since we just want to deploy this canary server group next to the one already active in production.        Now let’s test this out.Change the application to respond with PickleRicks if that’s not already the case.Otherwise, make an insignificant change to the application and push the changes to GIT (master branch).This should trigger a build, which should push a docker image to the GCR.That on its turn should trigger the deployment to DEV, which - if successful - should trigger a deployment to PROD.Once that’s done, your cluster view should look like this:        Notice the V001 on DEV, it has replaced the existing manual server group deployment using the highlander strategy.Currently our canary is registered under the same load balancer as our production cluster. This means traffic is split between the canary and production.We could test the canary manually by going to the ingress endpoint of the load balancer as we did on DEV.This could be sufficient for your needs, but Spinnaker offers automated canary analysis (aka. ACA), capable of automatically investigating traffic sent to the canary.The ACA engine Kayenta will compare certain metrics between the currently running production version, and the newly deployed canary version.Since comparing a fresh deployment with an old, potentially degraded deployment, could produce unwanted results, it’s advised to deploy both a canary and a current production instance labelled baseline, next to each other.In the Deploy to PROD pipeline configure screen, add a stage in parallel with Find Image from DEV by clicking on the outer-left Configuration stage, and adding a new stage from that point on.        From that point forward, add another Deploy stage, with the prod server group as template.        At the bottom of the Deployment Cluster Configuration, switch the Container Image to the Find Image result for prod.        Add baseline as detail, and keep the strategy as None.        Save the pipeline.We now have our basic setup of both a baseline and canary server group to perform canary analysis.Canary AnalysisOur specific demo scenario uses Meeseeks from Rick and Morty as the new version to deploy.As people who watched the series probably will know, Meeseeks can quickly become a threat to our way of living if we let nature run its course.Therefore, when switching to Meeseeks, we also write Meeseeks in the logs to keep track of them.GCP uses Stackdriver for logging and monitoring, so if we’d like to use the logs as a source of information for canary analysis, we should make a Stackdriver metric using the Meeseeks logs.In the GCP left-hand menu, under the Stackdriver section, you can find Logging and drill down to Logs-based metrics.Add a new metric using the following filter, replacing the location and project_id by the zone name and project id from earlier in this guide:(resource.labels.cluster_name=\"spinnaker\" AND resource.labels.location=\"europe-west1-d\" AND resource.labels.namespace_name=\"default\" AND resource.labels.project_id=\"spinnaker-demo-184310\" AND textPayload:\"Meeseeks”)                Back in Spinnaker, head over to the Canary Configs view under Delivery.Create a new Canary Config called Demo-config, and add a filter template.The template will filter based on the replication controller of the server group:resource.labels.pod_name:\"${scope}\"        Now we can add actual metrics to analyse.Create a new Metrics Group called Meeseeks, with one metric underneath.        Since we’d also like to know whether our CPU or memory consumption has increased, let’s add some system metrics as well.We can investigate which filters we can construct by using the GCP REST API.Add a new group called Boring System Metrics, and add the following two metrics.                The only thing left to do for this Canary Config, is to configure thresholds for the Metric Groups.The marginal is treated as a lower bound.If an interval analysis fails to reach the marginal limit, the entire canary release will be halted and no further intervals will be analysed.The pass limit is the upper bound, qualifying the analysis as a success.Anything in between will be recorded and next intervals will be analysed.        Save the Canary Config, and go back to the Deploy to PROD pipeline configuration.Join both canary and baseline deployments into the Canary Analysis stage, by using the Depends On configuration.        Configure the canary analysis stage as follows.                Rollout or RollbackAfter the Canary Analysis has run, the new version can safely replace the existing production server group.Add a stage called Deploy to PROD, copying the production server group as template, and use the red/black (aka. blue/green) deployment strategy to avoid any downtime.        At the bottom of the Deployment Cluster Configuration, switch the Container Image to the Find Image result for DEV.                Regardless whether this pipeline actually succeeds or not, we need to make sure to clean up afterwards.Add a new pipeline called Tear Down Canary, with the following trigger.        Add two Destroy Server Group stages in parallel.        Configure the first one to destroy our baseline server group.        And finally also destroy the canary server group.        Running the demo scenarioAs explained in the introduction of the demo application, we have two versions of our application.As long as we keep deploying green versions with minor changes to other parts of the application (not impacting Meeseeks logs), the whole pipeline should pass, including the canary analysis.For a canary test to be successful, we need data.The more data our test can gather, the more informed the decision will be.In our demo scenario, we can continuously refresh the page to generate more load and more Meeseeks in the logs, but we can also use a script for that.In the root of the demo repository, a script called randomload.sh can be used to generate calls to the PROD ingress endpoint at a random interval.The script uses HTTPie to make calls, but you can also replace it with curl commands.Also, make sure you change the IP address in your forked repository’s file.SuccessA successful canary release would look like this.        Meeseeks logs should occur at a similar rate in the canary and the baseline server group.        CPU and RAM metrics are also part of the comparison.In the example below, the canary CPU metrics deviated too much from the baseline, resulting in a failure for that metric group.However, the weight of those metrics were not high enough to fail the verdict, but it did cause the outcome to be labeled marginal.                FailureWhen switching to the blue Meeseeks version, the initial DEV deploy would succeed, but our canary analysis should fail after one or two intervals.A failed canary release would look like this.        The canary server group generated a noticeable higher amount of Meeseeks than our baseline server group, resulting in a failed analysis.        Even though canary CPU and RAM metrics were quite in sync with the baseline, our Meeseeks metrics were enough to fail the entire pipeline.                ConclusionPickle Rick and Mr. Meeseeks have shown us the power of automated canary analysis using Spinnaker.There are still a few considerations we have to take into account, such as the importance of choosing the right metrics and filters and iterating on those after each canary release.Yet, having a tool like this at our disposal allows us to release more often to production, without compromising safety or quality.By reducing manual and ad hoc analysis only the most stable releases are deployed to production in a highly automated way."
      },
    
      "orchestration-2018-05-22-lagom-1-4-and-kubernetes-orchestration-html": {
        "title": "Lagom 1.4 and Kubernetes orchestration",
        "url": "/orchestration/2018/05/22/lagom-1-4-and-kubernetes-orchestration.html",
        "image": "/img/lagom-kubernetes.png",
        "date": "22 May 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Upgrading to Lagom 1.4  Lightbend’s orchestration tools  Adding Kubernetes support to our Lagom Shop Scala application  Building and publishing Docker images  Locally deploying to Kubernetes using Minikube  Generating Kubernetes resources for production  Conclusion  Extra resourcesIntroductionIn this blog post we will take a closer look at the Lightbend Orchestration tools.Tools helping you deploy your Lagom application to Kubernetes and DC/OS.It was already possible to deploy Lagom applications to Kubernetes as this guide demonstrates but it involved more manual tasks and having to write the Kubernetes resource and configuration files yourself, as it usually goes.As it currently stands, the tools are only supported in combination with sbt so Maven users cannot fully take advantage of it just yet.Maven support will follow soon however as the Maven equivalent plugin is nearing its first release version.If you are new to Lagom feel free to take a look at one of our earlier blog posts on Lagom:  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom 1.2: What’s new?Before getting to that though, we will upgrade our sample application Lagom Shop Scala, which was also referred to in our previous blog posts, from Lagom 1.2 to Lagom 1.4 to demonstrate the upgrading process.The application consists of two Lagom microservices combined with a frontend written in Play Framework.Afterwards we will take a closer look at how easy it is to integrate the Lightbend Orchestration tools into our project and how we can get our project up and running on Kubernetes.Note that this blog post is not an in-depth guide on the tools themselves but more a general overview and for us to share our impressions.Upgrading to Lagom 1.4In this section we will upgrade our sample application Lagom Shop Scala, from version 1.3.4 to 1.4.x.Lagom 1.4 uses Play Framework’s latest version, 2.6 for which we will also need to change a few things in our project.Lightbend provides a migration guide for each new version they release, so in this case we followed the Lagom 1.4 Migration Guide and the Play 2.6 Migration Guide.When upgrading multiple minor versions, it is advised to upgrade one minor version at a time to smoothen the process.In our case we are only limited to upgrading a single minor version so we can just use the latest migration guide right away.Upgrade the Lagom version in project/plugins.sbt:addSbtPlugin(\"com.lightbend.lagom\" % \"lagom-sbt-plugin\" % \"1.4.5\")Upgrade the sbt version in project/build.properties:sbt.version=0.13.16Upgrade the Scala version to 2.12.4 in build.sbt:scalaVersion in ThisBuild := \"2.12.4\"Upgrade Play JSON Derived Codecs to 4.0.0 which adds Play 2.6 support:val playJsonDerivedCodecs = \"org.julienrf\" %% \"play-json-derived-codecs\" % \"4.0.0\"Replace play.api.data.validation.ValidationError with play.api.libs.json.JsonValidationError.Mix in LagomConfigComponent, HttpFiltersComponents and AssetsComponents and remove lazy val assets: Assets = wire[Assets] in the application loader class extending BuiltInComponentsFromContext in the Play frontend project.abstract class Frontend(context: Context) extends BuiltInComponentsFromContext(context)  with I18nComponents  with AhcWSComponents  with LagomKafkaClientComponents  with LagomServiceClientComponents  with LagomConfigComponent  with HttpFiltersComponents  with AssetsComponents {Change override def describeServices to override def describeService in each Lagom project’s class extending LagomServerComponents as the other one has become deprecated.override def describeService = Some(readDescriptor[ItemService])Implement CSRF security in the frontend project by utilising CSRF form fields (@CSRF.formField) or one of the other approaches.Note that Lagom’s development mode service locator now listens on port 9008 instead of 8000 although this can still be changed by overriding the default port.To see a complete list of changes we did, refer to commit bdf5ecff.Lightbend’s orchestration toolsAs we mentioned in the introduction, Lightbend offers a developer-centric suite of tools helping you deploy your Play/Akka/Lagom applications to Kubernetes and DC/OS.The tools help you create a Docker image of all your applications, help with generating Kubernetes and DC/OS resource and configuration files based on the Docker images, and they allow you to deploy your whole Lagom project to Kubernetes using a simple command which can be pretty convenient for development. The generated JSON and YAML files could be put under version control after which they can be submitted to a CI/CD integrated central repository.The suite consists of three different tools:  sbt-reactive-app, an sbt plugin that inspects your projects and builds annotated Docker images.The Maven equivalent plugin is still being worked on and is nearing its first release version.  reactive-cli, a command-line tool with which you generate the Kubernetes and DC/OS resource and configuration files.You need to install this on the device or environment from which you will deploy to Kubernetes.Install guidelines are available in the documentation.For Mac for example this is easily accomplished with Homebrew:    brew tap lightbend/tools &amp;&amp; brew install lightbend/tools/reactive-cli    reactive-lib, a library for your application that is automatically included in your application’s build by the sbt-reactive-app sbt plugin.It allows your application to perform service discovery, access secrets, define health and readiness checks, and more as it understands the conventions of the resources generated by the command-line tool.Adding Kubernetes support to our Lagom Shop Scala applicationWe start off with adding the sbt-reactive-app sbt plugin in the project/plugins.sbt file:addSbtPlugin(\"com.lightbend.rp\" % \"sbt-reactive-app\" % \"1.1.0\")Now enable the plugin for each module in the build.sbt file:lazy val itemImpl = (project in file(\"item-impl\"))  .dependsOn(itemApi)  .settings(commonSettings: _*)  .enablePlugins(LagomScala, SbtReactiveAppPlugin)  lazy val orderImpl = (project in file(\"order-impl\"))  .dependsOn(orderApi, itemApi)  .settings(commonSettings: _*)  .enablePlugins(LagomScala, SbtReactiveAppPlugin)  .settings(    libraryDependencies ++= Seq(      lagomScaladslPersistenceCassandra,      lagomScaladslTestKit,      lagomScaladslKafkaBroker,      cassandraDriverExtras,      macwire,      scalaTest    )  )  .settings(lagomForkedTestSettings: _*)  lazy val frontend = (project in file(\"frontend\"))  .dependsOn(itemApi, orderApi)  .settings(commonSettings: _*)  .enablePlugins(PlayScala &amp;&amp; LagomPlay, SbtReactiveAppPlugin)  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies ++= Seq(      lagomScaladslServer,      lagomScaladslKafkaClient,      macwire,      scalaTest,      \"org.webjars\" % \"foundation\" % \"6.2.3\",      \"org.webjars\" % \"foundation-icon-fonts\" % \"d596a3cfb3\"    ),    EclipseKeys.preTasks := Seq(compile in Compile),    httpIngressPaths := Seq(\"/\")  )If you also have a frontend module it is important to define the httpIngressPaths, as you might have seen in the code sample above, in order to have your frontend be accessible from outside the cluster.Mix in the LagomServiceLocatorComponents trait in each module’s application loader:import com.lightbend.rp.servicediscovery.lagom.scaladsl.LagomServiceLocatorComponentsclass ItemApplicationLoader extends LagomApplicationLoader {  override def load(context: LagomApplicationContext): LagomApplication =    new ItemApplication(context) with LagomServiceLocatorComponents {      override lazy val circuitBreakerMetricsProvider = new CircuitBreakerMetricsProviderImpl(actorSystem)    }}Building and publishing Docker imagesThe tool suite comes with an easy way to deploy all your services to Minikube for development so you will want to set that up first.For installation instructions, consult the Minikube documentation.Start up Minikube with a sufficient amount of memory:$ minikube start --memory 8192Starting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file.You kind of need a decent device with a decent amount of RAM.I have tested all of this on a MacBook Pro with 8GB RAM and I simply was not able to assign a sufficient amount of memory to comfortably run everything locally.Execute the following command to have our Docker environment variables point to Minikube:eval $(minikube docker-env)We can check if Minikube is up and running with the following command:$ kubectl get nodesNAME       STATUS    ROLES     AGE       VERSIONminikube   Ready     &lt;none&gt;    1m        v1.8.0To figure out your Minikube IP you can utilise the following command:$ echo \"http://$(minikube ip)\"http://192.168.99.100It is also important to enable the Ingress addon:minikube addons enable ingressWe need to launch sbt:$ sbtAfter which we can publish the images locally (you might want to grab a coffee after executing this):docker:publishLocalUnderneath, SBT Native Packager is being used.Check out its documentation for Docker related configurations.Publishing to a Docker Registry is also possible by defining the dockerRepository sbt setting in the project and after authenticating to the registry, see Publishing to a Docker Registry.After doing so you can execute the publish command:docker:publishOur Docker images should then be available:$ docker imagesREPOSITORY             TAG                 IMAGE ID            CREATED             SIZEorderimpl              1.0-SNAPSHOT        e9be41c50eb2        32 seconds ago      159MBitemimpl               1.0-SNAPSHOT        357a9d546593        9 minutes ago       159MBfrontend               1.0-SNAPSHOT        7251c13a5373        6 days ago          141MBAs for the Maven equivalent plugin, the Docker images can be build by executing:$ mvn installLocally deploying to Kubernetes using MinikubeFor development we can make use of an sbt task for deploying everything to Kubernetes using Minikube.But before that, we need to do a couple of things.We first need to set up Lightbend’s Reactive Sandbox which is a Docker image that contains the usual components used when developing reactive applications using the Lightbend frameworks:  Cassandra  Elasticsearch  Kafka  ZooKeeperWe will use Helm, a package manager for Kubernetes, to set it up.Install instructions of Helm are available on the GitHub repository.For Mac for example you can install it using brew:$ brew install kubernetes-helmWith Helm we can then install the Reactive Sandbox:helm inithelm repo add lightbend-helm-charts https://lightbend.github.io/helm-chartshelm updatehelm install lightbend-helm-charts/reactive-sandbox --name reactive-sandboxAll set up, we can now utilise an sbt command to deploy all our services to Minikube!Start sbt in the root of your project:$ sbtAnd deploy everything to Minikube using a sbt task that comes with the sbt-reactive-app plugin.The task however is not yet supported on Windows unfortunately.deploy minikubeDuring the setup we encountered a connection initialisation error of Helm:Cannot initialize Kubernetes connection: Get http://localhost:8080/api: dial tcp 127.0.0.1:8080: getsockopt: connection refusedWe found a solution for this in the following Helm GitHub issue: Tiller pods can’t connect to k8s apiserver #2464.$ kubectl --namespace=kube-system create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:defaultIf all went well your application should be accessible at https://192.168.99.100.You can use the Minikube dashboard to inspect everything at http://192.168.99.100:30000.  Note that you can also deploy a single module instead of all of them.For example if we only want to deploy the frontend, you simply switch to the specific project before running the command:$ sbt[info] Loading global plugins from /Users/yannickdeturck/.sbt/0.13/plugins[info] Loading project definition from /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/project[info] Set current project to lagom-shop-scala (in build file:/Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/)&gt; projects[info] In file:/Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/[info] \t   common[info] \t   frontend[info] \t   itemApi[info] \t   itemImpl[info] \t   lagom-internal-meta-project-cassandra[info] \t   lagom-internal-meta-project-kafka[info] \t   lagom-internal-meta-project-service-locator[info] \t * lagom-shop-scala[info] \t   orderApi[info] \t   orderImpl&gt; project frontend[info] Set current project to frontend (in build file:/Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/)[frontend] $ deploy minikube[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/frontend/target/scala-2.12/frontend_2.12-1.0-SNAPSHOT.pom[info] Packaging /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/frontend/target/scala-2.12/frontend_2.12-1.0-SNAPSHOT-sources.jar ...[info] Done packaging.[info] Packaging /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/frontend/target/scala-2.12/frontend_2.12-1.0-SNAPSHOT.jar ...[info] Done packaging.[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/common/target/scala-2.12/common_2.12-1.0-SNAPSHOT.pom[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/item-api/target/scala-2.12/itemapi_2.12-1.0-SNAPSHOT.pom[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/order-api/target/scala-2.12/orderapi_2.12-1.0-SNAPSHOT.pom13:23:14.079 [pool-7-thread-1] DEBUG com.lightbend.lagom.internal.api.tools.ServiceDetector - Loading service discovery class: FrontendLoader[info] Sending build context to Docker daemon  54.52MB[info] Step 1/9 : FROM openjdk:8-jre-alpine[info]  ---&gt; b1bd879ca9b3[info] Step 2/9 : RUN /sbin/apk add --no-cache bash[info]  ---&gt; Using cache[info]  ---&gt; 193af79a4475[info] Step 3/9 : WORKDIR /opt/docker[info]  ---&gt; Using cache[info]  ---&gt; 741d2377c4e8[info] Step 4/9 : ADD --chown=daemon:daemon opt /opt[info]  ---&gt; Using cache[info]  ---&gt; d7884eead001[info] Step 5/9 : USER daemon[info]  ---&gt; Using cache[info]  ---&gt; cdedfe6fc10c[info] Step 6/9 : ENTRYPOINT [][info]  ---&gt; Using cache[info]  ---&gt; 2db1227ffe9e[info] Step 7/9 : CMD [][info]  ---&gt; Using cache[info]  ---&gt; bd147ead79fd[info] Step 8/9 : COPY rp-start /rp-start[info]  ---&gt; Using cache[info]  ---&gt; 340110c7c251[info] Step 9/9 : LABEL com.lightbend.rp.app-name=\"frontend\" com.lightbend.rp.applications.0.name=\"default\" com.lightbend.rp.applications.0.arguments.0=\"/rp-start\" com.lightbend.rp.applications.0.arguments.1=\"bin/frontend\" com.lightbend.rp.app-version=\"1.0-SNAPSHOT\" com.lightbend.rp.app-type=\"lagom\" com.lightbend.rp.config-resource=\"rp-application.conf\" com.lightbend.rp.modules.akka-cluster-bootstrapping.enabled=\"false\" com.lightbend.rp.modules.akka-management.enabled=\"false\" com.lightbend.rp.modules.common.enabled=\"true\" com.lightbend.rp.modules.play-http-binding.enabled=\"true\" com.lightbend.rp.modules.secrets.enabled=\"false\" com.lightbend.rp.modules.service-discovery.enabled=\"true\" com.lightbend.rp.modules.status.enabled=\"false\" com.lightbend.rp.endpoints.0.name=\"http\" com.lightbend.rp.endpoints.0.protocol=\"http\" com.lightbend.rp.endpoints.0.ingress.0.type=\"http\" com.lightbend.rp.endpoints.0.ingress.0.ingress-ports.0=\"80\" com.lightbend.rp.endpoints.0.ingress.0.ingress-ports.1=\"443\" com.lightbend.rp.endpoints.0.ingress.0.paths.0=\"/\" com.lightbend.rp.sbt-reactive-app-version=\"1.1.0\"[info]  ---&gt; Using cache[info]  ---&gt; 7251c13a5373[info] Successfully built 7251c13a5373[info] Successfully tagged frontend:1.0-SNAPSHOT[info] Built image frontend:1.0-SNAPSHOT[info] deployment.apps \"frontend-v1-0-snapshot\" deleted[info] service \"frontend\" deleted[info] ingress.extensions \"frontend\" deleted[info] deployment.apps \"frontend-v1-0-snapshot\" created[info] service \"frontend\" created[info] ingress.extensions \"frontend\" created[success] Total time: 19 s, completed May 11, 2018 1:23:31 PMWe can inspect everything with kubectl:$ kubectl get podsNAME                                     READY     STATUS    RESTARTS   AGEfrontend-v1-0-snapshot-cbdbdb68b-rbrrx   1/1       Running   2          6dreactive-sandbox-74fd955ddd-cjpw8        1/1       Running   7          6d$ kubectl get servicesNAME                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGEfrontend                         ClusterIP   10.103.128.78    &lt;none&gt;        10000/TCP                       6ditem                             ClusterIP   10.109.117.169   &lt;none&gt;        10000/TCP,10001/TCP,10002/TCP   6dkubernetes                       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                         6dreactive-sandbox-cassandra       ClusterIP   None             &lt;none&gt;        9042/TCP                        6dreactive-sandbox-elasticsearch   ClusterIP   None             &lt;none&gt;        9200/TCP                        6dreactive-sandbox-kafka           ClusterIP   None             &lt;none&gt;        9092/TCP                        6dreactive-sandbox-zookeeper       ClusterIP   None             &lt;none&gt;        2181/TCP                        6d$ kubectl get ingNAME       HOSTS     ADDRESS   PORTS     AGEfrontend   *                   80        6ditem       *                   80        6d$ kubectl get deployNAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEfrontend-v1-0-snapshot   1         1         1            1           6dreactive-sandbox         1         1         1            1           6dThere is currently no equivalent for Maven to deploy everything to Minikube in one simple command.In the next section however, we will look at how we can utilise the reactive-cli tool for generating resource and configuration files, and deploying everything to Kubernetes which is also the way to go for development in this case.Generating Kubernetes resources for productionThe following workflow could also be used for development but it is more suited for deploying to your production environment as the deploy minikube workflow in the previous section simplifies a lot of things for you.We will make use of the reactive-cli command-line tool to have it generate the Kubernetes resource and configuration files.Installing is easy, as described in the documentation.For Mac for example we can do this using Homebrew:brew tap lightbend/tools &amp;&amp; brew install lightbend/tools/reactive-cliVerifying if it was installed correctly can be done by checking the version:$ rp versionrp (Reactive CLI) 1.1.1jq support: UnavailableNow we can utilise it to generate Kubernetes resources.In the previous section we deployed our frontend and item service but we also have our order service to deploy.Let us generate the Kubernetes resources and deploy it to Minikube.$ rp generate-kubernetes-resources \"orderimpl:1.0-SNAPSHOT\" --generate-services --generate-pod-controllers --pod-controller-replicas 2 --env JAVA_OPTS=\"-Dplay.http.secret.key=simple\"---apiVersion: \"apps/v1beta2\"kind: Deploymentmetadata:  name: \"order-v1-0-snapshot\"  labels:    appName: order    appNameVersion: \"order-v1-0-snapshot\"spec:  replicas: 2  selector:    matchLabels:      appNameVersion: \"order-v1-0-snapshot\"  template:    metadata:      labels:        appName: order        appNameVersion: \"order-v1-0-snapshot\"    spec:      restartPolicy: Always      containers:        - name: order          image: \"orderimpl:1.0-SNAPSHOT\"          imagePullPolicy: IfNotPresent          env:            - name: \"JAVA_OPTS\"              value: \"-Dplay.http.secret.key=simple\"            - name: \"RP_APP_NAME\"              value: order            - name: \"RP_APP_TYPE\"              value: lagom            - name: \"RP_APP_VERSION\"              value: \"1.0-SNAPSHOT\"            - name: \"RP_DYN_JAVA_OPTS\"              value: \"-Dakka.discovery.kubernetes-api.pod-namespace=$RP_NAMESPACE\"            - name: \"RP_ENDPOINTS\"              value: \"HTTP,AKKA_REMOTE,AKKA_MGMT_HTTP\"            - name: \"RP_ENDPOINTS_COUNT\"              value: \"3\"            - name: \"RP_ENDPOINT_0_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_0_BIND_PORT\"              value: \"10000\"            - name: \"RP_ENDPOINT_0_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_0_PORT\"              value: \"10000\"            - name: \"RP_ENDPOINT_1_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_1_BIND_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_1_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_1_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_2_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_2_BIND_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_2_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_2_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_BIND_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_BIND_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_HTTP_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_HTTP_BIND_PORT\"              value: \"10000\"            - name: \"RP_ENDPOINT_HTTP_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_HTTP_PORT\"              value: \"10000\"            - name: \"RP_JAVA_OPTS\"              value: \"-Dconfig.resource=rp-application.conf -Dakka.discovery.method=kubernetes-api -Dakka.management.cluster.bootstrap.contact-point-discovery.effective-name=order -Dakka.management.cluster.bootstrap.contact-point-discovery.required-contact-point-nr=2 -Dakka.discovery.kubernetes-api.pod-label-selector=appName=%s\"            - name: \"RP_KUBERNETES_POD_IP\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_KUBERNETES_POD_NAME\"              valueFrom:                fieldRef:                  fieldPath: \"metadata.name\"            - name: \"RP_MODULES\"              value: \"akka-cluster-bootstrapping,akka-management,common,play-http-binding,service-discovery,status\"            - name: \"RP_NAMESPACE\"              valueFrom:                fieldRef:                  fieldPath: \"metadata.namespace\"            - name: \"RP_PLATFORM\"              value: kubernetes          ports:            - containerPort: 10000              name: http            - containerPort: 10001              name: \"akka-remote\"            - containerPort: 10002              name: \"akka-mgmt-http\"          volumeMounts: []          command:            - \"/rp-start\"          args:            - \"bin/orderimpl\"          readinessProbe:            httpGet:              path: \"/platform-tooling/ready\"              port: \"akka-mgmt-http\"            periodSeconds: 10          livenessProbe:            httpGet:              path: \"/platform-tooling/healthy\"              port: \"akka-mgmt-http\"            periodSeconds: 10            initialDelaySeconds: 60      volumes: []---apiVersion: v1kind: Servicemetadata:  labels:    appName: order  name: orderspec:  ports:    - name: http      port: 10000      protocol: TCP      targetPort: 10000    - name: \"akka-remote\"      port: 10001      protocol: TCP      targetPort: 10001    - name: \"akka-mgmt-http\"      port: 10002      protocol: TCP      targetPort: 10002  selector:    appName: orderYou could store the generated resources and tune it, but it is also possible to just generate everything that is necessary and just deploy it right away by chaining kubectl apply:$ rp generate-kubernetes-resources \"orderimpl:1.0-SNAPSHOT\" --generate-services --generate-pod-controllers --pod-controller-replicas 2 --env JAVA_OPTS=\"-Dplay.http.secret.key=simple\" | kubectl apply -f -deployment.apps \"order-v1-0-snapshot\" createdservice \"order\" createdWe can verify whether it is up and running:$ kubectl get deploymentsNAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEfrontend-v1-0-snapshot   1         1         1            1           6dorder-v1-0-snapshot      2         2         2            2           6mreactive-sandbox         1         1         1            1           6d$ kubectl get podsNAME                                     READY     STATUS    RESTARTS   AGEfrontend-v1-0-snapshot-cbdbdb68b-mwfqq   1/1       Running   1          50morder-v1-0-snapshot-5884754595-65wxd     1/1       Running   0          4morder-v1-0-snapshot-5884754595-wdbng     1/1       Running   0          4mreactive-sandbox-74fd955ddd-cjpw8        1/1       Running   7          6dConclusionUpgrading to Lagom 1.4.x and Play 2.6 went pretty smooth as the migration guides of Lightbend cover mostly everything in detail as usual.The orchestration tools make it pretty easy to test your Lagom application running in Kubernetes locally while still having the possibility to fine-tune the generated resource and configuration files.Integrating the tool suite into our project went smoothly.Kubernetes has gained a lot of popularity lately and with this, Lagom shows that it wants to embrace Kubernetes to deploy your applications onto next to ConductR.The single deploy minikube command is not yet supported on Windows but we imagine that it will be in the near future.Windows users can still utilise the reactive-cli command-line tool to generate the resource and configuration files and deploy it themselves via kubectl on their Minikube.Maven users will only need to wait a little bit longer to take advantage of most things the tool suite has to offer as the plugin is nearing its first release version.Extra resources  Our Lagom Shop Scala application GitHub repository  Lightbend Orchestration Documentation  Kubernetes instructions for the Lagom online-auction-scala application  Hello-World for Lightbend Orchestration for Kubernetes for a sample Play application  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom 1.2: What’s new?  Lagom Documentation  Kubernetes Documentation"
      },
    
      "agile-2018-04-30-scrum-vs-kanban-html": {
        "title": "Scrum VS Kanban",
        "url": "/agile/2018/04/30/scrum-vs-kanban.html",
        "image": "/img/agile/scrum-vs-kanban/boxers.jpg",
        "date": "30 Apr 2018",
        "category": "post, blog post, blog",
        "content": "Scrum VS Kanban  Scrum and Kanban are both agile frameworks that are used to deliver software, or any other sort of project for that matter, in an incremental and iterative way. Both terms have become incorrectly interchangeable. Although there are many similarities between the two, understanding the key differences between both Scrum and Kanban will bring clarity.The basicsAs with all agile frameworks, we have a product owner, a development team and an intermediate person, known in Scrum as the Scrum Master and in Kanban he or she is known as the Agile Coach.All three work together to get the most value to a client in the shortest amount of time.  Work is divided into stories and those stories are put on a product backlog. So far, so good, both Scrum and Kanban do the same at the very basic level. The way both handle these stories is where the difference lies.ScrumA Scrum team works in Sprints, a Time-boxed period in which the development team delivers a new iteration of the product.Most commonly these Sprints last 2 weeks, although 3 or even 4 weeks Sprints aren’t a rare sight.Each Sprint start with a Sprint planning meeting. In this meeting the development team, Scrum Master and Product Owner decide which stories the development team will tackle during this Sprint. These selected stories are known as the Sprint Backlog. This backlog should not change during the Sprint.On a daily basis in the Sprint, there is a Daily Standup meeting, a 15-minute standup meeting in which every member of the development team answers three simple questions: ‘What have I done yesterday’, ‘What will I do today’ and ‘Are there any problems that block me in going forward with my task’.After a Sprint there are two rituals; the Sprint review meeting, where all stakeholders are invited to a demonstration of new functionalities added to the product;and the Sprint Retrospective meeting, in which the development team discusses what went well, what went wrong and what could be improved in the next Sprint.The goal of the retrospective is to improve the way of working in the team in comparison to the previous Sprint.Some pitfalls of ScrumLet it be clear that Scrum isn’t as easy as it seems, Scrum has its pitfalls.Most of these can be avoided by making these issues clear from the start of a project.Some examples are:The Product owner not being available for questions of the teamWhen a Product Owner is always in meeting, or otherwise engaged, it could happen that the development team has to wait longer for answers on their questions, and by waiting for answers blocking development.Extending the Sprint with just a couple of daysSome teams will try to delay the end of a Sprint by a couple of days, just to make sure everything is ‘Done’. Every item that is not ‘Done’ at the end of a Sprint goes back into the Product Backlog.No exceptions. Extending it by just a couple of days would create a desync in the rhythm and possibly create a desync with another team that depends on your team.Changing the Sprint lengthChanging the Sprint length during a project is just not done. It will disrupt the team’s flow and the velocity of the team will go down. Once you’ve chosen a Sprint Length, your team should stick to it.Not having the same Sprint rhythm as another teamIn larger projects, teams will be dependent on one another. Not having the same Sprint rhythm will cause issues because you’ll have to wait for the other team to be ready for you, or vice versa, which would once again block progress.The Scrum Master being too involvedA Scrum master is a dedicated role in the team.It is very hard to be a Scrum master and develop at the same time. He or she should be like a firefighter, be idle at times, but be ready when an emergency occurs.Trying to solve problems within a Daily StandupThe Standup has to be a short time boxed meeting. Answering the questions and going forward. If there are problems, they should be handled before or after the Standup, not during.KanbanKanban is a continuous process. There isn’t a Sprint backlog. Only the columns on the board and the prioritized product backlog.The Kanban Board is crucial within Kanban. It is usually split into three columns: To Do, Doing and Done. More columns can be added to fit your needs but those three have to be there.To limit the work that can be done there is a Work in Progress limit on every column on the Kanban board which is related to the team’s capacity. For example, a team of four developers would be able to handle 6 to 8 stories at the time. The lower the limit, the faster work will pass through the board. Having it too low though will result in a bottleneck situation.Kanban has a Daily Standup meeting, a 15-minute standup meeting in which every member of the development team answers three simple questions: What have I done yesterday, what will I do today, are there any problems that block me in going forward with my task.Demo meetings are held at regular intervals.During these meetings all stakeholders are invited to a demonstration of the new functionality since the last demo.Kanban also has retrospective meetings, which have the goal to improve the way of working of the team and solve any issues that have risen since the last retrospective.In Kanban there are multiple ways you can do retrospectives.One way would be to do them on a regular basis (bit Scrum like). Although this doesn’t fit in the Kanban way of working,this provides a welcome disruption of the daily workflow and if conducted well the retrospective will result in a morale boost for the team since they have been listened to and know problems they have will be handled or have been handled during the retro.Since having retrospectives on a regular basis kind of goes against the way of working in Kanban, a simpler way of holding retrospective has been popping up the ‘Stop and Solve‘. This means that every time there is an issue (that the person who encountered it can’t solve on their own), an immediate retrospective involving the full team is called. Everyone looks into the problem, and brainstorms for possible solutions until a solution is found.There are pros and cons on both ways of working; what is chosen depends on the team.Some pitfalls of KanbanLike with every framework, Kanban has its pitfalls. Some examples:Kanban board not being up-to-dateOne of the most important pieces of Kanban is the Kanban board, if that is not up-to-date it will result in the team not having a up-to-date view of what needs to be done.You would be looking at the past.Not keeping to the WIP limitHaving too much tasks in one column will reduce the overview the team has, and tasks will get ‘lost’.A solution would be to implement physical limits to the board by drawing a line.Is there a Holy Grail?The answer to that question is quite simply: No.It all depends on the situation you’re in. Scrum would not work for teams that do not have clear business requirements. What we mean by that is a systems team or a helpdesk team. Kanban would be way more useful for these sort of teams; high priority tickets appear on the backlog and are handled as fast as possible by the team.An advantage of Scrum compared to Kanban is that the business will know what will be delivered during the next Sprint.Kanban is more dependent on the incoming requests.As said before; it all depends on the situation you’re in. Analyze the situation before deciding which agile framework will work best for your team!"
      },
    
      "spring-2018-04-28-spring-cloud-contract-meet-pact-html": {
        "title": "Spring Cloud Contract, meet Pact",
        "url": "/spring/2018/04/28/Spring-Cloud-Contract-meet-Pact.html",
        "image": "/img/2018-04-28-Spring-Cloud-Contract-meet-Pact/post-image.jpg",
        "date": "28 Apr 2018",
        "category": "post, blog post, blog",
        "content": "CDCT, or Consumer-Driven Contract TestingConsumer-driven contract tests are actually integration tests that are targetting your API, whether it’s REST-based or messaging-based.Imagine you’re working on an application that exposes its data using a REST API.Another team is using your exposed data for some functionality that they are providing.In order to guarantee that the functionality of the other team their application doesn’t break if we make changes to our API, we create a contract between the two teams.Key in this setup is that the contracts are defined by the consumer of your API instead of the developer that wrote the implementation of a certain functionality.With this aproach we can generate tests by using those consumer-driven contracts, and verify whether we’re going to break any of our consumers’ applications.You can find a couple of interesting and useful links on the bottom of this post which go further into detail.Spring Cloud Contract provides us with JVM-based libraries, allowing us to generate Groovy contracts, package them as a jar, and upload them to an artifact repository like Nexus or Artifactory.Great, but that means we can only use these contracts between two parties that are using a JVM language.What if the consumer of our API is a NodeJS or .NET microservice, or even an Angular application?For consumers or producers written in other languages, the lack of library support might be a concern.With the latest Spring Cloud release, that’s a thing of the past!Spring Cloud Contract, meet PactThe first release candidate of Spring Cloud Finchley, which was released on the 25th of April, also ships the first release candidate of Spring Cloud Contract.Our first release candidate of the Spring Cloud Finchley release train has been released.  Checkout the blog post for more information and as always feedback is welcome! https://t.co/8TK0tudkzr&mdash; Spring Cloud (@springcloud) April 25, 2018Spring Cloud Contract has updated its support for Pact and added the support to connect to a Pact broker.The Pact broker acts as a repository for sharing the Pact contracts and verification results.The most awesome feature in my opinion is the visualisation of the contracts between all known parties.In the UI you can see the last time there was a new version of a contract published, when it was verified and what status it had, and last but not least the details of the contract.                  List of contracts between parties with the timestamps and status                        Details of a contract between two parties                        A graph showing the dependencies between all known consumers and producers      Simply add the spring-cloud-contract-pact dependency, which will add the Pact Converter and Pact Stub Downloader, together with some configuration and you’re good to go!The Pact contracts will be retrieved from the Pact broker and converted to Spring Cloud Contract contracts for you, so these in turn can be used to generate stubs and tests.It’s also possible to write Spring Cloud Contract contracts, convert them to Pact contracts and upload them to the Pact broker.The upside to it is that these contracts are in fact language agnostic as opposed to Spring Cloud Contract, the downside to it is that it lacks some functionality regarding messaging compared to Spring Cloud Contract.Currently Pact contracts up until v4 are supported, which means that both request-response and messaging contracts can be used.Note that not all functionality is supported though, as is described in this section of the reference documentation.A typical workflow to define such contracts could look like this:  The consumer of the data writes a test for the feature by doing TDD.In this test the contract is defined which will be used as a stub.  Next up the missing implementation is written.  When we push our changes to Git our CI pipeline is triggered, in which we’ll upload all defined contracts to a central artifact repository, eg. a Pact broker, after all our tests have been run.  After our code’s been pushed, we’ll clone the producer’s application.  We create a test that uses the newly added CDC and file a pull request.  The team that works on the producer’s application can take over the pull request and add changes where necessary.In case you want to go the extra mile: you can also use the same approach for consumer-driven changes within the producer’s application!If you want a new REST endpoint or a message that will be delivered through some messaging system like Kafka or RabbitMQ, you can add an initial implementation of the missing functionality in the pull request.The other team then only needs to review the changes and add some adjustments if needed.Both teams can really benefit from such workflow: the consumer’s team doesn’t need to wait until the producer’s team has time to create the implementation, while the producer’s team can keep their focus on what’s on their product backlog.Useful linksThe three-second or three-minute tour give you a nice very brief walkthrough of Spring Cloud Contract.Consumer Driven Contracts and Your Microservice Architecture by Marcin Grzejszczak (@mgrzejszczak) and Adib Saikali (@asaikali).A step-by-step guide to Consumer Driven Contracts with Spring Cloud Contract.Implementation of the consumer driven contract library Pact for Javascript, from creating a test to generating the contract and uploading it to the Pact broker."
      },
    
      "spring-2018-04-16-spring-boot-2-0-anniversary-meetup-html": {
        "title": "Spring Boot 2.0 Anniversary Meetup",
        "url": "/spring/2018/04/16/spring-boot-2-0-anniversary-meetup.html",
        "image": "/img/spring-boot-2/051-thumb.jpg",
        "date": "16 Apr 2018",
        "category": "post, blog post, blog",
        "content": "  On March 1st, Spring Boot reached GA on its second major version.To celebrate this, we invited Spring Boot legend Stéphane Nicoll to give us an in-depth view on what’s new in Spring Boot 2.He talked about the new features while migrating a Spring Boot 1.x application to Spring Boot 2.0.Spring Boot 2.0Stéphane gave us an overview of the new features in Spring Boot 2.0.It was kind of a summary of Phil Webb’s announcement post Spring Boot 2.0 goes GA.Code says more than a thousand words.And like every talk I attended from Stéphane, he started live coding quite quickly.We migrated a Spring Boot 1.x application to Spring Boot 2.0.The migration process is very simple.In short, these are the steps you have to follow:  Change Spring Boot parent version number in your pom.xml  Replace deprecated property keys with the help of the spring-boot-properties-migrator module  If you’re working with passwords, define a PasswordEncoder  When you use a lot of hookpoints and Spring Boot classes directly, eg. SpringBootServletInitializer, migrating is slightly more work.The reason behind this is that a lot of the Spring Boot API’s have changed.Change version numberChange the Spring Boot Starter Parent version number in your pom.xml to the new Spring Boot version.&lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;&lt;/parent&gt;As from April 5th, this is the 2.0.1.RELEASE.Stéphane released it on the train to the Ordina HQ in Mechelen.Spring Boot 2.0.1 available nowhttps://t.co/WpYx0Pwff8@springboot @springcentral @ProjectReactor&mdash; Stéphane Nicoll (@snicoll) 5 april 2018Property keys and the properties migratorProperty key changesWhen you upgrade, you will get compilation errors saying there are unknown properties in your properties files.Some property keys have been deprecated and won’t work anymore.            Old property      New property                  spring.datasource.initialize      spring.datasource.initialization-mode              endpoints.health.path      management.endpoints.web.path-mapping      All major IDEs, eg. IntelliJ, Netbeans and STS, will inform you about the newer property key.Properties migratorYou can add the spring-boot-properties-migrator module to your Maven project.&lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-properties-migrator&lt;/artifactId&gt;  &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;The Spring Boot Properties Migrator looks for configuration properties it can replace and provides the programmer with feedback.For example, endpoints.health.path will automatically be replaced by the new management.endpoints.web.path-mapping property.Spring Boot 2.0 will also tell you you’re using an old key when you provide old keys as environment variable.It will not tell you the line on which you defined the property, but it will give an appropriate message.Some properties cannot be fixed automatically:  The security auto-configuration is no longer customizable  Replacement key 'spring.datasource.initialization-mode' uses an incompatible target type — String was replaced by an enumerationSpring Security changesAs from Spring Security 5.x, a lot of security features were redesigned.Some features are made more strict.Since Spring Boot 2.x now uses Spring Security 5.x under the hood, the Spring Security autoconfiguration has been redesigned as well.Previously with Spring Boot 1.x, you could have Spring Security configuration spread accross your application.With Spring Boot 2.0, if you want to know what the security configuration of your application is, you only have to look at one file, your SecurityConfiguration class.No password encoderWhen you haven’t defined a PasswordEncoder bean, Spring will throw the There is no PasswordEncoder mapped for the id \"null\" error when creating your application context.As from Spring Security 5.x, Spring Security enforces you to use a password encoder.Spring Security enforces this by activating the default DelegatingPasswordEncoder, which looks for PasswordEncoder beans.By adding a BCryptPasswordEncoder, the DelegatingPasswordEncoder will return that instance to encrypt passwords.@Beanpublic BCryptPasswordEncoder passwordEncoder() {    return new BCryptPasswordEncoder();}  If you really want to, you can override password encoding by adding {noop} to the password value.This will treat the password by activating the NoOpPasswordEncoder instead of the default DelegatingPasswordEncoder and will treat your password as plain text.Please note that this is not recommended if you deploy your app to a production environment!Actuator endpointsSome Actuator security endpoint settings aren’t modifyable by properties anymore.For example, management.security.roles=HERO won’t be picked up anymore.Only two endpoints are being exposed by default, /info and /health.You can include or exclude management endpoints by using the new property management.endpoints.web.exposure.include.            Old property      New property                  endpoints.health.path      management.endpoints.web.path-mapping      Other enhancementsConfiguration processorWhen you add the spring-boot-configuration-processor Maven module to your project, your IDE will be able to interpret your @ConfigurationProperties class and autocomplete the properties files.There were some other enhancements too.You can now use the Duration type for properties directly.private Duration delay = Duration.ofSeconds(3)The hello.delay autocompletion now also shows the unit behind the value, eg. hello.delay=3s.Spring Boot Dev Tools enhancementsDevTools is a feature in Spring Boot which adds nice development features.Spring created its own small LiveReload server, with a reload function.The communication protocol of LiveReload is open source.When you start the app, it also starts the LiveReload server.Something in your application watches the classpath for changes.If you change a template or a configuration property, Spring Boot will pick up the change, restart the Spring context and notify LiveReload.The restart only takes about 1 to 3 seconds, because the JVM is still hot.Spring Boot Dev Tools is not a new feature from Spring Boot 2, but the Spring developers have added some enhancements.One of those new features is that you get a delta of what changed and what triggered the LiveReload functionality.MicrometerIn Spring Boot 1.x, there was a metrics system with which you could register gauges etc.There’s an Actuator endpoint with which you could view those metrics.You could export those metrics to Prometheus or some other system.Micrometer is comparable to what SLF4J is for logging.You get an API for metrics that is independent of any vendor.You can record values and expose those values with a registry system to the outside world.&lt;dependency&gt;    &lt;groupId&gt;io.micrometer&lt;/groupId&gt;    &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;    &lt;version&gt;1.0.2&lt;/version&gt;&lt;/dependency&gt;For all projects that are supported by Micrometer, the metrics are exposed, eg. HikariCP.Spring Boot detects that you use HikariCP and automatically exposes those metrics to the different registries.To get more into detail we recommend you to check out this post, which covers Micrometer more in-depth.Q&amp;AHow should I convince my manager to upgrade?Because of some security issues in previous versions, Pivotal will stop providing support for those version.Another reason to upgrade is if you’re going to add new features to an application, eg. add metrics.When will Spring Cloud be ready to support Spring Boot 2?You can check http://start.spring.io/actuator/info in the spring-cloud section to see whether there are Spring Cloud versions which use Spring Boot 2.{  \"Angel.SR6\": \"Spring Boot &gt;=1.2.3.RELEASE and &lt;1.3.0.RELEASE\",  \"Brixton.SR7\": \"Spring Boot &gt;=1.3.0.RELEASE and &lt;1.4.0.RELEASE\",  \"Camden.SR7\": \"Spring Boot &gt;=1.4.0.RELEASE and &lt;=1.4.999.RELEASE\",  \"Edgware.SR3\": \"Spring Boot &gt;=1.5.0.RELEASE and &lt;=1.5.11.RELEASE\",  \"Edgware.BUILD-SNAPSHOT\": \"Spring Boot &gt;=1.5.12.BUILD-SNAPSHOT and &lt;2.0.0.M1\",  \"Finchley.M2\": \"Spring Boot &gt;=2.0.0.M3 and &lt;2.0.0.M5\",  \"Finchley.M3\": \"Spring Boot &gt;=2.0.0.M5 and &lt;=2.0.0.M5\",  \"Finchley.M4\": \"Spring Boot &gt;=2.0.0.M6 and &lt;=2.0.0.M6\",  \"Finchley.M5\": \"Spring Boot &gt;=2.0.0.M7 and &lt;=2.0.0.M7\",  \"Finchley.M6\": \"Spring Boot &gt;=2.0.0.RC1 and &lt;=2.0.0.RC1\",  \"Finchley.M7\": \"Spring Boot &gt;=2.0.0.RC2 and &lt;=2.0.0.RC2\",  \"Finchley.M9\": \"Spring Boot &gt;=2.0.0.RELEASE and &lt;=2.0.0.RELEASE\",  \"Finchley.BUILD-SNAPSHOT\": \"Spring Boot &gt;=2.0.0.BUILD-SNAPSHOT\"}Does Spring Boot 2 support Java 10?Yes.However, the Java 10 build step for Spring Boot 2.0.1 failed after Stéphane released it on the train to Mechelen.Eric De Witte posted a tweet about it during the meetup.I see @concourseci is being used pervasively at @pivotal I.e. @springboot tests against java 8, 9 and 10 pic.twitter.com/o2Z5tqX4ka&mdash; Eric De Witte (@vEDW) 5 april 2018Stéphane tweets the day after that the Java 10 build was fixed.Does @springboot 2 support Java 10?A picture is worth a thousand words pic.twitter.com/5jpG27gJnE&mdash; Stéphane Nicoll (@snicoll) 6 april 2018The meetupThe Spring Boot 2.0 Anniversary Meetup event started at 18h at the Ordina headquarters in Mechelen.Since you cannot learn on an empty stomach, we started off with Belgian French fries.Stéphane’s presentation started at 19h.He took us on a two hour ride through the Spring Boot landscape.After the presentation, a huge celebration cake was carried in.Our bakery had done its best to transform the Spring Boot logo into an immense nice looking cake.We think he did a good job!Check out Stéphane’s tweet.Hey @Lifeatordinabe, thank you so much for organizing this event and that @springboot 2 celebration cake was delicious! 🤗Looking forward to seeing the pictures pic.twitter.com/MvNhncLneI&mdash; Stéphane Nicoll (@snicoll) 5 april 2018A professional photographer took some atmospheric photos of the event.You can see all pictures on Elke.photos.Useful linksThe event was streamed live on YouTube.Subscribe to our channel for more Java- and JavaScript related videos.You can also consult the Spring Boot 2.0 Migration Guide for more information."
      },
    
      "node-js-2018-04-10-complete-introduction-to-nodejs-html": {
        "title": "Complete Introduction to NodeJs",
        "url": "/node.js/2018/04/10/complete-introduction-to-nodejs.html",
        "image": "/img/complete-introduction-node.png",
        "date": "10 Apr 2018",
        "category": "post, blog post, blog",
        "content": "What Is nodeNodeJs is a program that let’s you run JavaScript code on your machine without the need of a browser.Underneath the surface of node is the V8 JavaScript runtime which is the engine that allows your browser to run JavaScript code.On top of that, node adds some extra functionality to create server side applications(for example fs to interact with the file system, http or https to send and receive http calls, net for tcp streams, and many more).Use cases:Real time applications (chat, stocks, IoT)The event based nature of NodeJs and ‘keep-alive’ connections makes it ideal for real time applications, whenever an event occurs,for example a chat message being received or a stock price being updated, it can emit an event on its connected sockets to update the client’s chat screen or stocks chart.REST APIsThis will be a topic on its own, but with frameworks built on top of NodeJs like Express or Nest it is really easy to get a REST API up and running in no time at all.Serverless:NodeJs is supported with almost any serverless provider (Amazon Lambda, Azure functions, Google Cloud functions, …).So developers can focus on their code and business logic instead of maintaining and setting up complicated server architectures.File uploading: When writing applications that depend a lot on network access and accessing files on the disk we have to keep an eye on how the data is being transferred back and forward.For ultimate efficiency, especially when dealing with large sets of data, we need to be able to access that data piece by piece.When that happens, we can start manipulating that data as soon as it arrives at the server.Instead of holding it in memory until all chunks have arrived and writing it to disk, node can for example create a writable stream on the disk and write the chunks directly to the files without keeping them in memory and without blocking the entire application.This way it can also receive multiple files at the same time.Benefits of Javascript across the stackNot only does it make development quite a bit faster and easier by having a large community with lots of reusable code for your application (npm).It also lowers the barriers between frontend and backend developers by using the same programming language over the entire stack.So more efficiency and faster, leaner development which in turn means lower development costs.Also worth noting is that JavaScript is currently THE most popular programming language According to StackOverflow,so more developers will be able to easily understand and contribute to the application if needed.Another important criteria: when it comes to cloud hosting,RAM is probably the main influencing factor when it comes to pricing and since node is designed and encourages developers to write programs to use as less memory as possible it is often a cheaper alternative.MultithreadingThis is usually a big issue/talking point when it comes to node.In short: each NodeJs process is single threaded. If you want multiple threads, you have to have multiple processes as well.You could say that because of that, NodeJs encourages you to implement microservices when dealing with these larger and complicated applications.Which is a good thing since it makes not only your entire application but also each process individually very scalable.The downside is that this might introduce some added complexity to your application.But with Node’s lively modular ecosystem (npm) you can imagine there are already solutions to make setting this up a lot easier, (i.e. Moleculer, Seneca, …).An important characteristic of microservices is “shared nothing”.Node has a shared-nothing model:  A shared-nothing architecture (SN) is a distributed-computing architecture in which each node is independent and self-sufficient,and there is no single point of contention across the system. More specifically, none of the nodes share memory or disk storage.The advantages of SN architecture versus a central entity that controls the network (a controller-based architecture) include eliminating any single point of failure,allowing self-healing capabilities and providing an advantage with offering non-disruptive upgrade.A shared-nothing architecture (SN) is a distributed-computing architecture in which each node is independent and self-sufficient,and there is no single point of contention across the system. More specifically, none of the nodes share memory or disk storage.The advantages of SN architecture versus a central entity that controls the network (a controller-based architecture) include eliminating any single point of failure, allowing self-healing capabilities and providing an advantage with offering non-disruptive upgrade.(src: Wikipedia)Additionally, node has some other features to make use of multiple cores like for example the cluster:a single instance of NodeJs runs in a single thread. To take advantage of multi-core systems, the user will sometimes want to launch a cluster of NodeJs processes to handle the load.The cluster module allows easy creation of child processes that share server ports and automatically load balances across these processes.Blocking vs. Non-BlockingAs we’ve said before, NodeJs encourages you to take advantage of non-blocking code (like JavaScript promises).To demonstrate how this works, I’ll give you an example in pseudo code for reading a file from the filesystem.Blocking:    read file from filesystem,    print content    do something else`Non-Blocking:    read file from filesystem        Whenever we're complete, print contents (callback)    do something elseDifference:When reading two files, the blocking code starts reading the file.In case of a large file, let’s say this takes 5 seconds.After the file has been read, it logs its content. Then it starts reading the second file which again takes around 5 seconds and the content gets logged.In the non-blocking code, we tell the processor to start reading the file, and when it’s done, to “let us know” (resolve promise) so that we can do more stuff with it.At the same time since there is another file to be read, we start reading the second file and again tell the processor to notify us when it is ready so that we can do stuff with it.Whenever a ‘Promise’ of reading a file resolves, its callback (in our case, the pseudo code: print contents) gets executed(this also means that, when file #2 takes less time to be read, it will be resolved and printed first which is something you might want to keep in mind).V8 Runtime EngineNode uses Google Chrome’s V8 runtime engine to run JavaScript code, we’ve shown this video in one of our previous blog posts before,but since it might be useful to know how it works under the hood I’ve included the video once more.When it comes to node development there are some differences, since we don’t get events from the DOM.In node we can get them from the NodeJs event emitter but the way it works stays the same.It has some useful tips like avoiding to block the call stack.Philip Roberts: Help, I'm stuck in an event-loop.Installing NodeJsDownload the installer for your OS at https://nodejs.org/en/download/.Or, if you are a Mac user and have brew installed you can install it with brew.Open a terminal and run the following commands:Install nodebrew install nodeVerify if node was successfully installed (should output your node’s version number)node -v Node ModulesPreviously I talked about one of the benefits of node being its vast ecosystem of open source code that you can exploit.To avoid having to write the same common logic over and over again, node’s greatest feature is probably its modularity.You can put common logic in a node module that you can reuse over and over again in different components of your projects or even reuse them in other projects.NPMNPM or ‘Node Package Manager’ is an online registry for node modules. When you’ve written a useful module, why not share it with the world.Whenever you’ve implemented some common logic that can be reused across projects, it is a common practice in the world of JavaScript development to make it Open Source and share it with other developers who might want to implement the same logic.This way they don’t have to write it themselves which saves times and headaches.Just like using other people’s modules might do for you.Here are some useful npm commands to get you started:npm init  Start a new project. This creates a package.json file that keeps track of the installed modules (if you save them).npm install &lt;module_name&gt; Downloads a module that is registered (by name) on the npm registry.To see which modules modules are available, simply visit npmjs.com and search for whatever you need.The downloaded code will be saved in the node_modules directory. (Unless global install)npm install &lt;module_name&gt; —save  installs and also updates your project’s dependencies in package.jsonnpm install &lt;module_name&gt; —save-dev  installs and updates your project’s development dependencies in package.json (dependencies that you don’t need at runtime, i.e. testing frameworks like Jasmine or Karma, build frameworks like gulp or webpack, …)npm install &lt;module_name&gt; --global  installs the package globally, packages with command line interfaces like gulp-cli, angular-cli are installed globally.npm uninstall  uninstalls packages from your projectnpm update  updates your packagesFor a full overview of npm commands and further documentation of npm, check out this page: https://docs.npmjs.com/Node REPLOnce you have installed node you can open a terminal window and run node.This will return a node REPL where you can run JavaScript code. For example:function add(a, b){ return a + b } &lt;enter&gt;add(4, 7) &lt;enter&gt;// Returns 11To terminate the REPL hit CTRL + C.Hello WorldUsing the REPL can be useful sometimes, but when we want to make some persistent programs,we might want to write our code in a file and run the content of that file.In this example we’ll create a helloworld.js file.Create it in your favourite IDE or run touch helloworld.js and open it in your IDE or Vim/nano/…To keep it simple for this first project, we’ll simply make a program that logs ‘hello’ and ‘world’ in your terminal,we’ll log them separately just to show you that there’s different ways to log data with node.helloworld.js// process is a global variable that refers to the current node process you are running,// it has a stdout property that has a write method which we can call to output data.process.stdout.write('hello\\n'); // the \\n creates a new line in the terminal// or a bit simpler, the one we are used to from the browser, console.logconsole.log('world');Hello World AsyncAn example of non-blocking code// after 2 secs, print worldsetTimeout( ()=&gt; { console.log('world'); }, 2000);console.log('hello\\n');// prints  'hello' first, then 'world'When you’ve watched the video about how the V8 Engine works, you’ll know why ‘hello’ gets logged first and ‘world’ second:What happens is:  setTimeout is added to the call stack  setTimeout has a timer and a callback, this fires up  V8’s timer Web API  now that the Web API is taking care of the setTimeout, it gets removed from the call stack  console.log is added to the call stack, it logs ‘hello’, and removes console.log from the call stack  once the timer has completed, it pushes the callback to the task queue  since there are no more function calls on the call stack, the event loop adds the callback to the call stack  finally, ‘world’ is printed.There’s no need for an additional thread to pause the program for 2 seconds and after that log ‘world’.The V8 Engine handles this for us just like it does with any other async functionality in the browser.So this is a very simple example of how non-blocking code works in NodeJs, the timeout did not block our code, ‘hello’ got logged right away.Hello ModuleNow lets give you an example how to create a node module (a very simple and not a very useful one) but just to give you an idea of how you can export your code and use it in other files.We’re going to create a module that has a log function which takes a parameter (name) and logs ‘hello ' to the console.Then we'll import it in another file and call the function from there.log.js// create our custom hello functionconst log = (name) =&gt; { console.log(`Hello ${name}`); }// export this functionalitymodule.exports = log;hello.js// the way we import another module into our file is by using require(), require is a global module for node// when requiring local modules (not the ones we install with npm), we give it the path relative to the current file,// no need for extensions, since node looks for a .js filevar log = require('./log');// now that we have our functionality available in the log variable, we can use itlog('Mike');// logs 'hello Mike' to the consoleTo import modules that you’ve installed with npm, don’t specify a path, but give it the package’s name.For example:// this imports the express module from the express package if we installed it`require('express');To learn more about how to use require go check out this useful url: Requiring Modules.Hello ServerNow that we know how to require other modules, let’s create a basic server application.We’re not going to install any server frameworks (like Express) yet, instead we’ll require a module that comes with node.Node has some built-in modules that you can use like http (HTTP server), https, fs (file system), net (TCP sockets), … (a list can be found here: https://www.w3schools.com/nodejs/ref_modules.asp)For this program, we’ll use node’s http module.hello-server.js// import the http module (docs: https://nodejs.org/api/http.html)const http = require('http');// we create a server that will send plaintext 'Hello World' back to the client and put it in a variable serverconst server = http.createServer((req, res)=&gt; {  res.writeHead(200, {'Content-Type': 'text/plain'});  res.end('Hello world\\n');});// we tell the server to start listening on port 3000server.listen(3000, ()=&gt;{    // once the server has successfully started listening on port 3000    // (if the port isn't already in use)    // we'll log this to the console    console.log('Server running at localhost:3000/');});You can now open our browser, visit localhost:3000 and should see our ‘Hello world’ response from the server.Event EmitterAnother great feature that comes with NodeJs is the event emitter.The event emitter allows us to emit and listen for named events,whenever the EventEmitter emits an event all the functions attached to the named event are called synchronously.It’s real simple, let us show you with an example:// first we require the 'events' module that comes with nodeconst events = require('events');// next we'll create a new instance of the events module's event emitterconst eventEmitter = new events.EventEmitter();// we'll tell the event emitter that we are going to listen for the 'hello' event// and give it a callback function that gets called when the event is triggeredeventEmitter.on('hello', (data) =&gt; {  // as you can see, our callback function accepts a data parameter,  // we'll check if the event was emitted with data and has a 'name' property, if so we log 'Hello name'  if (data != null &amp;&amp; data.name) {    console.log(`Hello ${data.name}`);  } else {    // if no data was passed to the callback, we'll simply log 'Hello world'    console.log('Hello world');  }});// now that we are listening for the 'hello' event, we'll emit the event, once with data, and once without dataeventEmitter.emit('hello', {name: 'Mike'}); // logs 'Hello Mike'eventEmitter.emit('hello'); // logs 'Hello world'StreamsThere are many ways that you can utilise readable/writable streams with NodeJs, for example the file system to read/write to files.But to give you a simple example, let’s reuse our code from our hello server.Since the request object is a readable stream and the response object is a writable stream,we can create an application that pipes the data from the request, back to the response.Let’s see it in action with an example:streams.jsconst http = require('http');const server = http.createServer((request, response)=&gt; {  res.writeHead(200, {'Content-Type': 'text/plain'});  // The request is a readable stream, this means that the connection isn't immediately closed,  // the connection stays open for as long as the client keeps sending data.  // Streams inherit from the event emitter, so we can listen for the request stream's 'readable' and 'end' events  // the readable event is triggered whenever the request has sent a chunk of data that can be read.  req.on('readable', ()=&gt;{      let chunk = null;      // as long as we can read chunks from the request, we write those chunks to the response      while(null !== (chunk = request.read())){         // we can keep writing to the writable response stream as long as the connection is open,         // so we keep piping the readable data to the response          response.write(chunk);      }      // you can test it with curl from the terminal by sending a 'hello' string as data,      // simply run: curl -d 'hello' http://localhost:3000      // the 'hello' string is being sent back to the client  });  req.on('end', ()=&gt;{      // once the request stream is closed, we also close the response stream      response.end();  });  /*\tI've written it out completely to show you what is does,\tbut in fact we can use the pipe method to refactor the request's 'on readable' in some much simpler code\treq.on('readable', ()=&gt;{             request.pipe(response)        })   */});server.listen(3000, ()=&gt;{    console.log('Server running at localhost:3000/');});ClusterA single instance of NodeJs runs in a single thread.To take advantage of multi-core systems, the user will sometimes want to launch a cluster of NodeJs processes to handle the load.// require the cluster moduleconst cluster = require('cluster');// we'll set up a http server on all cpus and load balance between themconst http = require('http');// we need to know the amount of cpus our machine has available,// so we do this with the 'os' module's cpus method which returns an array of cpus, to get the amount we get the array's lengthconst numCPUs = require('os').cpus().length;if (cluster.isMaster) {  // the cluster will first start up a master process that forks itself onto the other cpus and handles the load balancing between these workers  console.log(`Master ${process.pid} is running`);  // fork this process to a worker for every cpu that is left (note the &lt; and not &lt;=)  for (let i = 0; i &lt; numCPUs; i++) {    cluster.fork();  }  // when a worker dies, it emits an exit event  cluster.on('exit', (worker, code, signal) =&gt; {    console.log(`worker ${worker.process.pid} died`);  });} else {  // if the cluster is not master, it (in this case) sets up our http server  // workers can share any TCP connection  http.createServer((req, res) =&gt; {    res.writeHead(200);    res.end('hello world\\n');  }).listen(8000);  console.log(`Worker ${process.pid} started`);}More information on clusters on https://nodejs.org/api/cluster.htmlFinallySo that’s it for this blog post. I hope it was useful to you.If you have any suggestions or feel like I’ve forgotten to mention some important stuff, feel free to comment below.I’m currently working on some follow-up tutorials:  Building REST APIs with NestJs (TypeScript),  Microservices with NodeJs (Moleculer),  Serverless with NodeJsOnce these are finished I’ll add the links below, so stay tuned!"
      },
    
      "angular-2018-03-30-angular-security-best-practices-html": {
        "title": "Angular Security Best Practices",
        "url": "/angular/2018/03/30/angular-security-best-practices.html",
        "image": "/img/angular-security-best-practices.png",
        "date": "30 Mar 2018",
        "category": "post, blog post, blog",
        "content": "Angular Security Best Practices  Software security is a hot topic nowadays.We, web developers, need to be up-to-date with all latest security issues that we could encounter when developing a web application.In this blog we’ll check what kind of best practices we should have in mind when building an Angular app so we limit the amount of security issues we could have.Up-to-date Angular librariesThe angular team is doing releases at regular intervals for feature enhancements, bug fixes and security patches as appropriate.So, it is recommended to update the Angular libraries at regular intervals.Not doing so may allow attackers to attack the app using known security vulnerabilities present within older releases.1. Preventing cross-site scripting (XSS)XSS enables attackers to inject client-side scripts into web pages viewed by other users.Such code can then, for example, steal user data or perform actions to impersonate the user.This is one of the most common attacks on the web.1.1. Sanitization and security contextsTo systematically block XSS bugs, Angular treats all values as untrusted by default.When a value is inserted into the DOM from a template, via property, attribute, style, class binding, or interpolation, Angular sanitizes and escapes untrusted values.This is the declaration of the sanitization providers in the BrowserModule:export const BROWSER_SANITIZATION_PROVIDERS: Array&lt;any&gt; = [  {provide: Sanitizer, useExisting: DomSanitizer},  {provide: DomSanitizer, useClass: DomSanitizerImpl},];@NgModule({  providers: [    BROWSER_SANITIZATION_PROVIDERS    ...  ],  exports: [CommonModule, ApplicationModule]})export class BrowserModule {}The DOM sanitization serviceThe goal of the DomSanitizer is to clean untrusted parts of values.The skeleton of the class looks like this:export enum SecurityContext { NONE, HTML, STYLE, SCRIPT, URL, RESOURCE_URL }export abstract class DomSanitizer implements Sanitizer {  abstract sanitize(context: SecurityContext, value: SafeValue|string|null): string|null;  abstract bypassSecurityTrustHtml(value: string): SafeHtml;  abstract bypassSecurityTrustStyle(value: string): SafeStyle;  abstract bypassSecurityTrustScript(value: string): SafeScript;  abstract bypassSecurityTrustUrl(value: string): SafeUrl;  abstract bypassSecurityTrustResourceUrl(value: string): SafeResourceUrl;}As you can see, there are two kinds of method patterns.The first one is the sanitize method, which gets the context and an untrusted value and returns a trusted value.The other ones are the bypassSecurityTrustX methods which are getting the untrusted value according to the value usage and are returning a trusted object.The sanitize methodIf a value is trusted for the context, this sanitize method will (in case of a SafeValue) unwrap the contained safe value and use it directly.Otherwise, the value will be sanitized to be safe according to the security context.There are three main helper functions for sanitizing the values.The sanitizeHtml function sanitizes the untrusted HTML value by parsing the value and checks its tokens.The sanitizeStyle and sanitizeUrl functions sanitize the untrusted style or URL value by regular expressions.How can we disable the sanitization logic?In specific situations, it might be necessary to disable sanitization.Users can bypass security by constructing a value with one of the bypassSecurityTrustX methods, and then binding to that value from the template.An example:import {BrowserModule, DomSanitizer} from '@angular/platform-browser'@Component({  selector: 'my-app',  template: `    &lt;div [innerHtml]=\"html\"&gt;&lt;/div&gt;  `,})export class App {  constructor(private sanitizer: DomSanitizer) {    this.html = sanitizer.bypassSecurityTrustHtml('&lt;h1&gt;DomSanitizer&lt;/h1&gt;&lt;script&gt;ourSuperSafeCode()&lt;/script&gt;') ;  }}  Be careful: If you trust a value that might be malicious, you are introducing a security vulnerability into your application!1.2. Content security policy (CSP)Content Security Policy (CSP) is an added layer of security that helps to detect and mitigate certain types of attacks, including Cross Site Scripting (XSS) and data injection attacks.These attacks are used for everything from data theft to site defacement or distribution of malware.To enable CSP, configure your web server to return an appropriate Content-Security-Policy HTTP header.You can find a very detailed manual how to enable CSP on the MDN website.To check if your CSP is valid you can use the CSP evaluator from google.1.3. Use the offline template compiler (aka AOT-compiler)Angular templates are the same as executable code: HTML, attributes, and binding expressions (but not the values bound) in templates are trusted to be safe.This means that if an attacker can control a value that is being parsed by the template we have a security leak.Never generate template source code by concatenating user input and templates.To prevent these vulnerabilities, use the offline template compiler, also known as template injection.If you use the Angular CLI, it’s easy to enable AOT:ng build --aotng serve --aotMore info can be found on the Angular Guide website.1.4. Avoid direct use of the DOM APIsThe built-in browser DOM APIs don’t automatically protect you from security vulnerabilities.For example, document, the node available through ElementRef, and many third-party APIs contain unsafe methods.Avoid interacting with the DOM directly and instead use Angular templates where possible.1.5. Server-side XSS protectionInjecting template code into an Angular application is the same as injecting executable code into the application.So, validate all data on server-side code and escape appropriately to prevent XSS vulnerabilities on the server.Also, Angular recommends not to generate Angular templates on the server side using a templating language.2. HTTP-level vulnerabilitiesAngular has built-in support to help prevent two common HTTP vulnerabilities, cross-site request forgery (CSRF or XSRF) and cross-site script inclusion (XSSI).Both of these must be mitigated primarily on the server side, but Angular provides helpers to make integration on the client side easier.2.1. Cross-site request forgery (XSRF)Cross-site request forgery (also known as one-click attack or session riding) is abbreviated as CSRF or XSRF.It is a type of malicious exploit of a website where unauthorized commands are transmitted from a user that the web application trusts.In a common anti-XSRF technique, the application server sends a randomly generated authentication token in a cookie.The client code reads the cookie and adds a custom request header with the token in all subsequent requests.The server compares the received cookie value to the request header value and rejects the request if the values are missing or don’t match.This technique is effective because all browsers implement the same origin policy.Only code from the website on which cookies are set can read the cookies from that site and set custom headers on requests to that site.That means only your application can read this cookie token and set the custom header.Angular HttpClient provides built-in support for doing checks on the client side. Read further details on Angular XSRF Support.2.2. Cross-site script inclusion (XSSI)Cross-site script inclusion (also known as JSON vulnerability) can allow an attacker’s website to read data from a JSON API.The attack works on older browsers by overriding native JavaScript object constructors, and then including an API URL using a &lt;script&gt; tag.This attack is only successful if the returned JSON is executable as JavaScript.Servers can prevent an attack by prefixing all JSON responses to make them non-executable, by convention, using the well-known string \")]}',\\n\".Angular’s HttpClient library recognizes this convention and automatically strips the string \")]}',\\n\" from all responses before further parsing."
      },
    
      "kickstarters-2018-03-29-kickstarter-trajectory-2018-light-html": {
        "title": "Kickstarter Trajectory 2018 Light Edition",
        "url": "/kickstarters/2018/03/29/Kickstarter-Trajectory-2018-light.html",
        "image": "/img/kicks.png",
        "date": "29 Mar 2018",
        "category": "post, blog post, blog",
        "content": "The Ordina Kickstarter trajectory is a collection of courses tailored and designed by the senior consultants of Ordina.These courses are created to give the beginning software developer a broad knowledge base while also providing an in-depth view on several technologies and best practices.This year the Kickstarter trajectory spanned 15 days, with topics ranging far and wide: backend to frontend, Spring Data JPA to TypeScript and everything in between.All of these courses will make sure the candidates will be able to hit the ground running on their first project as Ordina consultants.This post will summarize the training and experiences we’ve had while following the Kickstarter trajectory of JWorks, Ordina’s Java/JavaScript unit.BacklogWhat we have doneIntro Kickstarter Trajectory &amp; Dev EnvironmentOn our first day we got a brief introduction to Ordina JWorks by Yannick De Turck. We have learned that JWorks consists of 10 competence centers: Agile &amp; DevOps, API &amp; Microservices, Application Security, Cloud Native Platforms, Continuous Integration &amp; Delivery, Big Fast Data, Internet of Things &amp; Machine Learning, Javascript &amp; Hybrid Mobile, JVM Languages and Software Architecture. Every competence center is responsible for organizing workshops and presentations. As an Ordina consultant it’s possible to contribute to one of these competence centers. For example by writing a blog post on the JWorks Tech Blog, writing an article on the JWorks Docs, or assisting with a workshop.We have received two books to read: Oracle Certified Associate Java SE8 (to obtain the Java 8 certificate) and Clean Code. These books are an absolute must for every junior developer!Yannick showed us the preferred stack of JWorks for the backend and frontend.Later we have seen that Ordina operates according to the secure-by-design principle which means that every consultant will be trained to achieve certifications in security. Security should be inside every consultant’s DNA.In the afternoon we have learned about the following topics:  Integrated Development Environments (IDEs) and how important it is to choose the right one  Build tools: Maven, Gradle and npm  Version Control System: Git  Continuous integration  Scrum and Agile  Improving your productivity as a developerJavaOn Tuesday we got our first learning session from Yannick De Turck.In the morning we talked about all the new features since Java 7.Especially streams in Java 8 and the var keyword in Java 10 caught my attention.Streams and lambdas were the biggest change in Java 8 and severely altered the way we write code, I believe var will do the same for Java 10. But it will take some time before we can use it in production environments of course,since Oracle doesn’t provide long-term support for versions 9 and 10.In the afternoon we did some fun exercises where we had to implement methods to make pre-made tests pass.Of course most of these were about Java 8 features because…GitDuring the second day of our kickstart trajectory we’ve got a brief explanation by Yannick again.He explained us that Git is an open-source, distributed version control system that keeps track of your files and history.Basically this means Git offers us tooling to collaborate on code bases without overwriting each others changes so easily.We saw which workflow JWorks uses in Git and which commands we can use to do so.This way we learned how to create Git repos and create separate branches for features or different releases.And we even saw the different ways to merge these branches.One thing we’ll definitely won’t forget that fast is to rebase when pulling to keep a clean non-spaghetti like history,something that is preferred by many co-workers at JWorks! At least that is what Yannick has told us. ;)Spring and Spring bootThe lectures on Spring and Spring Boot were given by Ken Coenen and were spread over two days.During the first lecture, we got a recap of the concepts of JPA, beans, application contexts and other things that Spring uses in its core fundamentals. Next, we dug deeper into the framework and introduced ourselves with Spring Web Services and Spring Security and created a small backend application during the second lecture.Both lectures were rather theoretical, but very informative and elaborate with a lot of examples. So you have got everything you need to get familiar with Spring. You can find the course material on Ken’s Github page.MicroservicesThe workshop on microservices was lectured by Kevin Van Houtte and powered by Spring Cloud and Netflix OSS. It went pretty fast and at the end of the workshop, we had acquired a great overview of all the important aspects regarding the microservices architecture! The most important thing to know is that each microservice takes care of only one specific business function.Currently, monolith architectures are still being used a lot within companies, but as an up-to-date IT consultant it’s essential to know about microservices and where to use it.We learned about the 12-factor app methodology, which defines all the important aspects of microservices architecture: codebase, dependencies, configs, backing services, etc.In a hands-on approach we learned how to create a microservice, how to register it in a service registry (using Eureka), how to externalize our configuration (using Spring Cloud), how to create routing logic (using Zuul) and finally how to test the implementation using the Feign HTTP client.Unit testing and mockingOn Thursday we got a course about testing from Maarten Casteels, who works as a consultant for Ordina at Belfius.  The first part of the day was a very passionate and interactive theory session about the following subjects:  Goals of testing  What to test  Fixtures  Mocks  AssertionsAfter the lunch break we did some exercises together that showed us how to mock out dependencies and which pitfalls we should pay attention to.This gave us a better understanding of the theory we saw that morning.All in all it was a great course explaining the big picture of testing but also showing us the ropes in day-to-day working with tests and code.The open atmosphere enabled us to ask a lot of questions which Maarten always answered thoroughly.Frontend EssentialsAt the end of our first week we went over some of the frontend essentialsbefore diving deeper into the frontend frameworks and build tools the next week.This workshop was given by Yannick Vergeylen.Our colleagues from the VisionWorks department accompanied us since they use the topics covered in this workshop as well.After a theoretical recap about HTML, CSS and JavaScript we learned how to use HTML to create web pages and how CSS is used to style these pages and its components. We also used some JavaScript and learned how it is used to modify some of the HTML-components.During the workshop we were given an exercise in which we had to recreate a given example page with the above technologies.This way we had some hands-on experience straight away!Build ToolsWe started the second week with a solid introduction of frontend build tools. The topics of this workshop (given by Michael Vervloet) were Node.js, package managers and build systems &amp; generators (gulp, webpack and Angular CLI). After every topic we got the chance to put this newly acquired knowledge into practice. This started from scratch by installing Node.js and at the end we created an Angular project.AngularOne of the must see frontend frameworks is Angular of course.We’ve been introduced to it by Ryan De Gruyter.Ryan did a very good job and gave us a good base to get started with Angular.He taught us what Angular components are and how we can display data inside these components with the different types of data-binding.We also saw how we can let these components communicate with each otherand pass data from child components to its parent component and vice versa.On top of that, we saw how Angular directives are used to loop over objects to show multiple elements. And how we can use the *ngIf directive to hide/show elements and many more of these directives.But that’s not all, he also taught us about modules, services, dependency injection and much more.It was a very educational session for sure.Ryan did a good job on giving us some theoretical information about the different parts of Angular.After each theoretical part we made some exercises.And the cool thing about it?All these parts combined we made ourselves a small crypto currency listing application with real data!    DevOps &amp; CI/CDWe learned that developers should share the responsibility of looking after the system they build. And not just hand the release over to operations. The development team can also simplify the deployment and maintenance for the operation team. This could be done by introducing a DevOps culture. Yes, it’s not a role. It’s a culture. We have learned that DevOps aims to remove the isolation between operations and software developers by encouraging collaboration. It should also be easier to change your code and push it to production by using Continuous Delivery (CD).Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. We have learned how we can configure a CI tool. We had the chance to have some hands-on experience with GoCD.This workshop was given by Tim Vierbergen.SecurityNowadays, security is a hot topic and it’s important to handle sensitive (personal) information in a secure manner.Because it’s not only a PR nightmare for your business, it’s also a financial disaster because of GDPR that will take effect this May 2018. This fascinating lecture was presented by Tim De Grande on our last day of the Kickstarter trajectory.We discussed basic security fundamentals, common web vulnerabilities, and tips and tricks on how to secure your own applications.DockerWe started with a recap of the theory behind creating images and spinning up containers.Soon after, we were creating our own images and learned how to run our applications in a container. On the way we experienced the advantages of Docker and how it fits nicely in the processof CI/CD. Thanks Tom Verelst for guiding us into the Docker world!RecapNick: “Ordina has given me the chance to increase my knowledge by involving me in the Kickstarter program.         It was great to learn about the top-notch Ordina stack from our own experts.        This is exactly what I was looking for.         The Kickstarter program just ended and I’m eager to start using my knowledge in an enterprise environment again.        Ordina also provides plenty of other learning opportunities.         Since I arrived, I could join an interesting seminar or workshop every week.”Sam: “The opportunity you get at Ordina to learn from experienced developers is something you can’t miss.        The Ordina Kickstarter trajectory is the perfect example of how it should be.        You’re presented some of the newest technologies from really kind, helpful and experienced developers.        It’s the perfect program to get you started in your career as developer!”Dries: “As a backend developer with a few years experience I followed the program to see where I had gaps in my knowledge.         Thanks to the great atmosphere and experienced teachers, I was able to fill those gaps.         The sessions were very interactive which enabled me to ask any question, they were always met with well-founded answers.         The courses on JS and Angular also sparked my interest in frontend work, which will be very useful in my further career.”Maarten: “The courses were tons of fun and taught me a lot.          All of our teachers were competent individuals who made sure we learned as much as we could during the time we had.          I didn’t have any experience in frontend development, so the frontend courses were an eye-opening experience.          To anyone that’s having doubts about this Kickstarter trajectory: go for it!          I can definitely recommend it!”Johan:  “I’m really eager to learn more about new technologies and the Kickstarter course suited my needs. It challenged me in a good way on both a personal and technical level. Ordina really knows how to kick start newcomers into the astonishing world of technology.”Yunus: “Ordina gave me the opportunity to put my academic knowledge in practice and learn about the latest technologies. I’ve learned the best practices from our seniors during this intensive Kickstarter trajectory. Every graduate needs to have participated in a kickstart trajectory like Ordina’s, it’s the best way to start your career.”Yen: “The Kickstarter program at Ordina really got me fast on track with the latest technologies being used in the field. I’m fresh out of school where I had a focus on backend and here at Ordina this got greatly improved upon. It was also interesting to get in depth on frontend tech! You notice all the coaches are experienced programmers and it was a privilege to learn from them. And if you need any help after a workshop, they are always quick to help. To summarize, I really recommend this Kickstarter program to accelerate your career!”About Ordina’s kickstarter trajectoriesInterested in knowing more about Ordina’s kickstarter trajectories?More information is available on Ordina’s website."
      },
    
      "conference-2018-03-15-secappdev-2018-html": {
        "title": "SecAppDev 2018",
        "url": "/conference/2018/03/15/SecAppDev-2018.html",
        "image": "/img/secappdev-2018/secappdev_wide.png",
        "date": "15 Mar 2018",
        "category": "post, blog post, blog",
        "content": "  Last February I was able to attend the 2018 edition of SecAppDev.It’s a training/security conference in Leuven that lasts a week and which hosts top-notch speakers.It’s organised by Philippe De Ryck of imec-DistriNet, KU Leuven.SecAppDev for me was a week filled with learning, I’ll recap a few of the sessions I attended in this post.Security model of the web - Philippe De RyckThe most basic security control of the modern internet is the ‘Origin’.This was thought up over 20 years ago and, at the time, was adequate for its purpose.Nowadays however, origin is a poor security constraint: we load scripts from CDNs, we include frames from other providers, …Because of this, more security controls have been bolted on in the last years.In this talk, Philippe De Ryck explored some of these.We learned how to use X-Frame-Options and Content Security Policy (CSP) settings to limit who can include our pages in a frame.Next up he explained how to limit the power of other sites which you might need to frame in yours using the sandbox attribute, which was introduced in HTML5.Once you’ve limited what the frame can do, you can open up communications between your page and the frame through the Web Messaging API.Once you’re past frames, we come to scripts.Nowadays, we load scripts from all over the place, often knowing nothing more than a name.These scripts run within the context of your page and can do everything the current user can do.To make sure these scripts aren’t tampered with you’d ideally investigate them first and then use subresource integrity (SRI) to make sure they aren’t modified.Most CDNs nowadays offer this as a service: they provide you with the correct hashes for the scripts they host.That does mean you need to trust your CDN to host a non-malicious file at the time you include it.After a quick look at CSP, we came to the cookies.As we all know, cookies are not the best solution: they’re sent over both HTTP and HTTPS and they can be read and modified by (malicious) scripts.This allows for some interesting attacks like session hijacking and session fixation.An attempt was made to fix this through Secure and HttpOnly flags.Recently a new spec tries to restrict cookie behavior based on prefixes: __Secure- and __Host-.Because browsers send your cookies on all requests to your domains, this opens you up to an attack called Cross Site Request Forgery (CSRF).We discussed a few methods that can be used to mitigate this risk: hidden form tokens, “transparent” tokens, checking the origin header and samesite cookies.The session ended with a look at JSON Web Tokens (JWT).Contrary to popular belief, these represent data, not a transport mechanism.It’s perfectly fine to store a JWT in a cookie, rendering the whole cookie vs. tokens debate a bit useless.Putting your token on an Authorization header, does protect against CSRF, but introduces some other complexities.OWASP’s top 10 proactive controls (Jim Manico)In this session we had a quick look at version 2 of the OWASP proactive controls.These are the things every developer should do in order to harden their code.The full list has 10 items, but because of some very interesting discussions, we only managed to cover the first 5.1. Verify for security early and oftenThis is not an easy thing in today’s DevOps world as code is deployed to production a lot more often.Etsy deploys more than 25 times per day, while Amazon manages a deploy every 11.6 seconds!Make sure that security testing is part of the build process, doing that ensures that your security testing actually happens.There are several tools available that can help you out here (e.g. OWASP ZAP or Nessus) and you can combine them for increased coverage.Make sure you don’t end up on the “hamster wheel of pain” where you focus on the specific bugs they reveal, rather than the class of bugs.2. Parameterize queriesWe should all know by now that queries should never be built using string concatenations.Use parameterized queries instead to prevent SQL injections.Use parameters for everything: not just the user-supplied input, but configurations and hard-coded values as well.This can give you a performance boost as well, since parameterized queries are compiled by the database only once and then reused.3. Encode data before use in a parserThe best known vulnerability here is Cross Site Scripting (XSS).Allowing someone to inject HTML tags in your HTML pages gives them nearly unlimited power over your application.Make sure to encode all user input before feeding it to a parser (a browser is basically a very powerful HTML parser) to prevent these kinds of issues.For Java applications, you can use the OWASP Java encoder project to handle your HTML encoding.They also have tools available for other languages (.NET, PHP, …).4. Validate all inputsAnd don’t just do it client-side.Client-side validations are easily bypassed, so you need to repeat them server-side as well.If your users need to be able to post HTML, you need to sanitize it.For that you can use the OWASP HTML Sanitizer.Keep in mind that even valid data can cause issues: ' OR '1'='1'; -- is a perfectly valid password, and the Irish people will be grateful that you allow the use of ' in name fields.What about files?You also need to do this if your users are allowed to upload files.Files create even more risks: you need to make sure that the uploaded files are safe.First validate the file name, file type and decompressed size (preferably before decompressing).Run it through a virus scanner on a separate machine to protect against exploits against your antivirus.For images, you need to enforce size limits and you’ll want to verify that you’re actually dealing with an image.The easiest way to do that is to rewrite the image (e.g. using ImageMagick).Once again, you want to do this on a separate machine to prevent malicious images to take over your application.5. Establish authentication and identity controls.  Don’t limit the password (within reason). Don’t enforce arbitrarily short passwords or limit the type of characters that can be used.You do want to limit the length, if only to prevent DOS attacks, but 100+ characters shouldn’t be an issue.  Check the chosen password against a list of the 100k most common chosen passwords  Use a strong, unique salt.Each credential should have its own salt, and don’t skimp on the length.64 or 32 characters (depending on the hashing algorithm) should be the norm.  Impose a difficult verification on both attacker and defender.Use a hashing algorithm that’s appropriate, such as PBKDF2, scrypt or bcrypt.Alternatively, you could use HMAC-SHA-256( [private key], [salt] + [password] ) to only make it hard on the attacker.However, this introduces a lot more complexity in your system.Other authentication best practices should also be applied, such as two factor authentication, a proper lockout policy, …A practical introduction to OpenID Connect &amp; OAuth 2.0 (Dominick Baier)Dominick Baier gave a very interesting talk on OpenID Connect and OAuth 2.0.An important distinction he made at the start is the difference between a user and a client.Users are people (carbon based life forms) while the word “client” refers to applications (or silicon based life forms).OAuth2.0 is a protocol meant for client authentication while OpenID is the successor to SAML (and as such meant to authenticate users).OAuth is not meant for user authentication, even though it’s commonly (ab)used for that through various incompatible, proprietary extensions.OpenID ConnectOpenID Connect piggy backs on OAuth2.0.It adds support for logging out and key rotation.More importantly, it’s an open standard and it publishes a list of certified implementations.Compliance with the spec is guaranteed through a set of tests.EndpointsAn OpenID Connect server (or token service) has to implement a set of endpoints:  A discovery endpoint to discover where the other endpoints are.  An authorize endpoint (for users)  A token endpoint (for machine to machine processes)Discovery endpointAn example of a discovery endpoint is at https://accounts.google.com/.well-known/openid-configuration.It returns an unsigned JSON document: for security OpenID Connects relies entirely on HTTPS.The issuer must be the URL where the document is located.Authorize endpointThis endpoint handles authentication for web applications and is found in the authorization_endpoint field of the discovery endpointThe client (in this case the browser) makes a request to the authorize endpoint and passes along a few required parameters:  The callback url: the token service will verify that this url is allowed and perform a callback to this url after the user is logged on.  A nonce (number used once) which will be echoed to the client so it can verify server responses.  And a scope which needs to include openid.The server will then authenticate the user and show a consent dialog.This dialog shows the logged in user, the application that requests access and the access that’s being requested.When the user allows this request, the token service sends  response to the client containing a JWT based identity token as well as a cookie.This means that the token service will remember the user for future logon requests to other applications.Identity token validationWhen you use an identity token to authenticate to an application, the application needs to validate this token.It does this by making sure that:  The issuer name matches the value of the iss claim  The aud must contain the client-id that was used to register the application.  The proper signing algorithm must be defined in alg.  The current time must be before exp  If the token is too old (as defined in iat or “issued at”), it can be rejected  nonce must match what client sent  And you need to validate the signature. For that you check the kid field in the header and use find that key in the document you find at the jwks_uri field from the discovery endpoint.Session managementSince the token service places a cookie in the user’s browser, this means that you have one logon session active.When you access another application that uses the same token service, it just needs to show you the consent dialog, without asking you to log in again.This is called “Single Sign On” (SSO).OpenID Connect also supports “Single Sign Out”.When you log out of the token service (by calling the /end_session endpoint), it will try to sign you out from all applications.It support three different ways of doing this:Javascript based notificationIn order to use this, your application should always contain a specific iframe.The source of this iframe is defined in the check_session_iframe field of the discovery config.This frame is loaded in the same origin as the token service and it will do a JS call to the parent page to log out.Front-channel notificationEven though the spec calls this a “best-effort approach”, it’s still the method that’s most common.It requires each client to implement a clean-up endpoint.When the user logs out, the token service will render an HTML page that containing an invisible iframe for each client.These iframes will call the clean-up endpoints.Normally, these iframes will contain the session id in the url to prevent “signout spam”.Otherwise it would be too easy for a malicious site to add an image to their pages signing you out of your sessions, causing a DOS.The reason this approach is “best-effort” is that the browser might not be able to call all endpoints before the user navigates away from the log out page.Back-channel notificationThis is the safest option, as it guarantees that the user will be signed off from all applications.Unfortunately it’s also the most complicated to implement.In this method, the token service will call a server-endpoint on all client applications.This means that the application server will need to implement the clean up endpoint.Besides that, you also need to be sure that a network connection is possible between the ID provider and all application servers."
      },
    
      "iot-2018-03-14-stairway-to-health-2-html": {
        "title": "Stairway to Health 2.0 (the Ordina version)",
        "url": "/iot/2018/03/14/Stairway-To-Health-2.html",
        "image": "/img/stairwaytohealth2/banner.jpg",
        "date": "14 Mar 2018",
        "category": "post, blog post, blog",
        "content": "Harder, Better, Faster, StrongerHere we are again, another blog post about Stairway to Health.Why? Well, we’ve created our own Ordina version of the Stairway to Health application.There are quite a few interesting bells and whistles, among others, here are a few of the new features:  New (and awesome) frontend design, with Ordina theming obviously  Upgraded from Angular 4 to Angular 5  Material Design  Nest.js in stead of Express.js (still Express underneath, but cleaner code!)  Backend e2e tests with Mockgoose  Deployed on OpenShift  New type of sensors  Cheers feature, users can motivate and support each otherStairway to Health @ OrdinaAs you might have read in our previous post about Stairway to Health, the purpose of the application is to improve worker health in a fun and engaging way. With the app we try to encourage employees to take the stairs instead of the elevator.We’ve put up some sensors that can detect how much the stairs are used on a per floor basis and how many people take the elevator.In the app they can see the results and thus they can do an extra effort if they are falling behind.New in the Ordina version is that employees can now also cheer and motivate each other since we’ve added a chat feature to the application.Internet of ThingsThe Stairway to Health project is a simple yet great example to show what the Internet of Things can do:  LoRa sensors detect door openings, these are installed on the doors of the staircases  These sensors communicate via the LoRa network to report their status  In our case, sensor data is sent to the Proximus MyThings platform which processes the data  The data gets sent to the Stairway to Health application  The Stairway to Health application interprets and visualises the dataIn summary: We install sensors on the doors (things) to measure usage and we analyse the data to persuade people to move more.The result is a good example of how IoT can influence our daily lives.For more on this topic, check the application’s About pageDive into the technical detailsThe reason of us writing this blog post is mainly because we want to explain some of the technical changes and improvements we’ve madesince we’ve updated (pretty much rewritten) the application. So let’s get started.The APIExpressJs to Nest.js: The main difference here is that we’ve rewritten the application to use the new framework inf favour of the old implementation with ExpressJs.Migrating from Express to Nest is not that difficult, since Nest is a wrapper on top of the Express framework.It provides you with some nice TypeScript decorators which makes your code a lot cleaner, more compact and easier to read.ExpressJs exampleexport class EntityApi extends CoreApi {    private entityController: EntityController = new EntityController();    constructor() {        super();    }    // the create function would that have to be executed by the main server while bootstrapping the application    public create(router: Router) {        router.get( '/auth/entities',                    this.authenticate,                    this.requireAdmin,                    (req: Request, res: Response, next: NextFunction) =&gt; {                        this.entityController.getEntityList(req, res, next);                    });    }}NestJs example// automatically registered to the server by nest// all /auth routes require user to be logged in (doesn't come standard with Nest)@Controller('/auth/entities')@UseGuards(RolesGuard)export class EntitiesController {    constructor(private readonly entitiesService: EntitiesService) {}    @Get('/')    @Roles('admin')    async findAll(): Promise&lt;IEntity[]&gt; {        return await this.entitiesService.findAll();    }}Websockets with NestJsWorking with sockets is also a lot easier and cleaner when using Nest.We can utilise the @WebSocketGateway to create a new route/gateway, @SubscribeMessage to listen for certain events and @OnGatewayConnection or @OnGatewayDisconnect to know when users connect or disconnect to the server.There wasn’t any straight forward solution for broadcasting to all clients. Once a user sends a message, we want to update the messages for everyone that has the client open. So we solved this by pushing all connected clients to an array and when we receive a ‘cheer-created’ event, we loop over the array of clients and emit an event to them one by one.import {\tWebSocketGateway, SubscribeMessage, OnGatewayConnection, OnGatewayDisconnect,\tWsResponse} from '@nestjs/websockets';@WebSocketGateway({namespace: 'events/cheers'})export class CheerEventsComponent implements OnGatewayConnection, OnGatewayDisconnect {\tpublic clients = [];\tconstructor() {\t}\thandleConnection(client: any) {\t\tthis.clients.push(client);\t}\thandleDisconnect(client) {\t\tfor (let i = 0; i &lt; this.clients.length; i++) {\t\t\tif (this.clients[i].id === client.id) {\t\t\t\tthis.clients.splice(i, 1);\t\t\t\tbreak;\t\t\t}\t\t}\t}\t@SubscribeMessage('cheer-created')\tonEvent(): WsResponse&lt;void&gt; {\t\tthis.broadcast('cheer');\t\treturn;\t}\tprivate broadcast(message: string) {\t\tfor (let c of this.clients) {\t\t\tc.emit(message);\t\t}\t}}Optimising chart data and countsOn Stairway to Health we used mongo aggregations to get our chart data from the database. Once we hit 1.5 million logs, these calls put a lot of stress on our servers and took a long time to load, so in stead we now keep track of daily, weekly, monthly, yearly and total logs in their own collection.Whenever we receive a log from the MyThings stream we update all these collections. For example the daily logs collection contains documents that look like this:{\"date\": {    \"$date\": \"2017-12-20T21:49:15.532Z\"},\"friendlyName1\": \"C\",\"friendlyName2\": \"1\",\"hour\": 22,\"identifier\": \"20-12-2017\",\"counts\": 55}So when we want the hourly data from a certain day, we query the collection for the date we want and and simply return an array with all the different hours, if an hour doesn’t exist, we assume it didn’t send any logs/counts.When we receive a log, we check if there is an entry that has “date” and “hour” equal to the log’s date. If so, we update, otherwise we create a new entry (upsert).We still store the log in a “logs” collection, so that if ever our daily, weekly, … collections get corrupted, we can run a script that populates these collections with the correct data.async create(log: ILog, stream?: boolean): Promise&lt;ILog&gt; {    try {        // We insert the log into our logs collection        let item = await this.logModel.create(log);        // the identifiers so we can easily query for them        let dailyIdentifier = `${item.day}-${item.month}-${item.year}`;        let weeklyIdentifier = `${item.week}-${item.year}`;        let monthlyIdentifier = `${item.month}-${item.year}`;        let yearlyIdentifier = `${item.year}`;        // sensors send all their containers to us, we only need to update the collections        // if they are 'counters' and they have a numeric value        if (item.container === 'counter' &amp;&amp; item.numericValue) {            // update all collections            // by putting them in a variable, they all get executed without having to wait for each one to complete,            // and we have no 'callback hell', below te do a Promise.all so that we know when they are all done.            let dailyCountPromise = this.dailyCountsModel.update({                identifier: dailyIdentifier,                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                hour: item.hour            }, {                // increment, not overwrite the counts                $inc: {counts: item.numericValue}            }, {                // upsert makes sure that if the entry we try to update doesn't exist, we create one                upsert: true            });            let weeklyCountPromise = this.weeklyCountsModel.update({                identifier: weeklyIdentifier,                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                day: item.day            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            let totalCountPromise = this.totalCountsModel.update({                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            let yearlyCountPromise = this.yearlyCountsModel.update({                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                month: item.month,                identifier: yearlyIdentifier            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            let monthlyCountPromise = this.monthlyCountsModel.update({                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                week: item.week,                identifier: monthlyIdentifier            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            // once all collections are updated, we emit a 'stream-received' event,            // which will reload the charts on the client application            Promise.all([ dailyCountPromise,                          weeklyCountPromise,                          totalCountPromise,                          yearlyCountPromise,                          monthlyCountPromise]).then(() =&gt; {                              if (stream) {                                  socket.emit('stream-received');                              }                          }, (err) =&gt; {                            console.log(err);                          });        }        return item;    } catch (error) {        throw new HttpException(error.message, HttpStatus.BAD_REQUEST);    }}The Visible partsThe main changes we’ve made on the frontend are:  Changing the colours, we created a dark theme with Ordina branding  Used material design for a smoother user experience  Replaced Highcharts library with @swimlane/ngx-charts  Migrated to Angular 5                                                                                                            Since users should now be able to register to the application to cheer for and motivate each other we added these new screens and functionality.                                                                        Deploy on OpenShiftSince we’ve separated our frontend and backend code we used 2 separate Git repositories. The nice thing about deploying to OpenShift is that we can add a webhook to GitHub so that every time we merge a pull request from our develop branch to ourmaster branch to our Git remote, it builds and deploys the new code immediately.    The new sensors: Proximus MySenseFor the previous version of Stairway to Health we used Magnetic door sensors,these use a magnet mounted on the door frame and the sensor mounted on the door itself, when the door is closed the magnetmakes contact with the sensor and the sensor detects the door is closed. This means you need to mount at two places,and it needs to be carefully placed to align. This makes it not an ideal solution.A solution for this is the MySense sensor. This is a LoRa sensor programmable with JavaScript.The MySense is a small LoRa device containing multiple sensors.It contains a temperature sensor, a button, …But the most important sensor for our case is the accelerometer.Using the accelerometer we can detect when the door is moving. After detecting a motion we will blackout the sensorfor 30 seconds to allow the door to be closed again and not count multiple motions.To save battery we do not send on every motion,but count the amount of motions for 15 minutes and then send the counter,also when the counter is 0 we will not send to save battery.ConclusionWe made some major improvements when it comes to performance, maintainability and functionality.By deploying our application to OpenShift, we also improved our workflow and made it a lot easier to deploy our changes.By using the MySense as our sensor we only have to mount one piece per door. An extra advantage is that this sensor is a lot cheaper.Interesting Links  Stairway to Health 2.0  Blogpost Stairway to Health 1  Nest.js  OpenShift"
      },
    
      "security-2018-02-12-hpkp-deprecated-what-now-html": {
        "title": "HPKP is deprecated. What now?",
        "url": "/security/2018/02/12/HPKP-deprecated-what-now.html",
        "image": "/img/security/padlock_code.jpg",
        "date": "12 Feb 2018",
        "category": "post, blog post, blog",
        "content": "HPKP is deprecated. What now?  Recently Google announced their intent to deprecate support for Public Key Pinning (HPKP).Let’s have a look at the reasons for this and what technologies we can use to replace it.Deprecated? Why?As mentioned in the previous blog post, HPKP carries some very strong risks.It only takes a small mistake to render your site completely inaccessible, but that’s only 1 of the reasons Google mentions for deprecating support for HPKP.The other risks they call out are that it’s hard to build a pin-set that’s guaranteed to work and the risk of hostile pinning.Hostile pinning hasn’t been observed yet, but it’s an attack that allows someone to take your site hostage should they somehow be able to obtain a valid certificate for your domain.Because of the first 2 reasons, adoption rates of HPKP have remained very low and browser vendors have been looking for a viable replacement.Expect-CTOne of the new headers thought up to replace HPKP is Expect-CT (Expect Certificate Transparency).This tells the browser to check the Certificate Transparency (CT) logs to make sure the presented certificate is properly logged.Certificate TransparencyCT is a project by Google that provides a framework for monitoring and auditing SSL certificates in (almost) real-time.One of the reasons for its existence is the 2011 hack of the Dutch CA Diginotar. This resulted in the hackers being able to issue more than 500 fake SSL certificates (including for sites like facebook.com and google.com).In turn, these certificates could then be used by the attacker to perform a Man-in-the-middle (MitM) attack against these sites, without alerting the user that anything fishy was going on.CT is a tool that allows you to detect when a fake certificate has been issued. When a CA participates in the program, it has to log all certificates they issue in a publicly searchable log.These logs are monitored by applications which can report to you whenever a new certificate for one of your domains is issued.If the certificate was issued in error (or maliciously), you can immediately take steps to have it revoked.How does Expect-CT help me out here?Expect-CT tells the browser that you only want it to trust certificates signed by CAs that have Certificate Transparency enabled.When the server presents a certificate that’s not issued by such a CA, the browser will reject it and display a warning to the user.If you combine these 2 points, you can see how this protects your users:  By monitoring the CT logs, you can quickly identify any fraudulent or misissued certificates for yur domains and have them revoked.  If the certificate is issued by a CT that doesn’t pop up in these logs, it’s simply rejected by the browser.How to monitor CT logsOf course the whole premise of this solution is that you actually monitor the CT logs for your domains.If you don’t do this, you’re still at risk of someone obtaining a fraudulent certificate and impersonating you.Fortunately, there are plenty of companies and tools out there that can help you out with this.  SSLMate offers an open source tool called Certspotter  If you don’t want to run it yourself, you can pay them to do it for you.  For smaller (personal) projects, you can use Facebook’s monitor.  Or you use one of the other APIs or services that are available.What’s important is that you get the reports quickly so you can immediately take action.Use the headerSince (to my knowledge) browsers don’t have psychic powers (yet?), you still need to tell it that you expect the CA to have CT enabled.For that you’ll need to add the Expect-CT header on your responses.Obviously it will only look for these on an HTTPS URL, since on a simple HTTP connection it can easily be added or removed by a MitM.The header looks like this:Expect-CT: enforce, max-age=31536000, report-uri=\"https://example.com\"This tells the browser to enforce the CT rule and to do so for the next year.Any infractions will be sent to the report-uri you mentioned.As with all headers that contain a report-uri, you can also use report-uri.io to aggregate these logs.As with most things that stand a chance of rendering your site inaccessible, it pays to be cautious when adding this header.Typically you don’t want to start by adding this header as defined above.Instead, you want to deploy it first without the enforce directive (and preferably a very low max-age such as 0)Doing so will tell the browser that you don’t want it to block connections with a bad certificate, but just to send the error to the report-uri.This setup allows you to test without impacting your users: you can now monitor this for a while to see if everything still works as expected.After that, enable the enforce directive and slowly increase the max-age to the point you want it to be.RisksThe risk of adding this header is quite low, if you follow the procedure above.You should only make sure that your CA actually uses CT.However, since October 2017 Chrome has made this a requirement in order for CAs to be in the trust-store.The main risk lies in not monitoring the CT logs properly. If you don’t monitor the alerts or don’t have a procedure to deal with misissued certificates, you’re still at risk of impersonation.CaveatsUnfortunately, there’s one major caveat to using this header.At the time of writing, only Chrome fully supports Expect-CT.Mozilla has also indicated that they will support it, but Microsoft so far doesn’t seem to be following suit.Should you use it?Yes. The risk is minimal, the only downside at the moment is the lack of browser support.At the very least, a large percentage of your users enjoys added protection against MitM attacks.Certificate Authority AuthorizationThe downside of Expect-CT compared to Public Key Pinning (HPKP) is that you need to make sure that your monitoring is handled correctly.If you don’t notice on time that a certificate has been issued, an attacker may be able to impersonate you for some time.You can make this a lot harder on the attacker by using Certificate Authority Authorization (CAA).CAA is a way for you to indicate exactly who is allowed to issue certificates for your domain.How to implement?Since the issuance of certificates is not limited to websites, CAA is not implemented through HTTPS response headers, instead it’s a record that you need to put in your DNS settings.You simply add the correct record to your DNS like this:            Name      Type      Value                  example.com.      CAA      0 issue \";\"        Note: the example above prevents all CAs from issuing certificates for your site. Don’t just copy-paste this.You can have multiple CAA records and the value of these tells the CA exactly what you want.Since this is a bit cryptic, lets look a bit more in detail at what’s happening here.The value above consists of 3 parts:  the flag (0)  the tag (issue)  the value (\";\")The combination of tag and value can be referred to as “the property”.The whole of CAA is governed by RFC 6844FlagsCurrently, flags can have 2 values: 0 or 128.A value of 0 means the property is non-critical, while a value of 128 means that is is critical.If a property is marked as critical, the CA must completely understand it before it proceeds.Generally it’s correct to use 0, so it’s advised to always use that value.There is support for customized flags in the RFC, but that’s beyond the scope of this post.TagsThe current specification has 3 tags you can define:  issue specifies which CA is authorized to issue certificates  issuewild indicates which CA is authorized to issue wildcard certificates (e.g. for *.example.com)  iodef similar to report-uri you can use this to get reports on invalid requests (either to an email address or to an http endpoint)issueThis tag specifies which CA is allowed to issue certificates for the domain and its subdomains.This includes the wildcard subdomain (meaning that the certificate would be valid for all subdomains).A value of “;” indicates that no issuance is allowed.You’re allowed to define multiple CAs, but you’ll need to use a new DNS record for each one:            Name      Type      Value                  example.com.      CAA      0 issue \"ca1.com\"              example.com.      CAA      0 issue \"otherca.net\"      issuewildThis one is used explicitly for wildcard certificates.If issuewild is present, any values in issue may not be used in the issuance of a wildcard certificate.You can use this in case you never want a wildcard certificate to be issued or when the list of CAs that are allowed to issue wildcard certificates differs from the original list.iodefYou can use this tag to report invalid certificate requests:            Name      Type      Value                  example.com.      CAA      0 iodef \"mailto:certificates@example.com\"              example.com.      CAA      0 iodef \"https://certificate.example.com/endpoint\"      As you can see, you can either have these reports sent by email, of have them delivered to an HTTP endpoint.The report is sent in the IODEF format, which also means that your endpoint needs to be RFC 6546 compliant.The easy wayTo help you in creating your CAA, SSL Mate has released a useful tool: CAA Record Helper.It can help you create a CAA record and will tell you how to set it up in your DNS service.RisksA badly implemented CAA record can mean that your CA is not allowed to issue your certificate.The other risk is that it relies on DNS: DNS records can be spoofed and this might allow an attacker to trick a CA into issuing a fraudulent certificate.Because of this, the RFC recommends implementing DNSSEC (Domain Name Security Extensions).Should I use it?I’d suggest you do. While having an incorrect policy can prevent the CA from issuing a certificate, this situation can be rectified quickly and shouldn’t put your users at risk.It will make it a lot harder for an attacker to obtain a certificate."
      },
    
      "docker-2018-02-12-azure-draft-html": {
        "title": "Azure Draft",
        "url": "/docker/2018/02/12/Azure-Draft.html",
        "image": "/img/2018-02-12-Azure-Draft/draft-logo.png",
        "date": "12 Feb 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Installing Draft  Setting Sail with Draft  ConclusionIntroductionDraft is an open-source tool from Microsoft Azure.It attempts to make the development for Kubernetes clusters easier, by getting Docker and Kubernetes out of the way.Developers no longer require Docker,and can just push their applications to a remote Kubernetes clusters using Draft.Draft accomplishes this by using only two simple commands.The first command is draft create.This tool detects the application language, and writes out a Dockerfile and a Kubernetes Helm chart in the source tree.These files are generated based on Draft “packs”.These packs are simple scripts that only detect the languageand write out the Dockerfile and Helm charts.The idea is based on some features of PaaS systems like the Cloud Foundry’s buildpacks.The only difference is that the build and deployment descriptors are stored in the source tree.The second command is draft up.First, all source code will be uploaded to any Kubernetes cluster, local or remote.Then, the application is built on the cluster using the generated Dockerfile.Finally, the built image is deployed to a dev environment using the Helm Chart.Draft does not support many languages yet, but it currently supports most of the popular languages like Java, Python, Golang, JavaScript, Ruby, Swift, PHP, C# and Clojure.It also has support for Gradle and Maven projects.You can see all packs here.Installing DraftBefore you can start using Draft,there are quite a few components that need to be set up.However, if you are using a remote Kubernetes cluster,you will only need to do the setup once for multiple developers.Other developers will only need to install the Draft client to benefit.For this example, we will be using Minikube, a local Kubernetes cluster.The total list of tools required is the following:  Minikube: a local Kubernetes cluster  kubectl: the CLI tool for working with Kubernetes  Tiller: the Helm agent running on the Kubernetes cluster which manages installations of your charts.  Helm: the Helm client  Draftd: the Draft agent running on the Kubernetes clusterLet’s get started!Downloading all dependenciesWe will start by installing the latest release of Minikube using Homebrew.If you do not have Homebrew,you can check how to install Minikube here.$ brew cask install minikube==&gt; Satisfying dependenciesAll Formula dependencies satisfied.==&gt; Downloading https://storage.googleapis.com/minikube/releases/v0.25.0/minikube-darwin-amd64==&gt; Verifying checksum for Cask minikube==&gt; Installing Cask minikube==&gt; Linking Binary 'minikube-darwin-amd64' to '/usr/local/bin/minikube'.minikube was successfully installed!After Minikube has been installed,we can install Azure Draft!First, we add the Azure Draft repository by adding a Homebrew tap.$ brew tap azure/draftNow that we have added the repository,we can install Draft!$ brew install draft==&gt; Installing draft from azure/draft==&gt; Downloading https://azuredraft.blob.core.windows.net/draft/draft-v0.10.1-darwin-amd64.tar.gz/usr/local/Cellar/draft/0.10.1: 5 files, 45.9MB, built in 1 secondIf you do not use Homebrew,you can download the latest release of Draft here.You will have to unzip the download and add it to your PATH manually.Starting MinikubeNow we have downloaded all required dependencies,we can start setting up our cluster.Let’s start our Kubernetes cluster.$ minikube startStarting local Kubernetes v1.9.0 cluster...Starting VM...Downloading Minikube ISOGetting VM IP address...Moving files into cluster...Downloading localkube binaryConnecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file.The cluster is up and ready. As you can see from the output,Minikube also configured our kubectl client by automatically creating a .kubeconfig file.$ kubectl cluster-infoKubernetes master is running at https://192.168.99.100:8443To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.Enabling the Docker RegistryTo be able to use our Draft agent on the server,we will need to enable the embedded Docker registry on the cluster.Minikube makes this straightforward using an addon.We only need to enable it!$ minikube addons enable registryregistry was successfully enabledInstalling HelmNow that we have our Minikube up and running,we can install the Helm server agent (Tiller) and the Helm client.$ helm init$HELM_HOME has been configured at /Users/tomverelst/.helm.Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Happy Helming!Even though Tiller is installed now,you must wait for it to be deployed.Wait until there is one instance ready!$ kubectl -n kube-system get deploy tiller-deploy --watchNAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEtiller-deploy   1         1         1            1           4mInstalling DraftAll requirements are set up now for Draft.Let’s install the final component: Draft!$ draft init --auto-acceptInstalling default plugins...Installation of default plugins completeInstalling default pack repositories...Installing pack repo from https://github.com/Azure/draftError: Unable to update checked out version: exit status 128Error: exit status 1Uh, oh! Seems like Git cannot clone the Draft pack repo.According to this GitHub issue,this happens with Git version 2.16+.If you have this error, the workaround currently is to manually add a specific version of the pack repo.$ draft pack-repo add https://github.com/Azure/draft --version v0.10.0Installing pack repo from https://github.com/Azure/draftInstalled pack repository github.com/Azure/draftWe manually installed the Draft pack repo now. Let’s try to set up Draft again.$ draft init --auto-acceptInstalling default plugins...Installation of default plugins completeInstalling default pack repositories...Installation of default pack repositories complete$DRAFT_HOME has been configured at /Users/tomverelst/.draft.Draft detected that you are using minikube as your cloud provider. AWESOME!Draftd has been installed into your Kubernetes ClusterHappy Sailing!Great.The workaround works! As you can see, Draft is still in alpha and will not properly work yet.This setup is of course for local development.If you want to have a production ready, RBAC enabled, Draft setup on a remote Kubernetes cluster,you can take a look at the Advanced Installation guide.Setting Sail with DraftIf you managed to get to this point,you either went through all the effort to set everything up,or you skipped to this part!We can now start drafting up some applications. Since I am a fan of Go, I will start with drafting up a Go application.Here is a simple Go application that listens on port 8080 and returns “Hello Draft!”.package mainimport (    \"fmt\"    \"net/http\")func handler(w http.ResponseWriter, r *http.Request) {    fmt.Fprintf(w, \"Hello Draft!\")}func main() {    http.HandleFunc(\"/\", handler)    http.ListenAndServe(\":8080\", nil)}Let’s run it to see if it works.$ go run main.go# Open a separate terminal$ curl localhost:8080Hello, Draft!The application works. Now we can let Draft create the Dockerfile and the Helm chart.$ draft create--&gt; Draft detected Go (100.000000%)--&gt; Ready to sail$ lsDockerfile\tcharts\t\tdraft.toml\tmain.go$ ls charts/goChart.yaml\tcharts\t\ttemplates\tvalues.yamlDraft detected that it was a Go application, It generated a Dockerfile and the Draft deployment descriptor,and it also copied the Go pack to the charts directory.This is great, as it enables the possibility to customize the pack for this specific application.Let’s take a look at the generated Dockerfile.$ cat DockerfileFROM golang:onbuildENV PORT 8080EXPOSE 8080The official Golang onbuild image is used.This image is great for development purposes,but I would not recommend using this image for production purposes,as it is around 700MB, while the application is only a few lines of code.For demo purposes, let’s continue to use this generated Dockerfile,and try to deploy our application on Kubernetes using Draft.$ draft upDraft Up Started: 'goapp'goapp: Building Docker Image: SUCCESS ?  (60.1681s)goapp: Pushing Docker Image: SUCCESS ?  (63.0775s)goapp: Releasing Application: SUCCESS ?  (0.5346s)goapp: Build ID: 01C653GK70A7SR2FMT2325TBHDBuilding and pushing this application took around 2 minutes,which seems pretty long,but that is highly likely because of the 700MB base Docker image.This image first needs to be downloaded.Then it needs to be pushed to the registry.We can connect to the application using draft connect.$ draft connectConnecting to your app...SUCCESS...Connect to your app on localhost:50066Starting log streaming...+ exec appLet’s see how the application is installed on our cluster.$ kubectl get deploymentNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEgoapp-go   2         2         2            2           5m$ kubectl get svcNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGEgoapp-go     ClusterIP   10.103.78.13   &lt;none&gt;        80/TCP    4mkubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   1h$ kubectl get podsNAME                       READY     STATUS    RESTARTS   AGEgoapp-go-88f4b7bc7-4cltn   1/1       Running   0          4mgoapp-go-88f4b7bc7-wt7kx   1/1       Running   0          4mAs you can see,our application has successfully been deployed to Kubernetes,and is deployed using a Kubernetes Deployment resource.The services are not exposed by default,so we will need to either use kubectl port-forward &lt;pod&gt; 8080,or SSH into our cluster.$ minikube ssh$ curl 10.103.78.13Hello Draft! If you want to expose your applications automatically using Draft,you can use a Kubernetes Ingress Controller for this.You will need to enable an Ingress Controller in Kubernetes (minikube addons enable ingress),and initialize draft with the --ingress-enabled flag.More information about this can be found here.Deploying changesDraft is meant to be used during development,so it is important we can push changes.Let’s make a change to our application.func handler(w http.ResponseWriter, r *http.Request) {    fmt.Fprintf(w, \"Bye Draft!\")}Now that we have made some changes,let’s try to deploy our new version.This is done using the same command.$ draft upDraft Up Started: 'goapp'goapp: Building Docker Image: SUCCESS ?  (12.0163s)goapp: Pushing Docker Image: SUCCESS ?  (16.0110s)goapp: Releasing Application: SUCCESS ?  (0.2311s)goapp: Build ID: 01C65507WTBX5EAJKWWR53T652The build time has gone down from 2 minutes, to 28 seconds.This is because the Golang Docker image no longer needs to be downloaded and/or pushed to the Docker registry.The deployment is updated with the new version of the application.Old pods are taken down by Kubernetes and new ones are started.$ kubectl get podsNAME                        READY     STATUS              RESTARTS   AGEgoapp-go-6fb684d887-2kq69   0/1       ContainerCreating   0          23sgoapp-go-6fb684d887-qmth6   1/1       Running             0          23sgoapp-go-88f4b7bc7-wt7kx    0/1       Terminating         0          19m$ minikube ssh$ curl 10.103.78.13Bye  Draft! Our changes are now deployed to the Kubernetes cluster!ConclusionDraft is great for local development using Kubernetes.It is meant to be used before committing and pushing your code.Applications can be deployed to Kubernetes within minutes,without requiring to write Dockerfiles and/or Kubernetes resource files.Azure Draft is still experimental for now, but the development team is active, and I have not run into many issues yet.It brings one of Cloud Foundry’s best features, namely build packs, to Kubernetes.It’s definitely worth a try!Resources  Draft website  Draft GitHub  Helm"
      },
    
      "testing-2018-02-05-writing-tests-in-kotlin-with-mockk-html": {
        "title": "Mocking in Kotlin with MockK",
        "url": "/testing/2018/02/05/Writing-tests-in-Kotlin-with-MockK.html",
        "image": "/img/writing-tests-in-kotlin-with-mockk/mockk.png",
        "date": "05 Feb 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Mockito and its shortcomings in Kotlin  The idiomatic mocking framework for Kotlin  Summing it all up  Other useful linksIntroductionI have been pretty excited about Kotlin since JetBrains released the first official version on the 15th of February 2016.It did take me a while before I managed to get my hands dirty, which was in between the version 1.1 and 1.2 release.Besides developing in Java, which I’m doing full time as a senior Java consultant, I’ve also been dabbling in Scala for quite some years with Lightbend’s Play Framework.Everyone knows how verbose Java is, and how it lags a bit behind the newer, fancier programming languages.It still misses features such as pattern matching, case/data classes and local-variable type inference.Starting from Java 8 with the introduction of Lambdas, we have finally been given the option to add a more functional programming flavour to our code which was due in time.Scala felt very refreshing for me back then, when I started to use it which was shortly before the JDK 7 release.It felt clean and powerful, bringing the best of both worlds of object-oriented programming and functional programming.At the same time, Scala houses a lot of complexity since there are so many ways and styles to tackle problems.You could compare it a bit to having the toolkit available to build a space rocket when you only plan on building a small airplane. And this is where Kotlin comes in, being very similar to Scala but with a focus on practicality and simplicity.Coming from the industry instead of academia, it focuses on solving problems faced daily by programmers.I’m a big fan of Test-Driven Development and thoroughly testing the behaviour of my code by making use of mocks.Mockito has been my mocking framework of choice combined with PowerMock for mocking constructors, static and private methods, and more.As Kotlin also runs on the JVM, it can make use of the huge Java ecosystem.It was a no-brainer for me to immediately add these testing libraries to my Kotlin project for writing my tests.And thus I set off, creating a new Kotlin project to see how it fared.Mockito and its shortcomings in Kotlin    I started off with adding the Mockito dependency to my Kotlin project.&lt;dependency&gt;    &lt;groupId&gt;org.mockito&lt;/groupId&gt;    &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;    &lt;version&gt;2.13.0&lt;/version&gt;&lt;/dependency&gt;And wrote a first simple test in which I wanted to test a Service class that uses a Generator to generate a record and a Dao for persisting it.class ServiceTest {    class Generator { fun generate(): String = \"Random String that's not random\" }    class Dao { fun insert(record: String) = println(\"\"\"Inserting \"$record\"\"\"\") }    class Service(private val generator: Generator, private val dao: Dao) {        fun calculate() {            val record = generator.generate()            dao.insert(record)        }    }    val generator = Mockito.mock(Generator::class.java)    val dao = Mockito.mock(Dao::class.java)    val service = Service(generator, dao)    @Test    fun myTest() {        val mockedRecord = \"mocked String\"        Mockito.`when`(generator.generate()).thenReturn(mockedRecord)        service.calculate()        Mockito.verify(generator).generate()        Mockito.verify(dao).insert(mockedRecord)        Mockito.verifyNoMoreInteractions(generator, dao)    }}Writing the test went pretty smooth although the code looks a bit funky.When I ran it, I stumbled on this nice error:org.mockito.exceptions.base.MockitoException: Cannot mock/spy class be.yannickdeturck.HelloTest$GeneratorMockito cannot mock/spy because : - final class — anonymous classes — primitive typesAs all classes and methods are final by default in Kotlin, using Mockito appears to be a bit problematic due to how Mockito creates its mocks.If you are interested in how Mockito’s mocking works internally you should checkout this response on StackOverflow that roughly sums it up.I did a bit more research on using Mockito and stumbled upon this slightly tuned version for Kotlin, wrapping some of Mockito’s functionalities, providing a simpler API.I decided to try that one out and replaced my Mockito dependency with it:&lt;dependency&gt;    &lt;groupId&gt;com.nhaarman&lt;/groupId&gt;    &lt;artifactId&gt;mockito-kotlin&lt;/artifactId&gt;    &lt;version&gt;1.5.0&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;I rewrote my test a bit in order to make use of the cleaner syntax the library had to offer.Note how both defining and using the mocks is a bit more elegant:class ServiceTest {    class Generator { fun generate(): String = \"Random String that's not random\" }    class Dao { fun insert(record: String) = println(\"\"\"Inserting \"$record\"\"\"\") }    class Service(private val generator: Generator, private val dao: Dao) {        fun calculate() {            val record = generator.generate()            dao.insert(record)        }    }    val generator = mock&lt;Generator&gt;()    val dao = mock&lt;Dao&gt;()    val service = Service(generator, dao)    @Test    fun myTest() {        val mockedRecord = \"mocked String\"        whenever(generator.generate()).thenReturn(mockedRecord)        service.calculate()        Mockito.verify(generator).generate()        Mockito.verify(dao).insert(mockedRecord)        Mockito.verifyNoMoreInteractions(generator, dao)    }}Sadly, we still have the Mockito error.As I said, in Kotlin all classes and methods are final by default which Mockito cannot deal with.You would have to explicitly make your classes inheritable using the open modifier.Another approach would be to add interfaces to everything.Changing your code just for the sake of being able to write good tests is something I’m not exactly fond of, and in this case we are also getting around one of the key features of Kotlin.Starting from Mockito version 2.0.0 it did became possible to mock final classes although it is an incubating, opt-in feature.This however, requires a bit of a setup really.It basically consists of creating a file called org.mockito.plugins.MockMaker with as content mock-maker-inline and placing it under resources/mockito-extensions.It felt a bit hacky but apparently this is only a temporary way to set it up.Although there are supposed to be plans to make it more straightforward.Hadi Hariri wrote an extensive blog post on setting this up and you should check it out if you would like to learn more about it.Good, so this makes it possible to create mocks without having to add the open modifier to all your classes and methods!It does’t appear to be completely compatible with Mockito Kotlin even though the library depends on Mockito version 2.8.9.Trying to run the test resulted in the following error:org.mockito.exceptions.base.MockitoInitializationException: Could not initialize inline Byte Buddy mock maker. (This mock maker is not supported on Android.)At the time of writing there is a version 2.0.0 in alpha for Mockito Kotlin so I tried to switch to it to see if it changed anything.Note that the dependency is a bit different and you need to use the classes in the com.nhaarman.mockitokotlin2 package:&lt;dependency&gt;    &lt;groupId&gt;com.nhaarman.mockitokotlin2&lt;/groupId&gt;    &lt;artifactId&gt;mockito-kotlin&lt;/artifactId&gt;    &lt;version&gt;2.0.0-alpha02&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;It got rid of the above error.I did ran into some unexpected behaviour where my mocks’ behaviour was rather unexpected when I also added partial mocking using spies.I spent some time to get my head around it and during my quest for answers I stumbled upon this library called MockK, created by Oleksiy Pylypenko.I decided to check it out as I became a bit annoyed with Mockito in Kotlin so far.The idiomatic mocking framework for Kotlin    Although I am a huge fan of Mockito for mocking in Java, using Mockito in Kotlin just feels a bit too Java-ish when you have this elegant Kotlin code all around in your project. MockK’s main philosophy is offering first-class support for Kotlin features and being able to write idiomatic Kotlin code when using it.Adding MockK is as simple as ever as you only have to add the dependency to your project and you are set to go.Maven:&lt;dependency&gt;    &lt;groupId&gt;io.mockk&lt;/groupId&gt;    &lt;artifactId&gt;mockk&lt;/artifactId&gt;    &lt;version&gt;${mockk.version}&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;Gradle:testCompile \"io.mockk:mockk:${mockkVersion}\"The available MockK documentation provides a really nice overview of all the different features with a lot of examples, making it very easy to get started.If you have used a mocking framework before such as Mockito, everything should come natural as you usually do when writing tests with mocks.You have the same three parts in which your tests are divided:  Preparing the test data and setting up the mocking  Executing the logic that you want to test  Performing the necessary validation and verification checks to see if the result and behaviour matches your expectations.The test from above is written as follows:class ServiceTest {    class Generator { fun generate(): String = \"Random String that's not random\" }    class Dao { fun insert(record: String) = println(\"\"\"Inserting \"$record\"\"\"\") }    class Service(private val generator: Generator, private val dao: Dao) {        fun calculate() {            val record = generator.generate()            dao.insert(record)        }    }    val generator = mockk&lt;Generator&gt;()    val dao = mockk&lt;Dao&gt;()    val service = Service(generator, dao)    @Test    fun myTest() {        val mockedRecord = \"mocked String\"        every { generator.generate() } returns mockedRecord        every { dao.insert(mockedRecord) } just Runs        service.calculate()        verifyAll {            generator.generate()            dao.insert(mockedRecord)        }    }}Going over the example, everything should feel familiar but more elegant.You may however, wonder what the every { dao.insert(mockedRecord) } just Runs line is doing exactly.By default in MockK, mocks are strict so you need to provide some behaviour for them.If we were to omit the line, the test would fail as we would run into the following error:io.mockk.MockKException: no answer found for: Dao(#2).insert(mocked String)A feature I was immediately fond of as I like to write strict tests.Note that you can also define the mock as being a relaxed mock in order to avoid this strict behaviour:val dao = mockk&lt;Dao&gt;(relaxed = true)Mockito has something similar for verifying with Mockito.verifyNoMoreInteractions(generator, dao) which I also extensively use to enforce that all calls of mocked objects are verified.Of course, the above example is only the tip of the iceberg as MockK houses a ton of other features such as annotations to simplify creating mock objects, spying to mix mocks and real objects, partial argument matching, capturing arguments, verification order support, matchers, coroutine mocking support, and so much more.After fiddling with Mockito, I happily continued using MockK for my little Kotlin project.Summing it all upMockito for me felt a bit hacky/unnatural and too Java-ish when using it in a Kotlin project.I imagine it will probably become better in the near future.MockK, being a newer library specifically targeted at Kotlin, felt clean and pleasant to use with excellent documentation.Oleksiy is also actively working on the library as you can see in the repo’s releases section.I highly recommend checking out MockK for mocking in a Kotlin project as it is currently a better option in my humble opinion.Oleksiy is also very active on the MockK Gitter and he helps you out quickly should you have any questions.He is also open to feedback and enjoys being challenged in order to make MockK an even better library.If you want to learn more about MockK, you should definitely read Oleksiy’s blog post series in the next section.Other useful links  MockK documenation  MockK GitHub  MockK Gitter  Oleksiy Pylypenko’s Twitter  Blog post by Hadi Hariri: Mocking Kotlin with Mockito  Blog post by Oleksiy Pylypenko: Mocking is not rocket science: Basics  Blog post by Oleksiy Pylypenko: Mocking is not rocket science: Expected behavior and behavior verification  Blog post by Oleksiy Pylypenko: Mocking is not rocket science: MockK features"
      },
    
      "architecture-2018-01-27-visualizing-your-spring-integration-components-and-flows-html": {
        "title": "Visualizing your Spring Integration components &amp; flows",
        "url": "/architecture/2018/01/27/Visualizing-your-Spring-Integration-components-and-flows.html",
        "image": "/img/2018-01-27-Visualizing-your-Spring-Integration-components-and-flows/post-image.jpg",
        "date": "27 Jan 2018",
        "category": "post, blog post, blog",
        "content": "  This post can be useful for everyone who wants to have insights on their application’s internal architecture when integrating with other systems using Spring Integration.From the developer that just started in your team and who wants to have an overview, to the seasoned team member that needs to troubleshoot a problem in production.Currently we are working on the phased roll out of a microservices architecture at one of our clients.To ensure that everything works as it’s supposed to we are using a lot of Enterprise Integration Patterns to stitch both the old and the new landscape together.The best way to achieve a solution when using Java, is to use Spring Integration.A few days ago we wanted to have clear insights on how certain microservices are communicating with the existing systems.Creating your graph of Spring Integration components and flowsA first step to expose your Spring Integration components and flows is to add an IntegrationGraphServer bean to your application.This class resides in the o.s.i.support.management.graph package, between all the required classes to collect, build and render the runtime state of Spring Integration components as a single tree-like Graph object.Exposing the graphUsing the Spring Integration HTTP module you can easily expose the IntegrationGraphServer functionality as a REST service.Just add the @EnableIntegrationGraphController annotation to your application, and you’re good to go!Or, in case you are using XML config, add the &lt;int-http:graph-controller/&gt; XML element to your setup.Be sure to edit the allowedOrigins attribute of the annotation in case you’re accessing the endpoint between 2 domains.Sidenote: your application needs to be deployed on a web container, or it needs to use an embedded web container in case you are building on top of Spring Boot.Visualizing the exposed graph of components and flowsWith D3.js we are able to visualize everything within our graph.In this quick &amp; dirty gist I created, you can find a simple example of a possible visualisation.Download the index.html file, point the script to the correct endpoint by editing the graphEndpoint variable, open the file in your browser and you should see every component and flow!As Gary Russell pointed out in his reply to my tweet you can also use Spring Flo for the visualization.Spring Flo is an Angular based, embeddable graphical component for pipeline/graph building and editing.This is used as the basis of the stream builder in Spring Cloud Data Flow.You can find the sample application here.Taking it one step furtherIf you add the @EnableIntegrationManagement annotation or the &lt;int:management /&gt; XML element to your setup, the graph will even expose all the metrics of your Spring Integration components.This will definitely help you out when you want to monitor your components and flows, enabling you to troubleshoot problems even faster in case something goes wrong in production."
      },
    
      "tech-2018-01-20-jworks-tech-blog-html": {
        "title": "The JWorks Tech Radar",
        "url": "/tech/2018/01/20/JWorks-Tech-Blog.html",
        "image": "/img/techradar/TechRadar.png",
        "date": "20 Jan 2018",
        "category": "post, blog post, blog",
        "content": "  The JWorks Technology Radar is intended to showcase our opinion on the most important tech trends we see today.It is based on the Thoughtworks Technology Radar and uses its open source framework for visualization.We adopted the mechanics and methodology from Thoughtworks, as described below.Our tech radar can be reached through this link.The RadarThe Radar is a document that sets out the changes that we think are currently interesting in software development - things in motion that we think you should pay attention to and consider using in your projects.It reflects the idiosyncratic opinion of a bunch of senior technologists and is based on our day-to-day work and experiences.While we think this is interesting, it shouldn’t be taken as a deep market analysis.QuadrantsThe quadrants are a categorization of the type of blips:  Programming Languages and Frameworks. Quite straight-forward, languages and software frameworks.  Tools. These can be components, such as databases, software development tools, such as Versions Control Systems; or more generic categories of tools, such as the notion of polyglot persistence.  Platforms. Things that we build software on top of such as mobile technologies like Android, virtual platforms like the JVM, or generic kinds of platforms like hybrid clouds.  Techniques. These include elements of a software development process, such as experience design; and ways of structuring software, such as microservices.We don’t make a big deal out of the quadrants - they’re really just a way to break up the Radar into topic areas. We don’t think it’s important which quadrant a blip goes into, unlike the rings - which generate a lot of discussion.RingsThe metaphor of a radar says that the closer a blip is to you, the sooner it will be on top of you. Like most metaphors, you can’t take it too seriously, but there’s an essential sense to it.Our Radar has four rings, which we’ll describe starting from the middle:  The Adopt ring represents blips that we think you should be using now. We don’t say that you should use these for every project; any tool should only be used in an appropriate context. However we do think that a blip in the Adopt ring represents something where there’s no doubt that it’s proven and mature for use.  The Trial ring is for blips that we think are ready for use, but not as completely proven as those in the Adopt ring. So for most organizations we think you should use these on a trial basis, to decide whether they should be part of your toolkit. Typically we’re happy to use trial blips now, but we realize that most readers will be more cautious than us.  The Assess ring are things that you should look at closely, but not necessarily trial yet - unless you think they would be a particularly good fit for you. Typically, blips in the Assess ring are things that we’re currently trialling, on our projects.  The Hold ring is for things that are getting attention in the industry, but we don’t think are ready for use. Sometimes this is because we don’t think they’re mature enough yet: sometimes it means we think they’re irredeemably flawed. We don’t have an “avoid” ring, but we do throw things in the Hold ring that we wish our clients wouldn’t use.Unlike the quadrants, we do have some quite passionate arguments about which ring a blip should go into. We don’t tend to have angry debates, but rings are what generate the most energetic discussions. Over the course of making the Radar we’ve come up with some useful rules of thumb to help us put things into rings.We can only put blips into the Trial ring when we have experience of that blip on a real project. This can mean we sometimes look behind the technology curve, because we may like the look of a technology but haven’t yet persuaded a client to try it out - and until we do that blip cannot pass into Trial.For the Adopt ring, we only include items when we think it would be a poor and potentially irresponsible choice not to use them given the appropriate project context.More information can be found on the Thoughtworks website."
      },
    
      "testing-2018-01-05-gauge-automated-testing-html": {
        "title": "Automated testing with Gauge",
        "url": "/testing/2018/01/05/Gauge-automated-testing.html",
        "image": "/img/2018-01-05-gauge-automated-testing/gauge.png",
        "date": "05 Jan 2018",
        "category": "post, blog post, blog",
        "content": "After finishing a new feature in your web app, you test whether it works.However, while working on that new feature, you may have broken something else.If you want to know for sure, you have to test everything.Clicking around, filling in forms, … will cost you some valuable time.So, you’ll probably end up not doing it thorough enough or even not at all, assuming everything else still works fine.That’s exactly why you should have automated tests do it for you!Gauge is yet another test automation tool that serves that purpose.The founder is ThoughtWorks, the company that also created  Selenium and GoCD.It’s still in beta at the moment of writing, but it already works terrific!Gauge is comparable with Protractor or JUnit extended with Cucumber (if you haven’t heard of these, they are worth checking out).In this blog we’ll be mainly talking about automating browser testing using Selenium WebDriver, although that isn’t the sole purpose of Gauge.If you don’t know WebDriver, it’s what allows you to interact with the browser to traverse the DOM, click on elements and so on.Now, unlike Cucumber, Gauge itself only supports three languages at the moment: Java, Ruby and C#.Others may be supported by the community like JavaScript.You can define steps written in either one of those languages.These are the actual tests and can be identified by a sentence you can choose yourself.By combining these sentences, you can write test scenarios which are written in Markdown.That means you don’t need any programming experience to write test scenarios.You could create some sort of dictionary with the step sentences that others can use for the scenarios or the other way around.In other words, anyone is able to read and write test scenarios.The actual implementation of the steps does require some technical knowledge.Table of contents  Setup  Writing tests  Running the specs  Environments and configuration  Reports  ConclusionSetupGauge offers an installer which can be downloaded here.During the install, you can select which core plugins to install.In this blog the tests are written in Java, so we would need the Java core plugin.After the install, you’ll be able to run gauge from the command line.It can, for example, be used to install more plugins (as well as core plugins gauge install java).Next, in order to use WebDriver in our tests, we’ll need the Java SDK and Maven.When you’re a Java developer, you probably already have those installed.If not, you’ll find enough on Google on how to install those.To start a new project, create a new folder and run gauge init java in it.This will setup a basic Gauge project.Then we need to add a pom.xml file ourselves, because we need some dependencies such as Selenium WebDriver.Next, WebDriver needs to be set up in our test code.A good example of how you could do that can be found here.They created a DriverFactory so you can easily switch between browsers using environment variables (we’ll get to that).Gauge hooks are used to start and stop WebDriver when running the test suite.Just copy those pieces of code into your own project.Or, you could start from the Maven + Selenium example provided by Gauge which can be found on GitHub.That way you’d have some examples to start from.Now we can start writing tests.Writing testsOn the lowest level we have our Java functions that control the browser using WebDriver commands.To these functions we can assign a @Step annotation to be able to identify it.It’s usually a sentence describing the action being performed.The sentences can then be used to write the scenarios of our tests (or specs).If you’d like to combine multiple steps into once sentence, you can do so by creating so called “concepts”.A typical folder structure for a Gauge setup using WebDriver and Java is as follows:    Writing the specsThe specs are written in Markdown.Each spec file starts with a title and is underlined with ===.Next, some steps can be defined that will be run before each scenario.When listing steps, you need to prefix each step with an asterix (*) as in a Markdown list.After that, the actual scenarios can be written.They start with a title and are underlined with ---.Again, the steps for a scenario should be listed as in a Markdown list.You can also add some tags which can be used to only run certain specs and to search in the HTML reports.Here’s an example specification:Customer sign-up================* Go to sign up pageCustomer sign-up----------------tags: sign-up, customer* Sign up a new customer with name \"JWorks\" email \"jworks@ordina.be\" and \"password\"* Check if the sign up was successfulWriting the stepsThe sentences we wrote in the specs still need to be linked to Java functions.We can do so by simply adding a @Step annotation to a Java function.It doesn’t matter in which class you put the functions, you’re free to choose how to organize them.As long as they are under the src/test/java folder.You could, for example, group them per page or feature.public class CustomerSignup {    @Step(\"Sign up as &lt;customer&gt; with email &lt;customer@example.com&gt; and &lt;password&gt;\")    public void registerCustomerWith(String customer, String email, String password) {        WebDriver webDriver = Driver.webDriver;        WebElement form = webDriver.findElement(By.id(\"new_user\"));        form.findElement(By.name(\"user[username]\")).sendKeys(customer);        form.findElement(By.name(\"user[email]\")).sendKeys(email);        form.findElement(By.name(\"user[password]\")).sendKeys(password);        form.findElement(By.name(\"user[password_confirmation]\")).sendKeys(password);        form.findElement(By.name(\"commit\")).click();    }    @Step(\"Check if the sign up was successful\")    public void checkSignUpSuccessful() {        WebDriver webDriver = Driver.webDriver;        WebElement message = webDriver.findElements(By.className(\"message\"));        assertThat(message.getText(), is(\"You have been signed up successfully!\"));    }}As shown in the example above, you can easily pass parameters to steps.You simply have to wrap the keywords in &lt;&gt; in the @Step annotationand list the same keywords as parameters in the actual Java function.Obviously, you can then use them in your Java code.Page object patternA clean way to organize all your code would be to use the page object pattern.This means that for every page in your web app, you should create a class.Such a class contains all code to interact with that specific page.The example we saw earlier, could be transformed into this:Page object:public class SignUpPage {    public WebElement usernameField;    public WebElement emailField;    public WebElement passwordField;    public WebElement passwordConfirmField;    public WebElement commitButton;    SignUpPage() {        WebDriver webDriver = Driver.webDriver;        WebElement form = webDriver.findElement(By.id(\"new_user\"));        this.usernameField = form.findElement(By.name(\"user[username]\"));        this.emailField = form.findElement(By.name(\"user[email]\"));        this.passwordField = form.findElement(By.name(\"user[password]\"));        this.passwordConfirmField = form.findElement(By.name(\"user[password_confirmation]\"));        this.commitButton = form.findElement(By.name(\"commit\"));    }}Step definition:public class CustomerSignup {    private SignUpPage signUpPage = new SignUpPage();    @Step(\"Sign up as &lt;customer&gt; with email &lt;customer@example.com&gt; and &lt;password&gt;\")    public void registerCustomerWith(String customer, String email, String password) {        this.signUpPage.usernameField.sendKeys(customer);        this.signUpPage.emailField.sendKeys(email);        this.signUpPage.passwordField.sendKeys(password);        this.signUpPage.passwordConfirmField.sendKeys(password);        this.signUpPage.commitButton.click();    }}The great benefit of this approach is that you can reuse a lot of the code.You only have to locate the elements once instead of in every step.Functions can be added to the page objects as well.Suppose you have a dropdown, you first have to click to open it and then select an option from the list.You can write a function doing all that.In your step definition, you then simply have to call that function to select something from a dropdown.It’s a good way to avoid too much code duplication.ConceptsIf you find yourself repeating the same sequence of steps over and over,you could combine those steps into one step using concepts.These are also written in Markdown and you can pass arguments the same way as in the Java @Step annotations.They should be placed in the /specs/concepts folder and use the *.cpt extension.# Sign up a new customer with name &lt;name&gt; email &lt;email&gt; and &lt;password&gt;* Sign up as &lt;name&gt; with email &lt;email&gt; and &lt;password&gt;* Show a message \"Thank you for signing up! You are now logged in.\"Cucumber only offers this feature in some implementations, they don’t in Cucumber.js for example.They refuse to implement it because they believe this creates too much abstraction and makes you lose sight of the overall picture.Gauge does offer it, so it’s up to you whether you want to make use of it or not.Running the specsSince the project is setup with Maven, the tests can be run with mvn test.However, if you want to pass any arguments, you’ll need to use mvn gauge:execute instead.TagsYou may have noticed in the spec files that tags can be added.They can be used to run only certain specs.mvn gauge:execute -DspecsDir=specs -Dtags=\"sign-up &amp; customer\"ParallelSo far, Gauge didn’t stand out from other automated testing solutions.Although there’s one thing that really does stand out and that’s how easy it is to use parallel execution!mvn gauge:execute -DspecsDir=specs -DinParallel=trueRunning this command will start up a stream for each CPU core your computer has.For each stream it will open a browser window and execute the specs.So if you have four CPU cores, four browser windows will be opened.You can overwrite the number of parallel executors, but it’s recommended not to exceed the number of CPU cores.mvn gauge:execute -DspecsDir=specs -DinParallel=true -Dnodes=2Now, some specs may take longer to run than others.By default, specs are divided dynamically over the streams.So when a spec has finished, it will take the next one from the list of specs that still need to be executed.It’s possible to change this so the specs are divided on startup, but the command is deprecated and will be removed.Making tests independentTo make use of this parallel execution, you’ll have to make sure that your tests don’t rely on each other.I think it’s pretty clear why you shouldn’t do that.Anyway, suppose you test the sign up and sign in.If your sign in relies on the user being signed up through a previous test, these tests should be run synchronously.However, if you would want to test whether a user can sign in after having signed up, you should do so in one test.That immediately solves our problem and we are safe to use the parallel execution!Environments and configurationThe config files are located under env/default.You should have three files in that folder: default.properties, java.properties and user.properties.In the example by Gauge, they have an APP_URL parameter in that last file.I recommend using that approach as well, you can get parameters in your Java code using System.getenv(\"APP_URL\").It’s possible to create different environments by simply creating a new folder.There you can add *.properties files containing additional properties or properties overwriting the ones in the default folder.So, that means you don’t need to copy the whole configuration. Gauge will always load all the default properties. Then it will load those defined for the environment you wish to use. If a property is defined for an environment and it already exists in the default configuration, it will use the one defined for that environment instead.Now, suppose you want a different configuration for your CI-tool, you can create a folder named ci.When running the specs, simply pass an argument stating the environment.mvn gauge:execute -Denv=\"ci\"Environments can also be used to run gauge with another browser like in the example by Gauge.Create a folder named firefox for example and add a file called browser.properties.In that file you add browser = FIREFOX.When you then run the tests with the firefox environment, it will use FireFox as a browser instead.(This only works if you have your project set up like in the example, the Driver and DriverFactory files are required here.)ReportsTo get a HTML report, the plugin has to be installed first: gauge install html-report.That’s about it!After running the specs, a nice HTML report will be outputted to the /reports folder.It shows which tests succeeded and which failed with some additional graphs.In the report you’ll even find how long it took to run a test and each of its steps.There’s also a search functionality to quickly find a certain spec.    ConclusionIt’s a good idea to write automated tests.If you do it well, you don’t have to spend a lot of time manually testing your application.The chance of something being broken by your changes will decrease dramatically.Also, be sure to use the page object pattern and create functions for repetitive actions.It avoids code duplication and having to update the same code in multiple places.If you’re starting a new project or starting from scratch with writing browser tests, you should consider using Gauge.Even though it’s still in beta.With ThoughtWorks as the main force behind this tool, it’s here to stay!"
      },
    
      "testing-2018-01-04-3-stages-api-testing-html": {
        "title": "3 Stages of API testing",
        "url": "/testing/2018/01/04/3-stages-api-testing.html",
        "image": "/img/3-stages-api-testing/overview.png",
        "date": "04 Jan 2018",
        "category": "post, blog post, blog",
        "content": "  Continuous Integration with automated testing is more and more incorporated in the culture of software delivery companies.Running tests in different stages is a big part of it.In this post, we’ll have a look at our three stages of API testing we are promoting at Ordina.Table of contents  About the setup  Stage 1: Unit testing  Stage 2: Testing against a mock-end  Stage 3: Testing full environment  ConclusionSetupThe example we are going to use is part of a bigger microfrontend/microservice setup.The front-end part of this example is the actual header of this microfrontend setup. It’s the top bar, developed as a separate front-end application.This header provides the user with a search input field, where the user can search our database of competence centers.This part is written in Angular (5).The app gets dockerized after the unit tests (and build) are successfully completed.It is served by a simple Express server inside a Docker container.The back-end part provides the data of the competence centers.It’s nothing more than a simple REST API written in TypeScript using the Nest.js framework.The data provided by this service is a JSON file.Its content is parsed into memory and is exposed through this REST API.    The front-end (header) is providing the user with an input field.This field allows the user to perform a search on our back-end service.It also provides a clear button, so the user can remove the content from the input field and reset the local cache of search results.A second button is the filter button.When pressed, it will emit an event that can be listened to by other microfrontends.public filterCCs(): void {  if(isPlatformBrowser(this.platformId)) {    const event = new CustomEvent('filterCCs', { detail: { needle: this.needle} });    window.dispatchEvent(event);  }}public filterStats() {  if ( this.needle === '' ) {    this.data = [];  } else {    this._updateData();  }}private _updateData() {  this._ccService.getStats(this.needle).subscribe((response) =&gt; {      this.data = response;  });}public resetSearch(): void {  this.needle = \"\";  this.data = [];}When resetting the content with the clear button, we’re not sending a request.When there’s nothing to search for, the result would be an empty array.So we’re just resetting our local data to an empty array.    The back-end API is exposing three endpoints:@Get()async getAllCCs(@Response() res) {    const ccs = await this._ccsService.getAllCCs();    res.status(HttpStatus.OK).json(ccs);}@Get('/search')async searchCCs(@Response() res, @Query('needle') searchString) {    let filtered = this._ccsService.searchCCs(searchString);    res.status(HttpStatus.OK).json(filtered);}@Get('/:id')async getCC(@Response() res, @Param('id') id) {    const cc = await this._ccsService.getCC(+id);    res.status(HttpStatus.OK).json(cc);}The header will always trigger the endpoint at search with a needle.When the needle is undefined (or an empty string), the search endpoint will return an empty list.Stage 1: Unit testing the communication layer (front and back-end)    The first stage is unit testing each component. This step is almost always part of the component’s build. Let’s take a quick look at each component.Unit testing the communication part of the UI    For unit testing our front-end Angular 5 application, we are going to use the import { MockBackend } from '@angular/http/testing'; from Angular itself to mock our back-end....beforeEach(async(() =&gt; {    TestBed.configureTestingModule({        ...      providers: [        CCService, // our service that is handling the communication        {provide: XHRBackend, useClass: MockBackend} // our mock      ]    })    .compileComponents();  }));...describe('Should query ccs with an observable', () =&gt;    it('Should return data', inject([CCService, XHRBackend], (ccService, mockBackend) =&gt; {      mockBackend.connections.subscribe((connection) =&gt; {        const ccs: Array&lt;CC&gt; = [          {              ...          },          {              ...          }        ];        connection.mockRespond(new Response(new ResponseOptions({          body: JSON.stringify(ccs)        })));      });      ccService.getStats('tim').subscribe((ccs: Array&lt;CC&gt;) =&gt; {        expect(ccs.length).toBe(2);        expect(ccs[0].clEmail).toBe('tim.vierbergen@ordina.be');       });      })    )  );The Angular framework handles the communication part.We want to mock the $http call and see how we are handling the result.The mockBackend is working inbetween our own code and the provided $http part.Every call is triggering the mockBackend.connections, so the subscribers are triggered.We are providing our own data and returning it as the response of the $http call.Unit testing the communication layer of the service    For unit testing our back-end, we are using Jest.Jest is a testing framework by Facebook.If you are interested in Jest, make sure to watch this blog, as a post around this topic is in the making.Just as with our front-end, we trust the framework to correctly handle the communication itself.Our unit tests will start at the controller level of our API.describe('searchCCs', () =&gt; {    it('should return a filtered array of competence centers', async () =&gt; {      await ccsController.searchCCs(mockResponse, 'tim');      let data = JSON.parse(mockResponse.data);      expect(data.length).toBe(2);      expect(data[0].clEmail).toBe('tim.vierbergen@ordina.be');    });});Where mockResponse is exactly that.A simple mock of the response object.Purpose of these unit testsThe goal of these unit tests is to make sure that the functions inside the components are working as expected.This way the next stage of testing can only fail due to errors from outside this component.However, to make sure this is the case, the mock data should be as close to production data when it comes to data specific characteristics.This is more important for your front-end because you have less control over the data itself.Your back-end API is only responding to request params that are defined in the specs.So it’s easier to control them, or ignore unknowns.Stage 2: Testing against a mock-end    In our second stage we are going to use a mock service to test against.This means that we are going to mock ‘the other’ component by replacing it with an easy to use solution.Although we are mocking some parts, this can be seen as an end-to-end test for each component itself.We want to test our component by means of external services, just as it is supposed to work in a complete environment.Mocking our back-end to test our front-end.    For our front-end component (user interface), we are going to mock the back-end.Some front-ends are performing calls even without human interaction.However, in most cases, front-end communication is depending on human interaction.To end-to-end test this part, we are also in need of a framework to mock this user interaction.Gauge, Protractor and Nightwatch.js are some examples of these frameworks.Most of them depend on ‘Selenium WebDriver’.Node-RED for back-end mockingWe are using Node-RED for our back-end mock because it’s so easy to setup and dockerize.  Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways.Node-RED is much more than just a tool to mock a back-end or any other service.It comes with a great User Interface to define your flows and to deploy them on your server.For this example we will mock our three endpoints and return test data.This test data can come from different sources.Node-RED provides multiple ways of working with data.You can include a simple MongoDB in your setup and read (even write) data from it.Or you can just use functions where you hard code your data.To keep it simple, we will use the latter in our setup.    A simple mock for a http-call consists out of three parts:  The entry point definition itself (input)  The function that handles the data (can be static or database or …)  The response definition    User interface automationIn this setup we are using Protractor for the e2e tests.The user input is limited to an input field to trigger the search REST-call and two buttons, one button for clearing the input and one for sending the search string to other microfrontend components.Some of the use cases, such as ‘clearing’ the input, are already covered in the unit tests.Depending on the effort you can always retest them in these e2e tests, but for this example, those are not important.We want to trigger the search REST call by sending the search string tim to the input field, and testing the outcome in the user interface.    ...describe('Searching with Tim should show 2 results', () =&gt; {    page.setSeachText('mySearchString').then(function() {        it(\"Input field should contain 'tim'\", function() {            expect(page.getSearchText()).toBe('tim');        });        describe(\"Result should show 2 entries\", function() {    \t    it(\"will show the number 2\", function() {                expect(page.getResultNumber()).toBe('2');            });            it(\"will show a dropdown with 2 results\", function() {                expect(page.getResultList().length).toBe(2);            });        });    });});...Conclusion for our front-endThis mock e2e test is depending on a mock back-end and a user input automation system.We are running these tests on our GoCD setup with dockerized elastic agents.To run these tests, we are in need of a go-agent that can run these e2e test with Protractor, but we also need an environment where we can serve this front-end and the mocked service.We are doing this with a go-agent that first spins up a docker-compose (for our front-end and mock-end), runs the protractor tests to this new environment and then brings down the environment when tests are finished.Mocking our front-end to test our back-end.    For testing our back-end service, we only need one other service.This service will need to fire REST calls to our back-end service and analyse the response.We can use Postman to set this up.  Postman is the complete toolchain for API developers, used by more than 3 million developers and 30000 companies worldwide.Postman makes working with APIs faster and easier by supporting developers at every stage of their workflow.It’s available for Mac OS X, Windows, Linux and Chrome users.You can use Postman for more than just API testing.In our setup, we need to create a testing scenario and just run it against our back-end service.Postman provides a user interface to do so.However, because we are running our tests on a cloud elastic go-agent, we need to find a way to automate this step.Luckily, Postman also provides a command-line tool called Newman.Newman let’s you run your test scenarios from your command line.You can first configure everything through the user interface and then just export the scenario so you can use it through the CLI.  You can read more about Postman and Newman in our blogpost API Testing with Postman and NewmanBelow, you can find a part of the exported JSON configuration.This part will send a GET request to the search endpoint, providing the search string tim.It will then analyse the response and check if the resulting array contains 2 entries and verifies the data....    {\t\"name\": \"Search existing ccs\",\t\"event\": [\t\t{\t\t\t\"listen\": \"test\",\t\t\t\"script\": {\t\t\t\t\"type\": \"text/javascript\",\t\t\t\t\"exec\": [\t\t\t\t\t\"var jsonData = JSON.parse(responseBody);\",\t\t\t\t\t\"var firstResult = jsonData[0];\",\t\t\t\t\t\"\",\t\t\t\t\t\"tests[\\\"Status code is 200\\\"] = responseCode.code === 200;\",\t\t\t\t\t\"tests[\\\"4 results returned\\\"] = jsonData.length === 2;\",\t\t\t\t\t\"tests[\\\"First result contains id \\\"] ='id' in firstResult;\",\t\t\t\t\t\"tests[\\\"Cl name contains tim vierbergen\\\"] = firstResult.cl === \\\"Tim Vierbergen\\\";\"\t\t\t\t]\t\t\t}\t\t}\t],\t\"request\": {\t\t\"url\": {\t\t\t\"raw\": \":/ccs/search?needle=tim\",\t\t\t\"host\": [\t\t\t\t\"\"\t\t\t],\t\t\t\"port\": \"\",\t\t\t\"path\": [\t\t\t\t\"ccs\",\t\t\t\t\"search\"\t\t\t],\t\t\t\"query\": [\t\t\t\t{\t\t\t\t\t\"key\": \"needle\",\t\t\t\t\t\"value\": \"tim\",\t\t\t\t\t\"equals\": true,\t\t\t\t\t\"description\": \"\"\t\t\t\t}\t\t\t],\t\t\t\"variable\": []\t\t},\t\t\"method\": \"GET\",\t\t\"header\": [\t\t\t{\t\t\t\t\"key\": \"Content-Type\",\t\t\t\t\"value\": \"application/json\",\t\t\t\t\"description\": \"\"\t\t\t}\t\t],\t\t\"body\": {},\t\t\"description\": \"\"\t},\t\"response\": []}...        In our continuous integration system, we are running these tests on a simple go-agent that can run these Newman tests.This agent spins up our service container, runs these tests and bring down that container.Conclusion for our back-endTesting this back-end service with a mock front-end is pretty easy.Since our data is included in this service and it is limited to a simple JSON file, we are not running performance tests.However, when your back-end needs to communicate with a database and/or make calculations, you can and should already include some performance tests in this stage.You can, for example, include some Gatling tests in this stage and put some load on this service to check response times.The goal of this stage is to test the whole component, including the frameworks we are using for the communication.It is still a decoupled system.Failures in this stage will show you that some integrations are failing and you know exactly where to look for the errors.Stage 3: Testing full environment    We now want to end-to-end test the whole system.We can use docker-compose to spin up this environment and then again run tests against the user interface.It will communicate with the real back-end to query its data and show the results in the user interface.Again, we want to run those tests on an elastic go-agent, so we are in need of an automated system.Right, we already used this in the previous stage where we were testing the user interface against a mocked back-end.    This elastic go-agent must be able to run the Protractor tests (obviously). It will first need to spin up this environment, run the tests and tear down the environment. Same goes for performance tests in this stage.You can use some frameworks to put extra load on your front-end to see how it’s behaving when it needs to handle more REST-calls for different parts.Or you can run more instances of the user interface, resulting in more load on the back-end service.ConclusionDecoupling your system and running tests in different stages will make it easier to debug when something is going wrong.Finding errors earlier will also save you some time and resources.Being able to find bugs before taking it to the next stage and spinning up complete environments will reduce the cost of your system (cloud).Yes, writing tests can be more expensive in time and resources in the short run, but it will save you a lot more time and resources in the long run."
      },
    
      "conference-2018-01-03-js-conf-budapest-day-2-html": {
        "title": "JSConf Budapest 2017 Day 2",
        "url": "/conference/2018/01/03/JS-Conf-Budapest-day-2.html",
        "image": "/img/js-conf-budapest-2017/header.png",
        "date": "03 Jan 2018",
        "category": "post, blog post, blog",
        "content": "From JSConf Budapest with loveThis year’s edition of JS Conf Budapest returned to the first venue at Urania National Movie theater.  Uránia Cinema in the middle of the city, near the party-district.Designed by Henrik Schmahl in the late 1890’s, the interior is characterized by the ornamental motifs of the Venetian Gothic and Moor styles.The place is listed as the world’s 3rd most beautiful cinema on Bored Panda.Many tech conferences were hosted here recently, such as TEDx and Strech Conference, because of the unique atmosphere.JS Conf Budapest 2017 is hosted by Glen Maddern and Charlie Gleason.At 10:00 the second day of the conference started. Enough time to drink great coffee and enjoy the breakfast.Day 2: Talks  Don Burks: MVC - What a web app and a Mozart Violin Concerto have in common  Opher Vishnia: Web Animation: from Disney to SASS  Imad Elyafi: Migrating Pinterest profiles to React  Laura Carvajal: YES, your site can (and should) be accessible too. Lessons learned in building FT.com  Nikita Baksalyar: Exploring the P2P world with WebRTC &amp; JavaScript  Vaidehi Joshi: Goldilocks And The Three Code ReviewsDay 2: MorningDon Burks: MVC - What a web app and a Mozart Violin Concerto have in commonYou can find him on Twitter using the handle @don_burks.The presentation can be found here.Composers and developers of applications have more in common than you might think. In his talk, Don Burks pointed out the similarities in structure between music and apps.An app has a certain structure, being the architecture or the configuration of how all the different components interact with each other. The same applies to a composition, where the structure represents the different themes in the composition. When you look at an application, you see a system of databases, web servers, load balancers, etc. All these components are part of the system. Composers have the same vision on their composition.When creating the flow of an application, developers start from a certain point and move forward from one step to another, where each step describes an action or event in the app. These steps are like a melody in music that moves everything forward. Developers also think vertically to visualize the architecture of the application. All the technologies that are used, are represented in a vertical stack.The goal of the application is to deliver a complete package, where the horizontal problem-solving and the vertical architecture should be transparent to the user. In music it is important that every tune is played on time. The musician must deliver the right tune at the right time in order to create the music like the composer intended it to be. An app should run the same way: it has to deliver the right “tune” at the desired time to provide the best user experience.The talk gave us more insight about various aspects in development, seen from a musician’s perspective. Composing music is an art and so is coding.Opher Vishnia: Web Animation: from Disney to SASSYou can find Opher on Twitter using the handle @opherv.The presentation can be found here.Opher is a designer and developer. In his talk, Opher shared his vision on how animations come to live.After we got introduced to Opher’s cute dog named Blizzard, which grew up to a direwolf that can be casted directly in a TV show about dragons and an upcoming winter, Opher started his talk with some child nostalgia from Disney. He shared his amazement about the animations used in old animated movies such as The Lion King, especially since they were hand drawn, and how these animation bring the characters and surroundings to life.Now, how do animations come to life? Two of the influencing factors are Follow Through and Overlapping Actions, which are part of the 12 basic principles of animation. The Follow Through principle defines that not every part of a moving entity moves the same way and stops the same way. Rather when one part of an entity stops, the other parts will follow through and come back. To illustrate this, Opher showed us an animation of a moving carrot that stopped suddenly, causing the leaves to go further before they stopped moving as well.The Overlapping Action principle means that when an entity is moving, its looser parts start moving later. The animation of the moving carrot clearly showed that the carrot itself moved in a smooth way, while the leaves were being dragged behind the carrot.These animation principles are applied by design specifications of huge companies such as the Material Design specs of Google. By taking these principles into account, you can give components and also the flow of your application more realism, for example when responding to events such as user input.Now, how can we implement these animations in our app? Opher discussed three implementations with us by means of an animation he has been working on:  CSS  GSAP, GreenSock Animation Platform  WAAPI, Web Animation APICSSWith the CSS implementation, Opher’s animation performed well across different devices and was directly understood by the browser. However, he stumbled upon the limitations of CSS, which made it tricky to implement complex animations and dynamic animations were even a complete no-go. Besides that, debugging was not a joy.GSAPGSAP provides a great, robust API to implement animations and even dynamic animations. It also deals with inconsistencies of browsers, which makes the life of a developer easier. Unfortunately, there are some downsides too. One of them is that you depend on an extra lib. Additionally, the JavaScript where GSAP is based on is implementation-sensitive and more advanced features of GSAP are not free.WAAPIWAAPI provides a native JavaScript API for animations. Basically you query for the desired elements in JS and call the animation function of those elements where you configure the animation. The animate function accepts two parameters: the keyframes and the duration of the animation. The keyframes should be an array of objects, where each object defines the state of the object at a certain time. The second parameter can also be replaced by an options object which enables you to configure the animation way better, such as adding delays or repeating the animation infinitely. The downside is that WAAPI is not supported by many browsers, but there is nothing a good polyfill cannot fix.So, which one should you use in your next project? As for most situations, this depends on your specific requirements and your expertise with the different implementations. It is recommended to keep these principles in mind when implementing animations in the future. With a little effort you can bring your own app to life, just like the animators of Disney did in their fairy tales.Imad Elyafi: Migrating Pinterest profiles to ReactYou can find Imad on Twitter using the handle @eelyafi.The presentation can be found here.A similar presentation was recorded on PolyConf 2017 and can be found on YouTube.A detailed write-out of that presentation can be found on MediumImad Elyafi is a software engineer at Pinterest. In this talk Imad tells you the story of how Pinterest migrated to React, explaining the techniques they tried and challenges they faced.With the current availability of fantastic modern frameworks, Pinterest decided to migrate from their outdated Denzel framework to React.Why React?Imad started off by saying they had a list of requirements for the new framework.  Large developer community  Design patterns that are compatible with the existing stack to make the migration easier  Isomorphic rendering, therefore being able to reuse templates on server- and client-side  Performance  Developer ExperienceRoad to ReactRewriting the whole app from scratch would be risky and expensive.Also, Pinterest did not want to freeze code and stop shipping new features.So they had to migrate a service that is constantly changing. A very complicated challenge Imad compares to changing the engines of an airplane while mid-flight.The solution was to rewrite the app piece by piece. That resulted in creating a so called hybrid app where two frameworks can exist together for the time it takes to migrate from the old framework to the new one.The very first step they had to take to make this hybrid app was to change their infrastructure and enable server-side JavaScript rendering.Before they used the Jinja templating engine for server-side rendering in Python and the JavaScript equivalent Nunjucks for client-side rendering.By also enabling Nunjucks rendering on a stand-alone NodeJS server, they now achieved pure isomorphic rendering with JavaScript on the server and on the client.Secondly, Pinterest had to render React components in their old Denzel framework. So they added React-specific bindings to Nunjucks’ templating language with a new keyword (component), to represent the “bridge” between Denzel and React.An example of a Nunjucks template with a React-Denzel bridge:{% if in_react %}  {{ component('MyReactComponent', {pinId: '123'}) }}{% else %}  {{ module('MyDenzelComponent', pinId='123') }}{% endif %}Lastly, they had to create adapters for the old data resources. To do so, they used a technique called High-Order Components (HOC).A HOC is a function that takes a component and returns another component.This technique allowed them to compose components with a resource.When the component is added to the DOM, it will create the resource and send a request to the API.A callback function will update its state and trigger the rendering of the given component. With this approach, you can keep your components and data in sync all the time.You can read more about HOCs hereUI experimentsImad explained that they used an A/B testing framework to measure the impact of the migration.By doing this they managed to see consistent performance and engagement improvements, both of these metrics have improved by 20 percent.Last but not least, migrating to React was also great for the developers: less duplicated code, a single language on client and server and a large developer community Imad was definitely happy to be a part of.Laura Carvajal: YES, your site can (and should) be accessible too. Lessons learned in building FT.comYou can find Laura on Twitter using the handle @lc512k.The presentation can be found here.Laura Carvajali works at the Financial Times. She's responsible for the accessibility of their website so that even blind people are able to use it.In her talk, she explained how to achieve this.Accessibility doesn’t happen by accident, you have to make it happen yourself.A good starting point is to install pa11y (with npm).It checks your HTML and points out where you can improve.Color contrast issues, no or bad alt text for images and no related label for input fields are very common issues pa11y reports on.Pa11y-ci can be used to integrate it with your CI and can break the build when there are errors.Next, there are some extra steps that can be taken.The most expensive one is getting an external audit to get more feedback.They have people that test with voice control, keyboard only mode, text to speech and other tools.A cheaper option is to do customer research and user testing with users with various disabilities.Instead of having other people doing the testing, you could learn how to use the tools for people with disabilities yourself.This is of course the cheapest option.A MacBook for example already has a lot of tools built in for people with disabilities!Day 2 afternoonNikita Baksalyar: Exploring the P2P world with WebRTC &amp; JavaScriptYou can find Nikita on Twitter using the handle @nbaksalyar.A similar presentation as the one given at JSConf can be found here.Nikita Baksalyar is a Software Engineer at MaidSafe. During his talk he explained how we could use newer and not so new technologies to decentralize the web to its former state.The web becomes increasingly centralized. We trust our private data to be stored in data centers despite news about data leaks. We exchange our messages and they are handled to three-letter agencies without you knowing about it. Can we do better and return the web to its decentralized roots? A combination of proven and emerging technologies like WebRTC can help us.What is WebRTC?Whenever you visit a webpage, you’d typically enter a web address or click a link to view a page. A request is made to the server and that server provides the webpage you’ve requested. The key here is that you make a HTTP request to a locatable server and get a response back.Let’s say that you want to do a video chat with mom. Mom’s computer is probably not a web-server, so how will she receive my audio and video data? Enter WebRTC.WebRTC stands for web real-time communications. It is a very exciting, powerful, and highly disruptive cutting-edge technology and standard. WebRTC leverages a set of plugin-free APIs that can be used in both desktop and mobile browsers and is progressively becoming supported by all major modern browser vendors.The primary benefit of WebRTC is real-time peer-to-peer audio and video communication. In order to communicate with another person (i.e., peer) via a web browser, each person’s web browser must agree to begin communication, know how to locate one another and transmit all multimedia communications in real-time.Decentralized networksWhen you think of networks you immediately start thinking of network providers, hubs and the likes. We are moving away from the initial idea of the internet, which was supposed to be a decentralized network. Now what is a decentralized network? A good example of a decentralized network is BitCoin. Data is shared over multiple nodes and those nodes get updated by sending update events.The way forwardThe Internet started as a way to have data spread across the world to make sure that in case of a disaster, natural or human made, data would be preserved.Peer 2 Peer communication is key in both a decentralized and the internet of old. We are making moves towards this redecentralization with the power of WebRTC and other more commonly known technologies such as BitTorrent for file sharing, Distributed git for code, etc.Vaidehi Joshi: Goldilocks And The Three Code ReviewsYou can find Vaidehi on Twitter using the handle @vaidehijoshi.The presentation can be found here.A similar presentation was recorded on RailsConf 2017 and can be found on YouTube.A detailed write-out of that presentation can be found on Medium.The original intent behind code reviews was that they would help us take collective ownership in the creation of our software.In other words, we’d each be stakeholders in our development process by having a hand in controlling the quality of our products.While code reviews are generally understood as being a necessity to catch problems at the “lowest-value” stages (the time at which the least investment has been made and at which the cost to correct the problem is the lowest), Vaidehi Joshi asks whether they actually work and, if not, how can we try to improve upon the process.Based on Code Complete by Steve McConnell, she identified 3 major formats of code review:1. InspectionsInspections are longer, deeper code reviews that typically catch about 60% of defects in a program.2. WalkthroughsA walkthrough is shorter and is usually intended to provide teaching opportunities for senior developers to newer programmers, while giving junior developers the chance to change old methodologies.Typically, they catch about 20 to 40% of the defects in a program.3. Short code reviewsShort reviews are faster, but still in-depth. They focus on small changes, including single-line changes, that tend to be the most error-prone.McConnell’s research uncovered the following about shorter code review:An organization that introduced reviews for one-line changes found that its error rate went from 55 percent before reviews to 2 percent afterward.A telecommunications organization in the late 80’s went from 86 percent correct before reviewing code changes to 99.6 percent afterward.But what do developers think of code reviews?To know this, Vaidehi did a survey on Twitter and got about 500 responses.The survey had questions with a scale of 1 to 10, where 1 was strongly disagree and 10 was strongly agree.These are the stats:The quantitive dataThe question “Code reviews are beneficial to my team” had a clear answer.The average score was around 9 for most languages, with the top 3 containing Swift at an average of 9.46, Ruby at an average of 9.19 and JavaScript at an average of 9.1.Another question was “How many pull requests are reviewed”, on which the majority answered that all pull request were reviewed.However, about 10% of the answers indicated that pull requests where only reviewed when someone was explicitly requested to review.The qualitative dataSo, most developers think code reviews are needed and state that all code is being reviewed.But what do they think of the quality of code reviews?Ultimately, what seemed to make or break a code review experience depended upon two things: how much energy was spent during the review process and how much substance the review itself had.A code review was bad (and left a bad taste in the reviewer’s and reviewee’s mouth) if there wasn’t enough energy spent on the review, or if it lacked substance.On the other hand, if a code review process was thorough and time was spent reviewing aspects of the code in a substantive way, it left a much more positive impression overall on both the reviewer and the reviewee.ENERGYOn the question “Who all is doing the review and how much time are they spending on it?”, a lot of things could be learned.  A developer blindly thumbs-up everything or the second or third reviewer is more likely to agree when already seeing an approval.This makes the code review a formality, which doesn’t carry any weight.  A review is performed different depending on who submits.Seniors get no feedback, while juniors are picked to death.The reviews are unfair and can break confidence.  Commits are too big, which cause long review time, which in turn has a bad effect on future branches/PRs/merges.Long review times take too much energy, which causes them to be postponed.SUBSTANCEThe question “What exactly is someone saying, doing, or making another person feel while they review their code?” brought these answers.  An assessor who takes all the feedback for his own account, having a mentality of “see red squiggle, fix red squiggle”.They just change the code without second thought, as long as it makes the reviewer happy.  A reviewer’s comment is not clearly explained.The reviewee just has to change their code to the reviewers vision.  A reviewer is unable to distinguish between stylistic preference and functional difference, which causes nitpicking at syntax.Multiple reviewers might even have conflicting visions.  Words matter, an unkind review might break confidence.How can one do better?A bad code review almost made me leave the company. A great code review leaves me feeling better equipped to tackle future projects.  Use PR templates.Github provides some default templates for a PR, in which a couple of questions need to be answered short and clearly.  Include screenshots/gifs, providing more context on what is changed and why.  Use linters to eliminate style and syntax nitpicking.  Encapsulating PRs into small packages, aiming for small commits.  Assign specific reviewers, so they may provide valuable input and/or teach or learn something.But even more important  Review everyone: it’s a good horse that never stumbles.A senior developer is not infallible and might even be overconfident.  Develop empathy: call out the good stuff, too.Make people feel less vulnerable, push for a culture that values vulnerability — both in actions and in words.  Most importantly, iterate: start a conversation when feeling that the code review flow doesn’t work well.Give everyone the chance to propose their suggestions.This survey answer summarized the importance of the last part perfectly:I love code reviews in theory. In practice, they are only as good as the group that’s responsible for conducting them in the right manner.Afterparty at EXTRA Budapest by EPAMEPAM invited everyone to chill, have some drinks and games at the EXTRA ruinpub after JSConf Budapest. Beer and a selection of soft drinks and juices are on the house.After Movie    Got triggered?All talks were recorded by the JSconf team. You can view them here.Read our full report on day 1 of JS Conf Budapest 2017 here!"
      },
    
      "conference-2018-01-03-js-conf-budapest-day-1-html": {
        "title": "JSConf Budapest 2017 Day 1",
        "url": "/conference/2018/01/03/JS-Conf-Budapest-day-1.html",
        "image": "/img/js-conf-budapest-2017/header.png",
        "date": "03 Jan 2018",
        "category": "post, blog post, blog",
        "content": "From JSConf Budapest with loveThis year’s edition of JS Conf Budapest returned to the first venue at Urania National Movie theater.  Uránia Cinema in the middle of the city, near the party-district.Designed by Henrik Schmahl in the late 1890’s, the interior is characterized by the ornamental motifs of the Venetian Gothic and Moor styles.The place is listed as the world’s 3rd most beautiful cinema on Bored Panda. Many tech conferences were hosted here recently, such as TEDx and Strech Conference, because of the unique atmosphere.JS Conf Budapest 2017 is hosted by Glen Maddern and Charlie Gleason.First thing to do when entering the building was getting our badges.Then we could have breakfast at some standing tables on the first floor.For the coffee lovers, professional baristas served the best coffee possible. With a nice heart drawn on top if it.At 9:45 the conference would officially start so we went to the conference room.Day 1: Talks  Bodil Stokke: You Have Nothing To Lose But Your Chains  Stefan Judis: Watch your back, Browser! You’re being observed  Jonathan Martin: Async patterns to scale your multicore JavaScript… elegantly  Madeleine Neumann: Impostor syndrome - am I suffering enough to talk about it?  Eirik Vullum: JavaScript Metaprogramming - ES6 Proxy Use and Abuse  Sandrina Pereira: How can Javascript improve your CSS mixins  Kacper Sokołowski: You use Content Security Policy, don’t you?  Dan Callahan: Practical WebAssembly  Luke Bonaccorsi: How I ended up automating my curtains and shouting at my laptopDay 1: MorningBodil Stokke: You Have Nothing To Lose But Your ChainsYou can find her on Twitter using the handle @bodil.The presentation she gave can be found at her personal website.A talk about the open source movement and the Free Software movement it grew out ofThe talk started with a story about Richard and a Xerox printer. Richard is a developer suffering from a minorusability flaw in the Xerox printer at his office. Like the good developer he is, he wants to fix the issue and shareit with the world for everybody’s benefit. Therefore, he needs access to the code. However, it turns out that Xerox’ code for that particular printer is not publicly available. So, Richard can’t fix the issue. He will have to live with the inconvenience, as well as everyone at the office and even everyone using that same printer. The clue here is that a minor fix has to wait until someone at Xerox finds the time to solve the issue. Considering the minor status of the issue, it’s not even likely to happen… ever. With open source software this fix could be done by a motivated user in a few moments.This little intro sets the mood for the talk. One can consider it a bit opinionated, but there are with no doubt some powerful arguments for open source software. The talk also covers the free software movement that all started it and from which the open source movement branched of. The difference though is just in its philosophy. First of all, a commonmisunderstanding is that free software does not mean one can get it with zero cost. It says that anyone can get the code and is free to do with it as pleased. Modify, change, sell or use it for another purpose. On the other hand, open source software provides some restrictions. Therefore, open source software is more popular and used more widely, because it gives control.For example: a concurring company might purchase your proprietary software and then have access to the code. They could copy your product and sell it for a lower price. This can be done with Free software, but open source software has some licenses defined to prevent this. In the talk some of these licenses are covered. I took the liberty to list some of them here with a short explanation. Click trough to see how easily it is to use them and have a legal basis to rely on.The most popular and widely used licenses are:Apache License 2.0Designed to be simple and easy to comply with, but more extensive than the previous versions. One can use the licensed software for any purpose, to change and redistribute. Changes can be distributed under other licenses, but unchanged code needs to be distributed under the same license.3-clause BSD licenseDesigned to be simple and short. It allows unlimited redistribution for any purpose as long as its copyright notices and the license’s disclaimers of warranty are maintained. The license also contains a clause restricting use of the names of contributors for endorsement of a derived work without specific permission. In the 2-clause version that restriction is left out.GNU General Public LicenseSoftware under GNU GPL is free (as in: do with it as you please). The main restrictions defined by this license are thatyou should always mention the authors of the software and it must always stay under the GNU GPL license.MIT LicenseCreated by Massachusetts Institute of Technology. It has one simple rule: the copyright statement should stay in anycopy of the software. ‘Copyright (c) &lt;year&gt; &lt;copyright holders&gt;’Mozilla Public License 2.0Code under MPL can be copied or changed, but must stay under MPL. The code can be combined with closed source files.Open source should be considered by many companies, since many can benefit from open source. SpaceX for example benefits from open source software, non the less their own code is closed. Another company by the same founder, Elon Musk, has its code publicly available: Tesla, the electric car manufacturer. Here’s a part I found on  Tesla’s own blog. At Tesla we felt compelled to create patents out of concern that the big car companies would copy our technology and  then use their massive manufacturing, sales and marketing power to overwhelm Tesla. We couldn’t have been more wrong.  The unfortunate reality is the opposite: electric car programs (or programs for any vehicle that doesn’t burn  hydrocarbons) at the major manufacturers are small to non-existent, constituting an average of far less than 1% of  their total vehicle sales. While the competition might benefit from sharing your code, so does the world. This counts for Tesla in particular. While there might be a huge market for electric vehicles, we also need them as fast a possible. Open source software can help us achieve that goal.Stefan Judis: Watch your back, Browser! You’re being observedYou can find Stefan on Twitter using the handle @stefanjudis.The presentation can be found on speakerdeck.To get information from a browser, you always had to do a pull. However, it's now also possible to ask the browser to push this information to you when something has changed by using observables!Verifying whether an element has become visible in the viewport is a very common use case. If you have to pull that information from the browser, it’s also a very heavy one since the piece of code doing that verification, is run each time a scroll event is fired. A better way would be to have the browser letting us know when an element has reached the viewport. Therefore, browsers offer a so called IntersectionObserver through JavaScript. When creating an IntersectionObserver you can pass it a callback function which will be fired when the observed elements enter or leave the viewport. Optionally you can also pass some options such as how much of the element should become visible/hidden in the viewport.Unfortunately Safari doesn’t support this feature yet, but luckily, it’s polyfillable.There are several more observers such as:  MutationObserver - fires when an attribute of an observed element has changed (supported by all major browsers)  ResizeObserver - fires when an element is resized (behind a flag in Chrome, not yet supported in other major browsers)  PerformanceObserver - emits metrics about the performance of the web page (e.g. time to paint, mark statements, navigation time…) (supported by all major browsers except Edge)Another great benefit of these observers is that all functions RxJS offers us (e.g. skip, pairwise, filter …), can be used as well!The emitted values of the observers are collections so we can use functions such as map, filter and reduce there as well.As mentioned in the presentation, these two combined gives us “Collection super powers!”.Jonathan Martin: Async patterns to scale your multicore JavaScript… elegantlyYou can find Jonathan on Twitter using the handle @nybblr.The presentation can be found on speakerdeck.“JavaScript is single-threaded, so it doesn’t scale. JavaScript is a toy language because it doesn’t support multithreading.”Outside (and inside) the web community, statements like these are common.In a way, it’s true: JavaScript’s event loop means your program does one thing at a time.This intentional design decision shields us from an entire class of multithreading woes,but it has also birthed the misconception that JavaScript can’t handle concurrency.In fact, JavaScript’s design is well-suited for solving a plethora of concurrency problemswithout succumbing to the “gotchas” of other multithreaded languages. You might say that JavaScript is single-threaded…just so it can be multithreaded!Before diving into solving concurrency problems, Jonathan explained how the (V8) JavaScript runtime actually works and reacts under the hood.Next, he told us how the call stack, event loop WebAPIs and the callback queue works and how it handles synchronous (blocking) and asynchronous (non-blocking) code.Explaining that would be an entire blog post on its own. Luckily he gave us a great link to a video that explains it very clearly, so I’ll add that instead.Philip Roberts: Help, I&#039;m stuck in an event-loop.What is concurrency, multi threading and parallelismSo if you’ve just watched the video above, you know that JavaScript has one call stack (a single thread) and executes the functions in sequence.With multithreading, as the word says, we have multiple threads.This means that the program can assign these tasks to multiple stacks so that multiple tasks get executed at the same time.In a computer with a single processor and single core, to do multi threading,the processor would alternate between these tasks really fast so that they appear to be happening at the same time.Back in the early days of computing, this was the only option we had. This is called concurrency.Around 2005 Intel, AMD and the other chip manufacturers started creating processors with multiple cores. This meant it could actually do multiple things at the same time, since it had multiple “brains”.Processors could now assign different tasks to different cores and they would run at the same time. This is what we call parallelismJavaScript multi threading: impossible?Although your JavaScript code is single-threaded and only does one thing at a time, the JavaScript Runtime and Web APIs are multithreaded!When you pass a callback function to setTimeout() or start an AJAX request with fetch(),you are essentially spinning up a background thread in the runtime. Once that background thread completes and the current call stack finishes executing, your callback function is pushed onto the (now empty) call stack and run-to-completion.So your JavaScript code itself is single-threaded, but it orchestrates legions of threads!ES2017 async functionsThe title of his talk contained the word ‘Elegant’ and this is where the ES2017 async/await functionality comes in.This is a great alternative for dealing with promises in JavaScript. If you’re a JavaScript developer you probably know what ‘callback hell’ is, or at least heard of it.When writing complex programs, we could find ourselves in a situation where we would have to create multiple nested Promises to make sure we have the results of one call to continue with the next and so on.Async - declares an asynchronous function (async function someName(){...}).  Automatically transforms a regular function into a Promise.  When called, async functions resolve to whatever is returned in their body.  Async functions enable the use of await.Await - pauses the execution of async functions. (var result = await someAsyncCall();).  When placed in front of a Promise call, await forces the rest of the code to wait until that Promise finishes and returns a result.  Await works only with Promises, it does not work with callbacks.  Await can only be used inside async functions.// Promise approachfunction getJSON(){    // To make the function blocking we manually create a Promise.    return new Promise( function(resolve) {        request.get('https://myurl.com/example.json')            .then( function(json) {                // The data from the request is available in a .then block                // We return the result using resolve.                resolve(json);            });    });}// Async/Await approach// The async keyword will automatically create a new Promise and return it.async function getJSONAsync(){    // The await keyword saves us from having to write a .then() block.    let json = await request.get('https://myurl.com/example.json');    // The result of the GET request is available in the json variable.    // We return it just like in a regular synchronous function.    return json;}If you’re a beginner with async functions and want to learn more this topic, check out this videoFor further reading on how Jonathan used async patterns for multicore JavaScript, he has written an elaborate blog post about it. We suggest you go check it out!Madeleine Neumann: Impostor syndrome - am I suffering enough to talk about it?Madeleine is a front-end developer at 9Elements. She’s also a conference organiser of RuhrJS.You can find Madeleine on Twitter using the handle @maggysche.The presentation can be found on slideshare.The reason we struggle with insecurity is because we compare our behind the scenes with everyone else’s highlight reel.Madeleine wanted to share her life experience with us. While she attended secondary school, Madeleine was the creepy loner. ‘What’s wrong with me?’, ‘What did I do wrong?’ she asked herself on several occasions. ‘My behaviour must be wrong, I have to change’. So she decided to take up programming in high school and felt truly belonged.After Madeleine graduated high school, she started to work as a front-end developer where she was learning a lot, very quickly! However, she soon discovered that the speed at which she was learning gradually stagnated. She had mixed feelings about her profession and abilities, thinking she did not belong there and had no idea what she was doing.So, she decided to work even harder. All of her friends and colleagues congratulated her for her effort and hard work, but Madeleine still wasn’t satisfied. Shortly after, she learned about “the imposter syndrome”.Here are some common signs that someone might experience, where one feels like an imposter:  Does everyone overestimate you?  Do you tend to discuss yourself?  Do you compare your ability to those around you and think they’re more intelligent than you are?  Does the fear of failure freak you out?  Sometimes you’re afraid others will discover how much knowledge you really lack?  You can’t understand the compliments you receive?  You feel bad when you’re not ‘the best’ or at least ‘very special’?  You avoid evaluations if possible and have a dread of others evaluating you?  Do you focus more on what you haven’t done?Madeleine discovered that her answer to all the previous questions was ‘yes’ and came to the conclusion she sabotaged herself. Now, how do you escape the ‘imposter zone’?  You aren’t born to live a life of another person  Learn to be a healthy perfectionist  Answer on the following question ‘What would I do, if I was not afraid?’  Ask for help  Mentor people what you’re doing  It’s a good thing to know, what you don’t know  Talk about it  Bribe your friends  Being wrong doesn’t make you a fraud  Focus on providing value and write it down  Keep a file of nice things someone has said about you  Stop commenting compliments  And finally, take time for yourselfMadeleine learned that sometimes, it’s not that bad to be an imposter. Because if you are an imposter, you are an overachiever and you can surprise people with your talent.Day 1 afternoonEirik Vullum: JavaScript Metaprogramming - ES6 Proxy Use and AbuseYou can find Eirik on Twitter using the handle @eiriklv.The presentation can be found here.This very interesting talk handles metaprogramming in JavaScript. Recently a new feature in ES6 was added to all major browsers, making JavaScript even more exciting! First of all, what is metaprogramming? According to wikipedia: ‘The ability to read, generate, analyse or transform other programs, and even modify itself while running’. That is clear enough in my opinion.In metaprogramming one can define 2 branches. The first branch could be described as macros to extend your language. This happens during compile/transpile time. The second branch is called reflection and happens at runtime. There are three forms of reflection:  Introspection: the ability to examine itself  Self-modification: the ability to alter its structure  Intercession: the ability to alter its behaviourIn JavaScript they are possible by default. Lets call it a perk of this beautiful scripting language. However, it seldom results in readable code and you’ll probably need to write a lot of code for something we can now achieve in a much easier way.This talk covers some of the possibilities of proxies. Proxies couldn’t be used until recently, because it isn’tpolyfillable. It’s a feature that needs to be supported by the engine, where reflection truly happens. Therefore, nor typescript, nor babel, nor any other JavaScript preprocessor could solve that for you. By the way, preprocessors extend your language through macros, since their magic happens at transpile time.So what is this proxy I am so exited about? It’s called a proxy after the same principle we use in networking.A proxy is middleware that intercepts interaction with an interface. Therefore, it has access to the communication stream and it needs access to the interface it’s routing to. That’s very similar to how we can use proxies in JavaScript. We can wrap any object with a proxy and define a handler. That handler is an object containing traps. A trap is a function that ‘mocks’ a property or function from the object that is being proxied. The proxy then knows which actions will be performed (before they are actually performed) and can choose how to handle them. It could do something totally different or even nothing at all.let handler = {    set: (object, prop, value) =&gt; {        // do what you desire (alter the value for example);        object[prop] = value; // this wil execute the default setter        return true; // to indicate success    },        get: (object, prop) =&gt; {        let value = object[prop]; // this wil execute the default getter        // do what you desire        return value;    }};let mySquare = new Square(10,10);let myProxySquare = new Proxy(mySquare, handler);The above handler will intercept all get and set calls to a proxied class. get and set methods here are so called traps. For what purposes can we use this? One of the main purposes is to create developer friendly interfaces. In the slides you’ll find some nice examples of great uses. My favorite is the url builder, it’s glorious in its simplicity, check it out here.Now to wrap it all up, proxies are awesome, we can create powerful abstractions, be lazy and write less code and addfunctionality in a transparent way. Even though it might seem like magic for anyone else than yourself and despite a small performance cost, it’s still perfect if you want to create clean interfaces for others to enjoy.Sandrina Pereira: How can Javascript improve your CSS mixinsSandrina is UI Developer at Farfetch. You can find Sandrina on Twitter using the handle @a_sandrina_p.All code can be found on her GitHub page and the slides here.To write good tests, you have to know exactly what you need to do. When you know what to do, you do less.When you do less, you can do better!CSS and JavaScript work together more than ever these days. Using the good parts of both worlds ensures us that we can get better in web development. One of the reasons is because CSS primarily doesn’t have logic behind it. It’s simple and straightforward. However, when you have to start using logic in your CSS, you can for example add a loop with SCSS.When you find yourself reusing the same CSS code over and over, you can write a mixin.However, at the end of the day, things can get ugly. Therefore, many programmers use PostCSS to write logic in their CSS code. There are more than half a million downloads per month of PostCSS plugins!Here’s how you write a mixin in CSS:// index.css @define-mixin size $value { \twidth: $value;\theight: $value;}.avatar { \t@mixin size 20px;}This is how it works in JavaScript:// size.jsmodule.exports = (mixinNode, value) =&gt; ({ \twidth: value,\theight: value,})// postcss.config.jsmodule.exports = {    // ...   \tplugins: [\t\trequire('postcss-mixins')({ \t\t\tmixindsDir: '../src/mixins/',\t\t}, \t\t// ... \t]}// index.css.avatar {\t@mixin size 20px;}Now, we can’t test logic in CSS, but in JavaScript we can!// size.test.jsimport size from '../src/mixins/size.js';test('Size returns width and height', () =&gt; {  expect(size(null, '24px').toEqual({      width: '24px',      height: '24px'    });});So you started to use CSS mixins with JavaScript and ended up with a folder full of mixins to improve your CSS. Instead of using a series of mixins in the CSS file itself that only improve your project, we can create a custom property with the PostCSS plugin called ‘Boilerplate’.Using that, we can do the following:// index.css.avatar {  size: 20px;}// index.jsconst postcss = require('postcss');postcss.plugin('postcss-size', () =&gt; css =&gt; {  // let’s transform CSS with JS  css.walkDecls('size', decl =&gt; {      // 1. get the size value       const value = decl.value;      // 2. add “width” &amp; “height” properties      decl.cloneBefore({ prop: 'width', value });      decl.cloneBefore({ prop: 'height', value });      // 3. remove “size” property       decl.remove();  });});// index.test.jsconst plugin = require('./index.js');const postcss = require('postcss');function run(input, output) { ... };test('Sets width and height', () =&gt; {  return run(    '.foo { size: 1px; }',    '.foo { width: 1px; height: 1px; }'  );});After you execute the command npm publish in the console, you aren’t only going to improve your own project, but everyone’s projects.You can find other popular PostCSS plugins here.That’s why I came here today. To share something that improved my project and might improve yours as well. I believe sharing is what make us better.Kacper Sokołowski: You use Content Security Policy, don’t you?Kacper is a front-end developer for Codewise.He’s also a speaker and community organiser for KrakowJS.You can find Kacper on Twitter using the handle @kaapa_sThe presentation can be found hereEveryone knows that security is important right?The biggest companies like Facebook and Google spend tons of money on bug bounty programs to ensure that their products are secure.But is there a way that we can make our website prone to some of the most popular attacks?There is one security mechanism which can help, but yet not everyone knows and uses it.It’s called Content Security Policy.Kacper started his presentation with an example to demonstrate why security is hard.In 2005, Kamkar released the Samy worm, the first self-propagating cross-site scripting worm, onto MySpace.The worm carried a payload that would display the string \"but most of all, Samy is my hero\" on a victim's profile and cause the victim to unknowingly send a friend request to Kamkar.When a user viewed that profile, they would have the payload planted on their page.Within just 20 hours of its October 4, 2005 release, over one million users had run the payload, making it the fastest spreading virus of all time.XSSCross Site Scripting (XSS) was used to inject and spread the virus.It’s a technique to inject and execute any JavaScript code in the context of the page.What can you do with XSS?  Steal cookies  Steal localstorage data  Break the layout and style of the page  Whatever you can do with JavaScript…You can find a lot of information about XSS and other vulnerabilities on this website: https://www.owasp.orgHOW TO BE SAFE?!CSPContent Security Policy (CSP) is an added layer of security that helps to detect and mitigate certain types of attacks, including Cross Site Scripting (XSS) and data injection attacks.Inline code is considered harmful so don’t use something like this:&lt;script&gt;alert('hello JSConfBP!');...&lt;/script&gt;Instead externalise your code and do something like this:&lt;script src=\"...\"&gt;&lt;/script&gt;HTTP HEADERSWhen you have externalised your scripts, you need to make sure your site only loads these scripts.To enable CSP, you need to configure your web server to return the Content-Security-Policy HTTP header.Specifying your policy:Content-Security-Policy: script-src ‘self’ http://google.com …Specifying your directive(s):Content-Security-Policy: script-src ‘self’ http://google.com …Specifying the URL list:Content-Security-Policy: script-src ‘self’ http://google.com …Other directives you can use:  connect-src  img-src  script-src  style-src  …You can use the fallback directive for other resource types that don’t have policies of their own: default-srcConclusionMany parts of your website will probably break when you CSP for the first time.So, start using it as early as possible!Dan Callahan: Practical WebAssemblyYou can find Dan on Twitter using the handle @callahad.In this talk Dan explained what WebAssembly is all about. How it works, what it's for, the features that are already there and which features are yet to come.WebAssembly, what is it?Well, according to http://webassembly.org/:\"WebAssembly or wasm is a new portable, size- and load-time-efficient format suitable for compilation to the web.\"A compiler for the web:  Low-level, binary format for programs:  WebAssembly is a fast, portable, compact, cross-browser binary format for compilation to the web.  It’s an open standard supported by all major browsers. caniuse.com  Direct successor of asm.js  General purpose virtual architecture  It allows new types of applications and heavy 3D games to run efficiently in browsers.Why?Performance!WebAssembly is a binary format for JS.It has 2 major benefits:  The JS engine can skip the parsing step  It’s much more compact than the JS original sourcePortabilityAt the moment of writing this blog, there are two languages that can compile into wasm, those are C/C++ and Rust.This is great for portability since code written in C works on Mac, Linux and Windows.Is JavaScript dead?JavaScript is alive, but its client-side monopoly is dead.WebAssembly doesn’t replace JavaScript, but does expand the web and complements JavaScript:  High Level (JS) vs. Low Level (WASM)  Text (JS) vs. Binary (WASM)Unity SupportWhen it comes to creating 3D games, Unity also has experimental support for WebAssembly.Check out this demo of an in browser gameUnreal EngineThis is a video of Epic’s “Zen Garden” demo running in Firefox.The demo is built with WebAssembly and WebGL 2, both emerging standards that enable amazing video games and applications in the browser.    What about older browsersUse asm.js as a fallback.When using Binaryen with Emscripten, it can load the compiled code using one of several methods.By setting -s BINARYEN_METHOD='..' you can specify those methods, as a comma-separated list. It will try them one by one, which allows fallbacks.By default, it will try native support. The full list of methods is:  native-wasm: Use native binary wasm support in the browser.  interpret-s-expr: Load a .wast, which contains wasm in s-expression format and interpret it.  nterpret-binary: Load a .wasm, which contains wasm in binary format and interpret it.  interpret-asm2wasm: Load .asm.js, compile to wasm on the fly and interpret that.  asmjs: Load .asm.js and just run it, no wasm. Useful for comparisons or as a fallback for browsers without WebAssembly support.Can I compile JS to WASM?Don’t do that!Browsers will still have native JavaScript VM along-side wasm.There’s no reason to compile JS to wasm because you would also have to include a whole JavaScript VM.The resulting code would be huge and slower than the JS VM natively provided.Interesting Links:            Here you can translate C/C++ to WebAssembly and see the machine code generated by the browser.                YouTube video on what WebAssembly means for React        Tanks demo Unity game on webassembly.orgLuke Bonaccorsi: How I ended up automating my curtains and shouting at my laptopYou can find Luke on Twitter using the handle @lukeb_uk.The presentation can be found hereBeing lazy can lead to some great out of the box thinking and finding innovative solutions for common everyday stuff.Luke talks about how he created a chatbot that automates things for him to make them a bit less common and/or boring.WoodhouseHe would have named it Jarvis, but since this would be a far worse butler than Jarvis was, he named it Woodhouse after the butler character from the tv-series 'Archer'.Around mid-2014 he started working on a chatbot that does little bits in his house. Basically, he put together a Raspberry Pi running JavaScript code that actually serves as a router with some core functionality built in like:  Broadcasting  Preference storage  SchedulingThere’s two types of modules that make it up:  Plugins do all the heavy-lifting so you can interface with hardware (as long as it’s possible with JavaScript or the node ecosystem) or get it to send a message. You could for example let it connect to API’s to get it to do your builds on your CI tools.  The interfaces which are basically chat systems. They are the way to talk to the chat bots. If the system has a way for you to build stuff for it in JavaScript, you can connect to it and let it do stuff on for example: Facebook, Slack, HipChat and many more…Open sourceAll of it is open source (MIT) and is avaiable on GitHub. It’s written in JavaScript and runs on NodeJs.Automating Lamps    Sending a message in a chat application that gets picked up by Woodhouse and he/it then turns on his lamps at home.So, as he walks down the street getting to his house, instead of coming home and stumbling over things searching for the light switch in the dark,he can just send a message and the lights will be on when he gets there.Lamp plugs (they are from China, so super safe, right?). Maybe not, but they cost about £15 and are great for poking around.After doing so, he found out that there was a Google group that had been hacking around with them and found the SSH password for it.It turned out it runs OpenWrt which is a router firmware based on Linux.So, after being able to SSH into it and work with the Linux installed on it, you can run basic scripts on it (it has limited memory so you can’t just install everything you like on it).But most importantly, it’s got a web server built into it, so you can hit an endpoint and make the relay turn on. That’s how his relay works.There’s an endpoint on the plug and when he goes to that endpoint, it switches it on or off depending on a parameter.Automating the curtainsFor giving talks about the application, he wanted to add something new to the application and so… he automated his curtains.    The setup and parts for it are very basic and simple. It’s basically some string, plastic wheels, a servo and an esp82266.The esp8266 is a wireless Arduino type board, but the cheap Chinese version so you can buy loads of them and connect themto your network. So for about £2 each you can control stuff over your network from anywhere. It runs Mongoose OSwhich lets you write JavaScript on your hardware, it takes away a lot of the complexities of the lower level code and lets you use a language you know.Shouting at his laptopNot out of frustration or anything like that. Besides the chatbot, he wanted to add voice control to the application so that he could tell his laptop to open/close the curtains or turn on/off the lights.    It uses a NodeJs library for offline ‘hot words’ detection. So instead of having it constantly listening to him,he can just shout ‘Woodhouse’ which will make it reply to say that it’s listening. The rest of the complex speech to text is done by Google,since they have a lot more data than him. There are open source systems for doing speech to text, but you would have to train it yourself and well, we’re doing all this because we want to be lazy…So he created a few of these voice control units and spread them around the house and let them connect to one central instance. So he can activate it from wherever he is in the house.ConclusionSo instead of being lazy, he admits to being the stupid kind of lazy. He has spent about hundreds of hours coding for it to do simple stuff.So it’s not really about being lazy, but more being not driven to do those simple things.Party feat Live:JS by SINNERSCHRADERAfter a long day of JavaScript fun we were invited to a rooftop party at Corvinteto located near the venue.Imagine a party with awesome visuals, music &amp; beats, and lights - all powered and created by JavaScript!More info about the concept can be found here: LiveJS    Day 1: ConclusionAt the first day of the conference we were already inspired by some good talks. Wondering what day 2 would bring.Read our full report on day 2 of JS Conf Budapest 2017 here!"
      },
    
      "microservices-2017-12-30-secure-your-architecture-part2-html": {
        "title": "Securing your cloud-native microservice architecture in Spring: part 2",
        "url": "/microservices/2017/12/30/Secure-your-architecture-part2.html",
        "image": "/img/microservices/part2/part2logo.jpg",
        "date": "30 Dec 2017",
        "category": "post, blog post, blog",
        "content": "Since the rise of the digital era, most enterprises keep their data in a digital format.But if their sensitive data lacks security, it can cause the data to be unreliable, unstable and unavailable to their business.We have to be prepared if an attacker breaches into our network and tries to hack our sensitive data.Whether it is in motion or at rest, encrypting our data and using the proper protection mechanisms will make it worthless for the hacker to use.Overview  Securing your cloud-native microservice architechture in Spring: Part 1  Cryptographic Algorithms  Key Mechanics  Cloud-hosted Key management service  Spring Cloud Config ServerCryptographic AlgorithmsWhen implementing our application, every programming language will provide us with a set of known libraries for cryptographic algorithms. A big flaw is implementing an algorithm by yourself, the known algorithms have been reviewed, patched and been known for their excellent security. These are the most used types that you can use for encryption at rest:Symmetric EncryptionThe key used in encrypting data at rest is used for both encrypting and decrypting the data.This key becomes very vulnerable if anyone gets a hold on it.  Well known: Advanced Encryption Standard encryptionAsymmetric EncryptionIn asymmetric encryption, a pair of keys are used. A public key that is exposed and encrypts your data and a private key that is only known by the owner that decrypts your data.This key-pair can also be used to sign your data, so the application knows that it can trust the source of the data.  Well known:  Rivest–Shamir–Adleman encryptionKey MechanicsEncryption keys are another aspect of encryption, handling the keys becomes just as sensitive as the data itself. That’s why we need mechanisms on how keys are stored and shared so attackers can’t get a hold on them.Key rotationEncryption key rotation will provide protection especially when the certificate expires, is corrupted or the key management admin is no longer part of the company. Lets say, you got a good eye at detecting patterns and detect that the same key is being used for encrypting data.To avoid this, you rotate your keys, and every time the same data field is encrypted it will result in a different encrypted message.JSON Web Key (Set)We discussed in the previous post about retrieving a JWK(S) to verify our JSON Web Token in our microservice. A JWK is a JSON object that represents a cryptographic key that consists of information to verify a JWT. If you like to dive into signing JSON documents you can check out this blog post on Digitally signing your JSON documents.JWKS example:{\"keys\": [  {    \"alg\": \"RS256\",    \"kty\": \"RSA\",    \"use\": \"sig\",    \"x5c\": [      \"MIIC+DCCAeCgAwIBAgIJBIGjYW6hFpn2MA0GCSqGSIb3DQEBBQUAMCMxITAfBgNVBAMTGGN1c3RvbWVyLWRlbW9zLmF1dGgwLmNvbTAeFw0xNjExMjIyMjIyMDVaFw0zMDA4MDEyMjIyMDVaMCMxITAfBgNVBAMTGGN1c3RvbWVyLWRlbW9zLmF1dGgwLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMnjZc5bm/eGIHq09N9HKHahM7Y31P0ul+A2wwP4lSpIwFrWHzxw88/7Dwk9QMc+orGXX95R6av4GF+Es/nG3uK45ooMVMa/hYCh0Mtx3gnSuoTavQEkLzCvSwTqVwzZ+5noukWVqJuMKNwjL77GNcPLY7Xy2/skMCT5bR8UoWaufooQvYq6SyPcRAU4BtdquZRiBT4U5f+4pwNTxSvey7ki50yc1tG49Per/0zA4O6Tlpv8x7Red6m1bCNHt7+Z5nSl3RX/QYyAEUX1a28VcYmR41Osy+o2OUCXYdUAphDaHo4/8rbKTJhlu8jEcc1KoMXAKjgaVZtG/v5ltx6AXY0CAwEAAaMvMC0wDAYDVR0TBAUwAwEB/zAdBgNVHQ4EFgQUQxFG602h1cG+pnyvJoy9pGJJoCswDQYJKoZIhvcNAQEFBQADggEBAGvtCbzGNBUJPLICth3mLsX0Z4z8T8iu4tyoiuAshP/Ry/ZBnFnXmhD8vwgMZ2lTgUWwlrvlgN+fAtYKnwFO2G3BOCFw96Nm8So9sjTda9CCZ3dhoH57F/hVMBB0K6xhklAc0b5ZxUpCIN92v/w+xZoz1XQBHe8ZbRHaP1HpRM4M7DJk2G5cgUCyu3UBvYS41sHvzrxQ3z7vIePRA4WF4bEkfX12gvny0RsPkrbVMXX1Rj9t6V7QXrbPYBAO+43JvDGYawxYVvLhz+BJ45x50GFQmHszfY3BR9TPK8xmMmQwtIvLu1PMttNCs7niCYkSiUv2sc2mlq1i3IashGkkgmo=\"    ],    \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\",    \"e\": \"AQAB\",    \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\",    \"x5t\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\"  }]}Explanation properties:  alg: is the algorithm for the key  kty: is the key type  use: is how the key was meant to be used. For the example above sig represents signature.  x5c: is the x.509 certificate chain  e: is the exponent for a standard pem  n: is the modulus for a standard pem  kid: is the unique identifier for the key  x5t: is the thumbprint of the x.509 cert (SHA-1 thumbprint)Cloud-hosted Key management serviceKMS is a fully managed service that allows you to manage your encryption keys in the cloud.Most of these KMSs offer the best way for encryption and generate, rotate and destroy your keys. But the KMS is vendor lock-in so all your keys will stay on the platform.To avoid vendor lock-in, we can implement our own open source version for managing our encryption keys.A few examples to get an idea of KMS:  Google Cloud KMS  AWS KMS  Azure VaultA few examples of open-source variants:  Spring Cloud Config Server  HashiCorp’s Vault  KeywhizSpring Cloud Config ServerThe Spring Cloud Config Server provides a centralized external configuration management backed optionally by a Git repository or database.Using a REST API for external configuration, Config Server supports encryption and decryption of properties and yml files. First step is downloading the Java Cryptography Extension on our local pc.  JCE provides a framework and implementation for encryption, key generation, key agreement and message authentication code algorithms. You’re not installing JCE itself, because it’s packaged within the Java SE binary.However, you do need to update its policy files from time to time.Downloads are available for Java 6, 7 and 8.This will allow the config server to use the encryption tool of the JCE.After the download, the next step will be securing the config server by adding Spring Security to the classpath and configuring your Basic/OAuth2 authentication.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;Key ManagementThe config server supports encryption and decryption with a symmetric key or an asymmetric key-pair.The choice of which key you will need is within your security terms. The symmetric key is the easiest way to set up but less secure than the asymmetric one. To set up a symmetric key, you just assign a string to the key holder: encrypt.key=&lt;key&gt;To configure these asymmetric keys, we will need a keystore created by the keytool utility from the JDK.The public key will encrypt and the private key will decrypt your data.To create a keystore you can do something like this in your command line:$&gt; keytool -genkeypair -alias &lt;keyname&gt; -keyalg RSA -keysize 4096 -sigalg SHA512withRSA \\  -dname \"CN=Config Server,OU=JWorks,O=Ordina,L=Mechelen,S=State,C=BE\" \\  -keypass &lt;secret&gt; -keystore config-server.jks -storepass &lt;password&gt;This will generate a keystore for the config server to use. Place it in your repository project and configure it in your yml. Warning: Be aware if you package your keystore within your application jar/war file, the same encryption keys will be used across all of your environments!Example yml in the config server:encrypt:  key-store:    location: classpath:config-server.jks // resource location    password: &lt;password&gt; // to unlock the keystore    alias: config-server-key // to identify which key in the store is to be used    secret: &lt;secret&gt;EncryptionTo encrypt the data, start up your config server locally and enter this in your command line.$&gt; SECRET=$(curl -X POST --data-urlencode secret http://user:password@localhost:&lt;port&gt;/encrypt)$&gt; echo \"datasource.password=$SECRET\" &gt;&gt; application-dev.ymlWhen the encryption is done, we get an encrypted piece of data in your configuration in the form of:spring:  datasource:    username: dbuser    password: '{cipher}FKSAJDFGYOS8F7GLHAKERGFHLSAJ'What to store where?When designing your config server, you have different options on where and to which our config server has access.Using a Git repositoryThe default and most common way most of us use is via private Git repositories where we store our sensitive data where the config server can fetch it.Be aware, never put configuration inside your code repository, it violates the twelve-factor app which requires strict separation of config from code. Config varies substantially across deploys, code does not.Health checksYou can enable the health check to the config server within the application.If you do this, always look at which version control would be the best fit, always check when they go into maintenance. It could be that they host it in another timezone, which could lead to a cascading failure.In my opinion, you can just disable the health checks with spring.cloud.config.server.health.enabled=false and avoid further failures. If you expect that the config server might go down temporarily when your client app starts, please provide a retry mechanism after a failure. To enable a retry, first add spring-retry to your classpath with @EnableRetry annotation and spring.cloud.config.failFast=trueUsing JDBCNew to this list is the support for JDBC. This enables us to store configuration properties inside a relational database. By switching the active spring profile to JDBC and adding the dependency of spring-jdbc to your classpath, Spring Boot will configure the datasource you included on the classpath.To store the data you will need to set up new tables in your database.For more information: using JDBCUsing VaultHashiCorp’s Vault provides a centralized external management server. Vault can manage static and dynamic secrets such as username/password for remote applications/resources and provide credentials for external services such as MySQL, PostgreSQL, Apache Cassandra, MongoDB, Consul, AWS and more.Spring supports using the Vault as a backend for Spring Cloud Config.If you are using Spring Boot, a quick way to enable Vault is to set your spring profile to vault. Spring Boot’s conditionals will activate all the auto configuration for a connection with the Vault server.Using File SystemSo when you’re working locally on your machine, you can always look at the native profile to activate the file system as your “backend”.But I don’t recommend it for use in a deployment environment since it comes with various problems and extra setup.One of those problems would be high availability, unlike Eureka, the config server doesn’t have the concept of peers.The obvious option is to use a shared file system but it requires extra setup.ConclusionWith the latest technologies coming up, you can expect that our data will be stored in an immutable ledger that is secured by cryptography.But we have to be aware of the arrival of quantum computers. This could make the best encryption algorithms useless. But as always, we will find a way to protect ourselves…Sources  Advanced Encryption Standard encryption  Rivest–Shamir–Adleman encryption  Google Cloud KMS  AWS KMS  Azure Vault  The twelve-factor app  Using JDBC  HashiCorp’s Vault  Vault as a backend  Spring Cloud Config Server  Keywhiz  JCE 6  JCE 7  JCE 8"
      },
    
      "architecture-2017-12-27-deploying-web-applications-with-environment-specific-configurations-html": {
        "title": "Deploying web applications with environment-specific configurations",
        "url": "/architecture/2017/12/27/Deploying-web-applications-with-environment-specific-configurations.html",
        "image": "/img/2017-12-27-Deploying-web-applications-with-environment-specific-configurations/post-image.jpg",
        "date": "27 Dec 2017",
        "category": "post, blog post, blog",
        "content": "The problemRecently one of my colleagues came across a problem when he wanted to create an Angular application which needed to have different configuration values between environments.In 2016, Jurgen Van de Moere wrote a blogpost which explained how to create environment-agnostic applications with AngularJS.A year later, Rich Franzmeier explained in his blogpost a solution for Angular applications which was based on Jurgen’s post.Although both solutions work perfectly, they both have some downsides to it.Nowadays we want to push devops to all teams.This implies that our applications should be immutable, and that everything should be automated (as much as possible).If we want to deploy our application we have to overwrite our configuration file manually, so this was the main disadvantage of the solution they proposed.The solutionOur solution is build on top of theirs, and should be able to work for all kinds of frameworks (AngularJS, Angular, React, Vue,…).Here is how we did it.Code setupWe started with an env.js file which contains all environment-specific configuration.This file will expose the data as global variables.One could set the URL where our API is hosted like this:(function (window) {  window.__env = window.__env || {};  // API url  window.__env.apiUrl = 'http://dev.your-api.com';}(this));Next up is to expose this variable to our web application, which in our case is an Angular app.We created an injectable Configuration class, which will have a getter to retrieve our API URL.import {Injectable} from '@angular/core';function _env(): any {  // return the native window obj  return window.__env;}@Injectable()export class Configuration {    get apiUrl(): any {    return _env().apiUrl;  }  }All we need to do now is to inject an instance of the Configuration class in the class where we need the info.Deployment setupAfter building our Angular app, we need to package everything together.Since we want to make everything immutable, the obvious choice here is to use Docker.While starting a container, we can specify environment variables.This is where the second part of our solution starts.Using the envsubst bash command we’re going to convert a template file into an env.js file which contains all our data.Our env.js.tpl file looks like this:(function (window) {  window.__env = window.__env || {};  // API url  window.__env.apiUrl = '$API_URL';}(this));To perform this conversion we need to create a startup.sh script which will actually execute the envsubst command.envsubst &lt; /usr/share/nginx/env.js.tpl &gt; /usr/share/nginx/html/env.jsnginx -g 'daemon off;'Now we need to create a Dockerfile which in turn will be used to create our Docker image.FROM nginx:1.13-alpineCOPY dist /usr/share/nginx/htmlCOPY env.js.tpl /usr/share/nginx/env.js.tplCOPY startup.sh /usr/local/bin/startup.shENTRYPOINT [\"/bin/tini\", \"--\", \"/usr/local/bin/startup.sh\"]ConclusionThis is how we managed to build our application only once, and deploy across numerous environments.Although it is not as immutable as we want it to be, at least we have automated the process to get rid of most human errors.And this way we can easily create a separate environment if there is a bug in production."
      },
    
      "architecture-2017-12-22-tech-lead-html": {
        "title": "Twelve tips to become an awesome Technical Lead",
        "url": "/architecture/2017/12/22/Tech-Lead.html",
        "image": "/img/tech-lead/world.png",
        "date": "22 Dec 2017",
        "category": "post, blog post, blog",
        "content": "What is Technical LeadershipA Technical Lead has the responsibility to help the team move forward. The person assigned to the role, is someone who has sound technical experience and strong communication skills. He or she will be accountable for the technical direction of the project or product and serve as the go-to person for cross-team interactions.  When it comes to medium to large teams it is quite common to have a full-time Tech Lead present, responsible for important leadership activities such as\tGuiding the project technical vision.\tEg. what technology are we going to use, how are we going to deliver the project, what patterns will we use, etc.\tAnalyzing risks and cross-functional requirements.\t\t\t\t\t\tAnalysing risk means mitigating risk: can we chose a certain approach or does it have too many unknowns.\t\t\tWhat will the impact on the project be when taking a certain risk. \t\t\tEg. introducing new technology you saw at a conference.\t\t\t\tCoaching less experienced people.\tYou most likely will have mixed experience in your team. \tThis makes a lot of sense when it comes to cost of a project, mixing and matching skills and experience; thus educating less experienced people.\tBridging communication between stakeholders and the team.\tBusiness stakeholders are often less technical in nature then developers. \tThey will use a different language and the Tech Lead will need to mitigate that.Do we need a Technical Lead?Some people argue against the role; claiming a team of well-functioning developers can make decisions and prioritise what is important to work on. Even when these perfect conditions would exist, during which team members talk to each other openly, discussing pros and cons before arriving at an agreed solution, it doesn’t take much to upset this delicate balance.The Tech Lead role is just that – a role. Instead of focusing on whether the role should exist, it is better to focus on ensuring all Tech Lead responsibilities are met. As with every leadership position, a bad leader can make things worse. With these tips I would like to help you to make sure that doesn’t happen.Two sides to a story  As the job title implies, Technical  Lead  is a job with mixed responsibilities: there is a technical and a leadership side to the story.I will share tips for both sides, although the distinction is not always clear.It is very unlikely these sides will be equally divided. More on this in Tip 4.1. Advocate for Change  Advocate for change, means installing a mindset of positive evolution.When a proces is slow or cumbersome … try to turn that around and make it better.One way of doing this, is by using OODA loops: Observe, Orient, Decide, Act.More information on OODA, can be found in this earlier blogpost.In order to observe slow or cumbersome processes correctly, it is important to be part of the team and experience the same pain as everybody else on the team.You should adopt a state of mind that continuously wants to improve a certain situation. The Japanese call this “Kaizen”. In our case, the situation you want to improve is the efficiency and happiness of the team and the delivery of a software project.Seek out the issues that prevent good teamwork.2. Work through Failure and SuccessThings will failThings will fail. Don’t worry too much about failure.Builds will fail. Deploys will fail. Schedules will be missed. Crashes will happen. If you prepare for failure, it will be easier to cope with it.When things fail, don’t look for someone to blame. You are the Tech Lead. Take responsibility and use your energy to fix the problem at hand and learn from it. Of course, don’t fix the same bug twice. If you need to fix the same bug twice, then you made a wrong decision.Learning from failure, will shape your orientation and make for better decision-making in the future.Celebrate successWhen the team has a sense of achievement, they will be happy and motivated, to be the best they can. It’s important to celebrate smaller achievements, like a successful sprint or a completed feature. I did a project once, where we delivered a system and the customer was really happy with it … Unfortunately, the vision of the customer changed, and the project never made it to production. If that’s the moment you’ve been waiting for …When someone comes up with a new idea, maybe an approach or framework they saw at a conference, and if the idea delivers, it is important that whoever came with the new idea, should be credited. This is very rewarding and will lead to more cooperation, creativity and out-of-the-box thinking.A drink on Friday evening, a small lunch, maybe a team building are all good ideas to get a happy and motivated team. Oh, and it’s fun.3. Stay TechnicalA Tech lead has a lot of non-coding responsibilities, but it is very important not to neglect the hands-on technical activities:  Write code, do proof-of-concepts, define interfaces, … Depending on the maturity of the team your involvement will be different.  Do code reviews and have your code reviewed. When new people arrive at the project, I tend to do most of their code reviews and I will be pretty strict: I will write tests that cause NullPointerExceptions, I will ask them to adhere to conventions, to use the Single Responsibility Principle, to be careful about packaging and naming, etc etc.I will also elaborate on the reasoning for these remarks and for the choices that were made.This might challenge existing ways of working and increase the maturity of the codebase.The number of changes they have to do (after a review) will quickly become less.  Insure a technical vision exists and is shared by the team. This vision needs to be in line with the customers needs. Customer needs will lead to important constraints, eg. regarding reuse (a throwaway project for marketing vs. a multiyear enterprise endeavour … but be aware that this type of constraint might also change).Sharing how you got to this vision with your team, will have theyhuge impact its adoption. Try to involve the team to arrive at the technical vision. And make sure they know how they contribute in reaching that vision.  Keep an eye on the evolution of the code: after a while, the amount of actual coding you do might be lower, but you need to stay up to date on the evolution of the code. You need te maintain awareness of the system and its technical constraints.Most (if not all) developers will be happy to define frameworks, to advocate certain methodologies, etc. But some non-functional requirements (also called quality attributes) such as networking, security, deployment and consistency are often overlooked.4. Always Available  As a Tech Lead, you should always be available for your team; for questions, for support, for guidance or to make decisions. I started this blogpost by saying the technical leadership role has two important aspects and combining these is never easy. Something that makes a lot of sense (for me) is writing down the amount of effort you expect to put into certain tasks, eg.  Technical design: preparing work for the team (that includes you). Making sure it is clear what needs to be implemented and how. This will often take a lot of quality attributes like networking, security, … into consideration.  Business: talking to the customer, looking at their needs and goals and matching these with the technical vision of the project.  Project Management: defining user stories, estimating, follow-up.  Code: writing code, doing code reviews, etc.The assigned percentages will obviously vary for everyone and for every project. It’s also important to look at the actuals, because these will help you understand on what you are spending time.5. Be a mentor for your team    Mediator: A Tech Lead should be a mediator, that facilitates discussion.When people have different opinions, you should embrace this. Because it means they care enough about something to discuss about it. In the end we work towards the same goal. Everybody can learn from the opinion of others.Get input from the team and try to reach a consensus.If reaching a consensus is really impossible and a decision is necessary, decide. Not deciding will always lead to more discussion.  Mentor: A technical lead should be a mentor for developers. Be a teacher. When you review code or when you explain certain conventions, be sure to clearly explain the reasoning why you are doing something in a particular manner.  Effective Delegation: After a while, your team will adopt certain best practices and less (strict) reviews will be necessary or more people will do reviews. That’s the point where you can also give ownership of user stories to more developers. By transferring ownership to developers, they will be highly motivated to do a great job. A tech lead should not try to own all of the responsibilities. The tech leads needs to make sure responsibilities are taken by someone.  Match goals: match the individual goals of the developers with the larger goals of the project and the organisation. This is specifically targeted dynamic coaching. Dynamic, because goals can change. Communication is very important when it comes to matching goals: it will make people feel valued.  Optimise for the group: Individuals in a team are extremely important, but when it is difficult to find consensus, it’s the team you should focus on. Teams that collaborate well, will perform better and members of a well-performing team are happy members.A good Tech Lead  knows when to give input  knows when to make decisions  knows when to step back and allow the team to take more ownership.Share responsibility, give ownership … but stay accountable.6. Surround yourself with other Tech LeadsThere are many reasons to surround yourself with other Tech Leads. On a personal level, it presents an opportunity to learn from your peers: how do they provide input for their team and how do they divide their time between the different responsibilities of the role.On an organisational level, you should verify if there is a clearly understood overarching goal.If this is the case, you might want to investigate whether cross-organisational coordination is required to meet objectives.It is important to keep track of architectural guidelines to make sure your product will play along nicely with other components and to make sure the larger system is consistent.Chances are there will be dependencies on the product of other teams or on the members of other teams.Make sure these are taken into account when composing a sprint.This kind of coordination is a genuine problem at many (larger) organisations or customers.Investing time in networking, is necessary to avoid surprises beyond your control.7. Think Big, Bias for Action.Think Big and Bias for Action are two of the twelve leadership principles at Amazon.Thinking big, means creating and communicating a bold direction for the project or the product. This will inspire results, because people are working on something big. Something that makes a difference.Focus on the opportunities that might arrive in the future. Make decisions that are not limiting. An excellent book on this, is Liminal Thinking by Dave Gray.A Bias for Action means acknowledging that many actions and decisions are reversible and don’t need extensive study.Getting things done … matters.When you put a flywheel in motion, it will keep rotating. Focus on simple things to get the flywheel to move. It will encourage people to deliver as initial hurdles have been taken.8. Interviewing potential new team membersKnow what you are interviewing for.Are you looking for someone for the longer term or are you looking for someone for a short assignment?When you look at a resume, look for patterns: eg. duration of an assignment. Does this match with your needs? If it doesn’t, make sure you ask the candidate if he or she has certain preferences. Some people like long-term projects, others don’t. This does not have to be a blocking issue. But it is something to talk about. Also look at used languages, libraries and frameworks. Do these match with your current choices? When you are looking for a long-term team member, experience with certain tools is less important than the will, ability and eagerness to learn.I always try to focus on the mindset of a developer: thinking logically, identifying multiple approaches to tackle a certain problem.Personally, I strongly discourage using Stack Overflow to find questions.It is more important to ask questions that are relevant for your project.My personal pattern for conducting an interview is as follows:  Comfort  Offer options  Build on the responses  Show interest  Bonus questionOf course, always, stay polite. If the candidate doesn’t match with your specific goals, don’t send them home with a bad feeling.Beware: we still will need to get things done, even when we don’t have the time, resources or influence to fix the team composition.9. Embrace cultural differencesDiversity is invaluable.All people are different and live different lives.This is incredibly valuable, because your users will also be different.Surround yourself with passionate people.Nowadays most (if not all) teams use some kind of instant messaging.When working with teams in different time zones, this becomes even more valuable as it enables asynchronous communication and broadens the potential answers.I mentioned this before: everybody is part of the team and everybody’s opinion should be valued.10. Estimating is hard  Douglas Hofstadter  Hofstadter’s Law: It always takes longer than you expect, even when you take into account Hofstadter’s Law.Estimating is hard. When you do it more often, you will get better at it … but you still will get it wrong from time to time.In agile projects, the entire team can participate in a planning poker meeting. Planning poker can expose unknowns when estimating a user story.In general, there are two approaches to cope with these unknowns: doing a technical design before starting with the user story (eg. by defining a spike) or accepting the risks, together with your business stakeholders.As a technical lead, you will likely also need to do estimations before the team is actually building something or when responding to RFPs (request for proposals). This can be to give business stakeholders an idea of the potential cost, to decide on priorities or to evaluate staff.To achieve this, I suggest using three point estimates, where you do an optimistic, a best guess and a pessimistic estimate and use this formula: (O + 4BG + P) ÷ 6  to get the weighted mean.Depending on the nature of the estimation, the number of unknown unknowns might be large: the project can be very similar to other projects or completely different. Factor these in. Estimate for the team that will do the implementation: you are probably estimating a real project. This is not the fastest time you can possibly do something, in the best possible conditions. The estimations represent the ability to execute for a team; not your ability to do the implementation yourself. Also make sure, you know your deliverables. This can be more then code and deployment artefacts, eg. code quality assurance reports, manuals, …Document assumptions.Mastering estimation is a lifelong journey. It will set you apart. And your colleagues will associate you with professionalism, stability, and quality work.11. Interfacing with the outside world  The language used by non-technical stakeholders might be very different then that of the development team.A Tech Lead must find a way to communicate ideas in ways non-technical people can understand. Eg. by using analogies and using terms others can easily relate to.In a DDD world, this means establishing a ubiquitous language.Work closely with customers, try to detect requirements from them and continuously map their requirements with the on-going implementation.As a technical lead, I don’t think you should be the Single Point of Contact. Because then you introduce a potential liability in the project: a strong dependency on you. Include your team in certain discussions, but make sure you prevent continuous interruptions of your team members … So don’t be the Single Point of Contact, but try to be the First Point of Contact.12. Facilitate (agile) team workI would urge all Tech Leads to facilitate agile team working. Of course this works better, when the business is involved as well. But even when they are not involved, assign a proxy product owner. Chances are, this will be you.It doesn’t really matter if you use scrum, kanban or something else, but aim for short development cycles, feedback loops, etc.ConclusionYour team’s strength is not a function of the talent of individual members. It’s a function of their collaboration, tenacity, and mutual respect.If you’d like more information on Technical Leadership, you can check my slides on SlideDeck or this video on YouTube of my talk at Devoxx."
      },
    
      "iot-2017-12-20-virtual-reality-html": {
        "title": "An introduction to virtual and alternate reality",
        "url": "/iot/2017/12/20/Virtual-Reality.html",
        "image": "/img/virtualreality/banner.jpg",
        "date": "20 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  A look into the wonderful and exciting world of virtual, alternate and mixed reality.IntroThe concept of virtual reality is not a new one, neither is the one of augmented reality.It has been around for quite a long time already.Perhaps mixed reality will be the next big thing? I’ll leave that up to you to decide.Ever since we have been able to create visual representations of our own or other worlds, mankind has been fascinated by transferring the sensory perception into this world.It is in the recent years that technological advancements have made it possible to do this in ever increasing realistic and engaging ways.A trip down memory lane…In 1962 some crazy person built what is generally believed to be the first virtual reality experience.This machine pictured above is called a Sensorama. The user of the Sensorama sits on a tilted chair and watches a short film with stereoscopic 3D images in wide-angle view with stereo sound and even added effects such as wind and aroma. A predecessor of those 4D movies, available in cinemas and theme parks these days.This, of course, cannot be compared with the more advanced implementations we have today.In the 70’s and 80’s virtual reality really started blossoming in certain specialized areas.In 1968 Dr Ivan Sutherland created the Sword of Damocles at MIT.This head mounted display (or HMD) was so heavy it needed to be suspended from the ceiling.It was able to track head movements and show rudimentary 3D images.During the 90’s and 00’s: more sophisticated military, flight and combat simulators started showing up.In the privatized industry: simulators for airline pilots, doctors and surgeons.Different ways of interacting with this virtual world also started appearing:  Head tracking  Touch screens  Gloves  Entire flight simulators  …This is also when video game companies first tried to capitalize on this exciting new technology and bring it to a wider audience.But the technology was still too rough and did not catch on.Some of these early attempts include:  Nintendo Virtual Boy  Virtuality (arcade system)  iGlasses  VFX-1                                                                But then came 2012.The year in which the Oculus Rift was kickstarted. This sparked a renewed interest in virtual reality which, thanks to technological innovations, was looking much, much better it ever had!All kinds of realitiesAlright, so we have been talking about virtual reality for a while.But what exactly are virtual reality (VR), augmented reality (AR) and mixed reality (MR) and how do they relate.Above you see the reality - virtuality continuum.Let’s start on the left side.The left side depicts reality or the Real World. With the Real World we mean the world where you, as the reader of this article, are sitting in a chair looking at your computer screen or holding your smartphone.For the sake of this article we assume that the world is real and that there are no matrix-like shenanigans going on.Simply put, the world as you know it and experience it every day, the world you and I live in.On the other end of the spectrum we see the Virtual Environment.Everything in the virtual environment is virtual, so ‘not real’.It is the imaginary world that you see on a screen.The user is locked out of the real world.An extreme example we all know of would be The Matrix, in which the person lives their life inside The Matrix not knowing the entire thing is not real.Augmented Reality (AR) and Augmented Virtuality (AV) are situated in between these two ends of the spectrum.As their names suggest, AR is more closely aligned with the real worldwhile AV is closer to the virtual world.In other words AR consists of a mostly real world with a few virtual elements augmenting the experience.AV exists mostly in the virtual world with a few elements of our real world visible.Mixed or Merged reality (MR) is a term used to describe anything between reality and virtuality.This means it contains both elements of the real world and a virtual world.It can go all the way from a mostly real world with a bit of virtual world sprinkled on top to mostly virtual world with only trace elements of the real world remaining visible.With MR as opposed to AR and AV, the virtual world is aware of the real world and can interact accordingly.We see the term X-realities being used to describe any of the VR, AR, MR and any other realities.These terms are pretty vague and some have even changed meaning over the years.They outline the boundaries of the different areas in the spectrum, but they tend to bleed over into one another very easily, and get mixed up a lot!Virtual RealityWe call something virtual reality when the user is emerged in a virtual world and is completely blocked out from the real one.The simplest example of this is a 360° video experience.More advanced implementations have the user take part in a fully virtual world where their movements and actions are tracked and translated into actions in the virtual world.Some examples of virtual reality implementations are:  Games, both recreational and educational  Photo content: photo spheres  Video content: video spheres, 360 videos  Creative applications like 3D painting, sculpting, DJ applications and many others…Currently the most popular application for VR is video games. There are a lot of games with VR support on the market already, and more are getting released almost every day!Shooting games and simulators seem to be an exceptional match for the platform.Although mostly used for gaming purposes, a VR headset can also be used for viewing 360 scenes and videos.Thanks to head tracking, the viewers perspective is adjusted automatically when the head is moved.The more advanced headsets like the Oculus Rift and the HTC Vive even have external sensor arrays to track user movement, and have hand based controllers for interaction.Although you have to deal with additional setup compared to a phone where you can just drag the viewport or use your phone’s motion sensors.Augmented RealityLast year, the world exploded with people young and old crowding the streets in search for Pokémon.Pokémon Go is a fine example of an AR application.It is augmented reality because the application is still grounded mostly in the real world.The virtual elements are layered on top of the real world, but they do not interact with it.There are 4 types of augmented reality applications.  Marker  Inverse Marker  Markerless  ProjectionMarker basedMarker based AR apps use image recognition to recognize a specific pattern or marker.The marker is detected and replaced or covered with a virtual object.These types of applications are very simple and used for showcasing or displaying additional information.Inverse marker basedInverse marker based AR is very similar to regular marker based AR.These applications are used in conjunction with large screens with cameras where the user only has to control the marker.MarkerlessThese markerless applications use positional tracking and GPS to determine where to show things by mapping the environment and creating a spatial awareness to track objects when moving.The popular Pokémon Go and Ikea apps are perfect examples of this.Projection basedProjection based AR projects images rather than showing them on the screen.This requires hardware capable of projecting so is not as widespread.It is used more in manufacturing.The great thing about AR is that any recent smartphone is capable of running AR applications.Many AR apps only need a camera to work.While specialized depth sensing sensors exist, they are not as widespread and are used more in specialized industries.Mixed RealityMixed reality takes the best parts of both virtual and augmented reality.What makes mixed reality special is that it understands the environment, it can interact with and respond to changes or events happening in the real world.Hardware that supports MR takes the form of glasses and headsets with cameras and other sensors.These sensors are used to map the physical 3D space so the virtual objects know where they are situated and know what is in the real world.This is where you get the holographic experiences, even tough they aren’t real holograms, they look and feel exactly as you would expect them to.Hardware and current pain pointsCurrent devices are a big step up from what we had even a few years ago, but there are still many improvements to be made.The high-end VR devices require beefy computers equipped with high-end graphics cards.The headsets requiring a smartphone offer a more mobile experience but are still limited in the visual effects department.For a virtual reality experience to feel smooth and natural it needs to be rendered at a minimum of 90 frames per second.If the device can not keep up, the user might experience VR sickness, similar to motion sickness.Because our eyes are so close to the screen and the resolution our eyes can perceive is so high, pixels become visible, detracting from the experience.When looking closely, the screen looks like a raster.This can be solved with a higher resolution but that again requires more computational power.This much content at such a high pace also requires a lot of bandwidth and low latency.Another deterrent for VR headsets is the extensive setup required to make them work.High-end devices require base stations or tracking cameras to accurately measure movement.There are devices with sensors built in so that they are not dependent on external base stations, this technique is called inside-out tracking.Creating realitiesMaking software that is AR/VR/MR enabled can be quite different from traditional business application development.These type of applications are often heavily dependent on visual elements and can require extra artistic knowledge.For virtual and mixed reality game engines can be used to create “games” to create these experiences.These are some of the most popular game engines that are free for personal user:  Unity  Unreal Engine  CryEngineThink of these as a sort of photoshop for software.We can leverage the editors to see what we’re working on and make changes immediately.These tools are ideal for virtual reality.Both Unity and Unreal Engine have plugins available for augmented reality.The more business targeted applications are often augmented reality enabled applications.Both Apple and Google have made big efforts to support this on their respective OSes and basically every smartphone with a camera is now AR enabled.Apple’s ARKit and Google’s ARCore are the respective augmented reality APIs for their IOS and Android platforms.Since the inception of these two APIs the amount of AR apps has greatly increased, Google is even discontinuing its Tango project in favor of ARCore.There are many other SDKs available, many of which even have their own Unity plugin.Vuforia and ARToolkit seem to be the most popular ones.Microsoft created the Windows Mixed Reality framework for mixed, virtual or augmented reality apps to run on windows 10 based computers.Business cases &amp; ExamplesSo, as software developers, what can we do with all this fancy tech?Our Dutch colleagues already made an awesomely detailed VR application to help train personnel of the Royal Dutch Navy.Make sure to have a look since it is a truly impressive feat of software engineering.    Virtual reality is widely used for training purposes.Whether it is for a boat, an aircraft, a machine or a human body, a virtual space can prepare someone for situations that are otherwise very costly or difficult to simulate.Augmented and mixed reality can show information when and where you need it.On a mobile phone screen, through glasses or projected on a surface, having the correct information at hand is always useful.Visualizing a product before it is manufactured can enhance the design process by discovering points of improvement much earlier on.Visualization like Ikea’s app for trying furniture or a tour through historic Bruges or even business cards with an AR marker for increased memorability.For us at Ordina, we see most potential in AR and MR.Mobile based AR applications that can be used for showcases, in the IoT world, or even just augmented digital signage with inverse marker based AR.ConclusionThe years to come will bring even more advancements to the wonderful world of virtual, augmented and mixed realities.A lot of things are still shaping up and being developed, ever improving on earlier versions.A lot of solutions and standards are still being figured out.Which means that now is a great time for us to try stuff out, and get a feel for the technology, whilst preparing and working on proof of concept applications."
      },
    
      "conference-2017-12-19-xpdays-benelux-2017-html": {
        "title": "XP Days Benelux 2017",
        "url": "/conference/2017/12/19/XPDays-Benelux-2017.html",
        "image": "/img/xpdays-benelux-2017/XPDays-Benelux-2017.png",
        "date": "19 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  XP Days Benelux is a two day conference on agile software development for and by agile practitioners.In this post, I will take you along to the talks and sessions I attended and participated in.  Table Of Contents        The 8 Stances of a Scrum Master - Barry Overeem      Do Not Deal with Resistance! - Remi-Armand Collaris &amp; Linda Dorlandt      Motivate Your Team with Gamification - Jean-Jacques Courtens      An Integral View on Agile - Frederik Vannieuwenhuyse &amp; Johannes Schartau      The Art of Hosting Conversations that Matter - Johan Decoster &amp; Jef Cumps    The 8 Stances of a Scrum Master - Barry OvereemBarry Overeem is a freelance Scrum Master and Professional Scrum Trainer at Scrum.org. He’s an active member of the Scrum community and shares his insights and knowledge by speaking at conferences, facilitating workshops and writing blog posts.It is very common for a Scrum Master to get tasks assigned, that aren’t actually tasks a Scrum Master should be performing in the first place.The job description of a Scrum Master comes with a lot of other tasks than people, sometimes including the Scrum Master, assume.The session starts with an overview of the eight misunderstood stances of a Scrum Master:  Scrum Police: Scrum isn’t a hard set of rules to be followed. There’s absolutely nothing wrong with a bit of flexibility and empathy based on the team’s situation.Time boxing for example is very important but keep an open mind when the team is having a very valuable conversation after the time box has ended.Flexibility regarding time boxing is of course not meant to happen too often. So keep an eye on recurring discussions to see if they need to be addressed individually.  Hero: Managing and solving all problems and impediments like nobody’s business! While an important task for the Scrum Master, make sure not to get too focused on being the team’s impediment super hero.  Scribe: “Hey John, you’re taking notes again for this meeting, right? Thanks!”. Do you recognize this exchange? Congratulations, you’re the team’s personal scribe.This is especially to be avoided during retrospectives since this creates a false sense of ownership of the issues and actions towards the Scrum Master.  Admin: The Scrum Master is the workflow master. Need to add a board in Jira? Ask the Scrum Master. Need to start a sprint in Jira? Ask the Scrum Master. Now, hold up right there! The Scrum Master can of course perform these tasks but yup, so can the other team members.  Secretary: The Scrum Master plans all the work in the team members’ agendas. Keep up-to-date with everybody’s holidays, sick days and toilet breaks. (Can you sense the sarcasm?)  Chairman: While this is often the case, the Daily Scrum isn’t a meeting where the team members report back to the Scrum Master. It’s a meeting by and for the development team. In absence of the Scrum Master, the Daily Scrum must still happen.  Team Boss: The Scrum Master is the boss of the team. He/she decides who is in and who is out. A sick day? You’re fired! Buy the Scrum Master a chocolate cake? Here, have a raise! Oh, what a world it would be!  Coffee Clerk: Please do get your team members some coffee once in a while. But no, to everyone’s surprise, fetching coffee day-in day-out is not part of the Scrum Master’s job description.So, what are the eight preferred stances of a Scrum Master then?Glad you asked!The eight preferred stances are:  Teacher: There is much the Scrum Master can teach the Scrum Team. The Scrum Master must ensure that Scrum is understood and implemented properly by the entire team.He or she makes sure the team stays on track of the Agile practices and principles.  “The art of teaching is the art of assisting discovery.” - Mark Van Doren  Impediment Remover: An impediment is a problem that goes beyond the self-organization of the Development Team. Make sure the team understands and uses their own ability to solve problems and be self-organized. It might also create an opportunity for the team to come up with creative ideas to solve the impediments themselves.  Facilitator: Being a Scrum Master also means facilitating the team in transparency, inspection and adaptation. It is also through great facilitation that the Scrum Master succeeds in getting more value out of every event.  Coach: It’s important to stay away from the solution and ask questions in order to facilitate discovery of solutions. Of course, the Scrum Master may offer new perspectives to help the team reach a solution.  Servant Leader: Remember that the Scrum Master serves others. Being able to read the room, manage conflict and facilitate resolutions within the team is a very important responsibility of the Scrum Master. Make sure to lead by example and make others feel comfortable with failing.  Mentor: The difference between coaching and mentoring is that for mentoring, having in-depth knowledge is crucial. A mentor helps the team understand the practices and principles of Agile and transfers his or her knowledge of the subject.  Manager: The Scrum Master manages a whole bunch of things: the culture, the Scrum process, team health, Scrum values, impediments and boundaries of self-organization. These boundaries need managing because boundaries that work for one team might not work for another.  Change Agent: It’s essential to try to influence the company culture to open up to Scrum so the Scrum Team can flourish and thrive.More info on the 8 Stances of a Scrum Master can be found here.Do Not Deal with Resistance! - Remi-Armand Collaris &amp; Linda DorlandtRemi-Armand Collaris is a team and organisation coach who uses Agile, Scrum and LEAN ideas to find new ways to help people, both within teams as well as among teams, communicate and collaborate better.Linda Dorlandt is a mentor in change processes that help teams collaborate and reach a common goal. In order to do this, she uses and teaches methods for coaching and process management.The session by Remi-Armand and Linda started with two exercises. For the first exercise, we had to pair with one of our neighbours where one of us was the coach and the other the coachee.The coachee had to open up about an issue they’re facing that they cannot solve by themselves and the coach had to try to resolve it by suggesting solutions. After a few minutes, we had to switch roles. Both people then had to show to the group whether they felt better about the problem or not.The second exercise dealt with the same problem, but instead of immediately trying to suggest solutions, the coach asked questions to get to the bottom of the issue and figure out the goal of the coachee. Again, after this round, both people had to show to the rest of the group how they felt about their problem, if the feeling was better, worse, or stayed the same.The goal of these exercises was to show that trying to push someone towards a solution for their problem isn’t the best way to handle the situation since there will always be a ‘but’ coming from the person with the issue. Asking questions and trying to get the person to thoroughly think about their situation and problem will help them reach a solution themselves that they understand and accept.As an example of someone trying to actively deal with resistance, they showed the YouTube video called ‘It’s Not About the Nail’.The conclusion is easy and was mentioned in the paragraph above. Pushing someone towards a solution they don’t see fit will not give you the outcome you hope for.Remi-Armand and Linda discussed a series of steps that will help facilitate the conversation with a person that has a problem and is unable to come to a solution by themselves.  First step: Make a connection. Acknowledge the person’s issue and make them feel comfortable opening up to you.  Second step: “What is bothering you?” Try to figure out what the actual root cause is of their issue. Get to the bottom of it and focus the complaints.  Third step: “What do you think will help you?” Guide the person towards a change in language. It’s important they can see that a change in the situation is actually possible.  Fourth step: “When?” Create concrete plans for change. Don’t postpone taking action. Plan it and make it happen.More information can be found on the Dutch website ‘Praktisch op weg naar Teamresultaat’.Motivate Your Team with Gamification - Jean-Jacques CourtensJean-Jacques Courtens is the founder and managing partner at Adsdaq where he introduced gamification as a means of motivating the teams to deliver working products consistently.This was an Open Space session where Jean-Jacques discussed the way he implemented gamification in his company. At Adsdaq, they work with a reward system where points can be earned by completing certain tasks.They assign points to three sections: Timesheets, demos and sprint objectives.  Timesheets: Filling in the timesheet in time is worth 1 point  Demos: Doing a demo is worth 2 points  Sprint objectives: Each completed story point is worth 1 point. For sprint objectives, they also use a point multiplier. X0 when the sprint has failed, X1 when 90% of their main goal of the sprint was achieved, X2 when 100% of the main goal was achieved and X3 when the entire sprint was a complete success.For all three sections, they also assign badges. For example, the team can earn the golden badge when 15 sprints in a row are a complete success.The earned points are actually converted to Euros that are spent on team celebrations where 1 point is worth 1 Euro:  70% of the earned money goes towards team activities, such as team building, team lunch, etc.  30% is spent on personal rewards like cinema tickets, dinner for two, etc. This is only meant for team members that were part of the sprint.Jean-Jacques noted that the company probably spends around 3.500-4.000 Euros every year for this reward system, for a team of 8 people.It’s also important to note that the team is still being rewarded when the sprint fails and not being reprimanded for it. The reward is just a lot bigger when the sprints succeed.An Integral View on Agile - Frederik Vannieuwenhuyse &amp; Johannes SchartauFrederik Vannieuwenhuyse is a multidisciplinary generalising specialist and is continuously on a journey of discovery and learning how to grow effective, resilient and agile organisations.Johannes Schartau is an Agile Coach, consultant and professional Zombie Scrum fighter from Hamburg, Germany. He is passionate about creating environments for real collaboration and joyful creativity.The first part of this exercise consisted of reading six case studies and writing down on post-it notes how you would handle each situation, what questions you would ask.These post-it notes were used in a later exercise, after an explanation of the so-called Integral Theory:Integral Theory was created and shaped by Ken Wilber and consists of the idea that anything can be viewed as a thing by itself or as part of a larger group (Individual VS Collective).Anything can also be seen from within and from the outside (Interior VS Exterior). Based on this, Ken suggests that any kind of knowledge or experience can be assigned to any of the four quadrants shown above.  Leadership and Engagement: Interprets people’s interior experiences and focuses on “I”  Culture and Shared Vision: Interprets the collective consciousness of a group of people and focuses on “We”  Behavior and Metrics: Observation of the behavior of people and focuses on “It”  Organizational Architecture and Systems: Focuses on the behavior of a group of people as functional entities seen from outside: “They”The following image gives an overview of the questions that help organize knowledge into the quadrants:After explaining Integral Theory, we picked up our post-it notes and organized them into the quadrants that were put up across the room.When the post-it notes were assigned to their respective quadrant, Frederik and Johannes asked us to move towards the quadrant in the room that we felt we had the most experience in or we felt we were good at.Next, we had to discuss in our ‘quadrant groups’ whether the post-it notes that were put up in the quadrant were a good match for it or whether they belonged in another quadrant.The final exercise consisted of moving with your group to the next quadrants and listening to one person, that stayed behind from the initial group, explain the conclusions they reached during the initial discussion.The presentation of Frederik and Johannes can be found here.The Art of Hosting Conversations that Matter - Johan Decoster &amp; Jef CumpsJohan Decoster is an Agile coach and trainer trying to make a difference in the lives of the people he works with by uncovering everyone’s unique potential and looking deeper into the essence behind any theory or concept.Jef Cumps is an experienced coach and trainer supporting organisations in their transformation towards more agility and a more engaging, humane and effective way of looking at work.The session by Johan and Jef started with a small exercise. We paired up with our neighbour and asked each other the question, ‘Why are you really at this conference, and what has it meant to you so far?’The point of the interview was asking questions to guide the interviewee in telling their story. After these interviews, the person interviewing had to share what they had learned with the rest of the table and the others had to take notes of the summary.The steps for executing this exercise are covered in this picture:After this exercise, it was clear that asking appropriate and considerate questions is a key element of hosting a good and constructive conversation.This kind of conversation is built upon four key elements:Make sure the conversation stays polite and has a good flow of dialogue, but also take note of the two other sections.It’s very important to respect the differences in ideas between you and your conversation partner. It’s okay to voice your opinion, but be considerate and respectful and make sure to not overpower the other person.Listen often and do not judge too quickly. The other person might not be good in voicing their opinion clearly so suspending judgement is always a good idea. Things may become clearer later on in the conversation.Right before the break, people were invited to write down questions they had, where they could use the help of others to solve it.Eventually, the top 8 questions were picked and each question owner was going to be the conversation host for one of the tables.After the break, we joined a question owner at their table with a maximum of 4 persons per table. The other people helped the question owners answer their question by having three conversations based on the following questions:  What is the quest behind the question? Try to get into the question as deep as possible and figure out what the real reason is behind the question.  What is missing? Is anything from the previous conversation still missing? Are there any other additional reasons behind the question?  What actions can be taken? Decide on concrete actions that can be taken.Each question was assigned a time box of 15 minutes and you were only allowed to talk about that specific question during that time box.After each question, the other people at the table moved around to another table to help with someone else’s issue.So, after the exercise, every non-question owner was able to help three different persons by helping with a different question each time.This exercise is called a Pro Action Cafe, which is based on World Cafe and Open Space.The outcome of the exercise was a group of people, happy to have some concrete actions to start working with to try and solve their issues.More info on the Art of Hosting can be found in the following image and on the mentioned website."
      },
    
      "iot-2017-12-16-meeseeks-box-diy-guide-html": {
        "title": "Building a Meeseeks Box!",
        "url": "/iot/2017/12/16/Meeseeks-Box-DIY-Guide.html",
        "image": "/img/meeseeks/wallpaper.jpg",
        "date": "16 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  Ooh, I’m mister Meeseeks, look at me!A Meeseeks Box?For those of you who are already familiar with the show Rick and Morty, the Meeseeks Box should be a well-known object!For those who do not know Rick and Morty, go watch it now, I’ll wait!In short: The Meeseeks Box is a technological/magic box crafted by Rick. When the button on top is pressed, a Meeseeks is spawned.The Meeseeks can be given one assignment (like a wish) and he will try to fulfil said request.The Meeseeks will only disappear when the task has been completed.One caveat, existence is painful for the Meeseeks, the longer it lives, the more sanity it loses.Our colleague Dieter Hubau made a fully operational Rick and Morty themed example to demonstrate Spring Cloud Stream. You can read this excellent story about it on our tech blogBe sure to check it out, it’s a good read!This blog post will go into detail on building your own Meeseeks Box, which I integrated to work with the above Spring Cloud Stream demo.What should it do?The Meeseeks Box is intended to complement the Spring Cloud Stream demo mentioned above.If the button on top is pressed, like in the series, a Meeseeks is spawned in the demo application. (A new instance, see the Spring Cloud Stream blog post)The Meeseeks will then search for the Szechuan sauce until it is found.For the demo a maximum of three Meeseekses can be spawned, as to not overwhelm the people with Meeseekses, because they tend to get annoying if they live for too long.The hardware setupThe setup for the box is as follows:  A box (container)  Raspberry Pi 3 with GPIO connected to a button with an LED  Internal battery pack to power the boxSince the Raspberry Pi 3 has built in WiFi and Bluetooth it is possible to make the box fully wireless.The Pi has Node installed on it (the latest version) and is connected to the WiFi.The WiFi can be easily configured by placing the SD card in your computer and placing a file name wpa_supplicant.conf file in the root of the boot volume.This file contains the configuration for the WiFi network the Pi should connect to.wpa_supplicant.confcountry=BEctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1network={     ssid=\"SSID-goes-here\"     psk=\"key-goes-here\"     key_mgmt=WPA-PSK}Make sure you do not omit any of the first lines or your Pi’s WiFi will cease to function until a corrected version of the file is used!The build!The original idea was to make the box itself from wood or thick cardboard. But since I wanted to try something new that would entail less manual work with getting all the insets correct on the sides of the box, I decided to go for a 3D printed version.The box:You can download the file here.The lid:You can download the file here.These two 3D models were originally obtained from the Thingiverse but I’ve adapted and scaled them properly.I ordered the 3D prints via 3D Hubs and was surprised it was finished in one day.When I went to get the printed versions I was a bit concerned that they might not have turned out as I had hoped.And was I right:A rookie mistake, I didn’t check the model dimensions once I uploaded them into the online tool for processing. As that seemed to have converted up the measurements I used and set them to millimeters instead of centimeters.An easy fix and the second printed version was in the correct size, but printed by a colleague to keep costs down.Next came the task of painting the thing. As time was short and I only had cheap non-spray water based paint available, I decided to proceed anyway.I did apply a spray can based primer first, to make the box white again as the orange color was far from perfect to apply other colours.Many layers and hours later the box was painted.Nowhere near perfect but good enough for a first try at painting 3D printed models. The big issue with these paints and 3D printed models is that the paints tends to get in between the printed ‘lines’ and thus requiring a lot more paint without actually getting a nice result.                                The button on top was attached by very carefully drilling a hole in the top lid and pushing the base of the button through.The gap was tight enough for the button to stay firmly in place by friction alone, allowing it to be removed later on.The Raspberry Pi was attached to the underside of the lid with some standoffs and super glue.The lid fits on the box and is held in place by magnets.This prevents any moving parts that might fail due to material fatigue or attaching hinges, since attaching these to the box and lid would be cumbersome, as screws can’t easily take hold in the 3D printed material.A future, more elaborate version of the box could include cutouts for the lid in the box.The code behind itThe code behind the Meeseeks Box is a simple NodeJS application.As it is run on a Raspberry Pi we need to make use of raspi-io to make use of the GPIO on the board.I also use Johnny-Five as an abstraction layer. More information about Johnny-Five can be found on their extensive website.main.jsvar Raspi = require('raspi-io');var five = require('johnny-five');var http = require('https');var board = new five.Board({    io: new Raspi(),    repl: false});board.on('ready', function() {    var ctx = this;    var prevValue = 1;    this.pinMode(0, five.Pin.OUTPUT);    this.pinMode(7, five.Pin.INPUT);    this.digitalWrite(7, 1);    this.digitalRead(7, function(value) {        //console.log(value);        if(value == 1) {            //Enable this to disable the LED when the button is released!            //ctx.digitalWrite(0, 0);        } else if(value == 0 &amp;&amp; prevValue == 1) {            ctx.digitalWrite(0, 1);            doCall('POST');        }        prevValue = value;    });});The above code is very simple, it makes a new Board instance which we pass a new Raspi instance telling the Johnny-Five library that we are actually running on a Raspberry Pi and that it does not need to search for any other connected boards (like Arduinos).What you also might notice, for those who have used Johnny-Five in the past, is that we do not make use of the full power of Johnny-Five. We are not using the LED or Button classes and instead are taking a more lower level approach by controlling the IO pins directly.This has a very good reason.The Node application is run at startup, when the Raspberry Pi boots, as a Linux service.Starting it automatically breaks the REPL functionality of Johnny-Five which results in the application exiting after a good second, making it unusable.This is why the Board config has the repl parameter set to false, this prevents the REPL from starting and makes it so the application does not exit unexpectedly.This unfortunately also prevents us from using the full abstraction power of the Johnny-Five framework.The actual code is very simple. We wire up a pin as input for the button and another pin as output for the LED.We put the input pin to high, this prevent the input from flickering between high and low (essentially a pull-up to vcc).We then bind a function to the digitalRead which gets executed every time the state of the input pin changes (high to low -or- low to high).Since we do a pull-up to vcc our button will actually be connected to the GND which will result in the signal of the input pin going to low when the button is pressed and back to high when it is released.Please also be sure to wire up the LED with a correct resistor to prevent it from drawing too much current, as that might damage the IO pin it is connected to!Calculating such a resistor is an easy feat. If the LED needs 3 volts to function and uses 20 milliamps doing so: R = U / I = 5V (pin out) - 3V (LED) /  0,02A = 2V / 0,02A = 100ΩThis means that a 100Ω resistor needs to be put in series with the LED to prevent it from causing any damage to the IO pin/circuitry.main.jsfunction doCall(method) {    var request = http.request({        host: 'rnm-meeseeks-box.cfapps.io',        port: 443,        path: '/',        method: method,        headers: {            'Content-Type': 'application/json',            'Content-Length': 0        }    });    request.write('');    request.end();}The code above is a simple snippet used to make a call with no contents to the remote server.When the URL is called via the POST method, a Meeseeks is created.When the URL is called via the DELETE method the currently active Meeseekses are destroyed (for testing).You can edit this to perform any action you like.A video showing a fully operational Meeseeks Box:  Running Node as a systemd service on LinuxAs the Meeseeks Box needs to be simple to use, the application should automatically start when the Pi does.The best option was to make a systemd service and run it on system startup.First we need to create the systemd service file:sudo nano /lib/systemd/system/meeseeks.serviceThis will create a new file (if one does not exist yet).Place the contents below in this file and save it.[Unit]Description=Meeseeks Box serviceAfter=network.target[Service]Type=simpleUser=your-user-hereExecStart=/usr/bin/node /home/meeseeks/main.jsRestart=on-failure[Install]WantedBy=multi-user.targetThis file tells systemd what the service is and does, with what executable and which user.The After=network.target tells the service daemon that this service should only start if the network stack has already loaded!To test the service, first execute: sudo systemctl daemon-reloadThis reloads the daemon so it knows of the newly created service.Now we can manually start/stop/reload the service by using: sudo systemctl start meeseeks where you swap out start with the action you want to perform.To make the service run at startup use: sudo systemtl enable meeseeks and to disable it again, use the same command but swap out enable for disable.A far more detailed explanation about this matter can be found here.Meeseeks at DevoxxThe entire purpose of the Meeseeks Box was to be part of our booth at the well-known Devoxx conference in Belgium.Our booth drew quite the crowd this year, mostly because of the nachos and the totally real Szechaun sauce to go with them. Have a look at a couple pictures below:                                ConclusionThis was a fun side project to work on, even though the ‘deadline’ was a bit tight and I would have liked to have done some things differently, all in all everything turned out really well.A few lessons learned though:  Check measurements before ordering a 3D print  Non-spray water based paints are not the best match for painted 3D printed models  If you mess up the WiFi on the Pi it can be a real pain to debug it!  When starting Node as a service on Linux the Johnny-Five REPL does not work  Super glue is not always so super ;)"
      },
    
      "docker-2017-12-15-docker-basic-networking-html": {
        "title": "Docker basic networking",
        "url": "/docker/2017/12/15/Docker-basic-networking.html",
        "image": "/img/docker-basic-networking/docker-basic-networking.png",
        "date": "15 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  Containers are all the rage at the moment so I guess everybody knows how to build and run a container by now.But what use is one container by itself?In this post, I will show how you can create networks within Docker and what they are used for.  Afterwards, I will guide you through a step-by-step example on how to create a Docker network and add containers to it.This way, we will end up with a multi-tier application that is running on Docker containers in a basic network.Table of contents  Preface  Network setup  Database container  REST Backend container  Frontend container  ConclusionPrefaceIn the current application landscape, we see a strong rise of distributed applications.This is done by implementing a microservice architecture and deploying these applications in Docker containers.It’s important that these containers are able to communicate with each other, after all, what good is a microservice that is isolated?In order to achieve this, a couple of patterns are used.In this post I will demonstrate two of these patterns to you.  Communication within a Docker network between containers  Communication outside the Docker network by exposing internal portsOur final application setup looks like this:    All of our Docker applications will be deployed on one host machine.We will have a custom Docker network running with three containers attached to that network:  The database container is just a MySQL database running within Docker.  The backend container is a Spring Boot application that connects to the MySQL DB container and provides a REST service to the outside world.  The frontend container is an AngularJS application that consumes the REST service from the backend container.You can find all the code examples on Github.The only thing needed to complete this guide is a working Docker installation.Ready? Set. Go!Network SetupTo start this guide, let’s have a look at the Docker networks that are available on our machine:docker network ls  When you run a Docker container and you do not provide any network settings, it is by default attached to the bridge network.Containers that are connected to this default bridge network can communicate with each other by using their internal Docker IP address.Docker does not support automatic service discovery on this network.We want to be able to access our containers by using their container name instead of the internal IP address so we are going to create our own network:docker network create --driver bridge basic-bridgeThis creates a user defined network with the bridge driver that is called basic-bridge.If we look at the Docker network stack, we see that our user defined bridge network is added:    You can look at more details of the network by using following commanddocker network inspect basic-bridge    The basic-bridge network can use IP addresses from the 172.18.0.0/16 range and will use 172.18.0.1 as its default gateway.Database containerNow that we have created our own network, we can start attaching our containers to that network.Let’s start off by creating the MySQL database container.The following command pulls the MySQL image from the Docker repository and starts it as a container that is attached to our network:docker run --name=mydb --network=basic-bridge -p 3306:3306 -e MySQL_ROOT_PASSWORD=test -d MySQL:8.0.3That’s easy right?This container is attached to the basic-bridge network that we created in the previous step.Run the following command to look at the container in detail:docker inspect mydb    We can see in the output that it has gotten the 172.18.0.2 IP address and that it’s using the default gateway of the network that we created.Now we should set up the database in our container with the schema for our REST application.mysql -h 127.0.0.1 -P 3306  --user=root --password=testThis will connect a MySQL shell onto our localhost:3306.We can access this port because we exposed it when we started the container by using the -p flag.Note that this is done for convenience only, so we can access the container from our host and set up a schema.Now run following SQL commands in the MySQL shell.create database greeting;use greeting;create table greeting (    id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,  greetername varchar (50),  greeting varchar (255));  insert into greeting (greetername, greeting) values ('bas', 'Hello master');  insert into greeting  (greetername, greeting) values ('Jack', 'Hello slave');  select * from greeting;Now that we have setup our MySQL container and initialized the schema we are ready to create our REST backend service.Backend containerYou can find the code and Dockerfile for the backend container on the Github link earlier in the post under the /backend folder.The backend application itself is pretty simple.It just listens on port 8080 for requests on the path /greeting.You can pass the name request parameter to this path to get a customized response.The application fetches its greetings from the database container we set up earlier.I added following properties under src/main/resources to connect it to our database:    The important part here is that I referred to our database by using mydb:3306.We are able to do this because we will launch this backend service on the same docker network as the mydb container.This way, they will be able to resolve each others name by using the basic-bridge network we created.Next up, we will build the container from the Dockerfile I created.To do this run the following command in the /backend folder:docker build . -t rest-backendOnce the Docker image is built, let’s run it in a container:docker run --name=rest-backend --network=basic-bridge -p 8090:8080 -d rest-backendBy using the --network=basic-bridge, we attach the container to the basic-bridge network that we created earlier.You can look at the network details of this container by using:docker inspect rest-backend    We can see in the output that it is attached to the basic-bridge network just like our mydb container.We also used the -p 8090:8080 flag to expose the inner 8080 port (the port our Spring Boot application uses) to the outside world via the 8090 port.We can now curl or browse to localhost:8090/greeting to verify that everything is working:    We can see that our service successfully returned a response.Frontend containerWe can now create our frontend AngularJS application that consumes the REST service we just created.You can find the code for this application under the /frontend folder in the Github repository.Create the Docker image for the frontend container by running:docker build . -t angular-frontendNow let’s run the container by using:docker run --name=angular-frontend --network=basic-bridge -p 8080:80 -d angular-frontendAngularJS renders in the browser so it’s not rendered inside a Docker container.This means that our angular application would not be able to contact the REST backend container as that container is only known within the docker network.To work around this inconvenience I will explain 2 alternatives:  connect it to the internal Docker IP address of the rest-backend containerIn this case we connect the angular application to the ip address of our backend container. This only works because the ip address is known to our browser.The ip address is known because the machine on which our browser runs is also running the Docker network.Note that this is not very portable.If you would deploy the container somewhere else or you would browse to the application from another host this would break.      connect it to the public exposed port of the REST backend container.When we set the application up like this the angular application can access the backend container through the publicly exposed port 8090.Note that in this case the application would break as well if you deploy the frontend container somewhere else or if you would access it from another host.    The result in both cases is that our angular application can contact the REST backend and serves up a good response.    As I mentioned before both these alternatives have pretty obvious flaws in them.In a real world setup you would like to make your REST service publicly available on a webserver so that consuming applications would be able to connect to it by using that public URL.If there is interest in this kind of setup I can cover it in a later blogpost.Our application setup is now complete and our full setup looks like this:    ConclusionAs we saw in this guide it is actually pretty simple to create a single host Docker network and enable containers to communicate with each other over this network.When we created our Angular application, we saw that this approach has its limitations.Another limitation of this setup is that this kind of network is limited to a single host as it will not work over multiple hosts.Of course nobody wants to run a distributed application in containers on one host.I will make a follow-up blogpost where we look into Docker multi-host networks by using weave.NET.docker stop bas"
      },
    
      "conference-2017-11-15-javaday-ukraine-2017-html": {
        "title": "JavaDay Ukraine 2017",
        "url": "/conference/2017/11/15/JavaDay-Ukraine-2017.html",
        "image": "/img/javaday-ukraine-2017/javaday-ukraine-2017.png",
        "date": "15 Nov 2017",
        "category": "post, blog post, blog",
        "content": "  JavaDay Ukraine is an annual international two-day conference in Kyiv with more than 60 global speakers with various topics on Java, software architecture, machine learning, data science, and more.In this blog post we will go over some of the talks that we have attended.  Table Of Contents      Developing Microservices with Kotlin    Going Reactive with Spring Data - Christoph Strobl    Spring Boot 2.0 Web - Stéphane Nicoll    The API Gateway is dead! Long Live the API Gateway! - Spencer Gibb    Continuous Deployment to the Cloud using Spinnaker - Andreas Evers    10 tips to become an awesome Technical Lead - Bart Blommaerts    Hands-on introduction to CQRS and Event Sourcing with Axon Framework - Steven Van Beelen    Spring Cloud Stream — a new Rick and Morty adventure - Dieter Hubau    8 Steps To Becoming Awesome With Kubernetes - Burr Sutter  Developing Microservices with Kotlin - Haim YadidHaim Yadid is a developer, architect and group manager currently working as head of backend engineering in Next Insurance.In his search for a better programming language, he compared different strongly and loosely typed JVM languages such as Scala, Ceylon, Groovy, JRuby, Clojure and Kotlin.The chosen language would have to be concise, safe, versatile, practical and interoperable.Being a fan of strongly typed languages, Groovy and JRuby were no option.Scala was a good option but due to the complexity of the language, the long compilation times and lack of backwards compatibility assurance, it was also dropped.Kotlin proved to be the winner as it contained all of the above listed characteristics.It is also able to make use of the huge Java ecosystem and, being backed by Jetbrains, was very assuring.It also helped that Google made Kotlin the official language for Android Development. Not to mention it was the subject of 9 different talks at JavaOne 2017.In his talk, he wanted to share his findings and experiences when developing in Kotlin which he labeled as a huge success.The project he worked on contains a microservices backend over DropWizard deployed to AWS together with serverless endpoints in AWS Lambda.Used technologies, frameworks and libraries are amongst others Maven, DropWizard, AWS Lambda, PDFBox, XMPBox, Flyway, Stripe and Mockito Kotlin.Building the project was done via the Kotlin Maven plugin.He started with version 1.0.2 and immediately upgraded to every release which always went very smooth; even the migration to 1.1.0, which included Java 8 support, went without any issues.Onboarding new Java developers is never a hassle as they are capable of developing in Kotlin by the time they get to know the architecture.Haim really liked extension methods, which allow you to add functionality to an existing class or interface.The null safety, which is very similar to the null safety of Apple’s Swift - where nullability is part of the type of an defined object - was also well-appreciated.He also pointed out to us that Java open source libraries work extremely well with Kotlin. All you need to do is add the dependency to your build file and you are good to go.Data classes, similar to case classes in Scala, offering a concise way to define simple classes for holding data, were used for all their DTOs.Also worth mentioning is that IntelliJ has a converter functionality for converting a Java class to Kotlin.Obviously it’s mostly used as a starting point when migrating existing Java classes.We really liked Haim’s talk as we are very eager to try out Kotlin in a project.Haim’s presentation is available on SlideShare:   Building microservices with Kotlin  from Haim Yadid Going Reactive with Spring Data - Christoph StroblChristoph Strobl is a developer at Pivotal and is part of the Spring Data team.Starting from Spring Framework 5, reactive support was added to all the core Spring Framework projects.In a reactive architecture, it is important that your system is reactive from top to bottom in order to take advantage of the full performance gain, the persistence layer is no exception to this.During the talk, Christoph went over the classic imperative approach of a Spring application where Spring MVC is used and the performance problems that can arise when all threads are in use.A reactive architecture makes better use of server resources but in turn adds more complexity to your architecture.The publish-subscribe mechanism is heavily used in this architecture where, how can you guess it, publishers publish messages to which subscribers can subscribe.The mechanism also comes with back pressure for the subscribers, allowing them to define how many messages they want to handle next in order to avoid being overrun.It is important to note here that the reactive publish-subscribe mechanism is based on the push model. The subscriber will not actively fetch the data but will instead receive the data from the publisher who pushes the new messages to the subscriber when they’re available.In the other part of the session, Christoph went over several features of Spring’s Project Reactor, Spring Data Kay and Spring WebFlux.The publish-subscribe mechanism in Reactor is based on the Reactive Streams specification and there are two reactive types: Flux, an Asynchronous Sequence of 0-N items, and Mono, an Asynchronous 0-1 result.Spring WebFlux is the reactive brother of Spring MVC and uses Project Reactor under the hood for building reactive endpoints.Spring Data Kay is the newest version of Spring Data which now contains reactive repositories and reactive templates.At the time of writing this is only usable for MongoDB, Redis, Couchbase and Cassandra as the other databases lack a reactive JDBC driver.In the final part of the talk, Christoph held a demo of a Spring Boot 2 reactive application showcasing all the reactive features.All in all, a very interesting talk about building a reactive application using Spring.The demo code is available on GitHub.The presentation is available on Speaker Deck:Spring Boot 2.0 Web - Stéphane NicollStéphane Nicoll joined the core Spring Framework development team early 2014, being one of the main contributors to both Spring Framework and Spring Boot since.Stéphane’s session was all about Spring Framework 5 and Spring Boot 2.0.Spring 5 comes with Spring WebFlux which is the reactive brother of Spring MVC allowing you to build non-blocking APIs.He explained that there is always the issue of supporting all the different clients like desktops, laptops, smartphones and tablets, and their different internet speeds.Smartphones often have access to the slowest internet speeds and thus require the most optimal solution regarding bandwidth and performance.All the different concepts of building a reactive application with Spring Framework 5 and Spring Boot 2.0 were explained with a demo application called Smart Meter.Basically, you have all these different data inputs via sensors being gathered by an aggregator and then streamed to a dashboard.The frontend is written in Thymeleaf 3.0 which is the version in which reactive support was added.Besides the frontend needing reactive support, the persistence layer of the backend also needs it.In Spring Data Kay, reactive support exists for Redis, MongoDB, Couchbase and Cassandra.The other main databases such as Oracle, PostgreSQL and MySQL aren’t there just yet as they lack a reactive JDBC driver.In the demo, MongoDB is used.Stéphane also demonstrated some new additions to Spring Boot Actuator such as a unified way to implement custom endpoints, better output format,separate status endpoints (you now have /status and /health) and a simplified security model to specify who has access to (for example) status and info as users with a certain role may be allowed to see more.Properties in Actuator now also display the properties file in which they have been declared and the exact position.Stéphane concluded the talk by announcing that the release candidate was foreseen somewhere at the end of November.However, a recent tweet of his announced a small change to the release schedule:Spring Boot is having an extra milestone and RC1 is scheduled early December now.See https://t.co/6kOGdPMtfp&mdash; Stéphane Nicoll (@snicoll) November 9, 2017The demo code is available on GitHub.The presentation is available on Speaker Deck:The API Gateway is dead! Long Live the API Gateway! - Spencer GibbSpencer Gibb, Spring Cloud co-founder and co-lead, started by talking about the responsibilities of an API gateway.He started by revisiting Netflix’s Zuul which is servlet based and thus has blocking APIs, and referred to Mikey Cohen’s presentation on Netflix’s Edge Gateway Using Zuul in which Zuul 2 is also mentioned.Zuul 2 was supposed to be integrated in Spring Cloud but as it still hadn’t been released, Pivotal went with their own solution: Spring Cloud Gateway.It is built upon Spring 5, Reactor and Spring Boot 2.In order to have the gateway be non-blocking, there is a single event loop similar to how it is in Node.js.In another section, Spencer talked about the internals of Spring Cloud Gateway and the Spring Reactor features it uses.This involves the usage of classes such as HandlerMapping, WebFilter, Predicate, ServerWebExchange, PathPatternMatcher, RoutePredicateHandlerMapping and many more.As a filter to rewrite paths was commonly requested before, this was the first filter that they have written when implementing the Spring Cloud Gateway.Spencer also mentioned that they were also focusing on providing a simple API to write filters.Also neat to mention is that route configuration is now possible in YAML.In the final part, Spencer demonstrated an implementation of a Gateway showcasing the different ways of how to use the API to define different byhost, rewrite, hystrix and limited routes.He started off by visiting the legendary Spring Initializr webpage and created a Spring Boot 2 application with the Gateway dependency.httpbin is something that he is a big fan of, as it is really useful for testing whether for example the correct rerouting is happening and the right headers are being added to the requests.The presentation is available right here.Continuous Deployment to the Cloud using Spinnaker - Andreas EversAndreas Evers, principal Java consultant and Solution Expert at Ordina Belgium, held a session on Spinnaker for doing Continuous Deployment to the Cloud.Digital transformations usually require embracing a devops culture and adopting microservice architectures since without microservices, it is harder to go faster to the market.Moving your infrastructure to the cloud is possible via either IaaS or PaaS.With microservices, your deployment frequency explodes as it is way more flexible.Netflix for example deploys over 4.000 times per day and that number is still increasing.Andreas explained that cloud deployments are complex and that it is important to be able to do easy rollbacks.There is also the fact that we want to plan our deploy at the right time frame, preferably when traffic is lowest to have the least amount of users impacted.Andreas talked about a couple of other principles such as making sure that infrastructure is immutable, repeatable and predictable across the different environments through baking images or building containers by using for example Docker.Equally important are the deployment strategies like (rolling) blue/green (or red/black if that’s how you roll (pun intented). Looking at you, Netflix!).Using the blue/green deployment strategy you can deploy the new version right next to the old version. What happens next depends on how the strategy has been configured.Either the load balancer will reroute all traffic from the old version to the new version, or (if the rolling strategy has been configured) the traffic will gradually get rerouted to the new version. The last option is great for canary testing or smoke tests.A third principle is doing automatic deployments by defining a pipeline which is always possible by just scripting all of this yourself but this is usually rather brittle.This is where Spinnaker comes in to help you out with all of that.The internal structure of Spinnaker consists of a couple of microservices written in Spring Boot.Spinnaker fulfills all the principles we have just summed up and more:  It allows you to specify the pipelines together with all the different environments  It allows you to plan your deployments  You can configure and tune your deployment strategies  It has support for Chaos Monkey which allows you to test your system on how resilient and recoverable it is as VMs get taken out  It has canary analysis  Configuring, installing, and updating Spinnaker is done via HalyardSpinnaker is still heavily being worked on and there are a couple of nice features coming up:  Canary strategies  Rolling blue/green strategies  Declarative Continuous Deployment (config as code)  Operation monitoringFinally Andreas did a demo of Spinnaker using a simple application based on Rick &amp; Morty which is also available on GitHub.During the demo he demonstrated how you can set up pipelines, the deployment strategy, the traffic guard and canary testing in Spinnaker.People attending the talk were able to participate by going to the url to which the application was deployed in order to show how only a part of the traffic was routed to the new version.The slides of Andreas’ talk are available on Speaker Deck:10 tips to become an awesome Technical Lead - Bart BlommaertsBart Blommaerts, Application Architect at Ordina Belgium, presented a talk with tips on how to become a better and more awesome technical lead.Spoiler alert, there were actually 12 tips instead of 10!  Advocate for change:You should experience the same pain as your team.Try to work together more closely with the people in your team to see if there are any pain points or issues that your team members are experiencing.  Work through failure and success:It is inevitable that some things will fail now and then.It is important to prepare for failure and to take responsibility.Don’t finger point!Failing is an opportunity to learn and should be embraced.Success on the other hand, should be celebrated as early and as often as possible, and not only after the end of a project.You should celebrate for example the successful delivery of a sprint or when a feature has been completed.Congratulate your team and individuals often as it is good for the moral.  Stay technical:Reserve the right amount of time to code and review code.It is important to hold on to that technical vision and to see how your project’s code base evolves.At the same time it is important that you still grasp the technical aspect of your project as it will help you to make the right decisions for your project and your team.  Always available:You should always be available and easily approachable for your team members.According to Bart, your time should be spent about 35% on technical design, 25% with the business, 15% on project management and 25% on code.  Be a mentor of your team:As you have a key position in your team you should avoid being a strict ruler and decision maker, and try to make the best decisions for your team.Instead, try to be a mediator and a mentor for your team members.Effective delegation is important and try to hand out responsibilities to your team.Know when to give input, when to make decisions and when to step back.  Surround yourself with other technical leads:Each person is different and everybody has a different way to approach things.There is a lot to be gained by making use of cross-pollination and learning from other technical leads about how they approach and deal with things.It is important to be open for other approaches and to widen your vision.  Think big, bias for action:You should think big and differently.Try to focus on opportunities and to create a bold direction.Don’t be afraid to undertake action as actions are reversible.You don’t always need to do that time-consuming, extensive study before undertaking action as speed matters.  Interviewing potential new team members:Be prepared for interviewing potential new team members and be sure to go through the resumes.The mindset of a potential team member is more important than their knowledge of the tooling.You want the person to be eager to learn and to fit in your team.As for actually taking the interview, don’t blatantly copy questions from StackOverflow and expect the interviewee to come up with the exact same solution.Instead, first comfort the interviewee, offer them different options during the interview and try to build upon the responses they are giving.Show interest in the person and be sure to offer them a bonus question.  Embrace cultural differences:Everybody is different and diversity is invaluable.Have respect for everybody’s opinion and try to surround yourself with them as they offer you different points of view.Don’t forget that everybody in your team has the same shared end goal.If you are working with an offshore team, take the time difference into account.You can try to change your work hours to be more available to them.Focus on good communication and be sure to document the work and tasks well.  Estimating is hard:Bart quoted Hofstadter’s Law: “It always takes longer than you expect, even when you take into account Hofstadter’s Law”.In order to make more educated guesses, doing a planning poker can be useful.Define a sequence, set a baseline and allow reasoning.Don’t be afraid of uncertainty as it is inevitable.Bart suggests using the following formula: (O + 4*BG + P) / 6 where O is the optimistic estimate, BG the best guess estimate and P the pessimistic estimate.You should add 20% to the guess for properly testing, debugging, polishing, documenting and random wtf moments.Don’t forget, any estimate is better than no estimate, and make sure to share and review estimates.  Interfacing with the outside world:Adapt the way and the language when you communicate with non-technical people.Try to be the go-to-guy/girl for the management, the customer and other stakeholders.And very important: don’t be afraid to say “no”!  Facilitate (agile) team work:Be agile, use a prioritised backlog.Plan your sprints, use burn down charts and do sprint retrospectives.Your team’s strength is not a function of the talent of individual members but rather of their collaboration, tenacity and mutual respect.In short, facilitate an awesome team.The slides of Bart’s talk are available on SlideShare:   JavaDay 2017: 10 tips to become an awesome technical lead (v4)  from Bart Blommaerts Hands-on introduction to CQRS and Event Sourcing with Axon Framework - Steven Van BeelenSteven Van Beelen, Software Engineer at AxonIQ, held a hands-on session on CQRS and Event Sourcing using the Axon Framework which helps developers to focus on application functionality rather than the non-functional requirements of an application.The main advantage of event sourcing is that there is less info loss as you are storing all the different events, leading to the final state of records whereas in a classical example you only hold on to the final state of a record.By using event sourcing you also get a reliable audit log right out of the box.At the same time there is also a performance increase as events are processed in the background asynchronously, leading to faster response times.With event sourcing you mostly make use of a cache, as replaying events when looking up records can be time consuming.This is further compensated by making use of snapshots every 100 events for example.Some of the cons are that events are readable forever and that it is a lot of work if you decide to rewrite the event model and that you also have to think of versioning your model.Sourcing the model from lots and lots of events takes time but this is also resolved by making use of snapshots.The Axon Framework is open source (Apache 2 license) and supports concepts like DDD (Domain-Driven Design), CQRS (Command and Query Responsibility Segregation) and EDA (Event Driven Architecture).The framework helps you to focus on the business functionality as it takes care of the plumbing for you.The majority of the time was spent with live coding.Steven created a Spring Boot app with Kotlin.Useful to mention is that Axon has support for Spring Boot AutoConfiguration by adding the axon-spring-boot-starter dependency to your project.Axon will automatically configure the basic infrastructure components (Command and Event Bus) as well as any component required to run and store Aggregates and Sagas.Kotlin was chosen as it provides a concise way to write code, the data classes especially are very useful for writing commands and events as these are immutable POJOs.The application was about creating conferences and talks in order to demonstrate the framework.For the command model, it came down to marking aggregate classes using the @Aggregate annotation.In the aggregate classes, the identifier gets annotated with AggregateIdentifier.Your command handler gets annotated with @CommandHandler.This is the class where all the logic resides on how to handle all the different commands for the specified aggregate, usually resolving into events.Similar to the command handler, there is also an event sourcing handler annotated with @EventSourcingHandler containing the logic for processing the created events of the specified aggregate.Furthermore you have a controller and a command gateway.As for the query model, there is an @EventHandler that processes any events, updating your query models.The demo application is available on GitHub.For more information on the framework, be sure to consult the well-written reference guide.Spring Cloud Stream — a new Rick and Morty adventure - Dieter HubauDieter Hubau, principal Java consultant and competence lead Cloud &amp; PaaS at Ordina Belgium, presented his cool Spring Cloud Stream application featuring Rick and Morty.Spring Cloud Stream allows you to create message driven microservices and is based upon Spring Integration and builds upon Spring Boot.Briefly summarising the talk wouldn’t do it justice so instead we will link you to the blog post he has written on the topic available right here.The presentation is available right here:There is also a recorded video available on our JWorks YouTube channel, be sure to check it out!\t8 Steps To Becoming Awesome With Kubernetes - Burr SutterButter Sutter, Director for Developer Experience at Red Hat, gave a cool presentation about Kubernetes.After a lengthy introduction to DevOps, the challenges of creating and running microservice architectures and Kubernetes, we could dive into some of the more technical features Kubernetes has to offer.Burr introduced us to his eight step program to become awesome at Kubernetes:Step 1: InstallationBurr showed us the many ways you could setup a Kubernetes cluster, including Minikube or Minishift.There are plenty of guides on the web for deploying Kubernetes on any of the major infrastructure providers (AWS, Azure or Google Cloud), but there’s also the Kubernetes-as-a-service offerings from Google and Microsoft which can get you going very quickly.Running Openshift can be as easy as running oc cluster up on your local workstation, which sets up a local Openshift cluster for you using Docker.Step 2: Building imagesThere are several ways to build Docker images for Kubernetes or Openshift. We like the following ones:  The classical way would be to docker build a Docker image, push it to your local Docker registry and run it on a Kubernetes cluster using kubectl run ... or by creating a Kubernetes deployment using kubectl create -f deployment.yml  For Java applications, the Fabric8 Maven plugin can be used to build, run and debug Docker images on a Kubernetes cluster  You can use Helm Charts - think of Helm as yum/apt/brew for Kubernetes  If you’re used to Docker Compose and you have a lot of those config files lying around, you can use Kompose to convert them to Kubernetes config files  Openshift provides a way to create an image straight from your source code called Source to ImageStep 3: kubectl exec or oc execIf you want to find out what is going on inside of these black-box containers, you really should use the exec command which allows you to SSH into them so you can snoop around and learn about the internals of your applications.It’s very handy to debug applications, to figure out issues you might be having and to identify bugs or problems in advance.Step 4: LogsLooking at logs can make troubleshooting so much easier.Kubernetes allows the user to look at console logs for any pod in the cluster using kubectl log &lt;name-of-the-pod&gt; but it gets quite tiresome rather quickly.Luckily for us, the community has come up with another handy tool called KubeTail which allows us to tail multiple pods at the same time, with colorized output.Step 5: Service Discovery and Load BalancingIn Kubernetes, pods can be exposed as services, which are internal to the cluster but are really handy in inter-service communication.Inside of the cluster, services can address each other through DNS via their service names. A service called producer running on port 8080 can be called with the following URL: http://producer:8080.When creating a deployment containing a service inside of Kubernetes with two replicas, a ReplicaSet will be created for you, and all traffic to that deployed service will be load balanced automatically over those two replicas.For more specialized load balancing (for example when doing Canary Deployments) you can make use of multiple deployments.Step 6: Live and ReadyThis step is really helpful when just starting with Kubernetes, especially when you can’t figure out why your app keeps restarting over and over.Burr taught us about the existence of and differences between the liveness and readiness probes in Kubernetes.Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations.Sometimes, applications are temporarily unable to serve traffic.For example, an application might need to load large data or configuration files during startup.In such cases, you don’t want to kill the application, but you don’t want to send it requests either.Kubernetes provides readiness probes to detect and mitigate these situations.A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.Step 7: Rolling updates and Blue/Green deploymentsAs mentioned above for Canary Deployments, Kubernetes offers the possibility to balance load over different versions of your application.This makes it a great tool to manage rolling updates of new versions of your application, as well as Blue/Green deployments.Although I like the possibilities of the framework, I prefer to use other tools for this like Spinnaker or a service mesh like Istio.These tools will use Kubernetes for us, so that we don’t have to worry about changing and updating configuration files all the time.Step 8: DebuggingAs we’ve said before, debugging in Kubernetes is completely viable and there are several possibilities here.  Using kubectl exec to get inside of the container and live debug the application  Using the Fabric8 Maven plugin to debug the application  Even Visual Studio Code now has the possibility to live debug a running Java application  Red Hat JBoss Developer Studio can be used to debug Kubernetes or Openshift applicationsAs a bonus for his talk, Burr also explained one of the new service meshes out there called Istio.It promises to deliver an open platform to connect, manage, and secure microservices.Using a sidecar proxy inside each pod called Envoy and some governing tools such as Istio Pilot and Mixer, it solves many of the problems that microservice architectures pose, such as secure inter-service communication, service discovery, circuit breaking, intelligent routing and load balancing, etc…This seems like a very promising technology inside of the Kubernetes and Openshift world and we will keep a close eye on it.There was a talk from Ray Tsang at Devoxx about Istio which was very interesting and entertaining, as always.You can find Burr’s presentation right here on Google Docs."
      },
    
      "architecture-2017-10-20-ordina-architecture-kata-html": {
        "title": "First edition of the Ordina Architecture Kata",
        "url": "/architecture/2017/10/20/Ordina-Architecture-Kata.html",
        "image": "/img/kata/kata-6-thumb.jpg",
        "date": "20 Oct 2017",
        "category": "post, blog post, blog",
        "content": "  On the 18th of October 2017, Ordina Belgium organized the first Ordina Architecture Kata.The session was presided by Bart Blommaerts, cross-unit Competence Manager Architecture.A group of sixteen senior consultants, with different areas of expertise, were gathered in Mechelen to practice software architecture.What is a Kata?Kata is a Japanese word most commonly known for the presence in martial arts.The English term for Kata is form and it refers to the detailed choreographed patterns of movements practiced either solo or in pairs.You might know the saying practice makes perfect, and Architectural Katas are exactly that: practicing.These Katas were born out of a simple desire — Software architects need a chance to practice being software architects.  “So how are we supposed to get great architects, if they only get the chance to architect fewer than a half-dozen times in their career?” - Ted NewardPragmatic Architecture Today - RecapIn his conference talk and blog post Pragmatic Architecture, Today, Bart Blommaerts discusses the need to think about Software Architecture.Since this is very relevant to this Architecture Kata, we recap quickly what we learned back then.Why do we need an architecture?We need to build a system.A system is build for stakeholders.Customers, users, developers, … are all stakeholders of a particular system.Those stakeholders need to have a clear view on what needs to be built.Every system has an architecture, even those where architectural decisions weren’t formally made.An architecture is described in an Architectural Description.This description is also particularly useful for the stakeholders.An Architectural Description uses views, which are shaped by perspectives.OODAOODA is a re-entrant feedback loop, that consists of four stages:  Observe: Listen to customers, gather requirements, available resources, …  Orient: Assess comparable systems, use your experience to make sense of your earlier observations.  Decide: From the orientation stage, multiple alternatives might need to be considered. In the decision stage, we take a decision.  Act: Act on your decision, implement.An exercise that can help you in the different stages, is to start with some bullet points and then writing them out explicitly.Comparing the full text with the bullet points, will often be very insightful.To reach consensus when taking decisions, share these with customers, peers, … and verify if they share your ideas.Visualization of the architecture  “One cannot see the ocean’s currents by studying drops of water” — Grady Booch.To a certain amount, you can derive business logic from the code.One might say that the code is the truth, but not the whole truth.Goals of visualizing your architecture:  Consistency  Reporting — Architecture needs to be in the heads of the stakeholders  Checking and validating — Share the architecture with your different stakeholders  Share information — Other people might have experience with certain challengesUnified Modeling Language (UML)Using a language like UML can be useful, especially when doing model-driven development. Also, be very aware that this way of working can become very inefficient.When you are not doing MDD, UML can still be used, if there is shared understanding of the created diagrams.Boxes and linesBoxes and lines are a possibility too, and Bart recommends this more pragmatic approach.Don’t make things more complex than they need to be, boxes and lines are fine.Just make sure to be consistent and always provide a legend.Also make sure your stakeholders understand what you’re drawing.A legend will really help with getting the message across.It’s important that you can discuss a matter while speaking a common language.Avoid fluffy diagrams and mixed abstractions.Don’t mix eg. user interaction information with data flow information.Decision logDocument your decisions and alternatives in a Decision log, also known as Architecture Decision Record (ADR).It will prove itself useful in the future and requires you to think about a decision.There’s no need to invent the wheel here.There are several templates for different use cases available on the internet, for example in this ADR repo on Github.Only document what’s useful.ViewpointsViews help you to make architectural decisions.Bart explained the different views with sharp-cut examples.Context View — Describes the relationships, dependencies and interactions between the system and its environment.Added in the second print of the book.Bart thinks this might be the most important view of them all.Every component is a part of the greater system.Functional View — Defines the architectural elements that deliver the systems functionality.It documents the systems functional structure.You can make decisions on a functional level eg. two components are doing similar things.Should they be separate components?Information View — Models the system data and its state.The purpose of many applications today is capturing data.  Sidenote: Data modeling can be a long and complex process.As an architect, you need to do data modeling at an architecturally significant level of detail.Go to the level of detail that is needed for your team of developers.Concurrency View — Describes the concurrency structure of the system, mapping functional elements to concurrency units to clearly identify the parts of the system that can execute concurrently eg. process a file in blocks.You can solve a lot with specific language constructs and asynchronous messaging.If you want to dig deeper and want to know the nitty gritty details of messaging, a must-read is the book Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf.Development view — Describes the architecture that supports the software development process.When you have an experienced development team, this can be very high-level.Make sure you include the senior developers in the team, when constructing the development view.They have the experience and on top of that… they will be more motivated to be part of the decision making process and technical vision.Deployment view — Describes the physical environment into which the system will be deployed, including the system dependencies on its runtime environment.Make sure you include all information relevant for deploying the application, eg. OS, Apache HTTPD, Tomcat, etc.Operational view — Describe how the system will be operated, administered, and supported when it is running in its production environment.You can use a state chart to describe the operations process.PerspectivesPerspectives shape the views for non-functional requirements.When you introduce perspectives, you’ll have to make trade-offs.An architectural decision will favour certain perspectives and at the same time, hinder other perspectives.For example, strong encryption favours security but hinders performance.Here’s a list of very plausible non-functional requirements:  Accessibility — Ability of the system to be used by people with disabilities.  Evolution — Ability of the system to be flexible in the face of the inevitable change that all systems experience after deployment, balanced against the cost of providing such flexibility.  Location — Ability of your system to overcome problems brought about by the absolute location of your system’s components.  Performance and scalability — Ability of the system to predictably execute within its mandatory performance profile and to handle increased processing volumes.  Regulation — Ability of the system to conform to local and international laws, quasi-legal regulations, company policies, and other rules and standards.  Security — Ability of the system to reliably control, monitor and audit who can perform what actions on what resources and to detect and recover from failures in security mechanisms.  Usability — The ease with which people who interact with the system can work effectively.The KataOur kata for today — AM.I.SCK  Nurses that answer questions from patients via a chat platform.  250+ nurses  Access to medical histories  Assist nurses in providing medical diagnosis  Reach local medical staff, even ahead of time  Enable parts of the system for direct patient usage  Conversations are not considered medical recordsThe sixteen attendees were divided in groups of four.Each team had fifteen to twenty minutes to brainstorm about the case and create the first four views on a whiteboard together.Afterwards, each team had to present their views to the entire group.Bart challenged our opinions and gave practical tips on how to improve our thinking.After a second theoretical deep dive about how perspectives can have an effect on your views, we did the same excercise for the last three views.TakeawaysThe different viewpoints really complement each other.When drawing a view, you’ll notice that you might be able to add more information to another view and vice versa.When drawing a context view, focus on the interactions with other systems.Don’t be tempted in drawing eg. a frontend and a backend component for your system, unless these are separated by external systems.That granularity is not important for the context view.One view can contain several diagrams (eg. you can have multiple state diagrams in the Information View), additional text, tables containing data, etc.Use the experience of every team member to draw the diagrams.Think of similar projects and previous professional experiences.Ordina Accelerator 2018This course was part of the Ordina Accelerator program.With Accelerator, Ordina offers its employees the necessary tools to develop themselves further.Not only technical-, but also social- and organizational skills are included in the program.Medior and Senior experts get the chance to literally accelerate their career by extensively following courses and workshops over a period of two years.AssignmentAt the end of the kata, the participants received their assignment.Upon succesful completion of this assignment, they received an internal Architect Associate certificate.This badge is a pre-requisite to participate in the professional and master programs.The following people have earned the Architect Associate badge:  Andreas Evers  Benjamin Haentjes  Frederick Bousson  Geert Clissen  Jochen Van de Voorde  Johan Buntinx  Jos Clijmans  Ken Coenen  Kevin Bosteels  Koen Vanden Bossche  Mario Van Hissenhoven  Martin Kwee  Sebastiaan Tempels  Stijn Dierckens  Sven BovensThe list will be updated, when more people receive the badge.Their results, will help shape to future of the Ordina Architecture Katas.Links and resources  Recommended reading: Software Systems Architecture by Eoin Woods and Nick Rozanski.In this book, they discuss Viewpoints and Perspectives  Architectural Katas on neilford.com  https://archkatas.herokuapp.com/  ArchitecturalKatas Google User Group  Liminal Thinking  Enterprise Integration Patterns"
      },
    
      "tech-2017-10-18-javaone-html": {
        "title": "JavaOne 2017",
        "url": "/tech/2017/10/18/javaone.html",
        "image": "/img/j1.jpg",
        "date": "18 Oct 2017",
        "category": "post, blog post, blog",
        "content": "The last time I visited JavaOne was back in 2014.So, I was very excited to go back to San Francisco this year.TakeawaysJDK9JDK9 was the “big topic” of JavaOne, with Jigsaw getting a lot of attention.Jigsaw might break some code (eg. code that uses internal Sun APIs), but Java modularity will surely help further adoption of Java.Many libraries and frameworks already work together nicely with JDK9.We saw this in a nice demo of IntelliJ where the IDE does a lot of the Jigsaw heavy lifting.Another interesting change, is that Java will have a new release every six months from now on.These releases will also introduce a new numbering scheme, comparable to what Ubuntu has been using.While I applaud the idea to deliver faster, it will definitely come with a learning effort, for developers.The Good Cop/Bad Cop Guide to Java 9A very entertaining talk on JDK9 was the one from Simon Maple and Oleg Šelajev from Zeroturnaround where they discussed pros and cons of JDK9 modules, G1GC, JShell and other API updates.It was probably one of my favorite talks, because it was both funny and educational.FN ProjectDuring the first Java keynote, the FN Project was announced and open-sourced.The FN project is a container native serverless platform that you can use on any platform supporting Docker.This also means that local development becomes very easy, which isn’t always the case with other serverless solutions.It has out of the box support Java, Go, Ruby, Python, PHP, and Node.jsI definitely look forward to playing with it.Microservices BOFIn the microservices BOF on Monday evening, Chris Richardson presented what must be the perfect summary of the current state of everything going on in the microservices landscape.He created a pattern language for microservice architectures at microservices.io that I find particularly interesting.After Chris, Josh Long proved that he can bring a 45 mins talk in 15 mins with only live coding which was very amusing to watch.JavaOne surely showed us that microservices are still hot!ACID Is So Yesterday: Maintaining Data Consistency with SagasWhen creating a distributed system with microservices, using a database per microservice is generally seen as a best-practice.A drawback of this approach is that transaction management becomes a big hurdle and ACID (Atomicity, Consistency, Isolation, Durability) is no longer a desirable option.An alternative for ACID, is BASE: Basically Available, Soft state, Eventual consistency.BASE can be realized with sagas.A saga is a sequence of local transactions.Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga.If a local transaction fails because it violates a business rule then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.Clouds and containersAs expected, a lot of talks focused on cloud deployment and operation.One of the more interesting ones, compared the different container orchestration options and runtimes like Docker Swarm Mode, Kubernetes, Amazon ECS, Mesos/Marathon, Rancher and Triton.The talk demoed some differences between (for example) Docker Stack and Kubernetes, like the scaling of pods vs. the scaling of services.To make a correct choice between all options, it is imperative to take consistency, portability, build integration with CI / CD, community and transferability of skills into consideration.Source code of the comparison is available here.My TalkMy talk: 10 Tips To Become An Awesome Technical Lead was scheduled on Thursday in the exquisite Marriott Marquis.From the start, I had a lot of interaction with the attendees and that obviously is very motivating.Thanks a lot for that!I uploaded my slides on Slideshare.Networking EventsSilicon ValleyAfter landing in San Francisco, my colleague Andreas, picked me up at the airport and we went to visit some of the most famous tech companies in Silicon Valley.Silicon Valley, in the southern San Francisco Bay Area of California, is home to many start-up and global technology companies.Apple, Facebook and Google are among the most prominent.The size of some of these companies really was remarkable.Visiting the HP garage, the Android statues etc. was a really great way to cope with jetlag :)      Oracle OpenWorld Benelux Bike Tour 2017Sunday morning, Oracle Benelux organised a Bike Tour for the Benelux attendees, which I liked a lot.We biked the typical San Francisco route: Fisherman’s Wharf, Fort Mason, the Marina, Golden Gate Bridge, Sausalito and then back by ferry.Three years ago, I did a similar tour, but did not take the ferry back.The ferry was actually very nice: passing Alcatraz and watching the San Francisco skyline pop up.  Social events and partiesThere are a lot of parties happening during JavaOne.One of the highlights, for me, was PartyOne 2017 by ZeroTurnaround, Hazelcast, Tomitribe and BainCapital.This was an incredible opportunity to talk to some of the really big names of the Java industry in a very relaxed atmosphere as you can see in the following picture :)  Another fun event, was Oracle Cloud Fest: a concert of Ellie Goulding and The Chainsmokers in the AT&amp;T Park.We teamed up with our Ordina colleagues from the Netherlands to enjoy our last night in San Francisco.  "
      },
    
      "iot-2017-10-12-stairway-to-health-html": {
        "title": "Stairway to Health with IoT and the MEAN stack",
        "url": "/iot/2017/10/12/Stairway-To-Health.html",
        "image": "/img/stairwaytohealth/stairway-to-health.jpg",
        "date": "12 Oct 2017",
        "category": "post, blog post, blog",
        "content": "  Healthier at the office with the ‘Internet of Things’.What is Stairway to HealthIn an effort to improve worker health in a fun and engaging way, Proximus wanted to encourage their employees to take the stairs instead of the elevator.This is when the idea of a little game between the three towers came along. On different dashboards across Proximus and on the Stairway to Health website, the employees could see which tower had the most employees taking the stairs.They can also get a more detailed look of how many people taking the stairs where and when, with drilldown views for monthly, weekly, daily, and even hourly statistics.What does it do?The Stairway to Health project is a simple yet great example to show what the Internet of Things can do:  LoRa sensors detect door openings, these are installed on the doors of the staircases  These sensors communicate via the Proximus LoRa network to report their status  Sensor data is sent to the Proximus MyThings platform which processes the data  The data gets sent to the Stairway to Health application  The Stairway to Health application interprets and visualizes the dataIn summary: We install sensors on the doors (things) to measure usage and we analyse the data to persuade people to move more. The result is a good example of how IoT can influence our daily lives. Proximus was able to provide us with all the necessary building blocks to offer a complete end-to-end solution!MyThings and Stairway to HealthMyThings is the Proximus IoT platform for onboarding, managing, configuring and monitoring IoT sensors. By registering (onboarding) our sensors to the platform, we can let MyThings take care of decoding the messages and set up a stream to our application.This way every time a log comes in from the sensor, we get the decoded data posted to our designated endpoint.The Requirements  The usage of the stairways is measured and the results should be visualized on large screens in the towers.  These screens should have a QR code so that employees can easily visit the application on their mobile devices.  When visiting the website, they should be able to click on the results to get a more detailed view of the data.  The frontend application should be available in Dutch and French and the dashboard should switch between these languages every minute when viewing it on the large screens.  Admins should be able to manage locations (towers) and chart timespans.  It should have an info page with some information about the project and its purpose.So technically this translates to build an application that:  Has an endpoint to receive logs from the MyThings Application,  Stores the data to its own database,  Show the data in charts that have multiple layers to see more/less details,  Shows the ratio of the results per tower,  The frontend dashboard data has to reload automatically (since it is shown on some big screens @ Proximus),  Add multi-language (automatically switch languages when viewing on tower’s large screens),  Is performant (able to handle many logs coming in and calculate the data to be displayed in the graphs),  CRUDs for managing timespans and locations,  Use the timespans / locations when displaying data.Oh, and did we mention we were only given four weeks to complete this mission…The IngredientsSo given all the requirements listed above and the fact we didn’t have a lot of time to waste, we chose to use a MEAN (TypeScript) stack. MEAN stands for MongoDB Express Angular and NodeJS. It’s possible to use the mean stack with plain JavaScript, we chose to implement it with TypeScript since we wanted some strong typings on the backend application and we were going to use Angular 4 on the frontend which comes with TypeScript as well.NodeJs:Write event driven applications with asynchronous I/O powered by the ultra fast Google V8 Engine. Mostly known for running your local dev environment and automating build tasks for frontend developers. NodeJS is probably one of the best and easiest options out there for real-time applications (with socket.io), which is exactly what we needed for our application.MongoDB:Great to work with when dealing with JavaScript Objects. Good driver support with Mongoose for NodeJs. Document based structure, which makes it really flexible when it comes to modelling and it’s extremely scalable. We also took advantage of the very performant aggregation functionality for dealing with large amounts of data.ExpressJS:A node framework that comes with some great functionality for setting up your node server and makes it easy to create routes, middleware, handling requests/responses, serving files from the filesystem, configuring static files, easy connections to the database, and much more.Angular(4):A TypeScript-based open-source frontend web application platform led by the Angular Team at Google and by a community of individuals and corporations to address all of the parts of the developer’s workflow while building complex web applications.Socket.IO:Socket.IO enables real-time bidirectional event-based communication. It works on every platform, browser or device, focusing equally on reliability and speed. To trigger events on our frontend application we used this great library to be able to detect when new data has been received and refresh the dashboard.Highcharts:Interactive JavaScript library for creating dynamic charts. Highcharts is based on native browser technologies and not reinvent the wheel. Thousands of developers have contributed their work for us to use in our own projects. Also backwards compatible for IE.JavaScript across the stackNot only does it make development quite a bit faster and easier by having a large community with lots of reusable code for your application (npm), it also lowers the barriers between frontend and backend developers by using the same programming language over the entire stack, so more efficiency and faster, leaner development which in turn means lower development costs. Also worth noting is that JavaScript currently is THE most popular programming language, so more developers will be able to easily understand and contribute to the application if needed. And probably the most important criteria: when it comes to cloud hosting, RAM is probably the main influencing factor when it comes to pricing. NodeJs uses less RAM than comparable Java applications.Source and more about these testsNow that I’ve listed some of the pros of full-stack JS, I should also mention that it might not be the best solution for computation-heavy backend applications.For projects like machine learning or heavy mathematical calculations the single CPU core and having only one thread that processes one request at a time might be easily blocked by a single compute-intensive task. Yet, there are numerous ways to overcome this limitation. By simply creating child processes or breaking complex tasks into smaller independent microservices.Let me just note that the comparison with Java above here is not because we are claiming that one is better than the other, it’s just to demonstrate that they both have their use cases and can be equally worth considering when choosing a technology for your application.Some great use cases for JavaScript across the stack are:  real-time chat,  Internet of Things,  real-time finance (stocks),  monitoring applications,  event-driven applications,  server-side proxies,  many more…Blocking vs. Non-BlockingIn NodeJs you can take advantage of JavaScript promises. One of the benefits of this is that we can write non-blocking code.To demonstrate how this works, I’ll give you an example in pseudo code for reading a file from the filesystem.Blocking:read file from filesystem, set equal to \"contents\" print content do something elseNon-Blocking:read file from filesystem &nbsp;&nbsp;&nbsp;&nbsp;Whenever we're complete print contents (callback)   do something elseSetting up our dev environment / buildThe frontend part of this was really easy. We used angular-cli to generate a new project. In the future this also gave us the advantage of generating new components, services, pipes, testing and much more. Also for the charts and translations we choose for easy to use libraries like Highcharts and ngx-translate (previous ng2-translate).For the backend we decided to go with gulp. We added some tasks to transpile our server site TypeScript files to JavaScript so that node can execute it. For local serving we created a sequence task that combines running ng build from the angular-cli and a gulp task to use nodemon for running our server and restarting on changes. When working on the frontend, doing an ‘ng build’ was a bit too slow, therefore we added a --standalone flag, to the serve task so that we could just build the backend application and do the frontend serve with ng serve which is a lot more performant than having to do a ‘ng build’ on every change.Since we are using TypeScript throughout the application, it only felt right to use the TypeScript version of gulp as well. It takes a little effort to get used to, but once you get the hang of it, it makes writing gulp tasks a lot more fun and less error prone.Using the provided decorators, our gulp tasks look something like the following:@Task()    public environment(cb) {        return gulp.src('./dist/app/server/config/mongo.connection.js')                   .pipe($.if((yargs.env === 'prod'), $.replace('mongodb://localhost:27017/stairway', require('./secrets').mongoUrl)))                   .pipe(gulp.dest('./dist/app/server/config'));    }and create sequence tasks with:@SequenceTask()    public mocha() {        return ['buildApp', 'runMochaTests'];    }Now that we have a gulpfile.ts file, we need to ensure that the gulpfile gets transpiled as well, we did this by adding an npm script, so that we can use TypeScript compiler with the tsc command to transpile the file and make sure we are using the latest changes every time we use gulp.(to get the tsc command, install typescript globally with npm)Building Stairway to HealthAfter setting up our dev environment, database and getting a simple application up and running it’s time to start implementing our first features.Receiving data from MyThingsSo first things first, on MyThings we took a look at how we were going to structure the data that was going to be streamed to the Stairway to Health application.In the MyThings application every sensor can have a friendlyName1 and friendlyName2, we used these to specify which tower and which floor they represent. The sensors send a lot more data than just the magnetic pulse counts, therefore we needed the container field, to be able to filter on counter logs only (however, we store the other messages as well, maybe for future use). The value field is the amount of times the sensor was triggered, in other words, the actual counts. And of course a timestamp since we will show the data in time based graphs.The timestamp represents the time that the sensor has sent its message to the MyThings application, we also wanted to keep track of when our application has received the log, so before saving we added one extra field to store this in our database.After we defined our model/schema of our logs, it was simply adding an endpoint to our express router and our first feature was ready. Well not exactly, we needed to trigger an event to refresh the data on our dashboard, but we’ll get back to this later.The DashboardSince we created an Angular(4) application, we took advantage of the great features of angular-cli which makes it really easy to get a new project up and running and generate new components, services, tests and much more. We started by adding all the components needed for the application and adding the Proximus styling to the project. After that we imported the Highcharts library from npm to first make the charts on the homepage and later making the charts for the detailed views. All the charts were first made with mock data so that we could perfectly say from the backend what data we needed and in which format. From now on we knew how our JSON for the charts had to be made and we could implement the api endpoints for the dashboard and the details page. Finally after adding all the charts we started on adding the different languages to the application. Here we got our biggest ‘lesson learned’, it is much faster to start with I18N then to end with it, this is because you have to find all the normal text in the HTML files and copy them to the JSON-files. ALso we had to quickly create a translation list that the business could translate for us.Mongo AggregatesAs for displaying the daily, weekly and total counts below the buildings, we had to get this data from the database, keeping in mind that we would have to iterate over millions of sensor logs (at the time of writing this blog post, 1.4 million over 4 months). We had to make sure it was performant. This is where the Mongo aggregates come in handy. Instead of looping over the results and adding them up, we let Mongo take care of this with the $sum operator which in code looks like the following:this.sensorLogModel.aggregate([                {$match: {container: 'counter', value: {$ne: 0}}},                // group them by fn1 (tower) and add up all 'value' fields                {$group: {_id: '$friendlyName1', total: {$sum: '$value'}}}            ]);Remember, we store all the logs, but we only need counter logs. So for a little more performance, we leave out the ones with value 0 (a lot of them in the weekends), that’s what the $match is forThe result: an array with objects that have an _id field with friendlyName1 as value and a total field with the sum of all (counter) values per tower. We repeat this for daily and weekly, but add a start and end date (which we simply create with TypeScript). $match then looks something like this:{$match: {container: 'counter', value: {$ne: 0}, timestamp: {$gt: start, $lt: end}}}Later on we added some more calls to get the data by time span and location for the more detailed chart data, but you get the idea, we simply edit the timestamps or friendlyName1 (also by friendlyName2 on the hourly chart, which displays the hourly data per floor).Socket.IONow that we have data that can be retrieved and displayed on the frontend, time to implement some way to let our frontend application know when we receive some new data, so that it in turn can do a request for that new data.For this one to be clear we’re going to skip ahead in time and show a high level scheme of how the application is made up.    The bin (js) file is where we create our http, https and socket servers. To communicate between them, we use the node event emitter. The server.ts file (let’s call it the app) gets bootstrapped onto these servers and when creating the app, we pass the created event emitter to it. This enables us to listen and broadcast events back and forward. The event emitter emits events between the backend services and the socket.io server emits events to our frontend application.So in our case, to let the frontend know when the sensor-log endpoint has received a message, we emit a log-received event on the node event emitter. In the socket IO server we are listening on this event and we broadcast a data event to every connected frontend application. The frontend applications are listening for this data event and refresh their data by calling the dashboard endpoints.However, since we have about 60 sensors sending data, this event was triggering quite a lot and with the chart rendering animations on our frontend application we had to wrap the log-received in a timeout so that we would only refresh it once every 30 seconds (if a log was received).I’ve picked a few lines of code from our bin file to demonstrate how we pass the eventEmitter when bootstrapping our application on to the http and https services from node.const server = require('../dist/app/server');const http = require('http');const https = require('https');const events = require('events');const eventEmitter = new events.EventEmitter();const httpServer = http.createServer(server.Server.bootstrap(eventEmitter).app);const httpsServer = https.createServer(options, server.Server.bootstrap(eventEmitter).app);After that, we bootstrap the created https server on to the socket.io application. It too gets the same EventEmitter instance passed into its constructor.const io = require('socket.io')(httpsServer);const sockets = require('../dist/app/sockets');const ioApp = sockets.Sockets.bootstrap(io, eventEmitter).io;In our sockets file, the method that gets executed will listen on the logsReceived from our passed EventEmitter, and emits a data event on our io instance.public sockets(eventEmitter, io){    eventEmitter.on('logsReceived', (logs) =&gt; {        io.of('/socket/').emit('data', logs);    });}Configuration CRUDSince we did not want our configuration to be hard coded, we added some configuration screens to be able to change the time spans and entities (towers).                                    By the way, ‘gewicht’ in the first image stands for weight. To make sure the ratios are fair, we made sure that every tower has a ‘weight’ to multiply its log values by. These weights are calculated by the amount of employees/tower, with the largest tower having a weight of 1.Let’s take a look at how we set up our backend structure for creating crud endpoints.In our /routes directory we keep all files that define the urls and methods of every endpoint, and tell it which controller and method to use:timespan.route.tsrouter.get('/timespan/', (req: Request, res: Response, next: NextFunction) =&gt; {    this.timespanController.getTimespanList(req, res, next);});router.post(('/timespan/', this.authenticate, (req: Request, res: Response, next: NextFunction) =&gt; {    this.timespanController.createTimespan(req, res, next);});next under our /controllers directory we have our controllers where all our functionality/logic istimespan.controller.tspublic getTimespanList(req: Request, res: Response, next: NextFunction) {    return this.timespanModel.find({}, [], {sort: {start: 1}})    .then((result) =&gt; {        res.json(result).status(200);    }, (err)=&gt;{        res.statusCode = 500;        res.statusMessage = err;        res.send();    });}AuthenticationTo prevent everyone from changing these configurations of course we had to add some authentication functionality. As you can see in the router code above, we created an authentication middleware so that on every route that we want the user to be authenticated, we can simply add this.authenticate() to the route. This checks a JWT token in the headers. We check the token to be valid. If it’s not valid, we send an unauthorized response, and if it is valid, we decode it and add it as a user object on the request. This way we can access it in the controller and do some logic depending on its role, etc.this.authenticate is a method we added to the core.route.ts.Every route extends this super class so that we can put common code and middleware in this file.JWT stands for JSON Web Token and is a JSON-based open standard for creating access tokens that assert some number of claims. For example, a server could generate a token that has the claim logged in as admin and provide that to a client. The client could then use that token to prove that he is logged in as admin.DeployFinally we deployed it to the Proximus data center and watched the Proximus employees take on the challenge.                                ConclusionAfter four hard weeks of working and writing many lines of code, we delivered our project to Proximus and the contest could start. Things we would have done differently:  Use mongo indexes and aggregation for large amounts of data  Use javascript date in stead of timestamps in mongo, easier to create aggregate with dates  Dockerize! So far, the most work has gone into getting the application deployed  Implement I18N translations at the beginning, as it is better to add translations while working on the component  Also we learned how complicated it can be to have one component with multiple switching charts. Instead of switching components."
      },
    
      "conference-2017-10-09-perconalive2017-html": {
        "title": "Percona Live 2017 Dublin",
        "url": "/conference/2017/10/09/perconaLive2017.html",
        "image": "/img/percona-live-2017/logo2.png",
        "date": "09 Oct 2017",
        "category": "post, blog post, blog",
        "content": "    Percona Live Europe is a yearly conference on open source database organized by Percona.We had the opportunity to attend this year’s conference in the beautiful city of Dublin.Beside enjoying the local brews and drafts we attended several sessions out of which we highlight some sessions in this blog post.MongoDB Shootout: MongoDB Atlas, Azure CosmosDB and Doing It YourselfWhen running MongoDB in the cloud, you have several options. David Murphy compared MongoDB Atlas, CosmosDB and the good old DIY.Using Atlas, you get monitoring, automation and the possibility to pay for backups.You pay per instance and you have a wide variety of instances and regions to choose from on AWS, GCP and Azure.The biggest downside is that the cost is about 44% more than running you own servers in the cloud.This means that, in contrast to DIY, you continuously pay more instead of writing off your initial investment and paying less in the end.The monitoring is really good but the problem here is that if you have have a polyglot environment with your own monitoring and alerting, you can not integrate it with Atlas so you end up with yet another tool.Upgrading is really easy and just a click of the button thanks to the automation.Backups need to be paid for per GB and are taken continuously.CosmosDB is offered by Microsoft on Azure and claims to be MongoDB compatible. This is not completely true because they have no support for the aggregation framework.So you can only use it for simple CRUD operations.The pricing is based on a pay-per-operation model which means it’s really hard to figure out what your cost will be and how it will evolve over time.Here you also have the downside of continuously paying more than DIY.The monitoring is very basic and of no help when you run into problems or strange behaviour, it’s like a black box.Upgrading is done behind the scenes which means you don’t have to worry unless the upgrade means your code is no longer compatible, then you are stuck.Backups are very basic because they are taken approximately every four hours and only the latest 2 backups are stored.DIY has the most power to offer IF you have a mature, and complete DevOps team.With DIY you pay a high price up-front for hardware and people.You need to implement your own monitoring, i.e. with the Elastic stack or Percona’s PMM.Also, automation is a big part of the effort.And last but not least, you need to implement your own backup strategy.The biggest upside is that you have full control over what you implement and how you do things.You choose the cloud service provider or the hardware you want to use.You choose what you want to monitor and how you alert.You choose how often and when you backup.But of course, it’s all up to you.Visualize Your Data With GrafanaGrafana is used to build monitoring dashboards based on time series data.Grafana supports a wide range of data sources to get its data and generate the dashboard.There are already a lot of pre-built dashboards you can use and customize for your own needs.You can build your own dashboards with panels.Each panel is fed with data from a datasource based on a query.Grafana has created a query editor with support for different data sources, like PromQL for Prometheus, to make it easier to build the queries you need.The end result can look like this :Database Reliability Engineering: What, Why and How?The DBA from the old days, hidden in a basement behind closets, performing magic on that mysterious thing called a database, is dead.Enter the Database Reliability Engineer (DBRE, loosely based on SRE), a person who embraces the new paradigms in the IT-world.He is an advocate of how data should be treated and used, he teaches his colleagues, he takes part in pair-programming, he is an active team member in cross-functional teams.A DBRE’s knowledge is not confined to a single system, he can support polyglot persistence.He can support these systems on premise and in the cloud.He automates as much as possible and uses tools of the trade including source control systems and helps creating infrastructure as code.The DBRE enables his organisation to apply known principles of the software engineering world to the database world.In this role he applies principles from Database Reliability Engineering, like designing for scale, availability, operations and performance.Also visibility, alerting and database change and release management are just a few parts of the tasks to do.For more detailed information make sure to check out the book Database Reliability Engineering, a must read for everyone in the field.MongoDB Security ChecklistMongoDB has been in the news lately due to MongoDB ransomware attacks. This might make you wonder whether or not MongoDB is secure.Well, rest assured it is very secure.But you need to turn security on, at least until the next major release where security will be on by default.MongoDB has a plethora of security features in their community edition and the commercial offering provides even more goodies like LDAP integration and baked in encryption-at-rest.It starts with simple username/password authentication and moves on to x.509 certificates based authentication.Once authenticated you have authorization with either build-in or user-defined roles and privileges, so you can fine-tune which users have access to which database or collection and which actions they can perform on them.You can further lock-down your MongoDB by fixing the network interface it is listening to so it’s not open to the internet, or encrypting the communication between replica-set or sharded-cluster members.If you are running MongoDB, then reading the security checklist is a must!Improvements to MongoRocks in 2017MongoRocks is MongoDB using RocksDB as the underlying storage engine.From the MongoRocks website :\"RocksDB is a key-value library based on Log Structured Merge Trees. It is maintained by the Facebook Database Engineering Team, and is based on LevelDB, by Sanjay Ghemawat and Jeff Dean at Google\"MongoRocks differs from WiredTiger in the way it stores data.WiredTiger uses a B-tree where MongoRocks uses a LSM-tree structure.Both have pros and cons of course.An LSM-tree structure favours space and insert efficiency over read efficiency.An B+ tree structure favours update and read efficiency over space efficiency.So depending on your workload you can choose which might suit you better.Of course, nothing beats measuring what the effect of your workload is on performance of the choosen storage engine.Because, you know, silver bullets and such…So testing and measuring is key in deciding which engine you should choose.Nevertheless, MongoRocks is showing nice improvements over previous versions and has several interesting benefits over WiredTiger.Certainly when storage endurance is an issue or if your working set does not fit into memory.Automatic Database Management System Tuning Through Large-Scale Machine LearningThis is probably the most stunning talk of the conference.OtterTune is a tool developed by students and researchers at Carnegie Mellon to automatically tune your database.This is done by making clever use of previously collected data of other tunings and applying machine learning to it.The presented results showed that for the given workload, OtterTune was at par with DBAs which had double digit years of experience.Looking at this from the bright side, OtterTune would help DBAs to focus on areas other than figuring out which combination of the multiple settings they should use to tune their database.It would definitely help to do better Database Reliability Engineering.ConclusionThis is just a small portion of the huge amount of sessions at Percona Live, but of course, one needs to choose.It’s really great to see this conference putting open source on the foreground and displaying the wealth of choice and diversity of technologies in the open source database world.We see that this space is continuously expanding and that the future is looking even more promising than the present.Good times ahead!Useful links &amp; further reading  Percona Live Europe Dublin  MongoRocks  Grafana  MongoDB Atlas  Elastic  MongoDB security checklist  Database Reliability Engineering"
      },
    
      "kickstarters-2017-10-05-kickstarter-trajectory-2017-html": {
        "title": "Kickstarter Trajectory 2017",
        "url": "/kickstarters/2017/10/05/Kickstarter-Trajectory-2017.html",
        "image": "/img/kicks.png",
        "date": "05 Oct 2017",
        "category": "post, blog post, blog",
        "content": "  This year, 45 young professionals started the various Ordina Kickstarter trajectories.Five of those, participated in the Kickstarter trajectory of JWorks.Each of them looking for a challenge and a fresh start!For some, it was a transition between school and work, and Ordina handled this very well.The main goal of this trajectory was to widen every student’s knowledge of the IT world.They taught us the basics of every topic that’s hot at the moment.This will definitely come in handy during our first project.First impressionsWhen we arrived the first day, we were welcomed with breakfast and afterwards, we got a tour around the building.The corporate culture here is truly a plus for Ordina and we immediately felt at home as everybody is really friendly and helpful.We got our equipment consisting of a car and either a Windows laptop or a MacBook Pro depending on our preference.It’s clear that, starting from day one, Ordina makes sure that their consultants are well equipped to work successfully.What is the Kickstarter trajectory?The Kickstarter trajectory consisted of intensive training spanning two months.During this time, courses on different frontend and backend technologies, methods, tools and soft skills were given by senior consultants and external lecturers. This trajectory is ideal for graduating students and people who want to make a switch to IT.The constant guidance and support made this a good preparation for our first project.During the fourth week we had a team building event with all the kickstarters from the different units. The goal was to get to know the other kickstarters and strengthen our team spirit by shooting each other with bow and arrow!During the last week of the Kickstarter trajectory, the ‘JOIN Event’ was held.This day was split into two parts: the unit meeting where the whole JWorks unit gathers for a year report where every competence center presents its past and upcoming activities, and the completed and current projects are highlighted. And in the afternoon the JOIN Event itself. There were talks given by members of JWorks as well as by external speakers about upcoming or commonly used technologies such as Docker and Spinnaker, but also about Scrum and User Experience.In addition to gaining knowledge about these subjects, this event provides a unique opportunity to meet most members of the unit or at least see them, since there are over a hundred.Other events we could participate in were the CC-meetings and the Ordina Boardgame Night.These also provided opportunities to get to know both our and other units as we deepened our understanding of the presented subjects.As you can see Ordina employees are very involved.TrainingWe kicked off our career at Ordina in the JWorks unit.The main technologies used in this unit are Java &amp; JavaScript.Most of the workshops in the Kickstarter trajectory are based on these two technologies.Small side note, don’t be fooled by these names: Java &amp; JavaScript have no underlying connection.In general, the development of modern applications is divided into two main groups: the frontend part and the backend part.Java dominates in the backend part, while JavaScript is located in the frontend part.For two months, we’ve had the luxury to deepen our knowledge in both domains, in order to obtain a comprehensive understanding of the cutting-edge technology stack that’s being used in JWorks.Below you’ll find a brief overview of the technologies we explored and applied in the workshops.  Frontend          HTML5, CSS3, BEM, SASS      JavaScript, TypeScript      Angular (2+), Ionic      Npm, Bower, webpack        Backend          Newest features in Java 7/8/9      Java EE, Spring, Spring Boot with Maven      JPA, Hibernate      Unit Testing: JUnit, Mockito, etc.      MongoDB      Cloud solutions (PaaS, IaaS, SaaS): Pivotal Cloud Foundry and OpenShift      Docker, Git        Design principles and methodologies          Microservices      Agile, Scrum      DevOps, Continuous Integration      DDD, BDD, TDD      Rather than learning these technologies in isolation, we learned to create applications by combining them.For example, we used TypeScript, CSS and Ionic to build mobile apps, we wrote backend logic by uniting the forces of Spring Boot, Java and unit testing.We also combined Angular, TypeScript, SASS, HTML and npm to create a web application.In our projects, we learned to utilize the Version Control System Git to collaborate with a team and share projects and code progress.One thing we’ll never forget is that if we push breaking changes to the master branch and break the build, we’ll have to buy “boterkoeken” for the unit!Last but not least, we were introduced to the wonderful world where clean code and microservices are the heroes that kick spaghetti code and monoliths in the butt.During the trajectory we also had a couple of “free” days during which we had to read the Clean Code book and prepare ourselves for the Oracle Certified Associate, Java SE 8 Programmer I exam.Memorable moments“The soft skill sessions were real eyeopeners and gave me a better understanding regarding introducing myself, giving presentations and the Agile methodology.The introduction to microservices was very interesting since moving away from monoliths is the way to go.” – Ken“The tips received during the communication essentials sessions are a backpack full, good enough to present myself in a proper way.Diverse technologies used in JWorks e.g. turning a monolith to microservices, a different type of database like MongoDB, running your applications in Docker containers and such.” – Michiel“Learning about the existence of ‘microservices’ versus ‘monoliths’.Writing unit-tests to see if the code does what it should do before deploying the application in production.And the introduction to the MongoDB database and how to use it in the command line interface.” – Jef“The communication sessions were worth their weight in gold!How often do you get a chance to practice communication in the most awkward situations?I guess every day, but at least in these sessions there were no real repercussions :)” – Simon“Learning about all those different technologies.There really is too much to choose from.But if I have to pick something, it’s the Spring Boot session.I previously experimented with Spring Boot at my internship, where nothing seemed to make sense.But after this session, everything I was struggling with became clear and fell into place.” – NickThe new JWorks colleagues"
      },
    
      "angular-2017-10-04-testing-angular-with-karma-html": {
        "title": "Testing Angular with Karma 101",
        "url": "/angular/2017/10/04/Testing-angular-with-karma.html",
        "image": "/img/2017-10-02-testing-angular-with-karma/unit-tests.png",
        "date": "04 Oct 2017",
        "category": "post, blog post, blog",
        "content": "Testing your code is as important as writing the code itself. This also counts for frontend applications such as Angular apps. Unit testing is one way to do so. The goal of these kind of tests is to isolate classes and verify the output of its functions to be what you expect when they are called.We also need a tool to run our tests written in TypeScript.Karma is the one we’ll be using to run tests described in this blog.It will open a browser, execute pieces of JavaScript and report the results back to you.Now, I must admit that I’m not too fond of writing tests myself. However, I do strongly believe they help a lot towards improving the quality of the code. Writing unit tests can be quite a hassle, but with an application that is continuously growing and changing, they are an efficient way to prevent bugs getting to production.Table of contents  Setup  Writing tests  What to test  Tips and tricks  ConclusionSetupLet’s take a look at how it’s done in an Angular app using Karma.If you’re using the Angular CLI, you’re in luck because setting up the unit tests is easy. It’s already done! All you need to do is run ng test (or npm test). It will transpile your tests and run them using Karma. If you’re not using the Angular CLI yet, I recommend creating a new project with the CLI and copying your existing project to it.It will make your life a lot easier.Running ng test will run the tests in watch mode, meaning that every time you save a change to a file, it will automatically rerun your tests. Additional flags can be passed like --single-run to make it run only once. When passing the --code-coverage flag, it generates a report in HTML. By default it’s found under coverage/index.html and it indicates which parts of your code were covered by your unit tests.    Writing testsStructureNow that the setup is done, let’s look at how to write the tests themselves.First of all, test files should be named after the .ts file you’re testing, but with .spec added to the file name (e.g. when testing login.component.ts, the test file should be named login.component.spec.ts). It’s best practice to keep the spec file in the same folder as the ts file. So mostly, for a component, you’ll end up with a HTML, scss, spec.ts and ts file in one folder (unless you like to inline your HTML and CSS).    Next up, the content of a test file. The Jasmine spec is used to format the tests (more info).This means that individual tests are grouped together in a describe block. A test itself starts with it. Besides tests, you can also add other blocks to a describe, like beforeEach, beforeAll, afterEach, afterAll… What these blocks do, is quite self-explanatory. Here’s an example how it could be used: when testing a class, you’ll want to create an instance of that class for each test, so instead of writing the same code in each test to create an instance, you could put that code in the beforeEach clause. Simply pass a function (in lambda notation) to beforeEach containing the code you want it to run.Within a test itself, the class’ public functions can be called and assertions can be made. Assertions are made using the expect function. You can give it a variable or a call to a function and tell it what you expect the result to be with toBe, toEqual, toBeTruthy, toBeFalsy, toBeNull…Here’s an example:describe('NAME_OF_YOUR_CLASS', () =&gt; {    let component;    beforeEach(() =&gt; {        //initialize        component = new AppComponent();    });        //Actual tests    it('should have a car selected', () =&gt; {        //assertions        expect(component.carSelected).toBeThruthy();    });    it('should find my favorite car brand', () =&gt; {        //assertions        const carBrand = component.getFavoriteCarBrand();        expect(carBrand.name).toEqual('Mazda');    }); }); As you can see, you can pass some text as an argument in the describe call. This is usually the name of the class you’re testing and it’ll be shown when running the tests. For the tests themselves, you can also pass some text which will be shown. These are mainly used for you to be able to identify failing tests. The text should describe what’s being tested, for example “It should get the brand of the car”, could be written as it('should get the brand of the car', () =&gt; ....Writing the actual testsThere are multiple ways to write unit tests for an Angular app. Either you use the Angular TestBed, the ReflectiveInjector or you simply call the constructor of the class directly. ReflectiveInjector and TestBed have a similar approach, so I’ll only be discussing TestBed here.It’s something pretty cool Angular came up with in order to test your components. TestBed can create components and injects all its dependencies.The instance of the component that is returned can then be used for testing.Accessing the view is also possible.Now, although I said there are multiple ways to unit test an Angular app, there’s actually only one correct way: calling the constructor.Since TestBed loads the view as well as any components, directives… used in the view, you’re actually also testing how the class integrates with them.In other words, you’re entering the domain of integration testing, which is also important, but out of scope for this blog post.The unit tests you would write using the constructor approach, could practically look the same when you would use TestBed to instatiate the components. However, there are some problems with using the Angular TestBed for unit tests which I’ll be explaining below.1. TestBedSetting up the TestBed configuration for a component kind of looks like a module definition. You should list all components, directives and services that are used by the component you’re testing directly or by importing a module that includes them. Calling createComponent will return a ‘fixture’ which can be used to access the view and also get the instance of the class linked to it. With the fixture you can find HTML elements and perform actions on them, verify their content and attributes…The instance of the class can be used to test its public functions (unit test).describe('AppComponent', () =&gt; {    let component: AppComponent;    let fixture: ComponentFixture&lt;AppComponent&gt;;    beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [CarBrandService],            imports: [CommonLogicModule]        })        .compileComponents();    }));    beforeEach(() =&gt; {        fixture = TestBed.createComponent(AppComponent);        component = fixture.componentInstance;        fixture.detectChanges();    });    it('should test the class', () =&gt; {        //use component to test the class itself        const carBrand = component.getFavoriteCarBrand();        expect(carBrand.name).toEqual('Mazda');    });    it('should test the view', () =&gt; {        //use component to test the class itself        const carBrand = component.getFavoriteCarBrand();        expect(carBrand.name).toEqual('Mazda');        //use fixture to access the HTML (e.g. get h1 element)        const de = fixture.debugElement.query(By.css('h1'));        const el = de.nativeElement;        expect(el.textContent).toContain('Mazda');    });});MockingIn unit testing, we are only interested in testing the class itself and try to isolate it as much as possible. We also want to be able to easily control the output of all dependencies of our class, such as services.spyOnOne way to do so is by creating spies for all calls to functions of those dependencies. That’s where the spyOn function comes into play:describe('AppComponent', () =&gt; {    let component: RequestPopupContainer;    let fixture: ComponentFixture&lt;AppComponent&gt;;            beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [CarBrandService],            imports: [CommonLogicModule]        })        .compileComponents();    }));    beforeEach(() =&gt; {        fixture = TestBed.createComponent(AppComponent);        component = fixture.componentInstance;        const carBrandService = fixture.debugElement.injector.get(CarBrandService);        spyOn(carBrandService, 'findAll').and.returnValue(Observable.of([            { name: 'Mazda', country: 'Japan' },            { name: 'BMW', country: 'Germany' }        ]));            fixture.detectChanges();    });    ...});In the example above, you can see when the AppComponent would call carBrandService.findAll(), instead of making a HTTP call, an Observable is returned with a list of car brands which is defined in the test itself. This is pretty cool, but also very error prone. If you forget to place a spy on a certain function, it will perform the actual call, possibly a HTTP call.That’s something we do not want at all.Mock classesTo prevent forgetting to spy on a certain function, you could create mock classes and inject them instead of the actual classes:class MockCarBrandService {    findAll(): Observable&lt;CarBrand[]&gt; {        return Observable.of([            { name: 'Mazda', country: 'Japan' },            { name: 'BMW', country: 'Germany' }        ]);         }   }describe('AppComponent', () =&gt; {    let component: AppComponent;    let fixture: ComponentFixture&lt;AppComponent&gt;;            beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [{provide: CarBrandService, useClass: MockCarBrandService}],            imports: [CommonLogicModule]        })        .compileComponents();    }));    ...});Again we see that findAll() will return an Observable containing a list. By using this approach, you’ll get an error when you forgot to define a function in the mock class. This may solve our previous problem, but now we have created another one. Karma allows us to assert whether a function was called using toHaveBeenCalled and toHaveBeenCalledWith. The problem here is that we don’t have any spies, so those functions can’t be used.We can again add spies like in the first approach, but you can imagine that this is a lot of work and will get quite messy.Jasmine spy objectsSo, the first two approaches have some issues. Luckily there’s a better way, Jasmine spy objects:describe('AppComponent', () =&gt; {            let component: AppComponent;    let fixture: ComponentFixture&lt;AppComponent&gt;;     const mockCarBrandService = jasmine.createSpyObj('carBrandService', ['findAll']);    mockCarBrandService.findAll.and.returnValue(Observable.of([        { name: 'Mazda', country: 'Japan' },        { name: 'BMW', country: 'Germany' }    ]);    beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [{provide: CarBrandService, useValue: mockCarBrandService}],            imports: [CommonLogicModule]        })        .compileComponents();    }));    ...});The first argument of jasmine.createSpyObj is the name for the object and will be used to mention it in the console.This is usually the name you gave the instance of the corresponding class in the constructor. The second argument is an array containing all function names of that corresponding class that are called from the class being tested. In other words, not all functions offered by the class that’s being mocked have be listed, only the ones actually being used.  Also note that in the providers list, we have to use useValue instead of useClass since jasmine.createSpyObj already returns an instance.Using spyOn isn’t needed, a spy object is already being spied upon (hence the name) and you can call the toHaveBeenCalled and toHaveBeenCalledWith functions on it....it('should call the findAll method' () =&gt; {    component.getFavoriteCarBrand();    expect(mockCarBrandService.findAll).toHaveBeenCalled();}); ...I think it’s obvious to say that using Jasmine spy objects is the way to go. If you forget to define a function, you’ll get an error when it’s called. The functions that are defined, are also spied upon. So all the problems with the first and second approach are solved. There’s even another benefit when using spy objects. The implementation (returnValue or callFake) can be changed at any time, even in the middle of a test!Issues with unit testingA side effect of using TestBed is that when the component is loaded, the ngOnInit, ngAfterViewInit… lifecycle events are called automatically.This means you have less control over them.Getting all the imports, providers and declarations setup can be quite a struggle too. If there’s any subcomponent in the HTML of the component you’re testing, they should either be imported through a module or added in the declarations of the TestBed configuration.If you don’t feel like doing all that, you can also tell Angular to skip elements it doesn’t recognise by adding NO_ERRORS_SCHEMA to the TestBed configuration:TestBed.configureTestingModule({    declarations: [ AppComponent ],    schemas: [ NO_ERRORS_SCHEMA ]})It’s very likely that you’ll be using the Angular router in some of your components, so you’ll have to account for that too. You could mock the router dependency using a Jasmine spy object or you can add RouterTestingModule as an import instead of the RouterModule itself. The routes that are relevant can then be defined in the RouterTestingModule:imports: [RouterTestingModule.withRoutes([/*List mock routes here*/])]  To learn more about writing tests using Angular TestBed, I recommend reading this guide: https://angular.io/guide/testing.2. Calling the constructorA much better way to do unit testing is to simply call the constructor of the class you want to test.You should get an instance of each dependency that’s needed in the component’s constructor.Of course we want to mock these classes and as we saw in the Angular TestBed section, the Jasmine spy objects are the way to go.describe('AppComponent', () =&gt; {            let component: AppComponent;    const mockCarBrandService = jasmine.createSpyObj('carBrandService', ['findAll']);    mockCarBrandService.findAll.and.returnValue(Observable.of([        { name: 'Mazda', country: 'Japan' },        { name: 'BMW', country: 'Germany' }    ]);    beforeEach(() =&gt; {        component = new AppComponent(mockCarBrandService);    });    ...});Without the TestBed, you don’t have access to the view. However, your tests will run much faster as there are less things to load. When using TestBed, you’ll probably be including lots of dependencies just to make it work, giving you less control. This is something you do not want in unit testing as you want to isolate the class as much as possible. Another difference with TestBed is that you have to call the lifecycle events yourself, again giving you more control over the code you’re testing.it('should find the car brand', () =&gt; {    component.ngOnInit();    const carBrand = component.getFavoriteCarBrand();    expect(carBrand.name).toEqual('Mazda');}); Async, fakeAsync, tickAngular is full of Observables and writing tests for them is a little trickier. You might also be using the setTimeout and setInterval functions. To cope with all that, Angular provides the async and fakeAsync functions. You can simply wrap your test in an async and it should only finish after all async calls are finished. If you want to have more control, you can wrap the test in a fakeAsync instead. Then the tick() function can be called to advance time with one tick. By passing an argument to it, time can be advanced by more ticks at once: tick(500).Suppose we have this class:export class TimeoutExample {    counter = 0;    updateCounterWithDelay() {        setTimeout(() =&gt; {            this.counter++;        }, 100);    }}And this test:  it('should increase the counter with a delay', fakeAsync(() =&gt; {    const component = new TimeoutExample();    expect(component.counter).toBe(0);    component.updateCounterWithDelay();    tick();    expect(component.counter).toBe(0);    tick(10);    expect(component.counter).toBe(0);    tick(90);    expect(component.counter).toBe(1);}));It clearly shows how the tick function manipulates the advancement of time, although it isn’t really a useful test,ObservablesNow, what if you want to test a function that returns an Observable? Well, simply subscribe to it in an async block and check the result!it('should return a list of cars' async(() =&gt; {    service.findAll().take(1).subscribe(        (result) =&gt; {            expect(result.length).toBe(9);        },        (error) =&gt; {            expect(true).toBeFalsy();        }    );}));The error clause may seem strange. However, what if the findAll() call returns an error and you don’t have the error clause in your test? You’ll simply think that your test has passed because it appears green in the console. With code coverage enabled, you may notice that the part of the code you were testing isn’t marked as covered. By adding expect(true).toBeFalsy(); to the error clause, your test will fail because it shouldn’t get there!What to testNow that we know a little on how to test, let’s have a look at what to test.For starters, you don’t have access to private and protected variables/functions, so all you can do is test the public ones. All variables that are accessed by the view should be public, so those are the ones you can use for your tests. The constructor and all lifecycle events can be called as well as they are public. You should never ever set a variable or function to public in order to test it. If you can’t test it because it’s private, you’re doing something wrong. You should be able to get to it through other functions.Generally, you give an input and assert the output, it’s as simple as that. Your different inputs should also make sure that all branches are tested (e.g. an if gives you two branches, one where the if resolves to true and one to false).Unit tests in Karma also allow you to assert whether a function has been called and optionally with which parameters (toHaveBeenCalled and toHaveBeenCalledWith). This can be useful when for example testing a void function that calls a mocked function. That way you can still assert the output. So, think of possible scenarios for the functions to test, provide the input and assert the output using expect. Also try to cover other paths than just the happy paths!Testing getters and setters usually isn’t needed, unless they are more complex. In most cases they’re not and it’s quite pointless to call a setter and then assert whether it has been set correclty. Most of the time, these will be called indirectly when testing other functions.The code coverage report can help you find functions that aren’t fully tested yet. However, your goal shouldn’t be to get a 100% coverage. Getting a 100% isn’t that hard, simply calling all functions with some different inputs will get you there.It won’t mean that your code is fully tested.To give you an example, suppose you have a function that sorts a list.You write some tests with different inputs so all branches are covered and you get a 100% coverage.The ordering of the list could still be completely wrong and not what you expect, although it’s fully covered.By using expect to verify that the output is correct, you’ll be doing a way better job.Even then there may be scenarios that aren’t tested despite the coverage report stating that part of the code is covered.So try to think of the various possible scenarios (both success and error scenarios) and translate those to tests.Tips &amp; tricksOnly run certain testsWhen your test base begins to grow, you don’t always want to wait for all tests to have run when only testing a certain class or function. Therefore, you can choose to only run specific describe blocks or tests (it) by adding an f (which stands for focus) in front of them, such as fdescribe and fit. To exclude certain describe blocks or tests, you can prefix them with an x (exclude), like xdescribe and xit. This will certainly come of use.Nesting describe blocksDescribe blocks can also be nested. If you want for example different beforeEach blocks for your tests when testing a class, you can create a nested describe block for each case.describe('AppComponent', () =&gt; {    let mockCarBrandService = jasmine.createSpyObj('carBrandService', ['findAll']);    describe('Happy path', () =&gt; {        beforeEach(() =&gt; {            mockCarBrandService.findAll.and.returnValue(Observable.of([                { name: 'Mazda', country: 'Japan' },                { name: 'BMW', country: 'Germany' }            ]));        });        it(...);        ...    });        describe('Error path', () =&gt; {        mockCarBrandService.findAll.and.returnValue(Observable.throw('Error'));        it(...);        ...    });});Using the injectorDependency injection is used all over Angular meaning that it isn’t possible to simply call new for certain classes. Normally, you simply put the dependencies in the constructor of your class and Angular takes care of the rest (e.g. constructor(private formBuilder: FormBuilder)).When calling the constructor of a class in a test, you don’t always want to mock those dependencies, so you’ll need to get instances of them somehow. For example when using Angular’s FormBuilder or when you need it to create a FormGroup to use in your test. In that case, you can use Angular’s ReflectiveInjector which takes care of getting an instance for you.Here’s an example how:const injector = ReflectiveInjector.resolveAndCreate([FormBuilder]);const formBuilder = injector.get(FormBuilder);As you can see, you can simply pass the class name and it will return an instance of that class. That instance can then be passed in the constuctor of the class you’re testing.ConclusionWhen writing unit tests, it’s better to call the constructors direcly and not to use Angular TestBed. It will give you more freedom and more control, run the tests much faster and allow you to completely isolate classes. You should also write integration tests and TestBed will serve that purpose very well.To mock classes, Jasmine spy objects are simply the way to go.Changing their implementation or return value is easy and can be done at any time!Code coverage reports can be very useful to find parts of uncovered code. However, getting a high percentage of code coverage shouldn’t be your goal. Write useful tests and also, don’t limit your tests to the happy path!"
      },
    
      "spring-2017-10-04-spring-cloud-stream-rick-and-morty-adventure-html": {
        "title": "Spring Cloud Stream - A New Rick and Morty Adventure",
        "url": "/spring/2017/10/04/Spring-Cloud-Stream-Rick-And-Morty-Adventure.html",
        "image": "/img/spring-cloud-stream.png",
        "date": "04 Oct 2017",
        "category": "post, blog post, blog",
        "content": "IntroductionOne of the most interesting aspects of the Spring Framework and its ecosystem is abstraction.The Spring project maintainers and contributors have always succeeded in hiding complexity from the application developer, by adding different layers of abstraction.For example, the way a Spring Bean of a certain Interface can be autowired and how Spring will find a suitable implementation class at runtime, is a very obvious example of the Liskov Substitution Principle or how to abstract away implementation from specification.A second, higher level example is the Spring Data project which offers a common programming model for SQL as well as NoSQL databases, abstracting away the specifics of the database internals.Another great example of abstraction is the one I’ll be discussing in this blog post.DISCLAIMER: a big part of this blog post will explain how Spring Cloud Stream works by using heavy references to the animated series Rick and Morty, with the intention to be hilarious and informative at the same time.If you don’t know the show or have no sense of humor, this blog post will be informative only 😉I will ignore the obvious third option: this blog post might not be funny at all.Spring Cloud StreamI just can’t start explaining something without a definition, that would be cruel and irresponsible:  Spring Cloud Stream is a framework for building message-driven microservicesIt provides an opinionated configuration of message brokers, introducing the concepts of persistent pub/sub semantics, consumer groups and partitions across several middleware vendorsThe last part is what I like the most.Spring Cloud Stream abstracts away the complexity of connecting to different middleware solutions.It does this the Spring Boot way: by automatically configuring sensible defaults and allowing the developer to adapt the parts he wants.It might be surprising, but Spring Cloud Stream is not that new in the Spring Cloud ecosystem.The project was called spring-bus during its prototype phase and the first real commit was on May 28th 2015.Dave Syer performed the commit that changed it to its current name on July 8th 2015, so I will call that the birth of Spring Cloud Stream!The most active contributor up until now is probably Marius Bogoevici.Questions about the project can be directed to the most active contributors and community in the Spring Cloud Stream Gitter channel.Application ModelAs is described in the very detailed documentation, the following image details how a typical Spring Cloud Stream application is structured:An application defines Input and Output channels which are injected by Spring Cloud Stream at runtime.Through the use of so-called Binder implementations, the system connects these channels to external brokers.So once again, the difficult parts are abstracted away by Spring, leaving it up to the developer to simply define the inputs and outputs of the application.How messages are being transformed, directed, transported, received and ingested are all up to the binder implementations.Binder ImplementationsCurrently, there are two official Binder implementations supported by Spring, for RabbitMQ and Kafka.Next to those, there are several community binder implementations available:The current - non-exhaustive - list:  JMS (ActiveMQ, HornetQ, IBM MQ,…)  AWS Kinesis  Google Cloud Pub Sub  Redis  GemfireRick and MortyAs I have said earlier in the post, I will explain Spring Cloud Stream using a somewhat different approach, but I feel it helps to capture the power of the project.Behold, our first character appears on stage:RickThis is Rick Sanchez.He is Morty’s grandfather, a genius mastermind, inventor of inter-dimensional travel, the Microverse, a butter-passing robot and much, much more.He is also an asshole.Rick’s ObsessionIn the first episode of Season 3, Rick expressed his obsession with the 1998 Mulan Szechuan Sauce.The saying goes that a picture is worth a thousand words, so that means this video below will explain, like, a bajillion words or something:So now we know that Rick really wants this Szechuan sauce.Now, we have a purpose:  We will create a Spring Cloud Stream application, called Rick, which sole purpose is to retrieve Szechuan sauce from McDonalds!As with every Spring based application these days, it’s as easy as going to the happiest place on earth (next to production): https://start.spring.io.As our dependencies, we pick Spring Web MVC to create some handy web endpoints and Stream Rabbit since we want to send our messages over a RabbitMQ broker.We end up with the following dependencies:&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;So how does a basic Spring Cloud Stream application look like? Well, it’s actually not that different from a regular Spring Boot application:@SpringBootApplication@EnableBinding({ InputChannels.class, OutputChannels.class })public class RickApplication {\tpublic static void main(String[] args) {\t\tSpringApplication.run(RickApplication.class, args);\t}}Looks pretty familiar, doesn’t it? That’s because the only new thing in the snippet above is the @EnableBinding annotation, which automagically converts your application into a full-fledged messaging beast!The InputChannels and OutputChannels interfaces are specific to my application.Very simply explained, we can describe the Rick microservice with the following diagram:As you can see, we have defined one input channel called rick and one output channel called microverse.These are implemented in a Spring Cloud Stream application like this:public interface InputChannels {\t@Input\tSubscribableChannel rick();}public interface OutputChannels {\t@Output\tMessageChannel microverse();}  Holy sh*t Rick, this almost seems like it’s too easy!Well Morty, erm dear reader, that’s because it is!Didn’t I tell you that Spring is awesome at abstraction?Yeah, this is why.The only thing that is left for us to do, is write our “business logic”, or in our case: the part where we try to find our beloved Szechuan sauce!Since Rick is very lazy and an arrogant genius, he’s not gonna look for the sauce himself.I mean, he’s got adventures to go on, inventions to invent and generally be a pain in the ass of the Galactic Federation.Let’s add another output channel to our interface:public interface OutputChannels {\t@Output\tMessageChannel meeseeks();\t@Output\tMessageChannel microverse();}Meeseeks?! What the hell is a meeseeks?Patience my dear reader, all will be explained shortly.First, let me show you the evil, brilliant piece of code which is gonna get us the Szechuan sauce:@Componentpublic class SzechuanSauceFinder {    private static final String C_137 = \"C-137\";    private static final int minimumRequestIntervalInMillis = 50;    private static boolean SEARCHING = false;\tvoid findThatSauce() throws InterruptedException {\t\tif (!SEARCHING) {\t\t\tSEARCHING = true;\t\t\tint requestIntervalInMillis = 5000;\t\t\twhile (SEARCHING) {\t\t\t\tthis.outputChannels.meeseeks().send(buildMessage(I_WANT_MY_SZECHUAN_SAUCE, C_137));\t\t\t\tThread.sleep(requestIntervalInMillis);\t\t\t\trequestIntervalInMillis = Math.max(minimumRequestIntervalInMillis, requestIntervalInMillis - 200);\t\t\t}\t\t\tSEARCHING = false;\t\t}\t}\tvoid stopSearching() {\t\tSEARCHING = false;\t}}Isn’t that some of the most evil code you’ve ever seen?Nothing more evil than static variables controlling state of an application, or precisely placed Thread.sleep() commands.Okay, we’ve got a messaging microservice, pumping out messages at an increasing rate (up until 20 per second).How will we know if our meeseeks, whatever that is, has found the szechuan sauce?The rest of the code in this class will illustrate how an input channel can handle incoming messages:@Autowiredpublic SzechuanSauceFinder(InputChannels inputChannels, OutputChannels outputChannels) {    this.outputChannels = outputChannels;    inputChannels.rick().subscribe((message -&gt; {        GlipGlop glipGlop = (GlipGlop) message.getPayload();        if (glipGlop.getQuote() == ALL_DONE) {            stopSearching();            this.outputChannels.microverse().send(buildMessage(WUBBA_LUBBA_DUB_DUB, C_137));        }    }));}private Message&lt;?&gt; buildMessage(RickAndMortyQuote quote, String instanceId) {    return MessageBuilder.withPayload(new GlipGlop(quote, instanceId)).build();}Since the rick input channel is a SubscribableChannel, we can subscribe to it.Well duh Sherlock!A message can be of any type but we do need to cast it to our own format, a GlipGlop, but Spring has ways to make this easier for us.We could have created a method annotated with the new @StreamListener annotation, which would look like this:@StreamListener(InputChannels.RICK)public void handle(GlipGlop glipGlop) {    ...}Alright, so now we know what Rick wants, and how he intends to get it, we move to the next piece of the puzzle:Mr MeeseeksRick is such a genius, he invented a box that can spawn as many “Aladdin’s genies” as you want.Use with caution though, you have been warned!Meeseeks are creatures created to serve a singular purpose for which they will go to any length to fulfill:Finding the sauceSo our next task will be to create a Mr Meeseeks microservice.If I were to draw a very simple diagram of this application, it would look something like this:Same story as with the Rick microservice.We need a very simple Spring Cloud Stream application with one input channel called meeseeks.In this case, we want to send GlipGlops to McDonalds, Rick and the Microverse, so we’re gonna need three output channels.The only thing we really need to put some effort in - if you can even call it effort, I’ve had more effort tying my velcro shoes the other day - is the business logic:@Componentpublic class MrMeeseekRoutine {\tprivate final OutputChannels outputChannels;\t@Value(\"${INSTANCE_INDEX:${CF_INSTANCE_INDEX:0}}\")\tprivate String instanceId;\t@Autowired\tpublic MrMeeseekRoutine(InputChannels inputChannels, OutputChannels outputChannels) {\t\tthis.outputChannels = outputChannels;\t\tinputChannels.meeseeks().subscribe(message -&gt; {\t\t\tGlipGlop glipGlop = (GlipGlop) message.getPayload();\t\t\tif (glipGlop.getQuote() == I_WANT_MY_SZECHUAN_SAUCE) {\t\t\t\tthis.outputChannels.microverse().send(MessageBuilder\t\t\t\t\t.withPayload(new GlipGlop(RickAndMortyQuote.OOOH_YEAH_CAN_DO, instanceId))\t\t\t\t\t.build());\t\t\t\tthis.outputChannels.mcdonalds().send(MessageBuilder\t\t\t\t\t.withPayload(new GlipGlop(RickAndMortyQuote.PLEASE_GIVE_ME_SOME_SZECHUAN_SAUCE, instanceId))\t\t\t\t\t.build());\t\t\t} else if (glipGlop.getQuote() == YOU_ARE_A_WINNER) {\t\t\t\tthis.outputChannels.rick().send(\t\t\t\t\tMessageBuilder.withPayload(new GlipGlop(RickAndMortyQuote.ALL_DONE, instanceId)).build());\t\t\t}\t\t});\t}}It’s getting quite boring already, this is child’s play.What I’m obviously doing here, is:  subscribe to the meeseeks input channel  fetch the incoming GlipGlop  if its from Rick, comply and send a GlipGlop to the McDonalds channel requesting some Szechuan sauce  if its from McDonalds and a confirmation that we have just won some sauce, we let Rick know our task has been fulfilledLet’s see what our McDonalds microservice looks like.McDonalds: where the sauce isMore recently, to my surprise, McDonalds announced they were actually bringing back the now infamous 1998 Mulan Szechuan Sauce.So I guess my demo just got a bit more relevant and my powers of clairvoyance are proven once again.At this point, it’s just more of the same. Let me show you the diagram:  Spring Cloud Stream application  one input channel mcdonalds  one output channel meeseeksYou get it by now.Here’s the code yawn:@Componentpublic class McdonaldsCashier {\tprivate static final int ODDS_AT_FINDING_SZECHUAN_SAUCE = 500;\tprivate static final Random RAND = new Random();\t@Value(\"${INSTANCE_INDEX:${CF_INSTANCE_INDEX:0}}\")\tprivate String instanceId;\tprivate int luckyNumber;\t@Autowired\tpublic McdonaldsCashier(InputChannels inputChannels, OutputChannels outputChannels) {\t\tthis.luckyNumber = RAND.nextInt(ODDS_AT_FINDING_SZECHUAN_SAUCE);\t\tinputChannels.mcdonalds().subscribe(message -&gt; {\t\t\tGlipGlop glipGlop = (GlipGlop) message.getPayload();\t\t\tif (glipGlop.getQuote() == RickAndMortyQuote.PLEASE_GIVE_ME_SOME_SZECHUAN_SAUCE) {\t\t\t\tint randomInt = RAND.nextInt(ODDS_AT_FINDING_SZECHUAN_SAUCE);\t\t\t\tif (randomInt == luckyNumber) {\t\t\t\t\toutputChannels.meeseeks().send(\t\t\t\t\t\tMessageBuilder.withPayload(new GlipGlop(RickAndMortyQuote.YOU_ARE_A_WINNER, instanceId))\t\t\t\t\t\t\t.build());\t\t\t\t} else {\t\t\t\t\toutputChannels.meeseeks().send(\t\t\t\t\t\tMessageBuilder.withPayload(new GlipGlop(RickAndMortyQuote.SORRY_NO_LUCK, instanceId)).build());\t\t\t\t}\t\t\t}\t\t});\t}}Oh man, I think I’m getting bored even writing this.Stick with me, the demo is gonna be worth it.Don’t scroll to the bottom just yet!There’s only one piece of the puzzle left.Morty  “Aw djeez” - MortyMorty is a young teenage boy.He has short brown hair that he wears straight and neatly combed around his head.He wears a yellow shirt, blue pants, and white shoes.He’s cute and adorable and is always along for the ride.He gets to see all the incredible things that happen in the universe - and microverse - so he’s the perfect character to represent our frontend.I just want to clarify that I’m in no way a great frontend developer.I dabble in HTML, CSS and the occasional JavaScript, but my designer skills are abysmal.That’s why I love a framework like Bootstrap: easy, intuitive and fast to create a semi decent web application.So that’s why I choose to work with Bulma: the even easier, more intuitive version of Bootstrap.You can check out my horrible frontend code in the Git repository.The Morty microservice is a bit different than the others, since it needs to collect all the input messages and transfer them to a browser.We do this using server-sent events or SSE.Spring MVC has had support for SSE for a while and it’s actually very easy to use:@GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)public SseEmitter events() {    SseEmitter emitter = new SseEmitter();    return emitter;}As you can see, even Jerry could figure this stuff out.In the example above, nothing is actually being emitted.When someone browses to the endpoint, it opens an HTTP connection and waits for messages.It’s up to the server to actually start sending data messages from this emitter, which will trigger an onMessage JavaScript event at client-side.Let’s see how we implemented this for our Morty microservice:@Slf4j@RestController@RequestMapping(\"/events\")public class EventController {\tprivate final List&lt;SseEmitter&gt; emitters = new ArrayList&lt;&gt;();\t@Autowired\tpublic EventController(InputChannels inputChannels) {\t\tGlipGlopHandler glipGlopHandler = new GlipGlopHandler();\t\tinputChannels.rick().subscribe(glipGlopHandler);\t\tinputChannels.meeseeks().subscribe(glipGlopHandler);\t\tinputChannels.mcdonalds().subscribe(glipGlopHandler);\t\tinputChannels.microverse().subscribe(glipGlopHandler);\t}\t@GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)\tpublic SseEmitter events() {\t\tSseEmitter emitter = new SseEmitter();\t\temitters.add(emitter);\t\temitter.onCompletion(() -&gt; emitters.remove(emitter));\t\temitter.onError(throwable -&gt; emitters.remove(emitter));\t\temitter.onTimeout(() -&gt; emitters.remove(emitter));\t\treturn emitter;\t}\tclass GlipGlopHandler implements MessageHandler {\t\t@Override public void handleMessage(Message&lt;?&gt; m) throws MessagingException {\t\t\tGlipGlop glipGlop = (GlipGlop) m.getPayload();\t\t\temitters.forEach(emitter -&gt; {\t\t\t\ttry {\t\t\t\t\temitter.send(glipGlop);\t\t\t\t} catch (IOException e) {\t\t\t\t\temitter.complete();\t\t\t\t\temitters.remove(emitter);\t\t\t\t\tlog.error(\"IOException when trying to send event\");\t\t\t\t}\t\t\t});\t\t}\t}}DISCLAIMER: this code is not production-ready and can probably cause instant brain damage when observed.This code is for demo purposes only.A quick explanation of the code:  we subscribe to the four input channels and attach the same message handler since we want to handle all the GlipGlops equally  when a client performs a GET request to the /events endpoint, it is assigned an SseEmitter which is added to a list  whenever a GlipGlop on any of the four input channels is received, it is sent to all the registered SseEmitters  exactly nothing is done when errors occur - totally intentionalThe MicroverseEverything that is described in this post, is transpiring inside the miniature dimension called The Microverse:the MicroverseIn all seriousness - yeah, seriously - we are deploying our microservices on the Pivotal Cloud Foundry (PCF) platform.In this case, I’m using a paid account on Pivotal Web Services, their online version of PCF.Inside this powerful Platform as a Service (PaaS) offering, there’s this concept of organizations and spaces.Inside of our Ordina JWorks organization, I have created a space called microverse to house all of the applications in my demo.This way, my wacky adventures cannot interfere with any of our actually useful applications.Through the powerful service broker mechanism, I provisioned a RabbitMQ service and bound it to my applications.This means the freshly created RabbitMQ instance’s connection details are automatically shared inside of my application’s containers as system properties.Since Spring Boot kicks ass at taking system properties and ramming them inside some auto-configuration, we don’t have to worry about anything remotely resembling boilerplate code.SummaryBefore I go over to the demo, I wanted to share my grand clarification of the Microverse and all things which lie within:As you can see, I have drawn multiple Meeseeks instances in this diagram.That’s because I want to spawn multiple Meeseeks to perform my task.Without any extra configuration, every Meeseeks instance will pick up every GlipGlop posted to the meeseeks input channel.This means adding additional Meeseeks instances won’t help us very much (it will increase the total number of GlipGlops in the system and probably overload the server even faster).We want every separate Meeseeks instance to pick up a unique message on that input channel.This can be accomplished by putting the Meeseeks application inside of a consumer group.Only one property is required to do this:spring:  cloud:    stream:      bindings:        meeseeks:          group: szechuan-finderThis indicates we want the meeseeks message channel to be part of a consumer group called szechuan-finder.DemoThis could be quite anti-climactic, but you’re gonna have to touch Pickle Rick to see the demo.Go on… Touch him…    Press the Rick and Meeseeks image in the demo and enjoy the show!Resources  Github Repository with all the code: https://github.com/Turbots/szechuan  Slides about this topic: http://slides.com/turbots/spring-cloud-stream-rick-morty  Spring Cloud Stream documentation: https://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/  My presentation on our yearly JOIN event: https://www.youtube.com/watch?v=Nl9OIuNRYwI  Try out Pivotal Cloud Foundry on your local workstation: https://pivotal.io/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/introductionImprovements  Better error handling on Morty - too many browser connections when sending 150 messages a second over event streams is quite demanding apparently  Addition of Spring Cloud Data Flow in the mix - registering the applications and dragging around inputs and outputs should be fun - also, scaling!  Improved UI - obviously  Complete event-based demo instead of endpoints to force certain operations (spawning/killing Meeseeks, waking up Rick, …)"
      },
    
      "iot-2017-09-28-end-to-end-iot-html": {
        "title": "Building end-to-end IoT demos with LoRa",
        "url": "/iot/2017/09/28/End-to-end-IoT.html",
        "image": "/img/end-to-end-iot/booze-5.jpg",
        "date": "28 Sep 2017",
        "category": "post, blog post, blog",
        "content": "  To showcase end-to-end LoRa applications we built simple yet fun, real world demo applications. These applications show a full end-to-end implementation of the LoRa technology leveraging the Proximus MyThings Internet of Things platform.    The Booze-o-meter V2 at Devoxx Belgium 2016.Building end-to-end LoRa Iot SolutionsBuilding an enterprise IoT solution is challenging. Devices need to be enrolled, monitored and maintained.You can roll your own network and handle all of this yourself, this however will require quite the backend system to facilitate all of this.The Proximus LoRa network in combination with their MyThings platform takes away most of this and allows us to focus on the actual applications.Technologies overviewFor our rapid prototypes and small to medium applications we have chosen the following technical stack:  Proximus LoRa network for LoRa connectivity  Proximus MyThings platform for device management  NodeJS with TypeScript on the backend  Angular on the frontend (The older versions are still on AngularJS)We will look into each item in full detail below:  1. LoRaLoRa, short for LoRaWAN is a LPWAN (Low Power Wide Area Network) is meant for wireless battery powered devices or ‘things’.It offers a low power, low bandwidth secure network to transceive information across large distances. The network is laid out in star topology and can easily be extended by placing more base stations also called LoRa gateways.Some network parameters:  Range of 5 to 15 kilometers (3,1 to 9,3 miles) depending on the conditions and signal strength.  Data rate of 0,3 kbps to 50 kbpsMore detailed information and the full specifications can be found on the LoRa Alliance website.  2. Proximus MyThingsProximus MyThings is a LoRa device onboarding and management platform. It is used to enroll devices and sensors, to map their data to specific endpoints and provide tools for device management.The platform consists of three main parts:  MyThings Builder: Charts and sensor values (containers)  Mythings Manager: Online device onboarding and user management  MyThings Scanner: Offline (in the field) device onboarding  3. Node.js &amp; TypeScriptMost people should be familiar with Node.It is the JavaScript runtime built upon the V8 engine that Google Chrome uses.It is a lightweight and efficient runtime that uses an event-driven, non-blocking I/O model.This combined with the added type safety that TypeScript provides makes this an excellent choice for rapid prototyping.Some of our own demo applications make use of the Node Simple Server (NSS) application, while others just use Express. This depends on the needs of the project.If you are interested in the NSS project, we have a blog post about it here and it is on GitHub too!  4. AngularLike with Node, most people should be familiar with Angular (or the older AngularJS).Angular is a development platform for building modern single page web applications.It is a complete rewrite of the older AngularJS and therefor has some big changes in how things work.Angular is easy to set up and use, it also is fully cross platform/browser compatible.Our demo applicationsAll our demo applications are publicly available in the GitHub project of NSS.These demo applications are ever evolving as we are currently porting them from the older AngularJS to Angular with TypeScript.Our demo applications have been showcased and used at several events including internal Proximus events as well as conferences like Devoxx, Techorama and The Belgian IoT convention in Mechelen.Below we will go into detail about each application and how it came to be, as well as the iterations they went through.Aside from the Slotmachine and the Booze-o-meter we’ve also developed the Stairway to Health application for Proximus. A blog post about this will be available in October.For the impatient, the IoT talk at the annual JWorks JOIN event covered this topic already and can be viewed on YouTube.1. The SlotmachineThe Slotmachine application does mostly what its name suggests, but with a twist.The idea is simple:If required, the player registers him or herself in the application.A simple push button sends a signal to the backend application. The application dispatches an event via a websocket to the frontend application which turns the Slotmachine. The Slotmachine can either result in a win or a loss. A maximum of three attempts are possible per player, after which a new player registration is required to play again.The player registration can be disabled depending on the requirements of the event/conference.The light effects are also controlled by the application.If the user has registered the gentle fading switches to a running light effect and if the user wins, the effect changes to a carrousel of different colors.The effects are controlled the same way the button is controlled but in the opposite direction. The frontend application sends a websocket event to the backend application which controls the Arduino and the LEDs.      The Slotmachine V1 test setup.V1The first version was not LoRa enabled and used a push button and Arduino integration via Johnny-Five to allow interaction. This meant that an Arduino always needed to be connected to the server or laptop that was used as a server.    The Slotmachine V1 at Devoxx Belgium 2015.V2The second version of the Slotmachine application swapped out the Arduino and the required wired connection with a LoRa enabled push button.This allowed us to demonstrate the capabilities of the LoRa network in a fun and engaging way.The application remained unchanged for the user, and was adapted to be more configurable:Setting a win chance (up to 100%) and different images/styling for different events.2. The Booze-o-meterThe Booze-o-meter application is a drink dispenser that relays liquid fill level in the dispenser.It is a fun example to demonstrate how measuring the fill level of a container can be achieved.This idea can be applied to container in a whole range of different industries and use cases. From oil tanks to garbage cans and to containers.The application setup is extremely similar to the Slotmachine application. The sensors relay their data via the MyThings platform to our backend, which in turns dispatches an event on a websocket so the frontend application can display the change.      The Booze-o-meter V1 test setup with regular water.V1The first version of the Booze-o-meter used three sensors that can detect a liquid through a thin plastic container. This allowed us to represent the level in the container in a coarse way:  FULL (initial state)  HIGH (sensor)  MEDIUM (sensor)  LOW (sensor)The sensors have a simple binary readout, true if liquid is detected, false if not.This data gets represented on the frontend application as the four states as mentioned above.V2      The Booze-o-meter V2 at Devoxx Belgium 2016 with actual liquor!The second version of the Booze-o-meter application allowed us to get a more detailed reading of the remaining fluid level in the container thanks to the addition of an ultrasonic sensor.This sensor can measure the distance between itself and a surface, in this case the surface of the liquid in the container.The application was updated to support this more granular approach that is able to show the level in the container accurately to 1%.ConclusionOur demo applications have served us well in bringing across the idea of LoRa to customers and other interested developers. We will continue to evolve our demo applications by adding new features, technologies and keeping them up to date.Useful links &amp; further reading  LoRa Alliance  Proximus MyThings  Node Simple Server on GitHub  StairWay to Health JOIN Presentation  Angular  NodeJS  Express"
      },
    
      "microservices-2017-09-26-secure-your-architecture-part1-html": {
        "title": "Securing your cloud-native microservice architecture in Spring: part 1",
        "url": "/microservices/2017/09/26/Secure-your-architecture-part1.html",
        "image": "/img/microservices/part1/securitylogo.png",
        "date": "26 Sep 2017",
        "category": "post, blog post, blog",
        "content": "When developing cloud-native microservices, we need to think about securing the data that is being propagated from one service to another service and securing the data at rest. So how much security is enough to secure our architecture? Is it the user that identifies itself and decides what data he has access to?Overview  Our cloud-native architecture  Authentication &amp; Authorization Principle  Using the OAuth2 Protocol  Understanding JSON Web Tokens  Using a User Authentication &amp; Authorization Server  Securing your microservice  Securing data at restOur cloud-native architectureIn this blog series we will cover these questions and guide you in applying the security layer to your cloud-native blueprint.With this blueprint, we are going to use the Spring ecosystem throughout the series.Solving the following problems is crucial for building a cloud-native microservices architecture, but it should be technology-agnostic:  User Authentication &amp; Authorization Server: Spring Cloud Security OAuth2  Load Balancer &amp; Routing: Spring Cloud Zuul  Communication client: Spring Cloud Feign  Externalized Config: Spring Cloud Config Server  \t        Where our journey begins…When it comes to users interacting with our system, we want to verify that the person can identify him- or herself.Most of the time this appears in a login form where you enter your credentials, or in a login page from a third party application (Facebook, Google, etc).  \t          If you like more secure systems, you can add another level of complexity on top of it.Most commonly used is Two-factor-authentication, where the client will use an external provider (Google Authenticator for example) to issue a token for your registered application.AuthorizationAuthorization is the mechanism that uses the user’s data to verify what he is allowed to do.  For instance, who has access to which resources and what are his access rights (eg. read or write) to those resources.To use these two mechanisms in our system, we will be using a security protocol that fits our microservices architecture.Since we don’t want everyone to have an account for each (micro)service, we aim to have one single identity per person so that the user needs to authenticate only once.Using the OAuth2 ProtocolWhen searching for a security protocol, we don’t want to reinvent the wheel and look at what is supported by the Spring framework.Obviously, it depends on the use case of the applications that require resources from our system.Is it a third party application like Facebook or a first party like your own application? Or both? I will explain both OAuth2 and JSON Web Token and how they solve these requirements.The OAuth2 delegation protocol allows us to retrieve an access token from an identity provider and gain access to a microservice by passing the token with subsequent requests.When introducing the OAuth2 framework to our system, we will be using four grant types.These grant types are different ways to obtain an access token, some clients are more trusted than others.OAuth2 Grant TypesThird party applications: Authorization Code grant type and Implicit grant typeAuthorization Code is the most common used grant type for third party applications, where user’s confidentiality can be maintained.The user won’t have to share his credentials with the application that is requesting resources from our backend. This is a redirection-based flow, which means that the application must be capable of interacting with the user’s web browser.  The frontend (application) makes a request to the User Authentication &amp; Authorization server (UAA) on behalf of the user  The UAA server redirects to a permission window of a third party for the user to grant permission, the user authenticates and grants permission  The UAA server returns an authorization code with a redirect url  The frontend uses the authorization code and an application identification to request an access token from the UAA server  The UAA verifies the authorization code and returns an access token  Implicit grant type follows the same principle as the Authorization Code type but does not exchange an authorization code to issue an access token.First party applications: Password grant typeThis grant type is best used for first party applications,where the user is in a trust relationship with the application.The application authenticates on behalf of the user and receives the proper JWT.  The user provides his credentials to the frontend, commonly done with a login form  The frontend assembles a POST request with the credentials to the UAA server  The UAA validates the user and returns a valid JWTTrusted Service to Service communication: Client Credentials grant typeThe trusted service can request an access token using only its client-id and client-secret.When the client is requesting access to the protected resources under its control, it is very important that the client credentials grant type MUST only be used by confidential clients.  Zuul authenticates with his client-id and client-secret  The UAA validates the credentials and returns a valid JWTOAuth2 ScopesOAuth 2.0 scopes provide a way to limit the amount of access that is granted to an access token.If the scope is not defined, the client is not limited by scope.  An access token issued to a client can be granted READ or/and WRITE access to protected resources.If you enforce a WRITE scope to your API endpoint and it tries to call the endpoint with a token granted a READ scope, the call will failJSON Web Tokens (JWT)JSON Web Tokens (JWT) is a compact URL-safe means of representing claims to be transferred between two parties.The claims in a JWT are encoded as a JavaScript Object Notation (JSON) object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or MACed and/or encrypted.The suggested pronunciation of JWT is the same as the English word “jot”.The payload consists of some standard attributes (called claims), such as issuer, subject (the user’s identity), and expiration time.The specification allows these claims to be customized, allowing additional information to be passed along.Be careful when passing additional information, if you like to go deeper on this topic with a real use case, you can read Using JWT for State TransferJwt.io provides a quick way to decode your JWT.One of the challenges in a microservice-based architecture is identity propagation.After the authentication, the identity of the user needs to be propagated to the next microservice in a trusted way. JWT is used here to carry along information of the user.Based on a token, your microservice needs to be able to create a principal object. This principal object needs to contain all the necessary info so the system can decide whether or not the request should be executed or not.  \t        Dealing with timeWhen propagating the identity of the user, you don’t want it to last for a infinite amount of time. That’s why JWTs have an expiration time.When expired, the JWT will be invalid and the client needs to request a new JWT with the refresh token.These refresh tokens carry the needed information to issue a new JWT.Refresh tokens can also expire but are rather long-lived.JWTs have three fields that relate to time and expiry, all of which are optional.In most cases, you should include these fields and validate that the token:  is not expired (exp)  was created before the current time (iat)  should not be used before the current time (nbf)All of these times are expressed as UNIX epoch timestamps, and are best checked in the order as described above.Signed JWTsSigning a JWT helps establish trust between services, because it gives a recipient reason to believe that the message was created by a known sender and that the message was not altered in transit.JWTs are being signed by a public/private key pair.Almost all of the JWT libraries support signing. To check if yours supports it, visit JWT Libraries.For a deeper dive into signing JWT, check our tech post about Digitally signing your JSON Documents  The user requests a resource  The frontend assembles a request with an Authorization header and a Bearer token inside, fires off the request to Zuul  Zuul verifies the token in communication with the UAA server  If the token is valid, Zuul redirects the frontend to the correct resource on the proper microservice  The microservice checks for authorization to the resource, if access granted, the correct resource is returnedStatelessSince we are working with cloud-native applications, we can’t have any state within them.Because we have all the necessary information and create a new principal object for each request, the token eliminates the risk of having in-memory session state in the microservice.Using a User Authentication &amp; Authorization Server (UAA)The UAA server is an identity provider. It adds authentication to applications and secures services with minimum fuss.It’s primary role is that of an identity provider, issuing tokens for client applications to use when they act on behalf of users. It can also authenticate users with their credentials, and can act as an SSO service using those credentials.There are some options available as a UAA server:  Using a third party for issuing tokens (ex. GitHub, Facebook). Tutorial Github social login  Using KeyCloak, an open source solution aimed to make it easy to secure your application. Tutorial on how to use KeyCloak in Spring  Using Okta, a commercial OAuth2, SAML and general identity management service in the cloud.  Implementing your own UAA is not really best practice since other providers cover most of the use cases. Explanatory video of the UAA serverEnabling Single Sign-OnNow that we have a way to achieve Authentication and Authorization by applying OAuth2 and JWT, we still have one problem.Having multiple frontends in our architecture, the user will have to log in to each of these applications.With Single Sign-On (SSO) we can eradicate this problem just by using the existing user session and requesting an access token.Enable OAuth2 SSO flow on Zuul serviceThe @EnableOAuth2Sso  and @EnableZuulProxy annotation on our Zuul service will forward OAuth2 tokens to the services it is proxying.Sensitive HeadersZuul secures your sensitive headers by blocking these headers downstream (microservice).Since the default settings for sensitive headers blocks the Authorization header, we have to open this setting and send these headers downstream.You can choose to set the sensitive header per route or globally.How it works: Sensitive HeadersZuul FilterBased on Netflix’s Zuul, Spring’s implementation also brings a filter mechanism.Filters are capable of performing a range of actions during the routing of HTTP requests and responses.This can help you customize security on your incoming and outgoing traffic.Review the Zuul filter guide from Netflix about how filters work.Securing your microserviceWhen enabling security in your service, the most common issues are developer-induced.Either there is a lack of built-in or easy security controls, or we make trade-offs for functionality over security.Still, we have to think about who can access this functionality and what they can do with it.We got an access token, our gateway performed a coarse grained verification and proxied it to our microservice.We are in a ‘downstream service’, where data is being load-balanced from Zuul. The next questions are:  How do we decode this JWT?  How can we secure our code with the help of Spring Security?Assembling the PrincipalIt is the responsibility of a microservice (Resource Server) to extract information about the user from the access token.Decoding the token allows the extraction of the user’s information.With this information Spring Security will assemble a Principal object containing eg. the username and the user’s roles, and puts it in the security context. Using the security context the AccessDecisionManager will be able to make a decision whether or not the request should be performed.To enable this, we need to add spring security to our class path and add the @EnableResourceServer annotation to our application.Best practices with keysThe problem that might occur is that every microservice would need to connect with the UAA server for verification on every request.Zuul verificationObviously, we don’t want every microservice to depend on the UAA servers availability regardless of startup / testing / CI. The solution is to disable exposure of your microservices to the outer network and handle only incoming traffic via the gateway (eg. Zuul, HAProxy, nginx,…).Zuul will verify the token as a trustworthy client of the UAA server and will propagate the token to the downstream services.But what if a hacker gets inside of your platform?JSON Web KeysTo solve this issue, we need an extra validity check on the microservice.When verifying a token’s validity, it comes down to verifying if the token was issued by the UAA server.This can be done by requesting the public key used for signing the JWT. This is called a JWK or JSON Web Key.Basically, you can restrict the dependency on the UAA server to one single REST call, where the JWK is fetched from a public URI.Once a microservice has a cached JWK, it can be used to verify any JWT completely by itself.This greatly reduces network calls to the UAA server and still secures all of your microservices.When you want to rotate your private/public key pair, you can use JWKS. We will go deeper into detail in one of our next posts.Securing API endpointsAt last we’re going to secure our resources.Spring Security gives us a variety of tools to secure your application at class and method level.The one that’s used most often enables method security, which you enable by adding @EnableGlobalMethodSecurity(prePostEnabled = true) to your configuration.AuthorityFor the authorization, Spring Security provides us with authorities, extracted from the access token.The authorities are placed inside a Principal, which will be used throughout the existing security context of your application.You can then reference them using Spring Expression Language (SpEL) to secure your methods.There are plenty of options you can use for method security, but we’ll highlight the most common ones.You can find a complete list in the Spring documentation@PreAuthorizeMost commonly used, @PreAuthorize will decide whether a method can actually be invoked or not.  When a user logs in and you want the user to only access his detail information, or everyone’s data in case he’s an admin, you can use the @PreAuthorize annotation.@PreAuthorize(\"(authentication.principal.uuid == #uuid.toString()) or hasRole('ADMIN')\")User findByUuid(@Param(\"uuid\") UUID uuid); @PostAuthorizeLess commonly, you may wish to perform the access-control check after the method has been invoked.The returnObject is the returned value of that method.  A user can only view his own details and not those of someone else, but an administrator can.You validate this by checking if the user has the admin role or if the principal’s UUID is the same as the one of the returned user object.@PreAuthorize(\"hasAnyRole('ADMIN','USER')\")@PostAuthorize(\"returnObject!=null or hasRole('ADMIN') or returnObject.uuid.toString() == authentication.principal.uuid\")User findOne(@Param(\"uuid\") UUID uuid); Next stepIn the next post we will cover how to secure your data at rest.Sources  Spring Cloud Eureka  Spring Cloud Hystrix  Spring Cloud Zuul  Spring Cloud Feign  Spring Cloud Config Server  Spring Cloud Security OAuth2  Two-factor-authentication  OAuth2 Scopes  Josh Long UAA intro  Tutorial Github social login  KeyCloak  Tutorial on how to use KeyCloak in Spring  Okta  Spring OAuth2 developers guide  Sensitive Headers  Zuul Filters  Spring Expression Language  Authorities  JWT decoder  JWT Libraries  JWK or JSON Web Key  JWKS"
      },
    
      "conference-2017-09-18-browser-security-features-html": {
        "title": "Browser Security Features",
        "url": "/conference/2017/09/18/Browser-security-features.html",
        "image": "/img/security/padlock_code.jpg",
        "date": "18 Sep 2017",
        "category": "post, blog post, blog",
        "content": "Browser security featuresUpdate: due to the deprecation of HPKP, we’ve posted an update to this blog post.Browsers nowadays come with a ton of security features built-in.These features are there to protect the people using your application as well as protecting your application from malicious others.Most of these features are quite easy to implement, however for some of them (such as key-pinning) you have to be careful not to break your site.It’s this danger, combined with the lack of knowledge, that prevents people from taking full advantage of them.Table of contents  Transport Layer Security  HTTP Strict Transport Security  Public Key Pinning  Content Security Policy  Subresource Integrity  Cookie Protection  ConclusionTransport Layer SecurityThe first layer of defense is not a new one at all: Transport Layer Security (TLS).TLS is sometimes (incorrectly) referred to as SSL (Secured Socket Layer). In reality SSL is an obsolete technology, with TLS being its successor, but the name stuck.Having said this, why should you use TLS? First of all, most of the features described below only work when you’re on a secured connection.Besides that, it guarantees the end user that the site they’re communicating with is actually the site they think it is.It also provides the guarantee that the content they see was not tampered with while travelling over the network.Another thing TLS brings to the table is speed: it used to be true that a secure connection was slower than an unsecured one.Modern hardware however is more than up to the task of handling this efficiently for you.Besides that, HTTP/2 is only available over a secure connection and it allows for faster page loads.Have a look at HTTP vs HTTPS for a demo of the difference.Since speed should no longer prevent you from switching to HTTPS, there’s only cost.Even that is no longer true: a simple Domain Validation certificate can be obtained for free.But even if you need more protection, an Extended Validation certificate can be had for as little as $300 per year.How hard is it?The main issue is that all resources you use on your site should be served over HTTPS.This means that all third parties should use TLS as well.Furthermore, it depends on the complexity of your site.Nick Craver wrote an extensive blog post on their road to switching to HTTPS.Should you activate this on your site?Absolutely! Modern browsers are shifting from notifying users that a page is secure to warning them that it isn’t.On top of that, Google gives a slight ranking boost to HTTPS sites.HTTP Strict Transport SecurityOnce your server is properly configured to use TLS, your next step is to redirect your users to the secure version.You could do this by simply adding a redirect-rule in your web server for the non-secure pages.This means that users will still first connect to your non-secure site, allowing a potential attacker to intercept the request and do his nefarious deeds.Wouldn’t it be nice if you could tell the browser to just go straight to the secure version?That’s the thinking behind the HSTS (HTTP Strict Transport Security) header.HSTS simply tells the browser that you’re expecting it to use HTTPS for a certain time.As a result, the browser will automatically replace http:// with https:// before making the call.This means that even following a link that explicitly defines http:// will instead be called using a secure connection.The configuration of HSTS is as easy as can be: you simply add the following header to your response:Strict-Transport-Security: max-age=31536000; includeSubDomainsThis will tell the browser that for the next 365 days, it should connect to your domain using HTTPS.The includesubdomains directive tells the browser that your subdomains should also be called using https.Setting the max-age to 0 tells the browser that you no longer wish your domain to be HSTS-enabled.HSTS preloadOf course in this scenario, the user’s first connection will still take place over an unsecured connection.This would offer an attacker a brief period in which he can still hijack the connection.To prevent this, most major browsers (Chrome, Firefox, Safari, Edge, IE11 and Opera) offer an HSTS preload list.Domains on this list will automatically be loaded over HTTPS from the start, without having to go through the HTTP -&gt; HTTPS redirect.If you want your domain to be included in this list, you should add the preload directive to the HSTS header.Strict-Transport-Security: max-age=31536000; includeSubDomains; preloadAfterwards, you can register yourself for the HSTS Preload List.Are there any risks?Activating HSTS does offer some risks:  If you include the includesubdomains directive, you tell the browser that all subdomains need to be retrieved over HTTPS.If your internal applications are on a subdomain (e.g. internal.example.com), you’ll block access to those that haven’t enabled TLS yet.  Adding the preload directive is even more dangerous because this tells browser makers to hardcode your HSTS settings.If you’ve made a mistake in the setup, it can take a long time to be removed from the list. Since this list is in the browser, you’ll affect both your existing and your new users.These risks can be mitigated through extensive testing and conservative settings. Start with a short max-age and slowly increase its length, don’t include subdomains if you’re not 100% sure that all subdomains need to be includedand perhaps most importantly, don’t activate preload unless you’re 100% sure that everything works as intended.Should I activate HSTS?For those (subdomains) where TLS is enabled, you should start rolling out HSTS (while keeping in mind the warnings above).Public key pinningUpdate: HPKP is deprecated from Chrome 67, have a look at this post to see what next.Alright, now you’ve secured your site with HTTPS, and you’ve made sure your users can’t fall victim to a Man in the Middle attack.Or have you?It’s true that HSTS will make sure that the user only connects using a secured connection, but that doesn’t mean the HTTPS connection is actually made to your server.Over the last couple of years, there were several incidents where malicious actors were able to generate valid certificates for domains they didn’t control.When this happens, your users will think they’re safe (as their browser shows the green padlock), but the attacker can still manipulate your content.To protect yourself against this, there’s a mechanism called “HTTP Public Key Pinning” (HPKP [1]).With HPKP you “pin” the public key of your TLS certificate to the browser.In the future, that browser will compare the public key that’s actually used for the TLS connection, with the pinned one and, if they don’t match, refuse the connection altogether.An HPKP header looks like this:public-key-pins:pin-sha256=\"YLh1...uihg=\";pin-sha256=\"9dNi...Dwg=\";pin-sha256=\"Vjs...eWys=\";max-age=2592000;includeSubdomains;report-uri=\"report-uri\"The max-age directive tells the browser for how long these pins are valid. You can use report-uri to get a report when an invalid certificate is used.includesubdomains makes sure that the policy also applies to your subdomains.Finally, there are the actual pins.You need to pin at least 2 fingerprints: 1 that should be active at the moment and 1 that isn’t.What to pin?First of all, you need to pin at least one of the keys in your certificate chain.While you can pin the key of the actual certificate, that might not be the best idea.Doing this means that you need to update the keys every time your certificate is changed or you will risk your users being unable to visit your site.Alternatively, you could pin the key for the root certificate of your CA (Certificate Authority).While this is a lot safer, it does mean that if your CA, or any of its intermediates is compromised, they could issue valid certificates for your site.Finally, you have the option to pin the key to the intermediate certificate. Doing so limits the attack surface to that intermediate, while it also allows you to roll out new certificates whenever you need to.Of course you can’t control when your CA will change their intermediate certificate, so that’s a danger in its own.Besides that you also need to have a second key pinned that’s not in your current certificate chain, otherwise your HPKP header will be ignored.Fortunately, you don’t have to have certificates ready for this.It’s enough to pin the public key of a CSR (Certificate Signing Request).Obviously, you can’t use the CSR of your current certificate (as that would be valid for this chain), so you’ll need to create a backup CSR.You’ll need to keep this CSR and the associated private key in a secure location, because you don’t want these to be compromised together with the original.report-uriThe report-uri directive is used to tell the browser where to send reports if it encounters an invalid certificate.The browser will POST a JSON message to the URL you specify here.If you don’t want to implement your own processing of these reports, have a look at report-uri.ioIt will process the reports from your site and display the results in a nice format, allowing you to take action when you see something that’s wrong.Report-onlyBesides the normal HPKP header, there’s also the report-only variant: Public-Key-Pins-Report-Only.This header has the exact same specifications, but it won’t block access to your site if there’s no valid pin.As the name says, it will simply report violations to the report-uri.Obviously, this header isn’t meant to increase the security of your site on its own, rather it’s a way to help you on your way to a full HPKP implementation.DangersHPKP is quite a dangerous header: it’s quite easy to commit “pinning-suicide”.Pin the wrong certificate, have a CA change keys on you or have something else go wrong and your site is inaccessible until your users’ max-age expires.Be careful rolling out this one as it’s way too easy to shoot yourself in the foot.Should I use it?This header has some serious dangers associated with it.It’s not enough to know that the current configuration is correct, you also need to be sure that you’re equipped to deal with certificate updates without breaking the site.And then you need to be sure that you’ve got a backup in place in case you ever want to switch CAs.Unless you’re 100% sure that this won’t be an issue, hold off for now as it’s too easy to DoS your own site.Content Security PolicyEven though your connection is secured with TLS, that doesn’t mean that the content can’t be tampered with in other ways (such as cross-site scripting (XSS)).An attacker could use these kinds of attacks to load malicious content.The Content Security Policy (CSP) header is designed to prevent this kind of attacks.It allows you to specify exactly what content your site is allowed to load through a load of directives.Fetch directivesFirst of all, you can define what source content can be fetched from. There’s a specific directive for each resource type and a fallback directive default-src.You define the sources where the content can be loaded from as follows:  self only load content from the same origin as the page  none don’t load any content of this type  unsafe-inline lets you use inline javascript and CSS (although it’s preferable to use a nonce)  unsafe-eval allows the use of eval(), setTimeout(String), setInterval(String) and new Function(String). There’s a reason it has “unsafe” in its name though: these functions are typically used as attack vectors for XSS.  https: allow content loaded from anywhere, as long as it’s served over HTTPS  example.com allow content loaded from anywhere on example.com, both HTTP and HTTPS  You can also use wildcards to control which origins are allowed.  E.g. *://*.example.com:* will allow resources to be loaded from all subdomains of example.com, using any scheme and port. Note that it won’t allow you to load resources from example.com itself.  nonce-... Allows you to specify a nonce. Scripts or styles that have this nonce are then allowed to execute.It’s also important to note that you can pass multiple values to these directives: self https://example.com will allow resources to be loaded both from the domain itself as well as from https://example.com.You can use these to define default-src, but CSP gives you more fine-grained control over where each type of resource can be loaded from.For that you need to use the following properties instead:  script-src - Javascript  style-src - CSS  img-src - images  font-src - fonts  object-src - objects (e.g. &lt;object&gt;, &lt;embed&gt;, …)  media-src - media such as &lt;audio&gt; and &lt;video&gt; elements  connect-src - where the page can connect to using XmlHttpRequest, WebSocket or EventSource. The browser will immediately return a 400 status code when your page attempts to connect to a non-valid domain.  frame-src - Specify which locations can be embedded in a &lt;frame&gt; and &lt;iframe&gt;  worker-src - Worker scripts  child-src - Is either deprecated or serves as fallback for frame-src and worker-src, depending on the browser and the CSP level implemented.  manifest-src - defines which manifest can be applied to the resource. (This is still experimental though).Navigation directivesThese directives tell the browser what kind of navigation is allowed:  form-action limits to where forms can be submitted  frame-ancestors specifies who may embed pages using elements such as &lt;frame&gt;, &lt;iframe&gt;, &lt;embed&gt;, &lt;object&gt; and &lt;applet&gt;Other directivesBesides these, CSP allows for quite a few other directives:  report-uri works the same way as the report-uri directive of HPKP  require-sri-for allows you to force the use or Subresource Integrity ((SRI)[#Subresource-integrity]) for stylesheets, scripts or both.  Allowed values are script and style (or both).  base-uri defines which URLs you can use in the &lt;base&gt; element  sandbox to enable a sandbox for requested resources  (have a look at Mozilla’s documentation for more information)NonceA nonce, pronounced “/nɒns/” (nance), is a term that means “number (used) once”.It allows you to load specific inline scripts without allowing all of them. Only those scripts that have a nonce attribute that matches the value specified in the CSP header will get executed.Keep in mind though that you should never hardcode the nonce or use a value that can be guessed.It’s best to generate a new nonce for each request and add it to those scripts you need to execute.E.g. if you have the following CSP setting:Content-Security-Policy: script-src 'nonce-randomValue'will only execute scripts that have the nonce attribute defined like this:&lt;script nonce=\"randomValue\"&gt;    // ... script contents&lt;/script&gt;Multiple policiesYou are allowed to specify multiple CSP policies simply by specifying the header multiple times.If you do this however, it’s important to keep in mind that subsequent CSPs are not allowed to loosen the rules, only to tighten them.Report-onlyAs with HPKP, CSP also supports a report-only variant with Content-Security-Policy-Report-Only.Once again the specifications are exactly the same but it won’t block loading or execution of disallowed resources and simply report violations.You can then use the reports it generates to decide what you need to allow in your actual CSP header, before you deploy it (and break your site).Should I use it?CSP has some risks: it can break your site’s functionality, but overall it’s relatively easy to test it. The report-uri directive allows you to monitor if there are any issues and you can use the report-only version of the header to easily validate the setup you’re planning in the wild.If you have a system that relies a lot on third party content, it might not be for you.For everyone else, try out the report-only header and see if you get any issues.Subresource integrityWhen you’re developing a web application, you’ll often depend on some JavaScript frameworks such as Angular or jQuery.Loading these files from a CDN can speed up load times from your application, since it’s quite likely that the user already has a cached version of the script available.Of course it’s a good idea to be careful about the content of these scripts.Whenever you’re loading resources that aren’t under your control, you’re depending on someone else to make sure that they aren’t tampered with.To make sure that they aren’t changed without your knowledge, you can use subresource integrity (SRI).With SRI, you add an integrity attribute to your &lt;script&gt; or &lt;link&gt; tag.This attribute contains the hash of the file you expect.Your browser will then download the file, hash it with the same algorithm and compare the results.If the hash matches, the resource will be used; otherwise it will be ignored and an error will be shown in the console.&lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"     integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\"     crossorigin=\"anonymous\"&gt;&lt;/script&gt;As you can see in the example above, the integrity of the script will be checked using a ‘SHA256’ hash. You’ll also notice the crossorigin attribute: this attribute is required when loading SRI validated resources from a different origin.Possible values are use-credentials and anonymous, indicating whether a request will have the credentials flag set.If you’re using a CDN, you’ll probably want to use anonymous.Note that you only need to add the crossorigin attribute if you’re loading the resource from a different origin.For resources coming from the same origin, you can omit the crossorigin attribute.Calculating the SRI value.In order to add the integrity attribute, you need to know the correct hash of the file. The easiest way to calculate it is by simply specifying a random value and checking the resulting error in your browser.Should I use it?Most likely. If you’re depending on third party scripts, you should make sure that they aren’t changed without your knowledge.This does mean that you shouldn’t just include the latest version of a script (e.g. example.com/library/latest/) as that will change whenever a new version is released.Cookie protectionMost websites nowadays use a variety of cookies for different purposes.These too can be a source of problems: session cookies grant the user access to certain content or allow them to perform certain actions.If this cookie can be intercepted or altered, the consequences can be enormous.Because of this, it’s a good idea to protect your cookies as much as possible.Since you’re already running your site on HTTPS, it’s a good idea to make sure the cookies aren’t sent on insecure requests.You can easily do this by adding the secure flag to the cookies you send.To make the cookies even more secure, you’ll also need to prevent them from being read/modified by scripts running in the page.In most cases there’s no reason for a script to have access to these cookies, so you can simply mark them as HttpOnly.Should I use this?Yes.Your session cookies should not be available to scripts, so the HttpOnly flag should be set on those.If you’re using TLS (and you should) you should definitely set the secure flag as well.ConclusionBrowsers nowadays support a wide array of security features you can use to keep your users safe.But, as with all things, powerful tools require you to wield them carefully.If you apply them without proper thought, you can easily make your website inaccessible or render it unusable.Because of that, you need to be really careful when you implement (most of) these measures.Do proper testing and (where possible) use the Report-Only variant for a while to spot possible issues before they become real problems.Make sure you really understand what you’re doing and what the consequences are of getting things wrong.When you have all that, don’t be afraid to experiment, just make sure you do so safely.[1] Public Key Pinning with Spring Security"
      },
    
      "microservices-2017-09-17-monitoring-your-microservices-with-micrometer-html": {
        "title": "Monitoring your microservices with Micrometer.io",
        "url": "/microservices/2017/09/17/monitoring-your-microservices-with-micrometer.html",
        "image": "/img/2017-09-17-monitoring-your-microservices-with-micrometer/post-image.jpg",
        "date": "17 Sep 2017",
        "category": "post, blog post, blog",
        "content": "When we want to instrument our application, we don’t want to worry about which monitoring system we want to use, now or in the future.Nor do we want to change a lot of code throughout our microservice because we need to change from system X to system Y.Meet Micrometer!So what is Micrometer you ask?Basically, it comes down to this:  Think SLF4J, but for metrics.Micrometer provides a simple facade over the instrumentation clients for the most popular monitoring systems.It allows you to instrument your code with dimensional metrics with a vendor-neutral interface and decide on the monitoring system as a last step.Using this interface, we can support multiple monitoring systems and switch easily to an other system with little to no hassle.It already contains built-in support for Prometheus, Netflix Atlas, and Datadog, while InfluxDB, statsd, and Graphite are on their way!Using Micrometer in your applicationStarting with Spring Boot 2, more specifically since milestone M4, Micrometer becomes the defacto instrumentation library that will be powering the delivery of application metrics from Spring.Luckily for us, they also backported this functionality to Spring Boot 1.x through an additional library dependency!Just add the micrometer-spring-legacy module together with the additional monitoring system module, and you’re good to go!In Gradle:compile 'io.micrometer:micrometer-spring-legacy:latest.release'Or in Maven:&lt;dependency&gt;  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;  &lt;artifactId&gt;micrometer-spring-legacy&lt;/artifactId&gt;  &lt;version&gt;${micrometer.version}&lt;/version&gt;&lt;/dependency&gt;Creating metricsThere are a couple of ways to create meters.We will cover all different types, when to use them, and furthermore how to implement them.Dimensions/TagsA meter is uniquely identified by its name and dimensions (also called tags).Dimensions are a way of adding dimensions to metrics, so they can be sliced, diced, aggregated and compared.For example, we have a meter named http.requests with a tag uri.With this meter we could see the overall amount of HTTP requests, but also have the option to drill down and see the amount of HTTP requests for a specific URI.CountersCounters are a cumulative metric that represents a single numerical value that only ever goes up.They are typically used to count requests served, tasks completed, errors occurred, etc.Counters should not be used to expose current counts of items whose number can also go down, gauges are a better fit for this use case.        MeterRegistry registry = ...Counter counter = registry.counter(\"received.messages\");    counter.increment();GaugesA gauge is a metric that represents a single numerical value that can arbitrarily go up and down.Gauges are typically used for measured values like current memory usage, but also “counts” that can go up and down, like the number of messages in a queue.        MeterRegistry registry = ...AtomicInteger currentHttpRequests = registry.gauge(\"current.http.requests\", new AtomicInteger(0));Queue&lt;Message&gt; receivedMessages = registry.gauge(\"unprocessed.messages\", new ConcurrentLinkedQueue&lt;&gt;(), ConcurrentLinkedQueue::size);Instead of returning a gauge, the gauge method will rather return the thing that is being observed.This allows us to have quick one liners that both create the object to be observed and set up metrics around it.TimersTimers measure both the rate that a particular piece of code is called and the distribution of its duration.They do not record the duration until the task is complete.These are useful for measuring short-duration latencies and the frequency of such events.long startTime = System.nanoTime();MeterRegistry registry = ...Timer timer = registry.timer(\"timer\");    // this will record how long it took us to get a registry and create a new timertimer.record(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);Or we could just annotate a method with @Timed and let Micrometer do the rest for us@Timedpublic void doSomethingWhichShouldBeFastButIsActuallyReallySlow() {}Long task timersThe long task timer is a special type of timer that lets you measure time while an event being measured is still running.To time a long running task we use the same @Timed annotation, but we set the property longTask to true.@Timed(longTask = true)@Scheduledpublic void doSomethingWhichCanTakeALoooooongTime() {}It is up to the application framework to make something happen with @Timed.In case it isn’t able to do that, you can still use the long task timer.MeterRegistry registry = ...LongTaskTimer looooongTimer = registry.more().longTaskTimer(\"sync\");private void doSomethingWhichCanTakeALoooooongTime() {    looooongTimer.record(() =&gt; {        // actually do some synchronization which takes a loooooong time    });}Distribution summariesA distribution summary is used to track the distribution of events.It is similar to a timer but more general in that the size does not have to be a period of time.Usually it is used to sample observations of things like response sizes.MeterRegistry registry = ...DistributionSummary summary = registry.summary(\"response.size\");Summary statisticsMicrometer provides quantile statistics computed at instrumentation time and histograms for use in calculating quantile statistics at query time for monitoring systems that support this.QuantilesQuantiles are cutpoints dividing the range of a probability distribution into contiguous intervals with equal probabilities, or dividing the observations in a sample in the same way.Timers and distribution summaries can be enriched with quantiles computed in your app prior to shipping to a monitoring backend.Depending on the size of your deployments, computing quantiles at instrumentation time may or may not be useful.It is not possible to aggregate quantiles across a cluster.Four quantile algorithms are provided out of the box with different tradeoffs:  WindowSketchQuantiles - The importance of an observation is decayed as it ages.This is the most computationally costly algorithm.  Frugal2UQuantiles - Successive approximation algorithm that converges towards the true quantile with enough observations.This is the least costly algorithm, but exhibits a higher error ratio in early observations.  CKMSQuantiles - Lets you trade computational complexity for error ratio on a per-quantile basis.Often, it is desirable for higher quantiles to have a lower error ratio (e.g. 0.99 at 1% error vs. 0.5 at 5% error).This algorithm is still more computationally expensive than Frugal.  GKQuantiles - Lets you trade computational complexity for error ratio across all quantiles.This is used inside of WindowSketchQuantiles.HistogramsA histogram measures the statistical distribution of values in a stream of data.It samples observations, like HTTP request durations or database transaction durations, and counts them in buckets.They can be used to compute quantiles or other summary statistics like min, max, average or median.Because histograms buckets are exposed as individual counters to the monitoring backend, it is possible to aggregate observations across a distributed system and compute summary statistics like quantiles for an entire cluster.Naturally, the error rate of the computed summary statistic will be higher because of the lossy nature of putting data in buckets.BindersBinders define a collection of meters and are used to encapsulate best practices for monitoring certain types of objects or a part of the application’s environment.For example, the JvmThreadMetrics binder which gauges thread peak, number of daemon threads, and live threads.Micrometer ships with a basic set of binders:  JVM and system monitoring  Cache monitoring  Executor and ExecutorService monitoring  Logback monitoring"
      },
    
      "conference-2017-06-21-devoxx-pl-html": {
        "title": "Devoxx Poland 2017",
        "url": "/conference/2017/06/21/Devoxx-pl.html",
        "image": "/img/2017-devoxx-pl/devoxx-poland.jpg",
        "date": "12 Jul 2017",
        "category": "post, blog post, blog",
        "content": "Devoxx Poland 2017Krakow in the ICE Krakow Congress Centre.We started off day 1 with the keynote in the absolutely, phenomenal main room:Table Of Contents  Keynote: Speed without Discipline: a Recipe for Disaster  Feature Branches And Toggles In A Post-GitHub World  A reasonable overview of Java 9 and how you could think of it  The Language of ActorsKeynote: Speed without Discipline: a Recipe for Disaster (Venkat Subramaniam)Venkat kicked off the keynote, talking about a paradigm shift, that is happening right now in software development:In the nineties, everybody was doing imperative programming, using objects to implement functionality.Nowadays, this style of software development is shifting towards a more declarative approach.In imperative programming, developers focus on both what they want to do and how they want to do it. In declarative programming on the other hand, developers focus on what they want to do and use tools and libraries to facilitate their goal.Venkat went on to state that programming in a functional style is declarative, but that not all declarative code is functional.Functional style = declarative style + higher order functionslet names = [\"Dieter\", \"Tom\", \"Andreas\", \"Ken\", \"Yannick\", \"Tim\", \"Bart\"];let count = 0;for(const name of names){  if(name.length === 4)    count++;}console.log(count);console.log(names.filter(name =&gt; name.length === 4).size);Declarative vs ImperativeVenkat told the audience that he doesn’t like driving cars.He compared driving a stick shift to imperative programming.His goal is going from point A to point B and he does not want to be involved in changing the gears (Manipulating the DOM).A car with an automatic drive train, is a step in the right direction, but still requires too much focus on how he wants to reach his destination (Using a library like JQuery).Using the auto pilot functionality in certain modern cars is another step in the correct direction, but what he really wants is a car with a dedicated driver, like Uber or Lyft offer (Abstracting the DOM and using frameworks like Angular).In this comparison the ride-sharing service is the declarative approach.Testing  I automate my tests, not because I have a lot of time, but because I don’t.After an introduction to declarative programming, Venkat switched to the topic of testing.To really be agile, we need to be confident that implementing new features won’t cause failure.We can achieve this confidence by automating our tests and making sure they are repeatable.If we are really confident, we might even be able to ship software, without running the application.Writing software without writing tests is described as JDD: Jesus Driven Development. Pray that it works.Obviously, TDD (Test Driven Development) makes a lot more sense.Software development: a profession where people get paid to write poor quality code and get paid more later to cleanup the mess.&mdash; Venkat Subramaniam (@venkat_s) 27 september 2015Testing vs verificationTesting and verification are two different things.Verification is the process that checks if the code (still) works.This is not something anyone should do manually, verification is exactly what should be automated.Testing is the process that checks if a feature is correctly implemented.Code represents what you have typed, not what you might have wanted the system to do.It is the act of gaining insight in the application and the business.This could well be a manual task.Unfortunately, most of our industry has neglected this important difference.The maturity of software verification can be categorized in three maturity stages. Projects without verification automation are in denial, they are building up an increasing technical debt.The second stage describes projects that have some automated verification on the UI level. Venkat describes tools using WebDriver for UI level verification as a pathway to hell automation.This test method can be represented in the ice-cream cone anti-pattern. For projects with the right level of automation, the pyramid pattern is a good representation.The last maturity stage contains these projects with the right measure of automated verification.DisciplineVenkat drew a comparison with 1820, where patients died regularly within three weeks after being operating.Doctors (Joseph Lister, Louis Pasteur) started cleaning their tools after surgery and noticed a positive trend in survival.Analogous to the doctors back then, we need to discipline ourselves in software engineering.This discipline is needed to keep up to speed and to stay agile, so that teams can react rapidly to customer requests. To build up this discipline, automated verification can be seen as the software equivalent of exercising.  We’re practicing a beautiful craft, let’s go turn it into a wonderful profession. Focusing on quality and creative things.Feature Branches And Toggles In A Post-GitHub World (Sam Newman)Sam told us about his experience at a project where the team was having trouble merging branches.The release branch for the next release was called R3, but for a large refactoring, branch R4 was created.Afterwards, he described merging the branches as a car crash.They even needed to introduce a dedicated R3-R4 merge bug fix team.Later on, they set up Continuous Integration in order to prevent the merging issues.The code, pushed by the developers, would get automatically validated by the CI setup.The problem with the R3-R4 release was that validation was done only for a branch and not on the integrated branches.  The integration should be validated every day and when the build breaks, fix it!For unfinished work, we can wait until it is ready before checking in.This exposes us to the risk of losing work when it’s only on the developer’s computer.Feature BranchAn alternative would be to create a feature branch, which brings us back to the problem of merging branches.  Pain of merge = fn(size_of_merge, duration_since_last_merge)Merging branches can be a difficult task and might lead to a commit race, offloading the effort to a colleague.Trunk-based developmentA third option would be to ‘check in anyway’, called trunk-based development.Every commit integrates to the trunk and developers should integrate their local changes daily.Small changes and integrating often makes it easier to merge new code.New half-finished features can be hidden with feature toggles.These toggles can be managed using flags or configurations (eg, in Zookeeper, Consul, …).  A flag should be set and evaluated in as few places as possible, preferably only once each.Flags should be removed when the new implementation is done.More info: Trunk-based developmentChanges to an existing functionality can be done by providing an abstraction above the existing functionality.The new functionality can then be developed for the abstraction and when it is done, changed to the new implementation.Branch by abstraction has the side-benefit that it can be used for A/B and canary releasing.The Continuous Delivery book tells us to treat every check-in as a possible release candidate.Developers start with the assumption that it is worthy, the CI tool decides whether it truly is.Deploy frequently with small changes, making it easier to rollback and lowering the risk of running into problems.GitAnd then there was Git, developed by Linus Torvalds with the goal to merge a patch in less than three seconds.In Git, branches are much more lightweight and every local repository contains the full source history.In 2008, GitHub was founded and introduced pull requests.If you wanted to contribute to open source projects before pull requests you had to:  Develop it locally  Generate a patch file  Mail it over to the project ownersThis feature contributed to GitHub’s success as three years later in 2011, they passed SourceForge and Google Code in popularity.Sam made the remark that pull requests use branches, which might bring problems. On top of that GitFlow was introduced.Because GitFlow introduces even more branches, it is in controversy with fast deployment and small changes cycle.With tools like Split and LaunchDarkly, GitFlow is not needed, if merged frequently.The conclusion was that experimental and release branches, that might even never get merged, still have their uses.The pull request mechanism works well in open source projects.Except for experiments, releases and pull requests, Sam recommends to prevent branches and to keep batch sizes small, integrate often and ship often.A reasonable overview of Java 9 and how you could think of it (Oleg Šelajev - Slides)Since Java 9 does not seem to have a codename and Java 10 is called Project Valhalla, Oleg proposed codename Java 9 the Fury Road, a Mad Max reference.  Java 9 Release date: September 21st 2017JShellJShell is the new REPL (Read-Eval-Print Loop) for Java.It can be used to run commands and get results immediately.For user-friendliness, the semicolons can be omitted after the instructions in JShell.Example command:jshell&gt; List.of(1).getClass()$1 ==&gt; class java.util.ImmutableCollections$List1OptionalsSeveral improvements will be added to the Optional class.Optionals can be turned into streams and have filter, flatMap and map methods.For eager evaluation these functional methods can be applied directly to the Optional.jshell&gt; Optional.of(1).map(x-&gt;x*3)$2 ==&gt; Optional[3]When using stream() in front of the functional methods a ReferencePipeline is returned.This can be used for lazy evaluation.jshell&gt; Optional.of(1).stream().map(x-&gt;x*3)$3 ==&gt; java.util.stream.ReferencePipelineAn or() method will be added to chain a supplier to empty Optionals.jshell&gt; Optional.empty().or(()-&gt;Optional.of(\"Devoxx rocks!\"))$4 ==&gt; Optional[Devoxx rocks!]StreamsTwo new methods will be added to the Stream interface, dropWhile and takeWhile.For ordered streams, these methods drop or take elements while the predicate is true.In unordered streams, dropWhile returns a subset of elements starting from the first predicate match, takeWhile returns a subset of elements matching the predicate.Stream&lt;T&gt; dropWhile​(Predicate&lt;? super T&gt; predicate)Stream&lt;T&gt; takeWhile​(Predicate&lt;? super T&gt; predicate)jshell&gt; IntStream.range(1,10).takeWhile(x-&gt; x&lt;5).boxed().collect(Collectors.toList())$5 ==&gt; [1, 2, 3, 4]jshell&gt; IntStream.range(1,10).dropWhile(x-&gt; x&lt;5).boxed().collect(Collectors.toList())$6 ==&gt; [5, 6, 7, 8, 9]ConcurrencyCompletableFuture will be extended with a copy.The copied CompletableFuture is a defensive copy and completing it doesn’t complete the original CompletableFuture.jshell&gt; CompletableFuture.runAsync(()-&gt;{while(true){}})$7 ==&gt; java.util.concurrent.CompletableFuture[Not completed]jshell&gt; $7.copy()$8 ==&gt; java.util.concurrent.CompletableFuture[Not completed]jshell&gt; $8.cancel(true)$9 ==&gt; truejshell&gt; $8$8 ==&gt; java.util.concurrent.CompletableFuture[Completed exceptionally]jshell&gt; $7$7 ==&gt; java.util.concurrent.CompletableFuture[Not completed, 1 dependents]A new ProcessHandle interface will be added, it can be used to get information and control processes.Bits and piecesThe underscore will become a keyword, so assigning a value to _ does not work.This is probably a feature for the future where _ will be used for matching arguments of any type.Assigning a value to __ will keep on working.In Java 8, default methods were added to interfaces, in 9 they can be private.Property files will support UTF-8 and there is already Java 9 support in several IDEs.There will be several changes to improve String performance, for example using a more space-efficient internal representation for Strings.Javadoc will get an improved search, HTML5 compliance and more info on the module where the class or interface comes from.The use of agents will be more flexible, a process can attach an agent to itself and a JAR can contain multiple agents.ModulesThe Java Platform Modules System (JPMS) allows modularization for Java applications.A module can define dependent modules with requires, to provide an API the exports key word is used.To give access to everyone the opens keyword can be used.There is a method getAccessable().You should be aware though about using it for determining if a module is usable or not since it actually just returns the value of setAccessable(), a toggle that you have to set yourself.Actually it just returns the value of setAccessable(), a toggle that you have to set yourself.To make a smooth transition to the JPMS, any JAR on the classpath will become an automatic module.By default, --illegal-access=permit is the default mode for JDK 9, allowing modules access to all automatic modules.As a migration strategy, Oleg proposes to wait for dependencies to modularize before modularizing yourself.Otherwise you might need to modularize twice to align with the dependencies.Java 9 with Maven is complicated, many plugins need to be upgraded and a lot of functionality is not yet fully integrated with JPMS.Gradle releases fixes more often and currently supports more features.A multi-release JAR containing multiple versions for the same file, in the same JAR, is a new feature that should be used with caution.Garbage collectionThe G1 Garbage Collector (G1GC) will become the default in Java 9.Previous Garbage Collectors were not as scalable nor predictable.The G1GC promises a more scalable and more predictable system with few modification options.By default, a quarter of the physical ram will be allocated to the heap, unless the size is specified with the -mx flag.Due to its new heap division system, it might run into problems with large chunks of data.It is recommended to feed streams of data directly to parsers without first capturing it in a byte array, this also applies to JSON parsing and database operations.Another improvement is using immutable objects wherever possible.Using a StringBuilder instead of concatenating Strings will reduce heap usage. For more info, Oleg referred to a talk on Moving to G1GC by Kirk PepperdineHTTP/2 ClientThe JDK 9 will contain an incubator package with a HTTP/2 client with a fluent API.The modules in the incubator package are non-final APIs that can be finalized or removed in future releases.HttpClient.newHttpClient().send(        newBuilder(URI.create(\"https://google.com\"))            .GET()            .build(),        HttpResponse.BodyHandler.asString())    .body();)Oleg concluded by recommending the audience not to touch multi-release JAR, jlink and Unsafe, unless you are 100% sure what you are doing.For now, he recommends to upgrade your IDE and tools and upgrade Spring to version 5.0. Then add the --illegal-access=warn startup option and fix the easy fixable warnings and then wait a year or more until the classpath and the libraries, you depend on, are upgraded.Yannick, a colleague at Ordina did a nice presentation on Java 9: A first look at Java 9 by Yannick De Turck.Presentation and sources are available at GitHub.The Language of Actors (Vaughn Vernon - Slides)Vaughn started his talk by introducing Rear Admiral Grace Hopper to the audience.In the American Navy, she was a Computer Scientist and wrote software for a long time.She was really into not wasting cycles and emphasised on not wasting nanoseconds.Then Vaughn introduced Donald Knuth, another legend in Computer Science.Knuth is known from the quote Premature optimization is the root of all evil.But that is not exactly what he said, the full quote says:  We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.Another quote was shared of Donald Knuth:  People who are more than casually interested in computers should have at least some idea of what the underlying hardware is like. Otherwise the programs they write will be pretty weird.To further build his point, Vaughn told about a project that was written in Cobol.The code was across 5 diskettes thus user interaction was needed to run the application.To improve the usability of the application, it was rewritten in C, allowing the software to fit on just one diskette.With this introduction, Vaughn wanted to emphasize how hard it is to optimize software for resource usage.Threading is hardIn 1973, academics discovered the Actor Model. 13 year later in 1986 Joe Armstrong rediscovered the approach.Armstrong designed and implemented a programming language on this model, ErlangIn 2008, Jonas Bonér came up with Akka for the Java Virtual Machine and in 2011 José Valim came up with another Actor based language called Elixir.Because the Actor Model is Message Driven, it inherently is Reactive.Now is the time for the Actor Model, with the decreasing expense of memory, network and chips.Processors are having a lot of cores these days, Intel Xeon units go up to 88 cores, Intel Xeon Phi can have more then 200 coprocessors.The actor model allows us to embrace latency. If we design for latency, it will not have a blocking impact on the design.We are not at Google scale, why use actors?With the Actor Model you can do more with less. The total number of nodes can be reduced to just a few, several million actors per machine is not a problem.The actor model uses the essence of Domain Driven Design (DDD), the bounded context and ubiquitous language.DDD is excellent way to make complexity surrender, by knowledge crunching.Actors help us reason better by having less moving parts.This allows us to focus on business aspects, instead of the architecture around it.How to do DDD in projects:  Talk with customer (iterate)  Write some scenarios (iterate)  Strategic Event Storming (iterate)  Tactical Event Storming (iterate)  Implement acceptance tests and model (iterate)This concludes our recap of this amazing edition of Devoxx Poland."
      },
    
      "spring-2017-07-11-springio17-summary-html": {
        "title": "Spring I/O 2017 Recap",
        "url": "/spring/2017/07/11/SpringIO17-Summary.html",
        "image": "/img/springio2017.jpg",
        "date": "11 Jul 2017",
        "category": "post, blog post, blog",
        "content": "  On the 18th and 19th of May, we had another great edition of Spring I/O, brought to us by organizer Sergi Almar.In this blog post, we will go over some of our favourite sessions of the conference.  DISCLAIMER: we could not include ALL the talks from Spring IO in this blogpost. We provide an extensive summary of our favorite talks and created a curated list of all the talks and resources at the bottom of this post.  Table Of Contents      Keynote: The Only Constant Is Change    Bootiful Database-centric Applications with jOOQ    Google Spanner    Easily secure and add Identity Management to your Spring(Boot) applications with Keycloak    Spring Cloud Streams (Data Services)    The Road To Serverless: Spring Cloud Function    Reactive Spring Data    The Future Of Event Driven Microservices With Spring Cloud Stream    New in Spring 5: Functional Web Framework    Spring Auto REST Docs    References  1. Keynote: The Only Constant Is Changeby Andy Wilkinson and Stéphane NicollObviously, the keynote presentation was filled with announcements, interesting tidbits and two great presenters.  The biggest topic revolved around how Spring has always tried to enable developers to adapt their applications rapidly.This capacity of adapting to change increased dramatically when Spring Boot was released, which explains part of its success.They also reported the release of Spring Boot 2.0.0 M1.This release was announced very shortly after Spring Framework 5.0 went RC1 on the 8th of May.The keynote can be found here and the demo code here.2. Bootiful Database-centric applications with jOOQby Michael SimonsAs a personal fan of @rotnroll666, I went to see his talk, fully expecting to be impressed by the level of quality and code.As always, Michael delivered: he made me realize there are still plenty of PL/SQL developers out there in the enterprise landscape, who hold on to their code like kids to their favorite stuffed animal before bedtime.Luckily for us, someone created a library called jOOQ:  jOOQ generates Java code from your database and lets you build type safe SQL queries through its fluent APIThere are many use cases where jOOQ can prove useful, but the example that Michael used described the enterprise environment at his current employer:He was working for a big utility company in Germany, who developed applications on power and gas usage consisting of lots of time-series data.Almost all of the data was stored in SQL databases with a big layer of PL/SQL on top.They also developed some desktop GIS applications using Oracle Spatial to visualize the data.So the question they asked themselves at a certain moment:  Should we approach all of our data using plain PL/SQL? Should we use an ORM tool like Hibernate? Or can we use something in between?Of course, as with any good question, the answer is:  It depends. There is no silver bulletThere are many options in the Java space to approach this problem:  Plain JDBC  Using a JDBCTemplate  JPA with JPQL and / or Criteria APIMichael’s team solved most of their problems using Hibernate, and while that comes with several advantages, it doesn’t mean you have to use it for everything.One of the improvements they made was using Hibernate combined with jOOQ:  Use Hibernate for the regular database queries and day-to-day manipulations of your database  Use jOOQ for your complex queries and database migrationsTo briefly summarize what jOOQ can do for you:  jOOQ is SQL-centric which means jOOQ infers information from the actual database, not from the ORM model  It exposes a typesafe meta-model generated from your SQL  The Query Builder framework uses a very legible DSL (in a much more concise way than the Criteria API)  It generates a Java-based schema (using Maven or Gradle)  It can reverse-engineer an existing DB and generate SQL  Spring provides integration with Spring Boot through the spring-boot-starter-jooq dependencyAn ideal scenario to use jOOQ would be to:  Run your database migration with Flyway or Liquibase first  Run the code generator to generate the Java DSL context (this happens in the Maven generate-sources lifecycle phase)  Use the DSL context to write your typesafe queries, for example:BookRecord book = create.selectFrom(BOOK).where(BOOK.ID.eq(1)).fetchOne();For more information about jOOQ, you can check out their website.  Question: What’s the difference between the jOOQ Query API and using JPA Criteria API with the Hibernate ModelGen, which is also typesafe?  It resembles the native SQL much better  jOOQ provides standardization since it performs SQL transformations that work for any SQL dialect  It should make it easier to migrate existing PL/SQL applications  There is a much more extensive collection of SQL functions and possibilities  jOOQ provides POJO mappers which are also generated from the code generator  As much as you hate them, it supports calling Stored Procedures!The code from Michael’s talk can be found on Github.3. Google Spannerby Robert KubisGoogle Spanner is a globally distributed relational database service that provides ACID transactions and SQL semantics, without giving up horizontal scaling and high availability.When building cloud applications, you are no longer forced to choose between traditional databases that guarantee transactional consistency, or NoSQL databases that offer simple, horizontal scaling and data distribution.Cloud Spanner offers both of these critical capabilities in a single, fully managed service.With Spanner, your database can scale up and down as needed, and you only pay for the amount you use.Spanner keeps application development simple by supporting standard tools and languages in a familiar relational database environment.It supports distributed transactions, schemas and DDL statements, SQL queries and JDBC drivers and offers client libraries for the most popular languages, including Java, Go, Python and Node.js.As a managed service, Cloud Spanner provides key benefits to DBAs:  Focus on your application logic instead of spending valuable time managing hardware and software.  Scale out your RDBMS solutions without complex sharding or clustering.  Gain horizontal scaling without migration from relational to NoSQL databases  Maintain high availability and protect against disaster without needing to engineer a complex replication and failover infrastructure.  Gain integrated security with data-layer encryption, identity and access management and audit logging  4. Easily secure and add Identity Management to your Spring(Boot) applications with Keycloakby Sébastien BlancI must say, this was one of the funniest talks of the conference.Sébastien knows how to entertain the crowd and he kicked off with a great quote which, of course, I immediately stole and tweeted:Similar to a quite from @sebi2706 last week at #springio17 : forget about companies, it&#39;s all about community and code! 🙌&mdash; Dieter Hubau (@dhubau) May 26, 2017First of all, let’s forget that Keycloak was created by Redhat and that it is written in Java EE.The following aspects of Keycloak are more important:  It’s Open Source)  Redhat provides support through their commercial fork called Redhat SSO)  Great Spring Boot Integration through the use of a Spring Boot Starter)  Seamless Spring Security Integration  Supports OAuth 2.0, SAML 2.0, OpenID Connect  Integration with Active Directory, LDAP and even Kerberos (start drooling enterprise users!)It’s actually quite easy to setup Keycloak:  Download the Keycloak standalone server  Extract and run it  Start the server and create an admin user  Create a new realm  Create a new application  Add roles to your application  Create a user to authenticate with  Create a Spring Boot application at The Happiest Place On Earth) and include the Keycloak starter  Add the Keycloak properties to your application.yml:          server URL      realm      resource name of your application      security constraints for your users        Run!  There are many additional features for power users:  Automatic registration of applications should be possible using a one-time token (coming soon?)  Centralized User Management  CORS support for Single Page Applications  Social Login Integration  Registration and Forgot Password functionality, all out-of-the-box, configurable at runtime  UI Customization of all pages is possible through theming (start drooling designers!)All in all, the setup and demo went very smooth and I genuinely feel this product is about to become very popular, partly because of the Spring Boot integration, but also because it just seems very solid and user-friendly.There might be a dedicated blogpost coming soon about Keycloak, so stay tuned and check our blog regularly or subscribe to our RSS feed!5. Spring Cloud Streams (Data Services)by Michael MinellaMichael gave a summary about all the new projects in the Spring ecosystem that process data and / or messages very well.He explained that there are lots of big data frameworks out there (Hadoop, Spark, …), which can handle BIG amounts of data very well.However, they are usually too bulky / difficult / inappropriate for handling smaller volumes of data.Also, for quickly setting up something like Hadoop or Spark, the learning curve is too high and the effort doesn’t justify the benefits.Solution: data microservices  Developed and tested in isolation, also easier to test  Independently scalable depending on data processing load  Familiar development model, just like regular cloud-native Spring microservices  Easier to govern for Ops  So the need for data / app integration / composition arises  Which means the need for orchestration and operational coverage arises (lots of plumbing required)Spring Cloud Stream  Streams are thin wrappers around Spring integration  Supported binder for integration between services: Kafka, RabbitMQ, …  Source, Processor, Sink model is easy to comprehendSpring Cloud Task  Tasks are finite microservices, built around Spring Batch  “Microservices that end”  Contain Task repository which tracks run/start/end of the tasks  Has Spring Batch integration (partition steps using workers)  Has Spring Cloud Stream integration (eg. launch Tasks via Streams)  Simple annotation @EnableTask  Use cases: batch jobs, scheduled one-off processes, ETL processing, data scienceSpring Cloud Data Flow  AKA the new and Improved Spring XD  Data flow orchestrator  Use a shell or the UI which goes over REST endpoints  Has custom DSL  All the components are regular Spring Boot apps (Data Flow server, Shell, …)  Data Flow server has datastore for task repository, batch repository, configuration, …  Data Flow server does not do any of the actual work  We will be publishing a fun blogpost about Spring Cloud Streams soon, so stay tuned or subscribe to the RSS feed!6. The Road to Serverless: Spring Cloud Functionby Dr. Dave SyerFaaSIn recent years we’ve seen the emergence and evolution of following cloud abstraction layers in order of abstraction level:  Virtual Machines (IaaS)  Containers (CaaS)  Applications (PaaS)  Functions (FaaS)The Goal of each of these is raising the value line; in other words, the purpose of each of these is to abstract away various concerns that are of no business value(e.g. setting up and maintaining infrastructure, the environment the code has to run in…).The latest and most extreme level of these is FaaS (or ‘serverless’).Basically all the programmer should do in a FaaS environment is write a Function and hand it over to the platform.The platform takes care of:  Making sure the function is executed on demand  Deploying and undeploying (often on demand!)  Scaling up the amount of instances quickly and in parallel if the need arises (is easier with functions since they are simpler in nature than applications)  Managing integrations with other systems  The naming scheme of “serverless” is unfortunate; of course you’re gonna have servers, you just don’t care about themProblem  By now there are a lot of FaaS solutions out there; AWS Lambda, Google Cloud Function, Azure Function, IBM Openwhisk, Fission, Kubeless, …  Deploying and programming functions is different for each platform because you have to use their native APIs and they have their own platform to deploy on.  Running and testing these functions locallyEnter Spring Cloud FunctionThe purpose of the new project called Spring Cloud Function is to solve these problems by:  Keeping all advantages of serverless/functions, but with all the possibilities that Spring offers (DI, integration, autoconfig, build tools)  Providing a low entry level for Spring devs to jump on the FaaS model  Providing a low entry level for FaaS people without having knowledge of Spring  Making it possible to run the same business logic as web endpoint, stream processor or a task  Introduce an uniform programming model across providers and able to run standalone (not on a IAAS or PAAS).  Support a reactive programming model (Flux, Mono) as wellThe project will try to achieve this by:  Supporting the familiar Java 8 Function types:        @SpringBootApplication    public class Application {        @Bean        public Function&lt;String, String&gt; uppercase(){           return (value) -&gt; value.toUpperCase();        }        public static void main(String[] args) {            SpringApplication.run(Application.class, args);        }    }          As well as the Reactive Types Flux and Mono :        ...    public Function&lt;Flux&lt;String&gt;, Flux&lt;String&gt;&gt; uppercase() {        return flux -&gt; flux.map(String::toUpperCase);    }    ...          Building/deploying the Function as a web endpoint, a task or a stream  can be done by merely altering dependencies, for example:          Deploying the function as a web endpoint can be done by adding the dependency spring-cloud-function-web        In a similar fashion, it will also possible to build for a platform like AWSConclusionIn a way the goal of FaaS is similar to the Spring framework; allowing the developer (or the IT department) to focus on writing code that has real value.The purpose of FaaS is to help us with infrastructure, scalability etc for the functions we write while Spring cloud function will allow us to write and deploy these functions in an (almost) platform agnostic fashion.At the same time, it will enable the programmer to leverage the Spring Framework with it’s various features that helps the programmer to focus even more on his main purpose; programming things that deliver real value: business code!PS: Kenny Bastani has just published a VERY detailed blogpost about Spring Cloud Function on AWS Lambda.It’s a follow-up of his earlier blogpost about Event-driven Microservices using CQRS and Serverless.I would highly recommend his blog!Spring Break  7. Reactive Spring Databy Christophe StroblBiggest changes of Spring Data Kay M3  Java 8 baseline  ID classes don’t need to be Serializable anymore  breaking change: No more null in the repositories (arguments or return values)  breaking change: More consistent naming (eg. findOne -&gt; findOneById)  Composable interfaces (separate Readable / Insertable and make custom repositories as composable interface as well)  Builder style Template APIs  Kotlin extensions are coming in M4Data Store specifics  MongoDB:          breaking change: MongoDB driver baseline to 3.x      Introduction of ReactiveMongoTemplate      Enhanced aggregation support      Collation support        Cassandra:          breaking change: Update to Cassandra 3.2      No reactive native driver –&gt; mimicking reactive driver with thread pool (and blocking) underneath (with ReactiveCassandraTemplate)        Redis:          JRedis discontinued      Upgraded to Lettuce 5 (not GA yet though) supports native reactive driver        Gemfire:          Lucene index support      Off-heap, Redis Adapter, Security annotation config        Solr:          Upgrade to Solr 6        Spring Data REST:          CORS config mechanism      Improved headers      Improved media type support      8. The future of event driven microservices with Spring Cloud Streamby Kenny BastaniEvolutionMonolith applicationThere are some cultural problems with monoliths.One big application slows down the velocity of getting into production.Everyone has to use a shared deployment pipeline.For large code-bases it is harder for new engineers to get up to speed.The engineers that were there from the beginning, who designed the application, are busy explaining the history of the application to new engineers joining the project. These developers are creating change but might get blocked by DBA and Ops teams.Monolith organizationCentralized authority for operations, database and change management slows progress.These coordinated releases batch many changes together from different teams.Usually operations drives the runtime environment of applications because they take all operation responsibility including upgrades of virtual machines.The key problem is that everything is deployed at once or nothing at all.Move towards SOAWith Service-oriented Architecture the application is split up in components which can be deployed individually but now the key problem are the shared libraries.Releasing a change in an object that is not shared can be done separate, but still a problem for the shared ones.Now we arrived at MicroservicesThere are a lot of improvements but Microservices also adds the complexity of running a distributed system.Small, two pizza (5-8 members), teams organized around business capabilities with responsibility of running their own services.We gain independent deployability because each team produces and consumes rest APIs to communicate.Team also have more freedom to chose the best tool for the job they are facing.Microservices brings the challenge of eventual consistency, in a monolith you could rollback a transaction at the database level if something went wrong. Now eventual consistency is not guaranteed, inconsistency happens all the time. Rolling back transactions across multiple services is not easy.Is it a Monolith or Microservice?  If it takes longer than a day for an engineer to ramp up its probably a monolithIn a typical architecture the front-end teams integrate directly with the microservices.This is an anti-pattern in distributed systems. Consumers should not have to worry which instance of the replicated services they have to go to.You could use Spring Cloud, it allows you to centralize authentication with OAuth tokens and routing through an  API Gateway. The front-end does not need to be concerned with all these services, for them it looks like consuming a monolithic API.Splitting the monolithThe popular route from monolith to a microservice architecture is slicing off bits of functionality.This is hard in practice, splitting up a schema is usually the complex part.Refactoring out functionality and tables to new services can be hard because of foreign key constraints for example.Why we need event-driven microservicesThe problem with microservices is that there are no foreign key constraints between services.Furthermore, distributed transactions are brittle and distributed systems are hard.  You will drown in problems you didn’t know existed!Without event-driven microservices and an audit trail you will never know why something went wrong.This audit trail allows you to reason about what went wrong and roll back state.Rules for Event-driven microservicesA lot of these rules are from reference applications and Kenny’s work with Chris Richardson.Domain events are a first class citizen, every time you change some piece of data, domain events should be exchanged.These events can be used as an audit trail to determine why state changed in a system.Each domain event contains a subject with the project aggregate and a payload with immutable data.@Entity@EntityListeners(AuditionEntityListener.class)public class ProjectEvent {    @Id    @GeneratedValue(strategy = GenerationType.AUTO)    private Long eventId;        @Enumerated(EnumType.STRING)    private ProjectEventType type;        private Project entity;        private Map&lt;String, Object&gt; payload;To make this way of working accessible to the developers, hypermedia APIs need to expose links on the aggregates.A traversal list of command and a log of events that happened should be accessible.Command handlers trigger commands on aggregate and then the command is going to generate events.Every domain event applies a state transition to an aggregate.Event handlers are going to subscribe to an event and apply changes to an aggregate to change the state.In a graph representation, event handlers would be the nodes and events the edges.CQRS is used to create materialized views from streams of events.With CQRS you will have a command side and a query side.For example the commands might be written to an Apache Kafka event store.Then an event processor could be using Spring Cloud Stream to retrieve these events and create a data model.The data model is then written to a Data Store like MySQL, where the query side reads the data.An API gateway, like Spring Cloud Netflix Zuul, can be put in front so it looks for the consumer like a regular microservice.For deploying this application you can combine these components together or scale them independently.ServerlessChanges the pricing model for the execution on a cloud provider.With Serverless you are going to have a function in the cloud and you are going to pay for each execution.It is an event driven model, so if data is fed to for instance a AWS Lambda function this can invoke other functions in Python for example.Kenny concluded with a demo and recommended Dave Syer’s Talk on Spring Cloud Functions for more info about serverless.Spring break  9. New in Spring 5: Functional Web Frameworkby Arjen PoutsmaIn the keynote Andy Wilkinson and Stéphane Nicoll mentioned that the Spring framework and especially Spring Boot is all about providing choices to developers.The framework provides us with tools to tackle problems in multiple ways. In the light of this, starting from Spring 5 there will be a new functional alternative to handle incoming web requests.This new functional web library is an alternative to annotation driven approach that is broadly applied in current applications.  Arjen Poutsma states that some people are not happy with magic that happens behind the scenes when you use annotations like @RequestMapping or the newer @GetMappingThis was one of the reasons that made Spring develop this new library.In the next sections we show a quick introduction to what was shown at Spring IO about what this new framework has to offer.Handler function exampleThe following UserHandler class is the replacement of the Controller class that we would have annotated in the regular web framework. In this new functional style the way we handle requests is a bit different.We define functions that have a ServerRequest as parameter and we return a Mono with a ServerResponse.The request contains all the information we need.It contains the body of the request, pathvariables, request headers, …So no more injecting pathvariables and body objects, we have everything we need in this ServerRequest.What we return is the ServerResponse in which we can easily put all the information we want to give back to the client.And Spring provides us with an easy builder to create such a response as it already did with the ResponseEntity builder.You can see that these new objects and builders provide us with an easy and declarative way to handle requests and create responses, without the “magic” that we used previously with the annotations.public class UserHandler {    public UserHandler(UserRepository repository) {        this.repository = repository;    }        public Mono&lt;ServerResponse&gt; getUser(ServerRequest request) {        int userId = Integer.valueOf(request.pathVariable(\"id\"));        Mono&lt;ServerResponse&gt; notFound = ServerResponse.notFound().build();        Mono&lt;User&gt; userMono = this.repository.getUser(personId);                return userMono                .flatMap(user -&gt; ServerResponse.ok().contentType(APPLICATION_JSON).body(fromObject(user)))                .switchIfEmpty(notFound);    }        public Mono&lt;ServerResponse&gt; createUser(ServerRequest request) {        Mono&lt;User&gt; user = request.bodyToMono(User.class);        return ServerResponse.ok().build(this.repository.saveUser(user));    }        public Mono&lt;ServerResponse&gt; listUsers(ServerRequest request) {        Flux&lt;User&gt; people = this.repository.allUsers();        return ServerResponse.ok().contentType(APPLICATION_JSON).body(users, User.class);    }}We have defined how we want to handle requests and how we translate it to a response.What we need next, is a way to say which requests will be handled by which handler function.In Spring MVC, this was done by adding an annotation that declared some parameters, for example, to couple a request path to a controller method.The functional web framework does this by creating RouterFunctions.This RouterFunction is a function that takes a ServerRequest and returns a Mono&lt;HandlerFunction&gt;. To choose which requests get handled by which HandlerFunction, Spring again provides us with some builder functions.That way we can easily bind the handlers we just created with a path as shown in the next code example.Router examplepublic RouterFunction&lt;ServerResponse&gt; routingFunction() {    PersonHandler handler = new PersonHandler(userRepository);        return nest(path(\"/person\"),            nest(accept(APPLICATION_JSON),                    route(GET(\"/{id}\"), handler::getPerson)                    .andRoute(method(HttpMethod.GET), handler::listPeople)            ).andRoute(POST(\"/\").and(contentType(APPLICATION_JSON)), handler::createPerson));}Creating a Tomcat serverNow that we have declared which routes are handled by which functions we have to let our server know this.In the next code example we show how to create a Tomcat server and how to bind the RouterFunction to our server.public void startTomcatServer() throws LifecycleException {    RouterFunction&lt;?&gt; route = routingFunction();    HttpHandler httpHandler = toHttpHandler(route);        Tomcat tomcatServer = new Tomcat();    tomcatServer.setHostname(HOST);    tomcatServer.setPort(PORT);    Context rootContext = tomcatServer.addContext(\"\", System.getProperty(\"java.io.tmpdir\"));    ServletHttpHandlerAdapter servlet = new ServletHttpHandlerAdapter(httpHandler);    Tomcat.addServlet(rootContext, \"httpHandlerServlet\", servlet);    rootContext.addServletMapping(\"/\", \"httpHandlerServlet\");    tomcatServer.start();}And then in the main method we only have to start our Tomcat server and we’re up and running:public static void main(String[] args) throws Exception {    Server server = new Server();    server.startTomcatServer();}ConclusionThe new functional web framework gives us a more declarative and functional way to create a server and handle web requests. In my opinion this code is a lot clearer because you have a direct link between routing and handling requests.This code may also be easer to test than the annotation driven web request handling because we don’t necessarily need to fire up our spring context to test the routing.We can just create a unit test for our RouterFunction and verify our routes are correct.What I do still wonder about is how this integrates with Spring security.How can we define which users can access which handler.Do we still do this with annotations or will we get a new way to do this as well?The Spring functional web framework is an interesting new development and we will be following it closely to see how we can use it in our new projects.Spring break  10. Spring Auto REST Docsby Florian BenzSpring Auto REST Docs is an extension on Spring REST Docs (our post on Spring REST Docs can be found here).This extension helps you to write even less code by including your Javadoc into the Spring REST Docs.For a more detailed overview on what is possible and how to start using this extension, please visit the official documentation here.Imagine you have the following method in your controller:@RequestMapping(\"users\")public Page&lt;ItemResponse&gt; searchItem(@RequestParam(\"page\") Integer page, @RequestParam(\"per_page\") Integer per_page) { ... }With the following POJO:public class User {    private String username;    private String firstName;    private String lastName;        ...}And the test that generates Spring REST Docs:this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk())\t.andDo(document(\"users\",     requestParameters(         parameterWithName(\"page\").description(\"The page to retrieve\"),         parameterWithName(\"per_page\").description(\"Entries per page\")     ),    responseFields(            fieldWithPath(\"username\").description(\"The user's unique database identifier.\"),            fieldWithPath(\"firstName\").description(\"The user's first name.\"),            fieldWithPath(\"lastName\").description(\"The user's last name.\"),    )));When using Spring Auto REST Docs, this could be replaced by adding Javadoc to the method in the controller:/** * @param page The page to retrieve * @param per_page Entries per page */@RequestMapping(\"users\")public Page&lt;ItemResponse&gt; searchItem(@RequestParam(\"page\") Integer page, @RequestParam(\"per_page\") Integer per_page) { ... }And adding Javadoc to the POJO fields:public class User {    /**    * The user's unique database identifier.    */    @NotBlank    private String username;        /**    * The user's first name.    */    @Size(max = 20)    private String firstName;        /**    * The user's last name.    */    @Size(max = 50)    private String lastName;        ...}And then removing the requestParameters and responseFields from the test:this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk());You notice that I added the annotations @NotBlank and @Size in the POJO, these annotations will also be reflected in the resulting documentation.You could also create your own annotations.Result:            Path      Type      Optional      Description                  username      String      false      The user’s unique database identifier.              firstName      String      true      The user’s first name. Size must be between 0 and 20 inclusive.              lastName      String      true      The user’s last name. Size must be between 0 and 50 inclusive.      Because the description of the POJO is now added on field level, it is guaranteed that this description will be the same everywhere this field is used, meaning less maintenance is needed.11. ReferencesYoutube PlaylistAll the talks of Spring IO 2017 are available on Youtube.Talks: Day One            Topic      Presenter(s)      Resource(s)                  KEYNOTE - The Only Constant Is Change      Stéphane Nicoll, Andy Wilkinson                    Reactor 3, the reactive foundation for Java 8 (and Spring 5)      Simon Baslé                    Architecture Deep Dive in Spring Security      Joe Grandja                     The Spring ecosystem in 50 minutes      Jeroen Sterken                    Bootiful Development with Spring Boot and Angular [WORKSHOP]      Matt Raible                    Spring Boot at AliExpress      Juven Xu                    Database centric applications with Spring Boot and jOOQ      Michael Simons                    Testing for Unicorns      Alex Soto                    Front Ends for Back End Developers      Matt Raible                    The Beginner’s Guide To Spring Cloud      Ryan Baxter                    Microservices, but what about the UI      Marten Deinum                     Making the most of Spring boot: adapt to your environment! [WORKSHOP]      Arjan Jorritsma, Erwin Hoeckx                     New in Spring 5: Functional Web Framework      Arjen Poutsma                    Deep Learning with DeepLearning4J and Spring Boot      Artur Garcia, Dimas Cabré                     Easily secure and add Identity Management to your Spring(Boot) applications      Sébastien Blanc                     The Future of Event-driven Microservices with Spring Cloud Stream      Kenny Bastani                    Container orchestration on Apache Mesos - DC/OS for Spring Boot devs      Johannes Unterstein                     Building Spring boot + Angular4 apps in minutes with JHipster      Deepu K Sasidharan                     Hands-on reactive applications with Spring Framework 5 [WORKSHOP]      Brian Clozel, Violeta Georgieva                    DDD Strategic Design with Spring Boot      Michael Plöd                    Awesome Tools to Level Up Your Spring Cloud Architecture      Andreas Evers                    Surviving in a Microservices Team      Steve Pember            Talks: Day Two            Topic      Presenter(s)      Resource(s)                  Reactive Spring      Mark Heckler, Josh Long                    Spanner - a fully managed horizontally scalable relational database with ACID transactions that speaks SQL      Robert Kubis                     Reactive Spring UI’s for business      Risto Yrjänä                     Hands-on reactive applications with Spring Framework 5 [WORKSHOP]      Brian Clozel, Violeta Georgieva                    Data Processing With Microservices      Michael T Minella                    Protection and Verification of Security Design Flaws      Marcus Pinto, Roberto Velasco                     Experiences from using discovery services in a microservice landscape      Magnus Larsson                     Harnessing the Power of Spark &amp; Cassandra within your Spring App      Steve Pember                    It’s a kind of magic: under the covers of Spring Boot      Andy Wilkinson, Stéphane Nicoll                     Introducing Spring Auto REST Docs      Florian Benz                    Leveraging Domain Events in your Spring Boot Microservices [WORKSHOP]      Michael Plöd                    Functional web applications with Spring and Kotlin      Sébastien Deleuze                    Setting up a scalable CI platform with jenkins, docker and rancher in 50 minutes      Wolfgang Brauneis, Rainer Burgstaller                      The Road to Serverless: Functions as Applications      Dave Syer                     TDD with Spring Boot - Testing the Harder Stuff      Sannidhi Jalukar                     Splitting component containers to simplify dependencies      Eugene Petrenko                    Build complex Spring Boot microservices architecture using JHipster [WORKSHOP]      Deepu K Sasidharan                    Caching Made Bootiful      Neil Stevenson                    Getting Thymeleaf ready for Spring 5 and Reactive      Daniel Fernández                     Developing a Spring Boot Starter for distributed logging      Carlos Barragan                     Reactive Meets Data Access      Christoph Strobl                    Building on spring boot lastminute.com microservices way      Luca Viola, Michele Orsi                     Growing Spring-based commons, lessons learned      Piotr Betkier                     CQRS with Spring Cloud Stream [WORKSHOP]      Jakub Pilimon                     Develop and Run your Spring Boot application on Google App Engine Flexible      Rafael Sánchez                    Manage distributed configuration and secrets with Spring Cloud and Vault      Andreas Falk                    From Zero to Open Source Hero: Contributing to Spring projects      Vedran Pavic            "
      },
    
      "paas-2017-06-29-openshift-an-introduction-html": {
        "title": "OpenShift: An introduction",
        "url": "/paas/2017/06/29/Openshift-an-introduction.html",
        "image": "/img/Openshift.png",
        "date": "29 Jun 2017",
        "category": "post, blog post, blog",
        "content": "        Why my first blog about OpenShiftWhen I started as a developer, the cloud ecosystem started expanding and became the next big thing.So obviously I wanted to see what all the fuzz was about and started taking a deeper look at it.Soon, I got introduced with so many new technologies I was not familiar with: microservices, containers, pods, Kubernetes, load balancing, Docker, PaaS,…To be honest, for me it was really overwhelming.I told myself, I would never have the time to become a guru in all these technologies to start with cloud native development.So I just sat in a corner crying about why I became a dev in a time where things never looked more complicated and changed faster than ever before.But actually it’s not all that complicated.To be honest, deploying your containers in the cloud and managing things are easier than ever before with PaaS and OpenShift.A couple of months ago, I got introduced to OpenShift and got really excited about it!Recently, Ordina gave me the chance to visit the Red Hat’s partner conference and my excitement for OpenShift reached new heights.With my body being unable to contain all that excitement for OpenShift, I had to funnel it into a blog post or otherwise I would spontaneously combust.I do have to mention that if you want to work with OpenShift, you still need to have a basic understanding about containers, PaaS and Kubernetes if you want to understand some of its magic.If you have no idea what Docker containers are or what Kubernetes is, don’t panic! There are some great blogposts on the JWorks blog explaining more about them.What is OpenShiftOpenShift is a PaaS. For those who don’t know what a PAAS is, stop reading now, take a timecab to the year 2011 and check it out because PaaS is awesome.Gartner calls OpenShift a Cloud Enabled Application Platform (CEAP).For those who are not sure anymore, here is a quick reminder.  Platform as a service (PaaS) or application platform as a service (aPaaS) is a category of cloud computing services that provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an appThere are multiple PaaS providers available.For example, you have OpenShift, Cloudfoundry, Heroku, Google App Engine and more.Most of these platforms offer a lot of the same solutions, each with their own pros and cons, but today we are going talk about OpenShift specifically.When I look up OpenShift in Google (since that’s the first thing we do these days), it gives me the following explanation:  OpenShift Container Platform (formerly known as OpenShift Enterprise) is Red Hat’s on-premise private platform as a service product, built around a core of application containers powered by Docker, with orchestration and management provided by Kubernetes, on a foundation of Red Hat Enterprise Linux.Well that explains it!I never thought writing my first blog post would be that easy!Obviously you wouldn’t be reading this blog post if this was my only explanation since my pull request would never be accepted.If I would try to explain it with my own words to someone who never heard of OpenShift, I would define it like this.  OpenShift container Platform is a platform as a service you can deploy on a public, private or hybrid cloud that helps you deploy your applications with the use of Docker containers.It is build on top of Kubernetes and gives you tools like a webconsole and CLI to manage features like load balancing and horizontal scaling. It simplifies operations and development for cloud native applications.Okay, I know this is still pretty vague and it can do so much, so why don’t we simply start with seeing where OpenShift fits in.  As you can see in the image, the IT landscape has evolved a lot in recent years.We now have DevOps, Microservices, Containers, Cloud and Kubernetes.OpenShift combines all of those things in one platform you can easily manage.So it actually fits right on top of all of that.Overview  SELF-SERVICEDevelopers can quickly and easily create applications and deploy them.With S2I (Source-to-Image), a developer can even deploy his code without needing to create a container first.Operators can leverage placement and policy to orchestrate environments that meet their best practices.It makes your development and operations work fluently together when combining them in a single platform.POLYGLOT, MULTI-LANGUAGESince it deploys Docker containers, it gives you the ability to run multiple languages, frameworks and databases on the same platform.You can easily deploy microservices written in Java, Python or other languages.AUTOMATIONBuild automation:OpenShift automates the process of building new container images for all of your users.It can run standard Docker builds based on the Dockerfiles you provide and it also provides a “Source-to-Image” feature which allows you to specify the source from which to generate your images.This allows administrators to control a set of base or “builder images” and then users can layer on top of these.The build source could be a Git location, it could also be a binary like a WAR/JAR file.Users can also customize the build process and create their own S2I images.Deployment automation:OpenShift automates the deployment of application containers. It supports rolling deployments for multi-containers apps and allows you to roll back to an older version.Continuous integration:It provides built-in continuous integration capabilities with Jenkins and can also tie into your existing CI solutions.The OpenShift Jenkins image can also be used to run your Jenkins masters and slaves on OpenShift.ScaleWhen you want to start scaling your application, whether it’s from one replica to two or scale it to 2000 replicas, a lot of complexity is added.OpenShift leverages the power of containers and an incredibly powerful orchestration engine to make that happen. Containers make sure that applications are packed up in their own space and are independent from the OS, this makes applications incredibly portable and hyper scalable. OpenShift’s orchestration layer, Google’s Kubernetes, automates the scheduling and replication of these containers meaning that they’re highly available and able to accommodate whatever your users can throw at it.This means that your team spends less time in the weeds and keeping the lights on, and more time being innovative and productive.OpensourceThere are multiple versions of OpenShift (spoiler: it’s going to be the next topic in this blog post) but they are all based on OpenShift Origin.Origin provides an open source application container platform. All source code for the Origin project is available under the Apache License (Version 2.0) on GitHubOpenShift landscapeThere are a few different OpenShift releases depending on what you need.As of this writing, the OpenShift landscape looks like this:  OpenShift OriginIt’s the upstream community project used in OpenShift Online, OpenShift dedicated and OpenShift container Platform.It’s build around Docker and Kubernetes cluster management.Origin is augmented by application lifecycle management functionality and DevOps tooling.Origin updates as often as open source developers contribute via Git.Sometimes as often as several times per week.Here you get the new feature the quickest but at the cost of stability.OpenShift container platformFormerly known as OpenShift Enterprise.It’s the platform software to deploy and manage OpenShift on your own infrastructure of choice.It integrates with Red Hat Enterprise Linux 6 and is tested via Red Hat’s QA process in order to offer a stable, supportable product with may be important for enterprises.OpenShift dedicatedOpenShift dedicated is the latest offering of OpenShift.It’s OpenShift 3 hosted on AWS and maintained by Red Hat but it is dedicated to youOpenShift onlineOpenShift Online is managed by Red Hat’s OpenShift operations team, and quickstart templates enable developers to push code with one click, helping to avoid the intricacies of application provisioning.You can view it as OpenShift delivered as a SaaS (Software as a Service)Benefits for developersBefore I show you how easy OpenShift is for a developer, let me quickly explain Source-to-Image (S2I).Let’s see how easy your life can be with the following image:  Source-to-Image (S2I) is a toolkit and workflow that creates a deployable Docker image based on your source code and add it to the image registry. You don’t even need a Docker file anymore.It combines source code with a corresponding builder image from the integrated Docker registrySo now that you know S2I, let’s take a look at the next picture        Code:If you’re a developer I assume you know how to code and push it to Git, so nothing new here…        Build:The developer can push code to be built and run on OpenShift through their software version control solution or OpenShift can be integrated with a developer’s own automated build and continuous integration/continuous deployment system. Here is were S2I can get useful.        Deploy:OpenShift orchestrates where application containers will run and manages the application to ensure it’s available for end users.        Manage:With your app running in the cloud you can monitor, debug, and tune on the fly.Scale your application automatically or allocate capacity ahead of time.  A deeper lookTime to get a little bit more technical and take a deeper look at how it works.I already talked about the developer part of the picture below, so let’s focus on the rest!  InfrastructureOpenShift runs on your choice of infrastructure (Physical, Virtual, Private, Public).OpenShift uses a Software-Defined Networking (SDN) approach to provide a unified cluster network that enables communication between pods across the OpenShift cluster.This pod network is established and maintained by the OpenShift SDN, which configures an overlay network using Open vSwitch (OVS).  The OVS-subnet plug-in is the original plug-in which provides a “flat” pod network where every pod can communicate with every other pod and service.  The OVS-multitenant plug-in provides OpenShift Enterprise project level isolation for pods and services. Each project receives a unique Virtual Network ID (VNID) that identifies traffic from pods assigned to the project. Pods from different projects cannot send packets to or receive packets from pods and services of a different project.However, projects which receive VNID 0 are more privileged in that they are allowed to communicate with all other pods, and all other pods can communicate with them.In OpenShift Enterprise clusters, the default project has VNID 0.This facilitates certain services to communicate with all other pods in the cluster and vice versa.NodesA node provides the runtime environment for containers.Each node in a Kubernetes cluster has the required services to be managed by the master. OpenShift creates nodes from a cloud provider, physical systems, or virtual systems.Kubernetes interacts with node objects that are a representation of those nodes.A node is ignored until it passes the health checks, and the master continues checking nodes until they are valid.In OpenShift nodes are instances of RHEL (Redhat Enterprise Linux).PodsOpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason.OpenShift treats pods as largely immutable, changes cannot be made to a pod definition while it is running.It implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated.RegistryIntegrated OpenShift Container Registry:OpenShift Origin provides an integrated container registry called OpenShift Container Registry (OCR) that adds the ability to automatically provision new image repositories on demand.This provides users with a built-in location for their application builds to push the resulting images.Whenever a new image is pushed to OCR, the registry notifies OpenShift about the new image, passing along all the information about it, such as the namespace, name, and image metadata.Different pieces of OpenShift react to new images, creating new builds and deployments.Third Party Registries:OpenShift Origin can create containers using images from third party registries, but it is unlikely that these registries offer the same image notification support as the integrated OpenShift Origin registry.In this situation OpenShift Origin will fetch tags from the remote registry upon imagestream creation.MasterManaging data storage is a distinct problem from managing compute resources.OpenShift leverages the Kubernetes PersistentVolume subsystem, which provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed.The Kubernetes pod scheduler is responsible for determining placement of new pods onto nodes within the cluster.It reads data from the pod and tries to find a node that is a good fit based on configured policies.The Management/Replication controller manages the lifecycle of pods.For instance when you deploy a new version of your application and create a new pod, OpenShift can wait until the new pod is fully functional before downscaling the old pod leading to no downtime.But what if the master node goes down? That’s no high availability … You can optionally configure your masters for high availability to ensure that the cluster has no single point of failure.Service layerOn top of the domain and persistence layer sits the service layer of the application.A Kubernetes service can serve as an internal load balancer.It identifies a set of replicated pods in order to proxy the connections it receives to them.Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address.Persistant storageManaging storage is a distinct problem from managing compute resources. OpenShift Origin leverages the Kubernetes Persistent Volume (PV) framework to allow administrators to provision persistent storage for a cluster.Using Persistent Volume Claims (PVCs), developers can request PV resources without having specific knowledge of the underlying storage infrastructure.PVCs are specific to a project and are created and used by developers as a means to use a PV.PV resources on their own are not scoped to any single project; they can be shared across the entire OpenShift Origin cluster and claimed from any project.After a PV has been bound to a PVC, however, that PV cannot then be bound to additional PVCs. This has the effect of scoping a bound PV to a single namespace (that of the binding project).OpenShift.ioSo before ending this blog post, I have to quickly mention OpenShift.io.As of this moment, it’s not yet available but you can try to register for the preview.I haven’t had the chance to play with it, as I haven’t received my access just yet.Basically it’s an online development environment for planning, creating and deploying hybrid cloud services.It provides the following features:  Hosted, integrated toolchain  Planning tools for managing and prioritizing work  Code editing and debugging tools built on Eclipse Che  Integrated and automated CI/CD pipelines  Dashboards and reporting toolsConclusionOf course there is so much more to tell you and show about PaaS and OpenShift.I hope that with this post you got a nice introduction to OpenShift itself and some of the benefits it offers.If you enjoyed the post, I intend to write another post later this year about OpenShift, so make sure to regularly check our JWorks blog!May the PaaS be with you."
      },
    
      "architecture-2017-06-21-pragmatic-architecture-today-html": {
        "title": "Pragmatic Architecture, Today",
        "url": "/architecture/2017/06/21/pragmatic-architecture-today.html",
        "image": "/img/prag-arch/arch.png",
        "date": "21 Jun 2017",
        "category": "post, blog post, blog",
        "content": "Software development has evolved. Agile is now the de facto standard. The role of an architect in an agile project is very different from the typical role in a more classic waterfall approach. This article presents an updated interpretation of viewpoints and perspectives and will demonstrate how to make rapid, agile delivery sustainable in a constantly changing world. These viewpoints and perspectives can be linked to easy-to-produce models that can be used immediately. A good agile architect needs to strive for consensus and buy-in.Content  What?  Why?  How?What?  Architecture exists, because we want to create a system. A system is the combination of all the different components that together define an application.These components can be loosely coupled, eg. using Microservices; it can be a monolithic application or any other combination of runtime components that fulfill certain business needs.This is a different scope than a system of systems.That would be the goal of Enterprise Architecture where the focus is on the strategic vision of an enterprise.A system is built for its stakeholders. And stakeholders are diverse: the customer (who is paying for the system), the users, the developers, … I believe, sharing a crystal-clear vision with these stakeholders and getting buy-in from them, is necessary to create a successful system.Every system has an architecture, even when it is not formally defined. The architecture of a system is typically described in an Architectural Description.The architectural description documents the system for the stakeholders and needs to make architectural decisions explicit.The goal of the architectural description is to help in understanding how the system will behave.  Following the approach in the book Software Systems Architecture by Nick Rozanski and Eoin Woods, an architectural description is composed of a number views.These views describe what is architecturally significant: info that is worth writing down because the system can not be successful without it or because stakeholders say it is significant.Deciding what to put in these views, means making decisions.Woods and Rozanski identified the following viewpoints:  Context View  Functional View  Information View  Concurrency View  Development View  Deployment View  Operational ViewThese viewpoints will assist in the writing of an architectural description.The website of the book contains a nice summary of these viewpoints.The views are shaped by perspectives. These are the cross-cutting concerns that have an impact on the views. Sometimes perspectives are also called quality properties or non-functional requirements:  Accessibility  Availability and Resilience  Development resource  Evolution  Internationalisation  Location  Performance and Scalability  Regulation  Security  UsabilityAgain, summaries are available on the website of the book.If you want a more in-depth explanation, I really recommend reading the book.In today’s agile world, I believe the Evolution perspective is a key differentiator in any architectural description.Generally, perspectives shape the architecture and deserve the necessary attention.Example  This is the 2017 Mercedes F1 W08 Hybrid. It weights 702kg and has between 750 and 850 horsepower. It is made out of approximately 80.000 different components. The price of the car is an estimated 10 million Euro. That is just for the car, not for the R&amp;D that made the car possible.Looking back at the viewpoints from above, it is easy to identify how these relate to the construction of the car:  A Formula One car needs a very specific factory (Development View).It is not built in the same factory Mercedes builds its trucks.  The cars need to be transported all around the world (all the Formula One cars travel over 100.000 miles in the air).This can be documented in the Deployment view.  Maintaining a Formula One car during a race has a huge operational cost and requires a lot of coordination (Operational View).Just count the number of engineers during a pitstop.  …In the 2015 and 2016 season, the predecessors of this car won the Formula One World Championship.At the moment of writing, the 2017 car is also leading the championship.This pattern is quite common in Formula One.The older cars however, are currently up for display in a museum.They are rarely used anymore.This throw-away approach can also be noticed when comparing to other industries like smartphones or smartwatches.A lot of the success of the car, must be its architecture then.More specifically, its ability to change: to adapt to new rules, competitors and market change.If the architecture of a system, has the ability to change, it immediately has a competitive advantage.This is especially true in agile projects.  Grady Booch  Architecture represents the significant design decisions that shape a system, where significant is measured by cost of change.Often, it is very difficult to get a system right from the beginning.That is why creating a system, that has the ability to evolve, is important.Things are changing all the time: known change and unknown change.Within this evolving system, it is the responsibility of the software architect to make sure the system remains consistent.Multiple architectural patterns exist to support this:In the past, many systems were built with a configurable metamodel. Nowadays, loosely coupled, replaceable services are favoured.  When creating a 10 million Euro car, many teams (with different responsibilities) are involved.The people who design the engine are different from the people who design the brakes.Creating the best engine, creating the best brakes, … does not imply you will have the best car.Everything needs to work together.The integrity of the system is very important.This point is again proven by Formule One: other teams can buy the current Mercedes engine.They might win some races, but they haven’t won the world championship  Russell L. Ackoff  A system is more than the sum of its parts; it is an indivisible whole. It loses its essential properties when it is taken apart.To ensure system integrity, the software architect needs to be part of the project team.He must make sure that he enables the right people to collaborate on the system architecture.Being part of the team does not mean not taking responsibility.It is great to give ownership to members of the team, but in the end, the architect needs to stay accountable.When collaborating, an architect should not enforce all technical decisions.Part of working as a team, is accepting to be challenged and embracing this challenge.When team members have spirited discussions, it shows they are passionate enough about their craft to care.Disagreeing and discussing alternatives is a great way to come to a better solution and to learn from each other.Being part of the team, as an architect, will lead to a system with a consistent vision, where the implementation matches the architectural description.This also implies that an architect should be involved in the codebase of the system: writing code, performing code-reviews, doing proof-of-concepts, supporting others, …By being involved in the codebase, you can make sure that the architectual description is understood by the developers.Visual?  While code is a very important asset of a working system, code alone is not enough to have an accurate and lasting description of a system.  Grady Booch  One cannot see the ocean’s currents by studying drops of water.The goal of visually representing a system, through the architectural description, is to make sure the architecture of the system is in the stakeholders’ heads.The visual representation can be used to check for consistency, reporting, validation and sharing information.Some ground rulesWhile UML has its merits, often it is not necessary to create an extensive UML model for the architecture.It will be time-consuming and, unfortunately, it is often the case that UML is not correctly understood by stakeholders.An alternative to UML is to use plain boxes and lines.However, when using boxes and lines:  Be consistent (especially when collaborating on the architecture).Try to be consistent over multiple projects. Templates offer a good start, but not every architecture needs the same viewpoints.  Avoid mixed responsibilities.  Avoid fluffy diagrams. Documents should not be vague. They should be about one abstraction.  Always provide a legend.Explain what a certain line or box means. Don’t make stakeholders guess.  Don’t be afraid to add text to a diagram.  Don’t model what nobody needs. Eg. if you are not using a data store, do not create an Information View.  Make sure your stakeholders understand what you are documenting.Whatever your preferred visualisation approach is, keep a decision log.Document your decisions, the considered alternatives and the timing a decision was made.Since the system will (very likely) evolve, a decision log will keep track of the reasoning behind a certain decisions.Decisions might need to change, so keeping track of the rationale behind a decision is valuable.Why?  Up-front designSome up-front design is necessary to start efficiently and to prevent too much rework.This means thinking about the big picture:  Used technology  Automation  Architectural patterns  Layering  Evolution  …  Simon Brown  Just enough up-front design to create firm foundations for the software product and its delivery.But what does just enough mean?Just enough depends on a lot of variables like budget, scope, team, … The approach will also be different for greenfield projects or for existing projects.When you are working on a greenfield project, it is important to start with a high-level view of all components in the application.These components are all the pieces necessary for a system to operate.Other components and details can be added later.Working with existing systems benefits from a slightly different approach, where you can start with an accurate high-level diagram of the current architecture of the existing application.Once this diagram is available, identify the domain-of-change of the architecture: the reason people are working on the system.On top of that, adding extension points will enable evolvability.Communication  In the inception phase of a project, you will need to talk to all the different stakeholders and make sure that their desired product will be built.Aligning requirements from different stakeholders, will often be a challenge.  In the implementation phase, it is important for the team to share a technical vision.All team members need to collaborate to the same end-goal, which requires strong communication skills.Including team members in defining the technical vision is useful to make sure everybody knows how they, individually, are contributing to the technical vision on a day-to-day basis.PoliticsThe architecture of a system will have a large impact on the implementation, delivery and usage of the system.Systems generally consist of multiple parts and it is the responsibility of the architect to focus on system integrity, creating a system that has a built-in ability to respond to change.When the system lacks integrity, it will rapidly become a system nobody wants to touch.Unfortunately many enterprises have this fear of change embedded in their culture and it will take strategy and sound people skills to prevent this from happening.Influence Maps present an interesting way to map relationships between people and to visualise who influences who, in an enterprise.Being aware of these relationships might be a game-changer.How?  One way of creating an architectural description is OODA: Observe, Orient, Decide, Act.OODA can be compared with PDCA, also known as the Deming Cycle or with Discovery Activities.  Any architectural model introduces abstraction and removes noise.This model should be well-understood and feedback loops can help with this.As an example, comparing a written down version with bullet points of a certain idea, will help in verifying that the message hasn’t changed.This insight should be mapped on the model.  Observe: Observing both external and internal circumstances or dependencies of your systems.          Collect up-to-date information from different sources: stakeholders, competitors, similar systems, other viewpoints,…        Orient: Using your past experience to make sense of these observations.          Analyse the observed information and use it to update your current reality. View events, filtered through your own experiences and perceptions.        Decide: Deciding on a response, because there might be multiple alternative solutions.          Determine a course of actions.        Act: Execute the selected decision.          Follow through on your decision.      This is not a linear process. This process benefits from continuous feedback loops.Feedback loops imply that certain decisions may lead to new observations etc.The OODA process can be used as a means of creating an architectural description.Consequently, significant decisions will become part of it.Since the creation of (significant parts of) the architectural description, starts with (runtime) observations, capturing data and measuring stakeholder value will help to achieve better observations of the system."
      },
    
      "nodejs-2017-06-20-rest-api-nodejs-koa-html": {
        "title": "Creating a REST API with NodeJS, TypeScript and Koa.",
        "url": "/nodejs/2017/06/20/REST-api-NodeJS-koa.html",
        "image": "/img/nodejs-typescript-koa/koa-logo.png",
        "date": "20 Jun 2017",
        "category": "post, blog post, blog",
        "content": "  This article assumes you already have some knowledge of npm and JavaScript development in general. It will not be a detailed tutorial about how to write a REST API, it’s more of an extra explanation for the application I made, the libraries I used and my experience with them.Why?I started this little project because I wanted to be able to quickly write backends for small personal projects with little overhead. Coming from the Java backend world, I have been writing almost only JavaScript for close to 2 years now, but only frontend. I had tried NodeJS in the past for a small project with plain old JavaScript and had a very bad time. Now however, with my new experience in JavaScript, the arrival of ES6 and TypeScript, I wanted to give it another shot.What exactly did I make?The idea was to write a backend for an application called MovieListr. It’s a simple application to track movies you have watched or want to watch. The API allows you to create, delete, update and see movies and directors. A movie also has a one-to-one relation with a director.You can find the code on Github.SetupSetting up a node project with TypeScript doesn’t require a lot of effort, the following commands are enough to get started.mkdir &lt;project-name&gt;cd &lt;project-name&gt;mkdir srcnpm init //follow the setupnpm install --save-dev typescript tsc //install TypeScript and the TypeScript compilertsc --init //generates a `tsconfig.json`, a config for the TypeScript compilerThis is it, you still have to tinker with the tsconfig.json to get it to your liking, but after that you can just start writing code.Using async / awaitI want to start with talking about the async / await features. They were what really made this code so fast to write and easy to read. The async keyword marks a function that will always return a promise. The await keyword will automatically unwrap the value from the promise and continue the code when the promise has been resolved. A small example:      const promiseFn = (): Promise&lt;string&gt; =&gt; {\t    return Promise.resolve(\"Hello World\");    }     // Old way:     const asyncFn = () =&gt; {\t    promiseFn()\t\t    .then(value =&gt; {\t\t\t    console.log(value);            });    }     // With async / await     const asyncFn = async () =&gt; {\t    const text = await promiseFn();\t    console.log(text);    }You can see how readable it is with the async / await syntax. You can write asynchronous code in a synchronous way and I used it heavily everywhere in my code. I think this is one of the things that will really make writing JavaScript fun. No more callbacks, no more boilerplate code, just the important bits. For error handling you can rely on try catch statements to catch errors and act on them.To use the async / await syntax, you can have to add esnext.asynciterable to the lib array in the tsconfig.json file.The libraries I usedKoaKoa is a small node library to create REST APIs. It was made by the guys who created Express. It takes advantage of the new ES6 feature of generator functions and it allows you to write very readable code by using the async / await features (that are based on the generator functions). For a full understanding of koa and generator functions, I suggest the Koa course on Pluralsight from Hammarberg.Koa relies heavily on middleware, so for every “step” of the process we need middleware. For instance koa-bodyparser middleware will parse the request body to json, the koa-logger middleware will log all the incoming requests and the koa-router middleware will make it easy for us to configure the url mapping to certain actions. These middlewares are installed apart from the Koa framework or you can write them yourself.typescript-iocTo make testing easy, I started looking for a dependency injection framework for TypeScript. I wanted to more or less copy the way I wrote unit tests in Java, which is using dependency injection in your actual code and just creating an instance in your unit test while passing mocks instead of the dependencies. The first dependency injection framework I found, was Awilix. I got Awilix to work, and it worked quite well, but there was still a lot of boilerplate code to write to actually register the services to the container and to get it working. You can also pass folder names so it will register all the services in that folder, but I didn’t find this optimal. I was also using Webpack in the beginning (which I write about later in the article) to build my application and bundle my code, by bundling the code, the paths of the folders obviously didn’t work out anymore in the compiled code, so Awilix was no good for me. I kept searching and I found the library typescript-ioc. This library was based on annotations, so there is barely any configuration overhead and it worked much more like I was used to in Java. typescript-ioc requires you to set experimentalDecorators and emitDecoratorMetadata to true in the tsconfig.json file. You can then just write code like      import { Container } from \"typescript-ioc\";        class Foo {        doSomething(): string {        \treturn \"Hello World\";        }\t    }        class Bar {        constructor(@Inject private foo: Foo) {        }\t        \tdoAnotherThing(): string {    \t\tthis.foo.doSomething();        }    }        const bar:Bar = Container.get(Bar);    bar.doAnotherThing();typeormAt first I just saved the movies and directors in the services as an in-memory array for testing purposes, but in a real application you will want persistence of some sort, so I needed a database. I decided on a regular old MySQL database and an ORM library to do the mapping between the database records and my TypeScript model classes. For ORM I used typeorm. It’s pretty easy to use. It also uses the annotations like typescript-ioc, which makes code very readable. The experience with this library was more or less pain free, so I really recommend it. To check a real example from my repository, check the Movie model.TestingUnit testingFor unit testing I used the classic combination of Mocha, Sinon and Chai. Since I was using dependency injection, I also needed a good way of mocking my dependencies, for this I found ts-mockito. Ts-mockito is more or less a clone of the Mockito library in Java. It allows you to create mocks of classes, make functions return certain values and verify that calls have been made. This made it super easy to write tests. For examples check the tests folder in my repository. To execute the TypeScript tests, I used ts-node. Ts-node compiles the TypeScript and keeps the compiled JavaScript in memory while it executes it. This way you don’t have to create an additional folder to compile the tests to and execute them. You can then easily create an npm script like this:mocha -r ts-node/register test/**/*.spec.tsThis tells Mocha  to require the ts-node/register module (this is what the -r ts-node/register) means and then it just passes the path of the test files to it. This also worked pretty much painlessly.end-to-end testingI wanted to be able to do some real end to end testing. So I wanted to be able to spin up my application, pass some HTTP requests to it and then verify the output of the requests. The first question was how to pass the requests to my application. For this I found the library SuperTest. You can just start you Koa app and pass the HTTP server (the return value of the app.listen function) to the agent and it will make sure the app is started and you can do some requests and check the results. This worked pretty well.The second problem was a test database. I needed a database that was as close to the real one as possible. I ran the real database in a Docker container with a volume that mapped the /var/lib/mysql (the configuration / data folder for MySQL) to a host directory, so I could recreate the container without losing data. I figured I could more or less copy the Docker configuration for the database for a test database, only without the volume. Without the volume, the data would just be saved to the container itself, so it would be lost every time the container was recreated, which is perfect for end-to-end tests, because we want to start the tests with the exact same dataset, so we can make sure our assertions keep working.So I created an npm script to start the Docker and to do healthchecks to the Docker container until it told me that the entire container was up and running and MySQL was ready to take connections. Then I wrote a script to start the actual end to end tests, which was simply the same mocha call I wrote earlier, only pointing to the e2e folder instead of the test folder. At last I wrote an npm script to stop the Docker container and remove it. You can check these scripts here. I made heavy use of the shelljs package. This npm package allows you to execute shell commands, which I used to start Docker containers from JavaScript.NOTE: this setup works well, but the starting of the Docker container takes ~30 seconds, which is quite long, considering that the tests take maybe a few seconds. In a continuous integration build, this doesn’t matter as much, but when you are trying to fix tests, it does take a lot of time if you have to wait about a minute for each test run.Task runnerWebpackWhen I started this project, I was looking up some best practices for node. I came across an article that suggested you should use Webpack for backend too. I already have some experience with Webpack from frontend development, so at first it seemed logical to use it for backend too. When I was trying to get the dependency injection to work with Awilix, I realized that I could not pass any paths to libraries, because when my code was bundled, the paths would be invalid. Then I started to actually wonder why I was bundling my code. In frontend you bundle your code to make it as small as possible so you don’t waste the user’s bandwidth and make you website load faster, but in backend, that does not matter, since the code does not have to be sent anywhere. At this point I decided I didn’t need Webpack at all and I could just use npm scripts’ functionality to create tasks.npmNpm is actually the only build tool you need. If what you want to do is more than a single line command, you can just write scripts in either TypeScript (you can execute them with ts-node), bash, JavaScript, … whatever you like. I wrote my scripts in TypeScript, because to me it makes more sense to use TypeScript for everything, but I could just as well have written them using bash. Npm also gives you pre and post task hooks. So if you write a task with the name “e2e” as I did, you can also add a task with the “pre” prefix or the “post” prefix that will automatically be executed before and after the task is executed. This way I could easily separate the starting of the Docker container, the executing of the tests and the stopping of the Docker container into different scripts. I could then just execute npm run pree2e to check if my script to start the Docker worked. I really like this approach and the fact that I don’t need another tool to learn like gulp or Webpack.DebuggingApplication codeI had some trouble at the beginning with debugging my TypeScript. For some reason in the Chrome Devtools I could not get my sourcemaps working (even though they were inline sourcemaps). Then I tried the Visual Studio Code debugger and that worked much better. To get this to work, I did the following:tsconfig.json      {        \"compilerOptions\": {            \"inlineSourceMap\": true,            \"inlineSources\": true,        }    }package.json      \"scripts\": {        \t    \"start:debug\": \"ts-node --inspect=5858 --debug-brk --ignore false src/index.ts\",            }.vscode/launch.json      {\t    \"configurations\": [\t        {                \"type\": \"node\",                \"request\": \"launch\",                \"name\": \"Debug Application\",                \"runtimeExecutable\": \"npm\",                \"windows\": {                    \"runtimeExecutable\": \"npm.cmd\"                },                \"runtimeArgs\": [                    \"run-script\",                    \"start:debug\"                ],                \"outFiles\": [],                \"protocol\": \"inspector\",                \"sourceMaps\": true,                \"port\": 5858            }        ]    }The npm script will start the execution of the index.ts with ts-node in debug mode on port 5858 and the --debug-brk tells it to break on the first line of code. The launch configuration will just execute this npm script and attach it to the debugger.Test codeDebugging the test code is more or less the same as the application code, there is just a small caveat. When you create breakpoints in Visual Studio Code, they will appear gray as if they cannot be reached. But when you execute the code, it will break on the breakpoints and then they will become red like a normal breakpoint.package.json      \"scripts\": {\t    \"test:debug\": \"mocha --inspect --debug-brk --not-timeouts --compilers ts:ts-node/register test/**/*.spec.ts\",    }.vscode/launch.json  {    \"configurations\": [        {            \"type\": \"node\",            \"request\": \"launch\",            \"name\": \"Debug Tests\",            \"runtimeExecutable\": \"npm\",            \"windows\": {                \"runtimeExecutable\": \"npm.cmd\"            },            \"runtimeArgs\": [                \"run-script\",                \"test:debug\"            ],            \"outFiles\": [],            \"protocol\": \"inspector\",            \"sourceMaps\": true,            \"port\": 9229        }    ]}ConclusionI really had a good time making this project. I really love readable and compact code and with TypeScript and the async / await syntax, I really got what I asked for. My previous experience with node.js and regular old JavaScript was really bad, mostly because of the loose typing, which forces you to constantly write a lot of tedious checks on parameters. With TypeScript that is all in the past. Apart from that, the enormous amount of npm packages available, makes it very easy to find some package that does what you need. If for some reason you can’t find something, you can easily write it yourself and publish it to npm.I always used to use Java for my backends, but the setup is always a bit of work and you have to write more boilerplate code than with TypeScript. If I make more small projects in the future, I will probably use TypeScript and Node, but for me at this point, it’s hard to tell if NodeJS will hold up in bigger projects. I would assume so, since the structure for me at this point, is very similar to Java, just a more concise syntax."
      },
    
      "spring-2017-06-07-spring-io-2017-the-spring-ecosystem-html": {
        "title": "Spring IO 2017: The Spring Ecosystem",
        "url": "/spring/2017/06/07/Spring-IO-2017-The-Spring-ecosystem.html",
        "image": "/img/spring.png",
        "date": "07 Jun 2017",
        "category": "post, blog post, blog",
        "content": "  When I was at Spring IO back in May, I was intrigued by a presentation given by Jeroen Sterken.There he talked about the Spring Ecosystem in 50 minutes.Since he only had 50 minutes, he could not focus on all the projects Spring boasts.I wanted to get a feel of what the Spring team has to offer in all its glory, by getting to know all of the main projects.Jeroen Sterken (@jeroensterken) is a Java and Spring consultant from Belgium. He’s a certified Spring instructor and currently employed at Faros Belgium. His slides of his talk The Spring Ecosystem in 50 minutes can be found here.The Spring EcosystemThere are many ways to divide the Spring portfolio.One way could be based on architecture, another way could be based on popularity. Jeroen divided the Spring Ecosystem in three categories: classic, popular and other.Before we dive into the Spring ecosystem, let’s take a look at which projects our own JWorks unit have been using the most over the past two years. Here’s the JWorks top 10, beside the Spring Framework.        Spring Boot is currently at the top. Other notable mentions are Spring Session, Spring Social and Spring Cloud Data Flow.But what’s even more interesting are the Spring projects that aren’t that widely used: Spring Mobile, Spring for Android, Spring Kafka, Spring Statemachine, Spring Shell, Spring Flo and Spring XD.ClassicThe classic projects are showing a range of the many beloved portfolio projects, where for instance Spring Security and its LDAP module will help you build your secure applications at ease.Or where the Spring IO platform will show you the insights in its development.        Spring FrameworkThe core of Spring, currently at its fifth revision.It provides key components for dependency injection, web apps, transaction management, testing, messaging, model-view-controller, remote access, data access and more.Just add the modules you need and start programming.In the fifth version, the focus lays on reactive programming with reactive streams, as well as other features and revisions like support for JUnit 5.Spring 5 will require at least JDK 8 but is already being built continuously on JDK 9.The release is planned for the end of the year, regardless whether Java 9 is released or not.Spring IO PlatformThe Spring IO Platform is built on Spring Boot and is mainly used in combination with a dependency management system.It provides dependencies that work well together.It’s basically a library on the classpath of your application which gives developers production-ready features.It does this by providing a bill-of-material Maven artifact.The libraries used in the BOM file are all curated and normalized, so they work greatly together.But if that is not to your liking, you can easily just use your own versions.The platform supports JDK 7 and 8 and is still being updated frequently.Spring SecurityNowadays you can’t ignore problems of security failures and the importance of privacy.Spring Security provides your application with authentication and authorization.It will also protect your application against a handful of possible attacks.Spring Security supports many popular authentication protocols and services like OpenID, LDAP, HTTP, … and support is extended through the available third party modules.The fifth version of Spring Security will add OAuth 2.0 support.Spring LDAPSpring LDAP hides a lot of the boilerplate code for LDAP interactions.It makes sure all the connections are created and correctly closed.This library helps out with the looping through the results and filtering those.It’s also possible to manage your transactions with a client-side transaction manager.If you’re working with this Lightweight Data Access Protocol, this might definitely be worth your while.Spring IntegrationWhen an architecture revolves around events or messages, you can get the help of Spring Integration.This project focuses on the implementation of Enterprise Integration patterns.When you want to send something from point A to point B, there could be a lot of different network protocols or restrictions in between.Spring Integration minimizes the boilerplate code needed by implementing those patterns.It just makes it easy to send events and messages throughout different endpoints.Spring BatchWith Spring Batch it is possible to write an offline batch application using Java and Spring.It makes it very convenient when you’re used to the Spring Framework to execute a bunch of jobs.It features a possibility to read and write your resource and a way of dividing data for processing and much more.There is also support for a transaction manager, job processing statistics, job status changes and much more.Spring Web FlowThe Spring Web Flow was created to help users navigate through the different views of a stateful web application.A common example could be when shopping online.The process has a clear starting and finishing view, but in between, it can change state or views dynamically.Through guided navigations, the user makes changes and it should register those changes as well as the possibility to finalize those changes through a confirmation.All this is possible with Spring Web Flow.Although this project is listed with the main projects, there hasn’t been any progress over the last years, and will be removed when Spring 5 hits the shelves.Spring Web ServicesThere are several ways to develop a web service, one of which is used in combination with SOAP.Spring Web Services helps with creating contract-first SOAP web services which are flexible by manipulating the XML contents.But due to the popularity of the architectural style of REST, the interest in SOAP has diminished.This is noticeable in the maintenance of this Spring project which hasn’t had any significant version updates.Version 2.4.0 was released on August 26th 2016 and only brought some CI jobs that are built for every commit for Spring 4.2, 4.3, and 5.0.PopularWhen you look at modern applications and their infrastructure, you’ll see the power of the Spring portfolio coming to its use.With the easy of use of Spring Boot, you can quickly start the development of a secure application and use Spring Cloud to help you with the deployment and integration for your online service provider.        A modern application might look like this:        Spring BootBeing built onto the Spring Framework, the popular Spring Boot project provides an easy to use way for creating stand-alone Spring applications without code generation and configuration of XML files.If you want to get started quickly without too much hassle, Spring Boot is the way to go by adding the dependencies you need.Spring Beans don’t need to be defined in XML or Java, as they are mostly configured automatically by Spring Boot.This way, there is no need to find and configure libraries for your specific Spring version, Spring Boot tries to do that for you.However, if you wish, you can fine-tune the auto-configuration to your own needs by adding the library to the classpath of the application, setting some properties, or adding some annotations.When you want to deploy your Spring Boot application, there’s no need to build a WAR file, since you can build self-contained JAR files with an embedded servlet container such as Jetty, Tomcat or Undertow.Spring Boot also features a command line tool for quick prototyping with Spring.The easiest way to get started with Spring Boot is to go to the Spring Initializr and add the dependencies to the project.The Spring team is maintaining the Spring Boot project regularly as it’s becoming the de facto way of using Spring.Spring CloudSpring Cloud is an umbrella project which lets you build distributed systems by implementing many best practice patterns.It consists out of many sub-projects.With the use of Spring Cloud Config Server you can setup a server with a repository, like Git, as its data store and view the changes made in the configuration.Spring Cloud Contract allows you to write Consumer Driven Contract Tests with ease.Many of the Netflix OSS components are wrapped into Spring Cloud, which makes it a lot easier to deal with the complexity of microservice architectures.And of course with a cloud service there’s often a lot of security involved which is provided by the Spring Cloud Security.You can easily integrate this with Amazon Web Services or Cloud Foundry, through their related subprojects.Spring Cloud Security is build on OAuth2 and Spring Boot which provides single sign-on, token relay and token exchange.One of the latest projects in the Spring Cloud umbrella is Spring Cloud Function.It offers an extreme convention-over-configuration approach which can leverage all of Spring Boot’s capabilities while writing only a single function.The full list of sub-projects are available here.Spring Cloud Data FlowSpring Cloud Data Flow used to be know as Spring XD and is part of Spring Cloud.It’s an updated and revised toolkit for cloud-native message-driven microservices.The change was made by the Spring team after their experience with Spring Boot.Spring Cloud Data Flow is suitable for processing, analyzing and predicting data.Through streaming it can consume data from an HTTP endpoint and writes the payloads to a database of your choice.It also manages to scale the data pipelines to your liking without any interruptions.After development, an application can be easily executed in Cloud Foundry, Apache YARN, Kubernetes or Apache Mesos, but with the Service Provider Interface you can deploy your application to other runtimes.Spring DataWhether you’re working with relational or non-relational databases, Spring Data will soothe your needs.As an umbrella project it will ease your way into data access.It abstracts the complexity of data access layers by allowing the developer to simply extend an interface.Some of the related sub-projects will help you develop quicker for your favorite database, like Spring Data Mongodb, Spring Data JPA, Spring Data for Apache Cassandra or Spring Data for Apache Solr.And through the help of some community modules this is extended to several others.With Spring Data REST you can expose your Spring Data repository automatically as a REST resource.As usual with Spring projects, they provide an excellent base but can be customised to your own needs.A full list of sub-projects and community projects are available here.Spring HATEOASHATEOAS stands for Hypermedia As The Engine Of Application State.It enables the server to update its functionality by decoupling the server and client.With Spring HATEOAS it’s easy to create a REST resource implementation using the HATEOAS as an underlying principle.It helps the client by returning a response in combination with more information on what to do next.If the state of the resource changes, the information on the next steps will also vary throughout the application.As this is a subconstraint one of the core principles of REST, the uniform interface, using Spring HATEOAS you can achieve ‘the glory of REST’.Spring REST DocsWhen you develop a RESTful service, you’ll probably want to document it so it’s easy for other developers to implement your API.Spring REST Docs helps you with the documentation process to make it more accurate and readable.It does this by running integration tests, which generate guaranteed up-to-date request and response snippets when those tests succeed.Those snippets can be included in Asciidoctor templates, which are then converted to HTML output.Alternatively it can be configured to use Markdown.The advantage here is that the documentation is always up-to-date with your code, since the integration tests will fail otherwise.There are also options for you to customize the layout of the documentation.A more in-depth look at Spring REST Docs was presented at Spring IO 2016 by JWorks colleague Andreas Evers: Writing Comprehensive and Guaranteed Up-to-date REST API Documentation.Spring SocialSpring Social lets you connect your application with Facebook, Twitter and LinkedIn.But through its many community projects it’s possible to connect to dozens other like Google, Instagram, Pinterest, …The full list is of supported third-party APIs is available hereSpring SessionWhen someone uses your web application, they will be using an HTTP session underneath.Spring Session allows you to manage those sessions separately, outside of the servlet container.It supports multiple sessions at once and can even send the sessions in the header.Spring sessions isn’t specifically tied to any container.Although the project is quite popular and has very interesting features, the project hasn’t had any major changes over the past year.OtherThese projects are mainly focused on one specific (niche) part of an application.Some wil help you with the development of specific front-end applications, while others will help you implement specific patterns.        Spring AMQPAMQP is an abbreviation for Advanced Messaging Query Protocol which Spring AMQP implements.It helps you with routing, queuing, exchanging and bindings.Additionally, there’s a listener available when sending messages asynchronously.Spring AMQP also provides a template service for sending and receiving messages.In the upcoming second version of Spring AMQP it uses version 4.0.x of the library which has been developed by RabbitMQ.Spring MobileSpring Mobile is the Spring team’s attempt at making it easier to develop mobile web applications with the use of Spring MVC.Spring Mobile implements a way of detecting the type of the device used to view the url and tries to adjust its view accordingly.Unfortunately the project isn’t that well maintained as significant updates are several years ago.Spring for AndroidAnother project without any recent updates is Spring for AndroidSpring for Android brings some of the key benefits of using Spring to Google’s mobile operating system, Android.It has a REST API client for Android with authentication support.For your social media authentication, you can use Spring Social in conjunction with Spring for Android.But there’s no use of Spring’s dependency injection, transaction manager or some other useful Spring features.Spring ShellThe Spring team provided a way for building command-line applications.Through the use of Spring you could build a full-featured shell application with your very own commands or just use the default commands that are already implemented.Or you could get access to an exposed REST API.The Spring Shell hasn’t been updated with new functionality in more than 3 years.Spring XDSpring XD is the predecessor of Spring Cloud Data Flow and therefore hasn’t been maintained.End of support will be in July 2017.Spring FloThis JavaScript library was a foundation for the stream builder in Spring Cloud Data Flow.It provides a basic embeddable HTML5 visual builder.Spring Flo is especially focused on pipelines and simple graphs.It’s built using Grunt where the commands can be ran directly or indirectly through Maven.With the use of a drag and drop interface it’s easy to create real-time streaming and batch pipelines.Additionally you can also choose to use the shell instead of the GUI interface.Spring KafkaThis is Spring for Apache Kafka, an open-source streaming processing platform.Spring Kafka provides an interface for sending messages for Kafka-based applications.It also supports a listener container and a way of sending message-driven POJOs.Spring StatemachineSome applications may require state machine concepts being implemented.Spring Statemachine provides a framework that helps with that.It provides a lot of useful things for making complex configuration easy, but also provides listener states and much more.Spring RooSpring Roo gives you the possibility to easily build full Java applications.This is a tool for rapid development of Java applications that are fully written in Java.It is focused on using the new Spring projects, like Spring Boot and Spring Data, as well as other common Java technologies.However, since the introduction of Spring Boot, Spring Roo has become less of a necessity, as Spring Boot hides a lot of the boilerplate code Spring Roo was designed to generate.Spring ScalaWhen developing applications in Scala, you can make use of Spring through Spring Scala, a community project.This brings a lot of Spring technologies to the Scala programming language.This is one of the two presented community projects by the Spring team on their main project page, the other one being Spring Roo."
      },
    
      "blockchain-2017-05-10-blockchain-introduction-html": {
        "title": "Blockchain introduction",
        "url": "/blockchain/2017/05/10/Blockchain-Introduction.html",
        "image": "/img/blockchain/blockchainHeaderImagePNG.png",
        "date": "10 May 2017",
        "category": "post, blog post, blog",
        "content": "  A lot of people are talking about blockchain these days.They’re talking about blockchain as the next big thing after mainframes, computers, the internet and social networking.This introduction is the first part in a series of blockchain posts.TopicsIn this first article about the innovative blockchain technology, we’ll cover the following topics:  Blockchain and its relation to Bitcoin  What is blockchain  Types of blockchain networks  The consensus process  Smart contracts  Valid blockchain business cases  Existing platforms  Thinking decentralized  Conclusion  Recommended readingBlockchain and its relation to BitcoinFirst of all, Bitcoin and blockchain are two different things.People tend to use both words by each other in three different contexts:              1. Digital cryptocurrency                    2. Protocol and client for executing transactions        \t            3. The blockchain which stores all Bitcoin transactions    So when talking about Bitcoin or blockchain with people, it’s important to mind this terminology.Here’s a funny quote I read in the book Blockchain: A Blueprint for a New Economy,which describes this ambiguity very well:It's as if PayPal called the internet PayPal on which the PayPal protocol was run to transfer PayPal currency.In January 2009, the Bitcoin network came into existence.Bitcoin isn’t the first attempt to digital currency, but it’s the first one that uses a peer-to-peer network to create a platform for executing transactions without depending on central authorities who validates them.You should see Bitcoin as the first platform that implemented blockchain technology.What is blockchain?So forget about Bitcoin now.That’s not what this post is about.People say blockchain is as important as the introduction of the internet. The internet is a worldwide network to share information with one another, but it is far less suitable for transferring value.If you send someone a file, it is always a copy of your file, which means you and the receiver are both in possession of the file.As we already stated, that is ideal for sharing information, but not applicable for money, certificate of ownership, and so forth.And the latter is exactly what blockchain enables: digitalizing and transferring such values.Let’s take a look at the underlying decentralized ledger technology.We believe blockchain’s definition is a good starting point:  “Blockchain is a type of distributed database that stores a permanent and tamper-proof ledger of transaction data.”TL;DR versionBlockchain is a decentralized immutable data structure.In short the blockchain is a network of computers, called nodes. Every node has the same copy of the database which they manage together. A transaction is encrypted and signed on a mathematical way. When a transaction is saved in the blockchain, it is duplicated across all nodes in the network.That’s why we talk of blockchain as distributed ledger technology, a ledger of transactions, distributed across a computer network.Transactions are bundled in one block before they are validated by other nodes.Once the network reached consensus about the validity of these transactions, the block is appended to the existing chain of blocks.The block stores the validated transactions together with a hash and a reference to the previous block.Stored transactions cannot be undone, as this would invalidate all hashes in the chain.Now a little more in detail…Transactions are broadcasted to the network for miners to mine. They assess the non-validated transactions on the memory pool by solving a mathematical puzzle. A miner builds a block containing all transactions, a proof of work that the puzzle was solved (also known as the block root hash, which is also the ID of the block) and a hash to the previous block.A block also contains the following items:  A timestamp  a nonce  and a merkle root hashA merkle root does not verify transactions, but verifies a set of transactions.Transaction IDs are hashes of the transaction, and the merkle tree is constructed from these hashes.It means that if a single detail in any of the transactions changes, so does the merkle root. It also means that if the exact same transactions are listed in a different order, the merkle root will also change.So the merkle root is cryptographic proof of the transactions in the block together with the order they are in.The nonce number is a field whose value is set so that the hash of the block will comply with the predefined network rules (eg: a run of leading zeros in Bitcoin). Miners increase the nonce until the hash is valid. Sha-256 is used to hash.The miner appends the block to the blockchain. And the majority of the other nodes, 50% + 1, double-check by verifying the proof of work in the block.It sometimes occurs that miners will validate two blocks at the same time and they will be appended to the chain. When this occurs, which doesn’t happen often, the principle of Longest Chain Wins will be implemented.The longest chain remains and the conflicting chain will be discarded.The transactions of the discarded chains will be put back in the memory pool to be mined another time.You now have a basic understanding of why we call it the blockchain.  \t        Types of blockchain networksPublic blockchains (aka. permissionless)This is a blockchain that everyone in the world can view, write transactions to, expect that these transactions will be validated and added to the blockchain.In this type of blockchain network, any connected node can contribute the consensus process.This process is used to determine if a block is valid or not.You can read more about the consensus process further in this blogpost.The public blockchain is generally a complete peer-to-peer network. Its characteristics are:  The users from the chain get protected from the creators of the chain, because there are actions to the network that even they cannot perform.Developers are not the owners of the network and don’t have more or less privileges than normal users.  These chains are transparent because everyone can see what is happening inside the chain.\"In some cases, public is clearly better; in others, some degree of private control is simply necessary. As is often the case in the real world, it depends.\" - Vitalik Buterin of EthereumConsortium blockchainsIn this type of blockchain network, the consensus process is executed by a predetermined group of nodes in the network.Let’s take a consortium of fifteen financial institutions as an example, each with a node.From this group of fifteen, there are ten nodes that need to sign each block before it is valid.You could say that these ten take ownership of the data in the blockchain.They decide which transactions are valid and which ones are not.Read rights can be public or restricted to the members of the network, eg. we can limit public view to a set number of times.Public and consortium blockchain networks are decentralized, with the difference that the consortium network is not completely peer-to-peer, because not everyone is equal.Private blockchains (aka. permissioned)There is only a small difference between consortium and private blockchain networks: write rights are with one organization instead of multiple.The read rights can be the same as with a consortium blockchain.The following characteristics apply for a private blockchain network:  The company that controls the private chain can alter the rules of the chain. In some cases this can be necessary.  The nodes that confirm a block are known, so there can’t be a majority by a mining farm with a 51% attack.  Transactions are cheaper than with public chains. This is because they need to be verified by less nodes.  Nodes are well connected and errors can be fixed quickly with manual interaction.This means that these networks give a faster confirmation and they will always be faster than public networks.  Private blockchains are just better at privacy because the access to the blockchain can be limited.From a legal point-of-view, this characteristic can have significant impact on the type of blockchain network you’ll pick.The consensus processAs we mentioned before, the network must reach a consensus of 50%+1 for a transaction to be written to the blockchain.There are a few ways a blockchain network will do this.We will be discussing the two most used.Ronald Chan wrote a nice article about consensus mechanisms in Consensus Mechanisms used in blockchain.Proof-of-WorkThis is used to deter people from tampering with the blocks and launching (d)dos attacks. We let them do a feasible but not insignificant amount of work to get a consensus. For example in the blockchain they need to find the correct nonce number that is part of the block to create a hash that fits the predetermined rules. A rule can be that the hash must start with six zeros.Proof-of-StakeIn this case you don’t need to find a nonce number but you just need to proof that you have a certain stake in the network.The bigger your stake, the more you can mine from the network.Smart ContractsThe term smart contract has no clear and settled definition.So what is it?Smart contracts are traditional contracts and official documents, but written in code.As such, the contract is understandable for everyone across the globe, irrespective of the jurisdiction it is related to. Smart contracts are like If This Then That statements, only they tend to be a lot more complex.The different definitions usually fall into one of the following two categories:  Sometimes the term is used to identify a specific technology.Pieces of code that are stored, verified and executed on a blockchain.For example, a hello world program.  The term can also be used to refer to a specific application of that technology: as a complement, or substitute, for legal contracts.  \t        Valid blockchain business casesIt’s important to understand that blockchain isn’t a solution to all of your business problems.Like in any other project, you shouldn’t make critical technology decisions on hyped buzzwords.Instead you should focus on the business value it delivers.When we translate the blockchain characteristics to business values, it can potentially solve business problems in the following five key elements:  Transparency  Operation harmonization  Business continuity  Permanence  Security  DecentralizedWe’ll discuss each element in detail and explain why blockchain technology can be an answer to that business problem.TransparencyIn a public blockchain network, by default every member of the ecosystem can access all transactions stored in the chain.They can even access smart contracts.An example of improved transparency is in the supply chain. Documenting a product’s journey across the supply chain reveals its true origin and touchpoints, which increases trust and helps eliminate the bias found in today’s opaque supply chains.Manufacturers can also reduce recalls by sharing logs with OEMs and regulators.Another potential use involves the recording of patents and intellectual property. Due to blockchain’s 100% transparency and its unforgeable nature, the information cannot be altered.Because transactions are easily trackable, it’s the perfect solution for recording ownership of patents and properties.  You can only achieve 100% transparency if you setup a public, permissionless blockchain network.In a consortium- or private blockchain network, you can define access rules to say which members can query certain information, which reduces its transparent nature.Operation harmonizationBecause business logic is implemented as smart contracts, and smart contracts are replicated over the different nodes that execute them, you have decentralized business logic.This allows you to use the same open source technology in all departments of your business.As a result, business processes are joint together, in contrast to Enterprise BPM, where business logic reuse is limited due to single enterprise data silos.Business continuityBy using blockchain technology, you have less dependency on a central infrastructure.That is because all nodes can execute transactions.When one node goes down, other nodes take over the processing.You can say that in a blockchain network, you have automatic failover.PermanenceWe already talked about the fact that activities in a blockchain cannot be undone.They are immutable.Because of this characteristic, there’s an audit trail of what happened in the system.You could say that this audit trail has a lot of similarities with the architectural pattern Event Sourcing.With Event Sourcing, all changes to application state are stored as a sequence of events.This is comparable to how transactions are stored in the blockchain.It could be interesting to combine both blockchain technology and Event Sourcing principles in a project.If you want to learn more about Event Sourcing, make sure to visit the following pages:  There’s an excellent article on Martin Fowler’s blog  Our colleague Yannick De Turck also has a chapter on Event Sourcing in his blogpost about Lagom  Ken Coenen has written about CQRS and Event Sourcing too after Ordina JWorks was present at DDD Europe back in 2016  Please note that you can only achieve full immutability if you setup a public, permissionless blockchain network.In a consortium or private blockchain network, transactions can be altered because you know the nodes that validate them.SecurityBlocks are timestamped and protected with cryptographic technology that is considered unbreakable.If a block is added it can’t be removed or altered.If you change a single bit of a transaction, the hash of this transaction will be completely different. So the merkle root hash (Merkle trees are explained in the section What is blockchain) won’t be the same, the nonce number will then be wrong and the block will be considered invalid.In this way transactions are secure once chained to the blockchain.The cryptographic technology works with the principal of public and private keys, but hashing is also a part of this technology.The private key is linked to the public key, but you cannot find out the private key if you have the public key. The private key allows you to verify that you are the owner of the public key. To make transactions, you’ll need a unique key (private key) to make a digital signature to prove that you are the owner.The private key is stored in your wallet.  Your wallet doesn’t always need to contain money, it can also hold your identity.The network is also protected from (d)dos attacks because of the distributed nature of blockchain. If a hacker wants to take down the blockchain they would need to take down every node in the network. The proof-of-work can also help deter these attacks and spam because of the high costs of mining. Even if a hacker is able to penetrate one network and attempts to steal funds, there are multiple redundant copies of the same ledger stored around the world. If one is tampered with, the others could be used as a backup to prove what funds actually belong in each account.Existing platformsWe will now discuss a few platforms that can be used to set up a blockchain and also compare Bluemix and Azure.The first one is Ethereum, a public blockchain. Ethereum looks like the Bitcoin blockchain, but it uses Ether as the currency.It is faster than Bitcoin with a transaction taking seven seconds instead of ten minutes. We can also put smart contracts on the chain, with bitcoin you can only put transactions on there.Another big one we have is Hyperledger. This is a open source collaborative effort created by The Linux Foundation. Another big partner in Hyperledger is IBM because they helped them with development and donated some patents.Hyperledger is also more focused on private networks.The fun part is that you can run Hyperledger locally on your computer and try out the technology.That brings us to IBM Blockchain.IBM’s Bluemix platform focusses on private blockchains.It empowers businesses to digitize their transaction workflow through a highly secured, shared and replicated ledger.The current technology possibilities weren’t cutting it in terms of privacy so they added their code and patents to the Hyperledger project.The next one is Multichain. Multichain is an open source private blockchain, which is Bitcoin compatible.Next up is Openchain. Openchain is a little bit special because it doesn’t use the concept of blocks but the transactions are directly chained with one another, which makes it a lot faster.Openchain is an open source private blockchain. It also doesn’t use proof-of-work but proof-of-authority.BigChainDB is not really a complete blockchain but it is more a database with blockchain features like: decentralization, immutability, public/private and consensus.BigChainDB is also open source.Last but not least, we have Microsoft Azure blockchain left to discuss.As you may have guessed, Azure is the complete opposite of IBM’s Bluemix.Azure focuses on being public, although this does not mean they don’t believe in the private model.Microsoft has said that private networks will still be important for the commercial adaptation of the blockchain technology.Microsoft also don’t dedicate their platform to one type of technology like Hyperledger for Bluemix but they support many different technologies like Ethereum, Hyperledger and more. They do have a preference though for Ethereum because they joined the Enterprise Ethereum Alliance.  \t            Table: IBM is part of Hyperledger and Microsoft is part of Enterprise Ethereum Alliance.    Thinking decentralizedLast year, Ken Coenen gave a presentation about the popularity of APIs, and how companies team up to create innovative solutions.Data is freed from their silos and made available through APIs.It’s consumable for other departments and even other companies.However, when you think about it, all of this data is centralized and we need extra effort to expose it to other parties.When working with blockchain technology, your data is decentralized by nature.It’s funny when you think about it…Why do we want to store the data somewhere centralized in a silo and then make an extra effort to expose it?Isn’t is easier to start decentralized from the beginning and give access to the people who need it?What have we been doing all these years?We’ll give an example.All applications implement their own user profile functionality.All of this user data - your profile information - is duplicated across many companies.It’s already a big improvement that applications allow you to use another platform’s credentials.Logging in with your Facebook or Google account is becoming a habit.This gives the end user a way to minimize his/her digital footprint.Don Tapscott explains this really well in summer 2016’s TED Talk How the blockchain is changing money and business.Of course, blockchain technology is still in its early stages.It’s not even sure whether the technology will last.Although these statements are purely hypothetical, we find much food for thought in them.  \t        ConclusionWhen talking with people about the possibilities of blockchain, it quickly becomes clear that we still have a long way to go.People aren’t waiting for yet another technological revolution.Instead, we need to start small.Blockchain and distributed ledger technology in general will have to evolve naturally.Blockchain solutions like IBM Blockchain or Microsoft Azure Blockchain-as-a-Service make the technology very accessible to companies in an early stage.We believe that a private blockchain network is the best way to start for a company because of the following reasons:  Throwing all your data at the world is still a very scary idea  You have to take all legal aspects into account (think of the EU’s new General Data Protection Regulation)  You can start small and expose some transactions by defining permissionsCompanies are starting to develop applications on their proprietary Bluemix- or Azure platform, without exposing everything to the outside world.Get inspired by visiting State of the Dapps.Recommended readingYou can read the following books if you like to get a grasp on possible use cases which can be implemented using blockchain technology.Please note that neither of these books will deep dive into the technical aspects.  Blockchain: Blueprint for a New Economy by Melanie Swan  Blockchain Revolution: How the Technology Behind Bitcoin Is Changing Money, Business, and the World by Don Tapscott, Alex Tapscott"
      },
    
      "conference-2017-05-04-saturn-html": {
        "title": "SATURN 2017",
        "url": "/conference/2017/05/04/saturn.html",
        "image": "/img/saturn/denver.jpg",
        "date": "04 May 2017",
        "category": "post, blog post, blog",
        "content": "SATURN is the leading international conference for software architecture practitioners who look beyond the details of today’s technologies to the underlying trends, techniques and principles that underpin lasting success in our fast-moving field. SATURN offers a unique mix to learn, exchange ideas, and find collaborators at the leading edge of modern software architecture practice.2017 marks the 13th edition of the SATURN conference, organised by the Software Engineering Institute of Carnegie Mellon University.This edition was held at the Hilton Denver Inverness in Englewood, Colorado, set against the backdrop of the majestic Rocky Mountains.Pragmatic Architecture, Today  I was honoured to talk about Pragmatic Archtecture at SATURN 2017. I had done this talk a couple of times at developer conferences, so I was very anxious to compare the feedback to certain statements at an architecture conference.As it turns out, feedback was (surprisingly) similar:  Architects nor developers are very fond of UML. Both emphasize the importance of communicating the architecture of a system to stakeholders, in a clear and understable manner.  Virtually all architects have a developer background and want to be actively involved in code.  Architects understand the necessity to play the political game. Developers might also understand this, but prefer not to participate in these, sometimes frustrating, discussions.In general, I am very happy with this feedback and it shows that the two communities can (and have to) really work together nicely.Software is details (Kevlin Henney)On the first day of SATURN 2017, Kevlin Henney presented a very interesting keynote on details in software stating that everything is a detail, depending on the level of abstraction of a certain problem.  Marissa Mayer  Geeks are people who love something so much that all the details matter.And these details can be very important.In Architecture, abstractions are necessary to focus on specific parts of a system.Modern software systems are often too complex to grasp all at once so we restrict our attenton to a small number of the software system’s structures.To avoid the obvious discrepancy between abstraction and detail, Kevlin quoted Tom Gilb stating that Architecture is a hypothesis, that needs to be proven by implementation and measurement.This illustrates the importance of collaborating as a team.We will need to make decisions, given our current understanding of a certain problem.Implementing these decisions, might lead to new beliefs and insights for the architecture. If a plot works out exactly as first planned, we might not be working loosely enough to give room to imagination and instincts.This encourages looking at things from more than one point of view.An indepth look at event sourcing with CQRS (Sebastian von Conrad)While Event Sourcing is not new, it never gained wide adoption and often is a source of confusion.However, now that microservices really have become mainstream, Event Sourcing is back on everyone’s radar.Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states.So event sourcing offers a different way of storing information, by expressing data as events.Sebastian went to great lengths to explain that this is how the real world works.  Sebastian von Conrad  Delete sucks.Event sourcing is commonly combined with the CQRS (Command and Query Responsibility Segregation) pattern by performing the data management tasks in response to events, and by materializing views from the stored events.Zooming in on CQRS, we can use the following definitions and best practices:Reading with Queries  Clients never query the events directly.  Clients query denormalised projections that are optimised for querying.  Projections are build with projectors that process the event stream.  Projectors are decoupled from each other and don’t share any state.  Projections are cheap and easy to build and rebuild.Writing with Commands  Clients never write the events directly.  Clients express an intent to do something via commands.  If replaying gets slow, performance can be improved by snapshots.  Commands are validated by Aggreggates, which is a concept borrowed from DDD.          Aggregates fetch events from the Event Store, and replay them to reconstitute their current state.      If the Aggregate accepts the Command, it results in an event.      Event Sourcing with CQRS  Event Sourcing makes you store business facts as the source of truth.  Event Sourcing makes the system deterministic.  CQRS and the Circular Architecture work well with Event Sourcing.  Asynchronous reactors process the event stream and react to events according to business logic, outputting more events.ConclusionEvent Sourcing potentionally brings a lot of value, but it’s important not to impose Event Sourcing on a team that:  Lacks buy-in to try it  Lacks stakeholder support  Lacks intestinal fortitude  A related talk at SATURN, by Paul Rayner, provided a way to visualise large scale complexity using EventStorming. In EventStorming, developers and business experts use sticky notes to map out an event-based story of how a software system behaves.He recommended Alberto Brandolini’s book on EventStorming, to learn more on the concept.Paul’s slides are available here.How to Gain Influence as a Software Architect (Adi Levin)As a software architect, you need to deal with people: it’s important to encourage collaboration.As such, a software architect will need leadership skills and will need to know where he or she is on the leadership journey.John C. Maxwell talks about this journey to great length in his book “The five levels of leadership”.A nice summary is also available on his website.Adi shared a couple of really great tips:  Express your trust in people  Show your commitment          Never say I don’t care      Share responsiblity        Admit your mistakes  Let people know you understand them          Seek first to understand, then to be understood      Acknowledge people’s position      Encourage others to contribute to the design      Listen to people who disagree      "
      },
    
      "conference-2017-04-19-dockercon-linuxkit-and-moby-html": {
        "title": "DockerCon 2017: LinuxKit and Moby",
        "url": "/conference/2017/04/19/DockerCon-LinuxKit-And-Moby.html",
        "image": "/img/dockercon2017/thumbnail.jpg",
        "date": "19 Apr 2017",
        "category": "post, blog post, blog",
        "content": "Batteries included, but swappable.That has always been the philosophy of Docker.Since the incubation of Docker four years ago,the project has undergone many evolutions.Over the years, it has split up parts of Docker into smaller reusable components,which moved to their own projects.Docker, instead of being just one project,can now be considered a composition of multiple projects.We got runC, VPNKit, containerd, SwarmKit, InfraKit and so on.These projects are now used by many other projects other than Docker.Docker Inc. now open-sourced two new projects,called LinuxKit and The Moby ProjectLinuxKitLinuxKit is a toolkit to create small, lean Linux subsystems.The difference with other Linux distributions is the fact that you can create a distribution that only contains exactly what is needed.All system services are containers and can be removed or replaced at will.Docker partnered with multiple companies like Intel, HPE, ARM, IBM and Microsoft and the Linux Foundation,to create this new component.The minimal image size is only 35MB!These portable distributions can be used to run Linux on platforms that do not support Linux out-of-the-box.For example,LinuxKit is now being used to run Linux containers on Windows Server,using Hyper-V isolation techniques.This means you can run both Windows and Linux containers side-by-side,and create Linux/Windows hybrid clusters!The Moby ProjectUsers have been asking for the Docker-native experience on their favorite platform.These requests have not gone unheard.We received Docker for Mac, Docker for Windows, Docker for AWS and Docker for Azure.All these tools are built by composing the same open components that are used for Docker.Docker now, in total, has a library of over 80 containerized components.The problem here is,a lot of work is duplicated,to compose all these components together.Each project has its own assembly system.To fix this problem,Docker Inc. looked at the automotive industry,and copied the idea of common assemblies.Just like cars can be completely different,they can share the same chassis.And this is how the Moby project was born.It attempts to bring a set of standards and best practices together.Instead of spending months of work tying all these loose components together,you can now build a tool with the components you need within a few hours.Docker stays true to its battery philosophy.You can choose which version of the kernel you want,you can choose to use which components you want.It is all up to you!An example shown at DockerCon was “RedisOS”.They composed LinuxKit, containerd and Redis,and exported it to different formats that can run on Mac, Windows and GCP.Since these distributions are so small and portable,it is possible for companies to use this for IoT, cloud, desktop and many more platforms.You can find some examples in the LinuxKit examples repository.Trying it outI wanted to try it out myself,so I went to the LinuxKit Github repository and cloned it.$ git clone git@github.com:linuxkit/linuxkit.gitThen I ran make to build the moby tool:$ makeThis created the moby tool in the bin directory.I added this to my PATH:$ export PATH=$PATH:$(pwd)/binNow that I have the Moby tool available and on my PATH,I can build LinuxKit.Let’s take a look first at the YAML file that is used to build it.$ cat linuxkit.ymlkernel:  image: \"mobylinux/kernel:4.9.x\"  cmdline: \"console=ttyS0 console=tty0 page_poison=1\"init:  - linuxkit/init:42fe8cb1508b3afed39eb89821906e3cc7a70551  - mobylinux/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9  - linuxkit/containerd:60e2486a74c665ba4df57e561729aec20758daed  - mobylinux/ca-certificates:eabc5a6e59f05aa91529d80e9a595b85b046f935onboot:  - name: sysctl    image: \"mobylinux/sysctl:2cf2f9d5b4d314ba1bfc22b2fe931924af666d8c\"    net: host    pid: host    ipc: host    capabilities:     - CAP_SYS_ADMIN    readonly: true  - name: binfmt    image: \"linuxkit/binfmt:8881283ac627be1542811bd25c85e7782aebc692\"    binds:     - /proc/sys/fs/binfmt_misc:/binfmt_misc    readonly: true  - name: dhcpcd    image: \"linuxkit/dhcpcd:48e249ebef6a521eed886b3bce032db69fbb4afa\"    binds:     - /var:/var     - /tmp/etc:/etc    capabilities:     - CAP_NET_ADMIN     - CAP_NET_BIND_SERVICE     - CAP_NET_RAW    net: host    command: [\"/sbin/dhcpcd\", \"--nobackground\", \"-f\", \"/dhcpcd.conf\", \"-1\"]services:  - name: rngd    image: \"mobylinux/rngd:3dad6dd43270fa632ac031e99d1947f20b22eec9\"    capabilities:     - CAP_SYS_ADMIN    oomScoreAdj: -800    readonly: true  - name: nginx    image: \"nginx:alpine\"    capabilities:     - CAP_NET_BIND_SERVICE     - CAP_CHOWN     - CAP_SETUID     - CAP_SETGID     - CAP_DAC_OVERRIDE    net: hostfiles:  - path: etc/docker/daemon.json    contents: '{\"debug\": true}'trust:  image:    - mobylinux/kerneloutputs:  - format: kernel+initrd  - format: iso-bios  - format: iso-efiThis YAML file contains all the necessary information to create a distribution,that will be available in multiple formats.In this case,a random number generated and nginx are added as services.Let’s build it!$ moby build linuxkit.ymlExtract kernel image: mobylinux/kernel:4.9.xAdd init containers:Process init image: linuxkit/init:42fe8cb1508b3afed39eb89821906e3cc7a70551Process init image: mobylinux/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9Process init image: linuxkit/containerd:60e2486a74c665ba4df57e561729aec20758daedProcess init image: mobylinux/ca-certificates:eabc5a6e59f05aa91529d80e9a595b85b046f935Add onboot containers:  Create OCI config for mobylinux/sysctl:2cf2f9d5b4d314ba1bfc22b2fe931924af666d8c  Create OCI config for linuxkit/binfmt:8881283ac627be1542811bd25c85e7782aebc692  Create OCI config for linuxkit/dhcpcd:48e249ebef6a521eed886b3bce032db69fbb4afaAdd service containers:  Create OCI config for mobylinux/rngd:3dad6dd43270fa632ac031e99d1947f20b22eec9  Create OCI config for nginx:alpineAdd files:  etc/docker/daemon.jsonCreate outputs:  linuxkit-bzImage linuxkit-initrd.img linuxkit-cmdline  linuxkit.iso  linuxkit-efi.isoYou can see that a few init, onboot and service containers were added,and a configuration file was added for Docker.Finally,you can see the tool was outputted in multiple formats.Let’s try to run it:$ moby run linuxkit# ...# Lots of boot information# ...Welcome to LinuxKit                        ##         .                  ## ## ##        ==               ## ## ## ## ##    ===           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~           \\______ o           __/             \\    \\         __/              \\____\\_______// # [    2.464063] IPVS: Creating netns size=2104 id=1[    2.464434] IPVS: ftp: loaded support on port[0] = 21[    2.490221] tsc: Refined TSC clocksource calibration: 1993.943 MHz[    2.490613] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x397ba967053, max_idle_ns: 881590807276 ns[    2.713076] IPVS: Creating netns size=2104 id=2[    2.713560] IPVS: ftp: loaded support on port[0] = 21[    3.503395] clocksource: Switched to clocksource tsc/ #This image booted in just a few seconds!Now let’s see which service containers are running using runC:/ # runc listID          PID         STATUS      BUNDLE                        CREATED                        OWNERnginx       542         running     /run/containerd/linux/nginx   2017-04-19T12:34:42.1852841Z   rootrngd        601         running     /run/containerd/linux/rngd    2017-04-19T12:34:42.3200486Z   rootAs you can see,all the service containers are up and running.Within just a few minutes,I created a Linux distribution and got it up and running,with everything running in a container.If you would like to learn more about LinuxKit and the Moby Project,you can check out the following resources:  The Moby Project  Docker Blog: Anouncing LinuxKit  Docker Blog: Introducing Moby Project  GitHub: LinuxKit  GitHub: Moby"
      },
    
      "conference-2017-04-18-dockercon-multi-stage-builds-and-more-html": {
        "title": "DockerCon 2017: Multi-Stage Builds and More",
        "url": "/conference/2017/04/18/DockerCon-Multi-Stage-Builds-And-More.html",
        "image": "/img/dockercon2017/thumbnail.jpg",
        "date": "18 Apr 2017",
        "category": "post, blog post, blog",
        "content": "DockerCon 2017 has kicked off, and Docker has changed a great deal since last year’s edition.The authors of Docker have constantly stated that they wish to make the Docker experience as simple as possible.Nothing is less true if you look at some of the new features they released in the last few months,which are being presented now at DockerCon.I have compiled a list of the most useful changes and features.These will certainly help you when building your own Docker images!Multi-Stage BuildsBuilding an image is often done in multiple stages.First you compile your application.Then you run your tests.When the tests succeed, you package it into an artifact.Finally, you add this artifact to an image.You could put all these steps within one Dockerfile,but that would result into an image that is bloated with stuff that is not required for the final product, like the compilation and build frameworks.The Docker images would also be huge!A solution to this problem is to build the application outside of Docker,or to use multiple Dockerfiles.You can build the artifact with one build,extract the artifact,and use that artifact for a final build.However,this whole build process is often tied together with a script that has been hacked together,and does not truly feel like the Docker way of doing things.Docker has often been sceptical about adding new features or making changes to the Dockerfile syntax,but finally decided to tackle this build problem with a simple and elegant solution.Introducing multi-stage builds,it is now possible to define multiple stages by using several FROM statements.# First stage to build the applicationFROM maven:3.5.0-jdk-8-alpine AS build-envADD ./pom.xml pom.xmlADD ./src src/RUN mvn clean package# Final stage to define our minimal runtimeFROM FROM openjdk:8-jreCOPY --from=build-env target/app.jar app.jarRUN java -jar app.jarEach time FROM is used,you define which image is used for that stage,and in subsequent stages you can use the COPY --from=&lt;stage&gt; to copy artifacts from a previous stage.The final stage results in the image,which can contain the minimal runtime environment and the final artifact.Perfect!Using arguments in FROMUsing arguments isn’t a new thing with Dockerfiles.You could already use ARG statements to pass on arguments to the build process.These arguments are not persisted in the Dockerfile,and are frequently used to pass on versions,or secrets like SSH keys.Now it is also possible to use arguments in the version of the base image.ARG GO_VERSION=1.8FROM golang:${GO_VERSION}ADD . /srcWORKDIR /srcRUN go buildCMD [\"/bin/app\"]For above Dockerfile,I could build an image with another Go version!$ docker build --arg=GO_VERSION=1.7 .Cleaning up DockerA comment I often hear is that Docker takes a lot of space.This can be true,if you never clean up!Docker has added the docker system subcommand a while ago,You can use this subcommand to check the disk usage,and to free up space!The following command outputs the disk usage:$ docker system dfTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLEImages              7                   5                   1.247GB             769MB (61%)Containers          7                   2                   115.9MB             99.23MB (85%)Local Volumes       1                   1                   85.59MB             0B (0%)You can then use prune to clean up all resources that are no longer needed:$ docker system pruneWARNING! This will remove:\t- all stopped containers\t- all volumes not used by at least one container\t- all networks not used by at least one container\t- all dangling imagesAre you sure you want to continue? [y/N] yIt is also possible to prune certain subsystems:$ docker image/container/volume/network pruneWhich Ports?People often have trouble understanding or defining the published ports of a container,since the syntax can be confusing.Here’s a list of all possible formats that you can use to define which ports are published on a container:ports: - 3000 - 3000-3005 - 49100:22 - 9090-9091:8080-8081 - 127.0.0.1:8001:8080-8081 - 6060:7060/udpThis syntax is okay when using the CLI,but when you have to define a lot of them in a Compose file,it is no longer readable.To counter this issue,you can now use a more verbose format to define ports:ports:  - target: 6060    published: 7060    protocol: udpThis Volume Is Mounted Where?Just like the ports,volumes have a similar syntax.volumes:  - /var/lib/mysql  - /opt/data:/var/lib/mysql  - ./cache:/tmp/cached  - datavolume:/var/lib/mysql  - ~/configs/etc/configs/:roA verbose syntax has been added as well for volumes:volumes:  - type: bind    source: ~/configs    target: /etc/configs    read_only: trueTo be continuedThese few changes,especially the multi-stage builds,will certainly make your life as developer easier!I am curious what DockerCon has to offer more today and tomorrow!"
      },
    
      "architecture-2017-04-14-bredemeyer-html": {
        "title": "Bredemeyer: Architects Architecting Architecture",
        "url": "/architecture/2017/04/14/bredemeyer.html",
        "image": "/img/bredemeyer.jpg",
        "date": "14 Apr 2017",
        "category": "post, blog post, blog",
        "content": "Software architecture is getting a lot of attention.It looks beyond the details of today’s technologies to the underlying trends, techniques, and principles that underpin lasting success in our fast-moving field.It is critical to today’s business success; yet it requires technical, business and organizational talents and skills that warrant their own path of career development, education, and research.Earlier this month, I was lucky enough to participate in a four-day workshop organized by Bredemeyer Consulting in the Netherlands.The goal of the workshop: helping good architects become great architects.Bredemeyer Consulting intends to inspire and encourage architects to greatness, by helping them to visualize what is possible, and see how to get there.They aim to achieve this by providing the tools and techniques to help architects be successful.The training covers the essential technical tasks of architecting (architecture modeling and specification, architecture tradeoff analysis, component design and specification, etc.) as well as architectural leadership skills.Topics  Architecture  Strategy  Conceptual Architecture  Logical Architecture  Leadership1. ArchitectureWhatDefining architecture is never easy, as many different definitions are used by different organizations.Dana Bredemeyer defines architecture as a set of decisions that have multiple uses.These uses can span time, projects and places.Of course, this makes it easy to have too much or too little architecture.Finding the right balance, is difficult.To illustrate this difficulty, the following properties should be kept in mind when validating architectural decisions.An architectural description can be:  Good          Technically sound (eg. having well-defined interfaces)      Well-documented      Elegant        Right          A solution to the problem        Successful          When the system realizes value      It is up to the architect to make sure value is realized.HowDuring the architectural process, it is of extreme importance to look both at strategy and at implementation, as an architect generally moves between these two worlds.This process is comparable to the elevator approach from Gregor Hohpe.His book “37 Things One Architect Knows About IT Transformation” is  approachable to read and contains a large number of useful tips for aspiring and seasoned architects.Most certainly a recommended read.To facilitate decision-making, Dana Bredemeyer suggests using a re-reentrant discovery activity.In this method, decisions are taken early (and written down) because at an early stage it is still cheap to change them.However, when decisions become expensive (eg. impacting implementation), they should be taken at the last possible, responsible moment.The re-entrant nature of this model, facilitates change, dialogue and consensus.It is very similar to OODA (Observe, Orient, Decide, Act) and PDCA (Plan, Decide, Check, Act), also known as the Deming cycle.PrinciplesTo kickstart an architectural description, it is useful to define a strong foundation with a set of principles.These principles must be clear, unambiguous and actionable.They can not conflict with each other and must be followed.The reason to define these principles early, is to provide confidence when solving hard problems and to constrain decisions that get made numerous times.A couple of examples:  Ebay  Bredemeyer2. StrategyFor an architecture to be successful, it must support the business strategy of the system and the organization.One might define architecture as the translation of strategy into technology.An approach to achieve this is:  Clarify the business concept(s)  Brainstorm (with business stakeholders to look at possible and alternative business concepts)  Define high-level requirements  Define high-level architecture (From Conceptual Architecture to Logical Architecture)  ValidateAn architect must understand what business is trying to achieve and understand what an organization needs to be good at to realize success (or what the system needs to be good at).From idea to strategyWhen business concepts are clear, but the strategy isn’t (fully) established, shaping strategy from business concepts can be achieved by:  Writing down business concepts as a bulleted list  Translating this bulleted list into plain text  Looking at the difference between the list and the textThis will, more often than not, refine the business concepts and identify what is strategically most important.Another way to refine business concepts, is using the Business Model Canvas.3. Conceptual ArchitectureThe goal of the Conceptual Architecture, is to define components (subsystems) and the interaction between these components.The Conceptual Architecture must remain high-level, because in this phase, the architect wants to explore alternatives.Adding too much detail to the Conceptual Architecture will become expensive.White board sketching can be a useful method to determine to Conceptual Architecture: talking with the business users next to a white board and drawing the system together.Dana shared a couple of tips and tricks to increase participation from business users:  Make the diagram a bit sloppy. This will prevent participants from thinking it’s finished.  Have fun. Let the ideas flow.  Use colors and icons. Make it visual.Another tool to formalize components are CRC-R Templates. These are typically half a page narratives that define a component, its responsibility, its collaboration with other components and the rationale behind its responsibilities.The Conceptual Architecture can be used to validate the feasibility of alternatives.For example by going over use-cases or by identifying if the system can be in a state that renders the architecture invalid.4. Logical ArchitectureThe Logical Architecture details out full responsibilities per component and all the interfaces per component.It adds precision, providing a detailed blueprint from which component developers and component users can work in relative independence.These components ought to be derived from the Conceptual Architecture.By selecting the core behavior of the components and defining the data that moves between components (eg. in a sequence diagram), the architecture becomes actionable and ready for implementation.In larger systems, the sequence diagram can aid a component owner in understanding how his component lives in the bigger system.When the state of the data (moving between components) evolves, it might also be useful to draw a state diagram.A very interesting attention point to creating a logical architecture is that the value of a system is not the sum of its parts, but the sum of the interaction between the parts.This is beautifully explained by Russell Ackoff in this video on YouTube.5. LeadershipBeing successful as a leader often depends on the ability to influence others: getting from a lot of (possibly good) ideas to a shared vision: a philosophical harmony of values.Getting this buy-in from stakeholders only strengthens the importance of interaction and collaboration.The activities of a leader range from inspiring, mentoring, listening and setting directions.Settings directions means formally defining what a system must do or what a strategy wants to achieve.When a leader empowers his (or hers) team, trust will be established and more value will be realized.Passion and Discipline: Don Quixote’s Lessons for LeadershipWhy Don Quixote?What lessons can we learn from the fictional 16th-century gentleman who careered around the Spanish countryside tilting at windmills and challenging sheep to battle?  James G. March  We live in a world that emphasizes realistic expectations and clear successes.Quixote had neither.But through failure after failure, he persists in his vision and his commitment.He persists because he knows who he is. The critical concerns of leadership are not technical questions of management or power, they are fundamental issues of life.  – Source: Insights by Stanford BusinessBelieving in something can make others believe in it."
      },
    
      "angular-2017-04-04-optimising-performance-of-your-enterprise-angular-application-html": {
        "title": "Optimising performance of your Enterprise Angular Application",
        "url": "/angular/2017/04/04/optimising-performance-of-your-enterprise-angular-application.html",
        "image": "/img/optimising-performance-of-your-enterprise-angular-application.png",
        "date": "04 Apr 2017",
        "category": "post, blog post, blog",
        "content": "This blog post contains best practices that helped us optimise performance of our Enterprise Angular (v2+) Application we created for one of our clients.The project has been created in under 6 months with a dedicated team of 7 people of which 4 people are from the JWorks unit (2 front-end, 2 backend) and consists of two Angular Applications that use modules and components from a shared library.The use of Angular Universal does not apply (yet) for this project.Topics  Lazy loading  Code splitting and commons chunk plugin (webpack)  ChangeDetectionStrategy: OnPush  Reusable CSS with BEM and Sass  GZIP  AOT1. Lazy loadingLazy loading your project modules can greatly enhance performance.After each successful navigation, the router looks in its configuration for an unloaded module that it can preload.Whether it preloads a module, and which modules it preloads, depends upon the preload strategy.The Router offers two preloading strategies out of the box:  No preloading at all which is the default. Lazy loaded feature areas are still loaded on demand.  Preloading of all lazy loaded feature areas.We implemented the PreloadAllModules strategy in its default configuration, but know that it is possible to create your own custom preloading strategy.To do so, include the preloadingStrategy in your @NgModule like so:@NgModule({    ...    imports: [        RouterModule.forRoot(ROUTES, { preloadingStrategy: PreloadAllModules })    ]});And define your routes like this:{    path: 'performance',    loadChildren: 'performance.module#PerformanceModule',    canLoad: [AuthGuard] // Optional}Note that when using guards, the CanLoad guard blocks loading of feature module assets until authorised to do so.If you want to both preload a module and guard against unauthorised access, use the CanActivate guard instead.Want to get started with lazy loading?Maybe create a custom preloading strategy?Check out the talk Manfred Steyer gave at NG-BE 2016 about improving start-up performance with lazy loading or view the Angular docs.2. Code splitting and commons chunk plugin (webpack)Code splitting is one of the most compelling features of webpack.It allows you to split your code into various bundles which you can then load on demand — like when a user navigates to a matching route, or on an event from the user.This allows for smaller bundles, and allows you to control resource load prioritization, which, if used correctly, can have a major impact on your application load time.There are mainly two kinds of code splitting that can be accomplished with webpack: “Vendor code splitting” and “On demand code-splitting” (used for lazy loading).The CommonsChunkPlugin is an opt-in feature that creates a separate file (known as a chunk), consisting of common modules shared between multiple entry points.By separating common modules from bundles, the resulting chunked file can be loaded once initially, and stored in cache for later use.This results in pagespeed optimisations as the browser can quickly serve the shared code from cache, rather than being forced to load a larger bundle whenever a new page is visited.Among other optimisations the extra async commons chunk allows us to drastically improve performance by moving common modules out of the parent so that a new async-loaded additional commons chunk is used, which decreases initial load time.This is automatically downloaded in parallel when the additional chunk is downloaded.new webpack.optimize.CommonsChunkPlugin({  children: true,  // (use all children of the chunk)  async: true,  // (create an async commons chunk)});3. ChangeDetectionStrategy: OnPush3.1 The problemUnlike using a Virtual DOM, like ReactJS, Angular uses change detection to update the actual DOM presented to the user.Each component in an Angular application has its own change detector and in order to guarantee the latest data is always presented to the user, the default change detection strategy on an Angular component is set to always update.This means that any time JavaScript finishes executing, Angular will check for changes in all components.This usually works fast in small applications.However, when a component has a large subset of components (e.g.: a list with several items in which every row is presented by a component), performance may take a hit, even when (almost) nothing changes.The reason is that, due to the default change detection, Angular will also check for updates on a component when a change occurs on its siblings or ancestors or child components, while this is not needed in most cases.3.2 The solutionNext to trying to have less DOM, the solution is to use the OnPush strategy for change detection.The OnPush strategy will let the change detector run only in the following situations:  when an event handler is fired in the component  when one of its input properties changes  when you manually request the change detector to look for changes (using ChangeDetectorRef’s function markForCheck())  when a child’s change detector runs3.3 Setting up OnPushThere are two ways to set up the OnPush strategy3.3.1. Immutable input objectsThe simplest way is to use only immutable objects.   import { Component, Input, ChangeDetectionStrategy } from '@angular/core';   @Component({       selector: 'my-sub-component',       template: `{{ item.name }}`,       changeDetection: ChangeDetectionStrategy.OnPush   })   export class MySubComponent implements OnInit {       @Input() item: {name: string};       constructor() {}   }   The change detector will only run when the input property ‘item’ changes.   The key here is to update the reference to the object.   The change detector won’t run when something inside the object (e.g.: property ‘name’) changes.3.3.2. Observable input objectsAnother way is to use observables as inputs.   import { Component, Input, ChangeDetectionStrategy, ChangeDetectorRef } from '@angular/core';   @Component({       selector: 'my-sub-component',       template: `{{ myItemName }}`,       changeDetection: ChangeDetectionStrategy.OnPush   })   export class MySubComponent implements OnInit {       @Input() itemStream:Observable&lt;any&gt;;       myItemName: string;       constructor(private changeDetectorRef: ChangeDetectorRef) {}       ngOnInit() {           this.itemStream.subscribe(i =&gt; {               this.myItemName = item.name;               this.changeDetectorRef.markForCheck();           });       }   }   The change detector will run when the itemStream emits a new item.As change detection is run from top to bottom components, start by setting OnPush on the leaf components and work your way up.This allows to skip change detection in entire subtrees.4. Reusable CSS with BEM and SassSass has been an all-time favorite for writing structured and maintainable CSS for large projects.We combined this with the BEM methodology which helps to create extendable and reusable interface components.We used this approach on our project to create a style guide in the shared module that includes all working components used in the application.Once most of the components were available we could simply start including them in the modules that needed to be built.This greatly decreased the time needed to build the functional module.Things like colors, typography, utilities, etc. are bundled in separate files that can be included where needed.This prevents writing the same CSS over and over again and keeps the code base small(er).5. GZIPGzip is a file format and also a method of compressing files (making them smaller) for faster network transfers.It allows your web server to provide files with a smaller size that will be loaded faster by your browser.Compression of your files with gzip typically saves around fifty to seventy percent of the file size.You can easily enable gzip compression on your server by editing your .htaccess file:#Set to gzip all outputSetOutputFilter DEFLATE#exclude the following file typesSetEnvIfNoCase Request_URI \\.(?:exe|t?gz|zip|iso|tar|bz2|sit|rar|png|jpg|gif|jpeg|flv|swf|mp3)$ no-gzip dont-vary#include the following file typesAddType x-font/otf .otfAddType x-font/ttf .ttfAddType x-font/eot .eotAddType image/x-icon .icoAddType image/png .pngAddType image/svg+xml .svgAddOutputFilterByType DEFLATE text/plainAddOutputFilterByType DEFLATE text/htmlAddOutputFilterByType DEFLATE text/xmlAddOutputFilterByType DEFLATE text/cssAddOutputFilterByType DEFLATE application/xmlAddOutputFilterByType DEFLATE application/xhtml+xmlAddOutputFilterByType DEFLATE application/rss+xmlAddOutputFilterByType DEFLATE application/javascriptAddOutputFilterByType DEFLATE application/x-javascriptAddOutputFilterByType DEFLATE image/svg+xml#set compression levelDeflateCompressionLevel 9#Handle browser specific compression requirementsBrowserMatch ^Mozilla/4 gzip-only-text/htmlBrowserMatch ^Mozilla/4.0[678] no-gzipBrowserMatch bMSIE !no-gzip !gzip-only-text/html# Make sure proxies don't deliver the wrong contentHeader append Vary User-Agent env=!dont-vary6. AOTAt the time of writing, the application still runs using the just-in-time (JIT) compiler.But we are looking into how we can integrate AOT.JIT compilation incurs a runtime performance penalty.Views take longer to render because of the in-browser compilation step.The application is bigger because it includes the Angular compiler and a lot of library code that the application won’t actually need.Bigger apps take longer to transmit and are slower to load.Compilation can uncover many component-template binding errors.JIT compilation discovers them at runtime, which is late in the process.The ahead-of-time (AOT) compiler can catch template errors early and improve performance by compiling at build time.AOT ensures  Faster rendering  Fewer asynchronous requests  Smaller Angular framework download size  Earlier detection of template errors  Better securityConclusionWhile Angular states it’s performance driven out of the box, it is very important to optimise, where possible, especially when building a large Enterprise Angular Application.As you can see it’s not that difficult to integrate, so why wouldn’t you?Every bit of data that doesn’t end up downloading to the device of your users is a bless.Hat tip: Try to integrate these changes when setting up your project.It would be a shame to end up refactoring your code when halfway into development."
      },
    
      "ionic-2017-02-02-ordina-becomes-ionic-trusted-partner-html": {
        "title": "Ordina becomes Ionic Trusted Partner",
        "url": "/ionic/2017/02/02/Ordina-becomes-Ionic-Trusted-Partner.html",
        "image": "/img/ordina-becomes-ionic-trusted-partner.png",
        "date": "02 Feb 2017",
        "category": "post, blog post, blog",
        "content": "Ordina becomes Ionic Trusted PartnerWithin the JWorks unit of Ordina, Jan De Wilde has been following Ionic framework since 2014.Jan goes way back in the world of frontend development and Ionic allowed him to use his knowledge of JavaScript, HTML and CSS to create hybrid mobile applications.Along the way of experimenting, evangelizing Ionic within the unit and promoting Ionic at our clients we have had the opportunity to build some amazing Ionic (1 &amp; 2) applications for our clients.We strongly believe that Hybrid and Progressive Web Apps are the future and keep investing time in getting better in it and giving training to our employees.Some time ago Ionic opened up the Trusted Partner Program and after applying with a motivational letter and a description of projects we did for our clients, we have been selected as a Trusted Partner.  What does Ionic say about Trusted Partners?  Ionic Trusted Partners are certified consulting agencies that we connect with businesses looking to jumpstart their Ionic app development.  – Source: https://ionic.io/trusted-partners  Currently we work with both Ionic 1 and Ionic 2.We use technologies such as:  Firebase for realtime communication and data synchronization,  TypeScript and Webpack to organize our code,  and Karma, Jasmine and Protractor for testing.As mobile devices are increasingly used in almost all organisations, it is important for businesses to better protect these systems through Enterprise Mobility Management.By pushing Custom Device Policies to mobile devices via the EMM platform, administrators can control how these devices should behave within the organization, taking into account the already existing security rules.This can reduce the risk of data loss, unauthorized access and unapproved software installs on mobile devices with access to the company network.Mobile security is not just something for very large enterprises, but is relevant to all types of businesses.So it is important to take this into account from the very beginning.Want to work with us for your next mobile project? Or train your people to use Ionic?Contact Jan De Wilde.A selection* of our projectsProximus MyThings LoRa IoT PlatformFor Proximus, a big telecom operator in Belgium we have created a portal to manage LoRa connected IoT sensors and an Ionic 1 application for the field engineers to onboard and configure sensors. First part of the application allows the field engineer to identify and install/onboard a sensor using a QR code scanner or via manual input of the MAC address.Sensor type, company, location with lat/lng detection is provided in the application.Second part of the application is the ability to adjust or replace an existing sensor. Third part is a view where active/inactive sensors are presented together with their properties and last sent data. All data is exposed using web services created in Spring Boot and data is stored in a Mongo DB cluster and MySQL instance.This application is publicly available in the app store, but only accessible for field engineers.  Arcelor Mittal slab quality applicationFor Arcelor Mittal a steel processing company in Belgium we have created an application in Ionic 2 for quality assurance of the steel slabs that need to be processed. The application allows the quality assurer to look up a slab using a unique slab ID and check different properties such as: measurement, removal of edges, scarfing and cutting.The application integrates with web services provided by Arcelor Mittal and is privalely hosted by Arcelor Mittal.  4411 Parking Application for the city of CharleroiFor the city of Charleroi we created an Ionic 1 application that allows the parking guard to identify which vehicles are not parked according to their neighborhood subscription.The parking guard needs to identify himself in order to look up license plate numbers in the zone they are active.As a additional POC we integrated with a service that enabled the parking guard to take pictures of a parked car to identify the license plate number, car type, color, etc.This application is privately hosted by 4411.*Due to copyright, we are not able to display all Ionic projects."
      },
    
      "microservices-2017-02-01-lagom-1-2-html": {
        "title": "Lagom 1.2: What's new?",
        "url": "/microservices/2017/02/01/Lagom-1-2.html",
        "image": "/img/lagom.png",
        "date": "01 Feb 2017",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Looking back at the first version  Lagom 1.2  Message Broker API &amp; Kafka implementation  Generic read-side support, sharding support and automatic offset handling  JDBC support  Migrating from 1.0 to 1.2  Looking at the rest of our initial feedback  Lagom 1.3 preview  Conclusion  Extra resourcesLooking back at the first versionShortly after the first MVP version of Lagom was released we wrote a blogpost in which we wrote down our first impressions and in which we did an initial comparison to Spring Cloud and Netflix OSS.We also did an introduction presentation on Lagom which is available on YouTube.Lightbend let us know that they really appreciated our feedback.They definitely understood some of our remarks and suggestions and they were willing to work on some of them.One of our majors remarks was that Maven support should be added to Lagom in order to properly target Java developers.We weren’t the only ones with this feedback and Lightbend took note of it.In September 2016, Lightbend released Lagom 1.1. In this first new minor version they introduced Maven support which also includes support for running the Lagom development environment in Maven.More information about setting up a Lagom project in Maven is available in the documentation.A few months later they released Lagom 1.2 with a couple of new features which is what this blogpost will be about.We will also revisit our initial comparison against Spring Cloud and Netflix OSS.Lagom 1.2In this new minor version of Lagom, the read-side has been overhauled.Other notable additions are the following:  Message Broker API &amp; Kafka implementation  JDBC support  Generic read-side support, sharding support and automatic offset handlingNaturally, at the same time, a couple of existing issues and bugs were also resolved. A full list is available on Github.Message Broker API &amp; Kafka implementationThe introduction of message broker support is the most notable feature of Lagom 1.2.By adding message broker support, Lagom now allows both direct streaming of messages between services but also through a broker.With version 1.2, Lagom comes with out-of-the-box support for Apache Kafka, a very popular scalable message broker for building real-time data pipelines and streaming application.The message broker API has been designed to be independent of any backend meaning that support for other brokers may be added in the future.Compared to the existing Publish-Subscribe mechanism where messaging is only available intra-service, message broker communication happens between one service to many other services.Another key difference between the two is that with Publish-Subscribe it is possible that messages might get lost, for example due to network issues or a restart of a service, message broker based communication can provide at-least-once and at-most-once delivery semantics even if the subscriber is down.In Publish-Subscribe messaging, a subscriber will only receive a message after its subscription has been accepted by the Publish-Subscribe infrastructure.The message broker on the other hand will allow the subscriber to consume all the messages since the last message it has consumed, even if the subscriber was offline or down, thanks to its decoupling of services producing events from others consuming them.The Publish-Subscribe mechanism in Lagom is provided by the Akka Cluster underneath a Lagom service whereas message broker support is provided by a third party product such as Kafka.Lagom takes care of publishing, partitioning, consuming and failure handling of messaging and when executing the runAll command, Lagom automatically runs a Kafka server with you along with Zookeeper for you.Next to ServiceCall for communicating with other services mapping down onto HTTP, there now exists a Topic abstraction that represents a topic that one service publishes and that other services can consume after subscribing.In order to make use of it you need to add the Lagom Kafka Broker module to the dependencies for both the service publishing to a topic and the service subscribing to a topic.Note that Lagom Kafka Broker module requires an implementation of Lagom Persistence so you need to add either Lagom Persistence Cassandra or Lagom Persistence JDBC to the dependencies.In our simple demo project Lagom Shop we define a new method in the item-api project’s ItemService that will return a Topic of item creation events to subscribe on and we also add it to the descriptor:Topic&lt;ItemEvent&gt; createdItemsTopic();@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").withCalls(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).publishing(            Service.topic(\"createdItems\", this::createdItemsTopic)    ).withAutoAcl(true);}In the item-api project we define a new ItemEvent interface for external use in other services.An ItemEvent already exists in the item-impl project but you want this one to be solely used within the implementation project.It is considered a best practice to have a separate definition for external use as it would otherwise cause you to break clients if you would apply internal changes.@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\", defaultImpl = Void.class)@JsonSubTypes({        @JsonSubTypes.Type(ItemEvent.ItemCreated.class)})public interface ItemEvent {    UUID getId();    @JsonTypeName(\"item-created\")    final class ItemCreated implements ItemEvent {        private final UUID id;        private final String name;        private final BigDecimal price;            @JsonCreator        public ItemCreated(UUID id, String name, BigDecimal price) {            this.id = id;            this.name = name;            this.price = price;        }            @Override        public UUID getId() {...}            public String getName() {...}            public BigDecimal getPrice() {...}            @Override        public boolean equals(Object o) {...}            @Override        public int hashCode() {...}            @Override        public String toString() {...}    }}In the item-impl project we add the implementation to ItemServiceImpl that will publish all ItemCreated events to the topic:@Overridepublic Topic&lt;be.yannickdeturck.lagomshop.item.api.ItemEvent&gt; createdItemsTopic() {    return TopicProducer.singleStreamWithOffset(offset -&gt; {        return persistentEntities                .eventStream(ItemEventTag.INSTANCE, offset)                .filter(eventOffSet -&gt; eventOffSet.first() instanceof ItemCreated)                .map(this::convertItem);    });}private Pair&lt;be.yannickdeturck.lagomshop.item.api.ItemEvent, Offset&gt; convertItem(Pair&lt;ItemEvent, Offset&gt; pair) {    Item item = ((ItemCreated)pair.first()).getItem();    logger.info(\"Converting ItemEvent\" + item);    return new Pair&lt;&gt;(new be.yannickdeturck.lagomshop.item.api.ItemEvent.ItemCreated(item.getId(), item.getName(),            item.getPrice()), pair.second());}We now want to make use of this in our order-impl project.Using the injected ItemService instance we can now subscribe on the topic and act on each message.In this case we log something whenever we receive a new message.@Injectpublic OrderServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {    ...    itemService.createdItemsTopic()            .subscribe()            .atLeastOnce(Flow.fromFunction((be.yannickdeturck.lagomshop.item.api.ItemEvent item) -&gt; {                logger.info(\"Subscriber: doing something with the created item \" + item);                return Done.getInstance();            }));}As mentioned earlier, you have the option to make use of either the atLeastOnce or atMostOnceSource delivery semantic on the Subscriber instance.If we run the application, create an item and afterwards check the logs, we see that the order service is receiving messages:2016-12-23 22:04:22,337 INFO  b.y.l.i.i.ItemServiceImpl - Creating item: CreateItemRequest{name=newItem, price=15}2016-12-23 22:04:22,349 INFO  b.y.l.i.i.ItemEntity - Setting up initialBehaviour with snapshotState = Optional.empty2016-12-23 22:04:22,357 INFO  b.y.l.i.i.ItemEntity - Processed CreateItem command into ItemCreated event ItemCreated{item=Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}, timestamp=2016-12-23T21:04:22.357Z}2016-12-23 22:04:22,359 INFO  b.y.l.i.i.ItemEntity - Processed ItemCreated event, updated item state2016-12-23 22:04:22,412 INFO  b.y.l.i.i.ItemEntity - Processed GetItem command, returned item2016-12-23 22:04:22,413 INFO  b.y.l.i.i.ItemServiceImpl - Looking up item d370993c-dd75-4a88-bf8b-d9dba1820feb2016-12-23 22:04:25,472 INFO  b.y.l.i.i.ItemEventProcessor - Persisted Item Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}2016-12-23 22:04:25,776 INFO  b.y.l.i.i.ItemServiceImpl - Converting ItemEventItem{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}2016-12-23 22:04:25,883 INFO  b.y.l.o.i.OrderServiceImpl - Subscriber: doing something with the created item ItemCreated{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name='newItem', price=15}Generic read-side support, sharding support and automatic offset handlingUp until now, the read-side processor API was specific to Cassandra and required you to do the necessary offset tracking.This existing API, while still usable, has been declared depricated and instead a new specific implementation for constructing Cassandra read-sides and a more generic one for JDBC read-sides have been added.The read-side can now also be sharded by tagging persistent entity events with sharded tags.Instead of declaring only one tag, read-side processors now declare a list of them.The processing of these tags across the cluster is handled for you by Lagom.To compare the two of them, here is a sample of using a single tag:public interface ItemEvent extends Jsonable, AggregateEvent&lt;ItemEvent&gt; {        AggregateEventTag&lt;ItemEvent&gt; TAG = AggregateEventTag.of(ItemEvent.class);        @Override    default AggregateEventTag&lt;ItemEvent&gt; aggregateTag() {        return TAG;    }}And an example of using sharded tags:public interface ItemEvent extends Jsonable, AggregateEvent&lt;ItemEvent&gt; {    int NUM_SHARDS = 20;    AggregateEventShards&lt;ItemEvent&gt; TAG = AggregateEventTag.sharded(ItemEvent.class, NUM_SHARDS);    @Override    default AggregateEventShards&lt;ItemEvent&gt; aggregateTag() {        return TAG;    }}In your ReadSideProcessor you will have to override the aggregateTags() abstract method differently.When using a single tag:@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {    return TreePVector.singleton(ItemEvent.TAG);}And when using sharded tags:@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {  return ItemEvent.TAG.allTags();}Lagom now also provides automatic offset tracking, which until now, you had to do yourself in your read-side processors by explicitly loading and persisting offsets.This allows us to get rid of quite a bit of code and as less code means less bugs, this is definitely a good thing.In the section dealing with migrating to Lagom 1.2 there is a part that shows the code that we got rid of.That the existing Cassandra read-side support API still exists but it has been deprecated and might be removed in an upcoming version so it is a good idea to migrate as soon as possible.It isn’t really that much work to migrate to the new API, the exact work required is described in the Migrating from 1.0 to 1.2 section.JDBC supportJDBC support has been added to ease the introduction of Lagom into the existing organisation of potential users in order to provide support for using their existing relational database infrastructure.In Lagom 1.3.0, support for JPA will also be added which should become the preferred choice.A relational database is less preferred when setting up a non-blocking and reactive architecture but by providing support for it, it would allow potential users to start making use of the Persistent Entity API without the necessity of having to switch over to Cassandra.To make use of JDBC support you need to add the Persistence JDBC module to your project while also having to add the jar for your JDBC driver.Lagom makes use of akka-persistence-jdbc to persist entities to the database.At the time of writing only four relational databases are supported:  PostgreSQL  MySQL  Oracle  H2akka-persistence-jdbc uses Slick for mapping tables and managing asynchronous execution of JDBC calls.Slick requires you to configure it to use the right Slick profile for your database.An example of a Slick configuration in application.conf:db.default {  driver = \"org.postgresql.Driver\"  url = \"jdbc:postgresql://database.example.com/playdb\"}jdbc-defaults.slick.driver = \"slick.driver.PostgresDriver$\"A table and journal table are required by the akka-persistence-jdbc plugin and by default, Lagom is able to create these automatically for you.This automatic generation functionality can be disabled which you will probably want for any environment higher than development.lagom.persistence.jdbc.create-tables.auto = falseThe definition of the tables differ for each database, here is an example of the table definition for PostgreSQL:DROP TABLE IF EXISTS public.journal;CREATE TABLE IF NOT EXISTS public.journal (  ordering BIGSERIAL,  persistence_id VARCHAR(255) NOT NULL,  sequence_number BIGINT NOT NULL,  deleted BOOLEAN DEFAULT FALSE,  tags VARCHAR(255) DEFAULT NULL,  message BYTEA NOT NULL,  PRIMARY KEY(persistence_id, sequence_number));DROP TABLE IF EXISTS public.snapshot;CREATE TABLE IF NOT EXISTS public.snapshot (  persistence_id VARCHAR(255) NOT NULL,  sequence_number BIGINT NOT NULL,  created BIGINT NOT NULL,  snapshot BYTEA NOT NULL,  PRIMARY KEY(persistence_id, sequence_number));The definition scripts for each database are available here.If you are unaware of CQRS, Event Sourcing and the Persistent Read-Side in Lagom you could take a look at the CQRS and Event Sourcing section in our previous blogpost on Lagom.It might be a bit out of date regarding the API in Lagom but it should give you an idea.The new API for Cassandra read-side support, while very similar compared to the existing API, still differs in a few places.In the upcoming section we describe the migration process of upgrading to Lagom 1.2 and the changes we had to do to our read-side, so the required code changes can be read in that section.The API for the JDBC read-side support is rather similar compared to the Cassandra read-side support.Instead of using a CassandraSession for querying, you use a JdbcSession to retrieve a connection which in turn you will use for the execution of queries.The JDBC read-side API however, unlike the Cassandra read-side API, is not fully non-blocking which will lead to a performance difference.It could be an option to switch to Cassandra later on in the project if you want to get better performance out of it.But at least by having an API right now for these four databases it might be easier to integrate a new Lagom application within an existing architecture having these kinds of databases.Migrating from 1.0 to 1.2A migration guide is available with the steps necessary to upgrade your project to Lagom 1.2.At Ordina Belgium we streamed and recorded an introduction video on Lagom 1.0 in which we demoed a shop application.For the purpose of this blogpost, let us see how much work it is to upgrade to 1.2.First of all, we have to upgrade the Lagom version itself. Lagom 1.0 only supported sbt, so our demo is still using that.We change the version to be used in project/plugins.sbt:addSbtPlugin(\"com.lightbend.lagom\" % \"lagom-sbt-plugin\" % \"1.2.2\")For the Lagom Persistence module, Cassandra support has been pulled into its own module so you need to update the lagomJavadslPersistence dependency to lagomJavadslPersistenceCassandra in the build.sbt file.We also have to update the Scala version in build.sbt:scalaVersion in ThisBuild := \"2.11.8\"The ConductR version in project/plugins.sbt:addSbtPlugin(\"com.lightbend.conductr\" % \"sbt-conductr\" % \"2.1.16\")The descriptor() in the service interfaces needs to be updated since the .with(...) has been replaced by withCalls(...).We replace the existing code:@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").with(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).withAutoAcl(true);}With the following:@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").withCalls(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).withAutoAcl(true);}These are the necessary changes for us to successfully compile our project. Subsequently, we need to update our read-sides to make use of the new API.Starting with replacing the deprecated CassandraReadSideProcessor:public class ItemEventProcessor extends CassandraReadSideProcessor&lt;ItemEvent&gt; {With ReadSideProcessor:public class ItemEventProcessor extends ReadSideProcessor&lt;ItemEvent&gt; {Next step is to inject an instance of CassandraSession and CassandraReadSide via the constructor:private final CassandraSession session;private final CassandraReadSide readSide;@Injectpublic ItemEventProcessor(CassandraSession session, CassandraReadSide readSide) {    this.session = session;    this.readSide = readSide;}All code related to handling offsets can be deleted since Lagom now handles this for us.We delete the following:private PreparedStatement writeOffset = null; // initialized in prepareprivate void setWriteOffset(PreparedStatement writeOffset) {    this.writeOffset = writeOffset;}private CompletionStage&lt;Done&gt; prepareWriteOffset(CassandraSession session) {    logger.info(\"Inserting into read-side table item_offset...\");    return session.prepare(\"INSERT INTO item_offset (partition, offset) VALUES (1, ?)\").thenApply(ps -&gt; {        setWriteOffset(ps);        return Done.getInstance();    });}private CompletionStage&lt;Optional&lt;UUID&gt;&gt; selectOffset(CassandraSession session) {    logger.info(\"Looking up item_offset\");    return session.selectOne(\"SELECT offset FROM item_offset\")            .thenApply(                    optionalRow -&gt; optionalRow.map(r -&gt; r.getUUID(\"offset\")));}After having our ItemEventProcessor class extend from the ReadSideProcessor abstract we are prompted to implement two methods: buildHandler() and aggregateTags().aggregateTags() simply replaces aggregateTag() where as buildHandler() will contain the setup needed for our ReadSideHandler.The prepare(CassandraSession session) and defineEventHandlers(EventHandlersBuilder builder) methods that used to be overridden are now both implemented in buildHandler().The logic from the existing prepare() is split up into a setGlobalPrepare(), for creating Cassandra tables (note that these tasks should be idempotent), and a prepare(), for preparing statements, in buildHandler().We start by deleting the old prepare(CassandraSession session) and the defineEventHandlers(EventHandlersBuilder builder):@Overridepublic CompletionStage&lt;Optional&lt;UUID&gt;&gt; prepare(CassandraSession session) {    return            prepareCreateTables(session).thenCompose(a -&gt;                    prepareWriteOrder(session).thenCompose(b -&gt;                            prepareWriteOffset(session).thenCompose(c -&gt;                                    selectOffset(session))));}@Overridepublic EventHandlers defineEventHandlers(EventHandlersBuilder builder) {    logger.info(\"Setting up read-side event handlers...\");    builder.setEventHandler(ItemCreated.class, this::processItemCreated);    return builder.build();}Finally we perform the necessary refactoring to implement both buildHandler() and aggregateTags(), and we clean up the existing processing logic for the read-side:private CompletionStage&lt;List&lt;BoundStatement&gt;&gt; processItemCreated(ItemCreated event) {    BoundStatement bindWriteItem = writeItem.bind();    bindWriteItem.setUUID(\"itemId\", event.getItem().getId());    bindWriteItem.setString(\"name\", event.getItem().getName());    bindWriteItem.setDecimal(\"price\", event.getItem().getPrice());    logger.info(\"Persisted Item {}\", event.getItem());    return CassandraReadSide.completedStatements(Arrays.asList(bindWriteItem));}@Overridepublic ReadSideHandler&lt;ItemEvent&gt; buildHandler() {    CassandraReadSide.ReadSideHandlerBuilder&lt;ItemEvent&gt; builder = readSide.builder(\"item_offset\");    builder.setGlobalPrepare(() -&gt; prepareCreateTables(session));    builder.setPrepare(tag -&gt; prepareWriteItem(session));    logger.info(\"Setting up read-side event handlers...\");    builder.setEventHandler(ItemCreated.class, this::processItemCreated);    return builder.build();}@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {    return TreePVector.singleton(ItemEventTag.INSTANCE);}This leaves us with some refactoring to be done in our service implementations for registering the read-side.The sole thing that needs to be done is replacing the injected CassandraReadSide, which is now deprecated, with ReadSide.So going from:@Injectpublic ItemServiceImpl(PersistentEntityRegistry persistentEntities, CassandraReadSide readSide,                        CassandraSession db) {    this.persistentEntities = persistentEntities;    this.db = db;    persistentEntities.register(ItemEntity.class);    readSide.register(ItemEventProcessor.class);}To:@Injectpublic ItemServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,                       CassandraSession db) {    this.persistentEntities = persistentEntities;    this.db = db;    persistentEntities.register(ItemEntity.class);    readSide.register(ItemEventProcessor.class);}This concludes migrating our read-side logic to the new API.Since Lagom now supports multiple persistence backends, and not only just Cassandra, TestKit no longer starts with Cassandra enabled by default.This requires us to add a single line .withCassandra(true) to our server setup:@BeforeClasspublic static void setUp() {    server = ServiceTest.startServer(ServiceTest.defaultSetup()            .withCassandra(true)            .withConfigureBuilder);}All in all, migrating the code base to Lagom 1.2 took about fifteen minutes.Looking at the rest of our initial feedbackIn the previous blogpost on Lagom 1.0 we, like many other Java developers, made the point that only offering sbt as the build tool would repel many potential Java developers.We are glad that they addressed this quickly in the first new major version (1.1) they released.Regarding using Lagom in production without using ConductR, the bare minimum for this was to write your own service locator.Jonas Bonér started a service locator project for ZooKeeper and Consul for this purpose.Lightbend also plans to offer a free limited use evaluation license which will allow developers to start working with the Reactive Platform’s commercial features from the beginning.Including not only ConductR but other goodies such as monitoring for Lagom circuit breakers and Akka actors.In the blogpost we also wrote down several impressions compared to Pivotal’s Spring Cloud and Netflix OSS, currently the most popular choice of doing microservice architectures in Java.A strong point for Lightbend is that they want to distinct themselves from Pivotal by providing extensive commercial support if you get the Reactive Platform license.Pivotal also offers commercial support but only if you buy the commercial Pivotal Cloud Foundry.A key advantage of Lagom remains that it is non-blocking down to the core, starting from the persistence layer up to the endpoints, while Spring isn’t just yet.In the upcoming Spring Framework 5, of which a milestone version (M3) is already available, Pivotal are integrating their Spring Reactive initiative providing core reactive functionality and reactive web endpoint support.On the topic of Spring Reactor, a colleague of ours, Tom Van den Bulck, recently wrote a blogpost on Reactive Programming with Spring Reactor.In the blogpost, Tom writes about the presentation of Stephane Maldini at Ordina’s JOIN 2016 event, a small one-day conference hosted by Ordina, on reactive programming which has also been recorded and is available on YouTube. So it will be interested to see whether Spring Framework 5 allows them to catch up on being fully non-blocking and reactive, which up until now, remains a stronger point of Lagom.Lagom’s CQRS and Event Sourcing integration remain another advantage of Lagom, the out-of-the-box integration is easy to work with and they continue to improve on it, now with JDBC support and soon JPA support.In a Spring Cloud application a common solution for this is making use of the Axon Framework although it requires you to do the necessary gluing yourself.There is also the Eventuate framework written by Chris Richardson.Polyglot support is something that still lacks a bit on the side of Lagom.Spring has Sidecar for this purpose.Lagom services map down to ordinary, standard HTTP and WebSockets and as for other frameworks calling Lagom services, an integration client exists for JVM frameworks while others will have to invoke the Lagom services directly via REST.As for Lagom consuming other REST services, you define an API Service and implement the descriptor() to describe the external API. Documentation for this is currently lacking but an example with Httpbin and Slack’s Messages API exists.In order to further address the polyglot support, Lightbend is planning on implementing a solution for this probably based upon Akka Streams and Alpakka.The idea is that you should be able to generate a Lagom service interface from specifications, allowing transparent integration.Having binary coupling was another remark of ours and to address this, support for Swagger will be added in version 1.4.ConclusionWhile there are still a couple of important things in need of being addressed we believe that we can conclude that Lightbend is carefully listening to the feedback given by the community.They continue to improve Lagom with new features and to offer better user experience for the developers.At the same time, Pivotal is working on providing better reactive support with their upcoming Spring Framework 5.Lightbend and Pivotal make some nice rivals to each other which is nice since this will have a positive impact on both frameworks.Pivotal clearly still has an advantage over Lightbend due to how mature and well-known Spring is although Lagom seems to develop nicely and continues to improve on its strongest points while also trying to address weak points.We think that Lagom is worth adopting if you plan on getting a Reactive Platform license.Now that Maven support is available, a big hurdle for Java developers has disappeared.Without the Reactive Platform, development should be fine but you will probably miss the useful goodies it has to offer for running your system into production such as monitoring and service orchestration which you get all for free if you go with Spring Cloud and Netflix OSS.We are sure that using Lagom without the Reactive Platform will become more interesting given enough time for the community to come up with solutions for this.Scala developers will also be eagerly awaiting the release of 1.3 after which they can finally set their teeth into Lagom with the Scala API that it introduces.Lagom 1.3 previewThe first issue, created after Lagom was released, was the need to implement a separate Scala API.Something many users of Lightbend’s technologies were craving for.Until now it was already possible to use Scala with Lagom by using the Java API, see the following seed created by Mirco Dotta.Lightbend made work of it and at the time of writing, a release candidate for 1.3.0 has been made available which finally includes the Scala API for Lagom!Other features to be expected in 1.3.0 include JPA support and new test APIs for testing message broker integration.Extra resources  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom in Practice by Yannick De Turck  Lagom documentation  Lagom Twitter  Lagom Gitter  Lagom mailing list  Online Auction: Lagom 1.2 example"
      },
    
      "iot-2017-01-21-node-with-typescript-html": {
        "title": "Node with TypeScript",
        "url": "/iot/2017/01/21/Node-with-TypeScript.html",
        "image": "/img/node-with-typescript/node-ts.jpg",
        "date": "21 Jan 2017",
        "category": "post, blog post, blog",
        "content": "  NodeJS is a fantastic runtime to quickly and easily make projects.However as these projects tend to grow larger and larger, the shortcomings of JavaScript become more and more visible.This blog post will take a look at using TypeScript to write your Node application making it much more readable, introducing more OO like concepts whilst also making your code less error prone.NodeJS and its use cases  NodeJS has many use cases.It is an easy to pickup and use runtime.It uses Google’s V8 JavaScript engine to interpret and run JavaScript code.The user does not have to worry about threading.This is taken care off by the runtime.You write your code and make use of the many asynchronous operations provided by Node.This will take care of any multithreading for you.However, as you will read later in this blog post, making use of multiple Node instances to divide work is still possible!More on that later!Node can be used for a variety of tasks:  Small yet efficient web server  Code playground, test something quickly  Automation and tooling, instead of using ruby/python/…  IoT, Raspberry pi’s and other devices that can run Node!However, you should not use node for computationally heavy tasks!While the V8 engine is highly performant, there are other much more performant options available for computationally heavy operations!This blog post is not meant for people who have no NodeJS experience!Below are some resources for those that are new to the platform:  The main NodeJS website  The Node Package Manager  Code school intro to NodeJSThe old way, using plain JavaScript  Since NodeJS uses Google’s V8 JavaScript engine, it speaks for itself that node interprets and runs regular JavaScript code.This has some pros and cons.While it is an easy language to pick up, it can be hard to master.Javascript has always had some quirks and getting to know and how to avoid these can be tricky!It also does not require any compilation, which makes running your code very easy.However, this also removes any help from the compiler as no compile time checks are performed.No type checking, no checking for illogical structures or things that will just not work.Code for Node can be run by simple opening a command prompt or terminal window and typing    node    This will start a Node instance and present you with an interpreter.You can now type commands and press return to execute them.This can be handy to test something quickly.It is also possible to run a JavaScript file directly.This can be done via:    node path/to/javascript-file.js    However, most of the time you will not be using this way of running code.Most of the time you will use npm to install your dependencies and start the node instance:    npm install    npm start    This reads the package.json file and executes the scripts contained inside it.  Extensive documentation about the package.json file can be found on the NPM websiteTypeScript you say!?  TypeScript has been around for some years now.TypeScript is a superset of JavaScript.It uses the same syntax but adds among other things compile time type checking.It also adds a more Object Oriented model.A detailed explanation of the differences of the prototype based JavaScript and a more Object Oriented language can be found on theMozilla Developer websiteTypeScript developed mainly by Microsoft and is completely open source!This means developers can make suggestions and report bugs (and even fix these bugs if they want).  TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. Any browser. Any host. Any OS. Open source.TypeScript is very well documented and getting started with the language is fairly easy.A lot of common development tools have support for TypeScript syntax checking.These include, but are not limited to:  Intellij  Webstorm  Atom  Visual Studio Code  …As Node applications regularly use other NPM dependencies it is required for the TypeScript compiler to know about these dependencies and what types they use.You could make or generate these typings yourself.However, you can easily find these typings on TypeSearch website.The most commonly used dependencies have their typings available here!You can add the typings to the dependencies in the package.json file.    \"dependencies\": {        \"typescript\": \"2.0.8\",        \"@types/node\": \"0.0.2\",        \"@types/mime\": \"0.0.29\",        \"@types/johnny-five\": \"0.0.30\",        \"@types/serialport\": \"4.0.6\",        \"mime\": \"1.3.4\",        \"johnny-five\": \"0.10.6\",        \"serialport\": \"4.0.7\"    }    Making it all work: An exampleA few years back I started working on my own server application to host some web content and provide REST services.The code was written in JavaScript and ran on a Raspberry Pi 2 (by now a pi 3).For those of you that are interested the old code can be found on the following Github repositories:  WeatherGenie This was the initial implementation, a simple weather web application for checking the weather conditions for any city in Belgium  LoRa-IoT-Demo The second, extended iteration, based on the code from the WeatherGenie application.Because with the advent of IoT we needed a simple to extend/run/maintain solution to create IoT demos for clients.  NodeSimpleServer The third and current iteration. Written from the ground up in TypeScript and completely reworked to work better and be more maintainable.This is the application that will be detailed below!Node Simple Server: High level architecture  The Application starts in app.ts under the main src folder.This is the entry point for the application.This file contains the actual master instance code.The master instance is in charge of forking the workers and reviving them if they die.The master is also used to pass messages between the workers For this a specialized MessageHandler singleton is used.This MessageHandler instance (one per worker) is used to relay messages.The master instance itself will not execute any application logic.Its purpose is to manage the other workers and be the message bridge.    /**     * Forks the workers, there will always be one DataBroker and one IntervalWorker.     * HTTPWorker will be created based on the number of cpu cores. If less than two cores are available     * two http workers will be created.     */    private forkWorkers = (): void =&gt;{        //Fork data broker.        this.databroker = cluster.fork({name: 'broker', debug: this.isDebug});        //Fork interval worker.        this.intervalWorker = cluster.fork({name: 'interval', debug: this.isDebug});        //Fork normal server worker instances. These will handle all HTTP requests.        let cores:number                = os.cpus().length;        let numberOfHttpWorkers:number  = cores - 2 &gt; 0 ? cores - 2 : 1;        console.log('There are ' + cores + ' cores available, starting ' + numberOfHttpWorkers + ' HTTP workers...');        for (let i:number = 0; i &lt; numberOfHttpWorkers; i++) {            let worker = cluster.fork({name: 'http', debug: this.isDebug});            this.httpWorkers.push(worker);        }        //Revive workers if they die!        if(!this.isDebug) {            cluster.on('exit', this.reviveWorker);        }    };    The master will create a number of workers:  HttpWorker: Each HttpWorker is an endpoint for requests to be received.There will always be a minimum of two HttpWorkers created.If more CPU cores are available, more HttpWorkers are created.  DataBroker: For the application there is one DataBroker worker instance.This worker handles CRUD operations for data (for now in memory only).  IntervalWorker: For the application there is one IntervalWorker instance.This worker can run code periodically and is used to connect to other devices such as Arduino’s and the Raspberry Pi I/O pins.These workers are created by a WorkerFactory, as the master forks new Node instances, a process variable is set, the factory uses this to see which type the node instance should become.Each type of worker instance implements the basic NodeWorker interface.Each implementation will be detailed below.Handling HTTP requests: The HttpWorkerEach HttpWorker instance will create a Server instance.This instance will be used to receive HTTP requests.Node will automatically load balance requests between all instances that register a server on the same port.Simply put all HttpWorkers compete for the next request, the least burdened process (depending on OS/CPU process affinity) will be given the next Http request to handle.The Server class will also register the endpoints that are known to the application and can be handled.The EndpointManager is used to register endpoints.An EndPoint has a path, a method to execute and optional parameters.A Parameter is provided with a Generic type for compile time type checking, a name which should be used in the url, a description that provides information what the parameter should contain and an optional ParameterValidator.A ParameterValidator is used to validate the Parameter at runtime.If the check fails an error is shown to the user.    /**     * Maps the default endpoints.     * Endpoints can always be added at any other location and point in time.     * This can be done by getting the instance of the EndPointManager and calling the registerEndpoint method.     */    private mapRestEndpoints = (): void =&gt; {        this.endpointManager.registerEndpoint(            new EndPoint(                '/',                GenericEndpoints.index,                null            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/endpoints',                GenericEndpoints.listEndpoints,                null            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/helloworld',                GenericEndpoints.helloworld,                [new Parameter&lt;string, null, null&gt;('name', 'string field containing the name', new HelloWorldValidatorImpl())]            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/arduino/setArduinoMethod',                ArduinoEndpoint.setArduinoMethod,                [new Parameter&lt;string, null, null&gt;('method', 'string field that contains the method used for adruino implementations', new ArduinoMethodValidatorImpl())]            )        );    };    The Server instance forwards all requests to the Router instance.As the name suggests this will perform the routing.It will see if a resource is requested or and endpoint has been called.If a resource is requested it will be served if found.If an endpoint has been called, that endpoint will be executed and passed the parameters that were entered, but only after the correct amount of parameters has been passed and they are all valid.Handling data: The DataBrokerThe DataBroker is the Node instance in the application that will save and retrieve data.For the time being it is sufficient to only have in memory ‘caches’ on which basic CRUD operations can be performed.All methods on the DataBroker are called by sending an IPCRequest with the data that needs to be saved of the instruction for what data should be retrieved.The DataBroker will reply to the original worker by sending an IPCReply with the result of the operation.The DataBroker for now only has a concept of caches.A cache has a name, type and values (of said type).Values can be retrieved, added, updated and deleted from the caches.Caches can be retrieved, added and deleted at runtime.Handling asynchronous tasks: The IntervalWorkerThe IntervalWorker as its name suggest performs tasks at a certain interval.It is also used for other asynchronous workloads, such as connecting to an Arduino and running Arduino/Raspberry pi Johhny-Five scenarios.The IntervalWorker is handy when you need for example to update the content of a cache every so often.It can also run Arduino scenarios.These are Implementations that contain logic to perform actions on the Arduino or in response to something that happens on the Arduino.The IntervalWorker picks up what type of Arduino Scenario you want to run and starts the logic.    /**     * Sets up the connection to the Arduino and starts the desired Arduino Scenario.     */    private setupArduino = (): void =&gt; {        if(this.config.arduino.enableArduino) {            if(this.config.arduino.useSerialOverJohnnyFive) {                this.arduino = new ArduinoSerial(                    this.config.arduino.serialPortName,                    this.config.arduino.serialPortBaudRate,                    new PingScenario()                );            } else {                this.arduino = new ArduinoJohnny(new BlinkScenario());            }            this.arduino.init();        } else {            console.log('Skipping arduino setup, disabled in settings!');        }    };    There are two Arduino implementations available.Both can execute a Scenario.The first and simplest implementation is the Johnny-Five Arduino implementation.This allows you to make use of the Johnny-Five framework to write dynamic code for the Arduino that can change at runtime.This is possible because it uses the StandardFirmata firmware.Johnny-Five supports a lot of components and peripherals.Their website has extensive documentation and very clear examples.Johnny-Five also supports the Raspberry PI I/O pins.This allows it to be used on a Raspberry pi also.The second Arduino implementation uses no framework and communication is done via regular serial.In the type of scenarios you have to handle all the serial communication yourself.You also have to write Arduino firmware and thus it cannot be dynamically updated at runtime.Use this Arduino implementation if some component is incompatible or not supported by Johnny-Five.Inter Process Messaging: Communicating between different Node instancesHaving all these different worker instances is quite handy.However they are of not much use if there cannot be any communication between them.Each Node instance has its own allocated memory and cannot access variables or call methods on other instances.The Node cluster and process framework provide the option to send messages between Node instances.The IPCMessage instances that are sent exist in two forms.  IPCRequest: This is the initial message that is sent to a target.  IPCReply: This is the response (if any) from the target back to the original caller.This allows for easy two way communication and identification whether the message was a reply to an earlier message.Messages can be sent with or without a callback.The callback is executed when a reply to the original message is received.Because only basic data types can be sent across Node instances the MessageManager instance of the caller stores the callback reference and generates an unique id for said callback.This allows the application to send the callback ID across Node instances and execute it when it arrives back at the caller.    /**         * MessageManager singleton class.         * This class has an array of tuples of string and Function.         * The string field is the callbackId and the Function is the actual callback.         * The message manager is a per worker instance that can only execute callbacks on the same worker.         * The integration with the IPC framework allows messages to be sent to other workers and replies to be sent back to the original worker.         * It is important that the original worker is called to execute the callback since a function cannot cross a node instance!         *         * This singleton can be used to manage IPC messages.         */        export class MessageManager {            private static instance: MessageManager         = null;            private callbacks: Array&lt;[string, Function]&gt;    = null;            private workerId: string                        = null;            /**             * Private constructor for the singleton.             */            private constructor() {                this.callbacks = [];                this.workerId = cluster.worker.id;            }            /**             * Use this method to get the instance of this singleton class.             *             * @returns {MessageManager} The instance of this singleton class.             */            public static getInstance(): MessageManager {                if(!MessageManager.instance) {                    MessageManager.instance = new MessageManager();                }                return MessageManager.instance;            }            /**             * Sends an IPCMessage of the subtype IPCRequest to the given MessageTarget (one of the three worker types).             * A target function is also given and contains the name of the function that will be executed on the target.             * The target should implement a specific handler or switch statement to handle these different target function names.             * This message is sent without a callback. This means that when the target function has finished no reply will be sent to inform the caller.             *             * @param payload The payload for the target, can be of any kind.             * @param messageTarget The MessageTarget, being one of the three types of workers.             * @param targetFunctionName The name of the function to be executed on the target. This value is NOT evaluated by eval for security reasons.             */            public sendMessage(payload: any, messageTarget: MessageTarget, targetFunctionName: string): void {                let message: IPCMessage = new IPCRequest(this.workerId, null, payload, messageTarget, targetFunctionName);                process.send(message);            }            /**             * Sends an IPCMessage of the subtype IPCRequest to the given MessageTarget (one of the three worker types).             * A target function is also given and contains the name of the function that will be executed on the target.             * The target should implement a specific handler or switch statement to handle these different target function names.             * This message is sent with a callback. The callee sends a new IPCMessage of the subtype IPCReply to inform the caller and provide it with new information if needed.             * A reply can be sent by using the sendReply method on this class.             *             * @param payload The payload for the target, can be of any kind.             * @param callback The function that should be called when a reply has been received.             * @param messageTarget The MessageTarget, being one of the three types of workers.             * @param targetFunctionName The name of the function to be executed on the target. This value is NOT evaluated by eval for security reasons.             */            public sendMessageWithCallback(payload: any, callback: Function, messageTarget: MessageTarget, targetFunctionName: string): void {                let callbackId: string = process.hrtime()  + \"--\" + (Math.random() * 6);                this.callbacks.push([callbackId, callback]);                let message: IPCMessage = new IPCRequest(this.workerId, callbackId, payload, messageTarget, targetFunctionName);                process.send(message);            }            /**             * Sends and IPCMessage of the subtype IPCReply to the sender of the original message.             *             * @param payload A new payload to provide to the original sender.             * @param originalMessage The message the sender originally sent.             */            public sendReply(payload: any, originalMessage: IPCRequest): void {                let reply: IPCMessage = new IPCReply(this.workerId, payload, originalMessage);                process.send(reply);            }            /**             * For a given callbackId execute the callback function.             *             * @param callbackId The callbackId for which to execute the callback function.             */            public executeCallbackForId(callbackId: string) :void {                for (let callbackEntry of this.callbacks) {                    if(callbackEntry[0] == callbackId) {                        callbackEntry[1]();                        return;                    }                }            }        }    &lt;br/&gt; &lt;br/&gt;    /**     * MessageHandler singleton class.     *     * This singleton can be used to handle IPC messages.     */    export class MessageHandler {        private static instance: MessageHandler         = null;        private dataBroker : cluster.Worker             = null;        private intervalWorker : cluster.Worker         = null;        private httpWorkers : Array&lt;cluster.Worker&gt;     = null;        public emitter: EventEmitter                    = null;        /**         * Private constructor for the singleton.         */        private constructor() {        }        /**         * Use this method to get the instance of this singleton class.         *         * @returns {MessageHandler} The instance of this singleton class.         */        public static getInstance(): MessageHandler {            if(!MessageHandler.instance) {                MessageHandler.instance = new MessageHandler();            }            return MessageHandler.instance;        }        /**         * Initialises the MessageHandler for being a handler for the master NodeJS process.         *         * @param dataBroker The DataBroker worker instance.         * @param intervalWorker The IntervalWorker worker instance.         * @param httpWorkers The HTTPWorker worker instance.         */        public initForMaster = (dataBroker: cluster.Worker, intervalWorker: cluster.Worker, httpWorkers: Array&lt;cluster.Worker&gt;): void =&gt; {            this.dataBroker     = dataBroker;            this.intervalWorker = intervalWorker;            this.httpWorkers    = httpWorkers;            this.emitter        = new EventEmitter();        };        /**         * Initialises the MessageHandler for being a handler for a slave (worker) NodeJS process.         */        public initForSlave = (): void =&gt; {            this.emitter        = new EventEmitter();        };        /*-----------------------------------------------------------------------------         ------------------------------------------------------------------------------         --                         MASTER MESSAGE HANDLING                          --         ------------------------------------------------------------------------------         ----------------------------------------------------------------------------*/        //TODO: Separate master and slave message handling?        /**         * Handler function for messages sent by HTTPWorkers.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by an HTTPWorker.         */        public onServerWorkerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from server worker');            this.targetHandler(msg);        };        /**         * Handler function for the messages sent by the IntervalWorker.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by the IntervalWorker.         */        public onIntervalWorkerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from interval worker');            this.targetHandler(msg);        };        /**         * Handler function for the messages sent by the DataBroker.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by the DataBroker.         */        public onDataBrokerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from data broker');            cluster.workers[msg.workerId].send(msg);        };        /**         * This method is used to direct the IPCMessage to the correct target as specified in the message.         * This handler makes a distinction between messages of the types IPCRequest and IPCReply.         *         * @param msg The IPCMessage that is to be forwarded to the correct target.         */        private targetHandler = (msg: IPCMessage) =&gt; {            if(msg.type == IPCMessage.TYPE_REQUEST) {                let m: IPCRequest = &lt;IPCRequest&gt; msg;                console.log('Master received request');                switch (m.target){                    case MessageTarget.DATA_BROKER:                        this.dataBroker.send(msg);                        break;                    case MessageTarget.INTERVAL_WORKER:                        this.intervalWorker.send(msg);                        break;                    case MessageTarget.HTTP_WORKER:                        let index: number = Math.round(Math.random() * this.httpWorkers.length) - 1;                        index = index === -1 ? 0 : index;                        this.httpWorkers[index].send(msg);                        break;                    default:                        console.error('Cannot find message target: ' + m.target);                }            } else if(msg.type == IPCMessage.TYPE_REPLY) {                let m: IPCReply = &lt;IPCReply&gt;msg;                console.log('Master received reply!');                cluster.workers[m.originalMessage.workerId].send(msg);            }        };        /*-----------------------------------------------------------------------------         ------------------------------------------------------------------------------         --                          SLAVE MESSAGE HANDLING                          --         ------------------------------------------------------------------------------         ----------------------------------------------------------------------------*/        /**         * Handler function for the messages sent by the Master NodeJS process.         * This handler makes a distinction between messages of the types IPCRequest and IPCReply.         *         * @param msg The IPCMessage as passed on by the master process.         */        public onMessageFromMasterReceived = (msg: IPCMessage): void =&gt; {            if(msg.type == IPCMessage.TYPE_REQUEST) {                let m: IPCRequest = &lt;IPCRequest&gt;msg;                console.log('[id:' + cluster.worker.id  + '] Received request from master: routing to: ' + MessageTarget[m.target] + '.' + m.targetFunction);                this.emitter.emit(MessageTarget[m.target] + '', m);            } else if(msg.type == IPCMessage.TYPE_REPLY) {                let m: IPCReply = &lt;IPCReply&gt;msg;                console.log('Slave received reply!');                MessageManager.getInstance().executeCallbackForId(m.originalMessage.callbackId);            }        };    }    Every worker has an instance of the MessageHandler, it in its turn has an event emitter on which events from the messages are broadcast.The actual worker implementations register themselves on the emitter to receive said events.In a future version the message handling should be split up, because now a single file (with an instance on each Node instance) handles both master and slave messages.Final wordsIn conclusion; It is perfectly possible to make a more complex application for NodeJS with TypeScript.By using TypeScript you gain compile time type checking and a more robust and better readable codebase.Fewer errors and strange bugs are encountered because TypeScript ‘forces’ you to write better code.The Node Simple Server application was a great way to learn the ‘new’ TypeScript language.The project is not finished, as some parts could use some more work, but it should stand as a solid starting point.Feel free to fork the codebase, submit issues or start some discussion."
      },
    
      "conference-2017-01-17-oredev2016-html": {
        "title": "Øredev 2016",
        "url": "/conference/2017/01/17/Oredev2016.html",
        "image": "/img/oredev/oredev-logo.png",
        "date": "17 Jan 2017",
        "category": "post, blog post, blog",
        "content": "When arrived in Malmö city, we were welcomed with an exploration tour of the third biggest city in Sweden.The next day Øredev 2016 officially started!It was noticeable that there was a healthy combination for everyone: Microservices, Docker, UX, Management, etc…All these buzzwords were applied in various talks.This blog post will be about the talks I favoured and the extra knowledge I gained which I now get to apply in my nowadays projects.The Overview Wizard  Dockerizing Microservices  Secure your Docker  Kubernetes  Best Practices &amp; Traps to avoid in UX  Linked data API: The future of HTTP API’s?Dockerizing MicroservicesThe Slideless Microservices session - Adam BienAdam Bien is an Expert Group member for the Java EE 6 and 7, EJB 3.X, JAX-RS, and JPA 2.X JSRs. He has worked with Java technology since JDK 1.0 and with Servlets/EJB 1.0 and is now an architect and developer for Java SE and Java EE projects. He has authored several books about JavaFX, J2EE, and Java EE, and he is the author of Real World Java EE Patterns—Rethinking Best Practices and Real World Java EE Night Hacks—Dissecting the Business Tier. Adam is also a Java Champion, Top Java Ambassador 2012, and JavaOne 2009, 2011, 2012, 2013 and 2014 Rock Star.In this session Adam guided us in how to dockerize and communicate between microservices.He says that involving the container technology to the microservices ecosystem is considered the future and inevitable.When looking into the communication between dockerized microservices, there are several ways in doing so.Before communicating, we first make the service dockerized with the help of a Dockerfile.The Dockerfile will be the configuration for your image:FROM openjdk:8-jre                     // Which base image am I using?VOLUME /tmp                            // Is where you will send data toADD service.jar app.jar                // Copies the existing jar in to the imageRUN sh -c 'touch /app.jar'             // This will execute any commands in a new layer on top of the current imageEXPOSE 42001                           // Which ports should I open at runtime?ENTRYPOINT [\"java\",\"-jar\", \"/app.jar\"] // This will allow you to configure a container that will run as an executableAfter we created the Dockerfile we will have to setup a Docker network for connecting these containers.Since Adam is doing this manually, I will explain you how to do it easier with Docker Compose.Docker Compose will run all the containers you setup in the configuration.Since we work with the microservices ecosystem, we will need a lot of containers.To build our image using the Dockerfile, we will add a Docker dependency in our pom.xml.First add a property for naming your repository:&lt;docker.image.prefix&gt;testApp&lt;/docker.image.prefix&gt;And add support for docker:&lt;plugin&gt;  &lt;groupId&gt;com.spotify&lt;/groupId&gt;  &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;  &lt;version&gt;0.4.13&lt;/version&gt;  &lt;executions&gt;    &lt;execution&gt;      &lt;phase&gt;package&lt;/phase&gt;        &lt;goals&gt;          &lt;goal&gt;build&lt;/goal&gt;        &lt;/goals&gt;    &lt;/execution&gt;  &lt;/executions&gt;  &lt;configuration&gt;    &lt;imageName&gt;${docker.image.prefix}/${project.artifactId}&lt;/imageName&gt;    &lt;dockerDirectory&gt;${project.basedir}/src/main/docker&lt;/dockerDirectory&gt;    &lt;resources&gt;      &lt;resource&gt;        &lt;targetPath&gt;/&lt;/targetPath&gt;        &lt;directory&gt;${project.build.directory}&lt;/directory&gt;        &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;      &lt;/resource&gt;    &lt;/resources&gt;  &lt;/configuration&gt;&lt;/plugin&gt;First we make images of our services using the Maven Docker plugin from Spotify.Every time Mavens builds the project, a Docker image is generated.Docker ComposeTo tell the containers that they have to be under the same network, we will use Docker Compose.First we make a docker-compose.yml file and configure our microservices:version: '2'services:  cars:    image: testApp/cars-service    ports:      - \"8081\"    networks:      - backend  gateway:      image: testApp/zuul-service      ports:        - \"9900:9900\"      networks:        - backendnetworks:  backend:Variables explanation:  image          The image you will be using to create the container, we will be using our microservice image        ports          The port the service will listen to. When you want to expose your port use: “5432:5432”.        build          This will build the Dockerfile inside the same directory        networks as property          This be the network to which you want to be assigned to        networks          This will create one or multiple networks      When you want to add an Eureka service, follow the above steps and configure it inside the docker-compose.yml file.It is important that every container, you want to communicate with, resides under the same network.Secure systems with Docker - Emil KvarnhammerEmil Kvarnhammer is a Security Software Engineer at TrueSec,a leading edge company in IT security and development.He’s been involved in several security-critical projects,developing applications and components that are used by millions of users on a regular basis.Emil has found severe security vulnerabilities in Apple OS X. His recent focus has been securing systems using Docker and Amazon EC2.When entering the world of cloud and distributed systems, security can be challenging for developers and DevOps.Docker is a part of this world and Emil talks about Docker from a security perspective.When looking at it in a secure way, what are the input sources?Where does the base image come from?If we look closer we can see that it retrieves its images from Docker Hub.In addition to that, more complexity comes in when we add new packages on top of the base image that will be retrieved from the package manager server.Finally we have the application package that can come from a CI server and, if it is a fat jar, it can consist of different dependencies through Maven or some kind.The base image is retrieved from Docker Hub every time a Docker image is being updated.A hacker could hijack the image and squeeze in malicious software without anyone noticing.Same goes for the package manager server.What could an attacker do if he gets control of the channel between the Docker image andthe host operating system?The place where the image gets instantiated or the configuration related to the instantiated image.An attacker could infect the image with his own configuration or modify the orchestration tool.The most important issue is when the attacker gets control of code execution within our application.Emil says that it happens quite often and if it isn’t in your code, it is in the third party libraries or the package manager server you’re using.Now you might think that, seeing as the attacker is in a docker container, that he is unable to break out.Think again, he can break out and gain control of the whole operating system.Building secure images  Choose base image carefully  Strategy for security patches - Regularly update your Docker container so it gets the necessary updates  Continuously rebuild and test - Security patches in your build environment  In a secure build environment  Digitally sign Docker images - # docker build -t test --disabled-content-trust=falseInternal private registry - Push and pull the containers internallyThis private registry is insecure by design.If an attacker gains control of the network where the registry is located, he can easily read and modify each image.An attacker pushes a modified image and the orchestration will pull this in.If the attacker gains control of your CI/CD server he can again push modified images to the registry.To avoid this attack, be sure to have these activated inside your registry.  Secure transfer using TLS  User authentication  Limited access rights  Docker Content Trust which will digitally sign your images  Secured development environmentHow does the attacker inject code?  You use whatever Docker image available in the registry and add your backdoor binary in a Dockerfile,then you build the image and point it to the very same image in the registry.Security measurements inside the container  User namespaces          An extra security measure are user namespaces, this will make the root in the container differentiate from the root in the host.        Remote API          If you have no good reason for using the remote API then you should disable this option.      If you have to use it, be sure to use secure transport over TLS and that you have client authentication so you don’t let the world in.        Try to enable SELinux (enforcing) on the host  Drop unused capabilities that are installed by default and install the capabilities that you actually need.          --cap-drop=ALL      --cap-add=needed_cap         Only pull from private registriesThings you should avoid in Docker  --privileged: breakout is trivial and will give you access to different machines          Be sure to check that Kubernetes is not doing this        --insecure-registry : disable TLS requirement  --net=host: use host network stackKubernetes automation in production - Paul BakkerPaul Bakker is working as a software architect for Luminis Technologies, where he’s currently leading the team that develops the CloudRTI,a Kubernetes/Docker based service that takes care of DevOps tasks like blue/green deployment, failover, centralised logging and monitoring.He is the author of “Building Modular Cloud Apps With OSGi” published by O’Reilly and an advocate for modularity in software.He is currently writing his second book for O’Reilly “Java 9 Modularity” which is expected early 2017.He’s a regular speaker at conferences speaking both about modularity/OSGi related topics, and topics related to Kubernetes and cloud architecture in general.Kubernetes is momentarily the best tool to use for orchestrating containers in a clustered production environment.In this technical deep dive you will learn the architecture and production deployment of Kubernetes.Why Kubernetes?  Run Docker in clusters          Scheduling containers on machines - Load balance and failover containers      Networking - How to communicate between containers      Storage - Use a network attached database      Automation - A commit will result in a new deployment      The ArchitectureMasterThe master component is the most important component of Kubernetes, everything that happens in the cluster is handled by the master.Generally there is one master but you can have a failover setup and work with multiple masters.For now we simply see this as a single component with an API.When a master goes down, the rest keeps running, but during that moment you won’t get any scheduling or things like that.Replication ControllerThe controller is there to configure where and how many nodes the container has to run in.It will manage scheduling and monitoring inside the cluster,so if one container crashes for some reason, the replication controller will look at how many replicas you configured and if one is down, it will schedule a new instance into the cluster.It is important that we don’t start our containers ourselves but configure the amount of replicas that are necessary.After our containers are set up, the cluster will take care of it and monitor it.Worker nodesThis is where your docker containers will run in.PodsAn abstraction on top of containers inside the worker node:  May contain multiple containers  Lifecycle of these containers bound together          Don’t place microservices into the same pod        Containers in a pod see each other on localhost  Env vars for servicesEtcdIs a key-value store and contains the configuration of the cluster.You don’t want to lose your etcd otherwise your cluster will go down.If you want fault tolerance, you will need quite a lot of machines.At least one master and three worker nodes to meet the requirements.NetworkThese pods will have to talk to each other so you will have to open ports.  We run many pods on a single machine  Pods may expose the same portsHow to avoid conflicts?  Each pod gets a virtual IP  Ports are not shared with other pods  This can cause some troubles when your process crashes and obtains a new IP address.So how do they keep communicating? Are they tracking each other? Do we have to use something like a service discovery?It would be complex if you want to go there and at the end, you don’t want to depend on those virtual IP addresses.If you want the pods to communicate, we will be using services as a proxy on top of them.ServicesService are basically a proxy on top of the pod and if you look from a technical perspective, we see configured IP tables.When we create a service, it gets a fixed IP address and a DNS name so we can communicate through our services.Finally it will round-robin traffic to the destined paths and keep track of which paths are running.Multi component deployments (microservices):  Each component deployed as a pod  Individually update and scale pods  Use services for component communication  The Frontend talks to the service of the backend pod and this will round robin the call to the right instances.NamespacesIsolated world in Kubernetes where you can have different environments inside the same cluster.If you want to run a TEST, DEV and PRD environment it’s possible with Kubernetes to keep them isolated from each other.Kubernetes in ProductionHTTP load balancingKubernetes does not have support for the following points in production so be sure to have a look at the following points when deploying to PRD.  Expose Kubernetes services to the outside world  SSL offloading  Gzip  Redirects - redirect every HTTP call to HTTPSNodePort  Exposing service ports as ports on physical nodes  Beware of port conflicts!kubectl expose deployment my-nginx --type=NodePortKubernetes Ingress  Level 7 load balancer configuration  SSL offloading  Support for GCE load balancers  Future support for extensions (not quite there yet)  At the moment useful on Google Cloud EngineUsing a custom load balancer  Use HAProxy in front of Kubernetes  Configure HAProxy dynamically  The same works for NGINX and Apache  HAProxy does SSL offloadingHow does ha-proxy know about our services?  HAProxy uses a static config file  Auto-generate it based on data in etcd  ConfdBlue-green deployment  Deployment without downtime  Only one version is active at a time  Rolls back on failed deployment  When updating our cluster we have two options:We want to update our cluster with a rolling update,which updates one pod at a time, rather than taking downthe entire service at the same time or you can do a blue-green deployment where youmake an entirely new cluster of instances and let the load balancer go from blue to green.A developer’s guide to the UX galaxy - Tess FerrandezTess Ferrandez is a full stack developer on the Microsoft Platform.She is equally happy debugging nasty backend issues as she is developing apps or working on UI/UX design.She has been blogging at her blogsite for the better part of her career, and has spoken at lots and lots of conferences around the world.Software is built into two parts, it is what we see and how we do it.In this session Tess talks about what these features should look like.If you design ‘an order’ incorrectly and the user can’t figure out to place that order, you have a bad UX design and it could cost your company a lot of money.Now, two UX researches at Microsoft deep dived into hundred years of research of signs and symbols and listened to what people define as a good interface.Finally they categorized every piece into ‘Tenets’ (attributes of good user interfaces) and ‘Traps’ (bloopers and things to avoid).Tenets: attributes of good useWe will be using a scanner as example that you use in the supermarket.Understandable  Ask yourself, what can I do? Is it understandable?If you look at the scanner in a supermarket, do you know which button you have to press to register a product?It is important to let your users understand what they are interacting with because if they have to read the manual first or ask someone for help, you’re doing something wrong.Physically effortless  Is what I’m holding quick and comfortable to use?When scanning a product you want your users to feel that they can scan the product easily and without difficulty.Responsive  You get immediate feedback, when scanning a product you get an immediate response (ex: Hey! you scanned something)If you don’t get an immediate response, the user will keep trying to scan the product and at the end he will have ten milk boxes.Efficient  What is the most efficient way in a process?The best way to buy ten milk boxes is to scan the milk ten times than to write it down ten times with your other hand.Forgiving  I can undo my actions.If I scan too many items, I have a minus button to unregister the last item.It’s a bad practice if there is no button and you have to ask an employee to help you.Discrete  Don’t over share, don’t overreact.Having a siren on the scanner to point out that you did something wrong is too much.Protective  Always deliver a qualified product with no failure or data loss.A bad practice for example is that after you just filled in a form, clicked through to the next page and tried to go back to the form to add some missing information only to find out that your form has been completely reset.Habituating  Muscle memory, make it a habit to use.When users go to your website and know where to go to buy a specific product automatically after visiting it a couple of times.Try to avoid changing your structure over and over and if you do so, make sure that you remind your users of the change in advance.Beautiful  I find it attractive compelling.Make the user think that you put a lot of effort in it.Use the same buttons, nicely aligned and use the same colors.Traps: attributes to avoidEvery ‘Tenet’ has its own set of traps, each trap is there to teach us to avoid it as much as possible.Understandable TrapsPerceptibleInvisible ElementThe user has learned about a critical hidden capability but cannot see it.NoticeableEffectively Invisible ElementA critical cue goes unseen because it is outside of the user’s vision or unexpected.DistractionSomething is drawing the user’s attention from what they need to see.Comprehensible TrapsUncomprehended ElementIt looks wrong but it is right (ex. If they have to put a sticker on it, you are doing something wrong)Inviting Dead EndIt looks right but it is wrong (ex. Having a music icon on your IPhone and you expect iTunes, but it is not iTunes)Poor GroupingA link between two or more elements is not recognized (ex. Right icon with the wrong button)Memory ChallengeAn unreasonable demand is made on the user’s memory.Confirmatory TrapsAmbiguous FeedbackThe feedback or response to a user action is unclear or absentPhysically effortless Traps - Targeting and readability  Buttons should be big enough to pressAccidental activationTrying to click one button but clicking another.Physical challengeAlways use your design on the end product.When designing a mobile form, be sure you do it on a mobile phone.Invisible element on phoneAccidental activation if shutdownResponsive Traps  Be sure your response takes half a second at most  If the response takes longer you should display a loading circle or show the progression of the processEfficiency TrapsInformation overloadHaving a page that shows you too much information, be sure to keep it as simple as possible.System amnesiaWhen they show you information that you don’t needAutocorrectThe name says it all, when autocorrect changed your word in something unintendedHabituating Traps  Doing things so many times it gets automatic and then it changes.Non redundant gratuitous redundancyToo much of the same action (ex. if you have one button that does multiple different actions on other systems)InconsistencyMessage button on an older version is on another place or it is replaced with textBeautiful TrapsUnattractive designNot appealing design where the user will not stay on the website.Best Practices  Identify common scenarios using the site  Walk through all the different ways the user can complete a certain task  Identify and log any traps you observe  If you can’t find a trap, identify the tenet  Document and discussFeed the links, tuples’ a bag – An introduction to Linked Data APIs - Sebastien LamblaSebastien Lambla is a keen open-source advocate and contributor, a long-time REST proponentand distributed systems aficionado With a career spanning over 20 years on many platforms,he’s a keen speaker and trainer, and has been known to talk a lot about technical things and unicorns.Started with a calm song of tuples’ a bag from Mary Poppins, Sebastien guides us through a possible future of HTTP API’s.When talking about creating our own protocol you know that it is recommended not to not to reinvent the wheel and use HTTP instead.Someone else did the hard work, so why should you?This is called the network effect.How is HTTP designed?  Model things - Resources (Person, Animal, Fruit)  Interact with things - Operations (POST, GET, PATCH, DELETE)  Understand the result of the interaction - Common Responses (200 - OK , 201 - Created, 400 - Bad Request)  Reuse existing libraries  One interface to be taught for everythingCommon Media Types  Expose the structure of data in a common format  Reuse existing libraries (JSON)Hypermedia Media Types  Have structure and links understood and defined commonly  Typed links - A link that knows where to go next  Reuse existing libraries (HAL)Common LanguageJSON-LD will try to use a common language for everyone to understand.It is a lightweight Linked Data format, easy for humans to read and write.It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale.JSON-LD is an ideal data format for programming environments, REST Web servicesand unstructured databases such as CouchDB and MongoDB.Example:{    \"@context\":    {        \"author\": {            \"@id\": \"http://schema.org/url\",            \"@type\": \"@id\"         }    },    \"author\": \"https://serialsev.com/author/\"}In the above example, author is an uri which is the identifier of a resource.If you follow the uri, you will find a resource that is defined as a data type that everybody understands and everyone has agreed upon.Again, here we use the network effect because the last thing we want is to rebuild stuff.If you are interested in playing with the format, there is a playground where you can try out examples:PlaygroundWhen we open the playground, you can see a couple of examples you can look at.{  \"@context\": \"http://schema.org/\",  \"@type\": \"Person\",  \"name\": \"Jane Doe\",  \"jobTitle\": \"Professor\",  \"telephone\": \"(425) 123-4567\",  \"url\": \"http://www.janedoe.com\"}The context defines the uri where to go and the type that tells us which resource we are retrieving.Finally the exact uri link will be http://schema.org/Person.If you visit the uri, you’ll get this:When finding the Person resource we see that there are a lot of properties already defined and documented by several people so you don’t have to.When looking back at our example you can use the property already defined in the resource and use them to initialise.If you reuse the property defined, you will get the proper documentation for it.Some nice tools  Google Structured Data Testing Tool  Hydra console  PlaygroundConclusionØredev is one of the best conferences I attended, not only did it innovate in different talks, it managed to organize so many topics to attend.Not only did it improve my technical skills, it also taught me management skills and a realistic view of what the world is today.Very well organized, interesting talks, great local dishes from Sweden and beautiful sightseeing in Malmö."
      },
    
      "reactive-2016-12-12-reactive-programming-spring-reactor-html": {
        "title": "Reactive Programming with Spring Reactor",
        "url": "/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html",
        "image": "/img/reactive/reactor_logo.png",
        "date": "12 Dec 2016",
        "category": "post, blog post, blog",
        "content": "Overview  Stephane Maldini @ JOIN  The new normal that is not new  The Reactive Manifesto  Latency &amp; Blocking  The Contract  Reactive Types  Testing &amp; Debuging  Other Changes  RxJava  Spring Framework 5  Conclusion &amp; Do It YourselfStephane Maldini @ JOIN 2016On 5 October 2016, we had the pleasure to welcome Stephane Maldini at our JOIN event.A multi-tasker eating tech 24/7, Stephane is interested in cloud computing, data science and messaging.Leading the Reactor Project, Stephane Maldini is on a mission to help developers create reactive and efficient architectures on the JVM and beyond.He is also one of the main contributors for Reactive support in the upcoming Spring 5 framework, which can be seen as the new standard for reactive applications in the Java world.  You can rewatch his talk on on our Channel on Youtube.The new normal that is not newIt has been around for 30-40 years and boils down to Event-Driven ProgrammingWhat is new is “reactive motion bound to specification”, this means that reactive programming is based on something solid, a specification and no longer some functional concepts.Namely the Reactive Manifesto.Because of this specification, Spring found it the right time to start with Reactor as they could now build something, which would be able to work and where it was clear what people could expect.The Reactive Manifesto  According to the manifesto, reactive systems are  Responsive: respond in a timely manner if at all possible, responsiveness means that problems can be detected quickly and dealt with accordingly.  Resilient: remain responsive in the event of failure, failures are contained with each component isolating components from each other.  Elastic: stay responsive under varying workload, reactive systems can react to changes in the input rate by increasing or decreasing the resources allocated to services.  Message Driven: rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency.This boundary also provides the means to delegate failures as messages.Systems built as reactive systems are thus more flexible, loosely-coupled and scalable. This makes them easier to develop and to allow changes.They are significantly more tolerant of failure and when failure does occur they meet it with elegance rather than disaster.LatencyLatency is also a real issue, the real physical distance of various components and services becomes more important with cloud based systems.This is also a very random number which is difficult to predict because it can depend on network congestion.With Zipkin, you can measure this latency.The same latency can also exist within an application - between the different threads - although the impact will be less severe than between various components.Something needs to be done when latency becomes too big of an issue, especially if the receiver can not process enough.Too much data will fill up the buffer and can result, with an unbounded queue, to the infamous OutOfMemoryException().While you won’t run out of memory with a circular buffer, you risk losing messages as the oldest ones get overwritten.BlockingOne way to prevent out of memory exceptions is to use blocking.But this can be a real poison pill: when a queue is full, it will block a thread and as more and more queues get blocked your server will die a slow death.Blocking is faster and has better performance, than reactive, but reactive will allow for more concurrency.Concurrency is important if you have a microservice based architecture, as there you typically need to be more careful and more exact when allocating resources between services.As in, by being more concurrent you can save a lot of money when using cloud and microservices.ContractReactive is non-blocking and messages will never overflow the queue, see for the standard definition http://www.reactive-streams.org/.  Created by Pivotal, Typesafe, Netflix, Oracle, Red Hat and others.The scope of Reactive Streams is to find a minimal set of interfaces, methods and protocols that will describe the necessary operations and entities to achieve the goal—asynchronous streams of data with non-blocking back-pressure.With back-pressure, a consumer which can not handle the load of events sends towards it, can communicate this towards the upstream components so these can reduce the load.Without back-pressure the consumer would either fail catastrophically or drop events.  This contract defines to send data 0 .. N.Publisher is an interface with a subscribe() method.Subscriber has 4 callback methods:onSubscribe(), onNext() (which can be called 0 to N times), onComplete() and onError().The last two signals (complete and error) are terminal states, no further signals may occur and the subscriber’s subscription is considered cancelled.What is important is the reverse flow and the back-pressure.After subscribing, the subscriber gets a subscription which is a kind of 1 on 1 relationship between the subscriber and the publisher with 2 methods: request and cancel.  Request: this is the more important one, with this method the subscriber will ask the publisher to send x messages (and not more), a so called pull.  Cancel: the subscription is being cancelled.Spring Reactor focuses on the publisher side of the reactive streaming, as this is the hardest to implement and to get right.It provides you with the tools to implement publishers in a back-pressure way.The publisher is a provider of a potentially unbounded number of sequenced elements, publishing them according to the demand received from its Subscriber(s).The Reactive Streams specification has been adopted for java 9.DIY Reactive StreamsImplementing a Reactive Stream framework yourself is very hard to do, for Stephane Maldini this is the 4th or 5th attempt. For Davik Karnok, the tech lead of RxJava, it is attempt 7 or 8.The main difficulty is to make it side effect free.For example:Publisher&lt;User&gt; rick = userRepository.findUser(\"rick\");Note that a publisher is returned instead of directly returning the entity.By doing so it does not block the subscribers when querying for the user and the publisher will produce the user when ready.But by using the specification as is, your publisher might produce 0, 1 or N users, returning an Iterable as result.This is not really practical to work with, as most of the time we are only interested in a single user and not a stream of multiple results.When you would be building the method findOneUser() you also would not want to return an Iterable but just a single User.Also you will have to implement a subscriber to define the action to perform when the result is available.rick.subscribe(new Subscriber&lt;User&gt;(){...});Implementing this subscriber would not be that hard, because the specification has been made so that all complexity lies at the publishers side.Another issue is that you can only subscribe on the publisher, there are no other methods available like map, flatmap, …The other point is that when designing your own API you will also have to deal with the following issues:  Should work with RS TCK (otherwise it might not work with other libraries as well)  Address reentrance  Address thread safety  Address efficiency  Address state  For Many-To-One flows, implement your own merging operation  For One-To-Many flows, implement your own broadcasting operation  …  This is all very hard to do yourself.3 Years to MatureIt took Spring Reactor 3 years to mature.2.0 was not side effect free - also existential questions were raised around the project. At the same time Spring evolved and microservices became the norm.Spring needs to work nicely with these microservices, concurrency is important, can Reactor not be used for that?With 3.0 the team wanted to focus on microservices, take some ideas from Netflix OSS and implement these in a pragmatic way.Actually Reactor 3 was started as 2.5, but so many new features were added that the version had to be changed as well in order to reflect this.Since 3.0 Spring Reactor has been made more modular and consists of several components:    Core is the main library.Providing a non-blocking Reactive Streams foundation for the JVM both implementing a Reactive Extensions inspired API and efficient message-passing support.  IPC: back-pressure-ready components to encode, decode, send (unicast, multicast or request/response) and serve connections.Here you will find support for Kafka and Netty.  Addons: Bridge to RxJava 1 or 2 Observable, Completable, Flowable, Single, Maybe, Scheduler, and also Swing/SWT Scheduler, Akka Scheduler.  Reactive Streams Commons is the research project between Spring Reactor and RxJava as both teams had a lot of ideas they wanted to implement.Lots of effort was put in order to create real working, side-effect free operations.Map and Filtering for example are easy, but mergings, like Flatmap are hard to implement side-effect free.Having a proper implementation in the research project for these operations allowed the team to experiment and make it quite robust.This project contains Reactive-Streams compliant operators, which in turn are implemented by Spring Reactor and RxJava.Both the Spring and RxJava teams are very happy with this collaboration and this is still continuing.When a bug gets fixed in Spring Reactor it will also be fixed in RxJava and vice versa.Everything in Reactor is just reactive streams implementation - which is used for the reactive story of spring 5.There also exists an implementation for .NET, Reactor Core .NET and one for javascript Reactor Core TypeScript.Reactive TypesFlux vs Observable  Observable is not implementing Reactive Streams Publisher which means that if you would like to use the Spring 5 save(Publisher&lt;T&gt;) you first have to convert the Observable to a Flowable as you can see in Observable and Flowable.This was too much noise for the Spring team, they are less dependant on Android developers so they could go all in with Java 8.Flux is a Reactive Streams Publisher with basic flow operations, where you start from a static method which will describe how the data will be generated, just() is the simplest wayAfter that you have other operators like Flatmap(), Map(), … to work with that dataSome of the method names will be different to RxJava2, but the logic behind these methods has been aligned among RxJava and Spring .Flux.just(\"red\", \"white\", \"blue\")       .flatMap(carRepository::findByColor)       .collect(Result:: new, Result::add)       .doOnNext(Result::stop)       .subscribe(doWithResult);Interface CarRepository {    Flux&lt;Car&gt; findByColor(String color);}This Flux will retrieve all cars which match the color “red” then those with the color “white” and finally “blue”.So instead of just three elements, after this Flatmap we are going to have a lot more elements.This is all handled with back-pressure in mind, for example when the flatmap is busy merging data we will not ask for extra recordsIf the Repository implements Flux as a method signature, it will be picked up automatically as a reactive repository.This support for Flux will be part of the whole of Spring 5.Spring Data, Spring Security, Spring MVC, … are all good candidates who will have this kind of support.Mono  None is like a flux, but will return at most 1 result, so it does have less methods.Mono.delayMillis(3000)    .map(d -&gt; \"Spring 4\")    .or(Mono.delayMillis(2000).map(d -&gt; \"Spring 5\"))    .then(t -&gt; Mono.just(t + \" world\"))    .elapsed()    .subscribe()This Mono will wait for 3 seconds on the “call” to Spring 4 or 2 seconds on that of Spring 5.The fastest result will be the one which will be outputted.The Mono has as advantage over an Observable Future of Java 8 that a Mono will only be triggered if you subscribe to it.While with an Observable the call to send() will execute the operation.TestingBlock() exists for very specific use cases and for testing.Never, ever use this in production, as is it blocks your call, which does infer with the Reactive non-blocking statements. ;-)Mono.delayMillis(3000)    .map(d -&gt; \"Spring 4\")    .or(Mono.delayMillis(2000).map(d -&gt; \"Spring 5\"))    .then(t -&gt; Mono.just(t + \" world\"))    .elapsed()    .block()You can also make use of Stepverifier to test Flux, Mono and any other kind of Reactive Streams Publisher.@Testpublic void expectElementsWithThenComplete() {    expectSkylerJesseComplete(Flux.just(new User(\"swhite\", null, null), new User(\"jpinkman\", null, null)));}Use StepVerifier to check that the flux parameter emits a User with “swhite” username and another one with “jpinkman” then completes successfully.void expectSkylerJesseComplete(Flux&lt;User&gt; flux) {    StepVerifier.create(flux)            .expectNextMatches(user -&gt; user.getUsername().equals(\"swhite\"))            .expectNextMatches(user -&gt; user.getUsername().equals(\"jpinkman\"))            .expectComplete();}DebugWhen you use reactive libraries you will quickly realize that step debugging is hard especially when you try to read your stacktraces, there are a lot of recursive calls taking place.Before you invoke your operations you can enable an, expensive, debug mode.Hooks.onOperator(op -&gt; op.operatorStacktrace());try {    Mono.just(\"a\")        .map(d -&gt; d)        .timestamp()        . ...                } catch (Exception e) {    e.printStacktrace()}When an exception is returned it will contain the exact operation that failed and the backtrace to that operation.You must enable this Hooks.onOperator before the operations you want to track.More cool stuffParallelFluxIf you want to stress test your CPU you can use ParallelFlux which will spread the workload in concurrent tasks when possible.Mono.fromCallable( () -&gt; System.currentTimeMillis() )    .repeat()    .parallel(8) //parallelism    .runOn(Schedulers.parallel())    .doOnNext( d -&gt; System.out.println(\"I'm on thread \"+Thread.currentThread()) ).    .sequential()    .subscribe()    This basically avoids that you have to write flatMap(), where after the parallel(x) you will have exactly x number of Rails or Flux.Afterwards you can merge these back into a Flux with sequential().A nice feature is that it keeps the code more readable with everything on a single indentation level.But the cool part is that it is also very performant, with parallel, Reactor is very close to the bare metal of what the JVM can do as you can see in the below comparisation:        https://twitter.com/akarnokd/status/780135681897197568Bridge Existing Async codeTo bridge a Subscriber or Processor into an outside context that is taking care of producing non concurrently, use Flux.create(), Mono.create(), or FluxProcessor.connectSink().Mono&lt;String&gt; response = Mono.create( sink -&gt; {    HttpListener listener = event -&gt; {        if (event.getResponseCode() &gt;= 400) {            sink.error(new RunTimeException(\"Error\"));        } else {            String result = event.getBody();            if (body.isEmpty()) {                sink.succes();            } else {                sink.success(body);            }        }    };    client.addListener(listener);        emitter.setCancellation(() -&gt; client.removeListener(listener));});This create() allows you to bridge 1 result, which will be returned somewhere in the future, to a Mono.If you add a Kafka call, for example, where they have this callback so one can return onSuccess and onError you can use Mono.create(): see Reactor Kafka where this is used a lot.Also exists for Flux of N items but it’s tougher and more dangerous as you must explicitly indicate what to do in the case of overflow; keep the latest and risk losing some data or keep everything with the risk of unbounded memory use. ¯\\(ツ)/¯Create Gateways to Flux and MonoThere also exist some options to bridge the synchronous world with the Flux and the Mono.Like for example the EmitterProcessor which is a signal processor.  EmitterProcessor&lt;Integer&gt; emitter = EmitterProcessor.create();BlockingSink&lt;Integer&gt; sink = emitter.connectSink();sink.next(1);sink.next(2);emitter.subscribe(System.out::println);sink.next(3); //output : 3sink.finish();But you also have:  ReplayProcessor, a caching broadcaster.  TopicProcessor, an asynchronous signal broadcaster  WorkQueueProcessor, which is similar to the TopicProcessor but distributes the input data signal to the next available Subscriber.These are all an implementation of a RingBuffer backed message-passing Processor implementing publish-subscribe with synchronous drain loops.OptimizationsOperation fusion: Reactor has a mission to limit the overhead in stack and message passing.They distinguish 2 types of optimization:  Macro Fusion: Merge operators in one during assembly time, for example, if the user does .merge() - .merge() - .merge() spring reactor is smart enough to put this in a single .merge()  Micro Fusion: Because of the Reactive specification and the asynchronous nature of the response, queues are heavily used, but creating a queue for every request/response is very costly.Spring Reactor will avoid to create queues whenever possible and short circuit during the lifecycle of the request. They are going to merge the queue from downstream with the one from upstream - hence the name fusion.If the parent is something we can pull (an Iterable or a queue) then Reactor is going to use the parent as a queue, thus avoiding to create a new queue.This is very smart to do - but also very complicated to do yourself, because Spring Reactor has this in place you do not have to deal with this hassle..A Simpler APIReactor: a Simpler API, the entire framework just fits in 1 jar: reactor-core jar.Flux and Mono live in the reactor.com.publisher package, reactor.core.scheduler contains the FIFO task executor.By default the Publisher and Subscriber will use the same thread.With publishOn() the publisher can force the subscriber to use a different thread, while the subscriber can do the same with subscribeOn().For Reactor 3.x there will be more focus on the javadoc, as this has been lagging behind compared to the new features which have been developed.RxJavaWhy Reactor when there’s already RxJava2?RxJava2 is java 6 while for Reactor the Spring team decided to go all in and focus only on Java 8.This means that you can make use of all the new and fancy Java 8 features.If you are going to use Spring 5, Reactor might be the better option.But if you are happy with your RxJava2, there is no direct need to migrate to Reactor.Spring Framework 5It will still be backwards compatible. You can just take your Spring 4 application, put Spring 5 behind it and you will be good to go.But with Spring 5 you will be able to make use of the following new components/ Spring Web Reactive and Reactive HTTP.Which under the hood support Servlet 3.1, Netty and Undertow.The annotations are still very similar but you just return a Mono, so the User can now be retrieved in a non-blocking way.@GetMapping(\"/users/{login}\")public Mono&lt;User&gt; getUser(@PathVariable String login) {    return this.repository.getUser(login);}ConclusionSpring Reactor is a very interesting framework, after 3 iterations it has matured and gives you a good base to get started with Reactive Streams.With the upcoming support in Spring 5 it will also start to become more mainstream.Therefore I can see no better way then to get your hands dirty and learn more about Spring Reactor yourself.  reactive-programming-part-I:Provides you with a clear description of what reactive programming is about and its use cases.But also the different ways about how people have implemented reactive programming (actor model, futures, … ) and more specifially the different frameworks which implement reactive programming in java.          Frameworks like: Spring Reactor, Spring Framework 5, RxJava , Akka, Reactive Streams and Ratpack.        reactive-programming-part-II:You will learn the API by writing some code, how to control the flow of data and its processing.      reactive-programming-part-III:Here you will focus on more concrete use case and write something useful, but also on some low level features which you should learn to treat with respect.        reactor-api-hands-on:This hands-on will help you learn easily the lite Rx API provider by Spring Reactor. You just have to make the unit tests green.    On spring.io you can find more interesting blog posts which will give you more background around Spring Reactor and provide you with the resources to start coding."
      },
    
      "conference-2016-11-18-jax-london-2016-html": {
        "title": "JAX London 2016",
        "url": "/conference/2016/11/18/JAX-London-2016.html",
        "image": "/img/jax-london-2016/jax-london-logo.png",
        "date": "18 Nov 2016",
        "category": "post, blog post, blog",
        "content": "  JAX London is a four-day conference for cutting-edge software engineers and enterprise-level professionals. JAX brings together the world’s leading innovators in the fields of Java, microservices, Continuous Delivery and DevOps.For this year’s slogan they decided on: “Create, Innovate, Code”.Ordina was present at JAX London on the 11th and 12th of October 2016 where one of our colleagues, Bart Blommaerts, also presented a talk on The Serverless Cloud.In this blogpost we want to give the highlights of some of the talks we followed.Day 1Introduction to JAX London - Sebastian MeyenTo start off, Sebastian Meyen, program chair of the JAX conferences, gave an introduction to JAX London.He mentioned that only half of the attendants of JAX originate from the UK, making it an international conference.JAX is all about “openness”, we should be celebrating open source and embrace open source thinking as it makes our code smarter.Java is one of the most powerful ecosystems out there according to him and it comes with a unique culture.Sebastian explained that there are different cultures within a company:  Pioneers: They go into the wild and experiment and although they might fail often, they’re looking at what the next big thing might be  Settlers: They make a valid business model based on the results of the research of the pioneers and make stable technology for it  Town builders: They look at the portfolio of the settlers and decide what to industrialise, they want to create volumeHe stresses that innovation happens on all three levels and not just only at the pioneers or settlers.Furthermore he went briefly over the different genres of the talk and the conference app before introducing James Governor.Opening keynote: Java for Cloud Natives - James GovernorJames Governor opens up his talk by showing how Java is still high on the Programming Language rankings despite the “Java is dead”-doomspeak every now and then.Regarding Java frameworks, Spring and Spring Boot are really crushing it.He also mentions that most startups usually start with a new and fancy language but as they mature, a lot of them actually turn into Java shops.Examples of this being: Uber, LinkedIn, Netflix, Twitter, Amazon, Etsy, Facebook, Yahoo and Google.Twitter for example started in Ruby and during the US presidential election in 2012 they migrated to Scala and Java on the JVM for scalability and performance reasons.After the migration they managed to sustain peaks of TPS (Tweets Per Second) for hours, at one point even reaching 15,107 TPS.James went through a couple of companies and the transformations they underwent for staying competitive.Amazon for example started with a messy code base but they did manage to refactor it.Being a top down company, they also managed to create small teams.Netflix is a similar case, at a certain point they had a messy codebase but they really put a lot of effort into refactoring it.They also invested a lot in their software engineers and continue to do so.Open source is the new normal, there are a lot of cool open source frameworks around that you can contribute to such as Zookeeper, Spark, Kafka, Hadoop, Giraph, Jenkins, Cassandra,…It’s awesome when enterprises contribute to open source.  Bosch: They are doing interesting work with Eclipse Foundation, they know they need to do open source so they make open source contributions  Comcast: Everybody hates Comcast in the US, but they are making open source contributionsJames mentions a couple of things he finds important:  Microservices and container based deployment such as Docker and Kubernetes are very hot topics  Break down the technical model and teams  Continuous integration is very important, there are still people not using it!  Make people responsible for their own Quality Assurance  Embrace failure and graceful degradationFinally, he mentioned that there is always the need to deal with the politics and that governance is still important and needed.Oracle needs to give their commitment to Java and the Cloud and we as a community need to encourage Oracle to step up and not only complain about them.Links:  PresentationDeveloping Microservices with aggregates - Chris RichardsonThe goal of Chris Richardson’s presentation was to show how Domain-Driven Design aggregates and microservices are a perfect match.A microservice based architecture tackles complexity through modularisation.A microservice should be seen as a business capability for example a catalog service, review service or order service.By having service boundaries you enforce modularity.Also important to mention is that each microservice needs to have its own database, microservices do not share a database!Finally there should always be an API gateway in front of the microservices, which is the entrypoint for the frontend user interface and mobiles devices.They should never access your microservice directly!Chris strongly suggest reading Domain Driven Design by Evan Evans.The core building blocks of Domain-Driven Design are the following:  Entities  Value objects  Services  Repositories  AggregatesAbout Domain-Driven Design aggregates:  Cluster of objects that can be treated as a unit  Graph consisting of a root entity and one or more other entities and value objects  Typically business entities are aggregates e.g., Customer, Account, Order, ProductChris talked about the problems you have to handle when dealing with microservices and that practicing Domain-Driven Design can help you a lot to address these.He mentioned that you would probably ask yourself how you can enforce invariants if the microservices reside in different JVMs.You would be reliant on ACID transactions to maintain consistency.Transactions violate encapsulation and require 2-Phase Commits (2PC) which is not an option because of the following reasons:  It guarantees consistency but 2-Phase Commit is a single point of failure  It is a chatty protocol: at least O(4n) messages, with retries O(n^2)  Reduced throughput due to locks  Not supported by many NoSQL databases (or message brokers)  Doesn’t fit in a NoSQL mindsetAggregates are a solution to these ACID transactions since they allow you to use eventual consistency.You reference other aggregate roots via an identity, this being the primary key.If an update must be atomic then it must be handled by a single aggregate therefore aggregate granularity is important.You should have your aggregates as fine grained as possible.In an Event-driven architecture you can work with steps where each step publishes an event that trigger the next step in the sequence.You will need to write custom logic to compensate the well known ACID transaction rollback so careful design is required.Using Event Sourcing with Aggregates:  There is no notion of updating the database and then publishing the event but you rather just publish the event  For each Domain-Driven Design aggregate:          Identify state changing domain events (eg with Event Storming)      Define Event classes (for example Order events: OrderCreated, OrderCancelled, OrderApproved, OrderRejected, OrderShipped, …)        Persist events and NOT current state          Store the events themselves in a database      Replay events to recreate state        All Aggregates are storing their events in the Event Store, each aggregate subscribes to events of the other aggregates          When using CQRS, update the view after processing an event      Benefits of Event Sourcing:  Solves data consistency issues in a microservice/NoSQL based architecture  Reliable event publishing needed by predictive analytics, user notifications, etc  Eliminates object relationship mapping problems  Rectifies state changes:          Built in, reliable audit log      Temporal queries      Preserved history      Drawbacks of Event Sourcing:  Requires application rewrite (mechanical transformation)  Learning curve: weird and unfamiliar style of programming  Events can be a historical record of your bad design decisions          You will probably implement a mechanism to deal with that such as versioning        Must detect and ignore duplicate events:          Write idempotent event handlers      Track most recent event and ignore older ones        Querying the event store can be challenging:          Event store might only support lookup of events by identity id      Must use CQRS to handle queries meaning application must handle eventually consistent data      Example application: https://github.com/cer/event-sourcing-examples  Orders and Customers example with Kafka in between as backchannel  Both of them connected with the event store  Written in SpringSummary  Aggregates are the building blocks of microservices  Use events to maintain consistency between aggregates  Event Sourcing is a good way to implement an event-driven architectureSecure by design - Eoin WoodsEoin Woods went over security and that we should care about it for things like protection against malice, mistakes and mischance, theft, fraud, destruction, disruption,…He mentioned OWASP top 10 which he approves and should definitely be checked out if you haven’t seen it.There are four aspects of security practice:  Secure Application Design  Secure Infrastructure Design  Secure Application Implementation  Secure infrastructure deploymentHe explains that there are many sets of security design principles but he notes that there are many similarities between them at a fundamental level:  Viege and McGraw (10)  OWASP (10)  NIST (33)  NCSC (europe)(44)  Cliff Berg’s set (185)Out of all these, Eoin has distilled 10 key principles himself:  Least privilege          Why: Broad privilege allows malicious or accidental access to protected resources      Principle: Limit privileges to the minimum for the context      Tradeoff: It is less convenient, less efficient and adds more complexity      Example: Run a server process with the minimum privileges        Separate responsibilities          Why: Achieve control and accountability, limit the impact of successful attacks, make attacks less attractive      Principle: Separate and compartmentalise responsibilities and privileges      Tradeoff: Development and testing costs, operation complexity, troubleshooting can be more cumbersome      Example: Admins of a submodule should have no access to other module features        Trust cautiously          Why: Many security problems are caused by inserting malicious intermediaries in communication paths      Principle: Assume unknown entities are untrusted, have a clear process to establish trust, validate who is connecting      Tradeoff: Operational complexity, reliability, some development overhead      Example: Don’t accept untrusted RMI connections, use client certificates, credentials or network controls        Simplest solution possible          Why: Security requires understanding of the design, complex design is rarely understood, simplicity allows analysis      Principle: Actively design for simplicity - avoid complex failure modes, implicit behaviour, unnecessary features,…      Tradeoff: Hard decisions on features provided and needs serious design effort in order to be simple      Example: Does the system needs a dynamic runtime config via a custom DSL?        Audit Sensitive Events          Why: Provide record of activity, deter wrongdoing, provide a log to reconstruct the past, provide a monitoring point      Principle: Record all security significant events in a tamper-resistant store      Tradeoff: Performance, operational complexity, development cost      Example: Record all changes to “core” business entities in an append-only store        Secure defaults &amp; fail securely          Why: Default passwords, ports and rules are “open doors”. Failure and restart states often default to “insecure”      Principle: Force changes to security sensitive parameters. Think through failures - Must be secure but recoverable      Tradeoff: Convenience      Example: Don’t allow “SYSTEM/MANAGER” after installation. On failure don’t disable or reset security controls        Never rely on obscurity          Why: Hiding things is difficult - someone is going to find them, accidentally if not on purpose      Principle: Assume that the attacker has perfect knowledge, this forces secure system design      Tradeoff: Designing a truly secure system takes time and effort      Example: Assume that an attacker will guess a “port knock” network request sequence or a password encoding        Defence in-depth          Why: Systems do get attacked, breaches do happen, mistakes are made - need to minimise impact      Principle: Don’t rely on single point of security, secure every level, stop failures at one level from propagating      Tradeoff: Redundancy of policy, complex permissioning and troubleshooting, can make recovery harder      Example: Access control in UI, services, database, OS        Never invent security tech          Why: Security technology is difficulty to create, it’s a specialist’s job, avoiding vulnerabilities is difficult      Principle: Don’t create your own security technology, always use a proven component      Tradeoff: Time to assess security technology, effort to learning it, complexity      Example: Don’t invent your own OSS mechanism, secret storage, crypto libs, use existing proven ones        Secure the weakest link          Why: “paper wall”-problem, common when focus is on technologies and not threats      Principle: Find the weakest link in the security chain and strengthen it, repeat (threat modelling)      Tradeoff: Significant effort required, often reveals problems at the least convenient moment      Example: Data privacy threat met with encrypted communication but with unencrypted db storage and backups      Links:  Presentation  Book: Software Systems Architecture  UK Government NCSC  NIST Engineering Principles for IT Security  OWASP Security by Design PrinciplesEvent-driven Microservices - Jeremy DeaneJeremy Deane starts off telling that within Event-driven architectures (EDA) you have events representing a snapshot in time of an occurrence within a system.We should distinguish the following Enterprise Integration Patterns (EIP):  Event Message  Command Event  Event SourcingAs for possible Middleware solutions:  ActiveMQ (JSM), RabbitMQ (AMQP)  Kafka, ZeroMQ  AkkaThere are a couple of EDA principles to take in mind:  Events are emitted by a Producer and received async, and optionally acted upon, by a stateless Consumer  Streams are sets of related Events  Intermediate Processors can enrich the raw Event  Ideally, Producer and Consumer should be decoupled so they can evolve independently over time  Producer should be a magnanimous writer and consumer should be a tolerant reader  Consumers can listen to Event Queues or subscribe to Event TopicsTo give some EDA examples:  Fraud prevention  Medical Alerting (ER check-in)  Financial Portfolio Management  Supply Chain ManagementMicroservices Architectural Style:  Application as a suite of small services  Each running in its own process  Communicating with lightweight mechanisms  Built around business capabilities and independently deployable by fully automated deployment machinery  Bare minimum of centralized managementJeremy really likes Apache ActiveMQ for several reasons:  Not the fastest but the easiest to implement and maintain  Easy to learn  Great and active community  High Availability via a master/slave approachFinally he did a demo of an application built with ActiveMQ and Apache Camel to show how well and easy they integrate together.The source of the demo is available at the links below.Links:  Presentation  https://github.com/jtdeane/event-driven-microservices  https://github.com/jtdeane/camel-standalone-routerOperating the Spotify backend - Niklas GustavssonNiklas Gustavsson immediately starts off with the culture at Spotify.Spotify made two very nicely animated videos that explain their culture:  Spotify Engineering Culture Part 1  Spotify Engineering Culture Part 2Basically it comes down to having autonomous teams called squads containing of 7-12 team members:  A squad should be fully staffed for their “mission”  A squad can decide how they want to tackle their issue  Each squad is on call for their own microservicesAt Spotify they have hundreds of fairly small services that only do one thing well.Each service is owned by a squad that implements and deploys it into production.Later on, they started putting ops guys in the squads in order to be able to tackle issues a lot faster.Jeremy also stresses that it’s very important to automate as much as possible.At Spotify they have a simple incident process to avoid the same issues from reappearing:  Something explodes, an incident gets created and it gets fixed  (Blameless) post-mortem meeting  Remediations to make sure it never happens again  The incident is closedSystem-Z is the service registry in which all services in production get registered in and the following information is available for each microservice:  Who’s on call for each service  When a service is down  What the dependencies are for each service  The hardware in use  The amount of instancesAlso, in order to encourage squads to register their microservices, if you’re not registering your services in there you’re not getting any hardware and thus you can’t get your service into production.In order to maintain their hardware they utilise the Cortana Pool Manager where they have the following information available:  See hardware available  Specify how much you need like how many instancesThey currently do a lot of self-hosting but are planning to fully migrate to Google Cloud Platform.In order to manage their Docker containers they use Helios which is their own Docker orchestration platform.Links:  PresentationJAX Innovation AwardsThe JAX Innovations Awards reward the most innovative contributions in the shape of technology, idea, documentation, industry impact or community engagement.Most innovative contribution to the Java Ecosystem:  1st: Spring Boot  2nd: Camunda BPM Platform  3rd: SparkInnovation in Software delivery &amp; DevOps:  1st: Docker  2nd: Prometheus  3rd: habitatSpecial Jury Award:  Let’s EncryptExtra: Special Honour Award:  Pieter Hintjes (1962-2016) who passed away last week          “Look at the internet, that’s how software should be, so resilient”      Worth a read: Confessions of a Necromancer      Day 2Opening keynote: Decision-making in the face of disruption - Duncan PearsonDuncan is a Chief Architect at Anaplan.Anaplan is the leading planning and performance management platform for smart business.It provides a mechanism that helps the business understand the consequences of what they intent to do.Duncan first sums up the forms of disruption:  Market replacement          Film -&gt; Digital camera      Digital Camera -&gt; smartphone      GPS -&gt; smartphone        Market change          Free online ad-funded services      Comparison websites        New market          Personal gene technology      Recorded music        Environmental change          Natural disaster      War and civil unrest      Time is of the essence and it is important to act quickly to a window of opportunity when trying to tackle a changing situation over time.There are two perspectives on disruption:  The disruptors          High organisational growth leading to complexity transitions      High sensitivity to: growth rate mismatch and delay effects      For example: DataStax, Facebook, Salesforce, Groupon, Box, OpenDNS        The disrupted          New business models required      Disconnect between business functions and supporting systems      Entrenched organisational systems      For example: Yahoo, Toyota, PWC, Verizon, Vodafone, Cisco      Duncan mentions that there are two kinds of systems: modelling systems and execution systems.A good modelling system has the following characteristics:  Personal  Flexible  General purpose  It has to feel like a spreadsheetWhereas a good execution system tends to be:  Specialist, use-case-specific  Embed the “model” in the software  Secure / multi-user  User-task focusedAn execution system tends to be a system that has already been analysed in-depth.Influence on design:  Grids with pivots, recalculations, access to formulae everywhere, inline changes to names/formulae/structure          It has to support the task-focused application developer                  Easy screen and navigation design          Persistent navigational context          Security &amp; access control                    Completely flexible                  Anything can happen at any time          Remodelling the business should be easy          Fast delivery time                    Scale and perform                  If it looks like a spreadsheet then it had better behave like one even if you are changing billions of numbers                    A business management tool                  No programming          No DB design required                    Influence on the company:  We were the disruptor, on a tight deadline          Success through simplicity      Something that works today is better than something that is right tomorrow      Led by our customers, driven by the features that they need        We are undergoing inevitable complexity transitions          New HR systems, management processes, multiple dev locations      Standard Compliance (security, development process)      The Serverless Cloud - Bart BlommaertsBart Blommaerts starts off explaining that serverless, although as suggested by the name, does not actually mean that you’re not using any server.What it does mean is that the existence of a server is hidden for you.The name might be a bit misplaced and it would be more correct to simply call it Functions as a Service (FaaS).Serverless or FaaS might actually be the next step in the evolution of cloud computing.Going serverless will lead to less worries seeing as you no longer have the server management to take care of.Security updates, scalability and availability is all taken care off for you, by the provider.On the other hand, more trust must be put into the provider.The need for an ops team will not fade since you still need to take care of things such as monitoring, debugging support, memory management, application configuration and more.All these things could be handled by a specialised, outsourced team.Seeing as serverless is rather new, there is also the opportunity for tooling to be built!We can distinguish the following characteristics:  Event-driven  Pay per execution instead of server uptime  The actually invocation cost depends on how long it took to execute the callAn example of the pricing of AWS:  Pay per 100ms minimum each call  3.2 million free tier seconds per month  Outside of the free tier the price per 100ms is $0.000000208Serverless comes with seamless scaling:  No risk of under- or over-provisioning  Short-lived “compute containers” that:          are isolated from other functions      have their resources provided from the function config      may be reused      Serverless is stateless, there is no state persisted in between invocations.In order to preserve state you can still use a db, file system or cross-application cache.Bart then listed a couple of the current providers:  AWS Lambda          Integrates well with all other AWS services        IBM Bluemix OpenWhisk          OpenWhisk is open source and available on Github      Better UI than AWS and also has a dashboard for monitoring        MS Azure Cloud Functions          Mature system      Different payment model compared to AWS that could be more expensive in the long run        Google Cloud Functions          A new initiative by Google, but it benefits from services Google offered over 5 years ago      Currently reimplementing it      Currently in private alpha      Very active community        Auth0 WebTask          CLI      Really simple to use, you can be up and running in 30 seconds      Well documented with examples      Serverless enables experimentation due to how easy it is to get something up and running and the low running costs.It could lead to a more collaborative economy seeing as a lot of companies are sitting on a ton of data currently not being used.All this data could be made public by publishing an API and others could consume the data and combine it with their own data, enriching it.In Belgium there is already a company, Realo, that only combines data and they seem really popular.Bart did a demo using an Arduino measuring the temperature.The Arduino then sends the data off to multiple providers, enriching the data, after which it gets logged in the final step.    During the demo, Bart mentioned that he used the Serverless Framework for development.The Serverless Framework is a CLI, soon to be supplier independent (at the time of writing), with the following features:  Scaffolding  Best practices (grouping of functions)  Lifecycle support (create, deploy, invoke,…)The demo and code samples are available at:  https://github.com/bart-blommaerts/serverless-demo  https://github.com/ordina-jworks/lora-iot-demoBart notes that the size of the function is important as it has an impact on execution time and will affect the cost.For example a service written in Java could be 34MB whereas in NodeJS the exact same function may only be 100KB.Best practices:  Compare the following regarding supplier choice:          Integration      Offering      Tooling available      Execution time is limited so check if it fulfils your needs      SLA available?        Code          Initialise services outside of the function such as making a database connection      Limit function size      Use an external editor and a VCS      Bart also recently wrote a blogpost on The Serverless Cloud, you should definitely check it out if you’re interested on the subject.Links:  PresentationFour Distributed Systems Reference Architectures - Tim BerglundAccording to Tim Berglund, a distributed system is a collection of independent computers that appear to its user(s) as one computer.Independent computers should operate concurrently, fail independently and should not share a global clock.The point of Tim’s talk is to compare the following four systems:  Modern 3-tier Architecture  Sharded Architecture  Lambda Architecture  Serverless ArchitectureEach system has its strengths and weaknesses compared and finally receives a rating for the scalability, hipness and difficulty of use.Modern 3-tier ArchitectureThe classic Presentation, Business and Data tier.  Before: JSP, EJB/Servlet, Oracle DB  Now: React.JS, Node.JS, CassandraStrengths  Rich front-end framework (scale/UX)  Hip, scalable middle tier (such as Spring)  Basically infinitely scalable data tierWeaknesses  State in the middle tierRating  Scalability: 4/5  Hipness: 2/5  Difficulty: 3/5Sharded ArchitectureWith a Sharded Architecture the Business and Data tier is usually sharded.In between the clients and the business tier there’s usually a router that performs loadbalancing and routes the request to the service of the application.The Data tier contains multiple databases.Strengths  Client isolation is easy (data and deployment)  Known, simple technologiesWeaknesses  Complexity  No comprehensive view of data  Oversize shardsRating  Scalability: 3/5  Hipness: 1/5  Difficulty: 4/5Lambda ArchitectureStreaming data versus handling data in batch or even better, unbounded data vs bounded data.Event-driven, events are either stored in a database or processed via an Event Processing framework such as Apache Storm.Storing them in a database includes:  Long-term storage  Bounded analysis  High latency.On the other hand, processing them via an Event Processing framework includes:  Temporary queueing  Unbounded analysis  Low latencyStrengths  Optimises subsystems based on operational requirements  Good at unbounded dataWeaknesses  Complex to operate and maintain  Bad at mutable/update-in-place dataRating  Scalability: 3/5  Hipness: 3/5  Difficulty: 5/5ServerlessDoes not mean that there isn’t any server, just that you no longer maintain it.Also called Functions as a Service (FaaS), these can be seen as “extreme” microservices.Strengths  Potentially low resource cost  Rigorous microservices disciplineWeaknesses  Can be very complex  Requires a different way of thinkingRating  Scalability: 4/5  Hipness: 5/5  Difficulty: 5/5Links:  Presentation"
      },
    
      "cloud-2016-11-12-theserverlesscloud-html": {
        "title": "The Serverless Cloud",
        "url": "/cloud/2016/11/12/TheServerlessCloud.html",
        "image": "/img/serverless.jpg",
        "date": "12 Nov 2016",
        "category": "post, blog post, blog",
        "content": "  In recent years, the uprise of the cloud has brought us a lot of new and disruptive technologies. Everybody is talking about SaaS, PaaS, IaaS and other sorts of aaS. In 2014, Amazon launched AWS Lambda as the pinnacle of the cloud computing. It allows developers to focus on code, without spending time on managing servers.Part 1What?While Microservices have been reigning the Cloud landscape for a couple of years, today the Serverless movement is one of the hottest trends in the industry. Historically, software developers have been pretty bad at naming things and Serverless is no exception. Disregarding what the name suggests, Serverless does not imply the complete absence of servers. It implies that developers who are using the Serverless architectural style, are not responsible for managing or provisioning the servers themselves, but use a vendor-supplied Cloud solution. Serverless means less worrying about servers. Although in the future, it might be possible to install this kind of service on-premise, for example with the open-source IBM OpenWhisk implementation.In regard to this, the definition FaaS: Functions as a Service makes a lot more sense. Functions are short-lived pieces of runtime functionality that don’t need a server that’s always running. Strictly speaking a function can have a longer execution time, but most FaaS providers will currently limit the allowed computation time. When an application calls a function (eg. a calculation algorithm), this function gets instantiated on request. When it’s finished, it gets destroyed. This leads to a shorter “running” time and thus a significant financial advantage. As an example, you can find the AWS Lambda pricing here. FaaS functions are also a great match for event-driven behaviour: when an event is dispatched, the function can be started instantly and ran only for the needed time. A Serverless application is a composition of event chaining. This makes the Serverless style a natural match for API Economy.As a result of being runtime components, FaaS functions are stateless and need to rely on a database (or file system) to store state. Being stateless and short-lived naturally lead to extreme horizontal scaling opportunities and all major FaaS providers support these.NoOpsNoOps (No Operations) is the concept that an IT environment can become so automated and abstracted from the underlying infrastructure that there is no need for a dedicated team to manage software in-house. NoOps isn’t a new concept as this article from 2011 proves. When Serverless started gaining popularity, some people claimed there was no longer a need for Operations. Since we already established that Serverless doesn’t mean no servers, it’s obvious it also doesn’t mean No Operations.It might mean that Operations gets outsourced to a team with specialised skills, but we are still going to need: monitoring, security, remote debugging, … I am curious to see the impact on current DevOps teams though. A very interesting article on the NoOps topic, can be found over here.AWSAWS LambdaAWS Lambda was the first major platform to support FaaS functions, running on the AWS infrastructure. Currently AWS Lambda supports three languages: Node.js, Java, and Python. AWS Lambda can be used both for synchronous and asynchronous services.Currently the tooling for AWS Lambda is still relatively immature, but this is changing rapidly. At the time of writing, the AWS Lambda console offers the possibility to create a Lambda using blueprints. This is already easier than setting up a lambda by hand (using a ZIP-file). Blueprints are sample configurations of event sources and Lambda functions. Currently 45 blueprints are available. To give a short introduction, we’ll select the hello-world blueprint. This blueprint generates a very simple NodeJS function:'use strict';console.log('Loading function');  exports.handler = (event, context, callback) =&gt; {   console.log('value1 =', event.key1);   console.log('value2 =', event.key2);   console.log('value3 =', event.key3);   callback(null, event.key1); };After creating this function, it can be immediately be tested from the console, using a test event. If we want to call this function synchronously, we need to create an API endpoint with the AWS API Gateway. The API Gateway creates API’s that acts as a “front door” to your functions. To make this work with the events in our hello-world example, we need to select the resources of our API:In Integration Request, we add a body mapping template of type application/json with the following template:{ \"key3\": \"$input.params('key3')\",\"key2\": \"$input.params('key2')\",\"key1\": \"$input.params('key1')\"}In ‘Method request’ we add 3 URL String query parameters: key1, key2 and key3. If we then redeploy our API, hitting the Test button gives us an input form to add the 3 query parameters and the function is executed successfully:If you want to test this directly from a browser, you will need to change the Auth to NONE in the ‘Method request’ and do a new deploy of the API. The URL itself can be found in the ‘stage’-menu.This example obviously is not very interesting, so let’s try another blueprint: microservice-http-endpoint. This will generate a CRUD backend, using DynamoDB with a RESTful API endpoint. The code generated, covers all common use-cases:'use strict';letdoc = require('dynamodb-doc');letdynamo = newdoc.DynamoDB();exports.handler = (event, context, callback) =&gt; {   const operation = event.operation;   if(event.tableName) {      event.payload.TableName = event.tableName;   }   switch(operation) {   case'create':      dynamo.putItem(event.payload, callback);      break;   case'read':      dynamo.getItem(event.payload, callback);      break;   case'update':      dynamo.updateItem(event.payload, callback);      break;   case'delete':      dynamo.deleteItem(event.payload, callback);      break;   case'list':      dynamo.scan(event.payload, callback);      break;   case'echo':      callback(null, event.payload);      break;   case'ping':      callback(null, 'pong');      break;   default:      callback(newError(`Unrecognized operation \"${operation}\"`));   }};Obviously you will need a DynamoDB instance with some data in it:You can reference your new table, from your lambda, using the following event:{\"tableName\": \"garage-car-dev\",\"operation\": \"list\",\"payload\": { }} The only difficult part remaining, is finding out the required payload for the different operations :) This is a good start for creating new records:{\"operation\": \"create\",\"tableName\": \"garage-car-dev\",\"payload\": {   \"Item\": {      \"id\": \"1980b61a-f5d7-46e8-b62a-0bbb91e20706\",      \"body\": \"Lamborghini\",      \"updatedAt\": \"1467559284484\"      }   }}The blueprint also generates an API in the API Gateway that we can invoke with the above events as body mapping template in integration request of the method execution, just like the first example.Serverless FrameworkWhile the above approach works as expected, it’s quite cumbersome to get your first function working. Especially since we didn’t write any actual code in the previous examples. Luckily the Serverless Framework (formerly JAWS) is here to make our lives easier. Currently the Serverless Framework only supports AWS Lambda, but support for other IaaS providers is coming. A pull-request for Microsoft Azure already exists and other providers are also working on an implementation. Vendor-neutral FaaS would be a true game-changer!One problem with FaaS, is the (deliberate) mismatch between runtime unit and deploy unit. This is also true for other architectural patterns. It should be possible to deploy one specific function, but often functions will hang out in groups. I’d prefer to deploy a group of functions in one go, when it makes sense, eg. different CRUD operations on the same resource. This way, we benefit from the advantages of functions (scalability, cost, service independence, …) but also ease deployment. This is a key feature of the Serverless Framework.On June 29th, Serverless V1.0-alpha1 was announced. New Alphas and Betas will be released on a regular basis. Currently the documentation can only be found in their v1.0 branch on GitHub. Serverless V1.0 introduces the “Serverless Service” concept, which is a group of functions with their specific resource requirements. In essence Serverless V1.0 is a powerful and easy to use CLI to create, deploy and invoke functions. Serverless V1.0 uses AWS CloudFormation to create AWS resources. It uses the default AWS profile for access to your AWS account. Creating, deploying and invoking a “Hello World” NodeJS function with Serverless is as easy as:serverless create --name cars --provider aws serverless deploy serverless invoke --function hello --path data.jsonThis generates the following lambda:'use strict'; module.exports.hello = (event, context, cb) =&gt; cb(null,    { message: 'Go Serverless v1.0! Your function executed successfully!', event } ); The current version of the Serverless Framework (unfortunately) doesn’t use the region from the AWS config, so you might need to look for your function in a different region.Adding an API Gateway endpoint, is also very easy and can be done by adding this http-event:events:  - http:       path: greet       method: getThe actual URL can be found in the API Gateway in the stages section, as we saw before.Part 2In the first part of these article, I introduced the Serverless architectural style and focused on “market maker” AWS Lambda and on the Serverless Framework. In this part, I want to focus on other Faas providers.Auth0 WebtaskCompared to giants such as Amazon, Google, Microsoft and IBM, Auth0 is a rather small player. However acknowledging their experience with BaaS (Backend as a Service), FaaS is a logical choice for them. Currently Webtask only supports NodeJS.The recommended way of using webtask is through the wt command line interface. Auth0 has put the focus on easy of use. This is really visible by looking at their 30 second example. The wt create command wil generate a function (a webtask) and will automatically return an HTTP endpoint, supporting URL query parameters. Every query parameter is available in your webtask in the form of context.data JavaScript object. With AWS Lambda you need to configure these in the AWS API Gateway, which is both tedious and time-consuming.A very interesting feature of Webtask is the availability of built-in storage.Webtask code can store a single JSON document up to 500KB in size. This data can be stored with ctx.storage.set and retrieved with ctx.storage.get. While I don’t believe your function will often need this, it’s a very nice option.This small example (using Lodash), shows a webtask using a query parameter and built-in storage.module.exports = function (ctx, cb) {    var name = ctx.query.name;     if(name) {        ctx.storage.get(function(err, data){            if(err) cb(err);             data = data || [];             if(_.indexOf(data, name) === -1 ){                data.push(name);                 ctx.storage.set(data, function(err){                    if(err){                        cb(err);                    } else {                        cb(null, data);                    }                })            } else {                cb(null, data);            }        })    } else {        cb(null, \"422\");    }}Deploying this webtask, using the CLI:Webtask created You can access your webtask at the following url: https://webtask.it.auth0.com/api/run/wt-&amp;lt;your username&amp;gt;-0/query_store?webtask_no_cache=1Another way to access your webtask is as a CRON job, using the wt cron command or as a web hook.Contrary to AWS Lambda, you don’t need to bundle the NodeJS modules you want to use. The list of supported modules is available here. An option to bundle other modules is also available. Another difference is the use of query parameters.Not surprisingly, Webtask can be integrated with Auth0 for authentication and authorization.Google Cloud FunctionsGoogle Cloud Functions (GCF) was released early 2016 and is currently in private alpha. Being in private alpha not only means that you specifically need to request access to use the GCF API, but also that you’re limited in sharing information. While this is obviously very unfortunate, it also means that Google is very serious about releasing a complete product. The activity in their (again private) Google Group proves this.Like its competitors, Cloud Functions can be triggered asynchronously by events (from Cloud Pub/Sub and Cloud Storage) or invoked synchronously via HTTPS. Currently GCF only supports NodeJS. Tutorials on common use-cases are available in their documentation. To build functions with GCF, you will first need to download and install the Google Cloud SDK. With the SDK installed, you can create your initial function (replace datastore_gcf with your own staging bucket name):$ gsutil mb gs://datastore_gcfFrom the (very useful) (unofficial) GCF recipes by Jason Polites (Product Manager, GCP), we cloned the datastore example that will persist data to a Google Coud Datastore.From this repository, we deployed 2 functions ‘ds-get’ and ‘ds-set’ by executing:$ gcloud alpha functions deploy ds-set --bucket datastore_gcf --trigger-http --entry-point setThe names of the deployed functions, need to be exported in the Node.js module. These functions can be called with:$ gcloud alpha functions call ds-get --data '{\"kind\": \"test\", \"key\": \"kid\"}'or via the Cloud Functions Console.Your newly added data is also available in the Datastore Entities after selecting a project on the top. After executing a couple of functions, you can also find some metrics of your function (number of calls, execution time, …)Other arguments for the deploy command are listed in the reference documentation. These steps are also available in the Cloud Platform Console.After deployment, your webtrigger URL will be displayed similar to Webtask.Although much information on Google Cloud Functions is not (publicly) available yet, Google is well on its way to become a serious FaaS provider.Azure FunctionsSimilar to Google Cloud Functions, Microsoft Azure Functions is currently in preview stage, meaning it’s not (yet) meant to be used in a production environment. Azure Cloud Functions (ACF) support a variety of languages such as NodeJS, C#, Python, and PHP.Today, it can be used for these common cases:  Events triggered by other Azure services  Events triggered by SaaS services (not limited to Microsoft)  Synchronous requests  WebHooks  Timer based processing (CRON)creating quite a large number of possibilities.Azure Functions are grouped in App Services. This is quite different from AWS Lambda, where the functions are organised independently. Hardware resources are allocated to an App Service and not directly to an Azure Function. It’s important to select a dynamic App Service if you’re aiming for “pay-per-execution”.When creating a new function, you can start from different templates. This can be compared to the blueprints from AWS Lambda. Currently 44 templates are available (but some are very similar). When selecting HttpTrigger for example, Azure Functions will generate a function that is able to use all query parameters passed to the function, similar to Webtask. This short video demonstrates this use case.In the example below, an Azure Cloud Function will store entities in a Storage Table when it receives an HTTP request:function.json: \"bindings\": [    {      \"type\": \"httpTrigger\",      \"direction\": \"in\",      \"name\": \"req\",      \"methods\": [        \"post\"      ],      \"authLevel\": \"function\"    },    {      \"type\": \"http\",      \"direction\": \"out\",      \"name\": \"res\"    },    {      \"type\": \"table\",      \"name\": \"outTable\",      \"tableName\": \"entities\",      \"partitionKey\": \"functions\",      \"rowKey\": \"%rand-guid%\",      \"connection\": \"YOUR_STORAGE\",      \"direction\": \"out\"    }  ],  \"disabled\": false}index.js:    var statusCode = 400;    var responseBody = \"Invalid request object\";     if (typeof req.body != 'undefined' &amp;amp;&amp;amp; typeof req.body == 'object') {        statusCode = 201;        context.bindings.outTable = req.body;        responseBody = \"Table Storage Created\";    }     context.res = {        status: statusCode,        body: responseBody    };     context.done();};To retrieve the added entities:functions.json:  \"bindings\": [    {      \"type\": \"httpTrigger\",      \"direction\": \"in\",      \"name\": \"req\",      \"methods\": [        \"get\"      ],      \"authLevel\": \"function\"    },    {      \"type\": \"http\",      \"direction\": \"out\",      \"name\": \"res\"    },    {      \"type\": \"table\",      \"name\": \"inTable\",      \"tableName\": \"entities\",      \"connection\": \"YOUR_STORAGE\",      \"direction\": \"in\"    }  ],  \"disabled\": falseindex.js:    context.log(\"Retrieved records:\", intable);    context.res = {        status: 200,        body: intable    };    context.done();};What immediately struck me was the quality of their documentation (videos, tours, quickstarts, templates, …) and the user experience from the Azure Portal. The portal can be a little slow sometimes, but the experience is miles ahead of what Amazon and Google are offering. Azure Functions is open source and available on GitHub.Azure Functions will soon be supported by the Serverless Framework, which is a big step towards vendor-neutral FaaS.IBM Bluemix OpenWhiskBluemix OpenWhisk is also an open source service and currently supports NodeJS and Swift. Contrary to other FaaS providers, IBM emphasises on container integration. When an event or an API call invokes an action, OpenWhisk creates a container to run the action in a runtime appropriate to the programming language used. You can even create Docker functions (called actions in OpenWhisk) allowing you to build in any language. OpenWhisk can also run locally on your own hardware, which no other provider currently offers. IBM is very open about this and even provides guidelines on how this can be achieved.As expected, the documentation has a getting started guide to build and run a Hello World action. While working with the CLI works as advertised, it quickly becomes quite cumbersome, especially when integrating with other Bluemix services. After executing your first OpenWhisk function, you can see some metrics in the (pretty) OpenWhisk dashboard. The OpenWhisk dashboard will show all invoked actions, also from actions you didn’t implement yourself. For example when using existing packages.What’s even more impressive is the Openwhisk Editor. This editor only lists the actions you created yourself.As you can see from the screenshot, you immediately get links to the REST Endpoint.ConclusionCurrently it’s too soon to draw any conclusions. These services are constantly changing. What is obvious, is that all major cloud providers want to make sure that they don’t miss the FaaS opportunity. Cloud providers create value by integrating FaaS services with their other offerings. This confirms the value of a Serverless Cloud. The current FaaS solutions have a lot of similar characteristics and choosing one, will likely depend on what other services you already use (or want to use) from a certain provider. It’s important to know the environment your FaaS code lives in and the services available to it. In this phase available documentation also is crucial.Obviously, this high-level introduction doesn’t list all the differences or similarities, but it offers a nice starting point to experience the FaaS (r)evolution first-hand.Part 3In the first part of this article, I introduced the Serverless architectural style. In the second part, I compared all major serverless providers. In this third and last part, I would like to look at serverless as an enabler of collaborative economy.Collaborative EconomyWhat is collaborative ecomomy?  Benita Matofska: The Sharing Economy is a socio-economic ecosystem built around the sharing of human, physical and intellectual resources.It includes the shared creation, production, distribution, trade and consumption of goods and services by different people and organisations.The last part of Benita’s quote: shared creation, production .. of services by different people and organisations makes a very nice use-case for the serverless style of building applications.Your dataIn this day and age, all companies have become IT companies, meaning a lot of data is gathered and stored somewhere. Often the usage of the available data changes over time. If data is not used for the benefit of the enterprise or its employees, does it still hold value? Wouldn’t it be great if we could turn cost into profit?Thanks to its cost model (pay per execution), its focus on scalability (no risk of overprovisioning) and resilience, serverless enables companies to experiment with exposing their data:  Offering an API for others to consume  Enriching existing API’s with their data  …Your ideasServerless also makes a lot of sense for companies that don’t want to expose their data, but have great or new ideas on how to use others data:  Combining data from multiple providers  Filtering and transforming data  New business cases beyond the scope of the original API  …ExampleI implemented a small and simple application that will consume data from different serverless cloud providers. Every “hop” in the system will parse its input and add some new data.Component diagramDescriptionAny client can post a JSON to the first function, made with Auth0 webtask. The body of the post request is simple:{\"temp\":\"42\"}The WebTask will parse that input, add some input of its own and POST request to an IBM OpenWhisk action. The body of this POST request:{  \"hops\": [    {      \"provider\": \"Auth 0 Webtask\",      \"start\": \"2016-08-24T20:32:03.629Z\",      \"temperature\": \"42\",      \"stop\": \"2016-08-24T20:32:03.629Z\"    }  ]}To continue the chain, IBM OpenWhisk will POST the parsed JSON to a function on the AWS Lambda platform after adding a new “hop”:{  \"hops\": [    {      \"provider\": \"Auth 0 Webtask\",      \"start\": \"2016-08-26T18:38:25.021Z\",      \"temperature\": \"44\",      \"stop\": \"2016-08-26T18:38:25.021Z\"    },    {      \"provider\": \"IBM OpenWhisk\",      \"start\": \"2016-08-26T18:38:35.024Z\",      \"temperature\": \"42\",      \"stop\": \"2016-08-26T18:38:35.024Z\"    }  ]}The Lambda, created with Serverless V1.0 Beta 2 will parse the input again and create items in an AWS DynamoDB:The AWS DynamoDB table will stream events to another AWS Lambda that will log the content of the event to the logs of AWS CloudWatch:The source code of all these components is available on GitHub.Best practiceObviously I wouldn’t recommend anyone to use a different cloud provider for every function. Choosing the right one will depend on your specific needs, goals and current cloud landscape. In previous parts of this article, you may find some tips on how to make a reasoned choice.Final noteThis article was originally posted in three parts on the JAX London blog and is also available in German."
      },
    
      "kickstarters-2016-10-31-kickstarters-project-html": {
        "title": "Kickstarter project 2016",
        "url": "/kickstarters/2016/10/31/Kickstarters-Project.html",
        "image": "/img/kicks.png",
        "date": "31 Oct 2016",
        "category": "post, blog post, blog",
        "content": "  On August 1‘st it was D-day for all the kickstarters that had recently joined Ordina. A batch of talented new people were ready to embark on a new adventure. This year around fifty people joined Ordina and participated in the Kickstarter Project. The JWorks kickstarter group consisted of seven people, all of which were eager to get started. Six people joined the JWorks unit and one joined the Security unit. The purpose of the two month long kickstarter project is to broaden the knowledge of and prepare the kickstarters for their first project.Kickstarter project 2016First impressionsYou never get a second chance to make a first impression.– Harlan HoganAnd boy Ordina did a pretty good job!The reception on the first day was really great and pretty informal.First off the kickstarters received a tour of the company.They introduced themselves and got to know eachother in a pretty playful way.FINALLY the moment had arrived that everybody was waiting for !The kickstarters received their highly anticipated company car and laptop.The overal atmosphere is pretty loose, you can ask anyone anything and you can talk to everybody.After a few days the kickstarters also got the chance to go on a teambuilding day in Mechelen.During the course of this day, they had to work together as a team to complete some questions and games.The winners were rewarded with a cup. This enabled them to learn how to communicate in a team and under stress, because some of the tests had to be completed within an certain amount of time.Most of the kickstarters already had the chance to go to one of the Ordina events like JOIN or CC meetings, where they networked with lots of interesting people.August - LearningThe focus was primarily on learning during the first month of the Kickstarters project.The first few days the kickstarters had to improve their softskills by learning how to be more assertive towards the client when necessary.They also learned to introduce themselves properly with the emphasis on their qualities and strengths, to ensure they make a good first impression of themselves when going to the client.The kickstarters were brought up to date with the preferred technologies, editors and best practices used by JWorks.During the first month they received different courses in which they could improve their technical skills about:  backend          Java (7 &amp; 8) + JavaEE      Spring      JPA      Webservices (REST &amp; SOAP)      MongoDB        frontend          HTML &amp; CSS      Javascript &amp; TypeScript      Angular      Ionic      These courses were given by the JWorks unit who tried to teach the kickstarters a much as possible with theoretical material and some exercises afterwards.Unfortunately, these technical skills aren’t enough to survive in the forever changing IT world.This is why some extra help was provided in the form of books and courses about how to write clean code, how to work agile and learning how to work in a team while understanding and using the SCRUM principles.By paying attention to technical development with the necessary certification processes and also focusing on the development of soft skills like communication, advising and collaboration,Ordina commits to the personal development of these kickstarters.September - Dev-caseThe focus during the second month was on the implementation of this year’s dev-case.Although they still had to follow a few courses along the way, like the basic priciples of security, GIT and learning how to use MongoDB.SensyMcSenseFace was born– Chosen by popular vote, who would have guessed it…The kickstarters already had learned how to write clean code and how to do this in the best possible way.The purpose of the SensyMcSenseFace project was to give the kickstarters a use case where they could develop an end-to-end IoT solution, in which they could test and use their newly acquired skills.What did the client requestThe kickstarters were given the task to build an application that accepts incoming data, while being able to process this data and output it in a more user friendly way.The data would be sent by three different sensors:  Temperature sensor  Humidity sensor  Motion sensorThese sensors send some data every few seconds to the backend, the backend then processes this data and sends it back to the frontend.Here the frontend developers made sure that all the data has been received and outputted in the correct way.The following picture depicts the two meeting rooms that are equipped with three different sensors, which send their data back to the application’s backend.Each meeting room equipped with the sensors, which have their values read by an Arduino that then sends these across the Proximus LoRa network to the backend. For the initial stages and testing the LoRa part was omitted and a simple node server instance was used to relay the sensor values to the actual backend.This way, the client (Ordina) could figure out when they are using excessive power.For example people leaving the TV on for too long inside of the meeting rooms.Used technologies - sofware  backend          Maven      Spring      Spring Boot      MongoDB      Mockito        frontend          Angular 2      Angular Material 2        extra          Waffle      Cloudfoundry      GitHub      Process of the projectThe first week went pretty well, they divided themselves up into two groups.One group for the frontend and the other one for the backend.At first they started to create different user stories for their SCRUM board.Using the newly acquired scrum techniques during the first month.The kickstarters had to work in four short sprints of one week. During the first sprint they also decided to change their real life SCRUM board into an online version using Waffle.This software would track the pull requests and merges automatically from Github and change the board accordingly.Continuous Integration was pretty important during the course of the project.This way, whenever they made changes to the code and made a pull request to Github that failed to build, they had to fix their code before they could continue.Once the build on codeship succeeded and the pull request was merged. The main development branch and master branch would have their changes (if any) deployed to their Cloudfoundry instance.The process for the backend was pretty simple.They started out with around five people, so some of them started to pair program while others started to program on their own.But, with paying attention to the SCRUM principles and how to write clean code.Their first job was to start with the basic implementations of the sensor, room and notifications classes and writing the JUnit and Mockito tests.While the backend was pretty straight forward and relatively easy to begin with, the frontend team were confronted with some problems.The team consisted of only two people who had to tackle a lot of problems with the use of Angular 2 which was still in Beta at the time.Also the combination with the other frameworks wasn’t quite that easy to work with.Every morning the team did a stand up meeting where they would discuss their changes in eachothers code and what they were going to do next.During these four weeks they tried to work in these short sprints but after a week or two it became clear this wouldn’t be an easy task.A few team members already had left the group because they were assigned to projects, which messed up their sprints completely.During the first two weeks the backend team started with using Spring Data JPA, but soon figured out Spring Data MongoDB was the better alternative because of the large amount of data being pumped into the DB.A lot of time of went into providing REST documentation that covers all the different calls handled by the application.This documentation was created with MockMvc tests, which create code snippets that were easy to use.On the frontend side they didn’t have test cases yet, but nobody in the entire team had ever written frontend tests before.Which caused a little bit of a delay, also the webpages didn’t seem to be responsive at all.Luckily there were some online tutorials available on mocking and writing frontend tests.The responsiveness issue was sovled by using their own components and CSS code instead of using material design.When starting their two final weeks there were only three people left who were able to fully commit to the dev-case.During the last week the kickstarters also had to prepare and give a proper introduction to the management of Ordina.Showing off their newly learned presentation and introduction techniques.At the very end the core of the application was finished.  You are able to watch an overview of the rooms  You are able to check if a room is occupied or available  You are able to check which sensors are in a room and check their last history  You are able to check the sensor history between two timestamps  You are able to get notifications on your cellphone when certain values are exceededpossible future changesBecause there wasn’t enough time to completely finish the project, this project can still evolve in a lot of ways.  Later on it could be possible to add roles or users.  Make adding sensors or rooms more user friendly.  Add predictions to the applications for every room.  Add user management with users and rolesLessons learned during the Kickstarters project  How to be more confident  How to introduce yourself in a professional way  How to be more assertive  You have to keep an eye out for possible changes in your code that encourage clean code  How to work better and agile in a team and how to use SCRUM principles  Pitfalls and difficulties when using and combining new technologies  How to write proper tests (JUnit, Mockito, MvcTests)  How to write proper REST documentationThe new JWorks colleagues"
      },
    
      "conference-2016-10-10-percona-live-amsterdam-2016-html": {
        "title": "Percona Live Amsterdam 2016",
        "url": "/conference/2016/10/10/Percona-Live-Amsterdam-2016.html",
        "image": "/img/2016-10-16-Percona-Live/PLAM-16-01.png",
        "date": "10 Oct 2016",
        "category": "post, blog post, blog",
        "content": "Percona Live Open Source Database Conference 2016, Amsterdam  It was only three weeks before the conference, that I coincidentally discovered (and by Googling) that Percona was organizing one of the biggest Open Source Databases conferences in Amsterdam, Percona Live Europe.Until then I had never heard of Percona.Shame on me! But, Percona is mostly known in the US and according to db-engine.com it’s takes the 47th  place on the popularity list of Relational Databases, and the 97th spot if you consider all database systems.Yet, Percona is celebrating its 10th anniversary this year and among it’s more than 3000 customers worldwide it can count well-known brands like Cisco Systems, Time Warner Cable, Alcatent-Lucent, Groupon, BBC, Flickr, … among  it’s customers.It was at the conference that I uncovered that Percona Server is in fact a fork of the MySQL open source database, just like the more popular fork MariaDB.Percona presents itself on their website as the only company that delivers enterprise-class solutions for both MySQL and MongoDB across traditional and cloud-based platforms.So it was obvious that the focus of the three-day conference was on MySQL and MongoDB.With 16 tutorials on Monday, 48 sessions on Tuesday and even 64 sessions on Wednesday the line-up was impressive.Most of the sessions covered MySQL, MongoDB and PostgreSQL topics but other open source databases like ElasticSearch, Redis, RethinkDB, Clickhouse,… were also discussed.Even with two people of JWorks attending the Conference, picking the right sessions was difficult and the FoMO syndrome was clearly around the corner.While the conference mainly had a technical focus (sometimes in-depth), with subjects as analytics, architecture and design, security, operations, scalability and performance, I was pleased that these sessions were alternated with customer stories.Eventually it is always interesting to see real life uses cases and big names like Facebook, Uber, Dropbox and Booking.com talking about the open source databases they use, and especially how they use them.Facebook was represented by 10 of its employees involved in 7 talks.To pick some : Shared MySQL hosting at Facebook, Massive Schema changes in Facebook, MyRocks Deep Dive: Flash Optimized LSM Database for MySQL, and its Use Case at Facebook, Online Shard Migration at Facebook,…The conference made it also clear that interest in Open Source Databases continues to grow.More and more companies are looking to replace their proprietary databases to open source alternatives.The reasons for that are clear, open source databases maturity have risen to the level of the proprietary databases and some of them have even gone beyond that.And al of that, without the hefty price tag.  Top Internet applications have embraced open source databases a long time ago, and now traditional enterprises are catching up to.So me and my colleague Chris De Bruyne both returned with a backpack full of very useful information that we like to share with you in the coming months.We will definitely try out a lot of stuff, like the different open source monitoring tools for MySQL and MongoDB.The JWorks DBA / NoSQL competence center also advocates the use of open source alternatives when appropriate, and this conference perfectly matches with our ambitions.So we already reserved 25th - 27th of September 2017 in our agendas for the next Percona Live Europe in Dublin."
      },
    
      "conference-2016-09-27-join-2016-html": {
        "title": "JOIN 2016",
        "url": "/conference/2016/09/27/JOIN-2016.html",
        "image": "/img/join16.jpg",
        "date": "27 Sep 2016",
        "category": "post, blog post, blog",
        "content": "  Next week, on the 5th of October 2016, the JWorks Business Unit of Ordina will organize its yearly JOIN event. The purpose of this event is to share knowledge between colleagues and fellow Java, JVM, JavaScript, Cloud and DevOps enthusiasts. Last year, a total of 83 attendees visited Ordina Belgium’s headquarters in Mechelen to learn and talk about the hottest technology trends and developments.This year we expect to have more than 100 attendees, and what’s most exciting is that the event is completely free of charge and everyone is invited. Food and drinks are provided (including a barbecue).JOIN 2016 Schedule  Here are some of the highlights of the day:10AM - 12AM - Docker for Java Developers - Arun GuptaArun has been an avid Docker user for many years and is also one of the Docker Captains, among being a Java Champion and JUG leader.He will bring us a very informative talk about the current state of Docker and how Java Developers can get started with Docker in no time.We’re very much looking forward to this one!  During another talk he will talk about Couchbase, the product company he’s working for at the moment.More information about Arun in his Docker Community Spotlight.5PM - 6PM - The Google Cloud Platform - Koen MaesKoen Maes provides expert advice and development services for Google Cloud Platform and related products.He is a Google Cloud Platform Authorized Trainer &amp; partner and has been working in the software industry since the early nineties and with web/Internet technology since its inception.He designed key applications for several large corporations as well as running a handful of startups of his own, some more successful than others.Since his first encounters with AppEngine in 2009, he never looked back and has been specializing in Google Cloud Platform ever since.  We are currently working for one of our customers on a large scale greenfield microservices system in the Google Cloud Platform, so this will be especially interesting for us.6PM - 7PM - Reactive Programming - Stephane MaldiniA multi-tasker eating tech 24/7, Stephane is interested in cloud computing, data science and messaging.Leading the Reactor Project, Stephane Maldini is on a mission to help developers create reactive and efficient architectures on the JVM and beyond.He is also one of the main contributors for Reactive support in the upcoming Spring 5 framework.  David Karnok, RxJava project lead, identifies the Reactor project as the new standard for reactive applications in the Java world.Most of the developers in our JWorks unit are using Spring (as opposed to JEE), so this talk is going to be very interesting.7PM - 8PM - TypeScript: enjoying large scale browser development - Joost De VriesJoost De Vries is one of our Dutch colleagues at Codestar. He will talk about TypeScript and how it enables development of large scale front end applications.You can find him on Twitter and Github.FoodWe have foreseen food and drinks during the conference and we will end the night with something special.We hope and believe it will make everyone very happy! Another reason to JOIN us @Ordina, the 5th of October:  Morning reception:          Coffee or Tea      Mini biscuits and chocolates        Lunch:          Luxury sandwiches &amp; subs with salads, French cheese, grey shrimp, prawns or Parma ham      Viking bread with smoked salmon      Grilled chicken wraps      Vegetarian options also available        Afternoon coffee break:          Coffee or Soda      Candy bars or fruit salad        Dinner:          BBQ      Marinated prawn (scampi)      Greenway balls (vegetarian)      Spicy chipolatas      Marinated chicken satés      Steak chimichurri cut and grilled à la minute      Coleslaw, tomatoes, cucumbers, carrots, potato salad, mix of salads, red onions and olives      Bread, potatoes and sauces      "
      },
    
      "monitoring-2016-09-23-monitoring-with-prometheus-html": {
        "title": "Monitoring with Prometheus",
        "url": "/monitoring/2016/09/23/Monitoring-with-Prometheus.html",
        "image": "/img/prometheus.jpg",
        "date": "23 Sep 2016",
        "category": "post, blog post, blog",
        "content": "It is needless to say the world is shifting towards DevOps and microservices.This holy grail we aim for adds a great deal of complexity.Monitoring included.Rather than having to monitor one system,we are suddenly faced with the challenge to oversee our manifold services.There are numerous monitoring systems available,but not all of them are fit for monitoring large, distributed systems.Black box monitoring systems like Nagios allow you to check if an application is alive and healthy.This is done by e.g. pinging the service,checking if there is enough disk space,or monitoring the CPU usage.In a world of distributed architectures where high availability and fast response times are key,it is not sufficient to be only aware if a service is alive.It is crucial to know how a service is working internally as well.How many HTTP requests is it receiving?Are they handled correctly?How fast are requests handled for different endpoints?Are there many errors being logged?How many disk IO operations is the service performing?These are all important questions that need to be monitored to keep a service functional.Prometheus is a white box monitoring and alerting system that is designed for large, scalable environments.With Prometheus,we can answer all these questions,by exposing the internal state of your applications.By monitoring this internal state,we can throw alerts and act upon certain events.For example,if the average request rate per second of a service goes up,or the fifty percent quantile response time of a service suddenly passes a certain threshold,we could act upon this by upscaling the service.Overview  The Rise of Prometheus  Architecture  Data Model  Slice &amp; Dice with the Query Language  Instrumenting Your Services  Exporters  Scraping the Targets  Visualization and Analytics  Alert! Alert!  Monitoring Time!  Final WordsThe Rise of PrometheusAs with most great technologies,there is usually a great story hiding behind them.Nothing is different with Prometheus.Incubated at SoundCloud,the social platform for sharing sounds and music,Prometheus has come a long way.When SoundCloud was just a start-up,they originally developed their application as a single application.Many features later,this resulted in one big, monolithic application called the Mothership.With only a few thousand artists and users sharing music,the application performed sufficiently.However,nowadays,about 12 hours of music is uploaded every minute to SoundCloud.The platform is used by hundreds of millions of users every day.To be able to handle this size of volume,SoundCloud adapted a more scalable approach.Deciding against a complete rewrite of their whole technology stack,they stopped adding new features to the Mothership.Instead, new features were written as microservices,living next to the Mothership.If you want to know more about how SoundCloud moved from one monolithic application to a microservices architecture,you can find a three-partblog postseries on their developer blog (which is an excellent read, by the way).Moving towards a microservices architecture paved the way for many possibilities for SoundCloud,but it also introduced a lot of complexity.Monitoring a single application is easy.Monitoring hundreds of different services with thousands of instances is an entirely different story.SoundCloud’s original monitoring set-up consisted of Graphite and StatsD.This setup did not suffice for the new, scalable microservices architecture.The amount of generated events could not be handled in a reliable way.SoundCloud started looking for a new monitoring tool,while keeping the following requirements in mind:      A multi-dimensional data model,where data can be sliced and diced along multiple dimensions like host, service, endpoint and method.        Operational simplicity,so that you can setup monitoring anywhere you want,whenever you want,without having to have a Ph.D. in configuration management.        Scalable and decentralized,for independent and reliable monitoring.        A powerful query language that utilizes the data model for meaningful alerting and visualisation.  Since no existing system combined all of these features,Prometheus was born from a pet project at SoundCloud.Although the project has been open source from the beginning,SoundCloud did not make any noise about it until the project was mature enough.In January 2015,after 2 years of development and internal usage,the project was publicly announcedand a website was put online.The amount of attention it received was totally unexpected for the team at SoundCloud.After a post on Hacker News,which made it all the way to the top,things got serious.There was a sharp rise in contributions, questions, GitHub issues, conference invites, and all that stuff.The following image depicts the amount of stars the project received on GitHub since its inception.ArchitecturePrometheus’ architecture is pretty straightforward.Prometheus servers scrape (pull) metrics from instrumented jobs.If a service is unable to be instrumented,the server can scrape metrics from an intermediary push gateway.There is no distributed storage.Prometheus servers store all metrics locally.They can run rules over this data and generate new time series,or trigger alerts. Servers also provide an API to query the data.Grafana utilizes this functionality and can be used to build dashboards.Finally,Prometheus servers know which targets to scrape from due to service discovery,or static configuration.Service discovery is more common and also recommended,as it allows you to dynamically discover targets.Data ModelAt its core,Prometheus stores all data as time series.A time series is a stream of timestamped values that belong to the same metric and the same labels.The labels cause the metrics to be multi-dimensional.For example,if we wish to monitor the total amount of HTTP requests on our API,we could create a metric named api_http_requests_total.Now,to make this metric multi-dimensional,we can add labels.Labels are simple key value pairs.For HTTP requests,we can attach a label named method that takes the HTTP method as value.Other possible labels include the endpoint that is called on our API,and the HTTP status returned by the server for that request.The notation for a metric like that could be the following:api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}Now,if we start sampling values for this metric,we could end up with the following time series:            Metrics      Timestamp      Value                  api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}      @1464623917237      68856              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"500\"}      @1464623917237      5567              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}      @1464624516508      76909              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"500\"}      @1464624516508      6789      One of the great aspects of time seriesis the fact that the amount of generated time series is independent of the amount of events.Even though your server might suddenly get a spike in traffic,the amount of time series generated stays the same.Only the outputted value of the time series is different.This is wonderful for scalability.Prometheus offers four metric types which can be used to generate one or multiple time series.A counter is a metric which is a numerical value that is only incremented,never decremented.Examples include the total amount of requests served,how many exceptions that occur, etc.A gauge is a metric similar to the counter. It is a numerical value that can go either up or down.Think of memory usage, cpu usage, amount of threads, or perhaps a temperature.A histogram is a metric that samples observations.These observations are counted and placed into configurable buckets.Upon being scraped,a histogram provides multiple time series,including one for each bucket,one for the sum of all values,and one for the count of the events that have been observed.A typical use case for a histogram is the measuring of response times.A summary is similar to a histogram,but it also calculates configurable quantiles.Depending on your requirements,you either use a histogram or a summary.Slice &amp; Dice with the Query LanguageA powerful data model needs a powerful query language.Prometheus offers one,and it is also one of Prometheus’ key features.The Prometheus query language,or promql,is an expressive, functional language.One which apparently,by the way,is Turing complete.The language is easy to use.Monitoring things like CPU usage,memory usage, amount of HTTP request served, etc. are pretty straightforward,and the language makes it effortless.Using an instant vector selector,you can select time series from a metric.For example,Continuing with our API example,we can select all the time series of the metric api_http_requests_total:api_http_requests_totalWe can dive a little bit deeper by filtering these time series on their labels using curly braces ({}).Let’s say we want to monitor requests that failed due to an internal server error.We can achieve this by selecting the time series of the metric api_http_requests_totalwhere the label status is set to 500.api_http_requests_total{status=\"500\"}We can also define a time window if we only want to have time series of a certain period.This is done by using a range vector selector.The following example selects time series of the last hour:api_http_requests_total[1h]The time duration is specified as a number followed by a character depicting the time unit:  s - seconds  m - minutes  h - hours  d - days  w - weeks  y - yearsYou can go further back in time by using an offset.This example selects time series that happened at least an hour ago:api_http_requests_total offset 1hWe can use functions in our queries to create more useful results.The rate() function calculates the per-second average rate of time series in a range vector.Combining all the above tools,we can get the rates of HTTP requests of a specific timeframe.The query below will calculate the per-second rates of all HTTP requeststhat occurred in the last 5 minutes an hour ago:rate(api_http_requests_total{status=500}[5m] offset 1h)A slightly more complex example selects the top 3 endpoints which have the most HTTP requestsnot being served correctly in the last hour:topk(  3, sum(    rate(api_http_requests_total{status=500}[1h])  ) by (endpoint))As you can see,Prometheus can provide a lot of useful information with several simple queries that only have a few basic functions and operators.There is also support for sorting, aggregation, interpolation and other mathematical wizardry that you can find in other query languages.Instrumenting Your ServicesOne of the requirements to be able to query data and get results,obviously,is the fact that there must be data that can be queried.Generating data can be done by instrumenting your services.Prometheus offers client libraries forGo,Java/Scala,Python andRuby.There is also a lengthy list of unofficial third-party clients for other languages,including clients for Bash and Node.js.These clients enable you to expose metrics endpoints through HTTP.This is totally different compared to other,more traditional,monitoring tools.Normally,the application is unaware that it is being monitored.With Prometheus,you must instrument your codeand explicitly define the metrics you want to expose.This allows you to generate highly granular data which you can query.However,this technique is not much different than logging.Logging statements are,most of the time,also explicitly defined in the code,so why not for monitoring as well?For short-lived jobs,like cronjobs,scraping may be too slow to gather the metrics.For these use cases,Prometheus offers an alternative,called the Pushgateway.Before a job disappears,it can push metrics to this gateway,and Prometheus can scrape the metrics from this gateway later on.ExportersNot everything can be instrumented.Third-party tools that do not support Prometheus metrics natively,can be monitored with exporters.Exporters can collect statistics and existing metrics,and convert them to Prometheus metrics.An exporter,just like an instrumented service,exposes these metrics through an endpoint,and can be scraped by Prometheus.A large variety of exporters is already available.If you want to monitor third-party software that does not have an exporter publicly available,you can write your own custom exporterScraping the TargetsPulling metrics from instances is called scraping.Scraping is done at configurable intervals by the Prometheus server.Prometheus allows you to configure jobs that fetch time series from instances.global:  scrape_interval: 15s # Scrape targets every 15 seconds  scrape_timeout: 15s # Timeout after 15 seconds  # Attach the label monitor=dev-monitor to all scraped time series scraped by this server  labels:    monitor: 'dev-monitor'scrape_configs:  - job_name: \"job-name\"    scrape_interval: 10s # Override the default global interval for this job    scrape_timeout: 10s # Override the default global timeout for this job    target_groups:    # First group of scrape targets    - targets: ['localhost:9100', 'localhost:9101']      labels:        group: 'first-group'    # Second group of scrape targets    - targets: ['localhost:9200', 'localhost:9201']      labels:        group: 'second-group'This configuration file is pretty self-explanatory.You can define defaults for all jobs in the global root element.These defaults can then be overridden by each job,if necessary.A job itself has a name and a list of target groups.In most cases,a job has one list of targets (one target group),but Prometheus allows you to split these between different groups,so you can add different labels to each scraped metric of that group.Next to your own custom labels,Prometheus will additionally append the job and instance labels to the sampled metrics automatically.Visualization and AnalyticsPrometheus has its own dashboard,called PromDash,but it has been deprecated in favor of Grafana.Grafana supports Prometheus metrics out-of-the-boxand makes setting up metrics visualization effortless.After adding a Prometheus data source,you can immediately start creating dashboards using PromQL:                  Step 1: Create datasource                        Step 2: Profit      Alert! Alert!Prometheus provides an Alert Manager.This Alert Manager is highly configurable and supports many notification methods natively.You can define routes and receivers,so you have fine-grained control over every alert and how it is treated.It is possible to suppress alerts and define inhibition rules,so you can prevent getting thousands of the same alert if a many-node cluster goes down.Alerts can be generated by defining alerting rules.This is done in Prometheus and not in the Alert Manager.Here are a few simple alerting rule examples:# Alert for any instance that have a median request latency &gt;1s.ALERT APIHighRequestLatencyIF api_http_request_latencies_second{quantile=\"0.5\"} &gt; 1FOR 1mLABELS { severity=\"critical\"}ANNOTATIONS {  summary = \"High request latency on {{ $labels.instance }}\",  description = \"{{ $labels.instance }} has a median request latency above 1s (current value: {{ $value }}s)\",}ALERT CpuUsageIF cpu_usage_total &gt; 95FOR 1mLABELS { severity=\"critical\"}ANNOTATIONS {  summary = \"YOU MUST CONSTRUCT ADDITIONAL PYLONS\"  description = \"CPU usage is above 95%\"}After an alert is generated and sent to the Alert Manager,it can be routed using routes.There is one root route on which each incoming alert enters,and you can define child routes to route alerts to the correct receiver.These routes can be configured using a YAML configuration file:# The root route on which each incoming alert enters.route:  # The default receiver  receiver: 'team-X'  # The child route trees.  routes:  # This is a regular expressiong based route  - match_re:      service: ^(foo|bar)$    receiver: team-foobar    # Another child route    routes:    - match:        severity: critical      receiver: team-criticalThere are multiple types of receivers to which you can push notifications to.You can push alert notifications to SMTP,HipChat,PagerDuty,PushOver,Slack and OpsGenie.Additionally,you can use a web hook to send HTTP POST requests to a certain endpoint with the alert as JSON,if you wish to push notifications to somewhere else.Check out this guy’s audio alarm,which alerts him when his internet goes down!The receivers are configured in the same YAML configuration file:receivers:# Email receiver- name: 'team-X'  email_configs:  - to: 'alerts@team-x.com'# Slack receiver that sends alerts to the #general channel.- name: 'team-foobar'  slack_configs:    api_url: 'https://foobar.slack.com/services/hooks/incoming-webhook?token=&lt;token&gt;'    channel: 'general'# Webhook receiver with a custom endpoint- name: 'team-critical'  webhook_configs:    url: 'team.critical.com'Monitoring Time!Do you wish to get your hands dirty quickly with Prometheus?Perfect!I have prepared a project for demonstration purposes,which can be found on the Ordina JWorks GitHub repository.The project can be set up using only one command,leveraging Docker and Make.It covers most of the features discussed in this blog post.First clone the project with Git:$ git clone git@github.com:ordina-jworks/prometheus-demo.gitAfter the project is cloned,run make in the project directory:$ makeThis will compile all applications,build or pull all necessary Docker images,and start the complete project using Docker Compose.The following containers are started:$ docker psCONTAINER ID        IMAGE                              COMMAND                  PORTS                     NAMESc620b49edf4c        prom/alertmanager                  \"/bin/alertmanager -c\"   0.0.0.0:32902-&gt;9093/tcp   prometheusdemo_alertmanager_167b461b6a44b        grafana/grafana                    \"/run.sh\"                0.0.0.0:32903-&gt;3000/tcp   prometheusdemo_grafana_1920792d123bd        google/cadvisor                    \"/usr/bin/cadvisor -l\"   0.0.0.0:32900-&gt;8080/tcp   prometheusdemo_cadvisor_1215c20eb849b        ordina-jworks/prometheus-prommer   \"/bin/sh -c /entrypoi\"   0.0.0.0:32901-&gt;9090/tcp   prometheusdemo_prometheus_1f3cfc2f63f00        tomverelst/prommer                 \"/bin/prommer -target\"                             prometheusdemo_prommer_1574f14998424        ordina-jworks/voting-app           \"/main\"                  0.0.0.0:32899-&gt;8080/tcp   prometheusdemo_voting-app_166f2a00fcbcb        ordina-jworks/alert-console        \"/main\"                  0.0.0.0:32898-&gt;8080/tcp   prometheusdemo_alert-console_14fd707d4e80c        ordina-jworks/voting-generator     \"/main -vote=cat -max\"   8080/tcp                  prometheusdemo_vote-cats_15b876a131ad0        ordina-jworks/voting-generator     \"/main -vote=dog -max\"   8080/tcp                  prometheusdemo_vote-dogs_1As you can see,a lot of containers are started!You can view the public ports of the containers in this list,which you need to access the applications.The project consists of the following components:  Prometheus which scrapes the metrics and throws alerts  Grafana to visualize metrics and show fancy graphs  Alert Manager to collect all alerts and route them with a rule based system  cAdvisor which exposes container and host metrics  Prommer, a custom Prometheus target discovery tool  An alert console which displays the alerts in the console  A voting application which registers and counts votes  A voting generator which generates votesThe voting application exposes a custom metric called voting_amount_total.This metric holds the total amount of votes and is labeled by the type of vote,e.g. voting_amount_total{name=dog}.An alerting rule is configured in Prometheus that checks for the amount of votes.Once it passes a certain threshold,the alert is fired.This alert is sent to the Alert Manager,which in turn routes it to the custom alert console through a webhook.Inactive alertThe alert is firedThe alert console logs the JSON body of the POST request from the Alert Manager.We can check the output of these logs using Docker Compose:$ docker-compose logs -f --tail=\"all\" alert-consoleAttaching to prometheusdemo_alert-console_1alert-console_1  | {\"receiver\":\"alert_console\",\"status\":\"firing\",\"alerts\":[{\"status\":\"firing\",\"labels\":{\"alertname\":\"TooManyCatVotes\",\"instance\":\"172.19.0.5:8080\",\"job\":\"voting-app\",\"name\":\"cat\",\"severity\":\"critical\"},\"annotations\":{\"summary\":\"Too many votes for cats!\"},\"startsAt\":\"2016-09-22T17:09:22.807Z\",\"endsAt\":\"0001-01-01T00:00:00Z\",\"generatorURL\":\"http://215c20eb849b:9090/graph#%5B%7B%22expr%22%3A%22votes_amount_total%7Bname%3D%5C%22cat%5C%22%7D%20%3E%20100%22%2C%22tab%22%3A0%7D%5D\"}],\"groupLabels\":{\"alertname\":\"TooManyCatVotes\"},\"commonLabels\":{\"alertname\":\"TooManyCatVotes\",\"instance\":\"172.19.0.5:8080\",\"job\":\"voting-app\",\"name\":\"cat\",\"severity\":\"critical\"},\"commonAnnotations\":{\"summary\":\"Too many votes for cats!\"},\"externalURL\":\"http://c620b49edf4c:9093\",\"version\":\"3\",\"groupKey\":1012006562800295578}GrafanaThe default credentials for Grafana are admin:admin.After logging in,you must first configure a Prometheus data source.Prometheus is available at http://prometheus:9090 (from within the container).                  Configuring the data source                        Visualizing metrics      cAdvisorcAdvisor also has a simple dashboard which displays most important host and container metrics.Since Prometheus scrapes cAdvisor,these metrics are also available from Grafana.                  Network Throughput                        CPU Usage per Core      Final WordsJust a few months ago,the Prometheus team joined the Cloud Native Computing Foundation.  Today, we are excited to announce that the CNCF’s Technical Oversight Committee voted unanimously to accept Prometheus as a second hosted project after Kubernetes!You can find more information about these plans in the official press release by the CNCF.  By joining the CNCF, we hope to establish a clear and sustainable project governance model, as well as benefit from the resources, infrastructure, and advice that the independent foundation provides to its members.Cloud Native Computing Foundation (CNCF) is a nonprofit, open standardization organisation which commits itself to advance the development of cloud native technologies,formed under the Linux Foundation.It is a shared effort by the industry to create innovation for container packaged, microservices based, dynamically scheduled applications and operations.Prometheus has proven itself to be worthy to be an industry standard in alerting and monitoring.It offers a wide-range of features,from instrumenting to alerting,and is supported by many other tools.If you are looking for a monitoring tool,definitely give it a shot!"
      },
    
      "testing-2016-09-16-automation-testing-with-postman-html": {
        "title": "API Testing with Postman and Newman",
        "url": "/testing/2016/09/16/Automation-testing-with-postman.html",
        "image": "/img/postman.png",
        "date": "16 Sep 2016",
        "category": "post, blog post, blog",
        "content": "PrerequisitesFor the purpose of this tutorial it is mandatory to have Postman installed which is available has native apps for Windows, OS X and Linux.It is also mandatory to create an account at Algorithmia.Creating and selecting an environmentPostman’s environment functionality makes it very easy to switch between different environments.A set of variables can be configured per environment and when switching from one environment to another one these will be replaced accordingly.For example let’s create an environment called “production”.Click the “No environment” dropdown in the header and select “Manage environments”.Select the “Add” button on the popup that is presented to you.Add url https://api.algorithmia.com/v1/algo/ and key simNz9pf7hfAQNifdA224K1GFhs1.Don’t forget to replace the secret by your own key.Finally select the “Production” environment in the environment dropdown and let’s create our first request.Creating a POST requestEnter {{url}}/WayneS/Calculator/0.1.0 in the request field and change the method from GET to POST.We need to add some additional headers as well so add Content-Type application/json, Authorization Simple {{key}}.As you can see,we are using the environment variables {{url}} and {{key}} so when switching environments,those variables will get replaced.The {{...}} format can only be used in the request URL/URL params/Header values/form-data/url-encoded values/Raw body content/Helper fields.Postman also has a few dynamic variables which you can use. For example, {{$guid}} is generating a random v4 style guid,{{$timestamp}} is the current timestamp,{{$randomInt}} a random integer between 0 and 1000.More of those will be added in future releases.But for now,let us just simply enter \"x=log(2)\" as the raw content of our request.Finally let’s hit the “Send” button and if everything goes as expected,we should receive the following response.Next we are going to write our test, but first let us save our request into a collection. By clicking on the create collection button on the collections tab, the following popup will be displayed.  Simply enter “Calculator” as the name of the collection and hit the create button.Now hit the “Save” button next to the request field. Enter “Log” as the name of the request and select “Calculator” from the dropdown menu.Writing a testA Postman test is essentially JavaScript code which sets values for the special ‘tests’ object. To know which other objects and libraries are available while writing your test cases, make sure you check the following link. Let’s copy following code snippet in the Tests sandbox.tests[\"Status code is 200\"] = responseCode.code === 200;var jsonData = JSON.parse(responseBody);tests[\"Verify result\"] = jsonData.result.x === \"0.69314718056\";The test will run each time you hit the “Send” button. Let’s say we need a custom function to set some variables,this can easily be achieved in the pre-request sandbox as shown below:Here we are using the ‘postman’ object and are calling the setEnvironmentVariable function on it,this allows us to assign the result of our function to a variable on the environment scope for later use.Collection RunnerLet’s assume we want to run several tests at once.Postman has a Collection Runner utility that allows us to just do that,even thousands of tests if we want.To access the runner click on “Runner” in the top header then select “Calculator” as the collection and “Production” as the environment.We want the runner to do that 2 times so enter 2 in the iteration inputfield like shown in the screenshot below.Scroll down and hit the blue “Start Test” button. Following test report will be presented to you.Writing a request and tests for each different permutation of data could get tiresome and tedious.On the test runner screen we are given the option to choose a data file.This data file can be either a CSV or a JSON file,but will allow us to set up data in bulk to be run through the test runner.Create a new csv file and copy following snippet into it.input,expected_result2,\"0.69314718056\"224,\"5.41164605186\"3000,\"8.00636756765\"388949,\"12.8712035086\"We need to rewrite the body of our request so it will use the variable of our csv as follows.We also need to rewrite our test.Like you can see we use the ‘data’ object to call our expected_result variable.Back to the runner window.Select the “Calculator” collection and the “Production” environment.Click the “Choose Files” button and select the csv file you just created,click the “Preview” button to check for any inconsistenties.As there are 4 entries in our csv we want to use to feed our test enter 4 in the iteration inputfield.Hit the “Start Test” button and you will now see 12 green tests.Pretty neat, isn’t it?NewmanIntegrating Postman tests with build systems can easily be accomplished with Newman. Newman is the command line tool companion for Postman. It can be installed through the Node.js package manager, npm. You’ll find more information on how the install Newman here.After Newman is installed we can export our previously created collection and environment.Select the ‘Calculator’ collection and hit export and save as my_collection.json.To export the ‘Production’ environment select ‘Manage Environment’ and on the next popup hit export and save as ‘prod_environment.json’.Now run you test with Newman using following command where my_collection.json is the exported collection,my_data.csv the csv, prod_environment.json the environment and -n the number of lines from our csv.newman run my_collection.json -n 4 -d my_data.csv -e prod_environment.jsonSummaryIn this tutorial we saw how to create a request and a test.We saw how to create a collection and how to run it with the collection runnner and Newman.I hope you enjoyed this tutorial and if you have any question feel free to add these as a comment or to email me at gregory.rinaldi@ordina.be.Useful links  Importing Swagger files  Postman Slack invite  Importing cURL commands  Creating cURL commands  Making SOAP requests  Running Newman in Docker  Authentication helpers  Publish Documentation for your Collections  Conditional Workflows in Postman (work in progress)  Newman  Integrating Newman with Jenkins "
      },
    
      "microservices-2016-09-12-microservices-dashboard-1-0-1-html": {
        "title": "Microservices Dashboard",
        "url": "/microservices/2016/09/12/Microservices-Dashboard-1.0.1.html",
        "image": "/img/microservices-dashboard.png",
        "date": "12 Sep 2016",
        "category": "post, blog post, blog",
        "content": "So you’ve jumped on the hype train, built a bunch of microservices, and got your first releases under your belt. Now what?Our experiences taught us this is the easy part.With the newly obtained microservices freedom, teams easily plunge into a world of cowboys and unicorns.The big ball of mud is just around the corner.Panic, mayhem and chaos loom over the organisation, waiting for everything to spin out of control.Especially for any enterprise not residing in Silicon Valley, maintaining some sort of governance and compliancy is essential.What does a microservice architecture mean not just for the developers, but also for analysts and managers?What can we as developers do to offer them peace of mind?Managers like to have a grip on thingsThey want to get a sense of compliancy and maturity of the components part of the ecosystem.In theory a microservices architecture gives developers complete freedom to use whatever tools and frameworks they want inside their microservice.In practice, managers often want to slightly restrict that freedom to avoid complete chaos.It’s not uncommon for managers and architects to impose a set of choices developers can choose from, and goals the teams have to achieve.In order to facilitate recruitment and knowledge transfer, developers could be forced to choose between for instance Java or Javascript.Similarly, architects might enforce every microservice to have a quality gate in place and to have a technical debt less than five days.Aside from the technical aspects inside a microservice, compliancy is even more crucial at the contract level.They should be defined according to an architectural vision and comply to standards across the organisation.Having the ability to track these compliancy regulations and quality assurances is a key enabler for management to push for technical excellence.Too often managers are left clueless on how much effort is required to mature the architecture and which teams they have to chase.Having a dashboard at their disposal indicating where a lack of compliancy and maturity needs their attention can help to ensure budget and priorities are in line with the architectural goals.Aside from compliancy and maturity, managers want some level of change management in place.Oftentimes this is achieved through ticketing systems and cumbersome processes.A microservice architecture goes hand in hand with devops, including full automation and decoupling.In that respect, teams ought to be able to define their own release schedule as there is no need for a waterfall manual testing effort of months on end, and the impact on the ecosystem is contained and managed due to the decoupled nature of microservices.Change management in a devops organisation is much more a read-model instead of a process-heavy model.Managers want to know what is currently out there and what will be out there in the future.This doesn’t require a manual ticketing system, simply a smart dashboard with a timeline.Analysts need to know what functionality is out thereIn order to reuse functionality and avoid duplication, functional analysts have a strong need for an overview of the current functional landscape.Knowing which resources are exposed by what microservices, and which events and messages are being sent back and forth between microservices and queues, can go a long way in helping analysts understand the state of the architecture.Furthermore, impact analysis can significantly improve when an overview of components and how they are linked together is available to the analysts.Not only does it encourage analysts to identify and inform consumers of a changing service, it can help to avoid introducing breaking changes due to negligence or ignorance.During troubleshooting, testers and analysts should be able to find out what services and backends are involved in a certain functional flow.Just like managers, functional analysts are interested in upcoming features and releases.On top of that, analysts can benefit from being able to define the future state of the ecosystem.Especially when multiple teams are working on similar functionality, it can be notoriously difficult to avoid duplication and breaches of bounded contexts.Using a dashboard to define what is coming up, can help to give them an unambiguous view of the current and future landscape.Developers can benefit from a broader view as wellIn a devops organisation, developers have the responsibility to not only build but also run their services.Knowing which versions are deployed where, can assist developers in verifying whether their deployments are successful, but also to determine the versions of their dependencies.A graphical dashboard can go a long way in providing clarity to developers.But most of all, it can act as a hub for other tools and documentation available.Integrations can be made with for instance API documentation, performance tooling, service registries, in-depth instance-specific dashboards and perhaps even reactive insights.The introduction of the Microservices DashboardVisualising the state of the architecture and dependencies in the system can be a huge benefit to all stakeholders in the IT organisation.The Microservices Dashboard is a brand new open source project, which officially launched its first major release at Spring One Platform.Building on top of Spring Boot and Spring Cloud, it visualises your microservice architecture and integrates with tools every microservice architecture benefits from.This ranges from consumer-driven-contract testing over service discovery to hypermedia traversal and more.Microservices Dashboard is a simple application to visualize links between microservices and the encompassing ecosystem.This AngularJS application consumes endpoints exposed by its server component.It displays four columns: UI, Resources, Microservices and Backends.Each of these columns show nodes and links between them.The information for these links come from Spring Boot Actuator health endpoints, Pact consumer-driven-contract tests and hypermedia indexes, which are aggregated in the microservices-dashboard-server project.The architectureThe dashboard currently consists out of an AngularJS 1.x application which communicates over HTTP to a Spring Boot application.The frontend uses D3.js to visualise the nodes in the four columns.We are currently in the process of completely rebuilding the frontend stack.Next version will be running on Angular 2, TypeScript and EcmaScript 6.Most of D3.js will be taken care of by Angular 2 itself.Thanks to this refactor we’ll see the introduction of RxJS, making the frontend much more reactive in nature.This aligns our frontend and backend components goals, since the backend is already running RxJava.Our efforts currently focus on replicating all functionalities currently available in the dashboard, albeit with much more attention to quality and testing.Subsequently new features and enhancements will be built on top of a much more mature and extendible frontend application.Our server component is powered by Spring Boot’s auto configuration.It’s a library which, once on the classpath of a regular Spring Boot application, will automatically transform the Spring Boot application into a JSON graph-serving engine.It does so by using aforementioned RxJava.The idea of the server application is that it will fetch information from the microservices ecosystem, with for instance Spring Cloud’s integration of service registries, and collect details of components and their relation within said ecosystem.Needless to say collecting this information requires a lot of outbound calls, and can pose a serious performance burden in case the landscape gets bigger.Making intelligent use of the system’s resources is absolutely necessary, and RxJava does just that.In the future we might migrate to Spring’s Reactor which has a more formal integration of the ReactiveX specification and better integration with Spring itself.Once the frontend’s revamp is completed, the last step towards an end-to-end reactive flow is the HTTP connection between both components.Currently the server still converts the Observable to blocking, undoing a lot of the performance gains we could achieve.Yet even while eventually blocking, we’ve benchmarked a thirty percent performance gain in switching from CompletableFutures to Observables thanks to the more sustained async handling.Aside from its reactive nature, the server component of the dashboard is also built in a very pluggable way.Information is retrieved from the ecosystem through so-called aggregators.Currently four aggregators are provided: the health-indicators aggregator, the index aggregator, the mappings aggregator and the Pact aggregator.We’re looking into supporting Spring Cloud’s recent addition, Spring Cloud Contract, as a source for aggregation.New aggregators can be easily added, and all existing aggregators can be overridden, extended, turned on and off.In the next section we’ll go through these aggregators and their purpose.Collecting information from the ecosystemThe dashboard on its own doesn’t really make a lot of sense when it’s not connected to the architecture it’s supposed to visualise.Aggregators pull in information which eventually gets translated into nodes and links on the dashboard.Health-indicators aggregatorSpring Boot exposes production-ready endpoints through its Actuator module.The health endpoint returns information regarding the current health status of the application.The source of this information is a bunch of health indicators, describing various components and dependencies of the application.For instance, an application can have a dependency on a database, for which a health indicator will usually provide health information to the health endpoint, indicating whether the database is up and the connection pool hasn’t been depleted.Hence, health indicators describe an up-to-date relationship between the application they run on and its dependencies.Spring Cloud ensures health indicators are automatically enabled when you are using service discovery, circuit breakers, a config server or other Spring Cloud services. However, health indicators don’t automatically describe a relationship between an application and another application it calls.Luckily Spring Boot has a very easy way of adding custom health indicators.As such, developers can add a health indicator the moment a remote service call is added to the application.  What about real-time?  Using health indicators we are certain the application calls another application programmatically.This provides clarity in terms of the calls in the code and therefore the dependencies that exist across the applications.However, using this method we aren’t sure whether this remote call is actually being executed at runtime.These concerns are currently provided by other tools such as Twitter’s Zipkin.In the future we will integrate the dashboard with real-time traffic information from Zipkin or similar tooling.Index aggregatorREST over HTTP is arguably the most popular communicational style in microservices architectures.Therefore, gathering information on where and how REST is used can be quite useful.Index and mappings aggregators perform this specific task, albeit each in a different way.The index aggregator relies on a subconstraint of REST called HATEOAS.It stands for Hypermedia As The Engine Of Application State, and describes the idea of adding links in the payload of responses to other resources.This enables discovery of resources, much like we are using the world wide web from its inception.It prevents the need to bookmark URIs to resources, decoupling implementations and enabling independent evolution of the service.Similarly to a regular website, REST APIs using HATEOAS require a homepage or index from which the resource discovery starts.Simply creating an index resource with links to the other resources the service provides, and exposing this index resource at the root of the application takes care of this.Spring HATEOAS provides useful tools to add links to resources.Once every microservice has an index resource, we can use service discovery to discover all the services, and fetch all the index resources to map out the landscape of resources.This is an excellent source for the dashboard, as it shows the relation between microservices and the RESTful resources they expose.Mappings aggregatorOftentimes, RESTful resources are exposed in a more traditional way (using REST level 2) without the added complexity of HATEOAS.While this is not fully REST compliant, it is most common among APIs using JSON over HTTP.Spring Boot offers a handy endpoint in their Actuator module, called the mappings endpoint.It describes all the resources exposed by the application when Spring MVC REST is used.While also describing Spring’s own resources, a simple filter allows us to deduct node and link information from these endpoints to visualise in the dashboard.Pact aggregatorIn a microservices architecture, testing is absolutely crucial.As the primary benefit of microservices is faster time-to-market, changes happen all the time.Not only unit and integration testing is required, but also more advanced contract testing to act as a safety net.Consumer-driven-contract testing allows the consumer (the client) to define what he expects from the producer (the service), and ensure the producer validates that definition every time a change is made to the service.This allows the consumer to rest at ease, knowing the producer will remain backwards compatible or version accordingly, and gives the producer knowledge of who uses exactly which parts of its service.The latter gives the producer the chance to request consumers to update their service in case they are causing too much complexity on the producer’s side due to backwards compatibility.Tests like these document with guaranteed certainty relations between clients and services or services and services.Querying the contracts that define these relations offer a great source of information for the dashboard’s nodes and links between them.When working with the consumer-driven-contract testing framework Pact, a repository called the Pact-broker holds all the available contracts and exposes them through a REST interface.Our Pact aggregator makes use of this interface to pull the information into the dashboard.Spring Cloud recently added the Spring Cloud Contract module to their portfolio, based on Accurest.We’re planning to integrate the Microservices Dashboard with Spring Cloud Contract in the near future as well.ConclusionThe Microservices Dashboard gives managers, analysts and developers peace of mind when working in a microservices architecture.Not only does it map relations between components in a visually attractive manner, it can also be a great tool for compliancy, change management, functional analysis and troubleshooting.The dashboard is currently at version 1.0.1, and can be downloaded through maven central.To quickly get up and running, make sure to check out the reference documentation.Since the project is still fairly new, any feedback is greatly appreciated.You can reach us through Gitter or GitHub."
      },
    
      "conference-2016-08-09-s1p-html": {
        "title": "SpringOne Platform",
        "url": "/conference/2016/08/09/s1p.html",
        "image": "/img/s1p.jpg",
        "date": "09 Aug 2016",
        "category": "post, blog post, blog",
        "content": "  SpringOne Platform is the successor of SpringOne 2GX, with a focus mainly on Spring and Cloud Foundry. Next to these technical topics, SpringOne Platform also offered many sessions on cultural transformation and DevOps. Cultural transformation and DevOps are key to deliver meaningful solutions more quickly. This can be achieved by creating empowered teams, able to make independent decisions. To implement these collaborative teams, leadership buy-in is hugely important. Getting away from legacy thinking will allow enterprises to obtain short feedback cycles and thus continuously improve.From Imperative To Reactive Web Apps - Rossen StoyanchevThe marquee feature of Spring 5, will be first class support for Reactive Web applications. Reactive programming is about non-blocking, event-driven applications with back-pressure. Back-pressure helps to ensure a good collaboration between producers and consumers. The Reactive Manifesto is an interesting read on this topic.To support reactive, Spring 5 will use Project Reactor (led by Stéphane Maldini) through the Spring Web Reactive project. This blogpost, by Rossen Stoyanchev is a nice starting point to learn about Flux, Mono and the Spring Reactive world.Managing Secrets at Scale - Mark PaluchIn a world, where we run large amounts of microservices in orchestrated containers, we can never forget about security, encrypting passwords, storing keys, rotating secrets, etc. Today, applications consume both first and third party APIs and need authentication and authorization to do this in a safe way. Traditional patterns cannot keep the security bar high with dynamic deployment scenarios.As a Secure-By-Design company, this talk immediately caught my attention. In a Spring world, we can use Spring Cloud Vault Config, wrapping Vault. An interesting tutorial on this Spring library is available on spring.io.Slides from the talk are available online.It’s not you, it’s us: Winning over people for yourself and the team - Neha BatraThis was one of the non-technical talks, but aimed to help with the daily management of technical projects. Neha’s session was the most interactive one I attended at SpringOne Platform: about 10 minutes in the session, she wanted us to pair to do a personal SWOT analysis with a stranger in the room and see how we can learn from each other. Everyone participated and I believe this might actually be useful in the context of a project. Something to try out!She ended her session with a tool chest to prevent and mitigate issues as they come up:  SWOT analysis  Personal goals  Inception  Set schedule / cadence  Provide feedback  Provide a “safe haven”  Collect and discuss concerns  Talk in person  Write down useful conversations  Find a way to align first (eg. TDD + pair programming)  Daily retrosThe slidedeck of her talk is available on Slideshare.The five stages of Cloud Native - Casey WestAnother non-technical talk, from the talented and funny Casey West, on how companies are adopting the Cloud Native approach to software development.The talk was very entertaining and resonated with the audience to such an extent, that there was constantly someone laughing. The slides itself don’t say much without explanation so I’ll try to clarify them a bit here.Analogous to the Kübler-Ross model, there are five stages when adopting Cloud Native development:Denial@caseywest immediately grasps the crowd’s attention with these very familiar quotes:  “Containers are just tiny virtual machines”No they’re not. Stop treating them as such. Moving a huge application or database from a virtual machine to a container doesn’t really solve anything.  “We don’t need to automate Continuous Delivery because we already automate our infrastructure with Puppet”The problem is that these measures are not enough and they don’t solve enough of the problem. Managing infrastructure and deploying applications using Puppet scripts already is a great improvement by treating Infrastructure as Code but it still requires too much manual labour.AngerAgain, the goal of these funny quotes is prove a very valid point:  “It works on my machine”The following quote isn’t in the slides but might also sound familiar:  “Let me do a hotfix, I can figure it out”and my favourite:  “DEV is just YOLO-ing sh#t to production”These illustrate the problems, you are likely to get when trying to develop Cloud Native applications without the proper culture in place.This is a clear breakdown in communication and is more a people problem than an IT problem.It just doesn’t work, especially when also considering the compliance or legal aspect. There is a lack of acknowledgement that we need roles and responsibilities.Bargaining  “What if we create microservices that all talk to the same datasource?”Single data model and data ownership are not possible this way.  “We crammed this monolith in a container and called it a microservice”Applications need to adhere to some restrictions to run and scale in the cloud, otherwise you cannot take advantage of the benefits of a platform.Often, there also is the notion of something called bi-modal IT.This is basically dividing your company up into sad mode vs awesome mode.A lot of organizations believe they don’t need to change and prefer to stay in sad mode, they use bi-modal IT as an excuse. Honestly, nobody really wants to work in sad mode.DepressionOnce people actually start creating Cloud Native applications, the depression kicks in:  “We created 200 microservices and forgot to setup Jenkins”A common mistake is not to go for a fully automated CI/CD pipeline from the start.This should be your first action when you start a new project. It is necessary to automate your path to production.  “We have an automated build pipeline but release twice a year”When business is not on board with rapid, iterative delivery, you will never get the desired fast feedback loops.AcceptanceFinally, everyone start realizing the painful, but obvious truth:  All software sucksby which he means that creating software is not easy and will never become easy. But we can try to make it as easy as possible for ourselves.Casey also advises us to respect the CAP theorem, respect Conway’s Law and automate everything.Also, don’t expect to get all of these things right from the start. Taking baby steps and improving gradually over time is certainly possible. An example is to put a monolith inside a container and start breaking it up into more manageable pieces.&amp;TLDR;The (very pretty) slides can be found on Slideshare.OrdinaOrdina was represented at SpringOne Platform with 2 speakers and 3 talks:Writing your own Spring Boot Starter - Dieter HubauDieter Hubau gave a very nice introduction on how to write your own Spring Boot Starter. A Spring Boot Starter is the de-facto standard tool for starting with a greenfield Spring project. He started by explaining the magic behind Spring Boot Starters (and @AutoConfiguration) and ended with a cool game of Josh Long Pokemon, deployed on Cloud Foundry.His slides are available here.Writing Comprehensive and Guaranteed Up-to-date REST API Documentation - Andreas EversAndreas Evers talked about Spring REST Docs to generate documentation that is always up to date. To achieve this, a test-driven approach can be used: generate snippets from integration tests. Combine these snippets with manually written templates and finally generate HTML. Personally, I have always been a huge fan of “documentation-as-code” and Spring REST Docs is a great tool to achieve this goal.His slides are available here. This blogpost by Kevin Van Houtte provides more insight and examples on Spring REST Docs.Ignite: Microservices Dashboard - Andreas EversOn Monday evening, Andreas pitched the Ordina Microservices Dashboard that was released a couple of hours earlier. The Ordina Microservices Dashboard left a big impression:Definitely worth checking out. Expect an in-depth blogpost here soon!Simplifying the Future - Adrian CockroftThe closing keynote at SpringOne Platform was reserved for one of the most influential people in our industry: Adrian Cockcroft. Always at the edge of technology, Adrian often is credited with making Microservices a mature and useful architectural pattern. His talk focussed on:  Simplifying work  Simplify the organization  Simplify things we buildI really recommend watching his entire presentation on YouTube."
      },
    
      "security-2016-07-25-web-of-trusted-commits-html": {
        "title": "A web of trusted commits",
        "url": "/security/2016/07/25/Web-of-trusted-commits.html",
        "image": "/img/digitally-signing-your-json-documents.png",
        "date": "25 Jul 2016",
        "category": "post, blog post, blog",
        "content": "Who Do You Trust?When you’re building software with people from around the world, it’s important to validate that commits and tags are coming from an identified source. By using a distributed revision control system like Git, anyone can have an offline copy of your project’s code repository. In theory having a central repository is not necessary, but it can be used to provide an “official” source from which other developers can clone from and work on. These other floating repositories may contain malicious code because, unfortunately, it is remarkably easy to fake your identity when committing code using Git.The following command allows any individual with bad intentions to commit (malicious) code under your name, meaning that you will get the blame for the backdoor or exploit “you” committed:  # Individual commit.  $ git commit -a -m \"a message\" --author \"Sherlock H. &lt;sherlock.h@bakerstreet.org&gt;\"  # Global settings.  $ git config --global user.name 'Sherlock H.'  $ git config --global user.email sherlock.h@bakerstreet.org  Ensuring TrustThis blog post tells the story of Sherlock H. Sherlock is a witty developer who holds any security-related topic very close to his heart. After a fair amount of pondering about how he could solve the problem of black-hearted developers impersonating his personality, he decided to add a Digital Signature to his commits. By adding a signature Sherlock can finally sleep soundly at night because the signature indicates that he really issued the commit and that it has not been tampered with since he sent it. Moreover it can be used to trace the origin of malicious code that has made its way into a repository. The signature also assures non-repudiation, meaning that it becomes difficult for the signer to deny having signed something because the Digital Signature is unique to both the commit and the signer, and binds them together. Sherlock can now wholeheartedly vouch for the commit.Consider the following scenario:  Sherlock wants to send an urgent message to his fellow developer John W. telling that their application has been compromised by Jim M, a criminal mastermind who only has unkind intentions. John wants the guarantee that the message he received is sent by Sherlock and has not been tampered with by Jim.In order to securely exchange messages, both Sherlock and John will make use of their Key Pairs. A Key Pair consists of a Public and Private Key which are two unique mathematically related cryptographic keys. As its name suggests, the Public Key is made available to everyone by handing out copies or sharing them through a publicly accessible repository. The Private Key however must be kept confidential to its respective owner.Sherlock and John can do the following with the use of their Key Pair:  Signing          The message is still readable to everyone.      Guarantee of the sender’s identity (aka Sherlock).      Guarantee that the message has not been tampered with since it has been signed by the sender (aka Sherlock).        Encryption          The message is only readable by the designated recipient (aka John).      No guarantee of the sender’s identity (aka Sherlock).      Encryption can be done symmetrically by using a Shared Secret Key, a single key is then used for both encryption and decryption. Asymmetrical encryption (aka Public Key encryption) with a Public/Private Keypair uses one key for encryption and another for decryption. Note that the advantages and challenges of using either encryption type is beyond the scope of this blog post.      Enforcing TrustSherlock will combine a digital signature with encryption to convince John that his message is trustworthy.      Sherlock wants to send the following message to John: Data! Data! Data! I can’t make bricks without clay.. He calculates the Hash of this message by applying a publicly known hashing algorithm to the message. The calculated hash by using the SHA-256 hashing algorithm is d6ba26816599a75310c4c263126d4b44979c7026f90e1db8e9b317d6658f3811. The hash value is unique to the hashed data.        Sherlock encrypts the Hash with his Private Key. This encrypted Hash together with a certificate containing additional information about the sender forms the Digital Signature. The reason why the Hash is encrypted and not the entire message, is that a hash function can convert an arbitrary input into a fixed length value which is usually much shorter than the original message. This saves time since hashing is much faster than signing.        Sherlock sends the original message and its Digital Signature to John.        John receives the message and Digital Signature.        Whatever is encrypted with a Public Key can only be decrypted by using its corresponding Private Key and vice versa. Therefore John uses Sherlock’s Public Key to decrypt the Signature.        John also re-calculates the Hash of the original message by applying the same hashing algorithm as Sherlock.        John compares the Hash he calculated himself and the decrypted Hash received with Sherlock’s message.If they’re identical he knows the message has not been tampered with during transit.Should the message been compromised by Jim, then John would have calculated a different Hash than the encrypted Hash that Sherlock has sent along with his message.  Creating An IdentityIn order to sign his commits, Sherlock decided to use Gnu Privacy Guard (GPG) as his weapon of choice. GPG is a complete and free implementation of the OpenPGP standard. It allows to encrypt and sign data and communication, features a versatile key management system as well as access modules for all kinds of public key directories.      Download and install GPG from the official website        Open a command prompt        # Generate a new Key Pair.      $ gpg --gen-key            Sherlock accepted the default RSA and RSA key. RSA is a widely-used asymmetric encryption algorithm and is named after Ron Rivest, Adi Shamir and Len Adleman who invented it in 1977. Should you be interested in more mathematical details how this algorithm works, I can highly recommend watching “Public Key Cryptography: RSA Encryption Algorithm” on YouTube.        Enter the desired key size. I recommend the maximum key size of 4096 bits because they provide far better long-term security. While the default of 2048 bits is secure now, it won’t be in the future. 1024 bit keys are already considered within the range of being breakable and while technology advances 2048 bit keys will also become breakable. Eventually 4096 bit keys will be broken too, but that will be so far in the future that better encryption algorithms will also likely have been developed by then.        Sherlock accepted the default expiration for his key.        He entered his real name and email address. Sherlock provided the verified email address for his GitHub account. This will make it very easy to link his account with his Public Key.        Provide a secure passphrase. Choose wisely and be sure to remember it because else the key cannot be used and any data encrypted using that key will be lost.        Congratulations, a newly fresh Key Pair should be generated now.        # List all keys.      $ gpg --list-keys        pub   4096R/90C3C3DE 2016-07-24        uid     Sherlock H &lt;sherlock.h@bakerstreet.org&gt;        sub   4096R/586B3A7B 2016-07-24        Like many other developers, Sherlock is very active on GitHub and would like to link his Public Key with his account. He therefore will need to create a textual version of his Public Key. After having executed the command below, the content of the generated ‘pubkey.txt’ needs to be added to his account as described in the GitHub Help pages. More details about distributing and registering your Public Key to a key server can be found in the chapter ‘Distributing keys’ of the GPG Users Guide. For other usages like encryption and decryption, please refer to GPG’s Mini HowTo.    # Export the Public Key to a text file.    $ gpg --armor --output pubkey.txt --export 'Sherlock H'    Signing Your WorkOnce Sherlock generated his Key Pair, he can configure Git to use it for signing commits and tags. Following tools can be used to store a GPG key passphrase in a keychain so he doesn’t have to provide it every time he signs a commit: GPG Suite (Mac) or Gpg4win (Windows).  # Set the signing key by taking your Public Key id as parameter.  $ git config --global user.signingkey 90C3C3DE  # Automatically signs every commit.  $ git config --global commit.gpgsign true  # Manually sign a commit.  $ git commit -S -m \"some commit message\"  # Verify whether your commit has been signed.  $ git log --show-signature    commit 81314da640320c65896a4348842d303a754f37d2    gpg: Signature made Sun Jul 24 15:02:25 2016 CEST using RSA key ID 90C3C3DE    gpg: Good signature from \"Sherlock H &lt;sherlock.h@bakerstreet.org&gt;\"    Author: Sherlock H &lt;sherlock.h@bakerstreet.org&gt;    Date:   Sun Jul 24 15:01:52 2016 +0200  # Verify all signatures during merge. If the signatures can not be verified then merge will be aborted.  $ git merge --verify-signatures other_branch  Earlier this year GitHub announced that they now will show when commits and tags are signed and verified using any of the contributor’s GPG keys upload to GitHub. Keep your eyes open for commits and tags labeled with those green verified badges.Secure-By-DesignOrdina’s Secure-By-Design programme encourages to consider and take account of possible security risks as early as possible in a business process.So follow Sherlock’s example by embedding and safeguarding security in your daily work as a developer and Sign Your Work!Resources  GitHub’s Help on GPG  The GNU Privacy Handbook  GPG’s Mini HowTo  “A Git Horror Story” by Mike Gerwitz  “Public Key Cryptography: RSA Encryption Algorithm”"
      },
    
      "conference-2016-07-10-springio16-ddd-rest-html": {
        "title": "Spring I/O 16: Bridging the worlds of DDD &amp; REST",
        "url": "/conference/2016/07/10/SpringIO16-DDD-Rest.html",
        "image": "/img/springio.jpg",
        "date": "10 Jul 2016",
        "category": "post, blog post, blog",
        "content": "SpringIO 2016 in Barcelona was loaded with tons of interesting talks and workshops about Spring Cloud, Spring Boot, Spring Data, Microservices, REST &amp; HATEOAS, Reactive programming, and many many more.In this blogpost I will highlight Oliver Gierke’s 2 hour presentation about bridging the world of Domain Driven Design (DDD) and the world of Representational State Transfer (REST).Oliver Gierke (@olivergierke) is the lead of the Spring Data project at Pivotal and member of the JPA 2.1 expert group. He has been into developing enterprise applications and open source projects for over 10 years. His working focus is centered around software architecture, DDD, REST, and persistence technologies.Domain Driven DesignDDD is an approach to developing software that meets core business objectives by providing on the one hand tactical modeling tools which include well founded patterns and concepts such as entities, repositories and factories. On the other hand DDD also facilitates strategic principles and methodologies for analyzing and modeling domains such as Bounded Contexts and Context Maps.For an in depth understanding of DDD I highly recommend reading “Domain Driven Design - Tackling Complexity in the Heart of Software” by Eric Evans (@ericevans0). There’s also a short, quick-readable summary and introduction to the fundamentals of DDD made available by InfoQ.Oliver’s talk at SpringIO 2016 highlighted a few basic DDD concepts like Entities, Value Objects, Repositories, Aggregates and Bounded Contexts.Value Objects  Avoid Stringly typed codeValue Objects are vital building blocks of DDD. They are small immutable objects that encapsulate value, validation and behaviour. You can use them to group related values together and provide functionality related to what they represent, making implicit concepts explicit.Some common use cases for VOs are: EmailAddress, Money, ZIPCode, Status, … avoid writing these as just plain Strings!Writing VOs can be a cumbersome task but there are some source code generator frameworks out there like Project Lombok and Google’s AutoValue which can handle all the boilerplate code.Entities &amp; RepositoriesIn contrast to Value Objects which are identified by the attributes they carry, Entities are distinguished by their identity. Entity objects have a life cycle because their identity defines their responsibilities and associations. It is this unique identity and their mutability that sets Entities apart from Value Objects. This means that two Value Objects with the same properties should be considered the same whereas two Entities differ even if their properties match.  Aggregates form nice representation boundaries and become the key things to refer to.An Aggregate is a cluster of closely related entities that can be treated as a single unit. The common parent of that cluster is called an Aggregate Root. An example can be an Order and its Line Items, these will be separate objects but it is useful to treat the Order (the Aggregate Root) together with its Line Items as a single Aggregate.When trying to discover Aggregates, we should understand the model’s invariants. An invariant is a business rule that must always be consistent and usually refers to transactional consistency. When a transaction commits then everything inside the Aggregate should be consistent and any subsequent access by any client should return the updated value. In most cases it is a best practice to modify only one Aggregate in a single transaction. For updating multiple aggregates eventual consistency can be used. There will be an inconsistency window during which an access may return either the old or the new value but eventually all accesses will return the last updated value. The duration of the inconsistency window can be calculated based on factors like network delays, number of copies of the object, and the system load.A Repository is an abstraction over a persistence store for Aggregates. It acts like a collection by exposing methods to add and remove objects which encapsulate the actual interaction with the underlying data store. It also has elaborate query capabilities which return fully instantiated Aggregates whose attributes values meet the criteria.Bounded ContextDDD aims to create software models based on the underlying domain. A Bounded Context is the boundary that surrounds a part of a particular domain. This boundary isolates the model and language from other models and therefore helps reducing ambiguity and clarifying the meaning. When the boundaries are chosen well, greater decoupling between systems can be achieved which allows to easily change or replace the internals of a BC. Avoid having transactions across multiple BCs.The language that is structured around the domain model is called the Ubiquitous Language. It is important that this language is used by all team members (developers, analysts, business stakeholders, …) to connect all the activities of the team with the software. The vocabulary on its own does not have any relevance, it only has meaning inside a certain context. For example, an Item has a different meaning in the Orders BC than in the Products BC.Domain EventsA Domain Event is an extremely powerful tool in DDD. It is a type of message that describes something that has happened in the past and that is of interest to the business. (e.g. OrderShipped, CustomerBecamePreferred, …). It is important to model Event names and its properties according to the Ubiquitous Language of the BC where they originated. When Events need to be delivered to interested parties in either a local BC or broadcasted across BCs eventually consistency is generally used.Maturity LevelThe maturity level of the use of Domain Events can be categorized into 4 levels:  Level 0: no events at all          procedural code with just getters and setters      data just goes in and out        Level 1: explicit operations  Level 2: some operations as events          domain events are used as state transition      important domain events are exposed to interested parties via feeds        Level 3: event sourcing - all changes to application state are stored as a sequence of events          only event logs and snapshots are kept (Event Store)      separation of read and write operations (CQRS)      REST  REST ≠ CRUD via HTTP. Representation design matters.ResourcesJust like an Aggregate, a well designed Resource should be identifiable, referable and should have a clear scope of consistency.Exposing the core domain model directly via RESTful HTTP can lead to brittle REST interfaces because each change in the domain model will be reflected in the interface. Decoupling the core domain from the REST interface has the advantage that we can make changes to the domain and then decide in each individual case whether a change is needed in the REST interface and how to map it.Also avoid using HTTP PATCH or PUT for (complex) state transitions of your business domain because you are missing out on a lot of information regarding the real business domain event that triggered this update. For example, changing a customer’s mailing address is a POST to a new “ChangeOfAddress” resource, not a PATCH or PUT of a “Customer” resource with a different mailing address field value.This goes hand in hand with DDD’s concept of Event Sourcing because those state transitions are domain relevant events, not just some changes to the state of some object.HATEOASA RESTful HTTP client can navigate from resource to resource in two different ways. Firstly by being redirected as a result of sending data for processing to the server, and secondly by following links contained in the response of the server. The latter technique is called Hypermedia as the Engine of Application State or HATEOAS.The goal of Hypermedia is to serve not only data but also navigation information at the same time. This has a great impact on the client architecture because now we’re trading domain knowledge with protocol complexity. The client becomes dumber because it no longer needs to know business rules in a sense that its decisions are reduced to checking whether a link is present or not, e.g. whenever there’s a cancel link in the HTTP response, then display the Cancel button. This will make the client’s behavior more dynamic.On the other hand, the client becomes smarter because it needs to handle a smarter and more comprehensive protocol.Maturity levelIn analogy to the maturity level of Aggregates described earlier, Leonard Richardson’s model can be used to determine the maturity or our REST services.  Level 0: Swamp of POX          the HTTP protocol is used to make RPC calls without indication of the application state        Level 1: Resources          exposure of multiple URIs and each one is an entry point to a specific resource, e.g. http://example.org/orders, http://example.org/order/1, http://example.org/order/2      use of only one single method like POST.        Level 2: HTTP verbs          use of HTTP protocol properties (POST, GET, DELETE, …)      use of HTTP response codes, e.g. HTTP 200 (OK)        Level 3: Hypermedia controls          refer to description earlier in this blog post.      Translating domain concepts into web appropriate ones            DDD      REST                  Aggregate Root / Repository      Collection / Item Resource              Relations      Links              IDs      URIs              @Version      ETags              Last Modified Property      Last Modified Header      Sample implementationOliver also prepared a small sample implementation using Spring Boot, Spring Data and Lombok. The project is called Spring RESTBucks and is definitely worth checking out!Resources  “DDD &amp; REST” (slide deck used at SpringIO 2016) by Oliver Gierke  “Spring RESTBucks” (sample project used at SpringIO 2016) by Oliver Gierke  “Benefits of hypermedia” by Oliver Gierke  “Domain Driven Design - Tackling Complexity in the Heart of Software” by Eric Evans  “Implementing Domain Driven Design” by Vaughn Vernon  “Domain Driven Design Quickly” by InfoQ"
      },
    
      "conference-2016-06-30-springio16-spring-rest-docs-html": {
        "title": "Spring I/O 16: Test-driven documentation with Spring REST Docs",
        "url": "/conference/2016/06/30/SpringIO16-Spring-Rest-Docs.html",
        "image": "/img/springio.jpg",
        "date": "30 Jun 2016",
        "category": "post, blog post, blog",
        "content": "Spring IO 2016The main focus this year was definitely about cloud, reactive and microservices.But it is important not to forget other topics, like documentation! Keep calm, you don’t have to do it manually! Spring made it easy for us with Spring REST Docs! This year at Spring IO, Andy Wilkinson himself talked about why, how and when Spring REST Docs are being used. Last but not least, he talked about the new features that came out in version 1.1.Since I implemented Spring REST Docs in a project, I’ll use examples from my experiences.Andy WilkinsonAndy is a Spring Boot, REST docs committer and Spring IO platform lead at Pivotal. You can find him on Twitter using the handle @ankinson.Writing documentation is critical in the world of development. It is used to make an accurate and straight declaration and intent of what the service has to offer. Frontend developers will be able to know which endpoints they have to call and receive the right data. Now, we all know it's tedious for developers to write documentation...It's your lucky day! Spring REST Docs will make your life easier.While you are writing tests, Spring will generate a fully HTML api guide for you and your team. This blog post will take you through the best practices, how to and new features in 1.1.Why Test driven approach  It’s an accurate definition of your application (no side effects)  It describes the specific HTTP request and response  It’s straight forward without repetition  It’s easier to write (no annotations like Swagger)Markup languagesAsciidoctorAsciidoctor is a markup language that processes plain text and produces HTML, completely styled to suit your needs.If you are interested in writing in Asciidoctor be sure to check out the manual.Markdown (New in 1.1)With the newest version of REST Docs, the developer now has more options in terms of markup languages.The Markdown support is not as feature-rich as Asciidoctor, but Markdown can work very well when combined with existing documentation toolchains such as Slate.Here is a good sample working with slate.Andy’s pickAsciidoctor!Since Asciidoctor boasts more features than Markdown, it gives Asciidoctor the edge.Test ToolsWhen we want to use Spring REST Docs, we’ll have to use one of the test tools. Here are the different tools of choice. To use these tools we’ll have to initialise which document, Mockmvc and ObjectWriter we’ll be using.MockMvcA lightweight server-less documentation generation by the Spring Framework that has been the default use in Spring REST Docs.private MockMvc mockMvc;@Autowiredprivate WebApplicationContext context; @Before    public void setup() throws Exception {        this.document = document(\"{method-name}\");        mockMvc = MockMvcBuilders.webAppContextSetup(wac)                .apply(documentationConfiguration(this.restDocumentation).uris().withScheme(\"https\")).alwaysDo(this.document)                .addFilter(new JwtFilter(),\"/*\")                .build();        objectWriter = objectMapper.writer();        authToken = TestUtil.getAuthToken();        TestUtil.setAuthorities();    }RestAssured (New 1.1)As an alternative, you can use RestAssured to test and document your RESTful services. Available in V1.1, RestAssured will be more expandable than MockMvc.private RequestSpecification spec;@Beforepublic void setUp() {    this.spec = new RequestSpecBuilder().addFilter(            documentationConfiguration(this.restDocumentation))             .build();}Andy’s pickThis time he didn’t favor one but he mentioned that RestAssured gives you more functionality and extends your possibilities with HTTP.Snip, snip, snippets everywhere!Default SnippetSnippets are generated by the documented test method.Once you run the test method, you can add these snippets in your Markdown/Asciidoctor file. Be aware, these type of snippet will fail if you don’t have the correct response/request syntax.this.document.snippets(                links(                        halLinks(), linkWithRel(\"self\").description(\"The employee's resource\"),                        linkWithRel(\"employee\").optional().description(\"The employee's projection\")),                        responseFields(                                fieldWithPath(\"username\").description(\"The employee unique database identifier\"),                                fieldWithPath(\"firstName\").description(\"The employee's first name\"),                                fieldWithPath(\"lastName\").description(\"The employee's last name\"),                                fieldWithPath(\"linkedin\").description(\"The employee's linkedin\"),                                fieldWithPath(\"unit\").description(\"The employee's unit\").type(Unit.class),                                fieldWithPath(\"_links\").description(\"links to other resources\")                        )); mockMvc.perform(                get(\"/employees/1\").accept(MediaType.APPLICATION_JSON));Relaxed Snippet (New in 1.1)In contrast to default snippets, relaxed snippets don’t complain when something was neglected in the document.This is an advantage when you only need to focus on a certain scenario or specific part of the response/request.Reusable Snippet (New in 1.1)With the newly introduced reusable snippet, you can define a snippet at the beginning of your test class and reuse it every time you need it. When added to your test method, you can extend it with extra variables.// First we define a snippet for reuseprotected final LinksSnippet pagingLinks = links(        linkWithRel(\"first\").optional().description(\"The first page of results\"),        linkWithRel(\"last\").optional().description(\"The last page of results\"),        linkWithRel(\"next\").optional().description(\"The next page of results\"),        linkWithRel(\"prev\").optional().description(\"The previous page of results\"));// Then you perform the mockMvc and add the snippet to the document.// As you can see, it is expendable.this.mockMvc.perform(get(\"/\").accept(MediaType.APPLICATION_JSON))    .andExpect(status().isOk())    .andDo(document(\"example\", this.pagingLinks.and(             linkWithRel(\"alpha\").description(\"Link to the alpha resource\"),            linkWithRel(\"bravo\").description(\"Link to the bravo resource\"))));Type of Snippets:A snippet can be one of the following:Hypermedia linksWhen documenting your hypermedia application, you’ll have to define your links and where they go to. If you have dynamic links that can disappear at one time, you can use relaxed snippets or optional so it won’t complain. this.document.snippets(                links(                        halLinks(), linkWithRel(\"self\").description(\"The employee's resource\"),                        linkWithRel(\"employee\").optional().description(\"The employee's projection\")),                responseFields(                        fieldWithPath(\"username\").description(\"The employee unique database identifier\").type(String.class),                        fieldWithPath(\"_links\").description(\"links to other resources\")                ));Request fieldsThis defines the fields you request from the client.Normally Spring REST Docs will complain when you neglect a field but with v1.1 we now have support for Relaxed Snippets.Because I use constraints, I made my own method `withPath, this will add an extra column constraint to the documentation.  private static class ConstrainedFields {         private final ConstraintDescriptions constraintDescriptions;         ConstrainedFields(Class&lt;?&gt; input) {             this.constraintDescriptions = new ConstraintDescriptions(input);         }         private FieldDescriptor withPath(String path) {             return fieldWithPath(path).attributes(key(\"constraints\").value(StringUtils                     .collectionToDelimitedString(this.constraintDescriptions                             .descriptionsForProperty(path), \". \")));         }     } @Test public void postEmployee() throws Exception{         Employee employee = employeeRepository.findByUsernameIgnoreCase(\"Nivek\");         employee.setId(null);         employee.setUsername(\"Keloggs\");         String string = objectWriter.writeValueAsString(employee);          ConstrainedFields fields = new ConstrainedFields(Employee.class);          this.document.snippets(                 requestFields(                         fields.withPath(\"username\").description(\"The employee unique database identifier\"),                         fields.withPath(\"firstName\").description(\"The employee's first name\"),                         fields.withPath(\"lastName\").description(\"The employee's last name\"),                         ));          mockMvc.perform(post(\"/employees\").content(string).contentType(MediaTypes.HAL_JSON).header(\"Authorization\", authToken)).andExpect(status().isCreated()).andReturn().getResponse().getHeader(\"Location\");     }                              Response fieldsThis defines the result after consultation of a resource.  this.document.snippets(              responseFields(                       fieldWithPath(\"username\").description(\"The employee unique database identifier\"),                       fieldWithPath(\"firstName\").description(\"The employee's first name\"),                       fieldWithPath(\"lastName\").description(\"The employee's last name\"),                           ));Request/response headersDefines your request/response headers in your API. This is useful when there are extra headers to set. When the request has to involve an authorization header for security reasons, you can add this header to your document.mockMvc.perform(                get(\"/employees/1\").accept(MediaType.APPLICATION_JSON)                .header(\"Authorization\", authToken)                .andDo(document(\"headers\",                \t\t\t\trequestHeaders(                 \t\t\t\t\t\theaderWithName(\"Authorization\").description(                \t\t\t\t\t\t\t\t\"Basic auth credentials\")),                 \t\t\t\tresponseHeaders(                 \t\t\t\t\t\theaderWithName(\"X-RateLimit-Limit\").description(                \t\t\t\t\t\t\t\t\"The total number of requests permitted per period\"),                \t\t\t\t\t\theaderWithName(\"X-RateLimit-Remaining\").description(                \t\t\t\t\t\t\t\t\"Remaining requests permitted in current period\"),                \t\t\t\t\t\theaderWithName(\"X-RateLimit-Reset\").description(                \t\t\t\t\t\t\t\t\"Time at which the rate limit period will reset\")))));Request parametersThe parameters passed by in the uri as a query string are documented with the requestParameters.this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk())\t.andDo(document(\"users\", requestParameters( \t\t\tparameterWithName(\"page\").description(\"The page to retrieve\"), \t\t\tparameterWithName(\"per_page\").description(\"Entries per page\") \t)));\t\tRequest parts (new in 1.1)The parts of a multipart request can be documenting using requestPartsExampleRestAssured.given(this.spec)\t.filter(document(\"users\", requestParts( \t\t\tpartWithName(\"file\").description(\"The file to upload\")))) \t.multiPart(\"file\", \"example\") \t.when().post(\"/upload\") \t.then().statusCode(is(200));What makes good documentation?Andy’s pickHe told us that the GitHub API is one of the most complete and correct documentation there is. So if you want some guidelines, inspire yourself with this API.Structure and accuracyWhen documenting your application, your accuracy has to be 100% correct and understandable. The structure of your API is the representation of your application, so it better be good.Cross-cutting concernsAndy put forward to document cross-cutting concerns on a general documentation level, avoiding repeating yourself in every single documented API call.Concerns who made it to the top are:  Rate limiting  Authentication and authorisationAnd HTTP verbs/codes (PATCH VS PUT)To be RESTfull, you’ll have to follow the guidelines in having a correct API design. This picture shows you how and when to use the correct verbs and HTTP codes3 main questions if you are working with resources  What do they represent?  What kind of input do they accept?  What output do they produce?Last but not least: do not document uri’s!QuestionsWill constraints be officially added in future releases?The constraints snippets won’t be added in the future.This is because Andy wants to give the developers the choice of what they want to implement.ConclusionSince Spring REST Docs is so effective in bringing documentation to the fun part of development I highly recommend to use this in your future Spring applications. Not only you will be smiling when the API guide is being generated but the Frontend developers will get a more understandable view of the backend.Sources  @ankinson  Spring REST Docs  GitHub API  Verbs &amp; HTTP codes  Asciidoctor manual  Slate example"
      },
    
      "conference-2016-06-20-whats-new-in-docker-112-html": {
        "title": "DockerCon 2016 - What is new in Docker 1.12",
        "url": "/conference/2016/06/20/whats-new-in-docker-112.html",
        "image": "/img/dockercon/dockercon.png",
        "date": "20 Jun 2016",
        "category": "post, blog post, blog",
        "content": "Orchestration Made EasyLast week,I tried out the new orchestration tools that were made available on GitHub.My first impressions were very positive.The setup is easy and it works like a charm.Today,at DockerCon 2016,these new orchestration tools were officially announced during the opening session.There is also an official blog post.Before we start talking about orchestration,let’s take a step back and look at how easy it has become to set up a Swarm cluster.Creating a Swarm manager can be done with one simple command:$ docker swarm initYou can run this command on any Docker 1.12 host.After we created the Swarm manager,we can add additional nodes to the swarm by running the following command on other Docker hosts:$ docker swarm join &lt;IP of manager&gt;:2377That’s it.No messing around with key-value stores or certificates.Docker will automatically configure everything you need out-of-the-box.Under the hood,Docker uses a Raft consensus.There are two types of nodes: manager and worker.The first initial node is a manager.When adding more nodes to the Swarm,these nodes will be worker nodes by default.Manager nodes are responsible for managing the cluster’s desired state.They do health checks and schedule tasks to keep this desired state.Worker nodes are responsible for executing tasks that are scheduled by the managers.A worker node cannot change the desired state.It can only take work and report back on the status.The role of a node is dynamic.We can increment or reduce the amount of managers by promoting or demoting nodes.$ docker node promote &lt;node-id&gt;$ docker node demote &lt;node-id&gt;ServicesDocker 1.12 introduces a new service command.A service is a set of tasks that can be easily replicated.A task represents a workload and is executed by a container.A task does not necessarily have to be a container,but currently that is the only option.In the future,tasks can also be different types of workloads,for example Unikernels.The service command is very similar to the run commandand utilizes a lot of similar flags which we are used to work with.$ docker service create --replicate 3 --name frontend --network mynet --publish 80:80/tcp frontend_image:latestThe above command will create a service named frontend,add it to the mynet network,publish it to port 80,and use the frontend_image for this service.This does not only create the service,but it defines the desired state.The cluster constantly reconciles its state.Upon a node failure,the cluster will automatically self healand converge back to the desired state by scheduling new tasks on other nodes.You can also define a Swarm mode.For example,if you wish to create a service that runs on every node,you can easily do this using the global mode.This will schedule all the tasks of a service on each node.This is great for general services like monitoring.$ docker service create --mode=global --name prometheus prom/prometheusJust like we can put constraints on containers,we can put constraints on services:$ docker daemon --label com.example.storage=\"ssd\"$ docker service ... --constraint com.example.storage=\"ssd\" ...If we want more instances of our service,we can scale our services up and down:$ docker service scale frontend=10 backend=20This will change the desired state of the service(s),and the managers will schedule new tasks (or remove existing tasks) to attain this desired state.We can also apply rolling updates to our services.For example,if we wish to upgrade our service to a newer version without any downtime,we can use the service update command:$ docker service update myservice --image myimage:2.0 --update-parallellism 2 --update-delay 10sThis will update our service by replacing 2 tasks at the time,every 10 seconds.We can also use this command to change environment variables,ports,etc.As you can see,the new service subcommand is very powerful and easy to use.BundlesA Distributed Application Bundle (DAB) file declares a stack of services,including the versioning and how the networking is setup.It is a deployment artifact that can be used in your continuous integration tools,all the way from your laptop to production.Currently,one way to generate a .dab file is by creating the bundle using Docker Compose:$ docker-compose bundleThis command will generate a .dab or .dsb file,which is just a JSON text file.Here’s a partial example:{  \"services\": {    \"db\": {      \"Env\": [        \"constraint:type==backend\",        \"constraint:storage==ssd\"      ],      \"Image\": \"postgres@sha256:f76245b04ddbcebab5bb6c28e76947f49222c99fec4aadb0bb1c24821a9e83ef\",      \"Networks\": [        \"back-tier\"      ]    }  }}This feature is still experimental in Docker 1.12and the specification is still being updated.Docker invites everyone to provide feedback and hopes it will become the de facto standard for deploying applications.Routing Mesh NetworksA problem with load balancers is the fact they are not container-aware,but host-aware.Load balancing containers has been hard up until now,because you have to update the configuration of the load balancers as containers are started or stopped.This is done by overriding the configuration file of the load balancer and restarting it,or by updating the configuration in a distributed key-value store like etcd.Docker now has built in load balancing in the Engine using a container-aware routing mesh.This mesh network can transparantly reroute traffic from any host to a container.For example,publishing a service on port 80 will reserve a Swarm wide ingress port,so that each node will listen to port 80.Each node will then reroute traffic to the container using DNS based service discovery.This is compatible with existing infrastructure.External load balancer no longer need to know where the containers are running.They can just point towards any node and the routing mesh will automatically redirect traffic.Even though this introduces an extra hop,it is still very efficient since it uses IPVS.Security Out Of The BoxDocker now comes with out-of-the-box, zero-configuration security.Docker sets up automatic certificate rotation,TLS mutual authenticationand TLS encryption between nodes.There is no way to turn off security.One of the core principles of Docker is simplicity.Therefor,security must be so simple to use,that you don’t want to turn it off!Container Health Check in DockerfileA new HEALTHCHECK keyword is available for Dockerfiles.This keyword can be used to define the health check of a container.HEALTHCHECK --interval=5m --timeout=3s --retries 3 CMD curl -f http://localhost || exit 1In the above example,health checking is done every 5 minutes.A container becomes unhealthy if the curl command fails 3 times in a row with a 3 second timeout.New Plugin Subcommands (experimental)A new plugin subcommand has been added which allows you to easily manager Docker plugins.$ docker plugin install &lt;plugin-name&gt;$ docker plugin enable &lt;plugin-name&gt;$ docker plugin disable &lt;plugin-name&gt;Plugins also have a manifest file which describes the resources it needs.You can compare it to how a new app on your smart phone asks for access to different resources,like your photos or contacts.Try It Out!As of today,the Docker for Mac/Windows beta,which is already at Docker 1.12,is open for everyone!You can download it at docker.com/getdocker."
      },
    
      "conference-2016-05-13-js-conf-budapest-day-2-html": {
        "title": "JS Conf Budapest Day 2",
        "url": "/conference/2016/05/13/JS-Conf-Budapest-day-2.html",
        "image": "/img/js-conf-budapest-2016.jpg",
        "date": "13 May 2016",
        "category": "post, blog post, blog",
        "content": "From JS Conf Budapest with loveThis year’s edition of JS Conf Budapest was hosted at Akvárium Klub.Located right in the center of the city, below an actual pool, filled with water!  Akvárium Klub is more than a simple bar: it is a culture center with a wide musical repertoire from mainstream to underground.There is always a good concert and a smashing exhibition, performance, or other event happening here, in a friendly scene, situated right in the city center.JS Conf Budapest is hosted by the one and only Jake Archibald from Google.Day 2 started at 9 o’clock.Enough time to drink great coffee and enjoy the breakfast.Day 2: Talks  Suz Hinton: The Formulartic Spectrum  Oliver Joseph Ash: Building an Offline Page for theguardian.com  Nicolás Bevacqua: High Performance in the Critical Rendering Path  Anand Vemuri: Offensive and Defensive Strategies for Client-Side JavaScript  Sam Bellen: Changing live audio with the web-audio-api  Rob Kerr: Science in the Browser: Orchestrating and Visualising Neural Simulations  Stefan Baumgartner: HTTP/2 is coming! Unbundle all the things?!?  Claudia Hernández: Down the Rabbit Hole: JS in Wonderland  Lena Reinhard: Works On My Machine, or the Problem is between Keyboard and ChairDay 2: MorningSuz Hinton: The Formulartic SpectrumSuz is front-developer at Kickstarter. Member of the NodeJS hardware working group. Member of the Ember-A11y Project team.You can find her on Twitter using the handle @noopkat. She blogs on meow.noopkat.com.The physical world is just another binary machine.Data creation, analysis, and corruption combined with JavaScript can make new and unexpected things.Can you programmatically extract joy from the subjectivity it exists in?Can it be translated into intentional forms to hook others in?This session will gently take you along on a personal journey of how you can use code to expose new expressions of the mundane secrets we hold dear.Why are we here  Data &amp; Art  Make a messFeelingsWarning, a lot of feelingsPersonal history1994  Commodore 64 graphics book  Wants to make art on computer  The littlest artist  Accidental programmer (Suz didn’t really want to become a programmer)  Semicolon wars;; It doesn’t matter how you place your semicolon!This story is inspired by the movie Contact by Carl Sagan and makes Suz wonder: what does sound look like?Formulartic spectrum (made up word: art)  Analysing PCM data (Pulse Code Modulation -&gt; raw uncompressed data)  Resulted in only 13-ish lines of codeaudioContext.decodeAudioData(audioData)    .then(function(decoded) {    // just get the left ear, it's fine ;)    let data = decoded.getChannelData(0);    for (let i = 0; i &lt; data.length; i += 1) {        // convert raw sample to within 0-255 range        let hue = Math.ceil((data[i] + 1) * 255 / 2);        // convert HSL to an RGB array        let rgb = hslToRgb(hue, 200, 150);        // create the pixel        imgData.data[i*4] = rgb[0];        imgData.data[i*4+1] = rgb[1];        imgData.data[i*4+2] = rgb[2];        imgData.data[i*4+3] = rgb[3];    }    // put the pixels on a canvas element    canvas.putImageData(imgData, 0, 0);});Suz talked about programming and art.She spent a lot of time on the subway and was wondering if it would be possible to use the sounds of the subway to create art.So she started by taking the sound of the subway doors closing and analysing that part.  Sampling the audio to pixels resulted in 300k pixels  Make it smaller by converting to 16-beat songCheck out the visualisation!  The top section: Stand clear of the closing doors, please.  The mid section: white noise  The bottom section: ding dong!Suz created a visualisation of the sampled audio that resulted in cats sitting on an subway.  Cats can sit on 16 seats in subway car, each seat representing a beat  In total there were 308.728 samples which divided by 16 beats result in 19.295 samples per beat. Suz took the average of the sample values of each ‘beat’  The seats have different colors that represent the drum beat and oscillator note  When a cat is sitting on a chair, we get a guitar strum and noteThe subway example is made using:  SVG images  divs  CSS animationsCheck out the working example!But I’m better at hardwareSo Suz created a subway card with built in speaker!Recap  Creative coding gets you out of your comfort zone and teaches you to use tools you use everyday in another context  Art doesn’t care about your semicolons          Code can be messy      No one cares about semicolons, etc.        Art doesn’t care about perfection          Again, your code doesn’t really matter      Art is about what you learned  Write messy code  Make lots of mistakes  You deserve a break from being judged  Code like no one’s watching  Don’t ‘git rebase -i’          Show the history behind good code      Code evolves from a first idea to a final solution.      At first, code might not be perfect      Don’t rebase to hide this fact      View the slides of Suz’s talk here!Oliver Joseph Ash: Building an Offline Page for theguardian.comOliver is a software engineer working on the team behind theguardian.com.Being passionate about the open web, he aims to work on software that exploits the decentralised nature of the web to solve non-trivial, critical problems.With a strong background in arts as well as engineering, he approaches web development in its entirety: UX, performance, and functional programming are some of the things he enjoys most.You can find him on Twitter using the handle @OliverJAsh.You’re on a train to work and you open up the Guardian app on your phone.A tunnel surrounds you, but the app still works in very much the same way as it usually would, despite your lack of internet connection, you still get the full experience, only the content shown will be stale.If you tried the same for the Guardian website, however, it wouldn’t load at all.Native apps have long had the tools to deal with these situations, in order to deliver rich user experiences whatever the user’s situation may be.With service workers, the web is catching up.This talk will explain how Oliver used service workers to build an offline page for theguardian.com.Oliver talked about the functionality they created with service workers on The Guardian.When offline on The Guardian, you’ll get a crossword puzzle (always the most recent) that you can play.We summarized the key parts of the talk for you.Website vs nativeNative  Content is cached  Experience:          offline: stale content remains      server down: stale content remains      poor connection: stale while revalidate      good connection: stale while revalidate      Website  Experience          offline: nothing      server down: nothing      poor connection: white screen of death      good connection: new content      How it worksService workers  Prototype built in &lt; 1 dayWhat is a service worker?  A script that runs in the background  Useful for features that don’t need user interaction, e.g.:          Listen to push events, useful for pushing notifications      Intercept and handle network requests      Future                  Background sync          Alarms (e.g. reminders)          Geofencing                      A progressive enhancement  Trusted origins only (HTTPS only or localhost)  Chrome, Opera and Firefox stableFor now The Guardian is not yet fully on HTTPS, but they are switching at this time of writing.Some pages have service workers already enabled such as:  theguardian.com/info  theguardian.com/science  theguardian.com/technology  theguardian.com/businessHow did they do it?1. Create and register the service worker&lt;script&gt;if (navigator.serviceWorker) {    navigator.serviceWorker.register('/service-worker.js');}&lt;/script&gt;You can debug service workers in Chrome by selecting Service Workers under the Resources tab.2. Prime the cache  install event: get ready!  Cache the assets needed later  Version the cache. To check if a user has an old version so you can update with newer versions&lt;script&gt;var version = 1;var staticCacheName = 'static' + version;var updateCache = function () {    return caches.open(staticCacheName)        .then(function (cache) {            return cache.addAll([                '/offline-page',                '/assets/css/main.css',                '/assets/js/main.js'            ]);        });};self.addEventListener('install', function (event) {    event.waitUntil(updateCache());});&lt;/script&gt;3. Handle requests with fetch  fetch events          Default: just fetch      Override default      Intercept network requests to:                  Fetch from the network          Read from the cache          Construct your own response                    &lt;script&gt;self.addEventListener('fetch', function (event) {    event.respondWith(fetch(event.request));});&lt;/script&gt;It is possible to use custom responses when using Service Workers. E.g. Use templating to construct a HTML respose from JSON.&lt;script&gt;self.addEventListener('fetch', function (event) {    var responseBody = '&lt;h1&gt;Hello, world!&lt;/h1&gt;';    var responseOptions = {        headers: {            'Content-Type': 'text/html'        }    };    var response = new Response(        responseBody,        responseOptions    );    event.respondWith(response);});&lt;/script&gt;(Im)mutable  Mutable (HTML)          Network first, then cache      Page -&gt; service worker -&gt; server or cache -&gt; Page        Immutable (assets: CSS, JS)          Cache first, then network      Page -&gt; service worker -&gt; cache or server -&gt; Page      4. Updating the crosswordCheck if the cache has been updated and if it’s not up to date, update it and delete old cache.isCacheUpdated().then(function (isUpdated) {    if (!isUpdated) {        updateCache().then(deleteOldCaches);    }});Offline-firstWhy should we be building with offline first?  Instantly respond with a “shell” of the page straight from cache when navigating a website  It improves the experience for users with poor connections  No more white screen of death  Show stale content whilst fetching new contentProblems and caveats  Browser bugs in both Chrome and Firefox  Interleaving of versions in CDN cacheThis can be fixed with a cache manifest.// /offline-page.json{    \"html\": \"&lt;html&gt;&lt;!-- v1 --&gt;&lt;/html&gt;\",    \"assets\": [\"/v1.css\"]}Why? Is this valuable  Fun  Insignificant usage due to HTTPS/browser support          … but plant the seed and see what happens        Iron out browser bugs, pushes the web forward  “If we only use features that work in IE8, we’re condemning ourselves to live in an IE8 world.” — Nolan LawsonConclusion  Service workers allow us to progressively enhance the experience for          Offline users      Users with poor connections        It’s easy to build an offline page  A simple offline page is a good place to startThe slides of Oliver’s talk can be viewed on Speaker Deck.Nicolás Bevacqua: High Performance in the Critical Rendering PathNicolás loves the web. He is a consultant, a conference speaker, the author of JavaScript Application Design, an opinionated blogger, and an open-source evangelist.He participates actively in the online JavaScript community — as well as offline in beautiful Buenos Aires.You can find him on Twitter using the handle @nzgb and on the web under the name ponyfoo.com.This talk covers the past, present and future of web application performance when it comes to delivery optimization.I'll start by glancing over what you're already doing -- minifying your static assets, bundling them together, and using progressive enhancement techniques.Then I'll move on to what you should be doing -- optimizing TCP network delivery, inlining critical CSS, deferring font loading and CSS so that you don't block the rendering path, and of course deferring JavaScript.Afterwards we'll look at the future, and what HTTP 2.0 has in store for us, going full circle and letting us forego hacks of the past like bundling and minification.Getting startedMeasure what is going on and see what is going on!Use the Chrome DevTools Audits.  Per-resource advice  Caching best practicesPageSpeed Insights (Google)developers.google.com/speed/pagespeed/insights/  Insights for mobile  Insights for desktop  Get a rough 1-100 score  Best practices  Practical adviceWebPageTest (webpagetest.org)webpagetest.org  Gives analytics and metrics where you can act on  A lot of statistics  PageSpeed Score  Waterfall View: figure out how to parallelize your download to speed up loading  Makes it easy to spot FOIT  Calculates SpeedIndex: SpeedIndex takes the visual progress of the visible page loading and computes an overall score for how quickly the content painted  Inspect every request  Analyze TCP traffic  Identify bottlenecks  Visualize progressAutomate!But we can automate a lot!  Measure early. Measure often.PageSpeed Insights is available as npm module.npm install psi -gWebpagetest is also available as npm module but is a bit slower.npm install webpagetest-api underscore-cliYSlow is available for different platforms.npm install grunt-yslow --save-devBudgets  Enforce a performance budget  Track impact of every commit  What should I track? More info about this on timkadlec.com/2014/11/performance-budget-metrics          Milestone Timings: Load time, time to interact, “time to first tweet”      SpeedIndex: Average time at which parts of a page are displayed      Quantity based metrics: Request count, page weight, image weight …      Rule based metrics: YSlow grade, PageSpeed score, etc.      Budgeting can also be automated using the grunt-perfbudget plugin.npm install grunt-perfbudget --save-devWhat can we do beyond minification?Minification is usually the first thing developers think of when talking about optimizing your code for speed.But what are the things we can do beyond minification?A lot of best practices on optimizing performance in your app are described in the High Performance Browser Networking book written by Ilya Grigorik.For all the detailed tips and tricks we suggest to view the slides for Nicolás’s talk on ponyfoo.com.Anand Vemuri: Offensive and Defensive Strategies for Client-Side JavaScriptAnand is Senior Application Security Consultant at nVisiumYou can find him on Twitter using the handle @brownhat57.This talk will specifically focus on the other less common client-side vulnerabilities that are not as frequently discussed.Intentionally vulnerable applications developed with client-side JavaScript frameworks will be attacked and exploited live.Remediation strategies will also be discussed so that developers have tools to prevent these vulnerabilities.Through strengthening the security posture of JavaScript applications, we can take strides towards creating a more secure Internet.Break the web together!  They say the best offense is good defense.No. The best offense is offense.Hands-on vulnerability exploitation of medcellarAnand’s talk started by explaining the most common web application vulnerabilities that currently exist.We’re talking about SQL Injection, Cross Site Scripting (XSS) and Cross Site Request Forgery (CSRF).During the talk, Anand used an open source application that contains all of these vulnerabilities and which is available for you as a developer to fool around with.The application is called ‘MedCellar’ and you can find it on github.XSS &amp; CSRFWe saw how to perform XSS attacks and CSRF attacks on the MedCellar Application.These attacks weren’t extremely harmful at first but showed just how they could be exploited.Using the Burp Suite’s proxy, we were able to inspect all requests/responses the application was performing to get more insights in how the app actually worked.Burp Suite is an integrated platform for performing security testing of web applications.XSS  attacks users  JS Injection  Exploits can be bad, really badXSS is a serious vulnerability. It may not seem so for some people or clients but it really is!  How do we exploit apps where users have direct control  How do we attack web apps on a private networkCSRF Attacks!!  “Session Riding”  Attacker sends malicious URL to submit a form to a third party domain  Victim is tricked into interacting with the malicious link and performs undesirable actionsUsing a third party domain you can create a form (you won 1 million dollars) to perform an action like this.BeEFDuring the talk, Anand demonstrated how to perform XSS and CSRF attacks.However, it seemed like you were only able to hack yourself.Things got serious though, when Anand demonstrated how you could exploit these vulnerabilities way more by using a special Linux distro called Kali Linux and BeEF (Browser Exploitation Framework).Kali Linux is a linux distro designed specifically for Penetration Testing and Ethical Hacking.BeEF is a Penetration Testing tool that focusses on the browser and possible vulnerabilities in it and the applications running in it.Combining these two, Anand was able to do basically anything in the users browser and he demonstrated this by running some random audio in the users browser.Playing audio isn’t that harmful, but you could have installed a keyLogger instead and start tracking anything the user types on his computer.That seems to be a little bit worse than playing some audio!  When you enter a coffee shop and see someone using this, disconnect from the internet and run away as fast as you possibly can.” - Quote from AnandMitigate against these attacksImplementation of a CSRF mitigation is Tough!  Method Interchange  Beware of CSRF Token replay  Token must be tied to the user’s session on the server  CSRF Token exposed as GET Param: Could potentially have logs or some other network traffic see the CSRF token and intercept it that way.But, luckily for us, CSRF middleware which implements these mitigations has already been developed for us! You can find these libraries on github:  koajs  crumb  csurfKey takeaways  App Sec vulnerabilities can be used in combination  No state changing operations should be GET requests  Make sure the CSRF token is cryptographically secure          Random !== Cryptographically secure        CSRF Middleware Saves Lives!!Oh… And  Cross Origin Resource sharing (CORS)          Access-control-Allow-Origin: * IS BAD!      Day 2: afternoonSam Bellen: Changing live audio with the web-audio-apiSam is developer at Made with love.You can find him on Twitter using the handle @sambego.As a guitar player, I usually use some effects pedals to change the sound of my guitar.I started wondering: “What if, it would be possible to recreate these pedals using the web-audio-api?”.Well, it turns out, it is entirely possible to do so.This talk takes you through the basics of the web-audio-api and explains some of the audio-nodes I’ve used to change the live sound of my guitar.Presentation can be found here: https://github.com/Sambego/pedalboard-presentationGet the sound in the browser  Create new audio context.  Get the audio input of your computer: navigator.getUserMedia()  Create inputNode from the media stream we just fetched  Connect the inputNode to the audiocontext.destinationAdd effects to the soundVolume pedal  Create a gainNode = audioContext.createGain();  Value of gain is 0 tot 1  So for now we have input -&gt; gain -&gt; output.Distortion pedal  Make the audio sound rough.  Create a waveShaperNode = audioContext.createWaveShaper();  Set a value  So for now we have input -&gt; Waveshaper -&gt; output.Delay pedal  delayNode = audioContext.createDelay();  Set a value delayNode.delayTime.value = 1 (1 second)Reverb pedal  Some kind of echo on your sound  convolverNode = audioContext.createConvolver()  Load impulse-response-file and do some crazy stuffHow to create an oscilator  oscilatorNode = audioContext.createOscilator()  Set Hz valueweb-midi-api  Request access and start doing things with itRob Kerr: Science in the Browser: Orchestrating and Visualising Neural SimulationsRob works at IBM Research Australia.You can find him on Twitter using the handle @robrkerr.My talk will show how the old-school, computationally-heavy software used in science can be set free using the centralized power of cloud resources and the ubiquity of the browser.We'll see real-time, publicly-broadcast, simulations of the electrical activity in brain cells, visualised in 3D using Javascript.Neuroscience introductionThe topic for this talk was quite some heavy material.However, Rob managed to give us a quick, super high-level, introduction to neuroscience and more specifically an introduction to how neurons actually work.Very High level, there are 3 parts in a neuron:  Dendrites  Neuron body (Soma)  Axons.Neurons receive electrical signals through their dendrites, and transmit those to the neuron body, called the Soma.From the neuron body, new electrical signals travel to other neurons.Sending electrical current from one neuron to another is being done through its axons.So the axons actually send electrical signals to other neurons and those other neurons receive these signals trough their dendrites.A better, more thorough explanation of neurons is being described on Wikipedia, but we needed a super simplified explanation of neurons and their main components to further explain what Rob showed us.Science in the browserNeurons and their main components can be ‘encoded’ in special files .swc files.These files contain multiple records with an ID, X, Y, Z, Radius and Parent-link.Using all the records and their properties allows you to visually represent the neurons.There’s already an online repository containing these encoded neurons which you can find here.Now, what does all of this have to do with the browser or JS or anything you would expect at JSConf?Well, while he was working on his Ph.D. thesis, he started playing around with JS and its related technologies.And he continued to do so since then, all in function of the neuroscience domain.As we saw earlier, there’s already a webpage where you can upload swc files with neuron data to visually represent these, but these are rather static images.Instead, Rob decided to create a platform which can also simulate the behaviour of such a neuron when you trigger it with electrical current on its dendrites.Technology stackRob used a combination of tools and technologies to build the platform.Together with his colleagues at IBM research Australia, they built an entire Cloud platform that could perform these complex simulations.On their IBM Bluemix cloud they run Docker Containers running the algorithm that performs the neuron simulations.The algorithm is written in C and is based on mathematic formula which is shown in the below image.Hodgkin-Huxley Model of the Squid Giant AxonThe web application used to render the neurons used a combination of tools, most importantly:  Webgl: Web Graphics API. Javascript API for rendering interactive 3D graphics.  three.js: A Javascript 3D library that uses WebGL.  D3.js: Javascript library for visualizing data using HTML, SVG and CSSThe tool in actionIn the below video you can see what the tool looks and animations look like:\tThe tool enables researchers to replay a scenario where a certain spike is triggered in a branch of the neuron.This gives scientists a lot of knowledge and insights about how neurons behave.Rob gave a really entertaining talk with some really cool visuals of neurons in action.He introduced us to just the right amount of neuroscience to be able to follow what he was actually doing and showing!Stefan Baumgartner: HTTP/2 is coming! Unbundle all the things?!?Stefan is a web developer/web lover based in Linz, Austria.Currently working at Ruxit, making the web a faster place.He is also a co-host at the German Workingdraft podcast.You can find him on Twitter using the handle @ddprrt.In this session, we will explore the major features of the new HTTP version and its implications for todays JavaScript developers.We will critically analyze recommendations for deployment strategies and find out which impact they have on our current applications, as well as on the applications to come.Unbundle all the things?Everybody is saying to not bundle things, minify things, concatenate things, … when moving to HTTP/2.Tools like Browserify, Webpack, etc. would all become obsolete.But why? We need to question this and see if this is actually the truth.The best request is a request not being madeIn HTTP version 1.1 we need to do as few requests possible. Pages like Giphy have 40 TCP connection at a single time!HTTP/2 was made to prevent the bad parts of HTTP/1.1HTTP/2 allows a connection to stay open and transfer multiple things over the same connection.No need for handshakes for each file that needs to be transferred from the server to the client.Rule of thumbA slow website on HTTP/1.1 will still be a slow website on HTTP/2.You need to perform optimisations no matter what.Most important part: do not block the render path.Only serve what you really need.Again, the best request is a request not being made.So, unbundle all the things?So in some way, yes unbundle all the things.Because you don’t want to transfer bytes you don’t need, but there is something more to it.This article about packaging will get you on the way: engineering.khanacademy.org/posts/js-packaging-http2.htm.Create a lot of modules to update as flexible as possible and as small as possible.When using ES6 we can also use Treeshaking.  Create independent, exchangeable components  Create small, detachable bundles  Think about long-lasting applications and frequently of changeUse tools, not rules!Claudia Hernández: Down the Rabbit Hole: JS in WonderlandClaudia is Mexican front-end developer.You can find her on Twitter using the handle @koste4.What even makes sense in Javascript?For a language originally created in 10 days it surely has a lot of quirks and perks many JS developers are unaware of.Sometimes, it might even seem like we fell down the rabbit hole only to find that NaN is actually a Number, undefined can be defined, +!![] equals 1, Array.sort() may not work as you suspected and so much other nonsense that can trip any JS developer’s mind.This talk is a collection of Javascript’s oddities and unexpected behaviors that hopefully will prevent some future headaches and help understand the language that we all love in a more deeper and meaningful way.This talk by Claudia was so much fun! We didn’t write down anything because it was virtually impossible to do. You need to see this with your own eyes!You can view the slides on Speaker Deck.Be sure to check out jsfuck.com for some fun times and jQuery Screwed to get an idea of what you can actually do with JavaScript quirks.Lena Reinhard: Works On My Machine, or the Problem is between Keyboard and ChairLena is teamleader, consultant and photographer.You can find her on Twitter using the handle @lrnrd.In this talk we will look at the many facets that affect our decision making and interactions, and work out how we can change for the better.Together, we will take a look at the effects that our software has on the daily lives of the thousands of people who are using it.You’ll learn what you can do as an individual to support change into a positive direction, and how you can help debug this system and make a difference in the tech industry.You’ll leave knowing about practical things you can do in your daily life to make the tech industry a better, more inclusive and diverse environment that is a better place for everyone.Code debuggingDebugging can be hard and it becomes harder when working with complex software.Spaghetti code is difficult to read and maintain.It can be code that is not organised, has lots of dependencies and is difficult to debug.The Tech Industry is buggedA lot of people already contributed to the tech industry.It has grown very fast and has many flaws.That’s why we need to have a look at it and try to fix the defects.Understanding ourselvesTo be able to fix this we need to understand ourselves. Our flaws, limitations, …  We are privileged and need to understand that.Privilege: The human version of “works on my machine”.Privilege is sitting in your comfy home and not knowing a big thunderstorm is coming that could harm people.Privilege is being able to stand up when attending a standup and not having to sit because you are disabled.We are biasedWe need to understand we are biased.More often we are being objective and often that is not OK.We all have biases and we need to realise and understand.EmpathyWe need to understand that we need to be empathetic.Empathy is the right direction.CreativityCreativity is necessary to design and build good software.DiversityAnd so is diversity and understanding each other.InclusionInclusion means all people in the group are respected for who they are.The lack of inclusion and diversity is a real problem in our industry.The Tech IndustryLet’s look at some key points within our industry.Company  Lack of diversity  Lack of inclusion  HarassmentSociety  Racism  Patriarchy  CapitalismTech industry  Lack of diversity  Lack of inclusion  Harassment  Racism  Patriarchy  CapitalismSoftware can help peopleOur software can help people. A screenreader, accessibility, …But can also ruin livesOur software is racist.Our software (tools like Siri or Cortana or Snapchat) does not correctly recognise skin color, alters skin color and does not recognise harassment or racism.Animations in software can trigger panic attacks or epileptic attacks.  We have a collective responsibility and need to take that very seriously.Technology and our code is not neutral. Our work is political and has consequences on lives.Debugging the systemChange starts with you, starts with all of us.What can we do to debug the system?  Educate yourself, about systemic issues and oppression  Practice empathy, because we need it to be good designers and developers  Work on Humility, because none of us are Unicorns  Understanding Privileges, and use them for good  Address biases, and establish policies to address them  Listen, and actively look for voices outside of your networks  Amplify others’ voices, and speak less  Work on diversity, because it’s our moral obligation  Work on inclusion, to make spaces welcoming and safe  Give, our knowledge, time, technical skills, money  Work on being allies, constantlyQuite a talk on some serious matter to close the second day of JS Conf Budapest.Have you experienced these things yourself in the tech industry?Have you contributed to debugging the tech industry?Day 2: ConclusionJust like day 1, day 2 was one hell of a nice day packed full of great speakers and a superb atmosphere!The talks by Rob Kerr and the one of Lena Reinhart surely got the most attention.Rob’s talk because it was impressive to see what they achieved over a course of 2 years to visualise neurons in the browser.Lena’s talk because we got slammed in the face about how faulty the tech industry is at the moment.This year’s edition was, just like the one we attended last year a very good one!It is nice to see such a diverse community that cares about technology and people.This is something we should be very proud of.A big thank you to the organisers and volunteers to make JS Conf Budapest what it is!Find us on the family photo!Next yearIn 2017, JS Conf Budapest will be held on the 14th and 15th of September.We will surely be present for what will be another great edition! See you next year!JS Conf Budapest 2016, day 1Read our full report on day 1 of JS Conf Budapest here!."
      },
    
      "conference-2016-05-12-js-conf-budapest-day-1-html": {
        "title": "JS Conf Budapest Day 1",
        "url": "/conference/2016/05/12/JS-Conf-Budapest-day-1.html",
        "image": "/img/js-conf-budapest-2016.jpg",
        "date": "12 May 2016",
        "category": "post, blog post, blog",
        "content": "From JS Conf Budapest with loveThis year’s edition of JS Conf Budapest was hosted at Akvárium Klub.Located right in the center of the city, below an actual pool, filled with water!  Akvárium Klub is more than a simple bar: it is a culture center with a wide musical repertoire from mainstream to underground.There is always a good concert and a smashing exhibition, performance, or other event happening here, in a friendly scene, situated right in the city center.JS Conf Budapest is hosted by the one and only Jake Archibald from Google.After waiting in line at 8 o’clock in the morning to get our badges, we were welcomed at the main hall where some companies hosted stands.In another space after the main hall, tables were nicely dressed and people could have breakfast.When going downstairs to the right of the main hall, we entered the room where the talks would be given.For the coffee lovers, professional baristas served the best coffee possible.With a nice heart drawn on top if it.At 9 o’clock the conference would officially start so we went downstairs.After taking our seat, we played the waiting game and all of a sudden, we got this nice intro made with blender and three.js! Check it out for yourself!Day 1: Talks  Laurie Voss: What everybody should know about npm  Safia Abdalla: The Hitchhiker’s Guide to All Things Memory in Javascript  Yan Zhu: Encrypt the web for $0  Denys Mishunov: Why performance matters  Princiya Sequeira: Natural user interfaces using JavaScript  Maurice de Beijer: Event-sourcing your React-Redux applications  Rachel Watson: The Internet of Cats  Nick Hehr: The other side of empathyDay 1: MorningLaurie Voss: What everybody should know about npmLaurie is CTO at npm Inc.You can find him on Twitter using the handle @seldo.The presentation he gave can be found a slides.com/seldo/jsconf-budapest.npm is six years old, but 80% of npm users turned up in the last year.That's a lot of new people! Because of that, a lot of older, core features aren't known about by the majority of npm users.This talk is about how npm expects you to use npm, and the commands and workflows that can make you into a power user.There will be lots of stuff for beginners, and definitely some tricks that even most pros don't know.How does npm look up packages?In contrast to what most people think, npm does not download its modules from GitHub or other version control systems.They would not like it that such an amount of data is transferred on a daily basis.In short npm does this: You -&gt; CLI -&gt; Registry.Let’s dive in.  First, npm will take a look at your local cache and see if the package your are looking for is present.  Next, it will resort to the CDN network and use the server which is the closest as possible to your position.  Finally, if npm can’t find the package in local cache or the CDN network, it will look it up in the registry. The registry is a set of servers all around the world and it will try to match the best version that you are looking for.EACCESS errorA lot of people have issues with EACCESS errors because they used sudo to install things.The easy solution is to always keep on using sudo, BUT we can easily fix npm permission issues.package.jsonDon’t write your package.json yourself. Let NPM do it!It will always do it better. Use npm init, which will ask you some basic questions and generate package.json for you.ScopesA new feature in npm is scopes.These are modules that are “scoped” under an organization name that begins with @.Scopes can be public and private.Here is how to use scopes:npm init --scope=usernamenpm install @myusername/mypackagerequire('@myusername/mypackage')npm-init.jsTo extend the npm init command, it is possible to create an npm-init.js file.This file is a module that will be loaded by the npm init command and will provide basic configurations for the setup.By default the file is placed in the root of your project: ~/.npm-init.js.You can use PromZard to ask questions to the user and perform logic based on the answers.Remember that npm init can always be re-run.Why add stuff in devDependencies.Simple: because production will install faster! A lot of people don’t tend to do this, so please do this!When using this you can simple run the command below on production and be done with it.npm install --productionBundled dependenciesOne of the biggest problems right now with Node.js is how fast it is changing.This means that production systems can be very fragile and an npm update can easily break things.Using bundledDependencies is a way to get round this issue by ensuring that you will always deliver the correct dependencies no matter what else may change.You can also use this to bundle up your own, private bundles and deliver them with the install.npm install --save --save-bundleOffline installsA way to prevent npm to look up the registry, and ensure local installs, is by adding the variable --cache-min and to set it to a high value such as 999999.npm install --cache-min 999999Run scriptsIn package.json it is possible to define default run scripts as shown below.npm startnpm stopnpm restartnpm testOf course it is also possible to define your own run scripts.You can run these scripts like this:npm run &lt;anything&gt;Run scripts get devDependencies in pathDon’t force users to install global tools. That is just not cool.This way you can prevent to get conflicts over global tools, because different projects can use different versions.SemVer for packagesnpm uses Semantic Versioning, which is a standard a lot of projects use to communicate what kind of changes are in a release.It’s important to communicate what kinds of changes are in a release because sometimes those changes will break the code that depends on the package.Let’s take a look at an example.1.5.6Breaking Major . Feature Minor . Fix PatchThis is quite obvious, right?npm allows you to change the version (and to add a comment) by using the commands below.npm version minornpm version majornpm version patchnpm version major -m \"Bump to version %s\"Microservices architectureWhen working with a microservices architecture, it is possible to work with multiple packages for your services.This can be done by using the link function within npm.npm link &lt;dependency&gt;Let’s say we have a package named Alice and we have other packages that depend on this package.We can run npm link.In packages that depend on Alice, say Bob, we simply run npm link alice.All changes made in Alice will be immediately available in Bob without performing any npm update commands.Unpublish a packageBefore the recent events where a package called left-pad got pulled from npm and broke the internet, it was possible to unpublish a package just like that by using npm unpublish.Now this is restricted after the package has been online for 24 hours.To really unpublish the package you will need to contact support.A more friendly way is the use of npm deprecated that will tell users the package has been deprecated.Keeping projects up to dateBefore running npm update, it’s preferred to run npm outdated.This command will check the registry to see if any (specific) installed packages are currently outdated.npm outdatednpm updateBy doing so, you can prevent yourself from breaking the project if certain packages would not be compatible.Stuff everybody should know about npmA lot of things are available for npm that will make your life as a developer easier.  Babel: Transpile all the things! JavaScript, TypeScript, JSX, …  Webpack and Browserify  Greenkeeper (greenkeeper.io) is npm outdated as a service!  Node Security Project: Install by using npm install nsp -g. Use by running nsp check. You can use this to check if your project contains vulnerable modules.Why should I use npm?npm reduces friction.It takes things you have to do all the time and makes things simpler and faster.Safia Abdalla: The Hitchhiker’s Guide to All Things Memory in JavascriptSafia is a lover of data science and open source software.You can find her on Twitter using the handle @captainsafia or on her webpage safia.rocks.Slides and interactive tutorialThe slides of this talk can be found here http://slides.com/captainsafia/memory-in-javascript.Safia also created an interactive tutorial on how to use the Chrome DevTools for memory management.This talk will take beginners through an exploration of Javascript's garbage collector and memory allocation implementations and their implications on how performant code should be written.Attendees will leave this talk having gained insights into the under-the-hood operations of Javascript and how they can leverage them to produce performant code.Why should I care about memory?  It forces us to be (better) more inventive programmers, adds restrictions and forces us to use the best tools to create the best possible experience.  Memory is scarce. A lot of people still use devices that are not packed with a lot of memory.Not everyone has high performant development machines.  It helps us exercise our empathy muscles.What does it mean to manage memory?The Good, The Bad, The UglyHow does JS manage memory?Safia focuses on the V8 JS Engine.We have basic types in JavaScript:  booleans  numbers  stringsMemory is allocated in a heap structure and uses a root node which has references to other ones: booleans, string, etc.So basically: root node -&gt; references -&gt; variables.V8 allocates objects in memory in 6 contiguous chunks, or spaces:  New space: Memory gets allocated here when an object is created immediately.It is small and is designed to be garbage collected very quickly, independent of other spaces.  Old pointer space: Contains most objects which may have pointers to other objects.Most objects are moved here after surviving in new space for a while.  Old data space: Objects that just contain raw data (no reference or pointer) will end up here after surviving in new space for a while.  Large object space: Used to store large object tables.They get stored here so it doesn’t conflict with the store space of the above mentioned spaces.  Code space: Code objects are allocated here. This is the only space with executable memory.  Map space: Contains objects which are all the same size and has some constraints on what kind of objects they point to, which simplifies collection.How does V8 collect garbage memory?V8 uses a ‘stop the world’ technique that enables it to run a short garbage collection cycle.This means it will literally halt the program.V8 has different approaches on how it collects garbage in the new and old space.  New space: Garbage collection by using a scavenging technique.Each scavenging cycle will go through the entire heap starting from the root and will create copies.It will clear out what is currently in new space.Everything that is not reachable will be cleared out of the space.You need double the size of the memory that is available for the new space to use for the copy.  Old space: Mark and sweep technique.Remove unmarked objects on a regular basis.How do I write memory performant applications?Asking yourself the following two question will get you started!  How much memory is my application using?  How often do garbage collection cycles occur in my application?Of course you need to have the tools to work with.The Chrome DevTools HEAP allocation profiler will be our weapon of choice.It allows you to check the retain size and shallow size of objects.  Shallow size of an object is the amount of memory it holds of itself.  Retain size is all of its size and its dependents.Heap dumpHeap dump takes a snapshot of your heap at a specific moment.It will provide a file with .heap extension which enables you to load it in the Chrome DevTools for further inspection.npm install heapdumpLet’s practice!Follow this interactive tutorial on how to use the Chrome DevTools for memory management.Yan Zhu: Encrypt the web for $0Yan is engineer @brave and likes information freedom, breaking shit, cryptography, theoretical physics, free software, infosec, stunt h4cking, and an Internet that respects humans.You can find her on Twitter using the handle @bcrypt.Everyone in 2016 knows that websites should use HTTPS.However, there is a common misconception that TLS and other security measures are expensive and slow down both web developers and page load times.This talk will show you some easy tricks to make your site more secure without sacrificing performance or requiring a lot of effort.Is the web fast yet?Yes. Size of pages is rising. Amount of HTTPS requests is also rising!Is TLS fast yet?Yes. Netflix is going to secure streams this year over HTTPS.  2015: Netflix and chill  2016: Netflix and HTTPS and chillThe numbers aren’t entirely clear, so here they are:  Without encrypted Netflix streams, 65% of internet traffic is unencrypted. Only 29% of internet traffic is encrypted.  With encrypted Netflix streams, unencrypted internet traffic will drop to 26,9% and encrypted traffic will increase to 67,1%.Source: https://www.sandvine.com/downloads/general/global-internet-phenomena/2015/encrypted-internet-traffic.pdf  TLS has exactly one performance problem: it is not used widely enough.Everything else can be optimized.  Data delivered over an unencrypted channel is insecure, untrustworthy, and trivially intercepted.We owe it to our users to protect the security, privacy, and integrity of their data — all data must be encrypted while in flight and at rest.Historically, concerns over performance have been the common excuse to avoid these obligations, but today that is a false dichotomy. Let’s dispel some myths.Keep reading about this matter on istlsfastyet.com.HTTP/2Another technology that can help the adoption of TLS is HTTP/2.HTTP/2 offers:  Binary encoding instead of text encoding  header compression  Server push  multiple requests on single TCP connection!!HTTP/2 allows for requests to be sent in parallel rather than sequentially.Does HTTP/2 require encryption? No. However, Chrome and Firefox will only support HTTP/2 with encryption.Let’s EncryptLet’s Encrypt (a non-profit certificate authority) has left beta stage on the 12th of April and is a new Certificate Authority: It’s free, automated, and open.It is backed by some major sponsors such as Mozilla, Akamai, Cisco Chrome, and so much more.  The objective of Let’s Encrypt and the ACME protocol is to make it possible to set up an HTTPS server and have it automatically obtain a browser-trusted certificate, without any human intervention.This is accomplished by running a certificate management agent on the web server.Interested in Let’s Encrypt? Keep reading on letsencrypt.org.Get HTTPS for free!Manually setting up your free HTTPS certificates from Let’s Encrypt is also an option. You can do that on gethttpsforfree.com.Denys Mishunov: Why performance mattersDenys is frontend developer, speaker. Science aficionado. And writes for @smashingmag.You can find him on Twitter using the handle @mishunov or on his personal website mishunov.me.Performance is not about Mathematics.Performance is about Perception.Perception is what makes a site with very few requests nevertheless feel slow, while a site that delivers search results during tens of seconds can feel fast enough for your user.User’s perception of your website’s speed is the only true performance measure.This talk is about perception, neuroscience and psychology. The time is ripe to understand performance from the user’s perspective.In this talk Denys showed us that performance is not always in the numbers, but that it is most of the time perception.So next time you decide to invest a bunch of money in getting that request 100ms faster, make sure it will have impact!  Performance is about perception! Not mathematics.Houston Airport was used as an example to illustrate this quote.At Houston Airport, there were a lot of complaints about long waiting times at the baggage claim.They decided to optimize the baggage handling process.They managed to get luggage to the baggage claim in about 8 minutes (which is nice!).However, complaints weren’t dropping at all.It turned out that passengers needed only 1 minute to get from the plane to the baggage claim, which meant they needed to wait 7 minutes for their luggage.Eventually they decided to literally taxi and park the airplanes further so passengers now needed to walk 6 minutes from the plane to the baggage claim which reduced waiting times for luggage to 2 minutes.This caused complaints to drastically reduce!Speed!1 second gain will increase revenue by 1% for Company X. 1 second slower will decrease conversions by approximately 5%.The 20% rule.This rule defines that you should make a page load at least 20% faster, otherwise users will not notice.We’re talking about noticeable difference.A big difference with meaningful difference.Noticeable !== MeaningfulWe did a live test on the conference where the crowd needed to decide which of the two pages displayed loaded faster.The first page loaded in 1.6 seconds whereas the second one loaded in 2 seconds.Most of the people thought the second page, with 2 seconds load time was faster. This is all about perception!Another fun fact is when delaying audio on a video, our mind will trick us by syncing the audio with what is visible on the screen. Again perception!Key takeawayDon’t spend to much time optimizing the nitty gritty details of your code, instead try moving the active phase forward.As soon as there is activity being shown (pages being loaded), the brain enters the active phase.The user no longer feels as if he’s waiting (Remember the perception?).You can move the active phase forward by making use of:  async  Service workers  “The perception of performance is just as effective as actual performance in many cases” - Apple quoteDay 1 afternoonPrinciya Sequeira: Natural user interfaces using JavaScriptPrinciya works at Zalando Tech where she uses React and Redux. She’s also a startup enthusiast, teacher, speaker, DataViz Diva and has a love for food and JavaScriptYou can find her on Twitter using the handle @princi_ya and Zalando Tech using the handle @ZalandoTech.The way we interacted with computers on a large scale was stuck in place for roughly 20 years.From mouse to keyboard to joystick, it is game over.Today it is the era of gestures.Today’s gamers can do everything from slice and dice produce in Fruit Ninja to quest for a dragon in Skyrim.We’ve been captivated by these powerful, natural, and intuitive interactions; imagining what it would be like to have that power at our own fingertips.In this recent decade, we’ve seen some staggering advances in technologies bring us closer making these magical experiences a reality.In this talk I will present how we can create new, intuitive, interactions for these novel input devices using JavaScript.This talk takes on a different approach in user interaction, in way that different ways of input can result in the same output.At this moment we know the evolution of Typed, Clicked and Touched but currently we are evolving to Typed, Clicked, Touched, Guestures/Speech/… etc.Evolution of user interfaces.  CLI: Codified, Strict  GUI: Metaphor, Exploratory  NUI (Natural User Interfaces): Direct Intuitive. More natural and more intuitive.NUI + JS = NUIJSAt first, Princiya was trying to build a simulator for motion controlled 3D camera’s.A tool that is not dependent on any platform without using a physical device.The simulator is purely based on JavaScript and easily integrates with the device’s SDKs.Once the simulator was made, she tried to build some apps with it (using leap motion for example) to move a slideshow or any other purpose.The tool can be used for many purposes an a lot of devices are already available (VR, motion, …)  Augmented Reality.  Virtual Reality.  Perceptual Computing: bringing human like behaviour to devicesWhat next?Architecture  Step 1: USB controller reads sensor data  Step 2: Data is stored in local memory  Step 3: Data is streamed via USB to SDKLive demoPrinciya demonstrated a drawing board with a brush, both with mousepointer and LEAP motion.NUIJS will translate the input data from the mouse pointer to the Node.js Web Socket server and this one will process the data and send it back to the LEAP motion SDK.The same code can be used with the LEAP motion itself since it integrates nicely with the device’s SDKs.Other open source tools Princiya mentioned were Webcam Swiper and js-objectdetect.Viola-Jones AlgorithmMost of the tools will use or depend on the Voila-Jones Algorithm which can be used for object detection.Combined with other tools this can be very powerful.  HAAR feature selection  Creating an integral image  Adaboost training  Cascading classifiersMaurice de Beijer: Event-sourcing your React-Redux applicationsMaurice is a freelance developer/trainer and Microsoft Azure MVP.You can find him on Twitter using the handle @mauricedb.With Event-Sourcing every action leading up to the current state is stored as a separate domain event.This collection of domain events is then used as the write model in the CQRS model.These same events are projected out to a secondary database to build the read model for the application.In this session Maurice de Beijer will explain why you might want to use Event-Sourcing and how to get started with this design in your React and Flux applications.What is Event-sourcingEvent-sourcing is a way of capturing changes in the state of an application.The traditional way of doing this would be to just update the existing state of your application to whatever state it should be in.This way you always have the latest state of your data at your disposal.In Event-sourcing, you’ll capture all changes as events.These events will be stored in the sequence they were applied.You now have a complete log of events that happened in your application.This allows for features such as:  Complete Rebuild: Possibility to rebuild the entire application state by re-running all events.  Temporal Query: Determining the state of the application at a given point in time.  Event Replay: Replay incorrect events by reversing it and all subsequent events, then replaying the correct event and re-applying all later events.Common example of systems that use Event Sourcing are Version Control Systems.When to use Event-sourcing?Event-Sourcing is particularly useful in situations where you need to keep an audit trail of all changes that occurred to your data.Accountancy for example is a domain in which Event-Sourcing is very useful, because you need to be able to provide that trail for audit purposes.REPHRASE! -&gt; The immutability of events allows for more scalability in your apps also.CQRS and Event SourcingWhere Event-Sourcing describes the practice of storing all application state changes in individual events, CQRS describes the practice of separating the command from the read side.This means you’ll have a service exposing all write functionality in your application and a separate service exposing all read operations.This model works well with Event-Sourcing as you can use the Events occurring on your system as Commands in the CQRS model.During the talk, Maurice showed some samples of code that were the pieces of the puzzle in setting up Event-Sourcing and CQRS in your React-Redux application.Check out the slides for his talk here to find out more!Rachel White: The Internet of CatsRachel is a front end developer at IBMWatson. A lover of retro graphics &amp; horror &amp; coding &amp; games, but above all, of Cats.You can find her on Twitter using the handle @ohhoe.Find out more about her and her projects on rachelisaweso.me and imcool.onlineEver lose out on a good night's rest because your pesky cats keep waking you up at 4am for food?Rachel has.Many times.For her first project using node, socket.io, microcontrollers, and johnny-five, Rachel built a web-based feeder that delivers tasty cat chow on a configurable schedule or when triggered remotely.She'll walk you through her learning process and get you excited about trying new things in your own projects.Finally, she'll show you how to take the first steps to release your work to the open source community.One thing is for sure, Rachel really, really, really likes cats!Where a lot of people try to create things that improve others peoples lives, Rachel tries to do the same, instead, she does this for cats…One Question is constantly on her mind:  “How can we incorporate cats in technology?”Eventually, she decided to create a feeder bot for her cats and immediately thought of open sourcing “the thing”.The talk was mainly a tour of what she’s learned and encountered along the way.Trying new things is scaryRachel wasn’t exactly familiar with robotics or backend development, so she would be entering a whole new world.She’d have to try out new things and start a project without any idea of whether all of this would actually work out.  Embarking on a new project: will it succeed, will it suck?  Using new technologies for the first time: what will happen, will it work for me?  Contributing to Open Source: putting yourself out there is terrifying!Why so scary?Why is this so scary? It turns out the Open Source developer community can sometimes be quite a harsh environment…  Fear of rejection  Imposter Syndrome  Inclusiveness of Communities  Bad behaviour in General: e.g. Oh you didn’t know about THIS?, e.g. completely ignoring contributions  Your GitHub green timeline is not a representation of what you’re worth. Just opening a PR just for the sake of it sucks.  Don’t insult the contributor. Why on earth …  Vulgar and brutal harassment of the community, seriously, get a life!  PR’s that get ignored (for over a year) and then the maintainer writes the same fixes and says: Oops!Eventually, Rachel set up a Twitter poll asking people about what bad experiences in Open Source Software development they’d already encountered, showing off an entire list of Twitter responses. Which weren’t that positive (euphemism!)One of her Twitter contacts actually created (and open sourced) a tool called echochamber.js, which allows you to include a commenting form in your site that stores the comments only in the local storage.That way, you can be an a**hole and post really offensive comments without actually insulting people.Echochamber.jsProposals for new contributorsKnowing all of these things now, you might wonder if it’s even worth it putting yourself out there.The answer of course is YES, but consider the following tips when doing so!  Find something you are passionate about  Something new you want to try  Make something cool and open source it yourself  First point of contact is your peers  Constructive criticism!Building a cat feeder botNow, let’s talk about the actual Cat Feeder bot, which was most suitably named RoboKitty. Check it out at here! and hereIt’s a node based cat feeder that works over the web.You can use it to instantly feed your cat, or you can feed periodically using cron triggers.After some trial and error on choosing the right combination of hardware, the final list of technologies involved in creating the Cat Feeder Bot looks something like this:  Node.js  Johnny-five: Javascript Robotics &amp; IoT platform.  Particle Photon kit (with breadboard)  4xAA battery pack with on/off switch  Misc hardware accessoriesOther things learned along the way were:  A servo needs external power, so yeah, plugging it in the microcontroller is not enough! :D  No idea how to solder…? Worked out! -&gt; Youtube -&gt; Learn how to solder.Lessons learned  Don’t be afraid of the unfamiliar  Don’t be afraid to ask for help  People really like cat stuff  Don’t downplay your abilities: I mean, it’s a super cool kitty food dispenser!  I like nodebots a lotNick Hehr: The other side of empathyNick is an Empathetic Community Member, Front-End Dev @NamelyHR ,@hoodiehq Contributor, @tesselproject Contributor and @manhattan_js OrganizerYou can find him on Twitter using the handle @HipsterBrown.In an industry that is so focused frameworks &amp; tooling, we tend to lose sight of the people behind the products and how we work with them.I’ve found empathy to be a powerful resource while collaborating with teams inside companies and across the open source community.By breaking down The Other Side of Empathy, I will demonstrate how applying its principles to your development process will benefit the community and the products they create.EmpathyNick Hehr shares Rachels’ point of view on the sometimes rude Open Source communication and communication on Social media in general.In his talk, he addressed the way you should behave when volunteering to contribute or when giving feedback to contributors in Open Source Software (OSS) projects.And Empathy turns out to be key in this process.RantingIt’s all too easy to judge or express prejudice these days, through these social media channels and not think about the people who are actually behind the idea or concept you’re judging.People that decide to Open Source the work on which they’ve spend tons of effort (usually because it’s their passion, but still…) aren’t exactly waiting for trolls or rants from people who like this easy judging.Empathy also plays a huge role in the other way around.It happens all too often that people trying to contribute to OSS for the first time are being ignored (by literally ignoring their pull requests for example), being treated like idiots (instead of being given constructive feedback when there is room for improvement), etc…Saying nice things  “If you don’t have anything nice* to say, don’t say anything at all!”Nice in this context means constructive. Comment on something you think could use improvement and offer a solution.Compliment on certain aspects that really improve the tool.Due to the relative anonymity of social media and other communication channels, we tend to forget these principles.Key take-awaysKey points to take away from this session are:  Give constructive feedback!!!  Always keep in mind the language your using when commenting on Open Source initiatives          Don’t be to blunt or direct in your reactions.        Use the right channels for your communication          meaning, don’t ask for feedback on twitter      Instead turn to platforms such as Slack, IRC, Gitter…      Get (constructive) feedback from people you trust        People that open source their tools don’t owe you anything.          They’re not entitled to give up all their time for you.      They’re not here to start fulfilling all requests from a demanding user base. It’s open source, submit a pull request      Living by these rules will make the (web-)world a little bit of a better place, but won’t prevent other people from still continuing these bad habits.Don’t let these people get to you! Continue doing what you’re passionate about and seek those that will give you that constructive feedback.Afterparty with Beatman and LudmillaAfter a long day, it was time for some party time and since JS Conf Budapest was hosted at a club, this could only be good!We were presented a live set by Breakspoll 2015 winner Beatman and Ludmilla.Day 1: ConclusionDay 1 was packed full of great speakers and the atmosphere was superb! A lot of inspiring talks that gave us a lot of topics to cover for the months to come within the JWorks unit at Ordina Belgium.The after party with Beatman and Ludmilla was a perfect closing of the day. On our walk to the hotel we could only imagine what day 2 would bring.Read our full report on day 2 of JS Conf Budapest here!."
      },
    
      "microservices-2016-05-01-using-jwt-tokens-for-state-transfer-html": {
        "title": "Using JWT for state transfer",
        "url": "/microservices/2016/05/01/Using-JWT-Tokens-for-State-Transfer.html",
        "image": "/img/JWT/jwt-logo.png",
        "date": "01 May 2016",
        "category": "post, blog post, blog",
        "content": "At one of our clients, we have been using Json Web Tokens quite extensively.We even use it to persist state on the client.Why persist state on the client?When building microservices, we need to build so-called “cloud native” applications.One of the key tenets of cloud native application design is keeping your services stateless.The benefit of having stateless applications is foremost the ability to respond to events by adding or removing instances without needing to significantly reconfigure or change the application.More stateless services can easily be added when load suddenly increases, or if an existing stateless service fails, it can simply be replaced with another.Hence, resilience and agility, are easier to achieve with stateless services.Keeping your services stateless means we need to persist our state somewhere else.Since we are transferring state in a REST architectural style, we can use the client to retain our state.For scaling purposes this is a great solution, as the client will only ever have to store its own state, and the server will be relieved of the state of all its clients.At our client we have chosen to use JWT for this state transfer to the client.While JWT is primarily intended for authentication and authorization purposes, the specification allows us to add any data we’d like to verify later on.Looking goodImagine the following scenario:A list of products is fetched from the “products microservice”.The user isn’t allowed to view all products, so only those products the user has access to are returned.When the user wants to order a product, he sends an order request to the “orders microservice” with the id of the product he wants to order.At that moment the “orders microservice” needs to know whether or not the user is allowed to access this product, let alone order it.Since the rights to access and order are the same, we’d like to reuse the information returned from the first call to the “products microservice”.This flow is illustrated below.We could call the “products microservice” from the “orders microservice” and rely on caching, but that would still be an extra network hop and the cache could potentially be invalidated by the time the user orders the product.Using the JWT approach, state is given to the client (the list of product ids the user is allowed to access), and being passed to the server again the moment an order is placed.The signature of the token guarantees us that the state has not been tampered with, while residing on the client.Too good to be trueThis solution prevents the server from having to care about state.It allows the client to store its own state and send it to the server whenever the server requires it - while being guaranteed the data isn’t tampered with.While this might seem like a good idea, it can backfire quickly.CouplingIn distributed systems such as microservices, it’s very important to manage the way we talk between components over the network.Using protocols such as HTTP and especially with the REST architectural style, great care needs to go in defining the contracts between these components.While we can use content-negotiation to version our resources, and JSON for instance as content type, we can build our clients as tolerant readers.Headers don’t have any of these benefits.A header is basically just a key and a value, and in case of JWT, the value is encoded.Therefore it’s hard to do versioning or any kind of content management on the data transferred inside these tokens.In the aforementioned example, the token couples the “products microservice” with the “orders microservice”.If the “products microservice” changes the structure of the token, the “orders microservice” will no longer be able to read it.While this coupling would exist as well when the “orders microservice” would call the “products microservice” directly, we would manage that coupling as part of the contract between these two microservices.In our case we don’t know there is a link between the two microservices since they don’t call each other directly.Yet by transferring the token from one microservice over the client to the other microservice, we are creating a hidden dependency.It’s also hard to have versioning on headers unless we put the version inside the name of the header.ScalingAdding versions to the headernames, documenting which microservices expect which versions of tokens of other microservices, and making sure we implement the tolerant-reader principle when reading the tokens might be a step in the right direction to avoid mass hysteria when tokens have to be adjusted.But what is simply impossible to get around, is the size restriction of headers in HTTP requests and responses.The HTTP specification doesn’t put any restriction on header size (singular or combined).But web servers, reverse proxies, CDNs and other network components do.Why they do this is not entirely clear as the spec allows any size, but the fact of the matter is that these restrictions exist.Putting a list of ids in a header like in our products example, will eventually break as the list could get too long.It’s not even clear how long is too long.AlternativesWe see three possible alternatives to this failed approach to manage state.Instead of passing the state from one microservice over a client to another microservice, we could pass the state as part of the body of the request and response.The downside of this approach is that we can no longer use GET methods for the calls where we need to pass the previously fetched state.The second alternative is to persist the state in a key value datastore on the server.We could asynchronously fetch products data and store it inside a datastore owned by the “orders microservice”.This could get stale, but so could a cache on the “products microservice”.This approach seems most common in the industry and could be well be the most preferable.And when all else fails, we can still simply make a call from the “orders microservice” to the “products microservice” and count on caching.ConclusionUsing Json Web Tokens as a means to transfer state to and from microservices via the client seemed like a good idea, but in the end turned out to be quite an anti-pattern.It introduces hidden coupling which is hard to manage, and can outright fail completely when headers become too big.Transferring state through the body of requests and responses could be a better approach.Using key value datastores to cache data of other microservices on your own microservice feels like the best way to go."
      },
    
      "angular-2016-04-25-component-based-application-architecture-with-angularjs-and-typescript-html": {
        "title": "Component-based application architecture with AngularJS and TypeScript",
        "url": "/angular/2016/04/25/component-based-application-architecture-with-angularjs-and-typescript.html",
        "image": "/img/components-angularjs.jpg",
        "date": "25 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Introduction  Ideally, the whole application should be a tree of components that implement clearly defined inputs and outputs, and minimize two-way data binding. That way, it’s easier to predict when data changes and what the state of a component is.  – AngularJS documentationIn this article I will offer some basic guidelines on how to create a scalable AngularJS application with reusable, well encapsulated components that are easy to maintain and refactor.AngularJS (version 1.5.5 at the time of writing) and its latest features offers us the ability to structure our apps as a tree of components.  Each component contains its own controller and templateIt can even have its own (relative) routing configured if you take advantage of the new Component Router.If you are on a team with multiple front-end developers you can easily divide the work by letting each developer focus on a separate component.It also helps in migrating to Angular 2, though I cannot promise it will be an easy task.Another bonus point is you are getting into the mindset of modern front-end development: web components.My preferred toolchain when developing AngularJS applications consists of TypeScript, NPM and Webpack.The sample code in this article and the sample application are created together with these tools.You can find the sample application on Github:https://github.com/ryandegruyter/angularjs-componentsWhat is a component?  In AngularJS a component is a directive.More specifically we call it a component directive or template directive.It is an approach to writing your own custom HTML elements which browsers are able to read and render.HTML comes with a set of pre-defined elements, for example the &lt;div&gt;&lt;/div&gt; element or the &lt;span&gt;&lt;/span&gt; element.By combing and nesting these standard HTML tags we can build complex UI widgets.We can change their appearance and behavior dynamically with JavaScript and CSS.True web components can isolate their structure, appearance and behavior.They make use of a technology called the Shadow DOM, which isolates the component in a separate DOM tree.This element will have its styles and scripts encapsulated, they will not conflict with the styles and scripts inside the parent DOM.Angular 2 takes full advantage of this technology, but unfortunately AngularJS, the framework I will be talking about in this article, does not.  Components you write and register inside AngularJS do not get isolated into a separate DOM tree.Lucky for us we are able to mimic the effect of Web components by using directives.We can write a reusable UI element, declare it with a custom tag and configure it by supplying attributes on the element.My advice is to be sure to use correct naming conventions and a module system so styles and scripts will not conflict with each other.Directives and componentsTo create and register a custom element in AngularJS, we can use either methods:  .directive (name, factoryFunction)  .component (name, object)While components are restricted to custom elements, directives can be used to create both elements as well as custom attributes.There are 3 types of directives:  Component directive  Attribute directive  Structural directiveA Component directive is a directive with a template.The .component() method is a helper method which creates a directive set with default properties.An Attribute directive is declared as an element attribute and they can change the appearance or behavior of an element (ng-change, ng-click, …).A Structural directive is an attribute or element that manipulates the DOM by adding or removing DOM elements (ng-if, ng-repeat, …).When do we use .directive(), and when do we use .component()?Custom UI elements should be created with the .component() helper method because it:  enforces best practices and provides optimizations (isolate scope, bindings)  has handy defaults making it easy to create components  makes migration to Angular 2 easier  can take advantage of the new component router which will be the default router in Angular 2  Use the .directive() method when you want to manipulate the DOM by adding or removing elements (Structural directive) or when you want to change the appearance or behavior of an element (Attribute directive).Creating a component-based AngularJS applicationBeginning with a component-based application architecture we need to have a root component.Before creating a component you have to decide if it will be a  Presentational component or a Container component.Presentational componentAlso known as a dumb component.They are used to visualize data and can easily be reused.They don’t manipulate application state nor do they fetch any data.Instead they define a public API in which they can receive inputs (&lt; and @ bindings), and communicate any outputs (&amp; binding) with their direct parent.  Designers can easily work on Presentational components because they don’t interfere with application logic.These components are unaware of any application state, and they only get data passed down to them.A simple presentational root component.export class RootComponent implements IComponentOptions {    static NAME:string = 'app';    bindings:any = {        title: '@',    };    template:string = `        &lt;h1&gt;{{$ctrl.title}}&lt;/h1&gt;        &lt;currency-converter&gt;&lt;/currency-converter&gt;    `;}    angular            .module('currencyConverterApp, []')            .component(Rootcomponent.NAME, new RootComponent());This component has a very simple API with one input -  title - and zero outputs.It doesn’t call a service or fetch any data.It doesn’t update any outside resources or make any requests to manipulate application state.Also notice how easy it was to register this component directive.Let’s create the same component directive but register it with the .directive() method.export class RootComponent implements IDirective {    static NAME:string = 'app';    restrict:string = 'E',    bindToController:any = {        title: '@',    },    scope:IScope = {},    controller:Function = ()=&gt;{},    controllerAs:string = '$ctrl',    template:string = `        &lt;h1&gt;{{$ctrl.title}}&lt;/h1&gt;        &lt;currency-converter&gt;&lt;/currency-converter&gt;     `;    static instance():IDirective {        return new RootComponent();    }}angular.module('currencyConverterApp, []').directive(Rootcomponent.NAME, RootComponent.instance());As you can see, this has a lot more configuration compared to using the .component() helper method.Although it offers more power and flexibility, its more practical to have the .component() method when creating custom UI elements.  bindings are automatically bound to the controller  controllerAs defaults to $ctrl  always creates an isolate scopeContainer componentsAlso known as smart components.This type of component is more tightly coupled to the application and not intended for reusability.It fetches data, manages part of the application state and provides the data to its child components.The child component communicates any update on the data through its output bindings (&amp;).The container component eventually decides what action to take with the data, not the child component.Let’s look at an example of a container component, I will leave out the complete template for brevity’s sake, you can view the complete code in the companion repository.First we start with our component definition:export class CurrencyConverter implements IComponentOptions{    static NAME:string = 'currencyConverter';    template:string = `        ...        &lt;currencies-select            title=\"From\"            on-selected=\"$ctrl.fromSelected(selectedCurrency)\"            show-values-as-rates=\"true\"            currencies=\"$ctrl.fromCurrencies\"        &gt;&lt;/currencies-select&gt;        &lt;currencies-select            title=\"To\"            on-selected=\"$ctrl.toSelected(selectedCurrency)\"            show-values-as-rates=\"true\"            currencies=\"$ctrl.toCurrencies\"        &gt;&lt;/currencies-select&gt;        ...\t\t    `;    controller:Function = CurrencyConverterComponentController;    }The template contains two declarations of a presentational component &lt;currencies-select&gt;.When we look at the attributes of the currencies-select element, the component API consists of three inputs (title, show-values-as-rates and currencies) and one output (on-selected).Our container component can bind a callback method on the on-selected attribute which offers an opportunity for the currencies-select component to communicate with its parent component.Below we define our components controller, here we can set and manipulate our template’s view model.export class CurrencyConverterComponentController {    selectedFromCurrency:Currency;    selectedToCurrency:Currency;    amount:number;    result:number;    fromCurrencies:Currency[];    toCurrencies:Currency[];    static $inject = [CurrenciesDataService.NAME];        constructor(private currencyDataService:CurrenciesDataService) {    }    $onInit():void {        this.fromCurrencies = this.toCurrencies = this.currencyDataService.getCurrenciesByYear(2016);    }    convert(from:number, to:number):void {        this.result = (this.amount / from) * to;    }    fromSelected(currency:Currency):void {        if(this.selectedToCurrency){            this.convert(currency.rate, this.selectedToCurrency.rate);        }        this.selectedFromCurrency = currency;    }    toSelected(currency:Currency):void {        if(this.selectedFromCurrency){            this.convert(this.selectedFromCurrency.rate, currency.rate);        }        this.selectedToCurrency = currency;    }}This component injects a data service to fetch a list of currencies.We pass this list to each &lt;currencies-select&gt; element in the $onInit method.The $onInit is a component lifecycle method that gets called by the framework each time the component gets instantiated.In this method we set our view model properties _fromCurrencies_ and _toCurrencies_ equal to a list of currencies fetched from the data service.The fromSelected and toSelected methods are passed down as callbacks for the &lt;currencies-select on-selected&gt; output.So how does our presentational component definition look like?export class CurrencySelectComponent implements IComponentOptions {    static NAME:string = 'currenciesSelect';    bindings:any = {        title: '@',        currencies: '&lt;',        onSelected: '&amp;',        showSelected: '&lt;',        showValuesAsRates: '&lt;'    };    controller:Function = CurrencySelectComponentController;    template:string = `...`;}export class CurrencySelectComponentController {    public title:string;    public currencies:Currency[];    public onSelected:Function;    public showSelected:boolean;    public showValuesAsRates:boolean;    public selected:Currency;    constructor() {    }    onCurrencyClick(currency:Currency) {        this.selected = currency;        this.onSelected({selectedCurrency: currency});    }}Bindings define the components API, in the above case there are four bindings.Our previous example declared this component but we only noticed three inputs and one output.Apparently there is a fourth input called _showSelected_.We can guess that it’s a flag for showing the selected currency.But as a new developer, we cannot be sure.  This is one of the reasons why it is important to document your components API.It will save new developers and designers a lot of time figuring out how to correctly use your component.Your component will become more transparent and not just an abstract definition.As you can see this component does not inject any data services or manage any outside state.It only receives data through its input bindings:  @ stands for one way string binding  &lt; stands for one way any other primitive/type bindingThe output binding public onSelected:Function; gets called each time the onCurrencyClick method gets called, it passes the selected currency which gets communicated back to the parent component.Make sure the parameter object key matches the parameter name in the parent component’s viewmodel, or the component will not be able to communicate any data.in this case selectedCurrency:onCurrencyClick(currency:Currency) {  this.selected = currency;  this.onSelected({selectedCurrency: currency});}And inside the parent component’s template:&lt;currencies-select on-selected=\"$ctrl.toSelected(selectedCurrency)\" ...Another way of accessing selectedCurrency, is to use $locals.This is useful when you want to send multiple types of data back.The advantage is you don’t have to specify each parameter separately in the component’s template.The disadvantage is $locals is not descriptive.&lt;currencies-select\ton-selected=\"$ctrl.toSelected($locals)\"...To access the selectedCurrency you would use the property on the $locals object with the same name:toSelected($locals:any):void{\tvar selected:Currency = $locals.selectedCurrency ...}Component communicationOutput bindingIn our previous example we saw an example of child to parent communication by mapping an output binding:binding:any = {\tonSelected: '&amp;'}The parent component can pass a method to this binding which the child component can call back and optionally send back any data to.Mapping the require propertyA child component can also require its parent components controller by mapping it in the require property:require:any = {\tparentCtrl: '^parentComponentName'}The ^ symbol is important here.You should replace parentComponentName with the correct component name, you are free to choose a different name for the key, in this case parentCtrl.The parent controller will get bound on to the property parentCtrl.Be aware that this creates a tight coupling between the child and parent component.Using a serviceWe should access and manipulate application state in our container components, but only through services, a component’s controller primary responsibility is to manage the template’s view model.You can implement a custom observer pattern inside the service, or use the rootscope as an eventbus.export class SampleService{    static SERVICE_NAME:string = \"mysampleservice\";    static EVENT_NAME:string = \"sampleEvent\";    static $inject = ['$rootScope'];    constructor(private $rootScope:IRootscopeService){}    subscribe(scope:IScope, callback:Function):void {        var handler = this.$rootScope.$on(SampleService.EVENT_NAME, callback);        scope.$on('$destroy', handler);    },    notify():void {        this.$rootScope.$emit(SampleService.EVENT_NAME);    }}A component controller can get notified by any changes by subscribing to the service:    export class MyComponentController{\t\t        static $inject = ['$scope', SampleService.SERVICE_NAME];        constructor(private isolatescope:IScope, private sampleService:SampleService){}        $onInit():void{            this.sampleService.subscribe(this.isolateScope, ()=&gt;{                ...            });         }    }SummaryStart with a root component and work your way down building components that are composed of either presentational and container components.Data should flow down in one direction (&lt; and @ input bindings), and events should propagate back up (&amp; output binding).  Services manage application state  Controllers manage a templates’ view model  Application state is accessed only by container components through services.  Use .component() when writing custom HTML elements in AngularJS  Use .directive() when you need to manipulate the DOM or need to change the appearance or behavior of a DOM element  Minimize 2 way binding (ngModel and = binding)  Presentational components can contain both container components and presentational components and vice versa  Use the component router, which makes it easy to bind URL paths to components. A component can contain its own relative routes too  Document your component’s API so new developers and designers know how to use it correctly  Keep your controllers clean, their main purpose is to set and manipulate the templates’ view model. Delegate business logic to services"
      },
    
      "microservices-2016-04-22-lagom-first-impressions-and-initial-comparison-to-spring-cloud-html": {
        "title": "Lagom: First Impressions and Initial Comparison to Spring Cloud",
        "url": "/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html",
        "image": "/img/lagom.png",
        "date": "22 Apr 2016",
        "category": "post, blog post, blog",
        "content": "  “It’s open source. It’s highly opinionated.   Build greenfield microservices and decompose your Java EE monolith like a boss.” - LightbendTable of Contents  Just the right amount  Design philosophy  Building blocks  Getting started with Lagom  Anatomy of a Lagom project  Example of a microservice  CQRS and Event Sourcing  Lightbend Q&amp;A at the CodeStar launch event  Comparison with Spring  Our advice  Conclusion  Useful linksJust the right amountMeet Lagom, Lightbend’s (formerly Typesafe) new open source framework for architecting microservices in Java.On the 10th of March, Lightbend released the first MVP version of Lagom which is the current version at the time of writing.Although there is currently only a Java API, Scala enthusiasts should not fret because a Scala API is a main priority and well on its way.Lagom is a Swedish word meaning “just the right amount”.Microservices have often been categorised as small services.However, Lightbend wants to emphasize that finding the right boundaries between services, aligning them with bounded contexts, business capabilities, and isolation requirements are the most important aspects when architecting a microservice-based system.Therefore, it fits very well in a Domain-Driven Design focused mindset.Following this will help in building a scalable and resilient system that is easy to deploy and manage.According to Lightbend the focus should not be on how small the services are, but instead they should be just the right size, “Lagom” size services.Lagom, being an opinionated framework, provides a “golden path” from which the developer can deviate if necessary.Being based on the reactive principles as defined in the Reactive Manifesto, Lagom provides the developer a guard-railed approach with good defaults while also allowing to deviate if necessary.This blogpost will cover our initial impression on the framework together with our opinion on the choices made while architecting the framework.Note that we won’t go too deep into detail in all the different aspects of the framework, for more details refer to Lagom’s extensive documentation.As Lightbend is entering the microservices market with Lagom, we feel obliged to make a fair comparison with existing frameworks out there.In the Java world this is predominantly the Spring stack with Spring Boot and Spring Cloud, standing on the shoulders of giants such as the Netflix OSS.In this current stage, it would be a bit too early to make an in-depth comparison between the two, seeing as you would be comparing a mature project to an MVP.What we can share though, are our initial observations.Design philosophyLagom’s design rests on the following principles:  Message-Driven and Asynchronous: Built upon Akka Stream for asynchronous streaming and the JDK8 CompletionStage API.Streaming is a first-class concept.  Distributed persistence: Lagom favours distributed persistence patterns using Event Sourcing with Command Query Responsibility Segregation (CQRS).  Developer productivity: Starting all microservices with a single command, code hot reloading and expressive service interface declarations are some examples of Lagom’s high emphasis on developer productivity.Building blocksThe Lagom framework acts as an abstraction layer upon several Lightbend frameworks and consists of the following core technologies and frameworks:  Scala  Java  Play Framework  Akka and Akka Persistence  sbt  Cassandra  Guice  ConductRSeeing as it acts as an abstraction layer the developer doesn’t need to hold any knowledge of Play Framework and Akka in order to successfully use Lagom.Sbt has been chosen as the build tool because it also acts as a development environment.Lagom relies heavily on the following sbt features:  Fine-grained tasks  Each task may return a value  The value returned by a task may be consumed by other tasksAccording to Lightbend, Scala’s build tool ‘sbt’ offers many handy features to Lagom such as fast incremental recompilation, hot code reloading, starting and stopping services in parallel and automatic injection of configuration defaults.Sbt might be seen as a hurdle by most Java developers since it is Maven and Gradle (and to a lesser extent Ant) that rule most Java projects.Moving towards a microservices framework such as Lagom would already constitute quite a transition so we think that this might hold back Java developers from adopting the framework.Lightbend’s rebranding could be interpreted as a move away from a Scala-oriented company towards a more Java-minded company.In that regard it would make sense to lower the initial learning curve especially for a rather trivial component such as a building tool.After all, the most important thing to achieve adoption is allowing people to easily get started with the new technology. We think that providing integration for Maven or Gradle would have a positive effect on the adoption rate and although it may not be trivial to implement, it should help convince Java developers to give Lagom a go.Google’s Guice has been chosen for dependency injection since it is a lightweight framework.What is remarkable is that Guice is used as well for intermicroservices calls.Lagom acts as a communication abstraction layer and it does so by adding a dependency on the interfaces of remote microservices.Just like a shared domain model and shared datastores being antipatterns for microservices, having code dependencies from one service in another is as well.Changing the code of one microservice should not have an immediate cascading effect on other microservices.This is the very essence of the microservices architecture.In a monolith, having code changes in one component can result in immediate breaking changes in other components of the system.While this may be desired in order to keep technical debt low, this is an inherent characteristic of monolithic systems.One of the reasons microservices were introduced, is to decouple components on all levels, especially binary coupling.Using protocols between components instead of actual binary dependencies allows us to implement the tolerant reader principle and versioning through for instance content negotiation.Lightbend argues that sharing interfaces as code will increase productivity and performance, but we fear the result of this is a distributed monolith instead of an actual decoupled microservices architecture.While we question the default way of communicating between microservices in Lagom, we are enthusiastic that more ways of making intermicroservices calls are becoming available.Using HTTP is possible as well, and one of the upcoming features is a Lagom Service Client.The Guice approach might also be quite favorable for people migrating from monolithic applications to microservices.In the end it is a trade-off, but one that shouldn’t be taken lightly.As a default persistence solution, Apache Cassandra is used due to how well it integrates with CQRS and Event Sourcing.Lagom has support for Cassandra as datastore, both for the reading and writing data.It is possible to use other datastore solutions but this comes at the cost of not being able to take advantage of the persistence module in Lagom.ConductR is an orchestration tool for managing Lightbend Reactive Platform applications across a cluster of machines and is Lightbend’s solution for running Lagom systems in production.Note that ConductR comes with a license fee and is majorly targeted at enterprises.The other option we currently have in order to run our Lagom system in production is to write our own service locator compatible with Lagom.At the time of writing someone already started working on Kubernetes support and we are sure that, given more time, more options will become available.For now though, Lagom is still in an early stage where we either have to pay for the ConductR license, build our own service locator, or wait until someone does the work for us.Getting started with LagomIn order to start using Lagom, Activator must be correctly set up.Currently two Lagom templates exist that can be used for creating a new Lagom application.The Lagom Java Seed template should be the template of choice, the Lagom Java Chirper template is an example of a Twitter-like app created in Lagom.Creating a new Lagom application is as simple as using the following command:$ activator new my-first-system lagom-javaAfterwards the project can be imported in any of the prominent IDEs as an sbt project.In order to boot the system, we first need to navigate to the root of the project and start the Activator console:$ activatorAfter which we can start all our services using a single simple command:$ runAll&gt; runAll[info] Starting embedded Cassandra server.......[info] Cassandra server running at 127.0.0.1:4000[info] Service locator is running at http://localhost:8000[info] Service gateway is running at http://localhost:9000[info] application - Signalled start to ConductR[info] application - Signalled start to ConductR[info] application - Signalled start to ConductR[info] Service helloworld-impl listening for HTTP on 0:0:0:0:0:0:0:0:24266[info] Service hellostream-impl listening for HTTP on 0:0:0:0:0:0:0:0:26230[info] (Services started, use Ctrl+D to stop and go back to the console...)This command starts a Cassandra server, service locator and service gateway.Each of our microservices is started in parallel while also registering them in the service locator.Additionally, a run command to individually start services is available as well.Note that the ports are assigned to each microservice by an algorithm and are consistent even on different machines.The possibility to assign a specific port is available though.Similar to Play Framework, Lagom also supports code hot reloading allowing you to make changes in the code and immediately seeing these changes live without having to restart anything.A feature we’re very fond of.In general, a restart is only required when adding a new microservice API and implementation module in the project.Anatomy of a Lagom projecthelloworld-api           → Microservice API submodule └ src/main/java         → Java source code interfaces with model objectshelloworld-impl          → Microservice implementation submodule └ logs                  → Logs of the microservice └ src/main/java         → Java source code implementation of the API submodule └ src/main/resources    → Contains the microservice application config └ src/test/java         → Java source code unit testslogs                     → Logs of the Lagom systemproject                  → Sbt configuration files └ build.properties      → Marker for sbt project └ plugins.sbt           → Sbt plugins including the declaration for Lagom itself.gitignore               → Git ignore filebuild.sbt                → Application build scriptExample of a microserviceIn order to write a new microservice you create a new API and implementation project.In the API project you define the interface of your microservice:HelloService.javapublic interface HelloService extends Service {  ServiceCall&lt;String, NotUsed, String&gt; hello();    ServiceCall&lt;String, GreetingMessage, String&gt; useGreeting();  @Override  default Descriptor descriptor() {    return named(\"helloservice\").with(        restCall(Method.GET,  \"/api/hello/:id\",       hello()),        restCall(Method.POST, \"/api/hello/:id\",       useGreeting())      ).withAutoAcl(true);  }}A Descriptor defines the service name and the endpoints offered by a service. In our case we define two REST endpoints, a GET and a POST.GreetingMessage is basically an immutable class with a single String message instance variable.On the subject of immutability the Lagom documentation mentions Immutables, a Java library that helps you create immutable objects via annotation processing.Definitely worth a look seeing as it helps you get rid of boilerplate code.In the implementation submodule we implement our API’s interface.HelloServiceImpl.javapublic class HelloServiceImpl implements HelloService {  @Override  public ServiceCall&lt;String, NotUsed, String&gt; hello() {    return (id, request) -&gt; {      CompletableFuture.completedFuture(\"Hello, \" + id);    };  }  @Override  public ServiceCall&lt;String, GreetingMessage, String&gt; useGreeting() {    return (id, request) -&gt; {      CompletableFuture.completedFuture(request.message + id);    };  }}You’ll immediately notice that the service calls are non-blocking by default using CompletableFutures introduced in JDK8.Interesting to know is that Lagom also provides support for the Publish-subscribe pattern out of the box.We also need to implement the module that binds the HelloService so that it can be served.HelloServiceModule.javapublic class HelloServiceModule extends AbstractModule implements ServiceGuiceSupport {  @Override  protected void configure() {    bindServices(serviceBinding(HelloService.class, HelloServiceImpl.class));  }}We define our module in the application.config:play.modules.enabled += sample.helloworld.impl.HelloServiceModuleAnd finally register our microservice in build.sbt with its dependencies and settings:lazy val helloworldApi = project(\"helloworld-api\")  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies += lagomJavadslApi  )lazy val helloworldImpl = project(\"helloworld-impl\")  .enablePlugins(LagomJava)  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies ++= Seq(      lagomJavadslPersistence,      lagomJavadslTestKit    )  )  .settings(lagomForkedTestSettings: _*)  .dependsOn(helloworldApi)We can then test our endpoint:$ curl localhost:24266/api/hello/WorldHello, World!$ curl -H \"Content-Type: application/json\" -X POST -d '{\"message\": \"Hello \"}' http://localhost:24266/api/hello/WorldHello WorldSeeing as any good developer writes unit tests for his/her code so should we!public class HelloServiceTest {  private static ServiceTest.TestServer server;  @BeforeClass  public static void setUp() {    server = ServiceTest.startServer(ServiceTest.defaultSetup());  }  @AfterClass  public static void tearDown() {    if (server != null) {      server.stop();      server = null;    }  }  @Test  public void shouldRespondHello() throws Exception {    // given    HelloService service = server.client(HelloService.class);    // when    String hello = service.hello().invoke(\"Yannick\", NotUsed.getInstance()).toCompletableFuture().get(5, SECONDS);    // then    assertEquals(\"Hello, Yannick\", hello);  }  @Test  public void shouldRespondGreeting() throws Exception {    // given    HelloService service = server.client(HelloService.class);    // when    String greeting = service.useGreeting().invoke(\"Yannick\", new GreetingMessage(\"Hi there, \")).toCompletableFuture().get(5, SECONDS);    // then    assertEquals(\"Hi there, Yannick\", greeting);  }}Tests can be executed in Activator via the following command:$ test&gt; test[info] Test run started[info] Test sample.helloworld.impl.HelloServiceTest.testHello started[info] Test sample.helloworld.impl.HelloServiceTest.testGreeting started[info] Test run finished: 0 failed, 0 ignored, 2 total, 16.759s[info] Passed: Total 2, Failed 0, Errors 0, Passed 2[success] Total time: 21 s, completed Apr 14, 2016 10:06:41 AMCQRS and Event SourcingBeing an opinionated framework Lagom suggests to use CQRS and Event Sourcing seeing as it fits well within the reactive paradigm.In this blogpost we are not going to explain CQRS and Event Sourcing in detail seeing as it is very well documented in the documentation of Lagom.The gist of it is that each service should own its own data and only the service itself should have direct access to the database.Other services need to use the service’s API in order to interact with its data.Sharing the database across different services would result in tight coupling.Ideally we want to work with Bounded Contexts following the core principles of Domain-Driven Design where each service defines a Bounded Context.Using Event Sourcing gives us many advantages such as not only storing the current state of data but having an entire journal of events that tell us how the data achieved its current state.With event sourcing we only perform reads and writes, there are no updates nor deletes.All this makes it easy to test and debug and allows us to easily reproduce scenarios that happened in production by replaying the event log from that environment.Note that just because Lagom encourages us to use CQRS and Event Sourcing it isn’t forcing us to use it as it is not always applicable to every use case.It is perfectly possible to, for example, plug in a PostgreSQL database for our persistence layer.Someone has already set up PostgreSQL integration using Revenj persistence.However, Lightbend suggests that for best scalability preference must be given to asynchronous APIs because using blocking APIs like JDBC and JPA will have an impact on that.By default, when launching our development environment, a Cassandra server will be booted without having to do any setup ourselves besides adding the lagomJavadslPersistence dependency to our implementation in our build.sbt.Regarding the code, a persistent entity needs to be defined, combined with a related command, event and state.Note that the following code samples are mainly here to give an idea of the work needed for implementing all this.For more information and a detailed explanation, consult the excellent documentation on the subject.In the persistent entity we define the behaviour of our entity.In order to interact with event sourced entities, commands need to be sent.We therefore need to specify a command handler for each command class that the entity can receive.Commands are then translated into events which will get persisted by the entity.Each event has its own event handler registered.Example of a PersistentEntity:HelloWorld.javapublic class HelloWorld extends PersistentEntity&lt;HelloCommand, HelloEvent, WorldState&gt; {  @Override  public Behavior initialBehavior(Optional&lt;WorldState&gt; snapshotState) {    BehaviorBuilder b = newBehaviorBuilder(        snapshotState.orElse(new WorldState(\"Hello\", LocalDateTime.now().toString())));    b.setCommandHandler(UseGreetingMessage.class, (cmd, ctx) -&gt;      ctx.thenPersist(new GreetingMessageChanged(cmd.message),        evt -&gt; ctx.reply(Done.getInstance())));    b.setEventHandler(GreetingMessageChanged.class,        evt -&gt; new WorldState(evt.message, LocalDateTime.now().toString()));    b.setReadOnlyCommandHandler(Hello.class,        (cmd, ctx) -&gt; ctx.reply(state().message + \", \" + cmd.name + \"!\"));    return b.build();  }}Our PersistentEntity requires a state to be defined:WorldState.java@Immutable@JsonDeserializepublic final class WorldState implements CompressedJsonable {  public final String message;  public final String timestamp;  @JsonCreator  public WorldState(String message, String timestamp) {    this.message = Preconditions.checkNotNull(message, \"message\");    this.timestamp = Preconditions.checkNotNull(timestamp, \"timestamp\");  }  @Override  public boolean equals(@Nullable Object another) {    if (this == another)      return true;    return another instanceof WorldState &amp;&amp; equalTo((WorldState) another);  }  private boolean equalTo(WorldState another) {    return message.equals(another.message) &amp;&amp; timestamp.equals(another.timestamp);  }  @Override  public int hashCode() {    int h = 31;    h = h * 17 + message.hashCode();    h = h * 17 + timestamp.hashCode();    return h;  }  @Override  public String toString() {    return MoreObjects.toStringHelper(\"WorldState\").add(\"message\", message).add(\"timestamp\", timestamp).toString();  }In our command interface we define all the commands that our entity supports.In order to get a complete picture of the commands an entity supports, it is the convention to specify all supported commands as inner classes of the interface.HelloCommand.javapublic interface HelloCommand extends Jsonable {  @Immutable  @JsonDeserialize  public final class UseGreetingMessage implements HelloCommand, CompressedJsonable, PersistentEntity.ReplyType&lt;Done&gt; {    public final String message;    @JsonCreator    public UseGreetingMessage(String message) {      this.message = Preconditions.checkNotNull(message, \"message\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof UseGreetingMessage &amp;&amp; equalTo((UseGreetingMessage) another);    }    private boolean equalTo(UseGreetingMessage another) {      return message.equals(another.message);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + message.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"UseGreetingMessage\").add(\"message\", message).toString();    }  }  @Immutable  @JsonDeserialize  public final class Hello implements HelloCommand, PersistentEntity.ReplyType&lt;String&gt; {    public final String name;    public final Optional&lt;String&gt; organization;    @JsonCreator    public Hello(String name, Optional&lt;String&gt; organization) {      this.name = Preconditions.checkNotNull(name, \"name\");      this.organization = Preconditions.checkNotNull(organization, \"organization\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof Hello &amp;&amp; equalTo((Hello) another);    }    private boolean equalTo(Hello another) {      return name.equals(another.name) &amp;&amp; organization.equals(another.organization);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + name.hashCode();      h = h * 17 + organization.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"Hello\").add(\"name\", name).add(\"organization\", organization).toString();    }  }}And finally we want to define all events that the entity supports in an event interface.It follows the same convention as with commands, specifying all events as inner classes of the interface.HelloEvent.javapublic interface HelloEvent extends Jsonable {  @Immutable  @JsonDeserialize  public final class GreetingMessageChanged implements HelloEvent {    public final String message;    @JsonCreator    public GreetingMessageChanged(String message) {      this.message = Preconditions.checkNotNull(message, \"message\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof GreetingMessageChanged &amp;&amp; equalTo((GreetingMessageChanged) another);    }    private boolean equalTo(GreetingMessageChanged another) {      return message.equals(another.message);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + message.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"GreetingMessageChanged\").add(\"message\", message).toString();    }  }}The HelloServiceImpl.java class will look like the following:public class HelloServiceImpl implements HelloService {  private final PersistentEntityRegistry persistentEntityRegistry;  @Inject  public HelloServiceImpl(PersistentEntityRegistry persistentEntityRegistry) {    this.persistentEntityRegistry = persistentEntityRegistry;    persistentEntityRegistry.register(HelloWorld.class);  }  @Override  public ServiceCall&lt;String, NotUsed, String&gt; hello() {    return (id, request) -&gt; {      PersistentEntityRef&lt;HelloCommand&gt; ref = persistentEntityRegistry.refFor(HelloWorld.class, id);      return ref.ask(new Hello(id, Optional.empty()));    };  }  @Override  public ServiceCall&lt;String, GreetingMessage, Done&gt; useGreeting() {    return (id, request) -&gt; {       PersistentEntityRef&lt;HelloCommand&gt; ref = persistentEntityRegistry.refFor(HelloWorld.class, id);       return ref.ask(new UseGreetingMessage(request.message));    };  }}Lightbend Q&amp;A at the CodeStar launch eventOn the 24th of March we attended the launch event of CodeStar, the new unit from our Dutch Ordina colleagues focused on Full Stack Scala and Big Data solutions.CodeStar also hold a Lightbend partnership.One of the presentations was an introduction to Lagom by Markus Eisele, Developer Advocate at Lightbend.After his talk we had the opportunity to ask Markus and his colleague, Lutz Hühnken, Solutions Architect at Lightbend, several questions regarding Lagom.      What do you guys consider to be the major competitor for Lagom?Spring Cloud and Netflix OSS?          Yes, we would consider that stack to be Lagom’s main competitor. But we believe that with Lagom we have a number of unique features that makes us shine (because otherwise we wouldn’t have built it):1) Lagom’s development environment, in my humble opinion a major productivity boost2) Fostering good practices for building reactive services seeing as Lagom is opinionated, e.g. async communication by default, ES/CQRS, …3) Batteries-included, from development to production4) Streaming is first-class            Lagom suggests that Event Sourcing and CQRS should be used as the default solution for persistence but is it really applicable in the majority of the scenarios?          Lagom is an opinionated framework and will try to suggest using ES &amp; CQRS as the primary solution to use since it fits very well with the reactive mindset.Of course it also depends on the use case.            Don’t you think you encourage code coupling by having microservices depend on the interface of another microservice?          It is true that the default way to do service calls between Lagom services is to use binary dependencies, though of course it is not enforced. We have taken great care to ensure that service calls map down to idiomatic REST and/or websockets. We do have plans in the future to allow simple removal of the binary coupling. To make service interfaces go through a non-binary specification such as Swagger, where Swagger specs will be generated and interfaces will be generated from the Swagger specs.            Does Lagom support REST level 3? Is there support for hypermedia?          Currently not supported but we are open to it. Feel free to create a suggestions ticket at the GitHub project.            Don’t you think it is a bad idea to only support ConductR for production deployments?What about pet projects of single developers? This makes it less appealing to motivate people to pick up Lagom compared to for example Spring Cloud and Netflix OSS.          It is in the strategic planning of Lightbend to push ConductR forward as the main solution for your production environment.Do note that it’s perfectly possible to deploy your Lagom services elsewhere as long as you implement your own service locator (as an example, the integration needed to support Lagom in ConductR is available on GitHub).Looking at our Open Source Position Statement you will notice that one of the differentiators we see between our open source offerings and the commercial products is Time. Open source users tend to invest their time rather than their money. ConductR integration into Lagom could be seen as an example for this. If you would rather spend the money than invest time, buy ConductR. If you would rather invest time instead of money, build your own ServiceLocator implementation and use a different infrastructure.An example of this is the GitHub issue for implementing Kubernetes support.            How do you integrate with other non-Lagom microservices?          Currently you would call them via REST URLs. In the near future the Lagom Service Client could also be used to consume them. Additionally it should also be possible to integrate Eureka in Lagom.            What is the deployment procedure exactly? How do I prepare my Lagom application for deployment into production?          The deployment unit in ConductR is a bundle which is an abstract term that can mean a Docker image or a zip file with a certain structure.By default, when you have multiple services in one project, it will create multiple bundles. You call bundle:dist once on the top level and it will create a separate bundle for each service which can then be deployed to ConductR.You can put multiple components in one bundle so you could have multiple services in one bundle, but we think that it is unusual.Ideally, each service needs to be its own bundle managed in isolation by ConductR, for it to be able to be able to be developed, rolled out, upgraded and failed in isolation.            What about API versioning?          Currently there is no versioning for your services besides the “default” way to do it, e.g. via the header or by versioning your urls.            What do you think about the so-called serverless architectures like AWS Lambda or Google Cloud Functions?          We think that those architectures are part of the future. Lagom can be seen as a step in that direction since it decouples the stateless part of the service (the behavior) from the stateful (persistent entity), allowing the stateless part to be scaled out independently, and automatically by the runtime, in a similar fashion to AWS Lambda. A hosted version of Lagom could give a very similar experience.            About sbt, will you also support a more widely adopted tool such as Maven or Gradle?          Lagom relies on some sbt features, so supporting other build tools is not trivial. While it is probably doable to support Maven, we’d need to do build a proof-of-concept to verify this. This is currently not prioritized. We’ll be watching the community’s feedback on this.            Does the Lagom circuit breaker have a dashboard such as the Hystrix dashboard? Does Lagom in general have operational dashboards?          You could integrate the circuit breaker data with monitoring tools such as Graphite and Grafana.In addition, with Lightbend Monitoring you do get a suite of tools for monitoring your microservices. Lightbend Monitoring is included in the ConductR license.            Is it true that Typesafe rebranded to Lightbend to get a broader adoption than what was possible with a more Scala-orientated reputation attached to Typesafe?          That is correct.This doesn’t mean that we are giving up on Scala, it is still core to all of our technologies.      Comparison with SpringSpring has been out there for more than 10 years and with Spring Boot and Spring Cloud a trend has been set to move to self-contained applications as a basis for microservices development.Spring reaps the fruits of the Netflix OSS while offering Spring’s own components such as Spring Cloud Config and Spring Security as well.The Netflix and Spring stack comes with all the necessary tools to build and run microservices in production.Externalized configuration, out-of-the-box free dashboards for service registries, circuit breaker monitoring and distributed tracing, integration with service registries such as Eureka, Consul and Zookeeper, production-ready monitoring and metrics features with Actuator endpoints, integration with build tools such as Maven and Gradle and extensive security features including upcoming integration with Vault are only a subset of the features Spring has to offer.Seeing as Lagom is still in its early days, it wouldn’t be fair to Lightbend to make an in-depth comparison with the Spring stack.We hope that Lagom will continue to grow towards a more mature framework and a true alternative to Spring on all levels.The first steps we currently see look promising and we hope that they will consider our remarks for how they want to further evolve the framework.It is great to see more microservices frameworks become available and we applaud Lightbend for taking up the competition with Spring.Our adviceOur advice is to keep track of Lagom’s progress closely.If you are currently looking for a mature framework with integration capabilities for just about anything, go with Spring.If you want to use Event Sourcing, Lagom should be a great fit. Additionally, Lagom’s focus on CQRS and its reactive core are truly differentiators with other frameworks.Lagom has great potential and is eager to get community involvement. If you are willing to join forces with Lightbend, Lagom might already be a viable candidate for you.ConclusionWe think that Lagom looks very promising and we will definitely follow it up.Due to Lagom being an opinionated framework everything glues together well.Lagom is just a thin layer on top of Akka and Play, which is very mature and hardened over the years.It might be a bit too early to do an in-depth comparison between Lagom and Spring Cloud since we would be comparing an MVP against a mature technology.We do think that using sbt might be a hurdle for Java developers and it would ease adoption if there would be other ways to use Lagom in production besides ConductR.As it stands right now you would need to write a custom service locator yourself.It would close the gap with Spring if support would already be available for service discovery via for example Eureka or Consul.It is clear that Lagom puts a lot of focus on reactiveness and gaining the best performance. This could come at the cost of binary coupling, seeing as the default way to do service calls between Lagom services is to use binary dependencies. We are looking forward to Lightbend’s plans to go through non-binary specifications in order to reduce coupling on a binary level as well.Given that it is currently an MVP version we are interested in seeing how Lagom matures. Since it is all new and shiny, you will be able to give back to the community by helping to develop parts of this new and exciting framework yourself.Contributing to the framework is easy via pull requests and are actively reviewed by Lightbend developers.The developers are very active on their Gitter channel and they are quick to answer questions. We are also very excited to the release of the Scala API.Our colleague Andreas Evers, who has extensive knowledge on Spring Cloud and Netflix OSS, will soon be participating in a podcast with Markus Eisele hosted by Lightbend to discuss Lagom and microservices trends.The date should be announced soon.Be sure to follow Andreas and Lightbend to catch it!Useful links  Lagom  Lagom documentation  Lagom Twitter  Lagom GitHub  Lagom Gitter  Lightbend  Lightbend Twitter  Markus Eisele Twitter  Lutz Hühnken Twitter"
      },
    
      "microservices-2016-04-09-ordina-becomes-netflix-oss-contributor-html": {
        "title": "Ordina becomes Netflix OSS contributor",
        "url": "/microservices/2016/04/09/Ordina-becomes-Netflix-OSS-contributor.html",
        "image": "/img/netflix.jpg",
        "date": "09 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Netflix has officially added Ordina as active user and contributor to their open source cloud and microservices tools and frameworks.Ordina continues to be a leading force in cloud and microservice architectures in the BeNeLux.Since 2011, Netflix has been releasing more and more components of their cloud platform and utilities as free and open source software. These projects are available to the public through Netflix OSS.Starting around 2009, Netflix completely redefined its application development and operations models. They were driven completely by APIs and riding the initial wave of what we would come to know as microservices. Industry onlookers derided the company with disbelief and uncertainty. While this may work with Netflix, no one else can possibly do this. Fast-forward to 2016, when most of those sentiments changed to commitments of active migration to microservices. The concept is definitely both valid and powerful.Pivotal has embraced these technologies and made them approachable for the masses through the Java-based Spring ecosystem. In the Spring Cloud OSS program, an abstraction layer is added on top of Netflix’s components to ease adoption outside of Netflix. This allows local companies without silicon valley-grade scientists to embrace microservices and their benefits.Ordina has successfully architected and implemented the Netflix and Spring Cloud stack at clients in Belgium. We continue to do so and are proud we can call ourselves Netflix OSS and Spring Cloud contributors. Netflix has added our logo to their contributors page for our continued adoption and contributions to the Netflix and Spring Cloud ecosystem.Our expertise and experience using and contributing to the Netflix and Spring Cloud stack is second to none in the BeNeLux. We are always looking to help new or existing clients to migrate to microservices and make the step to Cloud Native architectures.If this sounds interesting to your company, or these architectures personally excite you, make sure to contact us directly or take a look at jobs.ordina.be."
      },
    
      "ionic-2016-04-07-ionic-protractor-html": {
        "title": "Protractor testing in Ionic app",
        "url": "/ionic/2016/04/07/Ionic-Protractor.html",
        "image": "/img/ionic-protractor.png",
        "date": "07 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Using Protractor in an ionic appSince a few days I’ve been playing around with Protractorand I am also involved on an internal project in which an Ionic app has to be created.So I thought:  Why not use Protractor in my Ionic app?So here we are.It’s not hard to get started and I will explain how I got it working.For the full example please refer to the ionic documentation1. Getting startedFirst of all you need Node.js.When Node.js is installed you should install Ionic and Cordova using npm (Node Package Manager):npm install -g cordova ionicI personally needed an app with tabs, but you can also start a blank app or an app with a side menu:ionic start &lt;your app name&gt; tabsYou can also choose blank or sidemenu instead of tabs2. Running the app in the browserOnce the Ionic app has been generated, you can run t it in the browser using the following command:cd &lt;your app name&gt;ionic serve  It’s worth noting that Ionic has live reload built in by default.So any changes will be immediatelyreflected in the browser.To view the application using the iOS and Android styling applied you can use the following command: ionic serve -lab There are many more awesome things you can do with the Ionic CLI.If you want to know more about the CLI you can find it in the Ionic documentation.3. Structuring the applicationAt this moment you are set up with an Ionic starter app.The first thing I did was refactor the code from technical to functional modules.                    --&gt;                  I strongly advise to use functional modules, it’s easier to work with.Related code should be in one folder and when testing you can use the same structure to test each module separately while coding.  You’ll find yourself navigating less trough your open tabs or a tree-view.Additionally, it makes your code more comprehensible to other developers.The style guide from John Papa on how to structure AngularJS applications is a very good resource.4. Sign-in pageThe one we’ll be testingAfter refactoring, I implemented a sign-in page, which has no access to the tabs.The code can be seen below.If you work in functional modules like I do, it is as easy as referring to the controller and the service from index.html, then pass starter.sign-in as a module to your application.sign-in/sign-in-controller.js:(function () {    angular        .module('starter.sign-in',[])        .controller('SignInCtrl',SignInCtrl);    SignInCtrl.$inject = ['$scope','$state','SignInService'];    function SignInCtrl ($scope,$state,SignInService) {        $scope.user = {};        $scope.signIn = function() {            SignInService.signIn($scope.user)                .then(function(data){                    if(data){                        $state.go('tab.rooms');                    }else{                        $scope.incorrect = true;                    }                })        };    }})();sign-in/signin.html:&lt;ion-view view-title=\"Sign in\"&gt;    &lt;ion-content&gt;        &lt;form name=\"frmLogin\" novalidate ng-cloak&gt;            &lt;ion-list&gt;                &lt;ion-item class=\"item item-input item-floating-label\"&gt;                    &lt;label&gt;                        &lt;span class=\"input-label\"&gt;Username&lt;/span&gt;                        &lt;input type=\"email\" ng-model=\"user.username\" ng-required=\"true\" placeholder=\"Username\" id=\"username\" name=\"username\"&gt;                    &lt;/label&gt;                    &lt;div class=\"assertive\" ng-if=\"frmLogin.$submitted || frmLogin.username.$touched\"&gt;                        &lt;div ng-if=\"frmLogin.username.$error.required\"&gt;Username is required&lt;/div&gt;                        &lt;div ng-if=\"frmLogin.username.$error.email\"&gt;Username is not valid&lt;/div&gt;                    &lt;/div&gt;                &lt;/ion-item&gt;                &lt;ion-item class=\"item item-input item-floating-label\"&gt;                    &lt;label&gt;                        &lt;span class=\"input-label\"&gt;Password&lt;/span&gt;                        &lt;input type=\"password\" ng-model=\"user.password\" ng-minlength=\"4\" ng-required=\"true\" placeholder=\"Password\" id=\"password\" name=\"password\"&gt;                    &lt;/label&gt;                    &lt;div class=\"assertive\" ng-if=\"frmLogin.$submitted || frmLogin.password.$touched\"&gt;                        &lt;div ng-show=\"frmLogin.password.$error.required\"&gt;Password is required&lt;/div&gt;                    &lt;/div&gt;                &lt;/ion-item&gt;                &lt;div class=\"padding\"&gt;                    &lt;div class=\"assertive\" ng-if=\"incorrect\"&gt;                        &lt;div&gt;Username or password is incorrect.&lt;/div&gt;                    &lt;/div&gt;                    &lt;button id=\"btnSignIn\" ng-disabled=\"frmLogin.$invalid\" class=\"button button-full button-positive\" ng-click=\"signIn()\"&gt;                        Sign in                    &lt;/button&gt;                &lt;/div&gt;            &lt;/ion-list&gt;        &lt;/form&gt;    &lt;/ion-content&gt;&lt;/ion-view&gt;sign-in/sign-in.service.js:(function() {    angular        .module('starter.sign-in')        .factory('SignInService', SignInService);    SignInService.$inject = ['$timeout'];    function SignInService ($timeout) {        var _user = {            email: 'yannick@gmail.com',            pass: '1234'        };        function signIn (user) {            return $timeout(function() {                return true;                //return !!(user.username === _user.email &amp;&amp; user.password == _user.pass);            },2000);        }        return {            signIn : signIn        };    }})();Next you need to provide a state, so add the following in app.js:state('signin',{    url:'/signin',    templateUrl: 'sign-in/sign-in.html',    controller: 'SignCtrl')}and change redirect to /signin by default:$urlRouterProvider.otherwise('/signin');For complete authentication you should check the authenticated state when changing pages,but that’s not in the scope of this blog5. Preparing protractorThe sign-in part is the one I am going to test with Protractor.First thing to do, is to install Protractor on your system:npm install -g protractorThe webdriver manager is a helper tool to easily get a Selenium server running.Run the following commands in order to start it:webdriver-manager updatewebdriver-manager startTo keep your code clean, you could put tests in a dedicated folder, but many argue against it.  Since I work in functional modules, tests of these modules should live in the module itself.Next I created a Protractor configuration file in the root of my project called protractor.config.js:touch protractor.config.jsprotractor.config.js:exports.config = {    capabilities: {        'browserName': 'chrome'    },    specs: [        'www/sign-in/sign-in.spec.js',    ],    jasmineNodeOpts: {        showColors: true,        defaultTimeoutInterval: 30000,        isVerbose: true    },    allScriptsTimeout: 20000,    onPrepare: function(){        browser.driver.get('http://localhost:8100');    }};  Don’t forget to set the correct URL to your running app.If not, you’ll see many errors, except that you might be referring to a wrong URL6. Preparing the testsAs you can see, there is already a spec file defined in the protractor config file, so let’s create it:cd www/sign-in/touch sign-in.spec.jsIn the newly created file, you can start writing your tests.If everything went well, you can simply add another test and it should validate to true.It only tests if the first page you see, is the login page:describe('Signing in', function(){    it('should start on sign-in view', function(){        expect(browser.getTitle()).toEqual('Sign in');    });});Basically, we define a describe function which will describe the whole scope of our specs.Every ‘it’ function is called a spec.We only created one for now.As you can see this is a very readable way of testing.We expect the browsers title to be equal to ‘Sign in’.If the expect statement evaluates to true, the spec has passed without failures, otherwise it will have a failure.  Feel free to change Sign in to something else to fail the test.To run the tests, we can execute the following command in the folder of our protractor.config.js file:protractor protractor.config.jsRunning this command will read the config file and run all the spec-files defined.You will get some output in the command line.At the end you’ll get a summary like 1 specs, 0 failures Finished in x.xxx seconds.This is a simple test but it doesn’t show the full potential of Protractor at all.Lets add a new spec as part of the describe.it('should be unable to click Sign-in button when fields are empty', function(){    var button = element(by.id('btnSignIn'));    expect(button.getAttribute('disabled')).toEqual('true');});So here we test the availability of the sign-in button when the fields are empty.Next is to test if the button becomes available if the fields are filled in with valid data.So lets add another test:it('should be possible to click Sign-in button when fields are filled in', function(){    var button = element(by.id('btnSignIn'));    var txtUsername = element(by.id('username'));    var txtPassword = element(by.id('password'));    txtUsername.sendKeys('yannick@gmail.com');    txtPassword.sendKeys('1234');    expect(button.getAttribute('disabled')).toBe(null);});All these tests should pass correctly in protractor.7. Page Object PatternYou might have noticed that your tests run synchronous after each other.In this scenario this might be useful, but sometimes you need to start with a ‘clean page’ which would mean you need to duplicate a lot of code (for finding the button and text-fields).  When you are working in an agile team, it is quite common that requirements or user stories change.This can implicate you’ll have to change a lot of duplicated code.How can we work around that.The solution is called the page object pattern.The general idea is to put your page in a JavaScript object.Lets dive into sign-in.page.js.This file should also be put into the module folder:var SignInPage = function () {    browser.get('http://localhost:8100/signin');};SignInPage.prototype = Object.create({}, {    txtUsername: { get: function () { return element(by.id('username')); }},    txtPassword: { get: function () { return element(by.id('password')); }},    btnSignIn: { get: function () { return element(by.id('btnSignIn')); }},    typeUsername: {value: function (keys) { return this.txtUsername.sendKeys(keys); }},    typePassword: {value: function (keys) { return this.txtPassword.sendKeys(keys); }},    clickSignIn: {value: function (keys) { return this.btnSignIn.click(); }}});module.exports = SignInPage;In the constructor we make sure our browser opens the signin page by passing the correct URL.Then we use the prototype method to link our HTML elements with the object.Finally, it is wise to create helper methods for basic functionality, such as filling in a username, in case you ever would want to change that behaviour.Then you only need to change that line and all your tests will still pass.  Using logical method names keeps your tests readable which is what you’ll want when you look back in a few months.We can now change our sign-in.spec.js to this:var SignInPage = require('./sign-in.page.js');describe('Signing in', function(){    var page;    beforeEach(function () {        page = new SignInPage();    });    it('should be unable to click Sign-in button when fields are empty', function(){        expect(page.txtUsername.getText()).toEqual('');        expect(page.txtPassword.getText()).toEqual('');        expect(page.btnSignIn.getAttribute('disabled')).toEqual('true');    });    it('should be possible to click Sign-in button when fields are filled in', function(){        page.typeUsername('yannick@gmail.com');        page.typePassword('1234');        expect(page.btnSignIn.getAttribute('disabled')).toBe(null);        page.clickSignIn();        expect(browser.getTitle()).toEqual('Rooms');    });});What changed?We created a page variable and before each it we assigned a new SignInPage object to the page variable.This way, your page gets loaded again before running every spec.This means it always returns in the same state.Now you can create your specs as user stories.8. ConclusionProtractor is an awesome way to test your app’s functionality.Using a descriptive syntax you can emulate almost every user action and run trough the whole app in no time, again and again.Using Protractor, you won’t have to spend a lot of time testing your application manually,and you can focus on feature development without having to worry about accidentally breaking some functionality.Protractor will ensure that your user gets a working app without frustrations!"
      },
    
      "ionic-2016-03-31-adding-typescript-to-ionic-framework-html": {
        "title": "Adding TypeScript to Ionic Framework",
        "url": "/ionic/2016/03/31/Adding-TypeScript-to-Ionic-Framework.html",
        "image": "/img/ionic-and-typescript.jpg",
        "date": "31 Mar 2016",
        "category": "post, blog post, blog",
        "content": "Ionic and TypeScript sitting in a treeSo, TypeScript is the all new thing that allows you to use features from ES6 (or ES2015), ES7 and beyond.Say goodbye to loosely typed variables and say hello to modules, classes, interfaces and so much more.In order to use TypeScript in an Ionic Framework project there are a few small things you need to do to get things running.1. Install and configure the gulp package  Install the gulp-tsc package and save it to the development dependencies in package.jsonnpm install gulp-tsc --save-dev  Next, require the package in your gulpfile.js like sovar typescript = require('gulp-tsc');      Add the following line to the paths object: ts: ['./src/*.ts', './src/**/*.ts'].You may have noticed two things here: All my TypeScript files are in a src folder which means I’m not using the www folder that Ionic provides by default.This way I can keep the TypeScript files and JavaScript files separated.Next to that I’m also targeting subfolders in that folder because I’m bundling my logic based on AngularJS modules.You can read more about structuring an AngularJS project in the John Papa AngularJS Style Guide.        Add our compile task  gulp.task('compile', function(){    gulp.src(paths.ts)        .pipe(typescript({ emitError: false }))        .pipe(gulp.dest('www/'));});  Add our task to watchgulp.task('watch', function () {    gulp.watch(paths.sass, ['sass']);    gulp.watch(paths.ts, ['compile']);});  Now change the ionic.project file and add the compile task to the gulpStartupTasks. If the gulpStartupTasks section is not present at all, just add it anyway.\"gulpStartupTasks\": [    \"sass\",    \"compile\",    \"watch\"]2. Add TSDTSD is a TypeScript Definition manager for DefinitelyTyped.TypeScript used TypeScript Definition files so it knows how to handle the TypeScript you are writing and gives you intellisense.Let’s install TSD so we can continue.$ sudo npm install -g tsd$ tsd install ionic cordova --saveThis will create a typings folder which contains a tsd.d.ts file with references to the typings needed for ionic and cordova.In the root of your project a tsd.json file will be created with all the installed definitions.All you need to do to use the typings in your TypeScript file is include it at the top like so:/// &lt;reference path=\"../typings/tsd.d.ts\" /&gt;Note: TSD has been deprecated in favour of Typings to manage and install TypeScript definitions.More info on how to switch from TSD to Typings can be found here.3. Prevent editor from compiling on saveNow to prevent your editor to auto compile TypeScript we add a tsconfig.json file to the src folder with this in it:{    \"compileOnSave\": false}4. Add TypeScript files in src folderNow that we have everything set up it’s time to start refactoring your application.It’s important to know that every JavaScript file is essentially TypeScript because TypeScript is a superset of the current JavaScript implementation.This basically means that you can take your JavaScript files from your www folder, paste them to the src folder and rename them from file.js to file.ts.Of course don’t forget to add the reference on top of your files to the tsd.d.ts file in the typings folder.If you now run ionic serve, you should see a message that looks like this one. “Compiling TypeScript files using tsc version x.x.x”.TypeScript will process these files and write ES5 files to the www folder.ConclusionAs you can see it is fairly simple - just 4 steps - to add TypeScript support to your Ionic project by changing the default gulp setup used by Ionic.It’s nice to know that Ionic 2 will have support for TypeScript built in so you won’t have to do it yourself.By adding a flag --ts to your Ionic 2 project setup it will be enabled.Personally I love using TypeScript and will use it whenever I can.It makes my life as a developer a lot easier by spotting errors before I even hit the browser.What are your thoughts about TypeScript? Feel free to add them in the comments section."
      },
    
      "angular-2016-03-16-angularts-html": {
        "title": "AngularTS: A new look @ Angular",
        "url": "/angular/2016/03/16/AngularTS.html",
        "image": "/img/angularts.jpg",
        "date": "16 Mar 2016",
        "category": "post, blog post, blog",
        "content": "Combining the best of two worlds.Since my introduction to the heroic AngularJS framework at Devoxx around 4 years ago, I was intrigued and set for an adventure.With the upcoming release of Angular 2 we have to prepare ourselves with the migrating road map coming up.One of the core changes in Angular 2 is the focus on using TypeScript.This post will cover the use of Angular components in TypeScript.But what is it?TypeScript is a superset of JavaScript that focuses on strong typing and new ES6 features: classes, interfaces and modules.Like in common object-oriented languages such as Java and C# these features aren’t new.These features give the developer the opportunity to build an object-oriented architecture in JavaScript.With that in mind, let’s see what the advantages are:TranspilingThe DOM can only recognize JavaScript.With this said they had to come up with a way to compile TS (TypeScript).Because TS is a superset of JS it can transpile to plain JavaScript before including it into HTML.Transpilers are integrated in the latest IDE’s. Any valid JavaScript is valid TypeScript.Strongly typedWhen you’re used to plain JavaScript, you notice that every time you need a variable, it is loosely typed.TypeScript gives you the opportunity for each of your variables to have its own type.This comes with great benefits like better refactoring, less bugs and better type checking at compile time.OO architectureTypeScript offers an object-oriented architecture experience, which means all code is defined in classes, interfaces and most of those classes can be instantiated into objects. It also supports encapsulation, which protects the data from unintended access and modification.Learning path of AngularTSIf you’re no stranger to AngularJS you will notice that the structure remains the same. Two way data binding, controllers, services, … But be aware that it has a different syntax in TypeScript. I will show you the different best practices to implement such components.TypeScript Definition FilesWhen using TS we will refer to TSD files.These files describe the types defined in external libraries such as Angular. To install the Angular TSD files we use typings.To use the typings manager we install it with:$ npm install typings --globalAfterwards install Angular with:$ typings install angular --ambient --save--ambient --save enables the flag and persists the selection in ‘typings.json’All the installed TSD files are gathered in the typings folder.In the main.d.ts file you will see the references the application will use for Angular.Since Angular has multiple libraries, you can use the search command to find the required definition.$ typings search AngularIt is possible that you have to declare the reference on top of your file./// &lt;reference path=\"../../typings/main.d.ts\" /&gt;ModulesAngular ModulesModules are here to help us modularize our code.It is a best practice to use one main module as the root of your application. Multiple modules are being used for third-party libraries or common code.To let the module know the existence of every component, they have to register themselves.Below every component declaration you will see a registration to the module. When registering the module you have to add all the libraries you want to depend upon.In this example we inject the routing service for navigation.module JWorks {    \"use strict\";    angular       .module(\"jworks360\", ['ngRoute'])}Internal TypeScript ModulesThese modules are similar to namespaces.You can define an unique namespace around your code.This will encapsulate variables, interfaces and classes. TypeScript supports sub namespaces for further encapsulation.module JWorks {    \"use strict\";}Transpiled JavaScriptvar JWorks;(function(JWorks){})(JWorks || (JWorks = {}));To encapsulate our code, the module will transpile to an IIFE (Immediately-Invoked Function Expression) around our components.This will avoid global code which helps prevent variables and function declarations from living longer than expected in the global scope, which also helps avoid variable collisions.Entity ClassNow that TypeScript supports object-oriented programming, we can analyse our business problem and define the business objects into entity classes.When you analyse and define these entities you can define which properties and methods each entity needs.If you have a couple entities, you can even establish a relationship.This will provide a clear view on what you want to achieve and have the possibility to create multiple instances of these classes. When building an entity class you can optionally define an interface to show what the intention of the class is.module JWorks {    export interface IEmployee {         username:string;         name:string;         eat(food:string):void    }    export class Employee implements IEmployee {           constructor(public username:string, public name:string){}          eat(food:string):void{             //implementation          }    }}To use your entity class in a controller you have to define the export key.This will expose the class to other classes.When exporting the interface you will use it as a data type.Class as propertyEmployee:IEmployee;Instance of the classemployee = new Employee();Access propertyemployee.nameCall methodsemployee.eat(eggs);ControllersAs you know the controller defines the model to the view of your application, methods for every action you require and the scope where you hold a two way binding.Because TS offers an object-oriented architecture, we can use classes and interfaces instead of functions.Interfaces, like in all object-oriented languages, are a contract that must be implemented by classes that use it.When implemented, all methods and properties have to be used.Classes declare and implement the properties and methods exposed to the view.Every class has his own constructor function, in this function we can declare default property values and other initialisation code.Controller Interfacemodule Jworks {    interface IEmployeeController{        person:IPerson;        save(person:IPerson):void;    }}The interface will show you the intent of our controller and declare the properties and methods that will be used.When you look at the syntax, you see that the properties are strongly typed and the type is declared after the colon.If you aren’t certain what type a property should have, you can fall back to the general type ‘any’.When you declare methods in an interface you have to specify the necessary parameters and return types.The parameters have the same syntax as the properties.Controller Classmodule JWorks {    class EmployeeController implements IEmployeeController{        static $inject = [\"EmployeeService\"];        constructor(private employeeService:IEmployeeService,public employee:IEmployee){             this.employee = new Employee(\"Nivek\",\"Kevin\");        }        save():void{             this.employeeService.addEmployee(this.employee).then(function(result){             console.log(\"added successfully!\");        });   }}angular.module(\"jworks360\").controller(\"EmployeeController\", EmployeeController);Dependency Injection in classesWhen a service is needed in your controller, it must to be injected before it can be used.In the above example it is important that you declare the static $injection on top of your constructor.By doing this the constructor will recognize and initialise the injected services.If you inject a custom service you have to reference to the related service./// &lt;reference path=\"../services/employee.service.ts\"/&gt;ConstructorTypeScript supports initialisation of your properties and injections in a constructor.When declaring properties in your class, you can declare them directly into your constructor.Although these two examples are correct you can have issues in your tests with the second example.So this:class Controller {    name:string;    constructor(public name:string){        this.name=name;    }}Becomes:class Controller {    constructor(public name:string){    }}Be sure to notice that we are using access modifiers to tell the controller which properties we want to expose to the view.The best practice is that you put your injections and Angular services private and all your properties you want to use on your view public.When initialising strings, TypeScript makes no distinction between double or single quotes.ControllerAsController classes use the controllerAs feature by default.So it’s important to declare this into your routes and views.In your HTML you will have to prefix your methods and properties with the ControllerAs syntax.module JWorks {   \"use strict\";   function routes($routeProvider) {       $routeProvider           .when('/', {               redirectTo: '/login'           })           .when('/profile', {               templateUrl: 'app/persons/profile.html',               controller: 'EmployeeController',               controllerAs: '$profile',           })           .otherwise({               redirectTo: '/'           });   }   routes.$inject = [\"$routeProvider\"];   angular.module(\"jworks360\")       .config(routes);}ServicesWhen you make a custom service, the code you implement is reusable and can be called in any other Angular component, including controllers and other services.It is important to know that services are singletons, so there will be only one instance for each service.With this in mind we can use the custom service to share data across all components in Angular.Communicating with an HTTP service to collect and share data with any other component by injecting the service.RestangularFor my project I used an Angular service that simplifies common verb requests with a minimum of client code.In my custom services you’ll see examples of Restangular in TypeScript.If you like to checkout what the difference is with $resource, you can check this listmodule JWorks {   export interface IEmployeeService {       username:string;       employee:IEmployee;       getEmployee(): Employee;       setEmployee(employee:IEmployee);       getEmployeeByUsername(username:string):ng.IPromise&lt;{}&gt;;       getEmployeeByLink(href:string):ng.IPromise&lt;{}&gt;;   }   export class EmployeeService implements IEmployeeService {       static $inject = [\"EmployeeRestangular\", \"$location\"];       constructor(private employeeRestService:restAngular.IService, private $location:ng.ILocationService, private employees:restAngular.IElement,public employee:IEmployee,public username:string) {            username =window.sessionStorage.getItem(\"username\");            this.employees = employeeRestService.all(\"employees\");            employees.one(username).get().then((data:any)=&gt; {                   this.setEmployee(data);               });           }       }       getEmployee():Employee {           return this.employee;       }              setEmployee(employee:IEmployee) {           this.employee = employee;       }                    getEmployeeByUsername(username:string):ng.IPromise&lt;{}&gt;            return this.employees.one(username).get()       }       getEmployeeByLink(href:string):ng.IPromise&lt;{}&gt;{           return this.employees.oneUrl(href).get();       }          }          angular.module(\"jworks360\")              .service(\"EmployeeService\", EmployeeService);       }In the above example, to use the Restangular service you have to install the proper typings.For services it is a best practice to declare an interface for data typing and getting a clear view of the intent.The service class will implement all methods related to the data communication with the backend and returns a promise to the controllers or services that will inject this custom service.Restangular has its own configuration you can modify in the .config component to point to the right api call.After the config you can inject the Restangular service and use its services to build up a request to the backend.DirectivesCustom directives allow you to create highly semantic and reusable components.A directive allows Angular to manipulate the DOM and add its own behaviour. These can either be a set of instructions or a JSON representation.To define a directive in TypeScript we use the directive service that Angular provides.module JWorks {   export interface IAnimate extends ng.IAttributes {       jwAnimate:string;   }   class Animate implements ng.IDirective {       restrict = \"A\";       static instance():ng.IDirective {           return new Animate();       }       link($scope, elm:ng.IRootElementService, attr:IAnimate,ngModel:ng.INgModelController):void {           $scope.right = function(){               $(this).animate({                   left: '+=150'               });                elm.fadeOut(\"slow\");           };           var direction = attr['jwAnimate'];           elm.on('click',$scope[direction]);       }   }   angular.module(\"jworks360\").directive(\"jwAnimate\", Animate.instance);}In the interface above we have to tell Angular what name will be used for our directive.The attribute service will be called to add the name to its attributes.Secondly the class has to implement the directive interface to be recognized by the compiler as a directive.Inside the class you have to declare the prefixed properties and override the methods you will be using.The static instance() method has to be declared to let your module know that there is a new directive.At the end you register the directive to your module with the instance as value.Final noteBest practices can change over time. With webpack for example the registry to the module is contained in one file.TypeScript keeps on growing, and in my opinion will be the default language for many future front-end projects.When it comes to testing our code, TypeScript will provide better support because of encapsulation. Finally, this is a nice learning path to take if you want to migrate to Angular 2."
      },
    
      "security-2016-03-12-digitally-signing-your-json-documents-html": {
        "title": "Digitally signing your JSON documents",
        "url": "/security/2016/03/12/Digitally-signing-your-JSON-documents.html",
        "image": "/img/digitally-signing-your-json-documents.png",
        "date": "12 Mar 2016",
        "category": "post, blog post, blog",
        "content": "What is a digital signature?A digital signature is a mathematical scheme for demonstrating the authenticity of a digital message or documents.A valid digital signature gives a recipient reason to believe that the message was created by a known sender, that the sender cannot deny having sent the message (authentication and non-repudiation), and that the message was not altered in transit (integrity).Digital signatures are a standard element of most cryptographic protocol suites.They are commonly used for software distribution, financial transactions, and in other cases where it is important to detect forgery or tampering.Non-repudiation refers to a state of affairs where the author of a statement will not be able to successfully challenge the authorship of the statement or validity of an associated contract.The term is often seen in a legal setting wherein the authenticity of a signature is being challenged.In such an instance, the authenticity is being “repudiated”.Meet JOSEJOSE is a framework intended to provide a method to securely transfer claims (such as authorisation information) between parties.The JOSE framework consists of several specifications to serve this purpose:  JWK – JSON Web Key, describes format and handling of cryptographic keys in JOSE  JWS – JSON Web Signature, describes producing and handling signed messages  JWE – JSON Web Encryption, describes producing and handling encrypted messages  JWA – JSON Web Algorithms, describes cryptographic algorithms used in JOSE  JWT – JSON Web Token, describes representation of claims encoded in JSON and protected by JWS or JWEJWKA JSON Web Key (RFC7517) is a JavaScript Object Notation (JSON) data structure that represents a cryptographic key.    {         \"kty\": \"EC\",        \"crv\": \"P-256\",        \"x\": \"f83OJ3D2xF1Bg8vub9tLe1gHMzV76e8Tus9uPHvRVEU\",        \"y\": \"x_FEzRu9m36HLN_tue659LNpXW6pCyStikYjKIWI5a0\",        \"use\": \"sig\",        \"kid\": \"Public key used to sign our messages\"    }In this example you can see a couple of parameters.The first of them “kty” defines the key type, which is a mandatory field.Depending on the type you’ve chosen other parameters can be set, like you see above.As our type is EC, or Elliptic Curve, we want to specify the type of curve and our point.Next to these parameters we also have the optional “use” to denote intended usage of the key and “kid” as key ID.At the time of writing there are three supported key types: “EC”, “RSA” and “oct”.While “EC” and “RSA” are used for asymmetric encryption, “oct” is used for symmetric encryptionJWSThe JSON Web Signature (RFC7515) standard describes the process of creation and validation of a data structure representing a signed payload.Assume someone wants to transfer an amount of money to his savings account.This action could be represented like the following JSON:    {         \"from\": {            \"name\": \"Tim Ysewyn\",            \"account\": \"Checking account\"        },        \"to\": {            \"name\": \"Tim Ysewyn\",            \"account\": \"Savings account\"        },        \"amount\": 250,        \"currency\": \"EUR\"    }In this example we are using a JSON document, but this is not relevant for the signing procedure.Before we can sign this we need to convert this to base64url encoding, which will be our payload.So actually we might be using any type of data!The result of the base64url encoding of above transaction is:eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9Additional parameters are associated with each payload.One of those is the required “alg” parameter, which indicates what algorithm needs to be used to generate a signature.Here we can also specify “none” to send unprotected messages.All parameters are included in the final JWS.These can either be sent as a protected or unprotected header.The data in the unprotected header is human readable associated data, whereas data in the protected header is integrity protected and base64url encoded.Assume we want to sign our payload using a key like we generated in the previous section.Our data structure would look like this:    {         \"alg\": \"ES256\"    }and base64url encoded this would be:eyAKICAgICAgICAiYWxnIjogIlJTMjU2IgogICAgfQ==The base64url encoded payload and protected header are concatenated with a ‘.’ to form raw data, which is fed to the signature algorithm to produce the final signature.Finally all of this output will be serialized using one the JSON or Compact serialisations.Compact serialisation is simple concatenation of dot separated base64url encoded protected header, payload and signature.JSON serialisation is a human readable JSON object, which for the example in this section would look like this:    {      \"payload\": \"eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9\",      \"protected\": \"eyAKICAgICAgICAiYWxnIjogIlJTMjU2IgogICAgfQ==\",      \"header\": {        \"kid\": \"e9bc097a-ce51-4036-9562-d2ade882db0d\"      },      \"signature\": \"DtEhU3ljbEg8L38VWAfUAqOyKAM6-Xx-F4GawxaepmXFCgfTjDxw5djxLa8ISlSApmWQxfKTUJqPP3-Kg6NU01Q\"    }Before we conclude this section, there is one more thing I would like to share with you.Because we want to sign and protect our messages, we always want to use asymmetric encryption.But, once our private key has been captured, anyone who has this can forge transactions.One way that COULD counter this is to generate a new key pair every session, or even per transaction.Including the public key in the protected header would not only give the server the ability the validate the signature, we will also be sure that it is the correct one since the protected header is integrity protected!JWEJSON Web Encryption (RFC7516) follows the same logic as JWS with a few differences:  by default, for each message a new content encryption key (CEK) should be generated.This key is used to encrypt the plaintext and is attached to the final message.Public key of recipient or a shared key is used only to encrypt the CEK (unless direct encryption is used).  only AEAD (Authenticated Encryption with Associated Data) algorithms are defined in the standard, so users do not have to think about how to combine JWE with JWS.To keep it short: While JWS can be read by everyone because of the simple base64url encoding, we could use JWE to encrypt some or all of our fields.JWAJSON Web Algorithms (RFC7518) defines algorithms and their identifiers to be used in JWS and JWE.The three parameters that specify algorithms are “alg” for JWS, “alg” and “enc” for JWE.Visit following links to view the list of supported algorithms for JWS and JWEJWTJSON Web Token (RFC7519) is used for passing claims between parties in a web application environment.Because the tokens are designed to be compact and URL-safe they are especially usable in a web browser single sign-on (SSO) context.JWT claims can be typically used to pass the identity of authenticated users between an identity provider and a service provider.JWT relies on all previously mentioned JSON standards.The JWT standard defines claims - key/value pairs asserting information about a subject.The claims include  “iss” identifies the principal that issued the token  “sub” identifies the principal that is the subject of the token  “aud” (audience) identifies the intended recipients  “exp” identifies the expiration time on or after which the token MUST NOT be accepted for processing  “nbf” (not before) identifies the time before which the token MUST NOT be accepted for processing  “iat” (issued at) identifies the time at which the token was issued  “jti” (JWT ID) provides a unique identifier for the tokenThese claims are not mandatory to be used or implement in all cases, but they rather provide a starting point for a set of useful, interoperable claims.So, how do we sign this JSON document in code?Ranging from Java and .NET to Node.js, there are already a lot of libraries available on the internet.And even JavaScript has its own implementation of the standard!Because of its fluent API, we are using the Java JWT implementation in this post.Since not all algorithms are implemented in Java, we are also going to use Bouncy Castle as our JCA provider.In our maven configuration we just add following two dependencies:    &lt;dependency&gt;        &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt;        &lt;artifactId&gt;jjwt&lt;/artifactId&gt;        &lt;version&gt;0.6.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt;        &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt;        &lt;version&gt;1.54&lt;/version&gt;    &lt;/dependency&gt;If you are working with a gradle project it would be:    runtime 'io.jsonwebtoken:jjwt:0.6.0',            'org.bouncycastle:bcprov-jdk15on:1.54'If we were to implement the examples from the previous sections, we would start of with generating a new public-private key pair.    KeyPair keyPair = EllipticCurveProvider.generateKeyPair(SignatureAlgorithm.ES256);It’s as easy as that!We want to have a key of type “EC” so we use the EllipticCurveProvider, and by specifying SignatureAlgorithm.ES256 we use the P-256 bit curve with SHA-256 hashing.Next we want to sign our base64url encoded payload    Jwts.builder()                .setPayload(\"eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9\")                .signWith(SignatureAlgorithm.ES256, keyPair.getPrivate())                .compact();Since we already encoded our original message in the JWS section, I’m not getting here into detail again.signWith(SignatureAlgorithm.ES256, keyPair.getPrivate()) does a couple of things.First it is going the create a header if not already present and it will add the “alg” key with the value of “ES256”.After that it will base64url encode that header and will append this with a ‘.’ and the encoded payload.This whole blob of data will then be signed using the private key of the previously generated key pair.Last, but not least, is the compact method.This will just output the base64url encoded header and payload with the generated signature, and all parts are separated with a dot.An outcome would be something like:eyJhbGciOiJFUzI1NiJ9.ZXlBS0lDQWdJQ0FnSUNBaVpuSnZiU0k2ZXdvZ0lDQWdJQ0FnSUNBZ0lDQWlibUZ0WlNJNklDSlVhVzBnV1hObGQzbHVJaXdLSUNBZ0lDQWdJQ0FnSUNBZ0ltRmpZMjkxYm5RaU9pQWlRMmhsWTJ0cGJtY2dZV05qYjNWdWRDSUtJQ0FnSUNBZ0lDQjlMQW9nSUNBZ0lDQWdJQ0owYnlJNmV3b2dJQ0FnSUNBZ0lDQWdJQ0FpYm1GdFpTSTZJQ0pVYVcwZ1dYTmxkM2x1SWl3S0lDQWdJQ0FnSUNBZ0lDQWdJbUZqWTI5MWJuUWlPaUFpVTJGMmFXNW5jeUJoWTJOdmRXNTBJZ29nSUNBZ0lDQWdJSDBzQ2lBZ0lDQWdJQ0FnSW1GdGIzVnVkQ0k2SURJMU1Bb2dJQ0FnSUNBZ0lDSmpkWEp5Wlc1amVTSTZJQ0pGVlZJaUNpQWdJQ0I5.MEYCIQCcwunLBiuHu2z_SlDVJyZuQv0NU8X4VYoOFN1EuIvObQIhAJeZuTeZw9k5uhpBc60iT13s3yb01ItSB2MhEd5pUSqCWe split the three parts for better visualisation, the JWS would be one large StringValidating the signatureFirst we will check if the JWS was actually signed.This can be accomplished by executing following line of code.    Jwts.parser().isSigned(jws);To parse the JWS, we use the parse() method.    Jwts.parser()        .setSigningKey(publicKey)        .parse(jws);Depending wether it is signed or not we might need to set the key for validation.In our case we need to specify the public key of our asymmetric key pair.If we would try to parse the JWS without a key an IllegalArgumentException will be thrown.Should a wrong public key have been provided a SignatureException would be thrown, telling us to not trust this JWS.If we were to pass our public key in the protected header like we said in the JWS section, we should use the setSigningKeyResolver() method.This custom resolver would read out the “jwk” field from the protected header and return a public key based on the data that was provided.Our own SigningKeyResolver implementation could look like this:    public class ECPublicSigningKeyResolver implements SigningKeyResolver {        public Key resolveSigningKey(JwsHeader header, Claims claims) {            return getPublicKey(header);        }        public Key resolveSigningKey(JwsHeader header, String plaintext) {            return getPublicKey(header);        }        private Key getPublicKey(JwsHeader header) {            try {                HashMap&lt;String, String&gt; jwk = new ObjectMapper().readValue(header.get(\"jwk\").toString(), HashMap.class);                            String curve = jwk.get(\"crv\");                BigInteger x = new BigInteger(jwk.get(\"x\"), 16);                BigInteger y = new BigInteger(jwk.get(\"y\"), 16);                String keyType = jwk.get(\"kty\");                ECNamedCurveParameterSpec ecNamedCurveParameterSpec = ECNamedCurveTable.getParameterSpec(crv);                            ECCurve curve = ecNamedCurveParameterSpec.getCurve();                ECPoint g = ecNamedCurveParameterSpec.getG();                BigInteger n = ecNamedCurveParameterSpec.getN();                BigInteger h = ecNamedCurveParameterSpec.getH();                ECParameterSpec ecParameterSpec = new ECParameterSpec(curve, g, n, h);                ECPoint ecPoint = curve.createPoint(x, y);                ECPublicKeySpec ecPublicKeySpec = new ECPublicKeySpec(ecPoint, ecParameterSpec);                KeyFactory keyFactory = KeyFactory.getInstance(kty);                return keyFactory.generatePublic(ecPublicKeySpec);            } catch (IOException e) {                e.printStackTrace();            } catch (NoSuchAlgorithmException e) {                e.printStackTrace();            } catch (InvalidKeySpecException e) {                e.printStackTrace();            }            return null;        }    }First we read all our data from the “jwk” field.Next we retrieve the ECNamedCurveParameterSpec based on the “crv” field and assemble a new ECParameterSpec.After that we create a new ECPublicKeySpec with the ECParameterSpec and an ECPoint out of the x and y coordinates.Finally we get a KeyFactory instance for our key type “kty” and generate the public key with our ECPublicKeySpec.ConclusionJOSE is a simple, compact and lightweight framework to sign and encrypt your payload messages.Because of the combination of base64url encoded messages and JSON data structures it is web friendly.With the wide range of libraries this can be used across platforms with native and hybrid applications, even web applications can use this!One particular disadvantage with the use of the compact dot notation is that you can’t send unprotected header data anymore.Final noteAbove examples should only be used as reference. In a production environment we need to use both JWS and JWE.One could embed a public key of an asymmetric key pair in the application.During login a new symmetric key will be generated, encrypted with that public key and sent to the server.This symmetric key can only be decrypted by the server with the private key, and should then be stored in the session.Every time we need to sign a JSON document, we would use the symmetric key to encrypt the JWS using JWE.It doesn’t matter how you encrypt your messages, and which algorithms you use.Once your application has been hacked, the whole system is vulnerable."
      },
    
      "conference-2016-03-10-javaland2016-html": {
        "title": "The 5 key trends of JavaLand 2016",
        "url": "/conference/2016/03/10/JavaLand2016.html",
        "image": "/img/javaland.png",
        "date": "10 Mar 2016",
        "category": "post, blog post, blog",
        "content": "  JavaLand is a software conference, held annually, in Phantasialand, Brühl (Germany). JavaLand focuses on Java enthusiasts, developers, architects, strategists, administrators and project managers. With more than 100 lectures, JavaLand caters to the interest of both beginners and experts. These are, what I believe, the 5 key trends of the conference.1. Microservices stay hot and are maturingJavaLand dedicated an entire track to containers and microservices. This resulted in a large variety of talks on the subject. I attended a couple, but the talk by Ordina’s very own Andreas Evers hit the sweet spot between introducing the microservice concepts and making them tangible. Microservices transfer a lot of the application’s complexity to the interactions between the services. Applying patterns such as circuit breakers and bulkheads are quintessential to building successful distributed systems. Andreas presented all of this in a clear and concise manner to the delight of the audience.2. TypeScript / Angular 2Frontend developers are in for a treat. Currently, nobody exactly knows when Angular 2 is going to be released, but everybody is eagerly looking forward to it. Rumor has it .. release will be very soon. Angular 2 promises to be a faster, more powerful, cleaner, and easier to use tool. The Angular team provides an upgrade path to migrate your old Angular 1 applications. What’s also really interesting is the Angular 2 Style Guide, that contains best practices on how to organize your project, name your components, etc.Angular 2 was migrated to TypeScript, because of the great tooling support. TypeScript isn’t the first language to compile to JavaScript, but supported by Angular 2, it just might be a game changer.3. Cloud-Native JavaJosh Long did a stunning job, giving a whirlwind talk on a large number of Spring tools to support the building of Cloud Native Java applications. He started with Spring Data REST to build a hypermedia-driven REST web service with a Spring Data Repository. Then he introduced Spring Cloud Config to externalize configuration files of the different microservices he was building. When building distributed applications, a config server is essential, in my opinion. Next up was Spring Cloud Netflix, which is a “Springified” collection of tools open sourced by Netflix. Josh demoed:  Eureka for service registry and discovery  Zuul as an API gateway  Hystrix for circuit breakersFinally he used Spring Cloud Sleuth with Zipkin to show us a nice dashboard of the different requests going through his freshly deployed microservices.A talk by Josh Long is always an event and we’re very proud to announce that he will be doing a presentation at Ordina Belgium later this month!4. Reactive“Reactive” is used broadly to define event-driven systems. Reactive Systems are responsive, resilient, elastic and message driven. Details on these key characteristics can be found in the Reactive Manifesto. The most popular Java library to compose asynchronous and event-based programs is RxJava. To build reactive applications, RxJava uses Observable sequences that make it easy to wrap synchronous methods in asynchronous calls.An interesting presentation to learn more about this topic is available on Youtube.While Reactive programming isn’t new, it has been gaining a lot of traction recently. For example thanks to the recently released Lagom framework from Lightbend. Last year the Spring team announced that Spring 5 will also focus on Reactive.5. KubernetesKubernetes is an orchestration system for Docker and Rocket containers, initiated by Google in 2014. In Kubernetes, containers run in Pods. These pods are managed by Replication Controllers (create, destroy, start / stop, failure, scaling, …). Since Replication Controllers can create and destroy Pods dynamically, we can’t rely on their IP addresses to communicate with each other. This can be solved by using Kubernetes Services.Kubernetes can schedule and run containers on clusters of both physical and virtual machines.An interesting discussion, after one of the Kubernetes talks, was about the differences between Kubernetes and a regular PaaS. This post on Stackoverflow provides a lot of input for that discussion, stating that Kubernetes is PaaS-like: Cloud Foundry can be considered an “Application PaaS” and Kubernetes a “Container PaaS”, but the distinction is fairly subtle and fluid, given that both projects change over time to compete in the same markets. The subtlety of the difference is demonstrated by the Kubernetes documentation, explicitly stating Kubernetes is not a PaaS.The Community ActivitiesIt’s impossible to talk about JavaLand, without mentioning The Community Activities. These focus on  Innovation discovery: Humanoid Robots, Virtual Reality (VR), Neural Networks, …  Gamification: Hacking sessions and contests, Dojos, …  Networking: User Groups, Jogging, Tours, …  Phantasialand: The theme park opened its door exclusively for JavaLand on Tuesday evening. What better way to bond with colleagues and leaders in the Java community than in a roller coaster :)"
      },
    
      "spring-2016-03-05-http-public-key-pinning-with-spring-security-html": {
        "title": "HTTP Public Key Pinning with Spring Security",
        "url": "/spring/2016/03/05/HTTP-Public-Key-Pinning-with-Spring-Security.html",
        "image": "/img/spring-security-logo.png",
        "date": "05 Mar 2016",
        "category": "post, blog post, blog",
        "content": "What kind of sorcery is this?HTTP Public Key Pinning, or short HPKP, is a security mechanism which allows HTTPS websites to resist impersonation by attackers using mis-issued or otherwise fraudulent certificates.This was standardized in RFC 7469 and creates a new opportunity for server validation. Instead of using static certificate pinning, where public key hashes are hardcoded within an application, we can now use a more dynamic way of providing this public key hashes.One caveat to remember is that HPKP uses a Trust On First Use (TOFU) technique.How does this work?A list of public key hashes will be served to the client via a special HTTP header by the web server, so clients can store this information for a given period of time.On subsequent connections within previous given period of time, the client expects a certificate containing a public key whose fingerprint is already known via HPKP.I strongly encourage you to read this article by Tim Taubert, where he explains what keys you should pin and what the different tradeoffs are.Imagine you want to terminate the connection between the client and a malicious server for your main domain and all of your subdomains, but also want to be notified when such events happen.In the next paragraph you can find the implementation details.The web server needs to send following header to the connecting client with the first response    Public-Key-Pins:        max-age=5184000;        pin-sha256=\"d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=\";        pin-sha256=\"E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=\";        report-uri=\"https://example.net/hpkp-report\";        includeSubdomainsBy specifying the Public-Key-Pins header the client MUST terminate the connection without allowing the user to proceed anyway. In this example, pin-sha256=”d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=” pins the server’s public key used in production. The second pin declaration pin-sha256=”E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=” also pins the backup key. max-age=5184000 tells the client to store this information for two month, which is a reasonable time limit according to the IETF RFC. This key pinning is also valid for all subdomains, which is told by the includeSubdomains declaration. Finally, report-uri=”https://www.example.net/hpkp-report” explains where to report pin validation failures.So how can we implement this with Spring Security?Retrieving  the list of public key hashesWe first need to get a list of public key hashes. Currently the standard only supports the SHA256 hashing algorithm. The following commands will help you extract the Base64 encoded information:From a key file\topenssl rsa -in my-key-file.key -outform der -pubout | openssl dgst -sha256 -binary | openssl enc -base64From a Certificate Signing Request (CSR)\topenssl req -in my-signing-request.csr -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64From a certificate\topenssl x509 -in my-certificate.crt -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64From a running web server\topenssl s_client -servername www.example.com -connect www.example.com:443 | openssl x509 -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64For now we will assume we got 2 public keys:  Our active production certificate: d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=  Our backup production certificate: E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=Configuring Spring SecurityAs of version 4.1.0.RC1, which will be released March 24th 2016, the HpkpHeaderWriter has been added to the security module. The 2 easiest ways to implement this feature is either by Java configuration or by using the older, but still supported, XML configuration. Below you can find both solutions:Java config\t@EnableWebSecurity\tpublic class HpkpConfig extends WebSecurityConfigurerAdapter {\t\t@Override\t\tprotected void configure(HttpSecurity http) throws Exception {\t\t\thttp.httpPublicKeyPinning()\t\t\t\t.addSha256Pins(\"d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=\", \"E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=\")\t\t\t\t.reportOnly(false)\t\t\t\t.reportUri(\"http://example.net/hpkp-report\")\t\t\t\t.includeSubDomains(true);\t\t}\t}XML config\t&lt;http&gt;\t\t&lt;!-- ... --&gt;\t\t&lt;headers&gt;\t\t\t&lt;hpkp\t\t\t\treport-only=\"false\"\t\t\t\treport-uri=\"http://example.net/hpkp-report\"\t\t\t\tinclude-subdomains=\"true\"&gt;\t\t\t\t&lt;pins&gt;\t\t\t\t\t&lt;pin&gt;d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=&lt;/pin&gt;\t\t\t\t\t&lt;pin&gt;E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=&lt;/pin&gt;\t\t\t\t&lt;/pins&gt;\t\t\t&lt;/hpkp&gt;\t\t&lt;/headers&gt;\t&lt;/http&gt;"
      },
    
      "spring-2016-02-06-generating-spring-rest-docs-without-using-integration-tests-html": {
        "title": "Generating Spring REST Docs without using integration tests",
        "url": "/spring/2016/02/06/Generating-Spring-REST-Docs-without-using-integration-tests.html",
        "image": "/img/spring.png",
        "date": "06 Feb 2016",
        "category": "post, blog post, blog",
        "content": "The problemA couple of days ago I was working on a project of one of our customers.One of their new applications needed to expose a public API, and of course we needed to hand over a set of documentation about those REST endpoints.Some people were already starting to do this manually in Confluence, but after a while (and we’re talking about a timespan just under 2 hours) this became a tedious job. We had to continuously adjust the input &amp; output contracts, the different endpoints,…Using Spring REST Docs I wanted to automatically document all of the public API endpoints, while we were also testing all of the components in the whole application.For some undisclosed reasons we simply couldn’t write integration tests, so we were stuck with our unit tests and mocked objects.The solutionImagine you have following service and controller in a simple Spring Boot application:    @Service    public class DeviceService {        public List&lt;Device&gt; getDevices() {        List&lt;Device&gt; devices = new ArrayList&lt;&gt;();            /*                Some business logic here...            */            return devices;        }    }    @RestController    @RequestMapping(\"devices\")    public class DeviceController {        private DeviceService deviceService;        @Autowired        public DeviceController(DeviceService deviceService) {            this.deviceService = deviceService;        }        @RequestMapping(method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE)        public List&lt;Device&gt; getDevices() {            return this.deviceService.getDevices();        }    }    Since this is a Spring Boot application both classes will automagically be instantiated.Because you need to annotate your unit tests at class level with @WebAppConfiguration and @SpringApplicationConfiguration, we can easily create a new Spring Boot application and use this for our documentation.In this new application we set the base package that needs to be scanned to our controller sub package, and create a mock implementation of our DeviceService.    @SpringBootApplication(scanBasePackages = { \"be.ordina.blog.controller\" } )    public class Application {        @Bean        public DeviceService getDeviceService() {            return EasyMock.createStrictMock(DeviceService.class);        }    }    Our DeviceControllerTests class will then look something like this:    @RunWith(SpringJUnit4ClassRunner.class)    @SpringApplicationConfiguration(classes = { Application.class })    @WebAppConfiguration    public class DeviceControllerTests {        @Rule        public RestDocumentation restDocumentation = new RestDocumentation(\"target/generated-snippets\");        @Autowired        private WebApplicationContext context;        @Autowired        public DeviceService deviceService;        private MockMvc mockMvc;        @Before        public void setUp() {            this.mockMvc = MockMvcBuilders.webAppContextSetup(this.context)                                            .apply(documentationConfiguration(this.restDocumentation))                                            .build();        }        @After        public void cleanup() {            EasyMock.verify(deviceService);            EasyMock.reset(deviceService);        }        @Test        public void getDevices() throws Exception {            Device firstDevice = new Device(\"iPhone 6\");            Device secondDevice = new Device(\"Nexus 5\");                        List&lt;Device&gt; devices = new ArrayList&lt;&gt;();            devices.add(firstDevice);            devices.add(secondDevice);            EasyMock.expect(deviceService.getDevices()).andReturn(devices);            EasyMock.replay(deviceService);            this.mockMvc.perform(get(\"/devices\").accept(MediaType.APPLICATION_JSON))                        .andExpect(status().isOk())                        .andExpect(jsonPath(\"$\", hasSize(2)))                        .andExpect(jsonPath(\"$[0].name\", is(firstDevice.getName())))                        .andExpect(jsonPath(\"$[1].name\", is(secondDevice.getName())))                        .andDo(document(\"device\"));        }    }    So this is how I managed to get rid of the manual, tedious work and keep my unit tests - and got back to the more serious part of my life: coding like a monkey. =)  PS: All of the code above is checked in at our public github repo, so you are free to clone the working application! You can find it here!"
      },
    
      "domain-driven-20design-2016-02-02-a-decade-of-ddd-cqrs-and-event-sourcing-html": {
        "title": "A decade of DDD, CQRS and Event Sourcing",
        "url": "/domain-driven%20design/2016/02/02/A-Decade-Of-DDD-CQRS-And-Event-Sourcing.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "02 Feb 2016",
        "category": "post, blog post, blog",
        "content": "  Command and Query Responsibility Segregation is the most misinterpreted pattern in software design. CQRS doesn’t mean eventual consistency, it’s not about eventing and messaging. It’s not even what most people believe about having separate models for reading and writing.  In his talk A decade of DDD, CQRS and Event Sourcing on DDDEurope 2016, Greg Young gives us a retrospective over the last ten years practicing CQRS and event sourcing.CQRS?Before I go any further, let’s start explaining what CQRS really is. It’s all about applying a design pattern when you notice that your class contains both query- and command methods. It’s not a new principle. Bertrand Meyer described Command-Query Separation in his book Object-oriented Software Construction as follows:  “Every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, Asking a question should not change the answer.”We can apply CQRS principles in many levels of our application, but when people talk about CQRS they are really speaking about applying the CQRS pattern to the object that represents the service boundary of the application. The following example illustrates this.Although this doesn’t seem very interesting to do at first, architecturally we can do many interesting things by applying this pattern:  One example is that the separation is more explicit and programmers will not find it odd to use different data models which use the same data. Reading records from the database must be fast and there’s no problem at all if you can achieve this by using multiple representations of the same data.  CQRS is also an enabler for event-based programming models. It’s common to see CQRS system split into separate services communicating with Event Collaboration. This allows these services to easily take advantage of Event Sourcing.You should be cautious however not to use it everywhere and only in some Bounded Contexts that need it, as everyone agrees that applying the CQRS principle adds complexity.History  “When you searched CQRS on Google a decade ago, it thought it was just Cars misspelled.”CQRS is not a new concept. You might even say that event sourcing has been around for thousands of years. The ancient Sumerians wrote accounting info on clay tablets and baked them. That document stored events in time. Immutable events. And documents are built up of this event information.As I said earlier, Meyer talked about the principle in his book which was released in 1988.It’s QCon San Francisco in 2006 which really gave a boost to the popularity of CQRS and event sourcing. Martin Fowler picked up CommandQuerySeparation in his Bliki and after that, things began to grow.CQRS is more of a stepping stone and you have to put it in its historical context. It was a natural consequence of what was happening with Domain-Driven Design at that time. CQRS is not an end goal, it should be seen as a stepping stone for going to an event sourced system.Good thingsThe community around CQRS and event sourcing is growing to about 3000 people. More and more domains are involved with event sourcing. In other domains, other added values were discovered. These people had breakthroughs by practicing CQRS, eg. in a warehouse system, instead of denying a user’s request because the system couldn’t handle the requests anymore, it accepts an event and processes it a later time.Another good thing about event sourcing, once you model events, you are forced to think about the temporality of systems: what happens at a specific time? how will this object behave in this situation?Event Storming exercises help you to figure out which domains you have in your system and give you a clear view on the different events. You can then formalize events and commands.Ideas about Event Sourcing have been spreading. Functional programming gained popularity in parallel with event sourcing. Event sourcing is a natural functional model. Every state is a left fold of your previous history.A lot of other things also pushed Event Sourcing forward:  Cloud computing  Popularity of Actor Models  MicroservicesBad thingsSome people see CQRS as a full-blown architecture, but it’s not. This is wrong. CQRS and event sourcing is not a top level architecture. You cannot build an Event Sourced system. Instead, you end up into building a monolith which is event sourced internally. Event sourcing is simply not a good solution to every problem. For example, once you deal with immutable events, you need to think about corrections to data. Whenever a user corrects a value and hits the save button again, you would need to have an event for that and it would be too complex to handle.A lot of little things are misinterpreted by the community and this caused dogmas to pop up:  “Value objects can be mutable in some use cases” - It’s not because Eric Evans once said “Value objects are normally immutable” that you have to think that in some situations, you can justify mutable objects. There is never an excuse to create mutable objects and they should be avoided at all times.  “The Write side cannot query the Read side” - There are times that you have to. When you have an invariant that crosses thousands of aggregates, you cannot avoid it.  “Inputs should always equal Outputs” eg. if i have an order command, an order event should be the result - This is not always the case and there are situations where input and output aren’t one on one.  “Must use one-way commands” There’s no such way as fire your command, put it on a queue and forget. One way commands don’t exist! They happened in the real world. They cannot be rolled back. Using commands gives you the opportunity to respond to it. One-way commands can however be changed in events in an event sourced system.Over the years some CQRS frameworks have been created. Greg’s advice is… Don’t write a CQRS framework! It will guaranteed be abandoned after a year. It’s not a framework, it’s more like a reference implementation. We also need to pull back away from Process Manager frameworks. You can probably solve your problem with an Actor Model.A queue of messages doesn’t work for all kinds of systems. You can probably linearize in 90% of the use cases, it will also probably be cheaper. For the other 10%, interesting things are happening. We’re gonna see a push to occasionally connected systems. When you choose availability and high throughput, you’ll have to move to message-driven architectures and linearization is not an option.Future thingsA lot of interesting things are happening in the software world. We’re growing to N-Temporal systems, where multiple things happen at multiple timeframes.Greg concluded with a quote of Ernest Hemingway.  “We are all apprentices in a craft where no one ever becomes a master.”Recommended readingGreg wrote a book about this matter, called Event Centric - Finding Simplicity in Complex Systems. In this book, he explains how to use DDD with Command-Query Responsibility Separation to select the right design solutions and make them work in the real world.Other sources  Martin Fowler on CQRS: http://martinfowler.com/bliki/CQRS.html  Greg Young on CQRS: http://codebetter.com/gregyoung/2010/02/16/cqrs-task-based-uis-event-sourcing-agh/ and http://www.squarewidget.com/greg-young-on-cqrs"
      },
    
      "domain-driven-20design-2016-01-29-dddeu16-odds-and-ends-html": {
        "title": "oDDs and enDs: Vaughn Vernon  on software projects in peril",
        "url": "/domain-driven%20design/2016/01/29/DDDEU16-Odds-and-Ends.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "29 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  There’s an interesting situation you will find in many software development projects.Often there is a team dedicated to keep the software alive.The team patches the system and deals with emergencies day after day.Almost every organization is dealing with this kind of situation.Obviously this is not the situation we want to be stuck in, but how do we alleviate ourselves from this?Vaughn Vernon gave a presentation today at Domain-Driven Design Europe.He started his presentation about the odd things that happen in software projects.Then he shed some light on the (future) solutions using Domain-Driven Design,the so called ends of the problems caused by the odds.oDDsWhat are the odd things that happen in software development projects?Cost centersAn insidious problem is that an IT organization within a company is considered a cost center.The business views software as something that costs a lot of money.You can almost say that the business almost wished they did not have to use computers,or employ software developers.What about the company and you?How does the business view you?Do they view you as a hacker?Someone who is thrilled about technology?If that’s the case, then you may be viewed as a cost center instead of a profit center.Budgets for software development projects are minimalOften a team only contains one senior developer and a lot of juniors developers.The senior developer must keep everything running and moving.Database-DrivenLooking at the business like a database might be a problem.How often does a developer think like this?Data needs to go from a view into a database and out of a database to a view like this.That is how a lot of developers think.We must be careful to be focused so hard on technology and not on business value.Shiny ObjectsSoftware developers are always looking for shiny objects.They want to learn about the latest technologies and work with them.DDD, BDD, TDD, Big Data, Machine Learning, Deep Learning, AI, Reactive, …How do we justify those things to the business?Big Data was a big buzzword 5 or 6 years ago.It is still a buzzword, but if you’re not saying Machine Learning as well, you’re not cool anymore.Are we using technology when it is appropiate?Sometimes the latest technology isn’t always the correct solution for the problem.A Not-So-Ubiquitous Language  It doesn’t matter what you name it. It’s just code.It is very true for the common developer that they think it doesn’t matter.But it does matter.The business wants to talk about something this way,but the developer calls it differently anyway. “It’s just code”.Poor CollaborationHow many organizations use JIRA as a collaboration tool and fail at it?Often someone spends days writing specifications and creating JIRA issues,yet developers don’t use them.Estimates are a big deal.Sometimes it takes longer to estimate than to fix the problem.Task Board ShuffleThis is where software design is entirely comprised of moving sticky notes.You move a sticky note from the Todo column to the In Progress column.After we have done this, we run back to our machine and start coding.Without thoughtful design, the code comes out of our fingertips.If you have a team compromised of a few developers working on the same problem,there will be multiple translations in one day of the same thing.Using the same terms is very important, but often neglected.Big Ball of MudMany organizations are deducing a Big Ball of Mud as software architecture.Everything is part of the same namespace and there are no bounded contexts.The software consists of entangled models that should be separate,but they are all in one place.This is the cause of many problems in the industry.You have to recognize a situation when a Big Ball of Mud is being developed and stay out of it.    Business logic is escaping to everywhereBusiness logic can be found in places outside of the core domain.Business logic in persistence logicYou often see business logic inside of persistence logic.Someone is ready to save an object to the backend storage and there is business logic in the persistence logic.The persistence logic is hiding the important business logic.You lose trace of your business logic because of this.Queries in business logicYou see business logic creating decisions by querying the database.Some part of the decision that is being made is hidden to the business logic, because it is inside that query.These queries can also be broken.Sometimes queries are so expensive, they shut down other operations because the tables are locked.Business logic in the UIThe biggest crime against business logic is putting it inside the UI.The business logic is put inside the view template or model instead of the domain model.CRUDCRUD does not work with complexity.It’s also an insidious problem where software developments teams think they can solve any problem with a database.Anemic Domain ModelThe Anemic Domain Model is one of the most widespread and adapted architectures.Often there is a domain model with objects which are connected with relationships.This all looks nice on paper, but there is no domain logic or any behavior inside these objects!Services live on top of the domain model. This is often called the Service Layer or the Application Layer.They contain most of the domain logic and use the domain objects for data.This is very contrary to object-oriented design.The data and the processes are combined together and it looks very much like procedural programming.This anti-pattern is so common, because most people have not worked with a real domain model.Wrong abstractionsA lot of times developers are thinking too much about abstractionsinstead of getting down to the business.They form a lot of “cool” abstractions that will make it better in the future.  “What if we have this sort of situation in the future?If we come up with this kind of abstraction, then this abstraction will take careof the situation in the future.”We cannot predict the future.The future of software is unknown.Coupled ServicesCoupled services are horrible. What if, for example, a REST controller calls a service, which calls another system.If the other system does not respond, you have a gap in your business logic, even if you use global XA transactions.What to do?It really could be that everybody else is normal.What if writing systems with the odd things actually is the norm?If that is normal, then wouldn’t you like to be the oddball in the crowd?enDsYou want to be the furthest point away from these problems.You want to come up with solutions that work.The business must not view you as a technologist,but as someone who is interested in  the business.You can’t just keep throwing technologies at the problem.You must come up with beneficial business solutions.Developer maturityIf you are a cost center, then you must come up with a way to make your advancesmore economical. You have to develop your maturity.You have to seek other ways to get the rest of your team to maturity.Urge them to go to DDD and software meetups.Do whatever you can, because you can only benefit if those around you are more mature than you.Passion is something we can’t always teach.But you must try to work with people who are passionate about their job.Profit centerYou must try to become a profit center.An entire unit of the business is a profit center.You can only become a profit center if you keep adding business value in a timely manner.Collaborate with the businessDon’t use JIRA to collaborate with the business.You will be amazed what you can learn if you get away from the desk and into a room.The business will tell you what they problems they have hated for years.Use an ubiquitous languageSome things cannot be explained by anybody. “Why do we call it this? Can we call it this instead?”You can learn those interesting and beneficial details by forming an ubiquitous language.Make it your goal to find that ubiquitous language with a bounded context.Concrete ScenarioShow concrete users in a concrete scenario and what goal that has to achieve.As developers we have to chase after deep models as shiny objects.It’s not just technologies.Technology matters.Try to experience with deep modeling through an ubiquitous language.You can use the Gherkin language to achieve this.With these concrete scenarios you can model your domain model and test it.Feature: Coffee Machine    Scenario: Buy Coffee        Given there is coffee left in the machine        When I deposit 1 euro        And I press the coffee button        Then I should be served a coffeeUse bounded contextsTo avoid the Big Ball of Mud, you must introduce bounded contexts and separate models.It is equally important to separate the models as it is to introduce core concepts in the core domain.You have to learn about event storming.You can understand what your bounded contexts are from an event storming event.    Metrics-based EstimatesThe artifacts that come out of an Event Storming event, you can use those as estimation units.If you can’t finish an iteration according what you’ve planned,move these estimation units in a retrospective and encur modeling debt.This modeling debt must be fixed as soon as possible.Know architectureYou must know good architectures, like the hexagonal architecture or CQRS. These architecture solve many of the above problems.They enforce bounded contexts and give the ability to do context mapping.    Decoupled ServicesServices have to be decoupled.A service which calls a peer service directly, is tightly coupled.It cannot work without the other service.What if the other service times out?You can use domain events and messaging systems to fix this problem.MicroservicesThe microservices architecture is another shiny object that a lot of people are chasing.The thing is, they are extremely similar to bounded contexts.Every microservice is master of their own model and usually has one point of access, like an aggregate root in Domain-Driven Design.Actor ModelThe Actor Model is an extremely powerful tool that we need to use in the very near futureby the majority of software development teams.CPU processing power is not increasing, but the amount of cores keeps increasing.The Actor model is a new way to leverage this power because it fully utilizes these cores with threads.    Summary  Many teams are in peril over poorly designed systems  Software development culture is broken  Developers must gain maturity and passion  DDD can be used to make a difference  Use the Actor model to design DDD based microservicesOne more thingVaughn Vernon announced an additional new book called Domain-Driven Design Distilled.It is a 200 page thick book that explains all of the core concepts of DDD.This is very light weight book, intended to rapidly not only teach your team members,but also the business about DDD. This book will be available within a month."
      },
    
      "domain-driven-20design-2016-01-29-ddd-europe-heuristics-from-the-trenches-html": {
        "title": "Heuristics from the Trenches by Cyrille Martraire",
        "url": "/domain-driven%20design/2016/01/29/DDD-Europe-Heuristics-from-the-Trenches.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "29 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  “Communication usually fails, except by accident” - Osmo WiioWith this quote of the Finnish researcher Osmo Wiio, Cyrille opened the second day of the DDD Europe conference. Osmo’s laws of communication are the human communications equivalent of Murphy’s law. Basically if communication can fail, it will and if a message can be understood in several different ways it is quite possible that it will be understood in a harmful way. With the quote Cyrille immediately wants to stress that with Domain-Driven Design, deep conversations with domain experts and careful attention to the language are key.Business domains are often very complex and hard to get into for individuals not familiar with the domain. The conversations’ game with domain experts is a game that takes many years and many failures in order to get better at. Cyrille explained that, even though it’s hard, it’s perfectly possible over time to extract a growing set of techniques, heuristics and best practices to boost the effectiveness of the interviews with domain experts, to learn faster and to converge more quickly to better models.Practices and tricks for talking to the domain experts“Why is it so hard?” you might ask yourself or “We don’t talk the same language as them and they don’t have time for us!”. While it is true that the people with the highest expertise within a certain domain often don’t have a lot of time for interviews or meetings, it is up to us, the developers, to make the necessary preparations prior to seeing them. You should first take some time in order to teach yourself some basic domain knowledge. It all starts with genuine curiosity, successful people are curious about their business domains. You may not believe this but here this will help you too! Without this you will have a bad time! Do your homework: perform the necessary research about the domain on the internet, Google around, read books, check Oasis, … Usage of ubiquitous language is very important.Note taking is also a very important aspect in the whole story. You need to be able to take notes like a pro! Learn to take notes effectively. Listen actively and don’t distort the stories the domain experts tells you. Keep the words as they are. It really is harder than you would think, so you should turn it into a game:  Write down the stories the domain experts tell you  Underline new words and add a definition for yourself, get familiar with all the domain terms  Take note of side remarks  If you think that you’ve encountered a synonym for an existing new word dig into it and ask for more detail  Show your knowledge to the domain user to establish credibility and to challenge them  But… Challenge them respectfully!All this is Domain-Driven Design!Talking to people is hardIt is not easy talking to people and it will often be hard to have productive conversations. However this is also something you can grow into and for which you can develop the right toolset. Have interactive conversations, that way you have control over the conversation and the way you can steer it. Start with “what exactly is the goal?”. Be precise when asking questions, we want to avoid having to reverse engineer the true need from an expressed solution. Be sure to scan the notes you took during a previous conversation and decide where you want the conversation to go next.  “I keep developers out of conversations about the domain because they always want to know ‘why this’ and ‘why that’. Just write your code.” - The Expert BeginnerYou should question everything: ask why but don’t go too far!Combine Domain-Driven Development with Behaviour-Driven Development. Both go hand in hand to interview domain experts. You want to discover the “unknown unknowns” as early as possible and to avoid misunderstandings as often as possible. Be sure to ask for concrete examples and genuine sample documents and data and although this doesn’t always come easy, ask and insist but as mentioned before know your limits and be sure not to push it.People always think that talking abstract is faster and will save time but think about it and ask yourself: “Is it truly faster if we were wrong or missed stuff that matters?”. If the domain expert seems hesitant about something also take note of this to take into account that the feature in question might be eligible for change later on, this way we can model our software design correctly.The domain expert delusionYou might assume that the person, you’re having a conversation with, is an actual system expert within the domain but chances are that he/she is not. The worst expert is the one whose expertise was built from the intricacies of the existing systems. It is therefor also our duty to help out the domain expert where possible:  Have empathy, try to put yourself in their shoes  Build a partnership, it should be a two-way conversation you’re having with the expert  Make it clear that the domain expert is always safe with you, that you have no plan to steal their job  Propose things, it doesn’t always have to be right but this way you will get somewhere  Look for invariants, for example: “Is there any other outcome that is also important?”  Always ask for validation of everything, even if you’re sure about itIt is common in companies that businesses often don’t want all power concentrated into the same hands. This also accounts for domains, you should therefor assume and probe. Get to know the business you are dealing with and their mentality. Not that if you happen to discover that there are multiple domain experts the situation gets a bit more tricky. Having multiple domain experts may lead to more confusion and makes it even more important asking for validation and challenging the experts. Something we want to achieve is that we want to suggest features from our code which could be very useful for the business. Instead of having software to support the domain we want to have software augmenting the domain.Keep improving yourselfGrow into it and build your own toolbox for implementing Domain-Driven Design. You might ask yourself why you should bother so much with all of this, after all we just want to get to it and write code. This however is a wrong approach because the complexity of the domain is there, it is just hiding behind a wall, you just don’t see it yet. You will discover the complexity sooner or later so you may as well want to get into it as early as possible to save precious time. After all, you want to become a domain expert too!Other useful resources mentioned by Cyrille:  Conversation Patterns for Software Professionals by Michael Bartyzel  Analysis Patterns by Martin Fowler  Living Documentation by Cyrille Martraire  Slides: http://www.slideshare.net/cyriux  Blog: http://cyrille.martraire.com  Twitter: @cyriux"
      },
    
      "domain-driven-20design-2016-01-28-evolution-of-ddd-html": {
        "title": "Eric Evans about the evolution of Domain-Driven Design",
        "url": "/domain-driven%20design/2016/01/28/Evolution-of-DDD.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "28 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  Why is it that DDD only has its own conference after 13 years? Why is this becoming a sudden hype? Why does it gain popularity and is it mentioned so many times in microservices presentations? Eric Evans talks on DDDEurope about the core idea behind Domain-Driven Design and its evolution over the last few years.But what is DDD?The subtitle of Evans’ book, Tackling Complexity in the Heart of Software, bundles two core principles of Domain-Driven Design:  It describes the process of translating complex real-life problems into software  The heart of software entails the domain that we’re working onKey in this activity is finding the core complexity in the critical part of the domain and focus on this and only this piece of complexity. Software developers and domain experts collaborate to develop models, simplified representations of the real-life problem. The written software should eventually explicitly reflect the model. Whenever a brainstorm session occurs, it almost always results in an adaptation of the models within the software.When we encounter multiple complex problems, we must think about them separately. Each problem requires its own model representation.When discussing with others about the domain, we must speak a ubiquitous language. You should use the same vocabulary for describing the problem you’re solving. However, when somebody asks you the meaning of a word, in many cases you have to ask the person: “In what context is it used?”. That’s why the language only means something within a well-defined bounded context.Domain-Driven Design is more like an attitude. Although it gives us principles and terminology to enable talking about it and have discussions, different people will do things differently. Each approach will be slightly different.Bounded contexts?A bounded context is an important principle when applying Domain-Driven Design. As i said earlier, language in itself doesn’t mean anything. It only means something when it’s used within a certain context. eg. Item can be a Stock Item, Sale Item, …Bounded contexts have the following characteristics:  Within a bounded context certain rules apply, eg. validation rules  It needs to be tangible in the software, eg. use packages for each contextAnother benefit of working with separate bounded contexts is that separate teams could take responsibility on separate bounded contexts.ConclusionWhen Erics book was released in 2003, only Java 5 and J2EE were used as a programming language for implementing projects. We only had EJB’s to solve problems and there was no other way of storing data but with SQL. If the technology is so complex and limited to implement something, you can’t focus on the real problem of software design.Nowadays, we have a lot of new tools available to implement a project: We can store data with a NoSQL database or not store it at all and keep it in-memory. We can explore other ways of approaching data with eg. event sourcing. On certain levels, Spring makes the technical aspect of writing software components a breeze. With the upcoming of microservices and each microservice having its own database, bounded contexts are much clearer to the teams working on the software. And there are probably tons of other examples on how today’s tools can help us achieving our goal: write good software.Better tools and a vivid community which masters these tools cause Domain-Driven Design to become more and more popular in ways of designing software. So maybe we can do better now than back in 2003. Maybe… Or maybe we’re not there yet. Fact is that everyday we are learning from mistakes in the past to do better in the future."
      },
    
      "conference-2015-11-09-devoxx15-docker-kubernetes-html": {
        "title": "#Devoxx Arun Gupta talks Docker",
        "url": "/conference/2015/11/09/Devoxx15-Docker-Kubernetes.html",
        "image": "/img/docker.png",
        "date": "09 Nov 2015",
        "category": "post, blog post, blog",
        "content": "DockerDocker is a tool used for container creation for software applications. We have all been aware of the existence of containers for some time, but Docker creates a standard for describing these packages.Docker is used for three things: Build, Ship &amp; Run your software.BuildCreation of a predefined container in a standardized way using the Docker CLI.Use a dockerfile containing a list of commands. The FROM command specifies an OS and additional software packages, eg. FROM jboss/wildfly. All  commands are compressed into one, customized image using the Docker CLI.ShipShare the container via DockerHub or your private repository.  Sharing = caring!RunDocker runs on a minimal operating system and uses the Union File System. On the bottom level, there’s the Bootfs kernel, on which the chosen base image or OS runs and finally, the user images ontop of that.Hosts running Docker are very environment variables oriented, so by using variables in the commands or on the machine itself you configure your application. Any other communication is usually done over HTTP/REST. The Docker images are stored on the Docker host so the actual client is very thin. A Docker app runs on the Docker engine; this is in contrast with regular VMs, running on full-blown operating systems.Docker MachineDocker Machine allows you to get started with Docker on Mac or Windows. It features the docker-machine CLI and uses the boot2docker image (32Mb small) under the hood.  Docker Machine is preferred over boot2docker for development purposes, but it is not production-ready (yet!)Easy way to set up a Docker host with docker-machine:docker-machine create --driver=virtualbox myhostListing all the installed Docker images:docker-machine lsListing all the environment variables of a newly created Docker container:docker-machine env myhostDocker Machine is also used to start, stop or restart containers. It even allows to update Docker itself.Many existing plugins provide support for various cloud platforms.Boot2DockerAn earlier version of docker-machine. As said above, it is being used by docker-machine under the hood.  My advice: migrate to docker-machine, at least for development purposes.Docker ToolboxEasily the best tool to get started with Docker.  Windows  MacOSHands-on Docker  docker helpfor all your docker needs!  docker psto check the running containers  docker imagesto check your images  docker buildfor quick build  docker run -it ubuntu sh for quick running an image in a shell  Docker images are like diapers: if they get shitty, throw’em away and take a fresh one.Docker ComposeAllows you to define and run multi-container applications. It has all the commands the regular Docker has and more.It provides a new way to link containers.Configuration is defined in one or more docker-compose.yml (default) or docker-compose.override.yml (default) files.It is a great tool for setting up Development, Staging and Continuous Integration (CI) environments.  Docker container linking is so passéA problem with container linking was that there was no possible way to work with different hosts. Docker Compose solves this by using volume mapping.It can help with running multi-host environments:  Bridge network span single host  Overlay network spans multiple hosts  Software defined networking is possible and preferred! Docker Compose solves this problem but it should still be used cautiously in production!Starting a set of Docker images using Docker Compose is easy:docker-compose up -f docker-compose.yml -f production.yml -dDocker SwarmDocker Swarm provides native clustering for Docker, fully integrated with Machine &amp; Compose. It either uses Etcd, Consul, Zookeeper or other solutions to store the cluster ID.Whenever you create a Docker Machine, you can add it to the cluster. It also serves the standard Docker API so anything that works on Docker, will work on multi-host environments.  They say the new Docker Swarm v1.0.0 release is production ready: I still have my doubts!References  Docker Docs are the de facto standard reference and are very well documented. They contain information on Docker, Machine, Compose &amp; Swarm: https://docs.docker.com/  Samples: https://github.com/javaee-samples/docker-javaQuestions or Remarks  Contact @arungupta  or @Turbots on Twitter  Create an issue or start a discussion on the Github repository (or on Gitter)"
      },
    
      "microservices-2015-10-13-microservicespatterns-html": {
        "title": "Applying software patterns to microservices",
        "url": "/microservices/2015/10/13/MicroservicesPatterns.html",
        "image": "/img/jax2015_logo.jpg",
        "date": "13 Oct 2015",
        "category": "post, blog post, blog",
        "content": "  During the week of October 12th, my colleagues Andreas Evers and Tim De Bruyn and me attended JAX London 2015. After attending over a dozen talks in three days, we went home with tons of insights about DevOps, microservices, Continuous Delivery, Spring Boot and much more. This is a write-up of Chris Richardson’s talk A pattern language for microservices.Chris Richardson is the author of POJOs in Action and is founder of the original Cloud Foundry, which was later acquired by VMWare and then SpringSource. Nowadays he is constantly thinking about microservices and founded a startup that is creating a platform for aiding the development of event-driven microservices (http://eventuate.io/). He also launched http://microservices.io/, describing microservice responsibilities and commonly accepted solutions as patterns.Problems in software engineeringChris started his presentation by pointing out a few problems in software engineering. First, we have a lot of sucks/rocks discussions between developers. JavaScript vs. Java, Spring vs. JEE, Java vs. .NET, functional programming vs. object-oriënted programming, containers vs. virtual machines, … Sounds familiar? But these discussions are usually very subjective and shallow. Back in 1986, Fred Brooks already mentioned in a paper on software engineering that there is no such thing as a silver bullet. So there is no right or wrong answer to which language or framework is better, it depends on the situation.A second problem is that we have a lot of new technologies these days. Typically, these technologies go through the Gartner Hype Cycle.At first, people discover an innovative technology and everybody wants to use it, which drives the technology into the Peak of Inflated Expectations. Docker is a good example of a technology in this phase. Then a huge drop follows, because people didn’t really understand the technology and misused it. When we start to understand the subject, that’s when productivity on the market increases.It’s clear that we need a better way to discuss and think about technology. That’s where software patterns come in.Pattern languagesPatterns help us to describe a reusable solution to a problem occurring in a particular context. The use of pattern languages is a a great way of talking about technology in general. You can see it as an advice around a topic. Describing what you want to solve and its context is much more important than the framework or tool you choose.A pattern description typically contains:  Pattern name  Context  Problem - The issue which we try to solve  Forces, which are a set of indicators why we want to use the pattern, eg. we need to do CD, run multiple instances, …  Solution. What would a pattern be without a solution?  Resulting context. Set of both benefits and downsides which we achieved, but also problems which we then have to solve next.Patterns can be related, they can be alternate solutions, solutions to problems that were caused by another pattern or more specific solutions to a certain problem. When you want to read more about writing and understanding patterns, you can read Martin Fowler’s blog post Writing Software Patterns.Patterns for the microservices worldMicroservices is another one of those new technologies which are in Peak of Inflated Expectations phase. To help us understand the complexity of implementing these kind of systems, Chris founded http://microservices.io/ . Here you can find a collection of microservices patterns.We can group the microservices patterns into several categories:  Core patterns  Deployment patterns  Discovery patternsCore patternsMonolithical applications tend to be simple to develop, test, deploy and scale. You can just run multiple copies of your monolithical application. When the application is large however, you end up in monolithic hell:  Millions of lines of code undermine developer productivity and knowledge of the system.  As one change might affect other parts of the application, there’s a fear of changing and refactoring code  Developer productivity decreases, as your IDE gets slower, startup times of the server take very long, …  Long-term commitment to a technology stackWith X and Z axis scaling, you increase the number of application instancess or you increase server resources. Y axis scaling on the other hand means you break up your application into separate microservices which group business functionality. Some benefits to this are:  Smaller, simpler apps, which are easier to understand  Less classpath hell  Faster to build and deploy  Improve fault isolation  Eliminates the long-term commitment to a single technology stackOf course, each solution always has some drawbacks, to which fortunately solutions exist.  Added complexity of developing a distributed system  We have to handle partial failures  Implement business transactions that span multiple databases  More complex testing: what do we do with transitive dependencies? Do we mock them?  What about managing communication for cross-service development and deployment?Deployment patternsForces to consider when deploying microservices are:  Variety of languages  Building and deploying must be fast  Isolate service instances  Deploying must be cost-effectiveFor these, you can look at the deployment patterns.  Service per VM = Packer.io is a great tool for running each servicein its own VM. Downside is you got the overhead of a whole VM permicroservice. It is very expensive and the deployment itself isrelatively slow. A positive thing though is that the AWSinfrastructure is very mature and reliable.  Service per container = Each service is in its own Docker container, which is started very quickly. A drawback is that these technologies are still very immature.Discovery patternsOne problem we need to address around the area of service discovery is that we need to know the IP address of the server. Simply having configuration files with the IP’s wont work anymore. On top of that, the set of API interfaces can change. And this is just a tip of the iceberg.There are several patterns related to service discovery:  Client-side discovery = The client will query the service registry,pick one from the load balancing configuration, and then use it.Netflix’ Eureka and Ribbon provide this functionality. MultipleEureka’s can be clustered.  Server-side discovery = At some level it’sthe same like client-side discovery. The difference is that the    client makes a request to the router, which then queries t he service registry. You can achieve this with eg. Nginx as the router and an Elastic Load Balancer from AWS. The advantage is that the client code is much simpler. Advantage is that it’s built in in some cloud/container environments, such as AWS ELB, Google’s Kubernetes, Marathon, …ConclusionOver the years, companies like Netflix, LinkedIn, Soundcloud and many others have applied the microservices architecture in their software landscape, with several tools and open-source libraries as a result. But deep down these tools have to tackle the same problems. Chris’ effort to describe these common problems and solutions in software patterns allows us to see the wood for the trees again. Because as I said earlier, knowing what you want to achieve is much more important than the framework or tool you choose."
      },
    
      "conference-2015-09-15-join2015-html": {
        "title": "JOIN 2015 from a developer's perspective",
        "url": "/conference/2015/09/15/JOIN2015.html",
        "image": "/img/2015-09-15-JOIN2015/join2015.jpg",
        "date": "15 Sep 2015",
        "category": "post, blog post, blog",
        "content": "  Last week the Oracle/Java unit held its yearly JOIN Event. The purpose of this event is to share knowledge between colleagues and fellow Java- and JavaScript enthousiasts. A total of 83 attendees visited Ordina Belgium’s headquarters in Mechelen to pack their heads with interesting technology facts.We started off with a couple of Ordina consultants, each giving a lightning talk (a 20 mins. talk) about a hot topic. Amongst others, there were talks about Docker, Polymer, IoT, microservices and Meteor. Afterwards everyone had the chance to attend three keynote talks from international professionals in the Java- and JavaScript world.The past, present and future of ECMAScriptTom Van Cutsem told us about the past and the future of JavaScript and how the ECMAScript standard was born. Tom is a member of the TC39 board, a group of technical people who decide which feature is in or out. There’s no political game going on in this group, which is a good thing. JavaScript was invented in 1995 by Brendan Eich, who worked at Netscape at that time, and quickly standardized as ECMAScript 1st edition in 1997. ECMAScript 5 was released in 2009 and is well supported by all major browsers, as the following compatibility table illustrates: http://kangax.github.io/compat-table/es5/. By introducing strict mode, which makes you as a developer use a more safer and sturdier JavaScript version, the guys at TC39 guaranteed the expansion of ECMAScript 6.This summer a brand new version was released: they did not call it ECMAScript 6 this time, but rather ECMAScript 2015 because they changed their release model to yearly releases. Features that aren’t ready for the release, will be skipped and released a year later. Tom then shortly introduced a subset of the new features available in ECMAScript 2015: Arrow functions are a shorter way to declare anonymous functions, a bit comparable to Java 8’s Lambdas. Second he explained probably the most important feature… classes! Actually it’s just about syntax, because they will be treated like regular functions by the engine. Most notable control flow features were promises and iterators. On the website mentioned before to check ECMAScript 5 compatibility, you can also switch to ECMAScript 2015 and see that browser compatibility is still poor. However, Tom also recommended that you can already use ECMAScript 2015 in your application by using compiling tools like Babel or TypeScript. These tools compile your ECMAScript 2015 code into ECMAScript 5. He also briefly mentioned Nashorn, the new Javacript engine developed by Oracle, released with Java 8.You can have a look at Tom’s presentation on http://t.co/Phwpx3Ig13.Boo(s)t your app developmentStéphane Nicoll enlightened us with a Spring Boot talk. He ex​actly had one slide, but blew us away with a full-packed demo of what Spring Boot’s auto-initialization features can mean for your project. Starting point is https://start.spring.io/, were you can kickstart a Spring project by checking and unchecking technologies. It’s as simple as that. When you’re working with IntelliJ, you can even do it inside your IDE with a wizard. Both result in the same project however.After creating a simple @RestController with a @RequestMapping and a Hello world, ​he added the JPA dependency, created an @Entity and a Spring Data repository. Now we only have to add a database to the project. Just by adding the H2 dependency in the pom.xml file, Spring Boot creates a database for you and attaches the created Repository classes to that database instance. It is able to do this by scanning for DataSource classes on the classpath.Actuator endpoints allow you to monitor and interact with your application. Spring Boot includes a number of built-in endpoints. For example, Spring Boot can automatically create a health status endpoint where you can check the health of your database, JMS queue or any other component that is registered with the Spring Boot system. You can even write your own.He also deployed the application to Cloud Foundry, Pivotal’s cloud platform. He demonstrated the possibility to remotely install another database on that server and bind this database instance to the application. Then he even demonstrated hot code replacement in the cloud… That’s really amazing!  We can conclude that Spring Boot makes Java development as it should be. By following the convention-over-configuration approach, we can achieve very much in very little time.A sneak peek at the new Angular 2.0Finally, Pascal Precht from Thoughtram gave some insights on Angular 2. Attention! You shouldn’t say AngularJS 2, but simply Angular 2. Pascal gave us a quick tour of Angular 2’s new syntax for property- and event binding. You will need to use square brackets around HTML attribute names for property binding and parenthesis for event binding, which looks a bit weird at first. You can read more about Angular 2’s syntax in his blog post Angular 2 Template Syntax Demystified.Angular 2 will also support Web Components, a new standard in developing custom components for web applications.On the question whether Polymer and Angular 2 weren’t tackling the same problems, Pascal replied that Polymer focuses more on Web Components whilst Angular 2 claims to be an end-to-end framework to build applications.Actually, at first the syntax seems frightening, but after hearing the reasoning behind it, it seems to me that the only difference is that Angular will embrace the standard DOM element properties instead of keeping their own in sync like in the previous version… And that’s why you could say that we don’t have two-way databinding anymore in Angular 2. Interesting things, but we’ll have to wait until 2016 to see the final syntax, because everything we saw… can already be different as we speak.​On the question when Angular 2 would be production ready, he opened up his browser and opened Is Angular 2.0 ready?. That tells enough. Pascal’s feeling is that a beta version will be released Q1 2016, but this was a non-official statement."
      },
    
      "spring-2015-05-09-springio15-general-html": {
        "title": "Spring I/O 2015 Barcelona",
        "url": "/spring/2015/05/09/SpringIO15-General.html",
        "image": "/img/springio.jpg",
        "date": "09 May 2015",
        "category": "post, blog post, blog",
        "content": "  Last week Barcelona was the place to be for Spring enthusiasts. With tons of Pivotal speakers and many more community leaders it was a two-day goldmine for anyone looking to update their Spring knowledge. This is my report ranging many different topics, including quite some one on one discussions and hacking sessions with the people behind the Spring ecosystem.Sergi Almar (event organizer), me and Josh Long (Spring developer advocate)One on one talk with Juergen Hoeller:Groovy has now become an Apache Incubator project, as Pivotal decided Groovy isn’t a core project for them. Most of the people behind Groovy are not working for Pivotal anymore but for other companies which means they can only work on it part-time. Juergen doesn’t like this but it wasn’t his decision. Luckily Groovy has a huge userbase and they truly love the language. That’s why Juergen is not too concerned about Groovy dying, but of course the speed of new developments will be a lot slower than the past two or three years.Gradle has strong dependencies on Groovy, as it’s using a Groovy-based DSL language. The story is different here however, as Gradle is backed by Gradleware, a Silicon Valley company with its own vision. Spring is using Gradle for most of its projects, because for one the devs like to use Groovy, but foremost because it offers more flexibility compared to maven which is necessary when dealing with an open-source framework.The Netflix stack is Amazon based as it’s the most important player in the market at the moment. Of course there is support for Cloud Foundry but there are no guarantees it will work on all clouds. Especially Google App Engine is kind of a mess as the team behind it doesn’t really cooperate with Pivotal or anyone else as far as Juergen knows.Currently it’s hard to find decent books about microservices but they should be coming up soon. For cloud there are some books out there but they could feel outdated already. What’s written in 2013 isn’t always completely valid anymore in 2015, especially in a field like Cloud computing.One on one hacking session with Oliver Gierke:Lots of stuff can still be moved from Spring Data REST to Spring Hateoas. This afternoon I’m sitting down with Oliver Gierke to do some hacking on the subject.It’s possible to have a resource with many different embedded resources inside by nesting domain POJOs in eachother. The thing Spring Data REST is missing is the possibility to distinguish between domain model nested classes and resource model nested classes (aka embedded resources). There is no way to embed a Car resource as an embedded resource into a Person resource without actually having Car as a property of Person in the domain model. Having the possibility to manually add embedded resources to a resource would solve this.To achieve this we should have an extension of ResourceSupport with a Set of EmbeddedWrappers inside. Using the EmbeddedWrappers class, we can add embedded resources to our resource in its Assembler. This Wrapper will take care of relation resolving, especially handy when dealing with collections which require plural-forms. The relation value can be annotated in the model or passed along as a second parameter.I will go into more detail with examples as a comment on the GitHub ticket about embedded resources which Oliver will try to follow up. An existing stackoverflow issue about the subject can be found here: http://stackoverflow.com/questions/25858698/spring-hateoas-embedded-resource-supportHaving different representations of the same resource with different fields (e.g. summary and full view) can be achieved using jsonViews. However, those jsonViews can be used for versioning as well, and having both at the same time could interfere. The Projection abstraction is a very nice feature of Spring Data REST that was moved to the Spring Data Commons package. This enables us to use this abstraction without the need of a persistence. We can use it in conjunction with Spring Hateoas without Spring Data REST.The main class to use is the SpelAwareProxyProjectionFactory. This factory can be used to create projections. It’s also possible to use the Page functionality and especially its page.map() function, which can link a projection interface with a domain class. This approach allows us to define an interface with the selection of fields from the domain class as getter methods. This defines the selection of fields which that projection should expose.This pattern is applied as well in the latest Spring MVC where a UserForm is used as a parameter of a POST endpoint method. Defining the UserForm as an interface works exactly by the same principle as the Projections of Spring Data. You can even have default methods in the interface for validation of that form.At my client I am integrating most of the stuff you can find in foxycart’s HAL browser but in a dynamic way. Exposing a graph of resources can be done using Spring Restdocs’ link documents generated in asciidoc. By parsing the results of these asciidocs and merging the results, a resulting json is aggregated and used to generate a graph of resources and their links using d3js. This graph is integrated in the HAL browser, and each resource links to its documentation. That documentation per resource can of course also be reached from the regular HAL browser documentation links using curies. To generate that documentation we are in turn using Spring Restdocs to show examples with their links, request and response fields and error scenarios. All of this is generated and guaranteed up-to-date (if it weren’t, it would have failed the build). The improvement which is still possible to do here, is to have our own hook into Spring Restdocs so we wouldn’t need to go through asciidoc links to generate the full relationship graph. Oliver asked me to open a ticket for Andy Wilkinson to allow hooks in the generation model.Oliver doesn’t fully agree with versioning on resource level. He prefers versioning APIs or not versioning at all, to avoid having a higher cost later due to legacy and lots of old versions we’ll need to support. The initial win of versioning would be insignificant compared to the technical debt it creates.Implementation of the hypertext cache pattern of HAL is quite straight forward according to Oliver. The client should be smart enough to search for the field it needs in the embedded resource, and if that field isn’t there, he should follow the link to the full resource. Keeping track of which representation is shown in which place (embedded vs linked) becomes unnecessary using that approach.More on Spring I/O 2015 Barcelona ….Boot your Search with Spring talk:Speaker: Christoph Strobl - Talk &amp; speaker descriptionSolr feels like an old kitchen sink for anything you want to do. Not exactly a fancy 2015 tool. They are catching up though and documentation is getting better. It’s scheme based. MongoDB does much more out-of-the-box which you have to do manually with xml configuration. Solr schemaless support exists but as long as it’s lucene-based, there’s no such thing as a schema-less index. Their type-guessing only goes so far until you try to add a record with a different type.Spring Data Solr does just what you expect: clean to-the-point interfaces with annotations that do the DAO magic for you. Spring Data Elasticsearch does that as well for the complexity of Elasticsearch. I never really liked the query system that Elasticsearch has so having this abstraction layer could prove really useful.Inside http://spring.io - a production spring reference application &amp; one on one talk with Brian Clozelon this blogReal-time with Spring: SSE and WebSockets talkSpeaker: Sergi Almar - Talk &amp; speaker descriptionSpring WebSockets is better than JSR356 because: there is a fallback with SockJS, there is support for STOMP subprotocol, Spring Security can jump in, and of course flawless integration with messaging components and the Spring messaging style. Security is important because there are no URLs anymore. We have to secure at message level.Spring Data REST - Repositories meet hypermedia talkSpeaker: Oliver Gierke - Talk &amp; speaker descriptionRecommended reading: Domain Driven Design. Although very boring, it introduces vital concepts in the repository world. When combining ALPS and JSON Schema, it should be possible to create a client that is smart enough to discover verbs and even fields of the payload.Building Microservices with Spring Cloud and Netflix OSS talkSpeaker: Dr. Dave Syer  - Talk &amp; speaker descriptionAnother great book is Release It!. It describes a lot of the patterns microservices use such as circuit breaker. It’s definitely a great book for devops.Master Spring Boot autoconfiguration talkon this blogCan Your Cloud Do This? Getting started with Cloud Foundry talk &amp; Building “Bootiful” Microservices with Spring Cloud workshop &amp; One one one talk with Josh Longon this blogEnjoy reading!"
      },
    
      "spring-2015-05-08-springio15-sagan-html": {
        "title": "A production Spring reference application &amp; One on One talk with Brian Clozel",
        "url": "/spring/2015/05/08/SpringIO15-Sagan.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Sagan is the name of the Spring.io website. It’s built by Pivotal Labs and maintained and extended by Brian Clozel. The project uses a best-of-breed set of tools. In Brian Clozel’s talk he sheds a light on which tools are used for which reasons. After the talk I sit down with Brian to discuss some more details.Sagan: A production Spring reference applicationSpeaker: Brian Clozel - Talk &amp; speaker descriptionSagan is the name of the Spring.io website. It’s built by Pivotal Labs and maintained and extended by Brian Clozel. The project uses a best-of-breed set of tools. Of course it uses GitHub as code repository and issue tracking. But GitHub can become a bit confusing and unclear when a lot of issues need to be tackled and tracked. Sagan uses Waffle to link GitHub issues and commits to scrum &amp; kanban practices. Formal communication goes through GitHub issues, and informal conversations are held through HipChat. Travis is used for continuous deployments. Asciidoctor is used for the guides on the website. They are stored in a GitHub repository, fetched by the website and rendered appropriately.Sagan is using a Gradle plugin which is triggering Green-Blue deployment. It calls Cloud Foundry to see which clone is active (green or blue) and deploys on the non-active one. Once deployment is done, the routing is switched automatically to the newly deployed one. In the short moment where the switch occurs, both clones are being routed to avoid a brief moment of downtime.Cloud Foundry takes the console log output of each application and aggregates everything. Either the result is exposed using webockets or it’s bound using a service that can show the logs as well as persist them. Redis is used in conjunction with Spring Session. This creates distributed session management for the cloud. Whenever the database needs to be updated, versioned FlyWayDB scripts are used. The upgrades could be small changes where local tests are sufficient, or a staging environment is used for testing.The slidedeck of this talk can be found here: https://speakerdeck.com/bclozel/spring-dot-io-1One on one talk with Brian ClozelThe Netflix guys have a strong implementation of Conway’s law. The organisation mimics the architecture and vice versa. This closely relates to the microservices pattern, where each microservice can have the best tools for the job, and those can definitely be different for each microservice. In Netflix this applies to teams as well, resulting in different approaches suggested by different people.Regarding database evolution Brian suggests the Netflix guys could have some good ideas about handling backwards-compatibility breaking evolution in a green-blue deployment with zero downtime. In any case there are many cases where teams use feature-switches to make certain user interactions read-only for a short time during migration. This avoids having transactions that have to be forcibly destroyed. After successful deployment the feature is turned on again ensuring the user has a seamless experience.Brian will check with the Netflix guys and let me know what they do for database migration"
      },
    
      "spring-2015-05-08-springio15-microservices-html": {
        "title": "&quot;Bootiful&quot; Microservices in Cloud Foundry &amp; One on One with Josh Long",
        "url": "/spring/2015/05/08/SpringIO15-Microservices.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Spring Boot, Spring Cloud and Cloud Foundry: a perfect match. Josh Long explains how to build Spring Boot microservices, deploy them in Cloud Foundry and manage them using the Netflix OSS stack through Spring Cloud. Including a One on One talk.Can Your Cloud Do This - Cloud Foundry talkSpeaker: Josh Long - Talk &amp; speaker descriptionIn Amazon there’s a good chance you’ll encounter “AMIs”. These are basically virtualizations with an operating system, and is perceived as a container. These containers need to be disposable. The moment you remove one, another should be ready to jump in. The idea is to treat your servers as cattle, not as pets. The moment you know the name of a specific server, it’s as if it’s your pet. And you don’t want pets because they’re not disposable.When choosing a cloud platform, it’s important to avoid vendor lock-in. At Google AppEngine, there was a huge community developing applications for the platform before it was even in GA. Once they got there, Google raised the prices significantly. The problem was however that all those applications were using Google-specific APIs and were really tightly coupled to the Google infrastructure. For most companies it was no longer viable to use Google’s platform without incurring debts and were unable to quickly move to another platform. Josh compares it nicely with Hotel California: “You can always check-in, but you can never leave”.It’s possible to deploy a jar to Cloud Foundry, but also a war. The war will be automatically wrapped in a container (using Warden, the linux container, which makes it similar to a docker image), but you can also push your own docker image with your war inside.Once you pushed your application into Cloud Foundry, you have to link it to backend services, such as a database. Doing this is child’s play. One cool backend service in particular is the logging service. You can use Splunk or Papertrail, which you can bind to e.g. your own account on the Papertrail site.An older version of the slidedeck of this talk can be found here: https://speakerdeck.com/joshlong/scaling-with-spring-and-cloud-foundryBuilding “Bootiful” Microservices with Spring Cloud (Workshop)The configprops actuator endpoint is especially useful to figure out what properties are available for certain functionalities. It beats debugging and is a pretty useful form of documentation. For more information check this out: http://docs.spring.io/spring-boot/docs/current/reference/html/howto-properties-and-configuration.html#howto-discover-build-in-options-for-external-propertiesThe configuration server supports configuration which is common to all microservices. If the configuration.yml is inside the resources folder of the configuration server itself, it’s used only as configuration for the configuration server. But if the server finds an application.yml inside the distributed configuration location (e.g. Git), then those configurations are shared for all other yml files, albeit with lowest priority.The Cloud Configuration Bus is interesting if you would like to have a distributed refresh of the configuration of your microservices. Basically instead of POSTing to http://yourmicroservice/refresh, you’d call http://yourmicroservice/bus/refresh on any clone and Spring Cloud Configuration Bus will forward the refresh using AMQP to all the other clones. This way you don’t need to call a refresh on each node separately.It’s also possible to poll for changes instead of pushing them. Simply inject the refresh endpoint as a bean and annotate it with the @Scheduled annotation.Since the 1.0.1 release of Spring Cloud, it’s now even possible to have the client microservice wait and retry until a configuration server is registered. (https://GitHub.com/spring-cloud/spring-cloud-config/issues/129)A cool new annotation is @SpringCloudApplication. It groups the following annotations: @SpringBootApplication, @EnableDiscoveryClient and @EnableCircuitBreaker.The high availability principle in Eureka is considered fulfilled when at least two Eureka instances are registered and peer aware in each zone.Ribbon, the client-side load balancer of Netflix, is integrated nicely into Spring’s RestTemplate. When EurekaClient is enabled on the microservice, Ribbon will be able to resolve the following command: restTemplate.exchange(“http://myservice/books”, …). It will look into Eureka for an application called myservice, and route to the appropriate server.The slidedeck of this talk can be found here: https://speakerdeck.com/joshlong/the-bootiful-microserviceOne on One talk with Josh LongIn many cases there are different access rules for different environments. This applies in a great deal on configuration as well. Most guides showcase one configuration server with all properties for all environments into a single repository. When facing more strict policies, there is no reason why you can’t have a configuration server per environment, or at least for production a dedicated one. That server could have different access rules. It’s an easy solution worth trying out.When you have both public and private microservices, Josh prefers to do the extra security for the private ones on the microservice level instead of in the gateway. It’s important to trust the developers, especially in a devops culture which is a requirement in a microservices architecture.At the company I work, management fears that the developers will introduce security issues, so the enterprise architects are looking into more governance-minded solutions. One of those solutions is having a full blown ESB as the gateway. Such an ESB would require you to register new endpoints - in case of REST these would be resources with their allowed verbs - and different types of handlers and interceptors. This is a very process-heavy solution where the microservice would have to request access through the ESB for every new resource, or verb on a resource, probably to another team in charge of the ESB.This goes completely against the microservices principles. The ESB becomes a bottleneck and increases the time-to-market. It also becomes a heavy single point of failure with lots of logic inside. Josh makes a fitting comparison with conductor versus choreographer. If your gateway is an ESB it acts as an conductor. When the conductor goes offline, the entire orchestra fails. When a choreographer drops out, the dancers can still independently continue. The power lies in the individual units, instead of one governing entity. Microservices need to be in charge of their own decision-making instead of an ESB and its separate team.We’re also facing strong opposition from the infrastructure team against the embedded-container approach. It’s probably not surprising they’re trying to push JEE standards as they’re heavily invested in a certain application server. Josh argues the battle between Spring and JEE was won a long time ago, and by a big margin. Case in point is the new exciting feature of JEE 8: to marshal and unmarshal data between POJOs and JSON. Spring had support for this via Jackson integration already three years ago. Currently there are almost no application servers that implemented JEE 7. It will probably take another five years or so until JEE 8 will be adopted in application servers. That means basic JSON binding on POJO support will only be available eight years after Spring started supporting it. Do you really want to wait that long for such a vital feature? Or should we still use XML-only?Cloud Foundry can be pretty expensive since it’s primarily aimed towards big enterprises. For other purposes, definitely check out the free OSS stack.Pivotal has a division called Pivotal Labs. It’s a super agile company that only takes clients that align with their agile views. They have a proven trackrecord that is pretty much unmatched and allows them to stick to their values. Although super expensive, they’re fast and deliver quality. Aside from taking projects on, the also help companies become more agile.Aside from Eureka, Spring Cloud also supports other registries such as Zookeeper and Consul. The difference between for example Eureka and Zookeeper is that Zookeeper is a shared hierarchical name space of data registers (also called registers znodes). It can be used very well as a service registry, but it offers a lot more features on top of that. One easy example is leader election. Due to the Spring Cloud annotation abstractions it’s possible to switch out Eureka with any other supported service registry."
      },
    
      "spring-2015-05-08-springio15-autoconfig-html": {
        "title": "Master Spring Boot auto-configuration",
        "url": "/spring/2015/05/08/SpringIO15-Autoconfig.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Spring boot allows you to extend its convention-over-configuration approach by creating your own autoconfigurations. There are some important details you shouldn’t forget.Master Spring Boot auto-configurationSpeaker: Stéphane NicollIn order to create your own autoconfiguration, it’s important to remember the spring.factories file in the META-INF folder of the autoconfiguration project.The autoconfiguration class itself should have @Configuration (of course) and utilise conditional annotations as much as possible. Especially on the bean initializations the rule of thumb is the more conditionals the better. This enables users of your autoconfiguration to override specific elements of the autoconfiguration class. Aside from fully overriding beans, you can also expose properties under your own namespace. Together both these concepts allow small configuration-based modifications and bigger bean-overriding modifications by the user.Spring offers many types of Conditional annotations. The regular ConditionalOnClass or ConditionalOnMissingClass and ConditionalOnBean or ConditionalOnMissingBean are the most common, but also ConditionalOnProperty, OnResource, OnExpression and others are possible. You can even have nested conditions and for the most specific needs you can always write your own conditional annotation with whatever logic you require.By looking how Spring’s autoconfiguration classes are built, it’s easy to figure out how to do it yourself. Especially the ConfigurationProperties are pretty straight forward once you see an example. Don’t forget to put the @EnableConfigurationProperties annotation on your autoconfiguration class however.To expose your configurationproperties to IntelliJ, the trick is in the maven dependencies. You should add the “spring-boot-configuration-processor” dependency. This will generate metadata regarding the properties of your autoconfiguration which IntelliJ uses. Once that’s done, IntelliJ will also autocomplete your properties in the property files.Support for yml configuration autocompletion in IntelliJ is coming by the way.Once your autoconfiguration class is created, it’s a good idea to bundle it into a maven module that can be used as a dependency by other projects. It’s important to use the recommended naming convention for your modules - especially if you’re building autoconfiguration for the community - or it might clash with the modules from spring boot itself. The recommended naming is xyz-spring-boot-autoconfigure and xyz-spring-boot-starter. The former module should contain the autoconfiguration class (and don’t forget the spring.factories file), and the latter should contain the recommended dependencies to enable the autoconfiguration. That way the user can independently have the autoconfiguration and the classes that enable it. The starter is entirely optional though.The spring.factories file should contain your configuration like this: a key being org.springframework.boot.autoconfigure.EnableAutoConfiguration with value the qualified name of your autoconfiguration class.To further allow the user to customize your autoconfiguration, you can expose a customize hook into your autoconfiguration. This should accept an xyzConfigCustomizer interface (you can create whatever interface you want basically) with a single customize method. This customize hook is executed after the autoconfiguration is executed, but before actual instantiation of the beans. The user then just has to create a bean that implements the customizer interface. An example is available on GitHub.One final concern of autoconfiguration is the order in which they are executed. This applies to conditions inside the autoconfiguration but also to combined autoconfigurations. Always make sure the cheapest condition comes first. So expensive SpEL expression conditions should come after conditionalOnBeans. Between autoconfigurations you can use either the annotation @AutoConfigureBefore and After on the autoconfiguration class, or the @Order annotation. When no order or before or after condition is specified, there is no guarantee when the autoconfiguration will be executed.Conditions are being executed in two phases: PARSE_CONFIGURATION phase and REGISTER_BEAN phase. PARSE_CONFIGURATION evaluates the condition when the @Configuration-annotated class is parsed. This gives a chance to fully exclude the configuration class. REGISTER_BEAN evaluates the condition when a bean from a configuration class is registered. This does not prevent the configuration class to be added but it allows to skip a bean definition if the condition does not match (as defined by the matches method of the Condition interface).The slidedeck of this talk can be found here: https://speakerdeck.com/snicoll/master-spring-boot-auto-configuration"
      },
    
    
      "jobs-001-cloud-engineer": {
        "title": "Cloud Engineer",
        "url": "/jobs/001-cloud-engineer/",
        "image": "/img/jobs/cloud-engineer.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                                            As a cloud engineer, you are multi-deployable.                    You feel at home in an agile development team or platform team.                    Within our agile development team you are jointly responsible for building features with business impact.                                                    In addition, you will strengthen the team with your expertise in automating the Software Delivery Lifecycle.                    This translates into, among other things, setting up CI/CD pipelines,                    application management via monitoring and logging,                    and configuration of the cloud infrastructure.                                                    If you are part of a platform team, you will create the outlines for cloud infrastructure on AWS.                    You will support multiple development teams through best practices, guidelines, tooling and support.                    You promote the DevOps values and culture! You build it, you run it!                                                    Who we're looking for                    Strong affinity with cloud-based provisioning, monitoring, troubleshooting and related cloud automation technologies is a must                    You have experience with IaaS, PaaS and cloud-specific SaaS technologies                    You have experience with containerization and orchestration tools such as Docker and Kubernetes                    You have experience with CI/CD tools such as AWS CodeBuild, Github Actions/Azure DevOps, ArgoCD, GitLab                    You have experience working in an agile environment                    Experience with Infrastructure as Code solutions such as Terraform, Pulumi, CloudFormation or CDK is a plus                    Experience with CNCF landscape tooling such as Helm, Thanos, Prometheus, Jaeger and OpenTelemetry is a plus                    Open source mindset and Linux experience are a plus                    You are genuinely enthusiastic about the latest technologies                                                                                What we offer                                    You contribute to challenging projects at  top international and Belgian companies from various sectors                    You work in a high-tech environment with a focus on knowledge and innovation, an open no-nonsense corporate culture with room for own initiative                    You are part of a vibrant community where knowledge sharing is central through blog posts, workshops, conferences and training courses                    You keep your technical and social skills up-to-date at our Ordina Academy, and you are supported in obtaining certificates                    You will receive a competitive salary package with additional benefits such as a company car, laptop, insurance package, meal vouchers, expense account, etc.                    You can participate in fun business and team-building activities                    You can optimize your net salary thanks to our IP reward                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            Anja.VanAcker@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-002-java-developer": {
        "title": "Java Developer",
        "url": "/jobs/002-java-developer/",
        "image": "/img/jobs/java-engineer.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                                            As a Java developer Java has no secrets for you, you also know the Spring stack through and through.                    You can participate in a scrum team to deliver custom software in an iterative way according to the requirements of the customer.                    You are proactive and have a can-do mentality. You participate in building and maintaining fast, secure and maintainable applications.                                                    Who we are looking for                    To be successful in the Jworks team you have a passion for all things Java.                    To take the Java developer position you will also bring at least 2 years of relevant work experience.                    This means you know Java and the Spring Stack (Spring Boot, Spring MVC, Spring Data, Spring Cloud, etc.. ) inside out,                    and you also have experience with automated testing: unit testing, integration testing and acceptance testing.                                                    Those are the hard requirements. Some nice to haves include knowledge of cloud platforms like AWS or Azure.                    You're also comfortable with methodologies like DevOps, Scrum and Kanban.                                                                                What we offer                                    You contribute to challenging projects at  top international and Belgian companies from various sectors                    You work in a high-tech environment with a focus on knowledge and innovation, an open no-nonsense corporate culture with room for own initiative                    You are part of a vibrant community where knowledge sharing is central through blog posts, workshops, conferences and training courses                    You keep your technical and social skills up-to-date at our Ordina Academy, and you are supported in obtaining certificates                    You will receive a competitive salary package with additional benefits such as a company car, laptop, insurance package, meal vouchers, expense account, etc.                    You can participate in fun business and team-building activities                    You can optimize your net salary thanks to our IP reward                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            Anja.VanAcker@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-003-software-architect": {
        "title": "Software Architect",
        "url": "/jobs/003-software-architect/",
        "image": "/img/jobs/arch.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                                            As a software architect you play a key role in the realization of projects,                    where architecture is the differentiator. You create high-level project designs that support and steer the client's strategy.                    You follow the latest trends and developments within your domain and you can make recommendations on how to adopt them.                    You share your knowledge with colleagues in competence centers on topics s.a. architecture, cloud and security.                    You do not back down for pre-sales activities and you use your expertise to win an offer.                                                    Who we're looking for                    You are passionate about technology and you've at least 5 years experience                    You have a technical background in the Java landscape, containerization & orchestration, and cloud platforms such as AWS                    You have a deep knowledge of development methodologies, standards, technical risks and architectural patterns: event-driven, cloud-native architecture, n-tier, onion architecture, ...                    You have experience with IaaS, PaaS and cloud-specific SaaS technologies                    You have experience with containerization and orchestration tools such as Docker and Kubernetes                    You have experience with CI/CD tools such as AWS CodeBuild, Github Actions/Azure DevOps, ArgoCD, GitLab                    You have experience with automated testing and the setup of delivery pipelines.                    You have a passion for coaching people and guiding a team                    You will support people and teams throughout the agile process - as a servant leader                    You have experience with application modernization or integration pattern                    You speak fluent Dutch (or French) and English                                                                                What we offer                                    You contribute to challenging projects at  top international and Belgian companies from various sectors                    You work in a high-tech environment with a focus on knowledge and innovation, an open no-nonsense corporate culture with room for own initiative                    You are part of a vibrant community where knowledge sharing is central through blog posts, workshops, conferences and training courses                    You keep your technical and social skills up-to-date at our Ordina Academy, and you are supported in obtaining certificates                    You will receive a competitive salary package with additional benefits such as a company car, laptop, insurance package, meal vouchers, expense account, etc.                    You can participate in fun business and team-building activities                    You can optimize your net salary thanks to our IP reward                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            Anja.VanAcker@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-052-technical-analyst": {
        "title": "Technical Analyst",
        "url": "/jobs/052-technical-analyst/",
        "image": "/img/jobs/technical-analyst.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                                            As a  technical analyst  at Ordina, your responsibility goes beyond just mapping out the customer's needs and expectations.                    You also support the product owner and scrum masters in writing user stories, and help the software engineers in writing technical specifications.                                                    You follow up the development process and take responsibility for the end-to-end tests.                    In short, you are multi-purpose within a scrum team.                    You believe in servant leadership and you communicate efficiently with all stakeholders.                                                    Who are we looking for ?                    You have at least 3 years of relevant work experience                    You have a technical background, following topics will ring a bell: Jira, XML, XSD, BPMN, UML, Json, SQL and Java                    You have experience with writing end-to-end tests, knowledge of Cypress or Selenium is an added value                    You have knowledge of requirement analysis and design concepts for OO, multi-tier and web-enabled, and/or UML environments                     You feel the needs of your stakeholders and you can translate them into clear requirements                    You have experience with agile and scrum                    You are a negotiator who can quickly defuse conflicts                    You have experience in managing a team                    You are assertive and like to share your ideas with your team                    You are flexible when it comes to your job and you like new challenges                    You can communicate in Dutch and English, French is a plus                                                                                What we offer                                    You contribute to challenging projects at  top international and Belgian companies in various sectors                    You work in a high-tech environment with focus on knowledge and innovation, an open no-nonsense company culture with room for own initiative                    You will be part of  JWorks, a dynamic full stack unit within Ordina                    Standing still is going backwards, together we work out a personal growth path                    You keep your technical and social skills up-to-date at our  Ordina Academy  and are supported in obtaining certificates so you always stay ahead in your profession.                      You can participate in our competence center workshops to keep your knowledge up-to-date or to expand it.                    You receive a competitive salary package with additional benefits such as a company car, laptop, insurance package, meal vouchers, expense account, etc.                    You can participate in fun business and team building activities                    You can optimize your net salary thanks to our IP-reward                                                    With the Technical Leadership Practice within JWorks, we go beyond development and ensure that we bring projects to a successful conclusion in Ordina teams.                    We do this by teaching colleagues skills in various areas including: servant leadership, agile practitioner, pre-sales, networking and customer interaction.                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            Anja.VanAcker@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-053-development-lead": {
        "title": "Development Lead",
        "url": "/jobs/053-development-lead/",
        "image": "/img/jobs/lead-developer.jpeg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                                            As development lead at Ordina, your responsibilities go beyond building software applications..                    Thinking along with the software architect in designing the technology landscape and coaching and supporting the software engineers are part of your responsibilities.                                                    It goes without saying that you have experience in managing a team and believe in servant leadership.                    You are a preacher of clean code and clean architecture and expect the same from your team.                                 As an all-round professional you can be flexibly deployed for all kinds of tasks within a scrum team.                    You like to participate in pre-sales activities and use your technical expertise to win a bid.                                                    Who are we looking for ?                    You have at least 5 years of relevant experience as a developer                    You are a born negotiator who can quickly defuse conflicts                      You communicate clearly, assertively and efficiently with all your stakeholders                     You are familiar with cloud-native development, microservices, NoSQL, single page applications, reactive programming and domain-driven design                     You have the necessary technical baggage and communication skills to discuss software requirements with the customer                     You have knowledge of Spring Core, Spring Boot, Angular, React-Test-Driven Development and tools like Git, Jenkins, Sonar and Maven.                      You've already been exposed to Cloud platforms like AWS and Azure                     You are a fan of DevOps, agile and scrum                      You can communicate in Dutch and English, French is a plus                                                                                 What we offer                                    You contribute to challenging projects at  top international and Belgian companies in various sectors                    You work in a high-tech environment with focus on knowledge and innovation, an open no-nonsense company culture with room for own initiative                    You will be part of JWorks,  a dynamic full stack unit within Ordina                     Standing still is going backwards, together we work out a personal growth path                    You keep your technical and social skills up-to-date at our Ordina Academy and are supported in obtaining certificates so you always stay ahead in your profession                     You can participate in our competence center workshops to keep your knowledge up-to-date or to expand it.                    You can participate in fun business and team building activities                    You can optimize your net salary thanks to our IP-reward                                                    With the Technical Leadership Practice within JWorks, we go beyond development and ensure that we bring projects to a successful conclusion in Ordina teams.                    We do this by teaching colleagues skills in various areas including: servant leadership, agile practitioner, pre-sales, networking and customer interaction.                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            Anja.VanAcker@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-unsolicited-application": {
        "title": "Unsolicited application",
        "url": "/jobs/unsolicited-application/",
        "image": "/img/jobs/unsolicited-application.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Want to suprise us with your talents?                                    While all the listed positions are about a specific role, we are aware that certain other profiles might come in handy.                    JWorks is always on the look-out for like-minded individuals with the right drive to reinforce the area.                    Tell us about yourself and about the role that you are looking for, and let's get to know each other.                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            Anja.VanAcker@ordina.be                                                                                                                "
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
        </div>
    </section>
</div>


<div id="over"></div>


<footer>
    <div class="contact">
        <div class="address">
            <div class="icon"><i class="fa fa-fw fa-home"></i></div>
            <div class="text">Ordina Belgium<br/>Blarenberglaan 3B,<br/>2800 Mechelen, Belgium</div>
        </div>
        <div class="phone">
            <div class="icon"><i class="fa fa-fw fa-phone"></i></div>
            <div class="text"><a href="tel:003215295858">+32 15 29 58 58</a></div>
        </div>
        <div class="email">
            <div class="icon"><i class="fa fa-fw fa-envelope-o"></i></div>
            <div class="text"><a href="mailto:jworks@ordina.be">jworks@ordina.be</a></div>
        </div>
    </div>
    <ul class="social">
        <li>
            <a href="https://twitter.com/lifeatordinabe" target="_blank">
                <i class="fa fa-fw fa-twitter"></i><span>Twitter</span>
            </a>
        </li>
        <li>
            <a href="https://www.facebook.com/lifeatordinabe" target="_blank">
                <i class=" fa fa-fw fa-facebook"></i><span>Facebook</span>
            </a>
        </li>
        <li>
            <a href="https://www.linkedin.com/company/ordinabelgium" target="_blank">
                <i class=" fa fa-fw fa-linkedin"></i><span>LinkedIn</span>
            </a>
        </li>
        <li>
            <a href="/youtube" target="_blank">
                <i class=" fa fa-fw fa-youtube"></i><span>YouTube</span>
            </a>
        </li>
        <li>
            <a href="/github" target="_blank">
                <i class=" fa fa-fw fa-github"></i><span>GitHub</span>
            </a>
        </li>
        <li>
            <a href="/feed.xml" target="_blank">
                <i class=" fa fa-fw fa-rss"></i><span>RSS Feed</span>
            </a>
        </li>
    </ul>
    <div class="copyright">
        &copy; 2022 Ordina JWorks. All rights reserved.
        <br /> Disclaimer: Opinions expressed on this blog reflect the writer's views and not the position of Ordina
        <img id="analyticsImg" src="" width="1" height="1" style="border: 0px"/>
    </div>
</footer>
<!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.magnific-popup.min.js"></script>
<script src="/js/owl.carousel.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/jquery.pin.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]>
<script src="/js/ie/respond.min.js"></script><![endif]-->



<script src="/js/main.js"></script>

<!-- Google Analytics -->
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o);
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m);
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-70040502-1', 'auto');
    ga('send', 'pageview');
</script>
<script type="text/javascript">
    window.addEventListener('load', function () {
        if (window.ga && ga.create) {
            console.log('GA loaded correctly');
        } else {
            console.log('GA is blocked or failed to load - tracking manually...')
            document.getElementById('analyticsImg').src = 'https://jworks-techblog-analytics.cfapps.io/collect?title=' + document.title;
        }
    }, false);
</script>

<!-- Vertical timeline -->
<script src="/js/vertical-timeline/modernizr.js"></script>
<script src="/js/vertical-timeline/main.js"></script>


</body>
</html>
