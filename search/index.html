<!DOCTYPE html>
<!--suppress ALL -->
<html>
<head>
        <meta charset="utf-8">
    <meta name="description" content="We build innovative solutions with Java and JavaScript. To support this mission, we have several Competence Centers. From within those Competence Centers, we provide coaching to the employee and expert advice towards our customer. In order to keep in sync with the latest technologies and the latest trends, we frequently visit conferences around the globe.
">
    <meta name="keywords" content="Ordina,ORAJ,JWorks,Blog,Java,JavaScript,TypeScript,Angular,DevOps">
    <meta name="author" content="Ordina Belgium">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/img/favicons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/img/favicons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/img/favicons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/img/favicons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/img/favicons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/img/favicons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/img/favicons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/img/favicons/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="/img/favicons/favicon-128.png" sizes="128x128" />

    

    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Search">
    <meta name="twitter:description" content="Ordina JWorks Tech Blog">
    <meta name="twitter:image" content="http://ordina-jworks.github.io/img/jworks/jworks-400x400.png"/>

    <!-- Facebook Open Graph -->
    <meta property="og:url" content="http://ordina-jworks.github.io/search/"/>
    <meta property="og:type" content="article"/>
    <meta property="og:title" content="Search"/>
    <meta property="og:description" content="Ordina JWorks Tech Blog"/>
    <meta property="og:image" content="http://ordina-jworks.github.io/img/jworks/jworks-1200x630.png"/>

    
        
        <title>Search &mdash; Ordina JWorks Tech Blog</title>
        
    

        <!-- Styles -->
    <!-- Font awesome CSS -->
    <link href="/css/font-awesome.min.css" rel="stylesheet">
    <!-- Magnific Popup -->
    <link href="/css/magnific-popup.css" rel="stylesheet">
    <!-- Owl carousel -->
    <link href="/css/owl.carousel.css" rel="stylesheet">

    <!-- CSS for this page -->

    <!-- Syntax highlighting -->
    <link href="/css/syntax.css" rel="stylesheet">


    <!--[if lte IE 8]>
    <script src="/js/ie/html5shiv.js"></script><![endif]-->
    <link rel="stylesheet" href="/css/main.css"/>
    <!--[if lte IE 9]>
    <link rel="stylesheet" href="/css/ie9.css"/><![endif]-->
    <!--[if lte IE 8]>
    <link rel="stylesheet" href="/css/ie8.css"/><![endif]-->

    <!-- Custom CSS. Type your CSS code in custom.css file -->
    <link href="/css/custom.css" rel="stylesheet">

    <!-- Vertical timeline -->
	<link rel="stylesheet" href="/css/vertical-timeline/style.css"> <!-- Resource style -->

    <!-- Favicon -->
    <link rel="shortcut icon" href="#">

    <script src="https://use.typekit.net/gzh0mbm.js"></script>
    <script>
        try {
            Typekit.load({async: true});
        } catch (e) {
        }
    </script>
</head>

<body>
<div id="header-image"></div>


<!-- Page Wrapper -->
<div id="page-wrapper">

        <!-- Header -->
    <header id="header">
        <h1>
            <a href="/">
                JWorks Tech Blog
            </a>
        </h1>

        <nav>
            <a href="#menu">Menu</a>
            <form action="/search/" method="get" class="header-search search-form">
                <input type="text" id="search-box-header" name="query" placeholder="Search &hellip;">
            </form>
        </nav>
    </header>

    <!-- Menu -->
    <nav id="menu">
        <div class="inner">
            <h2>Menu</h2>
            <ul class="links">
                <li class="menu-item"><a href="/">Home</a></li>
                <li class="menu-item"><a href="/about">About Us</a></li>
                <li class="menu-item"><a href="/jobs">Jobs</a></li>
                <li class="menu-item"><a href="/launch-your-career">Career</a></li>
                <li class="menu-item"><a href="/tech-radar">Tech Radar</a></li>
                <li class="menu-item"><a href="/search">Search</a></li>
                <li class="menu-item"><a href="/contact">Contact Us</a></li>
            </ul>
            <a href="#" class="close">Close</a>
        </div>
    </nav>

    <section id="banner">
        <header>
            <div class="inner">
                

                <h2>Search</h2>
                

                

                

            </div>
        </header>
    </section>

    <section id="wrapper">
        <div class="inner">
            <section class="wrapper spotlight style1 search-page">
    <div class="inner">
      <div class="content">
        <h2 class="major">
          Search for
          <form action="/search/" method="get" class="search-form">
            <input type="text" id="search-box" name="query">
          </form>
        </h2>
        <div class="spinner-container" style="width: 100%; height: fit-content; text-align: center;">
          <i id="search-spinner" class="fa fa-spinner fa-pulse fa-3x"></i>
        </div>
        <section id="search-results" class="features search-results">
        </section>
      </div>
    </div>
</section>

<script>
  window.store = {
    
      "agile-2019-04-04-the-scrum-framework-is-a-liberating-structure-html": {
        "title": "The Scrum framework is a Liberating Structure",
        "url": "/agile/2019/04/04/The-Scrum-framework-is-a-liberating-structure.html",
        "image": "/img/scrum-is-a-liberating-structure/main-image.png",
        "date": "04 Apr 2019",
        "category": "post, blog post, blog",
        "content": "Scrum is an Agile framework.What does that actually mean?Are we supposed to fill it? Could we grab it, put it on the wall and paint our image in it, the way we see it, the way it works for us?Probably, we can. Seemingly, many organizations attempt to connect the numbers and dots in precisely that way, trying to get a hold on their portfolio planning and utilising Scrum for metrics to fill their project reports - solely focusing on accelerating output and pushing velocity through the roof. But let’s be careful, since we can easily get trapped in this. A framework isn’t there to just fill it and use it. Did we take the time or the effort to ponder over further possibilities that the frame might entail?The framework is not meant to contain just our own painting, our own image, whatever it is that is known to us. It is there to draw attention to something beyond… like the frame on the photo. Let it be a window. A transparent artifact, always representing a mere part of reality, depending on the spot from where we are and from where we are looking, not at it but through it, giving us an opportunity to inspect what we are spending our efforts on, or maybe… what we are wasting them on?The power of the Scrum framework is in its simplicity. The predictively recurring Scrum events form a synchronized pulse creating a steady heartbeat for the members of the Scrum team. The drone is persistently present, but only discretely so, not drawing attention, because the frame is not what it’s about. It’s a minimalist frame, that vanishes after a while, and is merely setting the scene… trying to create a fertile setting for a space in which, as soon as the Scrum team is accustomed to the pattern of the heartbeat, all team members can focus - as one organism - on collaboration, innovation and co-creation.Interhuman friction due to role shifts, adapting to new responsibilities and accountabilities, as well as difficulties with alignment of expectations are well-known characteristics of a team’s storming phase. But Tuckman, with his forming, storming, norming and performing model of team dynamics was only partly right. We now know that storming will always be there to some degree, accompanying the complexity and quickly-changing environment of today. A team, in fact, is constantly hovering over its storming arrhythmia, longing for it to clear up, using the heartbeat of the framework to gradually pave the road to high performance, focusing on value and gradual improvement.Indeed, the Scrum framework is a ‘liberating structure’ in every sense. Agreeing as a team on the structure to work with - and taking up one of the roles and its respective responsibilities therein, provides a clarity on what to expect and how to cooperate within that constraint. The frame and heartbeat might appear to be limiting structures at first, but once applied as intended, prove to provide liberation – within the safety of knowing that the entire team is referencing through the same framework.Metaphorically: Imagine a team looking at the sea and the framework is not there. What are we looking at? What are we seeing? What area of the horizon are we exploring?It will be hard to be aligned unless someone or something tells us where to look and what to look for. Then put the frame, the window, on the beach. All of us are looking through it – together, exploring a far bigger wedge of the sea than we ever could imagine doing alone.At the same time it is enabling us to jointly aim for what could be beyond the horizon, when we all look in the same direction.Mind you, it is just a framework. “Metaphors are all nice and well, but what about the real world?” - you might dryly remark. Some teams indeed never really achieve this ‘selfless’ wavelength of high performance. And Scrum as a mere facilitative tool is certainly not to blame for that. Some teams tap into it rather effortlessly. But it requires a common understanding of the framework - and the discipline to jointly be accountable for its success. Furthermore, and most dauntingly, it builds on human trust, as an unconditional recipe to create psychological safety.Just like the Scrum master serves the team to enable every team member to be at her/his best, the Scrum framework is there to create the best possible circumstances for generating flexible value creation in a transparent and predictive way.Browsing through the Liberating Structures list of Henri Lipmanowicz and Keith McCandless1, it is easy to notice a parallel and detect some clear common attributes:  A Liberating Structure is simple to introduce. Just like Scrum they are easy to learn but can be hard to master. Having a good facilitator is a must.  They are result focused. Not used for the sake of it, only for the extra value it generates.  They involve rapid cycling, meaning fast iterative rounds generating input and feedback.  They are inclusive, asking everyone’s opinion to make informed decisions on the way forward.  They are seriously fun and boost a sense of freedom and responsibility within a group.These attributes help us to:  Share and spread vital knowledge  Cope with complexity  Include every member of the team and unleash their potential  Adopt a habit of creative adaptability  Promote anti-fragilityAnd on top of that, Scrum - being an Agile practice strongly rooted in Lean principles - reintroduces systems thinking and the routine of improvement in the process of co-creation.If there is one principle in the world that should never change, let it be this one.  “Stay agile, never change” - Adam Weisbart2References  1 For a full but ever growing list of these liberating structures, visit http://www.liberatingstructures.com/ls-menu/ and http://www.liberatingstructures.com/ls-in-development/  2 The epic quote by Adam Weisbart, concluding every podcast: see https://weisbart.com/agile-answers/"
      },
    
      "kickstarters-2019-04-02-kickstarter-trajectory-2019-light-html": {
        "title": "Kickstarter Trajectory 2019 Light Edition",
        "url": "/kickstarters/2019/04/02/Kickstarter-Trajectory-2019-Light.html",
        "image": "/img/kicks.png",
        "date": "02 Apr 2019",
        "category": "post, blog post, blog",
        "content": "IntroductionWe started this kickstarter trajectory with four kickstarters.Jago was freshly graduated from school, where as Giel and Yolan already had working experience in IT.Seppe had multiple years of working experience in Digital Signage but made a career change and was also new to IT.The main goals of the kickstarter course was to give every kickstarter a knowledge foundation of the best practices within JWorks and to introduce them to the IT world.First dayMorningOn the first day there, we were welcomed by Robbe Struys and Angela Gallo.They gave us the basic information about the HR working of Ordina.After receiving the keys to our new car and our laptop they showed us how to work with Ordina Connect.We made our first CV and filled in our first timesheet entry.They toured us around the office and introduced us to our future colleagues.They were very friendly and they all said that we made the right choice.This was of course very nice to hear and put us at ease.We had brunch together and then we had a group picture as well as our profile pictures taken.AfternoonWith every developer comes his/her personal development environment.To help us pick the best tools to suite our needs, we had help from Kevin Van den Abeele.He showed us the best IDEs for each language and best practices as to what we can do to improve our development experience.GitA tool all developers use is Version Control.At Ordina we prefer to use Git (this is preferred almost everywhere, who even uses SVN anymore?).So we learned to use Git, the best practices to get the best out of it and all this is done in the terminal of course.If you want to use a GUI for Git, they recommended GitKraken.Over the whole kickstarter traject, we would use Git to get our code examples and presentations.We went over good practices and learned by doing this hands-on on our own machines.Creating our own repositories, branching, merging, … .Yannick,our teacher for this course, was very clear to avoid spaghetti history by rebasing and squashing your commits to give a clean linear overview that is readable by your co-workers.DockerAs the era of containerization is rising, it only feels right to teach us the fundamentals about it and the importance of Docker in a project.That’s why they asked Tom Verelst to give us a detailed presentation about the mystical power of Docker.After the first introduction, we were soon ‘dockerizing’ our first full-stack application.We also combined everything together with Docker Compose, which made us start our whole full-stack application with just one command!The session gave us an overview as to how Docker is used in the real world, and we couldn’t wait to use an orchestration framework to deploy our containers into the cloud!DatabasesOn day 3, Tom Van den Bulck, Competence Lead in Big and Fast Data, gave us a course on SQL and NoSQL database systems.As some of us were not familiar with NoSQL, this was very interesting to see the difference in usage and possibilities between normal SQL systems which we were all used to using before.SQLFor SQL database systems we had a look at PostgreSQL, an open-source object-relational database management system that is increasing in popularity across bigger enterprises for reasons such as high scalability, extensive features and as it works cross-platform.NoSQL      Redis    Redis is an open-source key-value store that runs in-memory.Used where consistency and durability is less important than speed.        Cassandra    Cassandra is an open-source wide column store.Distributed across different nodes for high accessibility and low chance of downtime.        MongoDB  MongoDB is a document-oriented database system. Data in MongoDB does not need to be consistent and the data structure can change over time.      Neo4j  Neo4j is a graph database management system. No index is required and data with a lot of relations to other data can be accessed faster when dealing with higher amounts.Reactive programming with RxJSA course given by Orjan De Smet covering reactive programming, the advantages it brings and how and where to use it and how to use it in combination with unit testing.In short, reactive programming offers a solution to handling asynchronous calls with multiple events.Which means it offers more than one channel of communication so multi-step events can be handled efficiently.When coding in a traditional way you will often end up with a lot more code, could run into problems when for example a promise clogs a thread or you could end up with a mess of callbacks making your code extremely hard to read.DevOps and Continuous IntegrationAn introduction to DevOps &amp; CI given by Tim Vierbergen explaining this way of working and how it increases the productivity of a team.We also covered some best practices considering version control, building, testing and deploying with an example project to get a bit more familiar with the software used to do this.Software such as Git for version control, Jenkins for building, Jasmine for testing and Docker/Spinnaker for deploying.Security PrinciplesIn this presentation we went over the basics on how to protect your application and the user data it stores from malicious intent.We went over some good practices regarding the storage of data and the verification of your users.For example the hashing of passwords, enabling 2-factor authentication and deciding on the amount of allowed invalid login attempts before issuing a timeout.All of these things should be decided using a mix of guidelines and common sense.Clean CodePieter Van Hees gave us a course of clean code, this course was not focussed on writing new code but improving the way you write the code.Improvements:  Readability  Maintainability  Avoid rewritesThe biggest enemy of clean code is pressure, so Pieter advised us to take our time to write clean code.During this course we also did some exercises through public code katas available on the internet.This course only gave an introduction and he recommended us to read the book Clean Code by Robert Cecil Martin.Frontend Build tools, testing, package managers and moreThis course was led by Michael Vervloet, who is a full stack JavaScript/TypeScript developer at Ordina.He gave us the know-how on the building process, serving your application and doing this in an optimized way.He also showed us to use generators and build tools to create a whole lot of code and files in the terminal.The main topics of this course were Node.js, package managers and ways to build &amp; generate your code (gulp, webpack and Angular CLI).We went over them one by one and got the chance to test and install them on our machines to get a hands-on experience.In the end, we created an Angular application from scratch and played around with the generator to make some components and serving them to look at our work.Java Language FeaturesJava is a pretty popular language in the backend development world, and is our preferred backend language here at JWorks.That’s why Yannick De Turck explained us the newest features of Java versions 7, 8, 9, 10 and 11.Java 8 is currently the most used Java version.Yannick prepared some exercises for us so we could focus on the newest Java 8 features (lambdas, streams, optionals, …).One of the most useful features that Java 10 introduced is the ‘var’ keyword.How great is it that you don’t have to specify the type twice during the initialization of an object!?Java 11 is the newest LTS version, so it was important for us to get a detailed explanation about its newest changes and features.Other than that, there were a lot of extra useful features that will certainly be nice to have once we can use them.It was very entertaining to get a quick overview as to what is new, what is being removed or deprecated and what we can or should expect in the coming Java versions.Spring FrameworkFor a framework this big and popular, we followed a three-day course given by Ken Coenen.On the first day, we received a brief explanation as to how the Spring magic works behind the scenes (dependency injection, beans, …).We saw the basics of the most common components of the full Spring Framework such as Cloud, Security, … .On the second day, we dived into the magic behind Spring Boot.It’s remarkable how much Spring Boot does for you without any configuration needed, although you can fully configure Spring Boot to your needs and satisfactions.On the third day, Ken did a live coding session and created a Spring Boot application from scratch and explained how to fully initialize your Spring Boot project and get the most out of it through various steps and always showing the best practices for each implementation.Full House during the third dayOf course, afterwards we had some time to relax after three days of exploring the Spring Framework.We closed our three-day session on Friday with the best combination: pizza and beer!Pizza &amp; beer!Unit Testing and mocking in JavaWe got an introduction to Unit Testing in Java from Maarten Casteels.The red line:  Goals of Testing  What to test  Fixtures  Mocks  AssertionsIn the morning we got a very interactive theory session where we learned how important testing really is, the basics and what it all stands for.In the afternoon we learned to unit test our code, the best way to do this, how to mock dependencies, use fixtures and a whole lot more.Maarten also showed us the most common pitfalls to avoid, and some best practices like test-driven development (TDD) and how writing tests can help you with refactoring your code and lastly look at it with a different vision.For lunch we went to a place called Meals On Wheels were we were introduced to a whole other world of sandwiches.Once you’ve been there you will know what we mean by that, don’t go too often though.KubernetesKubernetes is an open source container orchestration framework which was first introduced to us by Tom Verelst during the kickstarter traject.It is made by Google and is now maintained by the Cloud Native Computing Foundation.First they introduced us to all the features that Kubernetes possesses (service discovery, horizontal scaling, load balancing, …).Soon we learned how to deploy Docker containers in the cloud by using Kubernetes, and afterwards we had an hands-on exercise where we could deploy a full-stack application to a Kubernetes cluster using Minikube.It’s wonderful how you can deploy a full-stack application through Kubernetes with just one configuration file needed.Of course, it takes some time to get used to it, but once you get the hang of it, you can do outstanding stuff with this platform!Cloud Providers &amp; PlatformsTo get a bigger picture of all the cloud providers and platforms that are out there conquering the IT world, we had a dedicated session about this topic given by Bas Moorkens and Dieter Hubau.Bas was focusing on Amazon Web Services and all its features that it has to offer.We quickly learned that AWS was very advanced and had lots of components to facilitate the life of a developer.It was a very interesting session and made me realise that AWS was a big part of the development scene.We are eager to use it and learn more of what is has to offer.As for cloud platforms, we got a very detailed explanation of how OpenShift (made by RedHat) works and what its features and options are.We also got a high-level explanation as to how an application in the cloud works and what the best practices are to achieve deploying your application in the cloud.Overall, it was a very interesting session for cloud-enthusiasts and we definitely want to learn more about it!TypeScriptAfter the session HTLML5, CSS3, JavaScript, Dimitri De Kerf learned us the benefits of TypeScript. He told us some benefits of using TypeScript instead of JavaScript.TypeScript is a wrapper around JavaScript, which means it has more methods to make your daily programming more pleasant.It also adds optional static typing for richer IDE autocomplete support.Dimitri De Kerf showed us how to configure our project to use TypeScript and to use these features.He explained us that it is important to know how to use TypeScript because it is used in popular frameworks like Angular and React.AngularRyan De Gruyter was our teacher for today.He quickly introduced us to Angular, a platform that is designed to easily create browser applications built by Google. The first version of Angular was AngularJS.It was very popular and used by many companies.Google decided to update Angular and created Angular 2 which was not welcomed by the industry at that time because it removed all the AngularJS concepts.It took some time for the industry to adapt and see the positive stuff of Angular 2: Open Source community, improved dependency injection, better performance, etc..Angular 2 is not the holy grail of frameworks. It still has some downsides like lots of ceremony and boilerplate thanks to the use of Angular CLI.After the information session, he showed us how easy it was to create an Angular project where we learned how to create an Angular application using small reusable pieces called components.Frontend hands-onJan De Wilde asked if we still had question about the Angular session. Because in this session we would create an Angular application using all the techniques we learned from the previous course and he wanted to be sure we understood everything before we started.So he went a bit deeper on some topics and showed us on how to execute calls to an API and to structure the project properly.After the lunch break, when we were still digesting our food, we started to write a complete Angular application. Jan De Wilde guided us through the process, showing us possible issues we could encounter and explained how we could solve those issues.Intro to Cloud-Friendly DevelopmentKevin Van Houtte introduced us to contract testing.It is a tool to write an exact input and output of an API call.After we run our project, our contract will generate tests for our controller, checking if the controller output is the same as we expected in the contract.The files, generated by the contract, can be imported into Javadoc for documentation.Afterwards we had some exercises where we could use all the skills we had learned in these courses.  API driven programming with contract tests.  Loading the API docs into our Java docs  Attaching a database to our Spring Boot application  Creating migration scripts and using these to populate the database with FlyWay  Creating a config server and connecting our Spring Boot application to it  Enabling actuator and using it to generate metrics dataAll these exercises help us prepare for a real project in the future.Agile DevelopmentTogether with Michaëla Broeckx, Practice Manager Agile, we saw different approaches to work as a non-agile team.Like the waterfall system that has some downsides such as getting late feedback from the business or end user.The feedback is only in the end of the life cycle of the project or when the project got tested.Applying an Agile approach offers a lot of benefits:  Quicker risk reduction  limit handovers  shorter term plans          to improve predictability, accuracy &amp; reliability.      to redone stress and unleash innovative emergent ideas        and so on!She proved her theory by doing a live exercise which involved folding paper airplanes as a team.At the end we would analyze the outcome.After this we learned some other Agile practices: we got introduced into the SCRUM framework and the practice of Extreme Programming, plus its benefits.The new JWorks colleagues  "
      },
    
      "iot-20smart-20tech-20smart-20glasses-20augmented-20reality-2019-04-01-vuzix-blade-html": {
        "title": "The Vuzix Blade",
        "url": "/iot,%20smart%20tech,%20smart%20glasses,%20augmented%20reality/2019/04/01/vuzix-blade.html",
        "image": "/img/2019-03-31-vuzix/banner.jpg",
        "date": "01 Apr 2019",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  The hardware  The software  Using the Vuzix  Developing for the Vuzix  Looking forward  ResourcesIntroductionAs we are strong believers in the potential that Augmented Reality has to offer in terms of business cases, we were wanting to get our hands on some actual hardware.We acquired some budget and went looking for ‘affordable’ smart glasses to experiment with.We came across the Vuzix Blade on their website and looked through the details and videos posted.Thanks to Ordina, we could preorder one unit.    It was a video like this one that had won us over to give the Vuzix Blade a try since the displayed features look very nice, if they all worked as promised…After a very long wait, with several delays and a lot of mailing back and forth, we got ourselves a pre-production handbuilt Vuzix Blade.We got these glasses to analyze the wearer’s experience and see how we could integrate it into the numerous business cases we could see it being used for:  Assistance for field technicians  Order picking  Communications platformsIn this blogpost we’ll go a bit into detail what makes the Vuzix Blade tick and how our experience with it has been so far.Read on ahead for all the juicy details!The hardwareThe Vuzix Blade is essentially an Android smartphone you can wear on your face.Well actually, it’s really more like an Android smartwatch you can strap to your face, but you get the idea.The device we received was a pre-production build, which was assembled by hand.This means we can’t really say much about what the final hardware will look like, if there will be any changes or if the build quality, which was very solid, will change.During our testing it has been through some light and heavy action, like daily office use, running, biking, etc, and still hasn’t shown any faults.We’ve have always found the idea of computing devices in the form factor of glasses quite intriguing as some of us have been cursed with nearsightedness and we have to wear prescription glasses constantly.If we have to wear the bloody things every day, might as well put some intelligence into them.Below you can find some specs about the device, but for us these are quite irrelevant for the moment.This device is all about showcasing innovation in two areas: form factor and display technology.And boy are we impressed.The glasses actually feel comfortable enough to wear for longer periods and the display technology is quite amazing!It’s not HoloLens levels of crazy but still very very good!But let’s get down the mandatory spec overview!The internals inside the glasses are alright, maybe a bit underwhelming.But it’s always a fine line to balance between power consumption and battery life.  Projected display resolution of 480 by 853 pixels  Quad core ARM A53 CPU  WiFi, Bluetooth  8MP camera up to 1080p video recording  470mAh batteryThe amount of RAM is not specified but seems to be just the right amount to get the job done.Overall, the device works but the speed and fluidity could be better, although this was improved significantly with the latest software update.There is no audio on the device as no regular or bone conducting speaker is present.Audio can be provided through either Bluetooth or USB audio, but an included speaker would have been nicer.Initially the video recording only supported up to 720p at a lower frame rate, which with the lack of OIS was not very usable in high motion scenarios.However, the latest software update added support for 1080p recording and as you can see in one of the videos down below is actually acceptable.All of this is actually quite irrelevant to us.There is no innovation in fitting a better camera or having oodles of computing power on the device.The technological marvel in this device is the display technology, named the Cobra Display Engine.It’s difficult to explain how well this works.So we’ll just rip off the movie “Contact” and say:  No words to describe, they should have sent a poet.So beautiful! I had no idea.The best description we could think of so far is:It’s like someone is following you from behind with a projector and is projecting the user interface on an invisible screen in front of you.Hold a smartwatch right in front of you in a readable position.It’s kinda like that, but transparent and without losing the functionality of one of your arms.So instead of describing it to people, we just put it on their face and they just are immediately captivated by what they’re experiencing.It takes a moment to learn how to switch your eyes’s focus on the heads-up display and back to your surroundings.Once you master, this it becomes very natural to interact with the display.Seeing that transparent interface is something a lot of people really have to take time to wrap their heads around.After this we show them some pretty pictures with a variety of colors.Really brings everything to life and shows off the unexpectedly good visual qualities of the display.For a concept device it really shows what the technology is capable of.It’s capable of running basic Android apps.The same ones you would expect to run on a smartwatch.Enough computing power to handle the video calling.Running TensorFlow Lite to do object detection and classification proved to be a bit too challenging.The softwareThe Vuzix Blade runs on Android 5.1.Due to the limited screen real estate of the device, the look and feel of the apps reminds a lot of smartwatch apps.There aren’t many out-of-the-box apps on the device installed:  Welcome dashboard  Camera  Gallery  Music control  SettingsOne of the most important features of wearables is notification mirroring, which works out-of-the-box.With the Vuzix Blade also comes a companion app for your Android or iOS smartphone.This companion app allows you to configure settings, fetch images and videos from the device, manage installed apps and explore the Blade app store.As this device doesn’t run the Google Play Store, a specific app store is needed.This app store allows Vuzix specific apps to be installed on the device.                                                                  Using the VuzixThe thing we like about the Blade is how comfortable it is to wear compared to other head-mounted wearable solutions like the HoloLens.The HoloLens is quite heavy and in our opinion not meant to be worn all day long.The Blade however is light enough to stay comfortable for long time wearing.Although Vuzix targets the Blade partially at the consumer market, we believe that there is much more potential in the enterprise market.Let’s hope they don’t make the same mistake Google made with Google Glass!But because they also target the consumer market, they thought about very important things like ergonomics and making for non-tech people.                                                Our colleague, Frederick tested the device for a longer period of time:  Sometimes I wear this device for a full day to get deeply immersed in the experience.As it is comfortable to wear, this wasn’t much of an issue.  My first experiment was to check how many would look funny at me during my morning commute.The good news is that during my train ride and walk around the office, not many people were or kept staring at me.However, the people that knew me asked what I had on my face.    The interaction models are quite straightforward.It’s a good platform to consume push content.Your screen lights up, you get your info, the screen dims.If you want to actually interact with the app, you can use the touchpad located near your right temple.Using gestures like:  Swipes          Up      Down      Left      Right        Two finger swipes  Tap  Double tap  Long tap  etcAgain, very similar to smartwatches.Support for Amazon Alexa is currently in a Beta program for which we’ve signed up.Really wondering how natural this voice interaction will be.As we said before, some of us wear glasses and the Blade display is readable when you have only minor nearsightedness, but the display is much sharper when you put the Blade on top of your regular glasses.For an additional markup it is possible to get prescription lenses with the Blade so people who wear glasses on daily basis can also use this device.Battery life is very much inline with smartwatches: It all depends on the usage.We can easily keep an app running with the screen being on for about two hours.If you are only consuming (push) notifications it’s possible to stretch this to a full day.For longer and more intensive usage an external battery pack is a must.Luckily it’s quite non-intrusive to equip a battery pack by using the USB port located on the side.Once you do this, battery life is not an issue anymore.We did some testing and actually went running and cycling while wearing an external battery pack and did not experience any hinder at all.Developing for the VuzixDeveloping for the Blade is just like developing for any Android device.You just develop in Android Studio, like you would normally do.This means Vuzix can leverage the huge amount of Android devs out there.Our Android devs found the learning curve to be relatively low.You do need to take into account that the Blade comes with its own design guidelines and UI components.The interaction model and how apps are structured is quite elegant and straightforward, no surprises here!Just import two Blade specific libraries with the components and you’re good to go.No other dependencies are needed!There is no Blade emulator available, but Vuzix has added support for the Android Studio design view.Although the layout of most screens will be very basic, it was still very handy to quickly prototype UIs.We brainstormed a bit about what would be a good app to leverage the innovative aspects of the Blade.As Frederick was recently training to regain his once athletic body, he bought a Polar H10 heart rate sensor which can connect to a smartphone using Bluetooth Low Energy (BLE).A lot of runners already have smartwatches to monitor their heart rate. Some of these watches even vibrate when you’re not running in the correct heart rate zone.More info on heart rate zones can be found here.Although runners already have access to this information on their watch, it’s not the best form factor to consume the data.Ever tried reading your watch while running and bouncing around at 10+ km/h?Having to shift your focus like this just completely gets you out of “the zone”.We thought this was a good showcase of the capabilities of the Blade: easily consume the information you need, enabling you to make the best decisions, while being as non-intrusive as possible.Because Polar implements the official Heart Rate device specification it was very straightforward to set up a BLE connection between the sensor and the Blade.Every second or so the BLE device pushes an update of the current heart rate to the BLE client.After tapping into this stream of sensor data, it wasn’t too difficult to build the app.Currently we only display the current time, heart rate and heart rate zone.The video below showcases the app.The user interface is still very minimalistic and the app itself is still a work in progress.However, it’s already very functional.    The video doesn’t do the app justice as you don’t get to experience the transparent display, allowing you to see the world around you.While experimenting with new technologies, we prefer to use the Minimal Viable Product (MVP) approach: focus on what brings most value and then validate this as soon as possible.This means field testing the concept in the most representative and harsh environment you can think of.Frederick ventured forth to a place where not many developer dare venture: outdoor in the sun.And Frederick did this by taking the Blade for a 10km run.In a future version of the app, we would like to add things like:  Average heart rate  Max heart rate  Calories burnedWith the latest software upgrade we can also tap into the GPS data from the smartphone via the companion app.This will allow us to also display things like: current speed, max speed, average speed, distance travelled, etc.It will be an interesting challenge getting all this data on the rather small display.This is something we will probably outsource to our UX / UI wizkids over at ClockWork.Looking forwardWhat we got with the Vuzix Blade looks already very promising even though there are a few small rough edges.Vuzix rolled out a big software update for the device that included new features like Alexa support and improved camera performance.If and when better battery technology is available these devices can have a definite positive influence on certain business cases.With the introduction of the second generation HoloLens from Microsoft it is clear what can be possible with these devices.Although the HoloLens is a much, much more complicated product than the Vuzix Blade, both serve a different market population.For now, we believe that devices like the Blade have the upper hand over others like the HoloLens, in price, wearer comfort, and day to day usability.Resources  Vuzix Blade Smart Glasses  Vuzix product videos  Vuzix app store  Microsoft HoloLens 2"
      },
    
      "cloud-2019-03-28-building-with-google-cloud-build-html": {
        "title": "Building with Google Cloud Build",
        "url": "/cloud/2019/03/28/Building-With-Google-Cloud-Build.html",
        "image": "/img/2019-03-28-cloudbuild/cloudbuild.png",
        "date": "28 Mar 2019",
        "category": "post, blog post, blog",
        "content": "In this post, we will have a quick overview on what is possible with Google Cloud Build.Google Cloud Build is a fully managed solution for building containers or other artifacts.It can integrate with Google Storage, Cloud Source Repositories, GitHub and BitBucket.A simple YAML fileWe can easily set up a build pipeline using a YAML file which we store in our source code repository.Each build step is defined using a container image and passing arguments to it.Here is an example:steps:  # Test Helm templates  - name: 'quay.io/helmpack/chart-testing:v2.2.0'    id: 'Helm Lint'    args: ['ct', 'lint', '--all', '--chart-dirs', '/workspace/helm', '--validate-maintainers=false']  # Build image  - name: 'gcr.io/cloud-builders/docker'    id: 'Building image'    args: ['build', '-t', 'eu.gcr.io/$PROJECT_ID/cloud-build-demo:$COMMIT_SHA', '.']  # Create custom image tag and write to file /workspace/_TAG  - name: 'ubuntu'    id: 'Setup'    args: ['bash', '-c', \"echo `echo $BRANCH_NAME | sed 's,/,-,g' | awk '{print tolower($0)}'`_$(date -u +%Y%m%dT%H%M)_$SHORT_SHA &gt; _TAG; echo $(cat _TAG)\"]  # Tag image with custom tag  - name: 'gcr.io/cloud-builders/docker'    id: 'Tagging image'    entrypoint: '/bin/bash'    args: ['-c', \"docker tag eu.gcr.io/$PROJECT_ID/cloud-build-demo:$COMMIT_SHA eu.gcr.io/$PROJECT_ID/ms-map-report:$(cat _TAG)\"]images: ['eu.gcr.io/$PROJECT_ID/cloud-build-demo']timeout: 15moptions:  machineType: 'N1_HIGHCPU_8'We are free to use any image that we like.Cloud Build already provides a set of base images (called Cloud Builders),including images for Maven, Git, Docker, Bazel, npm, gcloud, kubectl, etc.We can also customise some build options like the timeout of the build,or on which kind of node the build runs.Pricing is done based on the amount of build minutes. However, if we use the default node, the first 120 build minutes are free every day!If the build finishes successfully,Cloud Build will automatically upload the built images to the container registry.This is based on the images defined in the images array.Data usually needs be shared between steps.We might want to download dependencies in one step,and build your artifact in another step,or run tests in a separate step.Google has provided a simple solution for this.Each build step has access to the /workspace folder, which is mounted on the container of each step.Each build has access to its own workspace folder,which is deleted automatically after the build finishes.In the above example, a custom Docker tag is created and saved to the /workspace/_TAG file,and then read from again in the next step.To start the build, we can use the gcloud builds submit command,or create an automatic trigger on the Google Cloud console that triggers the build on new commits in the Git repository.After adding a trigger, we can also trigger the build manually in the Google Cloud console.Build parameters (substitutions)It is possible to pass in parameters (called substitutions) to our build.We can override substitutions when submitting a build: $ gcloud builds submit --config=cloudbuild.yaml \\     --substitutions=TAG_NAME=\"test\"Cloud Build provides the following default substitutions:  $PROJECT_ID: build.ProjectId  $BUILD_ID: build.BuildId  $COMMIT_SHA: build.SourceProvenance.ResolvedRepoSource.Revision.CommitSha (only available for triggered builds)  $SHORT_SHA : The first seven characters of COMMIT_SHA (only available for triggered builds)  $REPO_NAME: build.Source.RepoSource.RepoName (only available for triggered builds)  $BRANCH_NAME: build.Source.RepoSource.Revision.BranchName (only available for triggered builds)  $TAG_NAME: build.Source.RepoSource.Revision.TagName (only available for triggered builds)  $REVISION_ID: build.SourceProvenance.ResolvedRepoSource.Revision.CommitSha (only available for triggered builds)We can use substitions to define our own custom parameters.Note that the name of the substitution must start with an underscore (_),and can only use uppercase alphanumeric characters. Example:substitutions:    _CUSTOM_PARAM_1: foo # default value    _CUSTOM_PARAM_2: bar # default valueimages: [    'gcr.io/$PROJECT_ID/myapp-${_CUSTOM_PARAM_1}',    'gcr.io/$PROJECT_ID/myapp-${_CUSTOM_PARAM_2}']Securing your buildIf we require to use credentials in our builds,it is possible to do this securely using Google Cloud Key Management Service (KMS).We will not go into how to use and to setup KMS,but once we have set it up,we can start encrypting our build secrets.First, we will need to give Cloud Build access to KMS by adding the Cloud KMS CryptoKey Decrypter roleto our ...@cloudbuild.gserviceaccount.com service account.Encrypt our secret with KMS:$ gcloud kms encrypt \\  --plaintext-file=secrets.json \\  --ciphertext-file=secrets.json.enc \\  --location=global \\  --keyring=[KEYRING-NAME] \\  --key=[KEY-NAME]This will create an encrypted file which we can add to our application’s source code.Using KMS, we can decrypt this secret in our Cloud Build pipeline:steps:- name: gcr.io/cloud-builders/gcloud  args:  - kms  - decrypt  - --ciphertext-file=secrets.json.enc  - --plaintext-file=secrets.json  - --location=global  - --keyring=[KEYRING-NAME]  - --key=[KEY-NAME]This will decrypt the secret into a file in our workspace folder,which then can be used in subsequent steps.Debugging and running your build locallyWhen creating a build pipeline, we do not need to keep pushing our code to the source repository to trigger a build.We can use the cloud-build-local tool to run our build locally,using the Google Cloud SDK and Docker.If we are using the Cloud Builder images (gcr.io/cloud-builders/...),we must first configure our Google Cloud SDK to be able to pull the images:# Configure Docker$ gcloud components install docker-credential-gcr$ docker-credential-gcr configure-dockerThen install the cloud-build-local tool:$ gcloud components install cloud-build-localNow we can use the tool to test our build pipeline locally!To build locally, we run the following command:$ cloud-build-local --config=[CONFIG FILE] \\  --dryrun=false \\  --push \\  [SOURCE_CODE]  CONFIG FILE is our Cloud Build YAML config file  SOURCE_CODE is the path to our source code  --dryrun=false will cause our build to actually run. This is true by default and we must enable this explicitly to cause the containers to execute.  --push will cause the built images defined in images to be pushed to the registry.If we use some of the default substitions like $COMMIT_SHA in our build,we must pass these in with the --substitions flag in key=value pairs,separated by commas.Example: $ cloud-build-local --config=cloud-build.yaml \\   --dryrun=false \\   --substitutions COMMIT_SHA=$(git rev-parse HEAD),BRANCH_NAME=$(git rev-parse  --abbrev-ref HEAD) \\    /path/to/sourceCloud Build stores intermediary artifacts in the workspace folder.This workspace folder, as mentioned before,will be removed after the build finishes.If we want to debug our build and check what happened in the workspace folder,then we can copy the artifacts to a path on our computer,using the --write-workspace flag.Note that this path must reside outside of our source folder!$ cloud-build-local --config=cloud-build.yaml \\   --dryrun=false \\   --write-workspace=/path/on/computer \\   /path/to/sourceBuild eventsIt is possible to trigger other actions when a build starts, finishes, or fails.Notifications to our team’s chat,triggering a deployment pipeline,monitoring our build. These are just a few examples.Cloud Build pushes build events to Pub/Sub on the cloud-builds topic.This topic is created automatically when Cloud Build is used.We can easily create a subscription on this topic. There are two kinds of subscriptions we can use.The first one is a push subscription, which pushes the message to a HTTP endpoint you define.In this case messages are delivered the moment the event is published on the topic.{  \"message\": {    \"attributes\": {      \"buildId\": \"abcd-efgh...\",      \"status\": \"SUCCESS\"    },    \"data\": \"SGVsbG8gQ2xvdWQgUHViL1N1YiEgSGVyZSBpcyBteSBtZXNzYWdlIQ==\",    \"message_id\": \"136969346945\"  },  \"subscription\": \"projects/myproject/subscriptions/mysubscription\"}Messages that are received using a pull subscription have the following format:{  \"receivedMessages\": [    {      \"ackId\": \"dQNNHlAbEGEIBERNK0EPKVgUWQYyODM2LwgRHFEZDDsLRk1SK...\",      \"message\": {        \"attributes\": {          \"buildId\": \"abcd-efgh-...\",          \"status\": \"SUCCESS\"        },        \"data\": \"SGVsbG8gQ2xvdWQgUHViL1N1YiEgSGVyZSBpcyBteSBtZXNzYWdlIQ==\",        \"messageId\": \"19917247034\"      }    }  ]}Each message contains the Base64 encoded event of the Build resource.Here is an example:{  \"id\": \"a0e322f2-5d8d-4d56-a2b5-05cc18a350af\",  \"projectId\": \"myproject\",  \"status\": \"SUCCESS\",  \"source\": {    \"repoSource\": {      \"projectId\": \"myproject\",      \"repoName\": \"mygitrepo\",      \"branchName\": \"feature/my-branch\"    }  },  \"steps\": [    {      \"name\": \"gcr.io/cloud-builders/mvn\",      \"args\": [        \"mvn\",        \"clean\",        \"--batch-mode\"      ],      \"id\": \"Clean\",      \"timing\": {        \"startTime\": \"2019-03-23T15:01:25.421160679Z\",        \"endTime\": \"2019-03-23T15:02:04.363792008Z\"      },      \"pullTiming\": {        \"startTime\": \"2019-03-23T15:01:25.421160679Z\",        \"endTime\": \"2019-03-23T15:01:59.834114283Z\"      },      \"status\": \"SUCCESS\"    },    ... More steps  ],  \"results\": {    \"images\": [      {        \"name\": \"eu.gcr.io/myproject/myapp:d76cce6d732e6edc01e65a547997caf107411468\",        \"digest\": \"sha256:0bb2f72d3d267c6bfebee8478d06dbf553d5932e01a0b86b7fc298c3a9b4a1f2\",        \"pushTiming\": {          \"startTime\": \"2019-03-23T15:15:58.377229824Z\",          \"endTime\": \"2019-03-23T15:16:01.908997933Z\"        }      }    ],    \"buildStepImages\": [      \"\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:dbc62a5cd330fba4d092d83f64218f310ee1a61bdb49d889728091756bc38bac\",      \"sha256:d30ca59f3315232f539955a6179f2b287445ec56db41e7d7a41a622c9faee575\",      \"sha256:d30ca59f3315232f539955a6179f2b287445ec56db41e7d7a41a622c9faee575\",      \"sha256:d30ca59f3315232f539955a6179f2b287445ec56db41e7d7a41a622c9faee575\"    ],    \"buildStepOutputs\": []  },  \"createTime\": \"2019-03-23T15:01:16.591984806Z\",  \"startTime\": \"2019-03-23T15:01:17.438509785Z\",  \"finishTime\": \"2019-03-23T15:16:02.968224Z\",  \"timeout\": \"1800s\",  \"images\": [    \"eu.gcr.io/myproject/myapp:d76cce6d732e6edc01e65a547997caf107411468\"  ],  \"artifacts\": {    \"images\": [      \"eu.gcr.io/myproject/myapp:d76cce6d732e6edc01e65a547997caf107411468\"    ]  },  \"logsBucket\": \"gs://199957373521.cloudbuild-logs.googleusercontent.com\",  \"sourceProvenance\": {    \"resolvedRepoSource\": {      \"projectId\": \"mateco-map\",      \"repoName\": \"bitbucket_matecocloud_myapp\",      \"commitSha\": \"d76cce6d732e6edc01e65a547997caf107411468\"    }  },  \"buildTriggerId\": \"9bd093c7-9de4-4eae-bfea-ce8e46afafa8\",  \"options\": {    \"substitutionOption\": \"ALLOW_LOOSE\",    \"logging\": \"LEGACY\"  },  \"logUrl\": \"https://console.cloud.google.com/gcr/builds/a0e322f2-5c8d-4e56-a2b5-05cc18a350af?project=199957373521\",  \"substitutions\": {    \"_MOD_BRANCH_NAME\": \"$_tmpvar\"  },  \"tags\": [    \"event-f2d96d7b-22f5-41d7-9ded-a98a2a6f43ca\",    \"trigger-9bd093c7-9de4-4eae-bfea-ce8e46afafa8\"  ],  \"timing\": {    \"BUILD\": {      \"startTime\": \"2019-03-23T15:01:25.421114358Z\",      \"endTime\": \"2019-03-23T15:15:58.377209942Z\"    },    \"FETCHSOURCE\": {      \"startTime\": \"2019-03-23T15:01:20.519103589Z\",      \"endTime\": \"2019-03-23T15:01:25.368505523Z\"    },    \"PUSH\": {      \"startTime\": \"2019-03-23T15:15:58.377226850Z\",      \"endTime\": \"2019-03-23T15:16:01.909032379Z\"    }  }}Note that Cloud Build does not publish events between steps, but only when the build is queued, starts or ends.            Event      Build status                  The build is queued      QUEUED              The build starts      WORKING              The build is successful      SUCCESS              Build is cancelled      CANCELLED              Build times out      TIMEOUT              Step times out      TIMEOUT              Build failed      FAILURE              Internal error by Google Cloud Build      INTERNAL_ERROR      Using Google Cloud Function, we can easily trigger other actions based on these build events.Here is a small, redacted snippet of a Google Cloud Functionwhich sends build updates to a Slack webhook.It receives the build event, reads the Base64 encoded data,converts it into a Slack message and triggers the webhook with the created message.const IncomingWebhook = require('@slack/client').IncomingWebhook;const SLACK_WEBHOOK_URL = \"https://hooks.slack.com/services/XXXXXXXXXXXXXX\";const WEBHOOK = new IncomingWebHook(SLACK_WEBHOOK_URL);// Main function called by Cloud Functions.module.exports.cloudBuildSlack = (event, callback) =&gt; {    const build = eventToBuild(event.data.data);    WEBHOOK.send(createSlackMessage(build), callback);};    const createSlackMessage = (build) =&gt; {    const app = getApplicationName(build);    const branch = build.source.repoSource.branchName;    const subject = createSubject(build);    const tag = getImagetag(build);    return {        attachments: [{            fallback: `${subject} - ${app} - ${branch} - &lt;${build.logUrl}|Logs&gt;`,            title: subject,            title_link: build.logUrl,            fields: getFields(app, branch, tag),            color: getMessageColor(build)        }],        mrkdwn: true    };};// eventToBuild transforms pubsub event message to a build object.const eventToBuild = (data) =&gt; {    return JSON.parse(new Buffer(data, 'base64').toString());};...more functions    ConclusionCloud Build offers a simple solution and utilises the power of containers to offer a lot of possibilities.A build pipeline is set up in a few minutes, and your Docker images are uploaded automatically!It saves you a lot of time and trouble in setting up build infrastructure, because, well, you do not have to!If you wish to try it yourself,we have provided a demo application on GitHub.Enjoy Cloud Building!"
      },
    
      "streaming-2019-03-25-streaming-traffic-data-html": {
        "title": "Streaming Traffic Data with Spring Kafka &amp; Apache Storm",
        "url": "/streaming/2019/03/25/streaming-traffic-data.html",
        "image": "/img/2018-08-08-streaming-traffic-data/traffic.png",
        "date": "25 Mar 2019",
        "category": "post, blog post, blog",
        "content": "  Earlier I did a workshop at Ordina in order to introduce my colleagues to the wonderful world of stream processing.For that workshop I used traffic data, since especially in Belgium, traffic data is something everybody can easily relate to as we all have to endure it every workday.Table of content  Introduction  The Data  Native Java Stream Processing  Kafka Streams with Spring Kafka  Apache Storm  ConclusionIntroductionIn this blog post we will use traffic data made available by the Flemish government.Several examples will be provided about how this data can be processed in various ways:  Transform the data into events with Spring Cloud Stream  Do some stream processing using some plain old Java, the native way  Process these events with Kafka Streams via Spring Kafka  Do similar processing with Apache StormThe DataThe traffic data is registered on fixed sensors installed in the road itself.General information about the sensors can be retrieved from http://miv.opendata.belfla.be/miv/configuratie/xml.    &lt;meetpunt unieke_id=\"3640\"&gt;        &lt;beschrijvende_id&gt;H291L10&lt;/beschrijvende_id&gt;        &lt;volledige_naam&gt;Parking Kruibeke&lt;/volledige_naam&gt;        &lt;Ident_8&gt;A0140002&lt;/Ident_8&gt;        &lt;lve_nr&gt;437&lt;/lve_nr&gt;        &lt;Kmp_Rsys&gt;94,695&lt;/Kmp_Rsys&gt;        &lt;Rijstrook&gt;R10&lt;/Rijstrook&gt;        &lt;X_coord_EPSG_31370&gt;144477,0917&lt;/X_coord_EPSG_31370&gt;        &lt;Y_coord_EPSG_31370&gt;208290,6237&lt;/Y_coord_EPSG_31370&gt;        &lt;lengtegraad_EPSG_4326&gt;4,289767347&lt;/lengtegraad_EPSG_4326&gt;        &lt;breedtegraad_EPSG_4326&gt;51,18458196&lt;/breedtegraad_EPSG_4326&gt;    &lt;/meetpunt&gt;It is pretty static as these sensors do not tend to move themselves.Every minute the latest sensor output is published on http://miv.opendata.belfla.be/miv/verkeersdata.This is one big XML file containing all the aggregated data of every sensor of the last minute.    &lt;meetpunt beschrijvende_id=\"H211L10\" unieke_id=\"1152\"&gt;        &lt;lve_nr&gt;177&lt;/lve_nr&gt;        &lt;tijd_waarneming&gt;2017-11-20T16:08:00+01:00&lt;/tijd_waarneming&gt;        &lt;tijd_laatst_gewijzigd&gt;2017-11-20T16:09:28+01:00&lt;/tijd_laatst_gewijzigd&gt;        &lt;actueel_publicatie&gt;1&lt;/actueel_publicatie&gt;        &lt;beschikbaar&gt;1&lt;/beschikbaar&gt;        &lt;defect&gt;0&lt;/defect&gt;        &lt;geldig&gt;0&lt;/geldig&gt;        &lt;meetdata klasse_id=\"1\"&gt;            &lt;verkeersintensiteit&gt;0&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;0&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;252&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"2\"&gt;            &lt;verkeersintensiteit&gt;6&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;116&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;113&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"3\"&gt;            &lt;verkeersintensiteit&gt;1&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;118&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;118&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"4\"&gt;            &lt;verkeersintensiteit&gt;3&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;84&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;84&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;meetdata klasse_id=\"5\"&gt;            &lt;verkeersintensiteit&gt;5&lt;/verkeersintensiteit&gt;            &lt;voertuigsnelheid_rekenkundig&gt;84&lt;/voertuigsnelheid_rekenkundig&gt;            &lt;voertuigsnelheid_harmonisch&gt;84&lt;/voertuigsnelheid_harmonisch&gt;        &lt;/meetdata&gt;        &lt;rekendata&gt;            &lt;bezettingsgraad&gt;9&lt;/bezettingsgraad&gt;            &lt;beschikbaarheidsgraad&gt;100&lt;/beschikbaarheidsgraad&gt;            &lt;onrustigheid&gt;366&lt;/onrustigheid&gt;        &lt;/rekendata&gt;For more information (in Dutch) about this dataset you can go to https://data.gov.be/nl/dataset/7a4c24dc-d3db-460a-b73b-cf748ecb25dc.Over there you will also find the XSD files describing the XML structure.Transform to EventsSince I am using Spring Boot to kickstart the application, you can go to https://start.spring.io/ to get started.Some handy baseline dependencies to get started are: Web, Actuator and DevTools.Because the data is provided in a single XML file, we will transform it into separate events per sensor.This brings it also inline with how true sensory events would arrive within our system if we would not be dealing with a big XML file.A small Spring Cloud Stream application will be built to read in the XML, transform it to events and push these events to a Kafka topic.You might wonder, why would we use Spring Cloud Stream for this?It makes it very easy to read/write messages to Kafka with it.Add the appropriate starter:    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;    &lt;/dependency&gt;Define a Spring Boot application - make sure to enable scheduling.    @SpringBootApplication    @EnableScheduling    @EnableBinding({Channels.class})    public class OpenDataTrafficApplication {        public static void main(String[] args) {            SpringApplication.run(OpenDataTrafficApplication.class, args);        }    }Define some input and output topics.    public interface Channels {        @Input        SubscribableChannel trafficEvents();        @Output        MessageChannel trafficEventsOutput();        @Output        MessageChannel sensorDataOutput();    }Create a bean to read in the events.    public List&lt;TrafficEvent&gt; readInData() throws Exception {        log.info(\"Will read in data from \" + url);        JAXBContext jc = JAXBContext.newInstance(\"generated.traffic\");        Unmarshaller um = jc.createUnmarshaller();        Miv miv = (Miv) um.unmarshal(new URL(url).openStream());        log.info(\" This data is from \" + miv.getTijdPublicatie().toGregorianCalendar().getTime());        List&lt;TrafficEvent&gt; trafficEventList = convertXmlToDomain.trafficMeasurements(miv.getMeetpunt());        lastReadInDate = miv.getTijdPublicatie().toGregorianCalendar().getTime();        log.info(\"retrieved {} events \", trafficEventList.size()) ;        return trafficEventList;    }Next we will retrieve the data out of the XML and split it out into something more event like.For every sensor point per vehicle we will extract one TrafficEvent.    @Data    public class TrafficEvent {        private VehicleClass vehicleClass;        private Integer trafficIntensity;        private Integer vehicleSpeedCalculated;        private Integer vehicleSpeedHarmonical;        private String sensorId;        private String sensorDescriptiveId;        private Integer lveNumber;        private Date timeRegistration;        private Date lastUpdated;        /*        actueel_publicatie: 1 = data is less then 3 minutes old.         */        private Boolean recentData;        /*         Indicate if the sensor (meetPunt) was available when trying to retrieve the data        */        private Boolean available;        private Integer sensorDefect;        private Integer sensorValid;    }The VehicleClass is just an enum with the vehicle type.    MOTO(1),    CAR(2),    VAN(3),    RIGGID_LORRIES(4),    TRUCK_OR_BUS(5),    UNKNOWN(0);We will also retrieve the detailed sensor information from the XML containing the sensor descriptions.    @Data    public class SensorData {        private String uniekeId;        /*        MeetpuntId        */        private Integer sensorId;        /*        Meetpunt beschrijvende Id         */        private String sensorDescriptiveId;        private String name;        /*        Unique road number.            More info in the dataset of numbered roads in the \"Wegenregister\" (Roads registry), field: locatieide,            http://opendata.vlaanderen.be/dataset/wegenregister-15-09-2016            Or the dataset \"De beheersegmenten van de genummerde wegen\" by AWV, field ident8,            http://www.geopunt.be/catalogus/datasetfolder/12b65bc0-8c71-447a-8285-3334ca1769d8        */        private String ident8;        /*        Reference to the lane of the measurement point.          The character indicates the lane type.            R: Regular lane            B: Bus lane or similar            TR: measurement of the traffic in the opposite direction (p.e. in or near tunnels) on the corresponding R-lane.            P: Hard shoulder lane            W: parking or other road            S: Lane for hard shoulder running            A: Hatched area          Counting starts at R10 for the first regular lane of the main road. Lane numbers increase from right/slower to left/faster lanes.          Lanes 09, 08, 07, ... are positioned right of this first lane, and mainly indicate access/merging lanes, deceleration lanes, recently added lanes, lanes for hard shoulder running, bus lanes          Lanes 11, 12, 13, ... are positioned left of lane R10.          The lane number 00 is used for measurement points on the hard shoulder (P00).          The TR-lane is identical to the corresponding R-lane (TR10=R10,TR11=R11,TR12=R12,...), but returns the data of the \"ghost traffic\" instead.          (The data for TR10 and R10 are provided by the same detection loops.)         */        private String trafficLane;    }Write these events to a topic.    public void sendMessage(TrafficEvent trafficEvent) {        outputChannels.trafficEvents().send(MessageBuilder.withPayload(trafficEvent).build());        log.info(\"Send message to the trafficEventOutput channel\");        outputChannels.trafficEventsOutput().send(MessageBuilder.withPayload(trafficEvent).build());    }    public void sendSensorData(SensorData sensorData) {        outputChannels.sensorDataOutput().send(MessageBuilder.withPayload(sensorData).build());    }The events will be sent to Kafka as JSON messages.With the @Scheduled annotation Spring Boot will read in the events every 60 seconds.    @Scheduled(fixedRate = 60000)    public void run() throws Exception {        putAllEventsInKafka();    }When you are taking your data in, it is important to decide what you want to send in.You do not want to remove too much information nor do you want the events becoming too bloated.Meaning, that they contain too much information and you needing to spend a lot of time extracting information when analysing your data.Keep them as close to the actual event as possible, only adding in data if this is required.In our current example the sensor location does not need to be part of the traffic events as it is pretty static.If in your situation, you have another data entry where your sensor specific data changes every few events, it might be worthwhile to add it to your event when taking it in.So that later on you do not have to spend time joining that data together.Sometimes your intake data is also too large, it is not wrong to ignore certain properties when taking in data in your stream.In our case we ignore a lot of the properties within the XML, as they do not serve our example.Having less properties to analyze can make your life easier, but if that raw data is no longer available you have lost that information for good.Be wise with what you remove as time travel is not something we can code in, ignored data is lost forever.Takeaways  Think in events  Keep the data structure as flat as possible  Do not optimize your data too soonNative Java Stream ProcessingDo not forgetDo not forget that you can also process your events in native Java.You will not have a lot of fancy features available but it might get the job done.Especially when you take into consideration the extra cost involved in introducing a streaming framework.For both Kafka and Storm you not only need to set up a cluster of the framework itself, but also of Zookeeper.That setup does not come for free and will need to be maintained in the future.Easy to get startedWith Spring Cloud Stream it is easy to start processing your stream of data in native Java.First define a SubscribableChannel.    @Input    SubscribableChannel trafficEvents();Then you will need to define a MessageHandler which will describe what you will do with every message you process.    MessageHandler messageHandler = (message -&gt; {            log.info(\"retrieved message with header \" + message.getHeaders().toString());            log.info(\"retrieved message \" + message.getPayload().toString());            TrafficEvent event = (TrafficEvent) message.getPayload();            log.info(\" the sensor id is \" + event.getSensorId());            if (event.getTrafficIntensity() &gt; 0) {                log.info(\"We now have {} vehicles on the road {}\", event.getTrafficIntensity(), event.getSensorId());                int vehicleCountForEvent = event.getTrafficIntensity();                if (vehicleCount.get(event.getSensorId()) != null) {                    vehicleCountForEvent += vehicleCount.get(event.getSensorId());                }                log.info(\"We now had total: {} vehicles\", vehicleCountForEvent);                vehicleCount.put(event.getSensorId(), vehicleCountForEvent);            }            if (event.getVehicleSpeedCalculated() &gt; 0) {                if (lowestWithTraffic.get(event.getSensorId()) == null || lowestWithTraffic.get(event.getSensorId()).getVehicleSpeedCalculated() &gt; event.getVehicleSpeedCalculated()) {                    lowestWithTraffic.put(event.getSensorId(), event);                }                if (highestWithTraffic.get(event.getSensorId()) == null || highestWithTraffic.get(event.getSensorId()).getVehicleSpeedCalculated() &lt; event.getVehicleSpeedCalculated()) {                    highestWithTraffic.put(event.getSensorId(), event);                }                messages.add(event);            }        });Finally, link that MessageHandler to an InputChannel.    inputChannels.trafficEvents().subscribe(messageHandler);There you go, you are now processing your stream of data in native Java.It does become obvious that doing something more fancy, like windowing and aggregation, will require you to write all of that logic yourself.This can get out of hand pretty quickly, so do watch out for that.But for simple data processing, nothing beats some native Java.Takeaways Native Java  Can easily handle 1000 events per second  Easy to get started  You will lack advanced features like windowing, aggregation, …Kafka Streams with Spring KafkaKafkaSpring Kafka allows us to easily make use of Apache Kafka.Kafka is designed to handle large streams of data.Messages are published into topics and can be stored for mere minutes or indefinitely.It is highly scalable allowing topics to be distributed over multiple brokers.Kafka Streams allows us to write stream processing applications within the Kafka cluster itself.For this reason, Kafka Streams will use topics for both input and output allowing it to store intermediate results within Kafka itself.What “topics” does Kafka Streams useKStreamA KStream records a stream of key/value pairs and can be defined from one or more topics.It does not matter if a key exists multiple times within the KStream, when you read in the data of a KStream every record will be sent to you.KTableA KTable is a changelog stream of a primary keyed table, meaning that whenever a key exists multiple times within the KTable you will receive only the most recent record.GlobalKTableLike a KTable, but it is replicated over all Kafka Streams instances, so do be careful.KGroupedStreamThis is an intermediate format based on a regrouped stream of records based on a KStream, with usually, a different key than the original primary key.It is derived from a groupBy() or a groupByKey() on a KStream.Via aggregate(), count() or reduce() it can be converted to a KTable.KGroupedTableThis is pretty similar to a KGroupedStream, but a KGroupedTable is derived from a KTable via groupBy().It can be reconverted to a KTable via aggregate(), count() or reduce().Coding with Spring KafkaWe still have the Spring Cloud Stream topics to which we send in some data.Let’s use these but now using Kafka.First we are going to take in the static data of the sensors into a KTable.    KStream&lt;String, SensorData&gt; sensorDescriptionsStream =        streamsBuilder.stream(\"sensorDataOutput\", Consumed.with(Serdes.String(), new SensorDataSerde()));    KStream&lt;String, SensorData&gt; sensorDescriptionsWithKey =        sensorDescriptionsStream.selectKey((key, value) -&gt; value.getUniekeId());    sensorDescriptionsWithKey.to(\"dummy-topic\");    KTable&lt;String, SensorData&gt; sensorDataKTable =        streamsBuilder.table(\"dummy-topic\", Consumed.with(Serdes.String(), new SensorDataSerde()));The main reason we are using a KTable is that it makes it easy to be sure to only get the most recent state of that sensor, as a KTable will only return one result per key.dummy-topic is just the name I chose.For my example it is not that important to have a well defined topic name.But do realize that Kafka Streams will persist the state of a Ktable within Kafka topics.Subsequently we are going to enrich the traffic event with the sensor data.    KStream&lt;String, TrafficEvent&gt; stream =            streamsBuilder.stream(\"trafficEventsOutput\", Consumed.with(Serdes.String()                    , new TrafficEventSerde()));    stream.selectKey((key,value) -&gt; value.getSensorId())            .join(sensorDataKTable,((TrafficEvent trafficEvent, SensorData sensorData) -&gt; {                trafficEvent.setSensorData(sensorData);                return trafficEvent;            }), Joined.with(Serdes.String(), new TrafficEventSerde(), null))            .to(\"enriched-trafficEventsOutput\");Resulting in a new KStream with enriched TrafficEvents.The .stream(String topic, Consumed&lt;K,V&gt; consumed) will consume all entries from a topic and transform these into a stream. Mapping these to topic records with a key and a value.In our case the key is just a string, while the body of the topic will be a JSON message which gets converted into a TrafficEvent.With join(), full definition:    &lt;VT, VR&gt; KStream&lt;K, VR&gt; join(final KTable&lt;K, VT&gt; table,         final ValueJoiner&lt;? super V, ? super VT, ? extends VR&gt; joiner,         final Joined&lt;K, V, VT&gt; joined);We join our KTable with our TrafficEvent records using the ValueJoiner we pass along which will result in a new Joined result.The ValueJoiner is just a function in which we indicate what needs to be done with both records the function receives. In our case a TrafficEvent and a SensorData.The Joined describes the new record structure we will write towards Kafka using .to(String topic) sending the newly generated records to that Kafka topic.Once this stream has started, it will continue processing these events whenever a new record is inserted into the intake topic.For some of our further processing we do not care for all traffic events, so let’s filter out some.    KStream&lt;String, TrafficEvent&gt; streamToProcessData =         streamsBuilder.stream(\"enriched-trafficEventsOutput\", Consumed.with(Serdes.String(), new TrafficEventSerde()));    streamToProcessData.selectKey((key,value) -&gt; value.getSensorId())        .filter((key, value) -&gt; canProcessSensor(key));Filtering happens on the key of the records, so first we will use selectKey() passing along a KeyMapper to map to the new key.The KeyMapper is a function to which you pass along the field which you want to become the new key.    private boolean canProcessSensor(String key) {        return this.sensorIdsToProcess.contains(key);    }Then we will use filter() to filter out the keys we want to retain which match the given Predicate.In our case the predicate just verifies if a key appears within a List:For every record we will now do some simple processing with updateStats():    streamToProcessData        .selectKey((key,value) -&gt; value.getSensorId())        .filter((key, value) -&gt; canProcessSensor(key))        .foreach((key, value) -&gt; updateStats(value));The updateStats() method just updates some basic counters to track how much traffic has been processed since we started with the data intake to a hashtable.So that we know how many vehicles have passed, the highest speed detected, …WindowingIn an ideal world all events arrive in a perfect and timely fashion within our Kafka system.In an ideal world we can also process all the events we want to process.In the real world however, this does not compute.Events tend to arrive out of order and too late.If you want to get a count of all the vehicles which ran over your road network from 21:00 to 21:05 but one of your sensors sends its events too late, the count you have generated will not be correct.Windowing allows you to mitigate these risk by  Limiting the scope of your stream processing  Allowing you to catch some “late” events within a windowFor adding windows you use .windowedBy, in this example we define a window of 5 minutes which gets every 10 minutes.Then you will need to aggregate the results per window with .aggregate.Do not forget to provide the correct Materialized parameters so Kafka knows what type of key and value is used as input by the aggregation.    private void createWindowStreamForAverageSpeedPerSensor(KStream&lt;String, TrafficEvent&gt; streamToProcessData) {        Initializer initializer = () -&gt; new SensorCount();        streamToProcessData            .groupByKey()            .windowedBy(TimeWindows.of(300000).advanceBy(60000))            .aggregate(initializer, (key, value, aggregate) -&gt; aggregate.addValue(value.getVehicleSpeedCalculated()),                    Materialized.with(Serdes.String(), new JsonSerde&lt;&gt;(SensorCount.class)))                    .mapValues(SensorCount::average, Materialized.with(new WindowedSerde&lt;&gt;(Serdes.String()), Serdes.Double()))                    .toStream()                    .map(((key, average) -&gt; new KeyValue&lt;&gt;(key.key(), average)))                    .through(\"average-speed-per-sensor\", Produced.with(Serdes.String(), Serdes.Double()))                    .foreach((key, average) -&gt; log.info((String.format(\" =======&gt; average speed for the sensor %s is now %s\", key, average))));    }    streamToProcessData.filter((key, value) -&gt; canProcessSensor(key))                .selectKey((key,value) -&gt; value.getSensorData().getName().replaceAll(\"\\\\s\",\"\").replaceAll(\"-\", \"\"))        .to(\"traffic-per-lane\");    KStream&lt;String, TrafficEvent&gt; streamPerHighwayLaneToProcess =             streamsBuilder.stream(\"traffic-per-lane\", Consumed.with(Serdes.String(), new TrafficEventSerde()));    this.createWindowStreamForAverageSpeedPerHighwaySection(streamPerHighwayLaneToProcess);Takeaways Kafka Streams and Spring Kafka  When you have a Kafka cluster lying around, using Kafka Streams is a no-brainer  Excellent support within Spring  Easy to get started  Using the Kafka Streams DSL feels quite naturalApache StormTwitterIt was first created at Twitter who open sourced it as an Apache Project.One of the first streaming frameworks that got widely adopted.Spouts &amp; Bolts  When you work with Storm you need to think in Spouts, Bolts and Tuples.A Spout is the origin of your streams.It will read in Tuples from an external source and can be either reliable or unreliable.Reliable just means that when something goes wrong within your stream processing, the spout can replay the Tuple.While an unreliable spout will go for the good old fire-and-forget approach.Spouts can also emmit to more than one stream.Spouts will generate Tuples, the main data structure within Storm.A Tuple is a named list of values, where a value can be of any type.It is however important that Storm will serialize all the values within a Tuple, so for a more exotic type you will need to implement a serializer yourself.Bolts do all the processing of your streams.A Bolt can send out to more then 1 stream.It is also possible to define a Stream Grouping on your Bolts allowing you to tailor the distribution of your workload over the various Bolts of your Storm topology.Multiple instances of a Bolt will run as tasks.You have the following Stream Groupings:  Shuffle Grouping: completely random  Fields Grouping: based on the value of certain fields, Storm will make sure that all the Tuples with the same “key” will be processed by the same Bolt, handy for word counts for example - great business value  Partial Key Grouping: pretty similar to fields grouping, but with some extra load balancing  All grouping: the entire stream will go to all the tasks of a Bolt, use this with care  None Grouping: implies that you don’t care how it gets processed - which corresponds with a shuffle grouping  Direct Grouping: here the producer of the Tuple will decide which task of the Bolt will receive the Tuple for processing  Local or Shuffle Grouping: this will also take a look at the worker processes running the Bolt’s tasks, this in order to make the flow somewhat more efficient.Now let’s get started with some code.First take in some necessary dependencies:    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-core&lt;/artifactId&gt;        &lt;version&gt;1.2.2&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;        &lt;artifactId&gt;storm-kafka-client&lt;/artifactId&gt;        &lt;version&gt;1.2.2&lt;/version&gt;    &lt;/dependency&gt;The idea is to get to a Storm topology with one Spout and two Bolts.    final TopologyBuilder tp = new TopologyBuilder();        tp.setSpout(\"kafka_spout\", new KafkaSpout&lt;&gt;(spoutConfig), 1).setDebug(false);        tp.setBolt(\"trafficEvent_Bolt\", new TrafficEventBolt(sensorIdsToProcess)).setDebug(false)                .globalGrouping(\"kafka_spout\");        tp.setBolt(\"updateTrafficEventStats_bolt\", new TrafficCountBolt()).setDebug(true)                .fieldsGrouping(\"trafficEvent_Bolt\", new Fields(\"sensorId\"));        return tp.createTopology();First we will define a KafkaSpout which will take in the data of a Kafka topic.    protected KafkaSpoutConfig&lt;String, String&gt; getKafkaSpoutConfig(String bootstrapServers) {        ByTopicRecordTranslator&lt;String, String&gt; trans = new ByTopicRecordTranslator&lt;&gt;(                (r) -&gt; new Values(r.topic(), r.partition(), r.offset(), r.key(), r.value()),                new Fields(\"topic\", \"partition\", \"offset\", \"key\", \"value\"));        trans.forTopic(\"trafficEventsOutput\",                (r) -&gt; new Values(r.topic(), r.partition(), r.offset(), r.key(), r.value()),                new Fields(\"topic\", \"partition\", \"offset\", \"key\", \"value\"));        return KafkaSpoutConfig.builder(bootstrapServers, new String[]{\"trafficEventsOutput\"})                .setProp(ConsumerConfig.GROUP_ID_CONFIG, \"kafkaSpoutTestGroup\")                .setRetry(getRetryService())                .setRecordTranslator(trans)                .setOffsetCommitPeriodMs(10_000)                .setFirstPollOffsetStrategy(EARLIEST)                .setMaxUncommittedOffsets(1050)                .build();    }For completeness this is the retryService which just handles some retrying whenever your Kafka cluster is behaving naughty:    protected KafkaSpoutRetryService getRetryService() {            return new KafkaSpoutRetryExponentialBackoff(KafkaSpoutRetryExponentialBackoff.TimeInterval.microSeconds(500),                    KafkaSpoutRetryExponentialBackoff.TimeInterval.milliSeconds(2), Integer.MAX_VALUE, KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(10));    }Then we will emmit that data to a TrafficEventBolt which will filter out the events we want to process further.    public class TrafficEventBolt extends BaseRichBolt {        private OutputCollector collector;        private final List&lt;String&gt; sensorIds;        public TrafficEventBolt(List&lt;String&gt; sensorIdsToProcess) {            this.sensorIds = sensorIdsToProcess;        }        @Override        public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {            this.collector = outputCollector;        }        @Override        public void execute(Tuple input) {            log.info(\"input = [\" + input + \"]\");            input.getValues();            TrafficEvent trafficEvent = new Gson().fromJson((String)input.getValueByField(\"value\"), TrafficEvent.class);            if (sensorIds.contains(trafficEvent.getSensorId())) {                collector.emit(input, new Values(trafficEvent.getSensorId(), trafficEvent.getVehicleSpeedCalculated(), trafficEvent.getTrafficIntensity()));            } else {                collector.ack(input);            }        }        @Override        public void declareOutputFields(OutputFieldsDeclarer declarer) {            declarer.declare(new Fields(\"sensorId\", \"speed\", \"trafficIntensity\"));        }    }Finally we will send out the tuples to a TrafficCountBolt which will gather some general statistics.    public class TrafficCountBolt extends BaseRichBolt {        private OutputCollector collector;        private final HashMap&lt;String, Integer&gt; countPerSensors = new HashMap&lt;&gt;();        @Override        public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {            this.collector = outputCollector;        }        @Override        public void execute(Tuple input) {            log.info(\"input = [\" + input + \"]\");            Integer count = countPerSensors.get((String)input.getValueByField(\"sensorId\"));            if (count == null) {                count = 0;            }            count = count+ (Integer) input.getValueByField(\"trafficIntensity\");            countPerSensors.put(input.getString(0), count);            collector.emit(new Values(input.getString(0), count));            collector.ack(input);        }        @Override        public void declareOutputFields(OutputFieldsDeclarer declarer) {            declarer.declare(new Fields(\"sensorId\", \"count\"));        }    }WindowingStorm also knows about the concept of windowing.    public class CountPerSensorIdBolt extends BaseWindowedBolt {            private OutputCollector collector;        private final HashMap&lt;String, Integer&gt; countPerSensors = new HashMap&lt;&gt;();        @Override        public void execute(TupleWindow tupleWindow) {            for (Tuple input : tupleWindow.get()) {                Integer count = countPerSensors.get((String)input.getValueByField(\"sensorId\"));                if (count == null) {                    count = 0;                }                count = count+ (Integer) input.getValueByField(\"trafficIntensity\");                countPerSensors.put(input.getString(0), count);                collector.emit(new Values(input.getString(0), count));                collector.ack(input);            }        }    }Subsequently you can define this bolt within a topology at which moment you will also define the size or duration of the window:In this example we are just using windows with a fixed duration of five seconds.    private StormTopology getTopologyKafkaSpout(KafkaSpoutConfig&lt;String, String&gt; spoutConfig) {        final TopologyBuilder tp = new TopologyBuilder();        tp.setSpout(\"kafka_spout\", new KafkaSpout&lt;&gt;(spoutConfig), 1).setDebug(false);        tp.setBolt(\"trafficEvent_Bolt\", new TrafficEventBolt(sensorIdsToProcess)).setDebug(false)                .globalGrouping(\"kafka_spout\");        tp.setBolt(\"updateTrafficEventStats_bolt\", new TrafficCountBolt()).setDebug(true)                .fieldsGrouping(\"trafficEvent_Bolt\", new Fields(\"sensorId\"));        tp.setBolt(\"windowedProcessBolt\", new CountPerSensorIdBolt().withWindow(BaseWindowedBolt.Duration.seconds(5)))                .setDebug(true)                .globalGrouping(\"trafficEvent_Bolt\");        return tp.createTopology();    }You can also pass in an extra parameter slidingInterval to define a sliding window.    withWindow(Duration windowLength, Duration slidingInterval)Both the windowLength and the slidingInterval can also be represented by a Count, which will base the window duration on the amount of tuples being processed.Either determining the length of the window by the tuples, or when to slide.    withWindow(Count windowLength, Duration slidingInterval)    withWindow(Duration windowLength, Count slidingInterval)Even tumbling windows are possible:    withTumblingWindow(BaseWindowedBolt.Count count)    withTumblingWindow(BaseWindowedBolt.Duration duration)Please note that a tuple belongs to only one of the tumbling windows, while with a sliding window it is very much possible that a single tuple is processed within multiple windows.Stream APIThe Storm Streams API is pretty new.It tends to provide a DSL which corresponds more with other streaming DSLs, making your data processing feel more natural and less clunky, as compared to be thinking in spouts and bolts.In the background it will convert the DSL to spouts and bolts though, so knowing how Storm works internally is still pretty important.Takeaways Apache Storm  It is pretty mature  Low latency / high throughput  It does tend to feel pretty clunky thinking in Spouts and Bolts - for a developer it is not that big of a hassle, but for a data scientist I can imagine that at times it will be harder to wrap your head around itConclusionIn order to get started with basic stream processing you do not need Kafka or Apache Storm, native Java is good enough for you to take your very first steps when processing a stream of data.It is easy, everybody understands it and you will have less moving parts within your software landscape which can cause issues.Using a dedicated streaming platform will become necessary when you want to do more advanced streaming operations or when performance becomes more and more important.The existing platforms can easily scale up to the processing of thousands of messages a second, something which is going to be much harder to achieve when building your solution yourself.Do not make the mistake of re-inventing the wheel by writing your own streaming platform, others have done that hard work for you.Kafka Streams is a no-brainer to use when you have a Kafka cluster lying around, stream processing there feels natural and it is easy to get going.If however you do not have a Kafka cluster available, it will come with an extra cost of setting it up and maintaining it.There do exist managed solutions in order to make your life easier.Apache Storm is a pretty robust framework which has been around for some time and is used by many.However, writing the processing logic feels quite clunky and I can imagine that for a non-developer it also might feel quite unnatural. They are currently working on a new streaming API which should alleviate that issue though.According to their GitHub, a release of version 2.0 has already happened, but their website does not reflect it yet.When doing stream processing always think about how messages will be handled as most streaming or messaging platforms use an at-least-once approach, meaning that the same message can be processed more than once by the streaming pipeline. Both Kafka Streams and Apache Storm can be configured to provide exactly-once processing within their streaming pipelines.For Kafka Streams it means using Kafka transactions while for Storm this can be achieved by Trident.Even then, it is only within the streaming pipeline itself meaning that as soon as your processed results leave the streaming platform, you will be back to at-least-once guarantees."
      },
    
      "conference-2019-03-05-ddd-europe-html": {
        "title": "DDD Europe 2019",
        "url": "/conference/2019/03/05/ddd-europe.html",
        "image": "/img/2019-02-16-ddd-europe/ddd-europe.jpg",
        "date": "05 Mar 2019",
        "category": "post, blog post, blog",
        "content": "  This year, Pieter Van Hees and Kristof Eekhaut attended the Domain-Driven Design Europe  conference in Amsterdam.The conference was all about Domain-Driven Design and related topics, with loads of interesting talks from beginners and experts in their field.In this post you can read about some of the talks and workshops we attended.Table of content  When we lose sight of our domain by Carola Lilienthal  Make your tests tell the story of your domain by Anne Landro and Mads Opheim  Domain modelling towards First Principles by Cyrille Martaire  Collaborative Modelling hands on session by Marijn Huizendveld  Lost in transaction? Strategies to manage consistency across boundaries by Bernd Ruecker  Estimates or No Estimates, Let’s explore the possibilities by Woody ZuillWhen we lose sight of our domain by Carola LilienthalCarola discusses nine traps that developers fall into, and which prevent us to focus on the important aspect of developing software, the domain.Trap 1: Model monopoly  “In order for developers to learn about the domain, they have to talk to the users, in a language that the users understand.”The first thing to understand is that developers need to talk to the users, because if they don’t they will lose a lot of information. However, in a lot of companies, it is the analyst alone who talks to users when he/she gathers requirements.By having one or more analysts who communicate with users, they have the monopoly of the domain.When developers do communicate with users, they should do so in a language and/or model that the users understand.Sharing class diagrams or database models with users is counterproductive.The users will not understand this complex model and think it took a lot of effort to create.As a consequence they either cannot give relevant feedback because they don’t understand it, or they won’t dare to because they don’t want to discourage you.A better way to communicate the model between users and developers is to use e.g. a schema with icons and descriptive names for actions.Trap 2: Only look at the future without taking into account the presentLook at how they are working today instead of only looking what you want to achieve in the future.Ask yourself: “Who is doing what wherewith and what for?”Avoid using requirements documents without concrete examples.Trap 3: Forget about reuse in your domainFirst think about something being usable, and then see if it can be reused.The Don’t Repeat Yourself (DRY) principle should not be applied rigorously and blindly. If you apply DRY too often and too soon it often leads to leaky abstractions.Trap 4: Don’t try to be too generic, DDD is about being as concrete as possibleBy being concrete in your domain and your code you will have explicit and understandable code.Trap 5: if your components are too dependent on each other, you cannot scale them independentlyHigh coupling between components prevents you from splitting them into different services that could scale separately.Another disadvantage of high coupling is that it becomes difficult to let you software evolve, because changes in one component force changes in others.Trap 6: Large business classesFor example when modeling containers that move through different stages in a harbour.The large business class could be the container that manages all stages the container goes through.It would be better to model these stage as separate components.This is called functional decomposition.  “Don’t create big business classes that serve everybody.”Trap 7: How do we know what to buildHow do we split a big elephant into pieces?Let’s say we have four different types of elephants in our business domain.A common mistake would be to split elephant by different parts of the body. Where one component would be all four types of feet, another would be all four types of heads, etc.This might not necessarily be the best approach to split the four elephant types.The better approach would be to build one small elephant that is fully functioning, and then let it grow each iteration.This approach lets you learn from each iteration and allows for incremental growth and refactoring.Trap 8: The expert trapThe people who developed the elephant will start to think they are experts, and know everything there is to know about the elephant, because they built it from scratch.This assumption is false, because even the developers who built the elephant from scratch have assumptions, and assumptions can be false.The real experts are, and will remain, the users.Trap 9: Everything is new, and therefore betterPeople tend to believe that this new system they are building will be way better than the old system they’re replacing, because it looks better.What they forget is that the users know the old system very well and are often very productive in it.When the users will start to use the new system, they will feel like beginners again.They will be less productive than with the old application, at least for a little while.Make your tests tell the story of your domain by Anne Landro and Mads OpheimAnne and Mads tell us how they drastically reformed the testing approach for the Norwegian Court Case Management system from constant repetitive manual verification to automated testing.They explain that Value Chain tests have helped their team document the domain:PersonIsRegisteredAsDeadAfterStartOfACaseOfDeath {   createACaseOfDeath()   registerTheDiseased()   registerTheHeirs()   notifyTheNationalRegistrtyOfTheDeath()   assertThatThePersionIsRegisteredAsDead()}Each of these tests runs through a workflow of the domain and verifies the state at the end of it. They are high level tests that can be understood by all stakeholders, so that anyone - including domain experts and users - can look at a test and verify whether the result is what they expect.From a  quick glance at this code you can learn a lot about how the domain works. Their team also uses this technique to document special cases that they discover in the domain, so that bugs caused by these quirks don’t happen again.Domain modelling towards First Principles by Cyrille MartraireIn this great talk Cyrille explains us why he thinks that the Domain-Driven mind set of most teams is “too gentile” and we aim to “raise the waterline”.With DDD we learned to immerse ourselves in the domain, use our domain-driven skills to understand the domain and conceptualise the domain into conceptual models. But we should go further by defining theories for our models and spot the First Principles that the theory consists of. Then we can challenge them, so that we can suggest changes to the business instead of reproducing the domain as it is. This way we get more involved and get to the next step, which is:  Innovation!He points out a number of common problems that many teams have and suggest how we can improve them:The Human Compiler effectOne thing we often see is that we are given requirements piece by piece: the first sprint we get one case, then the next sprint another case and so on.But most of the time it turns out that all of these cases are actually special cases of some general case that we haven’t been told about.The reason for this is what Cyrill calls the Human Compiler effect:  someone behaving like a compiler, by taking the general problem and splitting it up and dumbing it down in separate simple solutions for every single consequence, so that a developer can implement them.This is obviously a bad approach, because by dumbing down the domain for the developers, we keep them “dumb” and unaware of how the domain actually works. This leads to a dumb - and often wrong - implementation of the domain.We should instead first describe the problem to the entire team. Then the team should build a theory upon it and challenge it by asking critical questions (Why? Why? Why? …).This leads to a better understanding of the domain and thus to building better solutions.Technical complexityOn the other hand, sometimes we are given an explanation about a problem and some developers turn it in something even more laborious.This increases technical depth and make the code unmaintainable.The solution for this: refactoring and using Test-Driven Development.Theory vs Residual MessWhen we start creating theories about the regular world of our domain, often someone from the teams asks: “But what about ALL the other business rules?”. We have an obsession for the “big bag of business rules”. As if every business is a bunch of data with a bunch of if-statements on top.We should realize that there is always some order in this mess and that a lot of things are more regular that irregular. We have to find out these regularities, find out the theories behind them and then we can create our domain model.Of course any business also has irregularities that do not fit into our theories and we can not just ignore these. We call this the Residual Mess.However we should not allow this mess to affect our beautiful theories. Instead we should - as Eric Evans explained before - define a Spill Zone in the application where we can put all the messy parts of the application.ConclusionCyrill advices us to:  Raise the waterline  Expect untold regularities  Practice TDD  Practice DDD  Build theories, not just lists of business rules  Learn to think based on First Principles. Disrupt and become innovative!Collaborative Modelling hands-on session by Marijn HuizendveldWe are divided in groups of five with one team leader.The goal: to model an application for the maintenance team of a car rental company in Amsterdam.The application must determine when a car is due and available for maintenance.New requirements are provided step by step on “requirement” cards, so that we have to adapt and reshape our model each time we discover a bit more about the domain. We learn the importance of visualising the solution (model) and talking about the problem based on what we have visualized in front of us. Putting notes on the board with the different concepts that we identify, sparks interesting discussions that make use think further about the problem:  Is the given name correct and clear?  Do we mean the same thing when we talk about …?  Do two words on the board actually mean the same thing?After each requirement card follows a card for the team leader to consider making changes to the way we work.One card tells the leader to look for someone who has been a bit quiet or outside of the discussion and move the group around so that he is next to the board.This immediately make this person more involved in the discussion and we also start paying attention to his view.Another card suggests to let someone go through the entire process that is modelled on the board and explain it step by step.We immediately find out that some definitions on the board are hard to explain and not as clear as we thought they were.With this excellent workshop Marijn shows us how easy it can be to come up with a great domain model that is understood and agreed upon by everyone involved.Lost in transaction? Strategies to manage consistency across boundaries by Bernd RueckerIn this talk Bernd explains the challenges we face when using transactions in big applications and distributed systems.He starts by reminding us that our Aggregates in DDD are usually our transactional/consistency boundaries. Meaning that within an Aggregate, you have an ACID transaction.If you were to have a transaction over multiple Aggregates, you would have a stronger coupling between them.For example you can’t split them easily into multiple separate microservices.What you could do is use two-phase commits to have you transaction over multiple Aggregates in separate services.But the problem is that two-phase commits don’t scale.  Grown ups don’t use distributed transactionsAn alternative solution is the alternative to ACID: BASE.  Basically  Available  Soft-state  Eventual consistencyBy applying Eventual Consistency you update one aggregate in one transaction and the other in a different transaction.This means that the system will be in an inconsistent state for a short time, but eventually it will be consistent.After that, Bernd explains different strategies how to implement this eventual consistency with an example.Let’s say that we have an credit card payment aggregate that charges a credit card aggregate, and that this communication happens through an asynchronous message. This communication can go wrong in multiple ways: the message might never arrive at the credit card service, it might arrive but the payment service doesn’t receive the feedback, etc.Strategy 1: CleanupIf the payment service can’t send the message, or if it doesn’t receive feedback that the message was received, it can send a payment failed event.The problem with this strategy is that this ‘payment failed’ event also might not arrive at the credit card service. Which means that it won’t be able to do his cleanup.Strategy 2: Keep stateStateful retryBy using a stateful retry the payment service would keep the state of whether or not the message was delivered to the credit card service.As long as the credit card service does not acknowledge that it processed the message, the payment service will keep sending the message.Stateful retry and cleanupWith this strategy the payment service keeps retrying to send the message until a timeout has passed or after X retries.After that it will send a payment failed event for which the retry policy might also apply.Strategy 3: Compensation/SagasChoreographyCompensation means that if something in the asynchronous process fails, a compensating process will be triggered.A classic example is a system where you book a hotel in one service which will trigger a car booking.If the car booking fails, it will emit an event that will be picked up by the hotel service which will cancel the hotel room related to the car booking.This system of services responding on events from each other is called orchestration. We don’t define in one place how the whole process works, but services know themselves on what to react on.However, this compensation saga implemented with choreography might become complex because it could be a trainwreck of cascading cancellations.E.g. a hotel booking triggers a car booking, which triggers a flight booking.If you have complex processes with a lot of services involved, this might become chaotic.OrchestrationBy using an orchestration approach there would be one service responsible for managing the whole process.In the hotel/car/flight example a trip service could be this orchestrating service that calls the other services and tells them to book or cancel.Bernd then argues that if you choose an orchestration strategy that BPMN tools and libraries can help a lot in defining these processes.You could for example define your business process and all compensating activities.Some libraries even provide quite nice DSLs where you can make your business process quite explicit.And the good thing is that this business process or saga is even part of your domain logic.Estimates or No Estimates, Let’s explore the possibilities by Woody ZuillWoody starts by pointing out that his workshop does not give answers, but does ask critical questions.His goal is to share some experiences he had, and he realizes that what works in some companies, does not work in others.After this disclaimer he talks about a big project he worked on where they experienced sprint after sprint that their estimates were always plain wrong.Every retrospective this frustration was mentioned and every time the solution management came up with was that they just had to get better at making estimations.  “Trying the same thing over and over again expecting different results is the definition of insanity” - EinsteinIn fact, Woody said, a constant in his 35 year career was that estimations were always off, and people were always trying to solve this by “getting better at estimates”.Wrong estimates are often not the problem itself, but a symptom of something else.It could be that they are off because the requirements were unclear, or that requirements keep changing.#NoEstimates#NoEstimates was originally used to refer to reference a blog post Woody had written on a project where they did not use or make estimates.But actually ‘No estimates’ is a placeholder for a larger conversation.Woody mentions that for some things in life we want estimates, but we never do because we know it’s impossible.E.g. how long will this clinical trial take?How long till we find a cure for cancer?How long till you finish this work of art?In fact if we have enough data to definitively say how long something will take to develop, we already built it and we don’t need to do it again.Next he asked the audience to explain in a single word what an estimate means. Quite some different answers were given, but in general it came down to this list:  Guess  Expectation  Lies  Misunderstanding  ApproximationFrom these answers the following working definition could be extracted:  An estimate is a guess of the amount of work time to create a project, a feature or some bit of work in developing software.Why do we estimate?Some reasons why we make estimates:  Planning / budget  Which approach do we choose / in what order do we do things  Dependencies on other teamsIn software development, estimates are often used to attempt to predict the future.When will it be done?How much will it cost?What can we get done this sprint?What can we get done for this amount of money?Basically we use estimates to help us make decisions.If we have to choose between making project A or project B, we would make an estimation of how long it would take to do either of them.But do we really want to choose between project A or B based on a guess?Wouldn’t it be better to do an MVP of both and see which is working best?Is on time or on budget a good measure of the results of our decision?No, because you cut features, make it unmaintainable, etc.Isn’t it better to measure customer satisfaction as a metric for success?ConclusionAfter this workshop the audience was left with even more questions.But what we did realize is that often people make estimations without any good reason. And sometimes it would be better to reflect on why we do estimations, and see if it really provides us value, and if there is no alternative solution for the problem we’re actually trying to solve with estimations.Summing it all upDomain-Driven Design Europe was a great conference where we got to learn more about software design and techniques that help us do what we love to do the most: creating great software for users.The organizers did an excellent job in creating a conference with great speakers.Next year’s conference will take place in Amsterdam on the 6th and 7th of February 2020."
      },
    
      "vue-2019-03-04-vue-with-typescript-html": {
        "title": "Vue with TypeScript",
        "url": "/vue/2019/03/04/vue-with-typescript.html",
        "image": "/img/vue-with-typescript/vue-plus-typescript.png",
        "date": "04 Mar 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Vue with TypeScript, an introduction  Creating a Vue project with TypeScript  A look into the files created by the Vue CLI  How to write your first component  Using your first plugin  Your first deployment  Conclusion  Resources and further reading1. Vue with TypeScript, an introductionWith the release of the Vue CLI 3 in August 2018, Vue now officially supports development in TypeScript.In September 2018, Evan You even announced that the next version of Vue will be rewritten in TypeScript.This does not mean that you are forced to use TypeScript, it will still be an option.TypeScript has numerous advantages such as static typing and transpiling of the latest ECMAScript features for full compatibility with older browsers.Especially the static typing is a very interesting feature for projects in a professional environment as it helps define more strict interfaces.With the use of types, you inherently provide documentation to other developers on how to use your code as it offers guidance on how to use your functions, components and so on.In this tutorial we will make a really simple blog system to showcase how you create a project, create a component, install a plugin and do calls via HTTP.Our little project will be called wordvue.At the same time we will explain some tips and tricks and give some background information about Vue with TypeScript so that you fully understand what the purpose is of each line of code.The project can be found in a GitHub repository so you can see a working version.  2. Creating a Vue project with TypeScriptUsing the Vue CLIThanks to the Vue CLI, it is the easiest way to create a new Vue project.First make sure you have the latest version of the CLI installed with NPM:$ npm i -g @vue/cliAfter that we create our project:$ vue create wordvueThe CLI knows some presets but we will go through the manual mode to be sure we select the TypeScript version.At the time of writing the current default language for Vue is JavaScript.  Babel + TypeScript  We check the TypeScript option and for the purpose of this article we will not look in detail at the other features of this screen and fall back on the default values.I also checked CSS Pre-processors just because I like SCSS.Make sure you have Babel selected, Babel will automatically add multiple polyfills.The polyfills will help with having backwards compatibility of ECMAScript features.Using the class-style component syntax  In the next screen we will get the question if we want class-style component syntax, for which we answer Yes.With this, we actually install the decorators that can be found in the Vue Class Component package.We will now explain the difference between using the class-style component syntax and using the classic Vue syntax.The classic Vue SyntaxIf you do not use the class-style component syntax, your components will look exactly as if you have rendered them with Vue with JavaScript, but with the addition of types:import Vue, { VNode } from 'vue'export const HelloComponent = Vue.extend({    data () {        return {            message: 'Hello',        }    },    methods: {        greet (): string {            return this.message + ' world';        }    },    computed: {        greeting(): string {            return this.greet() + '!';        }    },    render (createElement): VNode {        return createElement('div', this.greeting);    }});As you can see, you’ll still have the data function, the methods, computed properties and the render function that you can use in regular Vue.Class-style component syntaxWith the class-style component syntax, we would write the same component like this:import Vue from 'vue'import Component from 'vue-class-component'@Component({    template: '&lt;div&gt;&lt;/div&gt;'})export default class HelloComponent extends Vue {    message: string = 'Hello'    greet(): string {        return this.message + ' world';    }    get greeting() {        return this.greet() + '!';    }}When you compare it to the previous version, the data property message is now a regular property in our component class. Methods are present as class methods. And the computed properties can be defined as a getter.The other settingsAfter you’ve replied Yes to the “Use class-style component syntax?” question, you can continue with the default options.For the CSS pre-processor, you either choose between Stylus, Less and SCSS. We choose the default Sass/SCSS (with node-sass).  As for the linter, you can either choose between TSLint or ESLint with a bunch of configurations.I opt for the default TSLint option as the support for TypeScript in ESLint is (at the time of writing) fairly recent.But ESLint is certainly a valid option as the TypeScript has announced in their January to June 2019 roadmap that ESLint will be their own focus.  We can choose for the Lint on saveoption as we want to see immediately the effects of our linter.  Finally we have to choose if we want the configurations in dedicated files or all bundled together in our package.json.We opt for In dedicated config files as we prefer to not clutter the package.json with a lot of configurations.  The Vue CLI will now create the project with a Git repository, perform an NPM install and generate a README.  Launching your first Vue projectAfter the Vue CLI has created your Vue project, you can go with the terminal to the root folder of the project and launch it with:$ npm run serve  As you will see, the CLI starts the development server, starts the type checking and linting service.By default the project runs on http://localhost:8080 but if there’s already something running on port 8080, it will pick port 8081 or the next one available.This way you don’t need to specify a free port.A default Vue project looks like this:  3. A look into the files created by the Vue CLI  The files that the Vue CLI generated are mainly all the configuration files that we wanted separately.So we have a configuration file for Babel with babel.config.js, PostCSS (which contains the configuration for SCSS) with postcss.config.js, TypeScript with tsconfig.json and TSLint with tslint.json.You will also find a node_modules folder for all your NPM packages with a package.json in which we define all the NPM packages that we need in our project.If we would have opted for the In package.json option, we would have had a large package.json file.The main folders in which you will work are public and src. We will look at these more in detail later so you fully understand what their purpose is.PublicPublic is meant for static assets like images, favicons and more.It also contains your index.html which is very basic:&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\t&lt;meta charset=\"utf-8\"&gt;\t&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\t&lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0\"&gt;\t&lt;link rel=\"icon\" href=\"&lt;%= BASE_URL %&gt;favicon.ico\"&gt;\t&lt;title&gt;wordvue&lt;/title&gt;&lt;/head&gt;&lt;body&gt;\t&lt;noscript&gt;\t\t&lt;strong&gt;We're sorry but wordvue doesn't work properly without JavaScript enabled. Please enable it to continue.&lt;/strong&gt;\t&lt;/noscript&gt;\t&lt;div id=\"app\"&gt;&lt;/div&gt;\t&lt;!-- built files will be auto injected --&gt;&lt;/body&gt;&lt;/html&gt;It is only in rare cases that you should adapt the index.html.One example situation would be to add Google Analytics, add more meta tags or adapt the title tag.Vue will automatically inject the necessary generated JavaScript files right before the closing body tag.This will include the transpiled version of your own code as well as vendor code.The most important thing is the div tag with id app.This should always be present in your index.html as this is the tag on which Vue will bootstrap the entire application.Srcmain.tsimport Vue from 'vue';import App from './App.vue';Vue.config.productionTip = false;new Vue({\trender: (h) =&gt; h(App),}).$mount('#app');When you look in the src folder, you will find a main.ts file.This is the one that Vue will execute first.As you can see this creates a new instance of Vue in which we only define a render function.Vue will pass along h which is of type CreateElement.h has been chosen by the creator of Vue as it is short for Hyperscript, a term that is used in several virtual DOM implementations.A hyperscript is a script that will generate HTML structures.It takes one parameter: App.Thus in a Vue project, h will generate the HTML for our App component.App is our main component that was generated by the Vue CLI and we will dive into that after this section.main.ts should only be adapted to plug in new core functionalities of your application.For example, a main.ts of one of my own projects is this:import Vue, { CreateElement, VNode } from 'vue';import App from './App.vue';import i18n from './i18n';import './registerServiceWorker';import router from './router';import store from './store';Vue.config.productionTip = false;new Vue({    router, // custom router configuration    store, // custom store implementation    i18n, // my translations    render: (h: CreateElement): VNode =&gt; h(App),}).$mount('#app');As you can see, I have added three core functionalities: a router, a store and an i18n library.In each component that you make, these functionalities will be available.The reason why these will be available is because the Vue type gets extended by each of these libraries.For example in the typings of VueI18n (the i18n library that I use), we find:declare module 'vue/types/vue' {\tinterface Vue {\t\treadonly $i18n: VueI18n &amp; IVueI18n;\t\t$t: typeof VueI18n.prototype.t;\t\t$tc: typeof VueI18n.prototype.tc;\t\t$te: typeof VueI18n.prototype.te;\t\t$d: typeof VueI18n.prototype.d;\t\t$n: typeof VueI18n.prototype.n;\t}}This means that we will have a $i18n property available and five different functions.If you would use a different i18n library, you will have most of these things also readily available.For example vue-i18next defines $i18n as:declare module \"vue/types/vue\" {    interface Vue {        readonly $i18n: VueI18Next;        $t: TranslationFunction;    }}Vue itself does not provide an i18n implementation nor a store nor a router nor does it even support HTTP calls by default.Vue is designed to be as light as possible so that developers can keep a project as lightweight as possible.Vue does officially support specific NPM packages for these core functionalities.Other packages will follow the same naming conventions as the official supported libraries for convenience sake.Like said before we will only focus on the basic Vue functionalities and HTTP calls.The other topics will be for a future article.App.vueThe App.vue file is our first component that Vue bootstraps through our main.ts file.It is considered to be the root component.The Vue CLI generates the App component with one child component.&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;img alt=\"Vue logo\" src=\"./assets/logo.png\"&gt;\t\t&lt;HelloWorld msg=\"Welcome to Your Vue.js + TypeScript App\"/&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import HelloWorld from './components/HelloWorld.vue';@Component({\tcomponents: {\t\tHelloWorld,\t},})export default class App extends Vue {}&lt;/script&gt;&lt;style&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;Each page and part of a page that you will create, is a component that is a child of the root component.Together, your whole application should have a component structure which should look like a tree:App\t- HomePage\t\t- HelloWorld\t- NewsPage\t\t- NewsArticle\t\t\t- Reaction\t- ContactPageEach node of the tree is a component.With the use of the @Component(...) decorator, we define which components can be child components of the component that we’re defining.For example in our App component, we want the HelloWorld component (through the HelloWorld tag), thus we add the components option with HelloWorld in there.These components are local components.If you would want to write a component that is global, you have to register it like this:Vue.component('my-component-name', {\t// ... options ...})A global component can be accessed anywhere.Try to avoid this as much as possible as it fills up the global namespace.An example of a use case that is justified would be an icon library like Font Awesome:library.add(    faBars,    ...    faCameraRetro,);Vue.component('font-awesome-icon', FontAwesomeIcon);After this we can access the font-awesome-icon tag from everywhere.&lt;font-awesome-icon icon=\"arrow-down\" /&gt;4. How to write your first componentWe will keep the project as simple as possible for now.Firstly I will explain the basics of a Vue Component so that you will fully understand what happens when we write our first real component.The structure of a .vue file&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t...\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import HelloWorld from './components/HelloWorld.vue';@Component({\tcomponents: {\t\tHelloWorld,\t},})export default class App extends Vue {}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\t...}&lt;/style&gt;The standard way to write a Vue component is by using the .vue file extension.In a .vue file, we define three optional tags: template, script and style.According to the Vue documentation, you should always order the tags consistently with style being the last one.&lt;template&gt;…&lt;/template&gt;This is the visual part of your component, in here you define the HTML that will be used to display your component.Note that your custom HTML should always be surrounded by a div tag.The reason for this is that it allows Vue to encapsulate your custom CSS without unknowingly affecting the styling of your whole site.You can use this to add a custom id or class to the tag to help you identify the component in for example your e2e tests.Note that a .vue file can contain at most one template tag.&lt;script lang=”ts”&gt;…&lt;/script&gt;In the script tag, you can add your custom TypeScript code.The lang attribute is not required but if you do not add it, the default language will be JavaScript.In order for TypeScript to be available, you need to add lang=\"ts\".All of our TypeScript code should be present in this script tag, even the import statements.Note that a .vue file can contain at most one script tag.&lt;style&gt;…&lt;/style&gt;In the style tag we can define our own SCSS specific for this component.By default, all the styles you define in a style tag are global.By adding the scoped attribute to our style tag, our custom SCSS will be specific for that component.&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;p&gt;Hello World!&lt;/p&gt;\t\t&lt;ChildComponent&gt;&lt;/ChildComponent&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;...&lt;/script&gt;&lt;style scoped&gt;#app {\tp {\t\ttext-style: italic;\t}}&lt;/style&gt;&lt;style&gt;#app {\tp {\t\tcolor: red;\t}}&lt;/style&gt;In the example above, the style tag with the scoped attribute will only affect the p tag in our component and not the one in our child component.The color: red styling however will affect also the styling of p tags in ChildComponent.So it is best to be aware of the implications as it can have unwanted side effects.For a good way to structure your CSS, check out the BEM methodology as it will help with avoiding conflicts and will guide you in having clean CSS selectors.Be careful though as the scoped attribute leads to a certain performance hit.Simply by adding the same class or id that we used in the template, we avoid this performance hit and still can have some scoped SCSS.Note that this will also style the child components.A .vue file can contain more than one style tag.If you want to use global styles, you can either put them in App.vue or create your own CSS file that you import either directly in index.html or in App.vue.Should you put everything in a .vue file?For small components, a .vue file will be very interesting as you have all the elements that make up your component into one specific file.But what if you have for example lots of lines in the template tag?Or what if you just want to split up the file?One tactic you can use is the src attribute on the template, script and/or style tags.&lt;template src=\"./mycomponent.html\"&gt;&lt;/template&gt;&lt;script lang=\"ts\" src=\"./mycomponent.ts\"&gt;&lt;/script&gt;&lt;style src=\"./mycomponent.scss\"&gt;&lt;/style&gt;Personally I avoid using the src attribute as I like to force myself to keep my .vue files as small as possible.There’s also no performance difference between putting the HTML/SCSS separately or in the same .vue file.The Vue CLI will generate the same compiled code.How to organise your filesThe basic structureThe basic project structure in Vue is very simple:/public\t/index.html/src\t/assets\t/components\tApp.vueThe idea behind this is that you add all your custom components into /components and any assets that also need to be transpiled/compiled into /assets.For example a global stylesheet or an icon library that you want to treeshake fits perfectly into /assets.In the public folder we put all things static that don’t need to be parsed and treeshaked: the logo, the favicons, images, …A more advanced structureThe basic structure is enough for a simple single page application.But once you start to have complex pages, things will quickly need to be changed to accommodate the amount of files that you will create.In a more advanced project (like a personal project of mine), the structure could be like this:/public\t/index.html/src\t/assets\t/components\t/i18n\t/models\t/store\t/views\tApp.vueIn this project structure, both /public and /assets provide the same purpose.I use /assets for some local .json files that contain some of my data that is needed in the application.In /components I keep all my generic components that can be used by pages and other components.This is what you would put in a SharedModule in Angular for example./i18n contains all my translations of my website as well as the initialisation of an i18n library.Same goes for the /store that contains my implementation of a store.I had based myself on the structure proposed in the Vuex library where they group everything store related into /store and applied the same principle for other libraries.ModulesVue is designed to be as lightweight as possible and this can be seen in how the basic project is structured: no modules are present.Vue does support modules but not in the way like we know them from other frameworks like Angular.Vue modules are simply ES6 modules.BlogPost componentThe basic fileOur first component we will write is a BlogPost component in components/BlogPost.vue.In a first stage of our little project we will just hardcode a blogpost.The BlogPost component is small:&lt;template&gt;\t&lt;div class=\"blogpost\"&gt;\t\t&lt;h2&gt;{{ post.title }}&lt;/h2&gt;\t\t&lt;p&gt;{{ post.body }}&lt;/p&gt;\t\t&lt;p class=\"meta\"&gt;Written by {{ post.author }} on {{ date }}&lt;/p&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Prop, Vue } from 'vue-property-decorator';export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;}@Componentexport default class BlogPost extends Vue {\t@Prop() private post!: Post;\tget date() {\t\treturn `${this.post.datePosted.getDate()}/${this.post.datePosted.getMonth()}/${this.post.datePosted.getFullYear()}`;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;div.blogpost {\th2 {\t\ttext-decoration: underline;\t}\tp.meta {\t\tfont-style: italic;\t}}&lt;/style&gt;As you can see the template is rather small.We’ve grouped all the elements in a div with class blogpost.Vue expects us to wrap the content in one tag and by convention, they advise to use a div tag.Within the script tag you’ll notice that we have created a small Post interface to wrap our data.On the component itself, we have a member that is decorated with @Prop().With the decorator, we allow the use of the BlogPost component with the attribute post that should have type Post.You’ll notice we’ve added a ! behind post so we end up with post!.The exclamation mark is the non-null assertion operator which tells the browser that post will eventually be filled in with a value and that it shall never be null or undefined.&lt;BlogPost :post=\"blogPost\" /&gt;Where blogPost is an instance of Post in our component.After that we have a date member which is a computed property.Sadly there is no full type checking going on at the moment.If we were to use the BlogPost component, we can always pass along another object into the post attribute.We can pass along a type attribute in the Props decorator but even that is not that stable.@Prop({type: Object as () =&gt; Post})We end the component with our styling in which we make use of SCSS to nest all our styling.Using the BlogPost componentSo we have created the BlogPost component but how are we going to actually use it?We adapt App.vue as follows:&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;h1&gt;Elke's fantastic blog&lt;/h1&gt;\t\t&lt;BlogPost v-for=\"blogPost in blogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import BlogPost, { Post } from './components/BlogPost.vue';@Component({\tcomponents: {\t\tBlogPost,\t},})export default class App extends Vue {\tprivate blogPosts: Post[] = [\t\t{\t\t\ttitle: 'My first blogpost ever!',\t\t\tbody: 'Lorem ipsum dolor sit amet.',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 18),\t\t},\t\t{\t\t\ttitle: 'Look I am blogging!',\t\t\tbody: 'Hurray for me, this is my second post!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 19),\t\t},\t\t{\t\t\ttitle: 'Another one?!',\t\t\tbody: 'Another one!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 20),\t\t},\t];}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;As you can see we define a property on the App component that contains our blog posts, we add the components property to the Component decorator and add the BlogPost tag in the template.We simply loop over the blog posts with the v-for directive.We pass each blog post to the BlogPost component by binding it to the correct data attribute.This can be done through v-bind:post=\"blogPost\" but we use the shorthand method of :post=\"blogPost\".Vue transforms :post to v-bind:post behind the screens.Note that we also pass :key which we bind to the title of our blog post.The reason for this is that it allows Vue to keep track of the state of the list by only looking at the :key attribute instead of having to deep compare objects.Try to have an unique key of type number or string that can be used for actions such as identifying, ordering and searching.A blog post title is a good start but when ordering, updating or other modifying operations you might not have the wanted result if we have blog posts with the same title.It’s best to use something of type number or string as the key.Vue will tell you this in the console of your browser if you would take for example the datePosted as your key:  The reason behind this is that Vue relies on the built-in sort and find functionalities of JavaScript.For objects, Vue can not do this natively.When we serve our app with$ npm run serveWe see in our browser:  Great, you’ve written your first working component!Now it is time to extend it with some functionalities.Adding conditional elements to the componentAn important part of a component is to have some dynamic behaviour.For example what if we want to show a highlighted blog post? We could create a new component called HighlightedBlogPost but we could also extend our existing component.We can add a new paragraph with a v-if statement:&lt;p v-if=\"post.highlighted\"&gt;This post is highlighted!&lt;/p&gt;The contents of the v-if is a TypeScript statement that should return true or false.We extend our Post interface to accomodate this:export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;\thighlighted?: boolean;}After that we add highlighted: true, to the second blog post in App.vue.In our browser it looks like this:  We end up with this as our BlogPost component:&lt;template&gt;\t&lt;div class=\"blogpost\"&gt;\t\t&lt;h2&gt;{{ post.title }}&lt;/h2&gt;\t\t&lt;p v-if=\"post.highlighted\"&gt;This post is highlighted!&lt;/p&gt;\t\t&lt;p&gt;{{ post.body }}&lt;/p&gt;\t\t&lt;p class=\"meta\"&gt;Written by {{ post.author }} on {{ date }}&lt;/p&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Prop, Vue } from 'vue-property-decorator';export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;\thighlighted?: boolean;}@Componentexport default class BlogPost extends Vue {\t@Prop() private post!: Post;\tget date() {\t\treturn `${this.post.datePosted.getDate()}/${this.post.datePosted.getMonth()}/${this.post.datePosted.getFullYear()}`;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;div.blogpost {\th2 {\t\ttext-decoration: underline;\t}\tp.meta {\t\tfont-style: italic;\t}}&lt;/style&gt;Adding conditional CSS to our componentWe now know how to add a conditional element to our component, but we can also have conditional CSS.We will use this conditional CSS so our highlighted blog post is also visually highlighted.We can add in our div with class blogpost an extra v-bind directive:&lt;div class=\"blogpost\" v-bind:class=\"{ highlighted: post.highlighted }\"&gt;...&lt;/div&gt;With v-bind we define to which attribute we want to bind after the colon.So in our case, v-bind:class results in a binding with the class attribute in our HTML.v-bind:class accepts an object as parameter in which each key should be mapped to a boolean.For each key that is mapped to a truthy value, that key is added as a class to the HTML tag on which the v-bind is located.You will notice that we use v-bind to bind to the class attribute but that this attribute already exists on our HTML element.This is no problem as Vue will simply concatenate all the values.In the case that post.highlighted is truthy, we will thus end up with:&lt;div class=\"blogpost highlighted\"&gt;...&lt;/div&gt;And when it is falsy, we end up with:&lt;div class=\"blogpost\"&gt;...&lt;/div&gt;We extend our .blogpost to give the blog posts a width, center them and add a border with a background:div.blogpost {\twidth: 400px;\tmargin: 0 auto;\t&amp;.highlighted {\t\tborder: 1px solid #f4d942;\t\tbackground: #fff3b2;\t}\t...}In our browser it looks like this:  Note that we also have a shorter version of v-bind:attributename which is :attributename.So we can shorten v-bind:class to this:&lt;div class=\"blogpost\" :class=\"{ highlighted: post.highlighted }\"&gt;...&lt;/div&gt;We end up with this as our BlogPost component:&lt;template&gt;\t&lt;div class=\"blogpost\" :class=\"{ highlighted: post.highlighted }\"&gt;\t\t&lt;h2&gt;{{ post.title }}&lt;/h2&gt;\t\t&lt;p v-if=\"post.highlighted\"&gt;This post is highlighted!&lt;/p&gt;\t\t&lt;p&gt;{{ post.body }}&lt;/p&gt;\t\t&lt;p class=\"meta\"&gt;Written by {{ post.author }} on {{ date }}&lt;/p&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Prop, Vue } from 'vue-property-decorator';export interface Post {\ttitle: string;\tbody: string;\tauthor: string;\tdatePosted: Date;\thighlighted?: boolean;}@Componentexport default class BlogPost extends Vue {\t@Prop() private post!: Post;\tget date() {\t\treturn `${this.post.datePosted.getDate()}/${this.post.datePosted.getMonth()}/${this.post.datePosted.getFullYear()}`;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;div.blogpost {\twidth: 400px;\tmargin: 0 auto;\t&amp;.highlighted {\t\tborder: 1px solid #f4d942;\t\tbackground: #fff3b2;\t}\th2 {\t\ttext-decoration: underline;\t}\tp.meta {\t\tfont-style: italic;\t}}&lt;/style&gt;Using events in a componentAs a final extension to our blog, we also want to add some dynamic behaviour by reacting to events.For our example, we will bind a button to the click event in our App component with the v-on:click directive.&lt;button v-on:click=\"toggleHighlightedPostsVisibility\"&gt;Show/hide highlighted posts&lt;/button&gt;The syntax to bind to events is v-on:eventname.We can also use the shorthand version which is @eventname:&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;Show/hide highlighted posts&lt;/button&gt;After that we write the event handler along with some variables in our component.The code block is followed by an explanation about what we have done exactly:export default class App extends Vue {\t// ...\tpublic showHighlighted: boolean = true;\tprivate blogPosts: Post[] = [];\tget visibleBlogPosts() {\t\treturn this.blogPosts.filter((post: Post) =&gt; post.highlighted === undefined ||  post.highlighted === this.showHighlighted);\t}\tpublic toggleHighlightedPostsVisibility() {\t\tthis.showHighlighted = !this.showHighlighted;\t}\t// ...}First what we did was add the showHighlighted boolean.This is to keep track whether we should show or hide the highlighted blog posts.We also wrote a getter to only show the blog posts that are allowed to be shown.In our filter, we check if the highlighted member is defined and if so, we check if it equals our showHighlighted variable.The reason why we write this in a getter, is that we want to avoid putting business logic in our template.Thus we opt for writing a getter which is the equivalent of a computed property in Vue JavaScript.After this we have to adapt the v-for in our template so that we use the new getter:&lt;BlogPost v-for=\"blogPost in visibleBlogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;As a small bonus, we will make the text in our button dynamic.Currently we have Show/hide highlighted posts as the text but it would be cleaner if we showed Show highlighted posts and Hide highlighted posts depending on the state of the component.We update the button to the following code:&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;{{ showHighlighted ? 'Hide' : 'Show' }} highlighted posts&lt;/button&gt;In the end, we end up visually with this:  And our App component looks like this:&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;h1&gt;Elke's fantastic blog&lt;/h1&gt;\t\t&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;{{ showHighlighted ? 'Hide' : 'Show' }} highlighted posts&lt;/button&gt;\t\t&lt;BlogPost v-for=\"blogPost in visibleBlogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import BlogPost, { Post } from './components/BlogPost.vue';@Component({\tcomponents: {\t\tBlogPost,\t},})export default class App extends Vue {\tpublic showHighlighted: boolean = true;\tprivate blogPosts: Post[] = [\t\t{\t\t\ttitle: 'My first blogpost ever!',\t\t\tbody: 'Lorem ipsum dolor sit amet.',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 18),\t\t},\t\t{\t\t\ttitle: 'Look I am blogging!',\t\t\tbody: 'Hurray for me, this is my second post!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 19),\t\t\thighlighted: true,\t\t},\t\t{\t\t\ttitle: 'Another one?!',\t\t\tbody: 'Another one!',\t\t\tauthor: 'Elke',\t\t\tdatePosted: new Date(2019, 1, 20),\t\t},\t];\tget visibleBlogPosts() {\t\treturn this.blogPosts.filter((post: Post) =&gt; post.highlighted === undefined ||  post.highlighted === this.showHighlighted);\t}\tpublic toggleHighlightedPostsVisibility() {\t\tthis.showHighlighted = !this.showHighlighted;\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;5. Using your first pluginVue comes without any libraries, it is a super clean and lean framework where even functionality for HTTP calls is not present.However, every component is a Vue object and can be extended with a $http member that you can use in your code to perform HTTP calls.To provide this $http member, we have to add the correct plugin to our code.In the awesome-vue project on GitHub, we can find an extensive list of HTTP plugins.We will use axios as our HTTP library but we will use vue-axios for the bindings with Vue in TypeScript as axios does not provide the necessary typings for axios in Vue.Installing a pluginWe follow the installation instructions for vue-axios which are pretty straightforward:$ npm i axios vue-axiosAs you noticed we also installed axios.This is because vue-axios only focuses on the TypeScript bindings for Vue and does not include the actual axios library.Vue-axios basically turns the axios library into a plugin compatible for Vue.After that, we have to signal to Vue that we want to use this plugin.We add a Vue.use(plugin, options) statement in our main.ts so it looks like this:import axios from 'axios';import Vue from 'vue';import VueAxios from 'vue-axios';import App from './App.vue';Vue.config.productionTip = false;Vue.use(VueAxios, axios);new Vue({\trender: (h) =&gt; h(App),}).$mount('#app');The important part is that we put the Vue.use(...) statement before we actually bootstrap the application with new Vue(...).The effect of adding a pluginSo we have added a plugin, but what does that actually mean?What is the effect on our Vue code?The main effect is that we now have $http accessible in every Vue component.This means that we can now have this.$http in our classes in which a unique instance of the axios library for the whole application will be plugged.When we check the typings  from axios, we find that we now have methods like get(...), post(...) and many more default REST methods available in our code through the $http member in which an instance of axios is present.Methods like get(...) and post(...) will also exist on other HTTP libraries that we can add to Vue.It is not obliged by Vue to provide these same functionalities in another HTTP library.But it makes sense for library creators to comply to the standard set by the HTTP library of preference as chosen by Vue, in this case axios.Otherwise it would not be easy to change certain libraries for another one.Using the axios plugin for performing HTTP callsA Vue component has multiple lifecycle hooks with the most interesting ones for what we want to do: created() and mounted().Created is called by Vue when the object is created: reactive data is set up, event callbacks are ready and the object is not yet mounted on the DOM.The Vue object will thus be ready to go but it will not yet be visible to the user.The mounted hook is used for when the element is mounted into the HTML DOM, which means the rendering is performed by the browser.&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t...\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';@Component({})export default class App extends Vue {\tprivate created() {\t\tconsole.log('The app is created!');\t}\tprivate mounted() {\t\tconsole.log('The app is mounted!');\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {  ...}&lt;/style&gt;There are two reasons why we want to start our HTTP calls in the created method.The first reason is that we can limit the amount of time the user has to wait for data to be loaded and shown on the screen.The second one is that the mounted hook is not called when we would use serverside rendering.To ensure that our code is compatible with all use cases, we place the HTTP calls in the created method of our App.vue which results in this component:&lt;template&gt;\t&lt;div id=\"app\"&gt;\t\t&lt;h1&gt;Elke's fantastic blog&lt;/h1&gt;\t\t&lt;button @click=\"toggleHighlightedPostsVisibility\"&gt;{{ showHighlighted ? 'Hide' : 'Show' }} highlighted posts&lt;/button&gt;\t\t&lt;BlogPost v-for=\"blogPost in visibleBlogPosts\" :post=\"blogPost\" :key=\"blogPost.title\" /&gt;\t&lt;/div&gt;&lt;/template&gt;&lt;script lang=\"ts\"&gt;import { Component, Vue } from 'vue-property-decorator';import BlogPost, { Post } from './components/BlogPost.vue';import { AxiosResponse } from 'axios';@Component({\tcomponents: {\t\tBlogPost,\t},})export default class App extends Vue {\tpublic showHighlighted: boolean = true;\tprivate blogPosts: Post[] = [];\tget visibleBlogPosts() {\t\treturn this.blogPosts.filter((post: Post) =&gt; post.highlighted === undefined ||  post.highlighted === this.showHighlighted);\t}\tpublic toggleHighlightedPostsVisibility() {\t\tthis.showHighlighted = !this.showHighlighted;\t}\tprivate created() {\t\tthis.$http.get('http://localhost:3000/blogposts').then((response: AxiosResponse) =&gt; {\t\t\tthis.blogPosts = response.data.map((val: any) =&gt; ({\t\t\t\ttitle: val.title,\t\t\t\tbody: val.body,\t\t\t\tauthor: val.author,\t\t\t\tdatePosted: new Date(val.datePosted),\t\t\t\thighlighted: val.highlighted,\t\t\t}));\t\t});\t}}&lt;/script&gt;&lt;style lang=\"scss\"&gt;#app {\tfont-family: 'Avenir', Helvetica, Arial, sans-serif;\t-webkit-font-smoothing: antialiased;\t-moz-osx-font-smoothing: grayscale;\ttext-align: center;\tcolor: #2c3e50;\tmargin-top: 60px;}&lt;/style&gt;As you can see we have added a private created method since this should not be publicly available to other components.We call an API and map the response into our Post array.Now we need to set up our API.How we have set up a local APITo simulate a real API call, we set up json-server, a small tool that launches a web server with a REST API that serves a JSON file which we call db.json present in our assets folder:{\t\"blogposts\": [\t\t{\t\t\t\"title\": \"My first blogpost ever!\",\t\t\t\"body\": \"Lorem ipsum dolor sit amet.\",\t\t\t\"author\": \"Elke\",\t\t\t\"datePosted\": \"2019-01-18\"\t\t},\t\t{\t\t\t\"title\": \"Look I am blogging!\",\t\t\t\"body\": \"Hurray for me, this is my second post!\",\t\t\t\"author\": \"Elke\",\t\t\t\"datePosted\": \"2019-01-19\",\t\t\t\"highlighted\": true\t\t},\t\t{\t\t\t\"title\": \"Another one?!\",\t\t\t\"body\": \"Another one!\",\t\t\t\"author\": \"Elke\",\t\t\t\"datePosted\": \"2019-01-20\"\t\t}\t]}We install json-server with NPM and then we launch it with:$ npm i json-server$ json-server src/assets/db.jsonBy default, json-server will launch on port 3000.When we launch wordvue and open it in the browser, we will see that the blog posts are now coming from our local API.Now you know how to install a plugin and retrieve data with axios over HTTP.6. Your first deploymentDevelopment mode versus production modeJust like other frontend frameworks, Vue has its development mode and production mode. The development mode is available with:$ npm run serveWhile production mode is available with:$ npm run buildNow what is the difference between both modes?            Development mode      Production mode                  CSS &amp; HTML bundled into JS      CSS separately, HTML bundled into JS              Warnings in console      No warnings in console              Additional checks to identify warnings      No additional checks, ignores any situation that would trigger warnings              Everything in one app.js      Separate app.js and vendor.js              Heavy use of eval() for hot reload      No use of eval(), no hot reloading necessary              Basic bundling of all code, use of minified libraries only when available      Bundling &amp; maximum minification              No minification of index.html      Minification of index.html        vendor.js: Contains all the node_modules code that your project uses  eval(): JavaScript function that executes strings as if it’s a line of code and should never be used in productionAll the minification, avoiding the use of eval(), removing of warning checks and so on results in a much smaller size of the code.If we do a npm run serve and check our Developer Tools in Chrome, we see the size of our application:  In development mode, our application is more than 2MB large. We see the index.html alongside a generated app.js that contains all our own code and all the node_modules that we are using.While when we build with the production mode via npm run build, we get an application that is in total less than 125KB:  The only thing we changed to get a dist folder that is so small, was adding a vue.config.js file in the root of our folder which exports an object with the settings we want:module.exports = {    productionSourceMap: false};We only had to add the productionSourceMap set to false to disable the creation of source maps.More configurations can be found at cli.vuejs.org but most of the configuration is already done for a maximum optimised production build.Building for productionWhen running the npm run build command, you’ll get the following output:  So what the Vue CLI does is take all the SCSS out of the components and minifies it, compiles all the components into an app.js file and treeshakes  all used libraries into a chunk-vendors.js file.After that, it Gzips all those files to ensure that everything is as light as possible.If you have any assets, it will also clone those into the dist folder.The result is a dist folder which contents you can directly deploy onto your favourite server.7. ConclusionCongratulations, you have built your very first Vue application with TypeScript!The end result can be found in my GitHub repository  so you can see the working version.You now know how to write a basic component with the use of decorators, create a component structure and fill it with data coming from an API.After that you can also deploy it onto a server.A next step would be to add routing, add a store or an i18n library.Vue is a lightweight framework that primarily focuses on visualisation.If you want to add more functionality, you will have to rely on plugins who either support Vue integration directly or you can use a plugin like vue-axios that will facilitate the integration of another library like axios.8. Resources and further reading  Vue CLI: cli.vuejs.org  Awesome-vue, overview of Vue plugins: github.com/vuejs/awesome-vue  Axios, HTTP library: github.com/axios/axios  Vue-axios, typings for using Axios in Vue: github.com/imcvampire/vue-axios  Vue styleguide: vuejs.org/v2/style-guide  Vue-class-component: github.com/vuejs/vue-class-component  JSON server: github.com/typicode/json-server  Wordvue repository: github.com/ordina-jworks/vue-typescript-wordvue"
      },
    
      "frontend-2019-02-21-observables-html": {
        "title": "Observables: The right way",
        "url": "/frontend/2019/02/21/observables.html",
        "image": "/img/observables-the-right-way/cover.png",
        "date": "21 Feb 2019",
        "category": "post, blog post, blog",
        "content": "Table of contents  Intro  Setup  Refactoring  Example  ConclusionIntroDuring my consultancy projects, I often come across the same implementations and problems when colleagues are trying to implement an observable strategy.A lot of frameworks are offering observables out of the box for their communication layer.Almost all Promises are replaced by Observables nowadays.Angular 2+ HttpModule for example, is using the rxjs library.Each http.get() is returning an Observable&lt;HttpResponse&gt;.The setup is almost always the same.A (visual) component needs to render some data.So next to the component (HTML/template for view, and a JavaScript(TypeScript) component as the controller), a service gets created.This service’s purpose is to provide data to the component’s controller by calling the HttpClient’s functions (POST, GET, DELETE, PATCH, …) and returning the Observable to the component.Sometimes they are remapping the Observable&lt;HttpReponse&gt;to a more defined type, for example Observable&lt;MyData&gt;, by using one of the rxjs operators such as flatmap, map, … .All of this works pretty well, as long as only one component is in need of the data and its changes.With changes, I refer to refreshing the data, or requerying it with another filter, paging, or …  Every time the query parameters change, the component is just executing the same call in the service, which in turn is calling the right HttpClient-function.Again an (new) Observable is returned.Again the result can get remapped before throttling it back.The subscriber, the component controller in this case (or the HTML if you are using Angular’s async pipe), receives the remapped data.What happens when we have another component that is in need of this data (or maybe just a part of it)?Let’s say we have a header and a datatable.And we are NOT using push events, but simple REST calls (for the sake of this explanation).The datatable is the component we were talking about earlier.It needs to display messages in a simple datatable.The header is the second component that needs this data.It needs to display the number of unread or critical messages.datatable.component.tsthis._dataService.get(this.filter).subscribe((page:Page) =&gt; {\tthis.data = page.data;})header.component.tsthis._dataService.get(this.filter).subscribe((page:Page) =&gt; {\tthis.count = page.number;});data.service.tspublic get(filter?: Filter): Observable&lt;Page&gt; {\treturn this._http.get('urlToData' + this.parseFilterToString(filter)).pipe(\t\ttap((data) =&gt; this.logService.log(data)),\t\tmap((response) =&gt; {\t\t\tlet page: Page = new Page();\t\t\tpage.data = response.body;\t\t\tpage.number = response.headers.get('x-count');\t\t})\t);}In the setup we have so far, both components will use the same service for requesting the data.They will both subscribe to an Observable, however, it will be a different one.When you refresh the data in the datatable, the header will not receive a new value in its subscription and therefore will still show the old number of messages.In some use cases this might be the desired outcome, but in most cases you want more components to be able to subscribe to the same Observable.SetupIn our new setup, we want both, header and datatable, to subscribe to the same Observable, so a call for new data will result in an update in the datatable and the header.To make this happen, we will use some kind of layer in between them.This new layer will provide our components with one and only one and the same Observable and will mask the communication layer from the view’s controller.Both components will subscribe to this service, so they both get updated with the same result.We can do this by creating a simple Subject in our service and returning it as an Observable to our components.We can then implement other calls for this service that will trigger an update of the data, and send it through the subject to both components.Because we are not providing a filter when we call the getter for the Observable (Subject) we should also find a way of providing the filter to the service, before requerying the data.This means we are going to use one shared filter, for both components, which makes sense in this case, but not in all use cases.  RefactoringWe actually don’t need to refactor any of our components.They will still subscribe to an Observable of the service, and react on the incoming data.The service however will get refactored.Start by defining the Subject as a local property, and because we are going to implement a getter, we can make it private.data.service.tsprivate _data$: Subject&lt;Page&gt; = new BehaviorSubject&lt;&gt;({});...public data(): Observable&lt;Page&gt; {\treturn this._data$ as Observable&lt;Page&gt;;}...public reload(): void {\tthis._http.get('urlToData' + this.parseFilterToString(this._filter)).pipe(\t\ttap((data) =&gt; this.logService.log(data)),\t\tmap((response) =&gt; {\t\t\tlet page: Page = new Page();\t\t\tpage.data = response.body;\t\t\tpage.number = response.headers.get('x-count');\t\t}).subscribe((page: Page) =&gt; {\t\t\tthis._data$.next(page);\t\t})\t);}This way, every component or service that is subscribing on this subject, is getting data when some other component or service triggers the reload.There are even a lot more options to this setup:  Clearing data  Resetting to default filter  Refreshing the current filter  Caching data  Manipulating data through other services or components  Adding an event consumer that also updates the datatable  …ExampleI’ve build a simple example to demonstrate this behaviour.A header that is displaying an alert icon when there are unread, critical messages.A sidebar that is displaying the amount of unread messages next to its navigation link, and an overview of the messages, with a basic paging implementation.  A simple backend that is written in Node.js with Express provides a few endpoints:  api/message (with paging and filter, although the filter isn’t implemented in the frontend example.)  api/message/:id (not used in the example)  api/stream  api/refreshThe service is not reloading data as long as the page or the filter hasn’t changed.While the service is still loading the data, a new reload will not fetch again the data.You can find the code on GitLab.  Server-Sent Events are added to update the read status of a message when the envelope gets clicked.This will also trigger the observable.To run front- and backend together, execute the following command in the root of the project:$ npm run startThis way, a proxy is added to the serve command to overcome CORS blocking going from localhost:4200 to localhost:3000Don’t mind the backend server, it’s a quick and dirty solution and is not implemented as it should.ConclusionAlthough observables are a great feature, and are easy to use, it’s always better to have your own layer of control.Especially when it comes to using observables from frameworks.I can accept, for simple applications, that you don’t want to ‘over-architect’.But in most cases, you want to control the distribution yourself.For those of you that know Redux (RxJS), you can compare this implementation with effects and store-subscriptions.If you trigger an effect, you will only see the result when you have subscribed to the ‘key’ that is responsible for providing you with the data,and not to the ‘key’ that is responsible for triggering the effect."
      },
    
      "cloud-2019-01-14-infrastructure-as-code-with-terraform-and-aws-serverless-html": {
        "title": "Infrastructure as code: Terraform and AWS Serverless",
        "url": "/cloud/2019/01/14/Infrastructure-as-code-with-terraform-and-aws-serverless.html",
        "image": "/img/2019-01-14-Infrastructure-as-code-with-terraform-and-aws-serverless/featured-image.png",
        "date": "14 Jan 2019",
        "category": "post, blog post, blog",
        "content": "Table of content  Infrastructure as Code  Introduction and demo  Creating the application  Prerequisites  Terraform: the basics  General  Database: DynamoDB  IAM  Lambda Functions  API Gateway  Endgame  Resources and further readingInfrastructure as CodeInfrastructure as Code (IaC) is a way of managing your devices and servers through machine-readable definition files. Basically, you write down how you want your infrastructure to look like and what code should be run on that infrastructure. Then, with the push of a button you say “Deploy my infrastructure”. BAM, there is your application, running on a server, against a database, available through an API, ready to be used!And you just defined all of that infrastructure using IaC.  IaC is a key practice of DEVOPS teams and integrates as part of the CI/CD pipeline.A great Infrastructure as Code tool is Terraform by HashiCorp.(https://www.terraform.io/)Personally I use it to provide and maintain infrastructure on AWS.And I’ve had a great experience doing that.  Introduction and demoI will demonstrate IaC by working out an example. We are going to set up an application on AWS.I provisioned the code on GitLab: https://gitlab.com/nxtra/codingtips-blog.A user can enter a coding tip and see all the coding tips that other users have entered.The tips are stored in a NoSQL database which is AWS DynamoDB.Storing and retrieving these tips is done by the Lambda Functions which fetch or put the tips from and to the database.For the application to be useful, users have to be able to call these Lambda Functions.So we expose the Lambda Functions through AWS API Gateway. Here is an architectural overview of the application:  You could couple these functions to a web page where users can enter tips and see all tips that have been given.Below you see the final result:  Let’s dive in!Creating the application  I will now go over the steps to set up the application you see in the demo above.IaC is the main focus.I will show the code and AWS CLI commands that are necessary but I will not explain them in detail since that is not the purpose of this blog.I’ll focus on the Terraform definitions instead.You are welcome to follow along by cloning the repository that I linked to in this blog post.Prerequisites  Install Terraform  Install AWS CLI  Checkout the repository on GitLab: https://gitlab.com/nxtra/codingtips-blog  Be ready to get your mind blown by IaCTerraform: the basicsThe main things you’ll be configuring with Terraform are resources.Resources are the components of your application infrastructure.E.g: a Lambda Function, an API Gateway Deployment, a DynamoDB database, …A resource is defined by using the keyword resource followed by the type and the name.The name can be arbitrarily chosen.The type is fixed.For example:resource \"aws_dynamodb_table\" \"codingtips-dynamodb-table\"To follow along with this blog post you have to know two basic Terraform commands.  terraform applyTerraform apply will start provisioning all the infrastructure you defined.Your databases will be created.Your Lambda Functions will be set up.The API Gateway will be set in place.  terraform destroyTerraform destroy will remove all the infrastructure that you have set up in the cloud.If you are using Terraform correctly you should not have to use this command.However should you want to start over, you can remove all the existing infrastructure with this command.No worries, you will still have all the infrastructure neatly described on your machine because you are using Infrastructure as Code.We’ll put all infrastructure that is defined using Terraform in the same folder.The files need to have a .tf extension.GeneralLet’s start out by creating a file general.tf.provider \"aws\" {  region = \"eu-west-1\"}# variablesvariable \"lambda_version\"     { default = \"1.0.0\"}variable \"s3_bucket\"          { default = \"codingtips-node-bucket\"}The provider block specifies that we are deploying on AWS.You also have the possibility to mention credentials that will be used for deploying here.If you have correctly set up the AWS CLI on your machine there will be default credentials in your .aws folder.If no credentials are specified, Terraform will use these default credentials.Variables have a name which we can reference from anywhere in our Terraform configuration. For example we could reference the s3_bucket variable with ${var.s3_bucket).This is handy when you are using the same variable in multiple places.I will not use too many variables throughout this blog post since that will add more references to your Terraform configuration and I want it to be as clear as possible.Database: DynamoDB  Let’s start with the basis.Where will all our coding tips be stored? That’s right, in the database.This database is part of our infrastructure and will be defined in a file I named dynamo.tf.resource \"aws_dynamodb_table\" \"codingtips-dynamodb-table\" {  name = \"CodingTips\"  read_capacity = 5  write_capacity = 5  hash_key = \"Author\"  range_key = \"Date\"  attribute = [    {      name = \"Author\"      type = \"S\"    },    {      name = \"Date\"      type = \"N\"    }]}Since Dynamo is a NoSQL database, we don’t have to specify all attributes upfront.The only thing we have to provide are the elements that AWS will use to build the partition key with.When you provide a hash key as well as a sort key, AWS will combine these to make a unique partition key.Mind the word UNIQUE.Make sure this combination is unique.  DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. All items with the same partition key value are stored together, in sorted order by sort key value.– from AWS docs: DynamoDB Core ComponentsFrom the attribute definitions in dynamo.tf it is clear that Author (S) is a string and Date (N) should be a number.IAM  Before specifying the Lambda Functions we have to create permissions for our functions to use.This makes sure that our functions have permissions to access other resources (like DynamoDB).Without going too deep into it, the AWS permission model works as follows:  Provide a resource with a role  Add permissions to this role  These allow the role to access other resources:          permissions for triggering another resource (eg. Lambda Function forwards logs to CloudWatch)      permissions for being triggered by another resource (eg. Lambda Function may be triggered by API Gateway)      # ROLES# IAM role which dictates what other AWS services the Lambda function# may access.resource \"aws_iam_role\" \"lambda-iam-role\" {  name = \"codingtips_lambda_role\"  assume_role_policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": \"sts:AssumeRole\",      \"Principal\": {        \"Service\": \"lambda.amazonaws.com\"      },      \"Effect\": \"Allow\",      \"Sid\": \"\"    }  ]}EOF}# POLICIESresource \"aws_iam_role_policy\" \"dynamodb-lambda-policy\"{  name = \"dynamodb_lambda_policy\"  role = \"${aws_iam_role.lambda-iam-role.id}\"  policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Effect\": \"Allow\",      \"Action\": [        \"dynamodb:*\"      ],      \"Resource\": \"${aws_dynamodb_table.codingtips-dynamodb-table.arn}\"    }  ]}EOF}In the example above, the first resource that is defined is an aws_iam_role.This is the role that we will later give to our Lambda Functions.We then create the aws_iam_role_policy resource which we link to the aws_iam_role.The first aws_iam_role_policy is giving this role permission to invoke any action on the specified DynamoDB resource.The second role_policy allows a resource with this role to send logs to CloudWatch.A couple of things to notice:  The aws_iam_role and the aws_iam_role_policy are connected by the role argument of the role_policy resource  In the statement attribute of the aws_iam_role_policy we grant (Effect attr.) permission to do some actions (Action attr.) on a certain resource (Resource attr.)  A resource is referenced by its ARN or Amazon Resource Name which uniquely identifies this resource on AWS  There are two ways to specify an aws_iam_role_policy:          using the until EOF syntax (like I did here)      using a separate Terraform aws_iam_policy_document element that is coupled to the aws_iam_role_policy        The dynamodb-lambda-policy allows all actions on the specified DynamoDB resource because under the Action attribute it states dynamodb:*You could make this more restricted and mention actions like\"dynamodb:Scan\", \"dynamodb:BatchWriteItem\",\"dynamodb:PutItem\"Lambda Functions  There are two Lambda Functions that are part of this application.The first Lambda is used to get or retrieve the coding tips from the database further referenced as the getLambda.The second Lambda is used to post or send the coding tips to the database further referenced as the postlambda.I am not going to copy paste the code of the Lambda Functions in here.You can check it out in the repository linked to this blog (GitLab repository: https://gitlab.com/nxtra/codingtips-blog).Here I will demonstrate the example of the getLambda function.The postLambda is deployed in the same way and you can find the Terraform definitions in the Git repository.A Lambda Function is a little different from the other infrastructure we defined here.Not only do we need a Lambda Function as infrastructure.We also need to specify the code that runs in this Lambda Function.But where will AWS find that specific code when deploying the Lambda Function?They don’t have access to your local machine, have they?That is why you first need to ship your code to a S3 Bucket on AWS where it can be found when your Function is being deployed.That also means creating an S3 Bucket, which you can do with this command when you want it in region eu-west-1 (Ireland):aws s3api create-bucket --bucket codingtips-node-bucket --region eu-west-1 --create-bucket-configuration LocationConstraint=eu-west-1Now you have to zip the code of your Lambda Functions:zip -r getLambda.zip index.jsAnd upload that file to s3:aws s3 cp getLambda.zip s3://codingtips-node-bucket/v1.0.0/getLambda.zipMind that I am sending it to a bucket named codingtips-node-bucket in a folder v1.0.0 with filename getLambda.zip.Okay, the code is where it needs to be.Now let’s see how we specify these functions using Terraform.resource \"aws_lambda_function\" \"get-tips-lambda\" {  function_name = \"codingTips-get\"  # The bucket name as created earlier with \"aws s3api create-bucket\"  s3_bucket = \"${var.s3_bucket}\"  s3_key = \"v${var.lambda_version}/getLambda.zip\"  # \"main\" is the filename within the zip file (index.js) and \"handler\"  # is the name of the property under which the handler function was  # exported in that file.  handler = \"index.handler\"  runtime = \"nodejs8.10\"  memory_size = 128  role = \"${aws_iam_role.lambda-iam-role.arn}\"}resource \"aws_lambda_permission\" \"api-gateway-invoke-get-lambda\" {  statement_id  = \"AllowAPIGatewayInvoke\"  action        = \"lambda:InvokeFunction\"  function_name = \"${aws_lambda_function.get-tips-lambda.arn}\"  principal     = \"apigateway.amazonaws.com\"  # The /*/* portion grants access from any method on any resource  # within the specified API Gateway.  source_arn = \"${aws_api_gateway_deployment.codingtips-api-gateway-deployment.execution_arn}/*/*\"}  Notice that we tell Terraform the S3 Bucket and directory to look for the code  We specify the runtime and memory for this Lambda Function  index.handler points to the file and function where to enter the code  The aws_lambda_permission resource is the permission that states that this Lambda Function may be invoked by the API Gateway that we createdAPI Gateway  I kept the most difficult one for last.On the other hand, it is also the most interesting.I hand Terraform a Swagger definition of my API.You can also do this without Swagger, but then you will have to specify a lot more resources.The Swagger API definition looks as follows:swagger: '2.0'info:  version: '1.0'  title: \"CodingTips\"schemes:  - httpspaths:  \"/api\":    get:      description: \"Get coding tips\"      produces:        - application/json      responses:        200:          description: \"The codingtips request successful.\"          schema:            type: array            items:              $ref: \"#/definitions/CodingTip\"      x-amazon-apigateway-integration:        uri: ${get_lambda_arn}        passthroughBehavior: \"when_no_match\"        httpMethod: \"POST\"        type: \"aws_proxy\"    post:      description: \"post a coding tip\"      consumes:        - application/json      responses:        200:          description: \"The codingtip was added successfully\"      x-amazon-apigateway-integration:        uri: ${post_lambda_arn}        passthroughBehavior: \"when_no_match\"        httpMethod: \"POST\"        type: \"aws_proxy\"definitions:  CodingTip:    type: object    description: \"A coding tip\"    properties:      tip:        type: string        description: \"The coding tip\"      date:        type: number        description: \"date in millis when tip was entered\"      author:        type: string        description: \"Author of the coding tip\"      category:        type: string        description: \"category of the coding tip\"    required:      - tipIf you do not know Swagger yet, copy the above and paste it in the online (Swagger Editor).This will grant you a nice visual overview of the API definition.  There is only one AWS specific thing in the Swagger specification above and that is x-amazon-apigateway-integration.This is specifying the details of how the API is integrating with the backend.  Remark that this is always a POST even if the HTTP method of the resource path is a GET  aws_proxy means that the request is passed to the Lambda Function without manipulation  when_no_match passes the request body to the backend without tranforming it when no requestTemplate is specified for the Content-Type  uri is referencing a variable eg. ${get_lambda_arn} that Terraform passes to the Swagger definition.We’ll see this in a minute.As I already mentioned, using Swagger to define your API Gateway has some advantages:  It keeps your Terraform more concise  You can use this Swagger to get a nice representation of your APIresource \"aws_api_gateway_rest_api\" \"codingtips-api-gateway\" {  name        = \"CodingTipsAPI\"  description = \"API to access codingtips application\"  body        = \"${data.template_file.codingtips_api_swagger.rendered}\"}data \"template_file\" codingtips_api_swagger{  template = \"${file(\"swagger.yaml\")}\"  vars {    get_lambda_arn = \"${aws_lambda_function.get-tips-lambda.invoke_arn}\"    post_lambda_arn = \"${aws_lambda_function.post-tips-lambda.invoke_arn}\"  }}resource \"aws_api_gateway_deployment\" \"codingtips-api-gateway-deployment\" {  rest_api_id = \"${aws_api_gateway_rest_api.codingtips-api-gateway.id}\"  stage_name  = \"default\"}output \"url\" {  value = \"${aws_api_gateway_deployment.codingtips-api-gateway-deployment.invoke_url}/api\"}  We start by mentioning the aws_api_gateway_rest_api resource.It does what is says and provides an API Gateway REST API.          body references the Swagger file        The template_file datasource allows Terraform to use information that is not defined in Terraform (Swagger in our case)          Variables are passed to this template_file to fill the file        For a given rest-api to be usable, it has to be deployed          This is done by the aws_api_gateway_deployment resource      It references the REST API      It needs a stage which is like a ‘version’ or ‘snapshot’ of your APIThe stage name will be in the URL to invoke this API.        At last the URL on which the API can be invoked is outputted to the terminal/api is appended to have the correct resource pathEndgameAll right, let’s see it now.Does this actually work?Here I am running terraform apply within the repository linked to this blog.  Nice, it worked.And I only told Terraform about the infrastructure I wanted.The whole setup process goes automatically!You can now use the outputted URL to GET and POST coding tips.The body of the POST should look like:{  \"author\": \"Nick\",  \"tip\": \"Short sessions with frequent brakes\",  \"category\": \"Empowerment\"}When you need to couple the API endpoints to a frontend of your own design, you need to set the CORS headers correctly.If you want this challenge, there is another branch in the repository (cors-enabled) where I worked this out.Happy coding folks, Code that Infrastructure!Resources and further reading  Terraform website: Terraform.io  Terraform-Lambda-APIGateway: learn.hashicorp.com  Swagger editor: editor.swagger.io  Swagger official website: swagger.io"
      },
    
      "development-2019-01-10-flutter-html": {
        "title": "Flutter: Hybrid apps for mobile &amp; beyond.",
        "url": "/development/2019/01/10/Flutter.html",
        "image": "/img/2019-01-10-Flutter/Flutter.jpg",
        "date": "10 Jan 2019",
        "category": "post, blog post, blog",
        "content": "IntroMobile development has always intrigued me.Bringing data to life by visualizing it with different components and being able to carry your app along inside your pocket is something special.I still remember the excitement I felt when working on my first mobile app in university,even with the lack of good resources back then and the sluggish emulator which was available at that time.The mobile world has kept expanding with big improvements in resources, frameworks, tools and designs.From the first iPhone to having your refrigerator running your favorite apps, more and more possibilities and challenges have become available to keep you going.Nowadays, there are different paths that you can follow to create those apps.Native, hybrid or web apps, it all depends on what you want to achieve and how many resources are available.Each has its benefits and pitfalls, which doesn’t make the decision any easier.Do you want to give your audience the best native experience with great performance?Or does a hybrid app suffices where you might compromise in speed and in look &amp; feel of a native app?Flutter might be the answer, by providing you the best parts of both worlds.Beautiful native apps in record time  Flutter allows you to build beautiful native apps on iOS and Android from a single codebase.A promising statement which is presented to you when you browse to Flutter.io.A statement that explains perfectly what Flutter is in fact.Flutter is an open source mobile UI framework made by Google.With Flutter, you can build beautiful apps that run at native speed.Given the single codebase, you don’t have to develop the same app twice for both iOS and Android.Flutter is even the first-class citizen for Fuchsia, an upcoming mobile OS that is currently being developed by Google.Flutter apps follow platform conventions and interface details, so the scrolling, fonts, navigations, etc. will look natively respecting the specific platform.All of this results in beautiful apps that require less time and resources to develop and this without compromising on quality, features, performance or design.Performance at its coreHow did the Flutter team achieve all of this?Why aren’t there any downgrades in performance while being a hybrid app?The reason for this is that Flutter is built with performance in mind from the beginning.In fact, being performant was the main reason why the Flutter team started with the framework.Being hybrid was more of a side-effect because of the way the implementation is set up.Because Flutter is so performant, you are guaranteed that Flutter apps will run at 60 FPS and more.This leads to smooth animations and an instant responsive app which results in a great user experience.Your app will also render perfectly on older devices, while even some native apps may have trouble to keep running at a constant refresh rate.To achieve this performance, Flutter does something different than other hybrid solutions.The framework avoids having a JavaScript bridge between the app and the platform by using a language that compiles to native code.This bridge is typically the bottleneck when it comes to performance in hybrid solutions.The lack of the JavaScript bridge allows Flutter to communicate directly with the platform.Which language are we talking about?Well, all of this is made possible with Dart.‘Dart’ you say?Dart is a programming language that was also developed by Google.Its purpose is to build web, server and mobile apps.You can develop your app with Dart, which will either be compiled to JavaScript or into native code.The Flutter team considered different languages, but many of them had drawbacks in one of the four dimensions for evaluation that they used.Since Google had its own language ready to be used, the team also took a look at Dart.Dart scored high on all the requirements and criteria that the team had predefined, which is why the decision was obvious.There are some huge benefits when using Dart compared to other languages.One of them is the Dart runtime and compiler.This enables Dart code to be compiled both AOT (Ahead Of Time) and JIT (Just In Time).With AOT, Dart is compiled to native code which ensures that the execution is fast, high-performing and predictable.Your app will start up faster and it will feel smoother while running it.JIT enables stateful hot-reloading, which gives developers an extremely fast development cycle.Once you hit Save in your editor, the code changes are applied to your running app within a second without losing state.This gives productivity an enormous boost and helps you to reach your goals faster than before.No more refilling the same form to test some validation or navigating to a specific screen for you.It is really impressive to see hot reload in action, especially when you’re used to the development cycle within native mobile development.To me, it kind of feels like magic.You might be asking yourself if it is worth learning a new language for a new framework.From my experience, Dart is really easy to adopt, especially if you’re used to Java or JavaScript/TypeScript.Dart feels natural to use and is a powerful language.Most of the time when I’m playing around with Flutter I even don’t realize that I’m writing in another language.In fact, it just works writes.Everything is a widgetThe native performance of Flutter apps is great, but how can you use Flutter to build your app?The answer is widgets.They are the building blocks that Flutter uses to build up your interface.Widgets are responsible for the native look and feel that you want to create, so they are really important.With widgets you’ll be able to create beautiful apps, exactly how your design team imagined them to be.In Flutter, everything is a widget!Ranging from a button, an image, to the app itself.Even the padding, positioning or navigation are all defined by the use of widgets.You combine widgets to build up the interface to your liking.Flutter will generate a widget tree out of it and uses that tree to render the layout on the screen of the device.Flutter uses its own set of widgets, which assures you have a pixel perfect layout on every device.You’re not dependent, nor limited by the widgets provided by the platform.The only thing that Flutter needs is a canvas to draw on.You can compare Flutter with a game engine, or in this case rather an app engine.By providing its own set of widgets, you can customize all of them to your liking.This enables you to include your company branding through your app, ranging from colors to the shape of buttons.All the widgets are written in Dart.Because Flutter is an open source project, you can use the source code as a reference while applying the widgets.With Flutter you don’t have to worry anymore about support libraries to render your apps on old devices or about OEMs that decide to alter the platform widgets because they can.You can even enjoy the beauty of Material Design on devices that were released years before Material Design was introduced.Your app will also be future proof, as new design implementations of platform widgets won’t affect or break the layout of your app.If there are any breaking changes with future OS versions, then it’s a bug for Flutter to resolve instead for you.Flutter even added notch-support for the iPhone X before the phone was released.Here you can find a quick example how you can build up your layout by combining different widgets together.The JWorks widget is rendered inside a default Material app template on the iPhone XS.import 'package:flutter/material.dart';class JWorksWidget extends StatelessWidget { // I'm a widget  @override  Widget build(BuildContext context) {    return Card( // I'm a widget too      elevation: 4.0,      child: Padding( // Yep, another widget        padding: const EdgeInsets.all(16.0),        child: Image.network( // We're all widgets!          'https://ordina-jworks.github.io/img/jworks/jworks-400x400.png',        ),      ),    );  }}One hybrid framework to rule them allOn the 4th of December 2018 the first Flutter Live event was hosted.A lot of exciting announcements were made, which definitely shows that the Flutter team is determined to keep improving.The first stable version was released almost two years after the first Alpha version was released.Another huge announcement was revealed about the future plans of Flutter.The first step is Hummingbird, or Flutter for the web.Since Dart can also compile to JavaScript, this was a logical step to take.Being able to run on the web, you can also create a Progressive Web App with Flutter, so your mobile app which runs in the browsers becomes a web app.Appception right there.Google is also working to bring Flutter to desktop with Flutter Desktop Embedding.To prove this concept, the presentation of Flutter Live was running in a Flutter app on a laptop.This means that Flutter won’t be a mobile SDK solely, but it might become the way to go SDK for hybrid apps across mobile, web and desktop.Maybe later on, any device that can render pixels will be able to run Flutter apps.Furthermore, the Flutter team is working to provide integration between Flutter and your existing native apps.Not everyone can start from scratch, so having a way to gradually move your app to Flutter is a very welcome addition if you’re planning to do so.This project, which contains all the APIs and tooling, is named Add2App and is currently in a preview state.With Add2App, you can launch a view containing your new Flutter app from your existing native app.You can also work in the opposite way with the introduction of Platform Views.These views allow you to add native content inside your Flutter apps.Platform Views unlocks Flutter to render Google Maps and WebViews inside the Flutter app.ConclusionFlutter keeps getting better and the community keeps on growing.More and more companies start to embrace Flutterand developers are excited and positive when using Flutter in their apps.It surely looks promising that Flutter can become a big player in the mobile world.While I was getting in touch with Flutter and digging through the documentation and examples,I got more and more fascinated about all the possibilities that you can achieve with this new mobile SDK.The Flutter journey reminded me back of the feeling I had when I was working on my first mobile app,discovering a new mobile world full of possibilities, this time built out of widgets."
      },
    
      "conference-2018-12-17-devoxx-ma-html": {
        "title": "Devoxx MA 2018",
        "url": "/conference/2018/12/17/Devoxx-MA.html",
        "image": "/img/devoxx-ma-2018/devoxx-ma.png",
        "date": "17 Dec 2018",
        "category": "post, blog post, blog",
        "content": "  Devoxx MA is a yearly conference in Morocco.Previously it was held in Casablanca but for their 4th edition, xHub, the organisation behind Devoxx MA, decided to hold it in Marrakesh, in the lovely Palm Plaza Hotel on the 27th, 28th and 29th of November 2018.Aside from the conference, the speakers were also offered an exclusive trip.Four colleagues of Ordina JWorks: Yannick De Turck, Tim Ysewyn, Tom Van den Bulck and Maarten Casteels attended both the conference and the speakers trip.Three of them were also featured as speaker.In this blog post we share our impressions and experiences.Table of contents  The Speakers Dinner  VueJS Animation In Action by Charles-Philippe Bernard  MockK, The Idiomatic Mocking Framework For Kotlin by Yannick De Turck  Stream Processing Live Traffic Data with Kafka Streams by Tom Van den Bulck and Tim Ysewyn  Applying (D)DDD and CQ(R)S to Cloud Architectures with Spring Boot and Docker by Benjamin Nothdurft  Resiliency for 140 PB Cluster by Meriam Lachkar  La Keynote De La Nouvelle Generation by Saskia and Lois  The Speakers Trip in EssaouiraThe Speakers dinnerThe evening before the conference started, we had the speakers dinner sponsored by Lightbend.The dinner was held at restaurant Kasar El Hamra in the center of Marrakesh.On our way to the center we got to experience the Moroccan traffic which seemed to be pretty chaotic with all the cars and motorcycles zipping around, evading each other on the streets.Once arrived, we got treated with Moroccan dishes.One after the other, each of them truly delicious.VueJS animation in action by Charles-Philippe BernardCharles-Philippe explained why he loves VueJS so much.He criticised Angular and React because of the companies behind the frameworks, as well as the multiple variations of utilities and plugins. Instead, he prefers a community that stands as one behind the framework. Which is why he really likes VueJS as it is community-driven.He explained that for each functionality, there is exactly one solution promoted by the community.During his session, Charles-Philippe went over several libraries that he often uses for animated websites:  CSS3  Velocity.js  Animate.css  anime.js  Vanilla-tilt.jsA convenient trick that we learned during the session is how to make a JPG transparent using CSS.Note that this will only work if the image has a white background.img {    mix-blend-mode: multiply;}Be sure to check out Charles-Philippe’s amazingly animated slides.MockK, the idiomatic mocking framework for Kotlin by Yannick De TurckOur colleague, Yannick, gave a talk about MockK.MockK is a mocking framework specifically made for Kotlin. As a Java developer, he is a huge fan of Mockito for using mocks in his unit tests.When he picked up Kotlin, Mockito was also his first choice to try out.He explained however that using Mockito in a Kotlin project doesn’t go all that well due to how Mockito creates its mocks, which is by overriding classes and functions.And because Kotlin’s classes and functions are final by default, you can see that this poses some challenges.Yannick shared his experiences and mentioned that even though there is an incubating, opt-in feature to allow Mockito to mock final classes and functions, the user experience isn’t all that nice.He looked for other frameworks and stumbled upon MockK, a mocking framework created for Kotlin by Oleksiy Pylypenko.MockK’s main philosophy is offering first-class support for Kotlin features and being able to write idiomatic Kotlin code when using it.He was pretty enthusiastic about MockK and went over all its features and its ease of use.There is also a blog post written by Yannick specifically about his experiences with Mockito and MockK in Kotlin projects.Stream Processing Live Traffic Data with Kafka Streams by Tom Van den Bulck and Tim YsewynOur colleagues, Tim Ysewyn and Tom Van den Bulck, gave a talk about stream processing live traffic data with Kafka Streams.Tom presented the theoretical part of the talk starting with the bigger picture. He explained the stream processing concept which is basically computing data directly as it is produced or received.In the image above we can see an example without stream processing.In this case, the data is stored in databases and file storages.Using a scheduler, applications can retrieve and process the stored data.With stream processing the data will be processed directly as streams of events, creating other event streams for other applications if needed.The applications will react on events instead of scheduling jobs to retrieve and process data stored in databases and file storages.Following up the theoretical part, it was time for the demo.Key part of the demo was of course the data that was going to get processed.The Flemish government offers XML documents with live traffic data.These documents are created every minute so by using a scheduled job, an event could be created out of it.The theory behind all of this can be a bit abstract if you are not familiar with the concept which is why Tim first did some live coding to demonstrate how the events can be handled properly.Afterwards we learned how Spring can be used to help us with Kafka and how we can periodically fetch the data.We did this by first using pure code and secondly with the help of some convenient annotations to do the same thing with less code.At the end of the demo we created a small application that calculates the average speed for a specific sensor during a time frame of two minutes.The slides are available on SlideShare. The demo code can be found on GitHub.Applying (D)DDD and CQ(R)S to Cloud Architectures with Spring Boot and Docker by Benjamin NothdurftBenjamin started off by giving a brief introduction on Domain-Driven Design explaining the different building blocks such as domains, domain events, ubiquitous language and Event Storming.He also mentioned the famous two books: Domain-Driven Design by Eric Evans and Implementing Domain-Driven Design by Vaugn Vernon.Benjamin went through all the different steps of Event Storming.The goal is to bring people of different silos together, such as developers, analysts, architects and business experts.Together you want to create a logically ordered sequence of events to document a system using an ubiquitous language i.e. everybody using the same vocabulary and terms.Events describe things that have happened and are thus always in the past tense e.g. product added to cart.In a next step you want to identify commands, which are the triggers of events e.g. add product to cart.There are also aggregates which represent the data that is interacted with.And finally you want to identify the bounded contexts grouping relevant parts together.Benjamin then explained how this all gets translated to your system architecture.Each bounded context can be mapped to a single microservice.He covered different context map patterns such as event publisher, shared kernel, customer/supplier and anti-corruption layer together with detailed code samples.Afterwards he went through a CQRS example with many code samples and the questions you should be asking yourself when determining the right architecture.We really liked how in-depth everything was as many presentations about Domain-Driven Design usually remain rather abstract and high-level.You can check out Benjamin’s slides on slides.com.Resiliency for 140 PB Cluster by Meriam LachkarMeriam works at Criterio which is a marketing company managing Europe’s largest Hadoop cluster.Criterio uses various technologies:  Batch Processing: Map/Reduce and Apache Spark.  Stream Processing: Apache Flink and Apache Kafka.  Machine Learning: Spark ML and Tensorflow.Her talk focused on the Hadoop setup of Criterio.The current cluster in Paris has 220 PB of hard disk, 550 TB of memory and 100.000 cores.But since it is almost “full”, with 160 PB used, a new cluster has been set up in Amsterdam.Between both clusters there is a dedicated 400 Gbit fiber installed.Every day, 1 PB of data is generated.Meriam currently works at a project in which they want to synchronise the data between both clusters.This was not a trivial thing to do and the main question was how they were going to sync the data between both clusters, as copying over all data would fill the existing line for an entire day.RSync was just too slow, also some jobs are non-deterministic which means that executing the job a second time will yield a different result compared to the first time.Various options were considered:  Double run: this means that the Paris data center would still become the bottleneck and would not yield real business value  Producer push  Consumer push  Dedicated central service which will determine where the jobs will run, on which data center.A dedicated central service was the chosen solution.Codenamed “Mumak”, as it is the convention to name everything in the Hadoop ecosystem to an elephant.Dataset by dataset will be progressively moved to Amsterdam, so that jobs will eventually be distributed between both data centers.La keynote de la nouvelle generation by Saskia and LoisThe closing keynote was presented by Saskia (13) and Loïs (10) Blanc,the children of Sébastien Blanc. They started the closing keynote by asking the audience to close their eyes for a few seconds and to think about the near future and how we would imagine it.The audience had to explain what they were thinking of.Most people shared the same things such as self-driving-cars and robots to help in the household. Saskia and Loïs in turn, shared their vision of the future which pretty much aligned with ours.Saskia started the first demo by giving an introduction on the Logo programming language while Loïs was doing some live coding.A simple square was drawn first but it got more interesting as they added rotations to draw more complex figures.Loïs on the other hand explained Scratch, a program where you can create your own games, animations and interactive stories. He showed the community around the program and gave a really entertaining demo about two figures walking around.Saskia explained us that she became more interested in what her dad was doing and that she wanted to get into more real programming.Her father being a Java developer, told her about the Groovy programming language.Saskia gave a demo in Groovy and explained some basics while live coding a small program.The program consisted of an Animal interface and a Cat class that could meow.Saskia and Loïs ended their keynote mentioning that they both want to become developers of the new generation.Given their impressive presentation, we definitely see that happening!The Speakers Trip in EssaouiraAfter three interesting days of Devoxx MA, we went on the speakers trip to Essaouira, considered as one of the best anchorages of the Moroccan coast.We started off by visiting Chez Ali in Marrakesh on the last evening of the conference.As we arrived we were heartily greeted by horsemen and Moroccans playing authentic music.We got to explore Ali Baba’s cave before passing by all kinds of folkloric groups.Dinner was served in big tents and consisted of multiple courses of Moroccan dishes.After the dinner we got to watch an amazing spectacle of stuntmen on horses, belly dancers and cavaliers.The morning after, we set off to Essaouira with three buses filled of speakers and people part of the Devoxx MA organisation.During the three-hour long ride we stopped by the magnificent “goat tree” and Arganomade, where they manufactured organic argan oil all by hand and where we got to see the whole manufacturing process.In the early afternoon we arrived at Essaouira Lodge where we would be staying during the trip.After checking in and unpacking our luggage, we set off to the Essaouira beach where we had delicious sea food at restaurant Fanatic.With our hunger satisfied, we set off with our buses a bit further along the beach where we got to ride camels in caravans.With our newly allocated trusty steeds, we rode around the coast with the sun setting off in the background.After a 30-minute ride in the sand we arrived at a big tent where we would spend the rest of the evening.As dinner was being prepared in many tajines, we were entertained by live Moroccan music and dancers with a big cozy bonfire blazing about.This was truly an amazing and memorable day!The day after we went back to Essaouira beach and visited the fortress city with the walls still intact.As we strolled through the alley streets we got to visit all kinds of shops and markets.We had lunch on a roof terrace at Il Mare with an amazing view on the sea.Some more exploring was done around the city before we returned to our lodges.We spent the rest of the evening in our lodges where we invited all speakers in our lodge for a nice last evening together with a couple of Casablanca beers.As we would not have been able to catch our flight back to Belgium in time, the four of us booked a beautiful riad right in the center of Marrakesh.We explored the Djemaa el Fna together with its back alleys and visited an authentic tannery.Summing it all upDevoxx MA was amazing experience for us where we got to meet and befriend a lot of great people.We would like to thank the organisers for organising both Devoxx MA and offering us the chance to see more of Morocco with the speakers trip!The date for next year’s edition has already been announced and will happen on the 12th, 13th and 14th of November 2019.Be sure to mark it in your agenda!"
      },
    
      "agile-2018-12-13-catching-the-waves-of-servant-leadership-html": {
        "title": "Catching the waves of servant leadership",
        "url": "/agile/2018/12/13/Catching-the-waves-of-servant-leadership.html",
        "image": "/img/catching-the-waves-of-servant-leadership/main-image.png",
        "date": "13 Dec 2018",
        "category": "post, blog post, blog",
        "content": "As human beings, we’ve always had a complicated relationship with change.On the one hand, we see it all around us and try to cope with the complexity of it.Attempting to avoid or even sabotage it in order to hang on for dear life, to what we have always known to be normal, and therefore believe it to be what’s best.On the other hand, it’s what allows us to hope and strive, what drives us towards a dream for a better future.And so we look for ways to form it, to mould it, even manipulate it at times, thinking that whatever comes out of the change will be better and lasting.But does change have an outcome? Or is it endlessly moving forward? This continuous tension between coping and moulding makes our relationship with change a very personal one. Every day we strive, all of us, in this global community, to find a moment of peace with ourselves and with the ways we try to manage this uncertainty and complexity.And sometimes, when the odds are right, in a cursory moment of clear thoughts, we discover that precious balance, and see the bigger picture. That’s when we embrace change and sense the opportunity…When projecting this mechanism from the personal into the organisational area, the same elements are at play. But the personal aspect is still the crucial driver (and the key to unlock it), that can push corporate change forward or go against it like a stubborn forceful undercurrent.Tapping into that current, though, is the key to unlocking the potential of an engaged community of professionals, and so helping the organisational culture grow organically. And doing that needs a ‘feel’ for what our present corporate culture is about. But more so it requires an understanding of what the employees building our brand - and the people creating the customer value – really strive for, personally and professionally.Easier said than done, you think? Maybe so, especially when you’re working in a traditional corporate world, with siloed processes, with lots of gates and handovers, often distorting communication and slowing down value delivery. It does indeed call for a veritable paradigm shift, not just new management practices, but a management revolution; a shift towards a new approach, focused on what these value creators actually require in order to thrive in your organisation, for the benefit of your brand. Enabling them to become better, more inventive, more efficient, and allowing them to listen closely and collaborate with the customer, is the most successful way forward in our age of agile.This is of course not a one (wo)man show. Complexity is not something we can tackle with a single mind solution. Agile transformation, like cultural transformation, can only be set in motion by natural leaders, and by this I don’t necessarily mean ‘managers’. They can be emergent leaders from any level in the organisation. People who spread vibes, generate positive energy, think beyond the borders of their own roles and put their brains to work to reimagine how value is created. What defines them is that they ‘service’ their community, and that service is exactly one of their personal goals.As a manager, make sure these emergent leaders experience the freedom to act. Each one comes with a natural community, call it a ‘tribe’. Explore every nook and cranny of the company to trace them. Then help them to understand the agile strategy, and support them to instigate an agile mindset and a zest for growth, by taking small but consistent steps, clear and aligned.The organisation will be surfing their powerful waves. And with the help of these servant leaders throughout the company, you’ll be able to blow the wind in the right direction, towards the customer.  “Not one thing ever does it, it’s a series of consistent things that makes people say, ‘Aha, it’s time for change’” - Oprah WinfreyRecommended reading  Stephen Denning - The Age of Agile: How smart companies are transforming the way work gets done  Sunil Mundra - Enterprise agility: Being agile in a changing world  Seth Godin - Tribes  Em Campbell-Pretty - Tribal Unity: Getting from teams to tribes by creating a one team culture"
      },
    
      "conference-2018-12-12-dot-css-dot-js-2018-html": {
        "title": "dotCSS and dotJS",
        "url": "/conference/2018/12/12/dot-css-dot-js-2018.html",
        "image": "/img/2018-11-08-dot-css-dot-js-2018/dotjs.jpeg",
        "date": "12 Dec 2018",
        "category": "post, blog post, blog",
        "content": "Paris!This year for the first time Ordina JWorks travelled to Paris to attend a two day conference on CSS and JS, more exactly dotCSS and dotJS which are part of the dotConferences. dotConferences is a series of developer events in Paris that started in 2012.We only included talks on which we could elaborate. You can find all talks on the YouYube channel of dotConferences.Table of contents  Day 1: dotCSS          ‘Power of SVG’ by Sara Soueidan      ‘Reading Hex codes’ by David DeSandro      ‘Variable fonts’ by Mandy Michael      ‘CSS taught me…’ by Dan Cederholm      ‘Breaking the norm with creative CSS’ by Aga Naplocha        Day 2: dotJS          ‘The State of JS’ by Sacha Greif      ‘Minecraft is getting a JavaScript runtime’ by Tobias Ahlin      ‘Learning to Love Type Systems’ by Lauren Tan      ‘Choosing Your JavaScript Framework’ by John Papa      ‘JavaScript on the Desktop, Fast and Slow’ by Felix Rieseberg        ConclusionDay 1: dotCSS‘Power of SVG’ by Sara SoueidanSome websites really have nice visuals that contribute to a pleasant user experience when visiting them. Think of brands that incorporate their logos in images for instance, or cover pictures that look like some Instagram filters were applied to them. You would think that such assets are created by designers and that they are applied directly to the webpage, but there is also a way to get those Photoshop effects straight into your browser.Sara Soueidan showed us a few possibilities to apply some stunning visual effects on images and text with code. The cool thing is that when using plain text, it’s still searchable! First she discussed some techniques that designers use to create templates in Photoshop and afterwards she applied the same techniques by only using SVG. It was very impressive to see how you can manipulate images by applying a set of SVG filters. Hopefully using those SVG techniques will become more common in the future so that more visual appealing websites will be created with the power of SVG.            View talk      View slides      ‘Reading Hex codes’ by David DeSandroThis was one of the most mind blowing talks of dotCSS by far.David DeSandro begins the talk with saying he has a special ability.He can read color hex codes. At that moment the most of us were like “What?”“You can tell the color based on the hexcode?”“Is that even possible?”And well, he proved it to us that you really can tell the color based on the hex code.He even showed us how to do so.And so he told us that it requires 5 steps to read color hex codes.  3-digit shorthand  Line graph  Hue from shape  Lightness from total  Saturation from rangeJust like this I bet you have no idea what these steps mean.But let us teach you the magic of reading color hex codes as well thanks to David.To get started he taught us that the best way to describe colors is with the HSL color model.HSL stands for Hue, Saturation and Lightness. Hue is the pure pigment of a color and can be described with 12 color names.We’ll show you the color names later in Step 3.Saturation is how vibrant or muted the hue is and can be described as saturated, washed, muted or gray.Lightness speaks for itself and describes how light or dark the color is and this can be done with light, middle or dark.Now that you know how you can describe a color all you have to do is to follow the 5 steps.And deterimine the hue, lightness and saturation.So let’s get started!Step 1: 3-digit shorthandTo better understand the process let’s take #D49B25 as an example.The first step is to retrieve the 3-digit shorthand of the hexcode.This can be done easily by breaking up the hexcode in 3 pairs (D4 | 9B | 25).Keep in mind that each pair represents a value of the RGB color channel.Now drop the second number of each pair and that gives us the shorthand code #D92.Step 2: Line graphWith the shorthand from step one we have to create a linegraph based on the numbers of the hexcode.With basic understanding of hexadecimal numbers we can visualize a little line graph for the channel values.D is high, 9 is around the middle, 2 is low.And that’s how we get our litle line graph.Step 3: Hue from shapeThis is what we think is the most tricky part about reading color hex codes.With the line graph we got from the previous step you’ll have to find a matching color on the color wheel.So you’ll have to remember this one by heart if you really want to show off.For our example color, it matches best with the color orange.Step 4: Lightness from totalTo determine the lightness you should look either at the total sum of the channel values (pairs) or at the values in the line graph we created.If the values are higher to the top, the color is closer to white and thus lighter.If the values are closer to the bottom, the color is closer to black and thus darker.For our example color #D92, the values are both high and low, so it has middle lightness.Step 5: SaturationSaturation is a measure of how vibrant or rich the hue/color is.To measure the saturation whe need to look at the difference between the highest and the lowest value in our shorthand code.The wider the range, the higher the saturation. Colors with small range have low saturation, appearing faded.A color with no saturation is a pure gray.With our color, #D92, D is the highest value, 2 is the lowest.D is high. 2 is low.That’s a wide range, but not completely wide.So our color has moderate saturation, thus making it a washed color.Now we have all three attributes for our colorSo we can say #D49B25 is Middle Washed Orange.Pretty amazing right?Since dotCss puts most of its talks online we definitely recommend watching this talk.He just explains it so well and goes a little bit deeper than we did here.            View talk      View slides      ‘Variable fonts’ by Mandy MichaelA variable font is an OpenType font format that includes new technology called OpenType Font Variations. Jointly developed by four of the most influential technology companies — Google, Apple, Microsoft, and Adobe — these font files contain a huge amount of extra data, compared to your average OpenType font. A variable font can contain a font’s entire glyph set, or individual glyphs with up to 64,000 axes of variation, including weight, width, slant, and, in some cases, specific styles, such as Condensed, Bold, etc.There are two main advantages when using variable fonts:  The average file size is smaller than separate font files  Only one request is necessary to load the necessary font variationsA good example is the font Source Sans Variable or the font Decovar.She also showed us the possibilities of variable fonts and what kind of awesome stuff you can do with them.Did you know that you can make text animations with them?Well you can, so check it out on her codepen.io collection.You can easily check for browser support using the @supports CSS rule: @supports (font-variation-settings).‘CSS taught me…’ by Dan CederholmDan talked about all the things he learned in his lifelong journey of working with CSS.But in the end it all came down to this:  Have side projects to keep your skills sharp  It’s OK not to use the latest and greatest in business‘Breaking the norm with creative CSS’ by Aga NaplochaThe last presentation of the dotCSS conference was about using other inspirations and tools to build webpages. Aga made a bold remark that most of the websites out there have the same structure and even look and feel the same. As a big fan of Brutalist Design she showed some examples to demonstrate what is possible when you think out of the box.With her talk she wanted to encourage developers to try and use other CSS Properties for building and designing websites. The three CSS properties she mentioned were clipping, masking and shape-outside (all in combination with SVG — using masking with images is resource intensive since it operates pixel per pixel). She showed each property with a clear example and discussed the differences between them, and mentioned the compatibility of each property in the different browsers.Saying that most of the websites have the same look and feel is a bold statement but after you look around for a while you notice that she actually has a point. Most of the websites are using frameworks that give you a uniform, recognizable look and feel but are also really easy to use. With the properties she mentioned you can certainly build a beautiful, well-designed website but it would take more time to make sure you have a responsive website that has the same look and feel over the different browsers and devices.Day 2: dotJS‘The State of JS’ by Sacha GreifAs we all know, the JavaScript ecosystem is richer than ever, and even the most experienced developers become victims of the amount of choices they have to make.It’s always changing. New libraries, frameworks, languagues…For this reason Sacha Greif, Raphaël Benitte and Michael Rambeau decided to create a global survey they called the ‘State of JavaScript’.The survey contains data from over 20.000 developers from all around the globe and you can find the results on their website.The survey tries to figure out what these developers are using these days, what they like and what they would love to learn.When they published the State of JS results of 2018 there was a lot of commotion regarding the results.More specifically regarding the front end frameworks Angular, React and Vue.The State of JS survey declares that Angular is suffering a lot in comparison with React and Vue.The Angular results are very disappointing and show us a high rate of dissatisfaction within its userbase.On the other side there is only love for both React and Vue.This lead to a war on Twitter and other channels saying the survey is flawed.Olivier Combe who is a member of the core team called out to Sasha why they didn’t make the distinction between AngularJS and Angular.It makes total sense a lot of people are using Angular (read v2+) now and are not using AngularJS any longer.There even is a YouTube Video that takes a closer look at the results.In any case, we believe this survey can help lots of developers make choices and it gives at least some insights on the State of JavaScript.Besides the front end frameworks, the State of JS also offers lots of data of other subjects regarding JavaScript which are definitely worth checking out!‘Minecraft is getting a JavaScript runtime’ by Tobias AhlinTobias blew our mind with the message that Minecraft has a HTML based UI that is using a JavaScript runtime.Just think about it: a JavaScript runtime running on top of your games.The JS runtime provides access to UI elements and an API to give developers enough freedom to work with the interface.Worth mentioning is that this is completely mobile optimized and is based on flexbox.Using floats is just too expensive on processing power.‘Learning to Love Type Systems’ by Lauren TanSince the introduction of TypeScript in 2012, web development has consistently looked more towards building software with the use of static typing. Lauren discussed why we should embrace the use of optional static typing that TypeScript provides.By using types, we add more constraints to our code and how other developers can use our code. Thus this decreases the amount of possible bugs during the development phases of a project. According to Lauren, types are mathematical propositions on how a program should work and the written code that complies to the types is a proof of the type system.Lauren explained that while using types we should be as strict as possible. The lower cardinality we have in our types, the less bugs will occur as we limit the possible inputs to our functions.With TypeScript it’s convenient to use a type such as any but we’re better off avoiding using the any type. By consistently being as strict as possible in our type usage, we facilitate better integration between different developers and teams. It is definitely true that stricter typing helps with defining the limits of your code while also documenting your code indirectly.            View talk      View slides      ‘Choosing Your JavaScript Framework’ by John PapaWith all the current JavaScript frameworks out there it’s hard to pick one to work with. During his talk John Papa took a closer look to the three most popular frameworks at the moment: Angular, React and Vue.When choosing the framework for your project you could ask yourself questions like “Does it have all the features that I need?”, “What about the documentation?”, “Is it backed by a strong community?” or “How fast is the framework?”.In this case all three frameworks would apply. But they all have a different way to work with components, lazy loading, state management and other stuff.Take for example the language it’s written in. With Angular comes TypeScript, but React and Vue are by default JavaScript. Do you fancy TypeScript but you don’t like to work with Angular? No issue, you can perfectly use TypeScript in React and Vue.They are all perfect for building successful applications and you can go on for hours on which one is the best.But in the end it comes down to one question: “How does the framework make you feel when you use it?”.The only way to find out is by trying each one. That’s why John Papa created a Tour of Heroes project for each framework.  Tour of Heroes with Angular  Tour of Heroes with React  Tour of Heroes with VueDo you wanna know which one he prefers? Well, he did not want to reveal that during the talk…            View talk      View slides      ‘JavaScript on the Desktop, Fast and Slow’ by Felix RiesebergJavascript is everywhere today, even in desktop applications e.g. Battlefield 1, Nvidia GeForce Experience and Adobe Creative Suite. Felix reveals his four tricks that can make your application more efficient.Before importing a specific module, really consider if you need all of its functionalities as it might bring in some additional items that you don’t need. When you do require a specific module, make sure you embed it at the right place.When using Node.js - which uses the V8 Javascript engine - the engine runs every time you build and compiles your code into something your machine can execute. You can easily cache this with the module v8-compile-cache.Repainting the screen is an expensive operation. You can check with Chrome Developer Tools how much this operation costs and maybe tweak some code.His third trick is that not all code is equal. Sometimes there is a more efficient way to structure your code or use different function calls with the same end result that could make your application faster. Using getElementByClassName instead of querySelectorAll for example would be 99% faster.And to close off, application lifecycle.If your application is minimized, pause the network requests and stop refreshing every couple of seconds. You can check this with document.hidden.            View talk      View slides      ConclusionWe had an interesting two days at dotCSS and dotJS, and had a lot of fun engaging with the wonderful people at the conference.One thing we noticed is that the talks at dotCSS were way more technical than the ones given at dotJS and somehow that made us feel a little bit disappointed.Nonetheless we learned a lot of things and had a great time visiting the city of love."
      },
    
      "development-2018-12-11-stairway-to-health-html": {
        "title": "Visualising IoT data with tableau",
        "url": "/development/2018/12/11/stairway-to-health.html",
        "image": "/img/2018-11-16-stairway-to-health/stairway-to-health.png",
        "date": "11 Dec 2018",
        "category": "post, blog post, blog",
        "content": "VisionWorks meets JWorks: StairwayToHealthWe, from VisionWorks, were asked to rebuild the visualisation dashboard JWorks used in the application they built as a result of the internal Stairway to Health project (you can find more information about that project here). We decided to use Tableau, a popular BI Visualisation tool we largely use at our clients.We developed the dashboard working around some key questions while keeping the appearance of the dashboard in line with the dashboard JWorks developed.In the following section we will explain how the dashboard is currently set up and how to use it properly.Next, we will go over the features we can add in future releases to allow the user to go even deeper in their analysis.Dashboard overviewThe dashboard is built to answer the following questions:  What is the percentage of people taking the stairs or elevator at Ordina today?  How is the same metric on weekly, monthly or yearly basis? What is it in absolute numbers?  How does it evolve over time based on each day, week, month or year?  Are people taking the stairs more this week compared to last week?The dashboard will try to provide answers to these questions using the following three main parts.We will go over these parts and highlight which question(s) they try to answer.Part one: the TitleThe title is what the user sees first and answers the first question.By using the colors in the title, the dashboard shows the user - in a subtle way - what the colors in the next visuals represent.There is also an option to select another day as illustrated below.Part two: the horizontal bar comparisonIn part two the user can find an answer to questions two and three.The visual uses the selected day to show the division between people taking the stairs / elevator on a daily / weekly / monthly and yearly basis.When the user hovers over the chart he can also see the evolution of people taking the stairs / elevator within that day / week / month / year.Next to the chart, the total absolute number of all the observations measured is reported per period.Part three: the more detailed area chartThe third part visualises how the division stairs / elevator is evolving over time expressed in daily, weekly, monthly or yearly basis.This gives the user the possibility to look at trends and to see how the situation of today compares itself to past situations.In the title the user has the option to change the appearance of the data (absolute or shares).The amount of periods shown (starting from the most recent period) can also be changed.When the user hovers over the chart the same horizontal bar comparison can be seen. Comparisons can be made with the period selected above.Last feature to discuss here is how the user can change which period the chart is showing.This can be done by clicking the chart above.When you click on the day bar on the top chart the bottom chart is expressed on a day level.This also applies to the other period bases in the chart.What can we do next?While this dashboard already gives an answer to the most important questions and gives the user the possibility to explore the data over time, there are still some extra things that can be developed.The dashboard is currently built within a Tableau workbook which is using the data of the MongoDB database JWorks set up as an extract.This means we don’t have a live connection to the actual database JWorks has in their app.This brings us to the first thing we can still explore: deployment.In order to integrate the dashboard in the original application, we could publish the dashboard on the Tableau server of Ordina which is running on Microsoft Azure.Running this instance is not free so when taking a decision we should also take the user relevance in consideration: does the user really need to have a live connection to the data or does a nightly update cover the load?Secondly we can still do a lot on the analysis part. What are the reasons why some patterns in the data exist? Do people take the stairs less when it is hot outside?JWorks recently tracked on which floor the observation is measured, allowing us to look into difference by floor. Do people take the elevator more when they need to go from floor 1 to floor 3?We will keep you posted on further progress related to Stairway to Health. Thank you for reading and don’t forget: always take the stairs!"
      },
    
      "iot-20machine-20learning-2018-12-10-10-cool-ai-examples-html": {
        "title": "10 Cool AI/ML Examples",
        "url": "/iot,%20machine%20learning/2018/12/10/10-cool-ai-examples.html",
        "image": "/img/2018-12-10-AI10EX/banner.jpg",
        "date": "10 Dec 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  10 Cool examples  ResourcesIntroductionTo end the year on a lighter more inspirational note we’ll go into 10 cool examples achieved with artificial intelligence.These are all short videos with some additional information, most of these already have or will soon have an impact on our lives one way or another.The 10 examples can be divided in to three categories:How the models work and perceive the world  How neural networks see the world  Attack on human vision systemAudiovisual models for rendering, photography and impersonation  Style transfer for videos  Amazing night time photographs  Nvidia AI based image restoration  Noise reduction for path traced GI  Isolate speech signals  Impersonate anyoneModels used for and helped by gaming  Deepmind becomes superhuman in quake 3  Using games for deep learning researchFor now, there is still no need to fear Skynet becoming a reality.While progress in the artificial intelligence world proceeds at a staggering pace, we are no where near having a general ‘super’ AI.This however does not mean precautions do not need to be taken to prevent this from happening in the long run.Some people, like Elon Musk, are very vocal about this and question if we should even pursue the goal to create a ‘super’ AI.10 Cool examplesBelow are the ten selected examples we think you should see!In the resources section underneath all of them you can find more useful resources to use and watch about AI/ML.Example 1 - How neural networks see the world    Understanding a neural network is difficult, we don’t actually know what is happening inside of it.We need ways of visualizing and understanding what happens inside to help debug and improve these networks.For convolutional neural networks this helps us see what the network sees and how it identifies and uses parts of the input to get to the desired output.Example 2 - Attack on human vision system    Not only artificial neural networks are vulnerable to attacks to fool them.Our very own brain, a neural network as well, is also able to be tricked by some of these techniques.This video shows how such an attack works.    This video shows how neural networks can be fooled by changes to the input as small as a single pixel!It shows that caution needs to be taken in neural network based image recognition because a sufficiently witty/crafty attacker could fool the system by employing such an attack.Example 3 - Style transfer for videos    Style transfer is when the style of a given input image is transferred to a secondary input image while maintaining the content of that image but with the style of the first input.This gives you the option to apply the style of certain great works of art to regular images or even works with a totally different style.In this video the technique is applied to video content, but it is not just as simple as running the earlier technique on each frame of the video since it does not provide a result that is temporally coherent.The video is styled based on a given input and produces quite amazing results.Example 4 - Amazing night time photographs    Very noisy night time images might soon be a thing of the past.The technique in this video can turn unusably noisy photos into perfectly viewable photos.Something like this has been implemented in the google pixel phones recently.In a few years all cameras will have a mode like this implemented making unusable night shots a thing of the past!Example 5 - Nvidia AI based image restoration    Like the example above this is about denoising and is similar yet different.This AI has been trained without ever having been shown what noise is, so no before vs after comparison.It can remove noise from images, restore images that are almost only visible noise and even remove lots of text from a given image.This technique will make cleaning up images much easier and allow us to preserve and restore imagery that might otherwise be lost or unusable!Example 6 - Noise reduction for path traced GI    This video shows that denoising techniques can have other great benefits in the visual/gaming industry.Path traced global illumination (casting light in a 3D scene to determine lighting from a global source like the sun) is a very resource intensive task.Current solutions use all sorts of tricks to mimic this but they are not the real deal.This technique allows for path traced GI with a very low sample count and denoises the output whilst being temporally stable.Something like the new cards from Nvidia are now capable of!Example 7 - Isolate speech signals    Having an audio or video file with multiple people speaking at once or when there is a lot of background noise can be annoying for various reasons.It makes it harder to understand any of the speaking parties.This technique allows each speaker’s audio to be isolated and listened to without hearing the other sources of interference.It is helpful to clean up conversations or remove background noise.Example 8 - Impersonate anyone    Soon you will not be able to tell that what you see is actually what happened.This advanced technique improves on older versions, and allows you to transfer your facial and torso movements onto a target.Techniques like this make it clear that fake news and fake sources of media will become an even bigger problem in the future as this technology becomes even better.It might not be such a bad idea to invest in that blockchain backed media repository after all so the validity of media files can be tracked…Example 9 - Deepmind becomes superhuman in quake 3    In games you normally play against the AI.These AI’s are mostly cheaters though, they know more because they are fed insider information from the game itself.The AI’s in this video are actual players that only get the video output of the game and learn to play accordingly.This in the long run will allow games to have decent real AI in-game.Other sectors can also benefit from this as it can be applied to different fields where complex behavior with tactics and long term planning is required.Example 10 - Using games for deep learning research    Self driving cars are all the rage these days.Getting cars to drive themselves is an immensely complex task, requiring truly vast amounts of correctly classified data in a dataset.Classifying this data is a very time consuming process.This technique can use games like GTAV to create a dataset with imagery from the game.The game already knows what all the types of objects are in the scene, so classification can be simplified and automated.It also provides an easy way to simulate hard to recreate situations in real life.Time of day and scene composition can be easily changed which results in a vastly more extensive dataset.ResourcesA very good video to watch is How machines learn by CGP Gray.It generally explains how machine learning works and what some of the implied dangers are.All the videos used in this blogpost are from the the Two minute papers YouTube channel.This channel has short videos that showcase some scientific research in a visual and compelling way whilst not going too technical but still providing all the technical resources for those who want it.Lastly is the playlist about neural networks by 3Blue1Brown.It goes into how neural networks work and is very visual which helps greatly with understanding the subject matter.  How machines learn - CGP Gray  Two minute papers youtube channel  Neural networks playlist - 3blue1brownAll these videos and the accompanying channels on YouTube are from amazing content creators, all rights for the content goes to them.Do like I do and subscribe to these awesome channels to support them!"
      },
    
      "development-2018-12-06-mongodb-transactions-html": {
        "title": "Transactions in MongoDB 4.0",
        "url": "/development/2018/12/06/mongodb-transactions.html",
        "image": "/img/2018-11-08-mongodb-europe-2018/mongodb-acid-logo-thumb.png",
        "date": "06 Dec 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Relational vs. document database  When to use transactions?  Technical details  Practical  Best Practices  ConclusionIntroduction  How and When to Use Multi-Document Distributed Transactions, Aly Cabral  MongoDB: Building a New Transactional Model, Keith BosticThe sessions I was looking forward to the most at MongoDB Europe 2018 were the two sessions about multi-document transactions, the most talked about feature of the MongoDB 4.0 release.In the morning I attended a session by Aly Cabral which was very practically oriented.In the afternoon there was a more esoteric yet very interesting session by Keith Bostic who provided some insight into the inner workings of the WiredTiger storage engine and the difficulties the MongoDB team had to overcome to implement the new transaction model.To give some background, as a longtime Oracle DBA, I always found it odd that a database would lack what I had always considered a crucial database feature, so I was naturally curious to know more about MongoDB’s implementation and how it would compare to a typical relational database.In this post we will explore how multi-document transactions are implemented in MongoDB, how the implementation is similar to a relational database system and where they differ.Relational vs. document databaseAs I learned through working with MongoDB the past two years, there is less need for multi-document transactions in document databases.For the majority of use cases single document transactions suffice.This is because the data model you use with a document database is quite different from what you would use with an RDBMS.In a relational database system you typically normalize data in order to avoid duplication.A single entity more often than not has data spanning multiple tables, so when you perform updates to a single entity you have to update multiple rows in multiple tables concurrently, which necessitates transactions.In a document database like MongoDB though, you typically embed all data that represents one entity within a single document, in which case updates to that entity are also limited to a single document.You can see that there’s less need for transactions.In essence, the RDBMS approach prioritizes disk space efficiency above everything else whereas the MongoDB approach prioritizes ease of development and simplicity.Nevertheless, there are some scenarios where you may want to use multi-document transactions in a document database.Let’s see what some of those scenarios might be…When to use transactions?RelationshipsIn the case that your datamodel does have relationships between separate entitites, you may want to use transactions to update both of them at the same time.An example of this could be a customer and a car that the customer owns.They are distinct entities, but there is a relationship between them in the form of ownership information.If you update the ownership information on the customer document, you probably need to update it on the car document (or documents) as well.The only way to do this with guaranteed consistency is through a multi-document transaction.Event processingAnother use case of transactions is event processing.When a certain event occurs, it may need to atomically create, update or delete several entities at the same time.The example that was given in Aly’s presentation was the creation or invalidation of a customer’s account, which would require an update to all of the customer’s entities.Event logging or auditingConsider the case where, for logging or auditing purposes, you want to create an event trail of all changes that happen to a certain document or collection and you want to store this event trail in another collection.The event trail should be representative of what really occurred, so events that never occurred should not be logged nor should events that actually happened be lost.The only way to achieve this is to put the update and the logging of the update inside the same transaction.Technical detailsNow let’s explore some of the more technical features of multi-document transactions.AtomicityThis is pretty straightforward.In MongoDB transactions are atomic, which means that execution of multiple changes inside a transaction is an all-or-nothing deal: either all updates get committed, or none.Snapshot isolationWhen you start a transaction, MongoDB creates a snapshot of the current state of the database.During your transaction you will not see any updates made by other sessions.You are isolated from them.This guarantees that throughout the transaction your session will see one consistent version of the data.Internally MongoDB uses an update structure inside the WiredTiger cache to maintain this consistent view on the database.This structure grows as writes occur to the database and is only evicted from the database once the transaction is committed or aborted.The implication of this is that long running transactions or a high write volume can put pressure on the cache.It’s therefore recommended to keep the duration of any transaction as low as possible.To minimize cache pressure, you can use the server parameter transactionLifeTimeLimitSeconds to set a sensible maximum transaction time.If a transaction runs for a longer time than this value, it will be aborted.The default value of transactionLifeTimeLimitSeconds is 60 seconds.A sidenote for those DBAs or developers who are already familiar with Oracle: the update structure in the WiredTiger cache is similar to how rollback segments and undo tablespaces work in Oracle.The differences are that the snapshot information is kept entirely in memory instead of on disk, and that you don’t have to actively manage it by allocating a tablespace for it.Read your own writesMongoDB guarantees that you can read any writes you make inside your transaction, even before they are committed.It also guarantees that no other session can read your writes before they are committed.Again, these writes are handled by the snapshot structure inside the WiredTiger cache.Write locksWhen two sessions are trying to update the same document at the same time, you get a write conflict.In MongoDB this conflict is handled by write locks.There are basically two conflict scenarios:      Before a transaction updates a document, it will try to acquire a write lock.If the document is already locked the transaction will fail.        Before a non-transactional operation tries to update a document, it will try to acquire a write lock.If the document is already locked, the operation will back off and retry until MaxTimeMS is reached.  Note that reads never block writes.MongoDB is also smart enough to recognize a so called no-op write: if the document you are trying to update was not changed by the update, it will not attempt to acquire a write lock.LimitationsCurrently you can only use multi-document transactions with replica sets.Sharded clusters are not supported yet, though this feature is planned for a future release (4.1 perhaps?).Due to the WiredTiger cache pressure, long running transactions can be problematic. The MongoDB developers are working on improving this for future releases and plan to support transactions running for several hours or even days.PracticalAs for semantics, the MongoDB developers thankfully chose not to reinvent the wheel.Using transactions is similar to what most developers are used to on relational database systems.The precise syntax varies per programming language, so you will have to do some RTFM to learn it, but it always comes down to the following steps:  Open session  Start transaction  Update multiple documents  Commit or abort transactionFor example, in Mongo Shell syntax a transaction typically looks like this:mySession = db.getMongo().startSession();mySession.startTransaction();mySession.getDatabase(\"mydb\").coll1.insert({\"foo\" : \"bar\"});mySession.getDatabase(\"mydb\").coll2.insert({\"hello\" : \"world\"});mySession.commitTransaction();One important thing to note here is that once we open the session on the first line, every subsequent action must use the session variable (“mySession” in this case), otherwise they will be simple update operations not belonging to the transaction.Best practicesFinally, here are some best practices we learned in the session:  Don’t change your data modeling rules because of transactions.For example: don’t start normalizing data  Transactions shouldn’t be the most common operation.If they are, you’re doing it wrong.  Pass session information to all statements inside your transaction.  Implement retry logic.MongoDB returns errorcodes that tell you if a transaction has failed and if it failed with a retryable error or not.  To reduce WiredTiger cache pressure, keep transactions short and don’t leave them open, even read only transactions.  Take into account that long running DDL operations (e.g. createIndex() ) block transactions and vice versa.ConclusionMulti-document transactions are a useful and easy to use addition to MongoDB.They make MongoDB a better general purpose database and a stronger alternative for applications where you would traditionally have to choose a relational database."
      },
    
      "testing-2018-11-21-sse-spring-node-dev-ci-html": {
        "title": "Mocking server sent events: Development and CI",
        "url": "/testing/2018/11/21/sse-spring-node-dev-ci.html",
        "image": "/img/2018-11-21-sse-spring-node-dev-ci/sse-front.png",
        "date": "21 Nov 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Intro  What are Server-Sent Events  Java  Nodejs  Angular  Continuous Integration  ConclusionIntroI came across this topic during some consultancy a few months ago, and again a few weeks ago.As I stated in my previous blogpost about mocking a backend (Node-RED: Development and CI), we don’t live in an ideal world.Backends are not always finished before frontend development starts and personally I hate it when I have to include mock data into my frontend code.And again, even if that backend feature is finished and deployed somewhere so we don’t need to run it locally, sometimes you have less control over messages sent from the backend that need to trigger events in the frontend.For both of those projects, a use case arose where the system was in need of messages sent from the backend to the frontend, based on purely frontend and backend events.On older technologies and systems, these problems were solved with a polling mechanism.Every few seconds, the frontend is querying the backend for updates.The first technology that comes to mind when reading the specifications are Websockets.A websocket is a bidirectional TCP connection opened between 2 entities, in our case a frontend and our backend.Messages can get sent by a client to the backend, or the other way around.For more information about websockets a simple Google search will overload you with information and frameworks for Java, Javascript and others.For Javascript, take a look at  Socket.io.In our use case, we were only in need of unidirectional streaming, Server-Sent Events or in short SSE.Again, the goal was not to implement the backend, but to come up with an easy to implement mock that can be used during development by our frontend developers, and could get reused in testing the frontend against this mock backend.Ideally, this demo code could get reused by our backend developers as an example.Although Node-RED has add-ons for SSE, I decided to start writing one myself.Note: In real systems, multiple clients can connect to the backend and open a channel.What are Server-Sent Events  Server-Sent Events is a technology for enabling unidirectional messaging over HTTP. The EventSource API is standardized and part of HTML5.In our use case, the backend should be able to send messages to its clients at any time.These messages can get triggered by client-side events (over REST) or even triggers from external resources and queues or database changes.  To make SSE work, we need to keep some things in mind.The logical flow behind it is pretty straight forward.A client requests a channel by GET-ting a resource over REST.In Javascript you can make use of the EventSource API.A backend should respond with some specific headers:  Content-Type -&gt; ‘text/event-stream’  Cache-Control -&gt; ‘no-cache’  Connection -&gt; ‘keep-alive’This way, the connection between the client and backend is kept open.At any time, the backend can send a message (event) through this tunnel to the client.We will go a bit deeper into each section later.You can read more about the specs on W3schools and W3.JavaAround a year ago, Dieter Hubau wrote a blogpost about Spring Cloud Stream and ‘a’ microverse of Rick and Morty. He implemented SSE using org.springframework.web.servlet.mvc.method.annotation.SseEmitter.I figured, that’s a place to start.SpringStart by generating a Spring Boot application with some dependencies.Navigate to Spring initializr.Add data-repository, flyway and h2.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;    &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.flywaydb&lt;/groupId&gt;    &lt;artifactId&gt;flyway-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.h2database&lt;/groupId&gt;    &lt;artifactId&gt;h2&lt;/artifactId&gt;    &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;I’ve added the Flyway and H2 dependencies because I’ve generated test data online (sql).I’ve created an easy model which represents a message (notification), and maps to a database table, which can be sent to the frontend.@Entity@Table(name = \"notification\")public class Notification {\t@Id\t@GeneratedValue\t@Column(name = \"id\")\tprivate Long id;\t@Column(name = \"title\")\tprivate String title;\t@Column(name = \"message\")\tprivate String message;\tpublic Long getId() {return id;}\tpublic void setId(Long id) {this.id = id;}\tpublic String getTitle() {return title;}\tpublic void setTitle(String title) {this.title = title;}\tpublic String getMessage() {return message;}\tpublic void setMessage(String message) {this.message = message;}}I’ve created a custom CrudRepository&lt;Notification, Long&gt;:public interface NotificationRepository extends CrudRepository&lt;Notification, Long&gt; {    ArrayList&lt;Notification&gt; findAll();    Optional&lt;Notification&gt; findById(Long id);}And a basic service:@Servicepublic class NotificationService {    @Autowired    private NotificationRepository notificationRepository;    public ArrayList&lt;Notification&gt; getAll() {        return this.notificationRepository.findAll();    }    public Notification get(Long id) throws EntityNotFoundException {        Optional&lt;Notification&gt; notification = this.notificationRepository.findById(id);        if (notification.isPresent()) {            return notification.get();        } else {            throw new EntityNotFoundException();        }    }}Most logic is implemented in the Controller:@RestController@RequestMapping(\"/notification\")public class NotificationController {    private final List&lt;SseEmitter&gt; emitters = new ArrayList&lt;&gt;();    @Autowired    private NotificationService notificationService;    @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)    public SseEmitter events() {        SseEmitter emitter = new SseEmitter();        emitters.add(emitter);        emitter.onCompletion(() -&gt; {            emitters.remove(emitter);        });        emitter.onError(throwable -&gt; {            emitters.remove(emitter);        });        emitter.onTimeout(() -&gt; {            emitters.remove(emitter);        });        return emitter;    }    private void handleNotification(Notification notification) {        emitters.parallelStream().forEach(emitter -&gt; {            try {                emitter.send(notification);            } catch (IOException e) {                emitter.complete();            }        });    }    @Scheduled(fixedDelay = 2000)    public void receiveNotification() {        this.handleNotification(this.notificationService.get((long) (Math.random() * (100 - 1)) + 1));    }}The logic behind the code is again pretty straightforward.Querying this resource will respond with the correct headers (Content-Type -&gt; MediaType.TEXT_EVENT_STREAM_VALUE == ‘text/event-stream’) en open an event stream.This resource will create an SseEmitter for each request and add that emitter to a list.When an event needs to be sent out to the clients, you can then just loop over that list of emitters and send that event.If you loop at the example code, you can see that the emitter itself has some callbacks (completion, error, timeout, …).You can use those function for implementing a specific error strategy, monitoring and logging.For development purposes, I’ve added a @Scheduled-function that will fire every two seconds and send a random notification from the database through each emitter.For one of my clients, it wasn’t possible to work with Spring.A Google search resulted in a lot of other solutions for Java implementations of sse.  SseEventSink, SseEventSource  SseFeature  EventSourceServlet  …NodejsAlthough the Java implementation wasn’t finished yet, another problem arose.Not all of our frontend developers where happy with this approach.They still needed to run a simple Java backend, even if it was a simple Docker container.So I switched to a Nodejs implementation using Express as a webserver.Express doesn’t come with an SSE-feature out of the box, but there are plugins you can use:  sse-express  express-sse  …But instead of using a library, I’ve implemented my own middleware.Writing custom middleware is very easy and well documented in the docs.sse-middleware.js:sse_middleware = function (req, res, next) {    res.sseSetup = function() {      req.socket.setTimeout(0);      req.socket.setNoDelay(true);      req.socket.setKeepAlive(true);      res.setHeader('Content-Type', 'text/event-stream');      res.setHeader('Cache-Control', 'no-cache');      res.setHeader('Connection', 'keep-alive');      res.statusCode = 200;       }    res.sseSend = function(data) {      res.write(JSON.stringify(data));    }    res.sseOnClose = function(callback) {      req.on(\"onClose\", callback);    }    next()}module.exports = sse_middleware;As mentioned before, to make SSE work, you need to set the right headers (cfr. MediaType.TEXT_EVENT_STREAM_VALUE).I’ve implemented this in the setup of the custom middleware.Besides this initialization, I’ve also implemented an sseSend-function, for sending messages over the channel, and an onClose-callback that will fire whenever the connection closes.Instead of using an in-memory database, like I did in the Java part of this post, I decided to go with a basic Javascript file that I can switch later to a simple json-file with test data.database.js:var database = {    notifications: [        {type: 'test', title: \"TEST\", message: \"testmessage\"},        ...,        {type: 'test', title: \"TEST2\", message: \"testmessag2\"}    ],    updates: [        {              entity: 'contact',            data: {                id:'123456'                email:'contact123456@gmail.com'            }         },         ...,         {               entity: 'company',             data: {                 id:'123456'                 tel1:'+3234457645'             }         }    ]}module.exports = database;This time, I added different kinds of data lists to my mock data.Depending on specific parameters, you can then choose to send back a different type of event.Now, let us take a look at the server implementation.server.js:var express = require('express');var sse_middleware = require('./sse-middleware');var database = require('./database');var DATA_LENGTH = 10;var app = express();app.use(sse_middleware);var channels = [];var interval;function start() {  interval = setInterval(() =&gt; {    let data = this.createMockEvent(); // to implement yourself    for(let key in channels) {      if(channels.hasOwnProperty(key)) {        channels[key].sseSend(data); // console.log('Emitting to ' + key);      }    }  }, 2000);}app.get('/stream', function(req, res) {  console.log(\"New subscriber request\");  res.sseSetup();  channels.push(res);  res.sseSend(\"Connection open\"); // if you want to send feedback for opening connection  // res.sendStatus(200);  res.sseOnClose(()=&gt; {     // implement your own strategy for removing a channel  })})app.listen(8080, function() {  console.log('Listening on port 8080...');  start();})In the first lines, I just import my mock database and the middleware.I then initialize the express-app and tell it to use the middleware, app.use(sse_middleware);.When the server is started, the app also starts a simple interval that will produce a random (or fixed order for testing purposes) event each two seconds.To start this service:$ node server.jsTo test it, you can just open your browser and navigate to http://localhost:8080/stream.You should be able to see events appearing now.However, there is a catch, and it took me some time to figure out what was going wrong.In your browser you can see the content of the events, but if you run $ curl -X GET http://localhost:8080/stream you won’t see anything.However, if you would start the Java app, you’ll see the events appearing in your browser, and during your curl-session.The reason for this, lays in the specs of Server-Sent Events.  As you can see, a message expects a data field.Adjusting the send-method in the middleware will fix this problem:res.write('data:' + JSON.stringify(data) + \"\\n\\n\"));You can also add the other fields, just separate them with \\n\\n;For development purposes, it isn’t a bad idea to add a start en stop action for managing the interval.Just add the following to your server:app.get('/start', function(req, res) {  console.log(\"Starting stream\");  start();  res.sendStatus(200);});app.get('/stop', function(req, res) {  console.log(\"Stopping stream\");  clearInterval(interval);  res.sendStatus(200);});So you can start and stop the stream by triggering a REST-endpoint.$ curl -X GET http://localhost:8080/start to start the stream of events.`$ curl -X GET http://localhost:8080/stop to stop the stream of events.`AngularFrontend SSEThe frontend is an Angular 7 app, created with the angular-cli.Because of reusability the server-sent event receiver feature is bundled in a separate module that can get moved to a shared library later.In the most simple implementation, you only need a service to handle the connection and forward events to other components.In this service, you can make use of the EventSource API of plain javascript.  The API comes with an easy constructor and 3 callbacks:  EventSource.onerror  EventSource.onmessage  EventSource.onopensse.service.ts:import ...@Injectable({  providedIn: 'root'})export class SseService {  readonly url = 'api/stream';  private _eventSource: EventSource;  private _open: boolean;  constructor(private _http: HttpClient) {    this.init();  }  public init(): void {    this._eventSource = new EventSource(this.url);    this._eventSource.onmessage = (evt) =&gt; this._onMessage(evt);    this._eventSource.onerror   = (evt) =&gt; this._onError(evt);    this._eventSource.onopen = (evt) =&gt; this._onOpen(evt);  }  private _onMessage(message: MessageEvent): void {    this._handleEvent(JSON.parse(message.data));  }  private _onError(evt: MessageEvent): void {    console.log(\"Error:\");    console.log(evt);    // implement your own strategy for reconnection  }  private _onOpen(evt: MessageEvent): void {    console.log(\"Open:\");    console.log(evt);  }  private _handleEvent(event: MessageEvent): void {      // e.g. dispatch to ngrx store  }}You’ll notice that the url used is not mapping on the mock backend.For local development and testing, this doesn’t matter.Even if both paths would match, the user interface and backend can’t run both on the same port (http://localhost:8080 vs http://localhost:4200 (standard cli port for $ ng serve)).Requesting resource cross domain will result in CORS issues. A proxy to the rescue!ProxyTo overcome the CORS problems, angular-cli, the serve-command to be more precise, comes with an optional parameter to add a proxy configuration.In our production ready setup, all calls to /api to the same (sub)domain as where the user interface is getting served, get routed to the REST-API.Because we don’t want to add dev or test specific code in the app itself, we proxy the /api to our mock backend.Example given:proxy.config.json{    \"/api/*\": {        \"target\": \"http://localhost:8080/\",        \"secure\": false,        \"logLevel\": \"debug\",        \"changeOrigin\": true,        \"pathRewrite\": {\"^/api\": \"\"}    }}To use this proxy, serve the app with:$ ng serve --proxy-config proxy.config.jsonIf you take a look at the logs, you can see the system is logging the routes in the console.Frontend + BackendIf you want to run the mock backend (Nodejs) along with the frontend, you need to be able to run concurrent tasks.You can do this in a node environment using the concurrently-package.Just install it by running $ npm i --save-dev concurrently.Add an entry in the package.json scripts section:\"start:proxy\": \"concurrently \\\"ng serve --proxy-config proxy.config.json\\\" \\\"node path/to/your/server.js \\\"\"Because they are both starting at the same time, it might happen your backend is not ready while your frontend starts connecting to the stream.A good retry strategy will help you overcome this problem, that can also happen in real life systems as well.Continuous IntegrationAs mentioned before, this whole approach should result in a mock that can be used for testing as well.In one of our systems, we have a lot of different event types.Some only need to show a notification on screen, while others need to refresh data in a cached object, or even change permissions of the logged in user.To mock this behavior, you can just put all these events in an array and just loop over it.You can even define different delays for each event if that is what you need.If you are using my Node-RED setup from one of my previous posts you should give one of the add-ons a try, however, you can also run both mocks next to each other.In most approaches, you don’t run the application itself thought the dev environment ($ ng serve --proxy-config proxy.config.json).You should run your packaged app like you would do in production.In our case, we are running everything Dockerized.This means, we build our frontend application and wrap it into a Docker image (tag it, and push it to our registry).In a next stage, we run (deploy) an environment where we can run our tests against.In this case we are also not going to use the proxy from our development setup.An easy setup would be using a docker-compose (e.g.):version: \"3\"services:  nginx:    image: \"nginx:mainline-alpine\"    container_name: proxy    restart: always    volumes:      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro    ports:      - 80:80    links:      - your-web-app      - node-red  node-red:    image: \"nodered/node-red-docker\"    restart: always    container_name: node-red    volumes:      - ${userDirPath}/node-red:/data      - ${userDirPath}/data:/usr/src/node-red/data    ports:      - 1880:1880  your-web-app:    image: \"registry.your-domain.com/your-web-app:${TAG}\"    container_name: your-web-app    restart: always    links:      - node-red    ports:      - 9080:80      - 9081:8080  selenium:    container_name: selenium-grid    image: selenium/standalone-chrome-debug    ports:      - 4444:4444      - 5900:5900    volumes:      - /dev/shm:/dev/shm    network_mode: hostWe now need to include our own ss-mock backend into this compose.You can do this by easily adding a plain Nodejs service, map your folder to your server.js and overwrite the CMD.sse-service:  image: \"node\"  restart: always  container_name: sse-mock  volumes:    - ${pathToYourServer}:/sse-mock  ports:    - 8080:8080    command: node /sse-mock/service.jsDon’t forget to add the service to the links section of your nginx and to add the proxy rules in the nginx.conf.location /api/stream {    proxy_pass http://sse-mock:8080/api/stream ;}  As mentioned before, you could/should use the /start and /stop for the sse-mock.In this setup, this means adding extra rules in your nginx config.You want all your api calls to go to the other mock (Node-RED in this case) while proxying /stream, /start and /stop to your sse-mock.The advantage of implementing the start/stop functionality, is that you can tell your test framework to start the sse-mock events stream and then start watching the response in the UI.e.g. (protractor, jasmine):beforeAll(async () =&gt; {    await browser.get('/api/start'); // depending on the host/address});If you’ve build your test data/setup in a specific order, you know what to expect and test for in the user interface.ConclusionSetting up Server-Sent Events is very easy.It is a powerful tool for unidirectional streams to you clients.The hardest part is defining a strategy for your connections and event type differentiation.Setting up the CI part is easy as well.Although you can test a lot in your unit tests, implementing End 2 End testing, mock and real, is recommended."
      },
    
      "development-2018-11-20-mongodb-europe-018-html": {
        "title": "MongoDB Europe 2018",
        "url": "/development/2018/11/20/mongodb-europe-018.html",
        "image": "/img/2018-11-08-mongodb-europe-2018/main-image-mdbe.png",
        "date": "20 Nov 2018",
        "category": "post, blog post, blog",
        "content": "  MongoDB Europe is a yearly conference where MongoDB shows off their latest features and new products.This year the venue took place in Old Billingsgate Walk, London.Jan, Nick and Chris wrote this blog post to summarize several of the given sessions.Table of contents  Atlas  MongoDB University  Compass Aggregation Pipeline Builder  Common query mistakes  Stitch  Meet the experts  Streaming data pipelines with MongoDB and Kafka at AO  MongoDB Enterprise Operator for Kubernetes at Amadeus  MongoDB Charts  ClosingAtlas(MongoDB Atlas for your Enterprise, Vladislava Stevanovic &amp; Veronica Tudor)Atlas is the database as a service offering (DBaaS) by MongoDB itself.You can run your MongoDB in the cloud with the cloud provider of your choice, secured by default and automatically backed up.Getting startedIt is very easy to get started since a free tier is provided for everyone and you can deploy with the cloud provider of your choice (Azure , GCP, AWS).A cluster can be started for free in just a few clicks.  Start your own cluster: https://cloud.mongodb.comCloudMongoDB Atlas is a cross-platform database which you can run on Amazon Web Services, Google Cloud or Microsoft Azure. It provides you an availability map that shows you which users are served from where and what the expected latency is.  ScalabilityWhen your database grows it is easy to scale up or out.You can scale up by increasing the size of the instance on which your database runs.Scaling out is done by the process of sharding.Here we are storing data of the same collection across multiple smaller partitions so we can distribute these partitions over multiple machines and increase read-write performance.This way you do not run into the limitations of a single server.To ensure that MongoDB stores data equally across shards you need the right strategy of choosing a partition key.High availabilityWhen a primary node goes down, a new primary is chosen immediately by a system of voting.All nodes vote on who should become the new primary. The node with the majority of votes becomes the new primary.A general guideline is to have a replica set that consists of one primary node and at least two secondary nodes.To ensure maximum uptime the procedure to recover from instance failures is completely automated.MongoDB UniversityWith MongoDB University, Mongo has its own platform for online courses.A lot of them are available for free.You can pick out courses according to your needs or profession.There are training tracks for Developers, DBA’s and everyone else…The courses are ideal to get you started or to deepen your knowledge when you want to be more advanced.If you want you can even get certified!I speak from my own experience when I say that the University platform is great to work with, and the courses are very well taught.Find all available resources here: https://university.mongodb.com/.Compass Aggregation Pipeline BuilderRecently the aggregation pipeline builder was introduced in MongoDB Compass.This allows you to create an aggregation pipeline step by step and that makes it easy to debug the pipeline along the way.Let’s see an example: Suppose I have a collection which contains documents that represent a person, like this :Here are two examples of elements in the collections:{\t\"_id\" : ObjectId(\"5be40f6e7047ead15753d073\"),\t\"firstName\" : \"Didi\",\t\"lastName\" : \"Verhoft\",\t\"birthYear\" : 1996},{\t\"_id\" : ObjectId(\"5be40f6e7047ead15753d074\"),\t\"firstName\" : \"Nick\",\t\"lastName\" : \"Van Hoof\",\t\"birthYear\" : 1992}A person has the fields lastName, firstName and birthYear (and of course for some people more info could be stored).I want to build a pipeline with the following functionality:  I want to filter out all people that share my lastname “Van Hoof”  Then I want to count how many times these people also share the same firstname and birthyear  Next I want to group them by birthYear so that I can see how many people were named “Nick Van Hoof “ (my fullname) in 1992.  Finally, I want them sorted on year in ascending orderFilter all with last name  “Van Hoof” and group by lastName, firstName and year:  Group by year and sort in ascending order:  As you can see from the images above, Compass will show all the intermediary results. With one push of a button you can generate a command line query or the translation to a programming language.Compass tells me the full aggregate will look like :db.people.aggregate([{$match: {                      lastName : \"Van Hoof\"                    }}, {$group: {                      _id: {                         \"lastName\" : \"$lastName\",                        \"firstName\": \"$firstName\",                        \"year\" : \"$birthYear\"                      },                      count: {                         $sum: 1                       }                    }}, {$group: {                            \"_id\": {                                \"year\": \"$_id.year\",                            },                            \"occurences\": {                              \"$push\" : {                                  \"identity\": \"$_id\",                                  \"count\": \"$count\"                              }                            }                    }}, {$sort: {                      \"_id.year\": 1                    }}])It would have been a lot harder to write this query manually, without the pipeline builder.Common query mistakes(Tips And Tricks for Avoiding Common Query Pitfalls, Christian Kurze)Key takeaways from this sessionGenerally speaking, there are three major causes of query slowness:  Blocking operations  Using the $or operator  Case insensitive searchesIt’s not uncommon that a properly tuned query delivers a factor 1000 speed-up.So it’s definitely worth investigating.Problem 1: blocking operationsThis happens when you use an operator that needs all the data before producing results, so results can’t be streamed.The most common culprits are the aggregation operators such as $sort, $group, $bucket, $count and $facet.Possible solutions:  Create a compound index to support your query and make sure that the sort order in the index is the same as in your query.  Offload the query to a secondary member.  Work around the issue by using a precalculated count.Problem 2: $and is fast, $or is slowSometimes a query is fast when you use the $and operator but slow when you use the $or operator.Solution:  Use a compound index to support $and queries.  Use separate single field indexes to support $or queries.Problem 3: case insensitive searches are slow!It is much harder for MongoDB to perform case insensitive searches because it has to match all possible permutations of the search string. For example, if you do a case insensitive search for the string “ace”, it has to match “ace”,”Ace”,”aCe”,”ACe”, and so on…Solution:  (3.4 and higher) Support the query with a case insensitive index.  Alternatively, store a toLower() copy of the string in another field and index and query that field instead.General tips and tricks  Create an index on an element you are interested in instead of scanning the whole table.  When you query on a combination of fields create a compound index for these fields and not separate indices on each field.  …but be careful with the usage of $or!  Build indices in the background instead of making it a blocking operation.  Do not index all fields as this will negatively impact write performance. Investigate what you really need!  Use .explain() to analyze queries.  Ops Manager and Atlas have a Performance Advisor to help you identify problematic queries.  Train your people.  Work smarter, not harder!Stitch(Ch-Ch-Ch-Ch-Changes: Taking Your MongoDB Stitch Application to the Next Level With Triggers, Andrew Morgan)  Write less code and build apps faster!Stitch is the ‘Serverless platform from MongoDB’, and it comes with a free tier to play around!It provides a very easy way to create an application without having to write lots of code in a separate backend.The functionalities of Stitch are provided through an SDK.Currently there are SDK’s for JavaScript, React Native, IOS and Android.There is even an Electric Imp Library for IoT devices.Stitch has four main services :  Stitch QueryAnywhere  Stitch Functions  Stitch Triggers  Stitch Mobile SyncStitch QueryAnywhereQueryAnywhere enables you to query the database directly instead of going through a REST api.The benefit here is that as a client application you are not restricted to what a REST api would expose but you can use all the power of the MongoDB Query Language directly:const employees = mongodb.db(\"HR\").collection(\"employees\");    return employees.find({}, {    limit: 3,    sort: { \"salary\": -1 }  })    .asArray();Of course, all of this is secured with authentication and fine grained authorization based on the logged in user or the contents of the documents.Stitch FunctionsYou can write JavaScript functions in the Stitch serverless platform and combine database calls with cloud services.For example, send a message with Twilio to all users:exports = function (message) { var mongodb = context.services.get('mongodb-atlas'); var twilio = context.services.get('twilio'); var coll = mongodb.db('db').collection('users'); return coll.find().toArray().then(users =&gt; {  users.forEach(function (user) {   twilio.send({    to: user.phone,    from: context.values.get('twilioNumber'),    body: message   });  }); });};​// Then call callFunction from the client sidestitchClient.callFunction('sendMessage', ['Hello from Stitch!']);Stitch TriggersMongoDB does not provide triggers, as known in the RDBMS world.With MongoDB change streams you can build your own triggers in your application.This comes with the cost of handling the complexity of change streams yourself. For example: how to resume the change stream after a network issue?So that’s why there is Stitch Triggers to make this easier.Stitch triggers combines change streams with Stitch Functions.So when the inventory of an article goes up, Stitch Trigger calls a function that uses Twilio to send a text message to your client.Stitch Mobile SyncSince 4.0, MongoDB provides a Mobile version for IOS and Android.With Stitch, you can sync your data in your mobile application with your database.So now you can use the full MongoDB Query Language, including aggregations, on your mobile device and sync it with your database.Built-in external IntegrationsThe fun with Stitch really starts when you combine all the goodness of Stitch with its integrations with cloud services like Twilio, AWS, Google, etc…You can authenticate with Google, store files on S3 or spin up a cluster on Redshift after you send a text message with Twilio.All of this can be hidden behind a simple function call for your application, or a trigger on your Atlas cluster.Limited to Stitch UI?Luckily MongoDB builds its products with developers in mind.So you can import and export your Stitch applications and put them in a source control of your choice.Meet the expertsAt the conference you had the chance to book a 20 minute session with a MongoDB expert.This was of great help in getting to know the new MongoDB Aggregation Pipeline builder.The expert also gave some more tips in “thinking noSQL”.  When data is shown together it is stored together – MongoDB expert  Data should be stored in the same way it is used – MongoDB expertStreaming data pipelines with MongoDB and Kafka at AOAppliances Online, AO, is an international online appliances retailer.They wanted to solve the issue of having data locked in different places so they wanted a Single Customer View.The idea was to get the data from all the different places and consolidate this in MongoDB.We are talking here about data stored in legacy databases or messages going through queues.The data could be the usual customer data and phone calls with customer care.But also about parcels moving through the warehouse and delivery tracking.They wanted to get the data while it’s hot, not in hourly or daily (or worse…) batches.It was decided to use MongoDB to build up this materialised view of all different data streams, and Atlas to be able to focus on the application and not the database administration.The vast majority of the data resides in MsSql databases.Extraction happens with Kafka Connect SQL CDC to generate a stream of all create, update and delete operations into a stream, and push it to Kafka.All with a simple piece of configuration like:{  \"name\" : \"msSqlSourceConnector1\",  \"config\" : {    \"connector.class\" : \"io.confluent.connect.cdc.mssql.MsSqlSourceConnector\",    \"tasks.max\" : \"1\",    \"initial.database\" : \"testing\",    \"username\" : \"cdc\",    \"password\" : \"secret\",    \"server.name\" : \"db-01.example.com\",    \"server.port\" : \"1433\"  }}They use Avro for the schema definition in combination with a schema-registry.Interested clients can then read the data off the topics and do their single-view-thing on the data and save it to MongoDB.The view is being built up, message per message.Afterwards this view in  MongoDB is then pushed back to Kafka as another stream to provide this data to interested parties.This avoids locking the data in one place.To finish it of they shared some lessons learned :  Watch out for frameworks generating queries. They can create bad performing aggregations or queries. For them it was better to write some queries explicitly.  Use custom _id for unique determination of your model, it saves an index and RAM  Watch out for unbounded document growth.MongoDB Enterprise Operator for Kubernetes at AmadeusAmadeus is the world’s largest technology company dedicated to the travel industry.They have developed an impressive MongoDB farm, a large environment with 100 clusters.Some of these clusters run more than 100 shards, while others have 100TB MongoDB databases.Amadeus processes more than 1 trillion flight availability requests per day. For each single search you do on a website they receive 500.000 availability requests.So search responsibly ;-)The number of requests per day grows by 50% each year.The second of January has by far the most requests, due to new years resolutions!If this day is in the weekend all systems are pushed to their limits.The airline fare database for one of their big clients, Kayak, is 100TB in size and changes daily.That’s some pretty big numbers there.No wonder that Amadeus is a happy user of the MongoDB Enterprise Operator for Kubernetes.Starting with the MongoDB Ops Manager v4.0, MongoDB officially supports the management and deployment of MongoDB in Kubernetes with Backup, Automation, Alerting and Monitoring.A MongoDB Enterprise Kubernetes Operator has app-specific awareness about stateful applications, so it knows how to deploy them.This operator helps automating scripted tasks and enables MongoDB-as-a-service for developers.It talks to Ops Manager and delegates the creation of clusters, shards, backups and automation to Ops Manager.The underlying necessary Kubernetes infrastructure is orchestrated by the operator itself and so they work in conjunction.This provides for clusters to be setup, scaled up/down/out/in, with a single simple yaml file.And kubernetes provides the self-healing capabilities, how nice is that!?The following yaml file is all you need to spin up a 3 node replica set :apiVersion: mongodb.com/v1kind: MongoDbReplicaSetmetadata:  name: myReplicaSet  namespace: mongodbspec:  members: 3  version: 4.0.1  project: projectName  credentials: myUberSecretCredentialsI kid you not, that’s it.Scale out or back in with a simple change in the config yaml andcubectl apply -f file.yamlMongoDB Charts  Bringing Data to Life with MongoDB Charts, Guillaume Meister.Currently, if you want to visualize data in a MongoDB database you either have to code it yourself, or rely on a 3rd party tool and migrate your data to a different platform (for example: Kibana with Elasticsearch is very popular).Needless to say that this can be quite cumbersome.MongoDB Charts intends to solve this.So what is it? MongoDB Charts is a visualization tool that runs as a standalone webserver so you can access it via a web browser.In Charts you define data sources by pointing to a collection or view on a MongoDB instance.Then you can create all kinds of visualizations based on the data source, using various aggregation and filter functions.Finally, you can combine charts into dashboards with a customized layout and you can share these with other users.A picture is worth a thousand words, so to give you a better idea of what it is all about, let’s look at an animation of Charts in action:  Charts is still in beta but you can already try it out. MongoDB provides a docker image that you can download via the MongoDB download center.ClosingIt was a great day being submerged in MongoDB-knowledge. This conference gave us plenty of opportunity to talk to other experts and learn about the new and upcoming features.Keep an eye on this space for more MongoDB goodness."
      },
    
      "conference-2018-11-16-join-2018-html": {
        "title": "JOIN 2018",
        "url": "/conference/2018/11/16/JOIN-2018.html",
        "image": "/img/join-2018/join-2018.png",
        "date": "16 Nov 2018",
        "category": "post, blog post, blog",
        "content": "JOIN is back!As has become tradition, Ordina JWorks organised the annual JOIN conference on Oct 4th.This edition was the 6th and once again it was bigger and better than the previous editions.JOIN is a free conference hosted every year by Ordina JWorks, by and for our own employees.External colleagues, and basically anyone interested in Ordina JWorks, is also invited to come over and indulge themselves in the JWorks atmosphere.To learn about new technologies, trends and best practices in our domain.Talks &amp; Workshops  Frederick Bousson: Unbeatable at Connect 4 with Wearables and Computer Vision  Tom Verelst: Testimonial TVH MyAssetPlanner  Dieter Hubau: Visualizing Mandelbrot fractals using Riff and Spring Cloud Function  Pieter Van Hees: Testimonial HZIV  Julia Müller: The 10 worst mistakes your product owner can make  Remco Runge: Deep learning in practice  Bart Blommaerts: Innovation without asking permissionUnbeatable at Connect 4 with Wearables and Computer Vision by Frederick BoussonFrederick Bousson kicked off the technology track with his talk about wearables.Wearables are becoming more mainstream every day as proven by the prevalence of fitness trackers and smart watches.Smart glasses on the other hand are still very rare.Mainly because they still look awful.But improvement is on its way here as well: better looking sets are starting to appear like the Vuzix Blade.This is important as humans are very vision-oriented: 90% of the information on the internet is processed via the eyes.Connect 4To really demonstrate the power of smart glasses, Frederick was looking for an interesting case he could use as a demonstration.The result of this quest was a way to help you become unbeatable at Connect 4.Connect 4 is a “solved game”, which means that there’s an algorithm that can’t be defeated (provided you get to go first).Frederick went hunting in the Open Source community and discovered a program written by Brian Borowski which contained this algorithm.4 years ago, Scott Bouloutian added visual processing code on top of that and yet another person ported the result to Google Glass.Nice to see how, with open source, you have people who build upon each others’ work.Image processingIf you want to have your smart glasses solve your game of Connect 4, you’ll need to do some image processing.And since the game revolves around colour which means your program doesn’t just need to see, it has to reason about these colours as well.Reasoning in colour is a pain in the ass though.OpenCV, Open Computer Vision, is a great library for image processing and it’s available for a lot of programming languages.Unfortunately it doesn’t use RGB (Red-Green-Blue) as a color space, it uses BGR (Blue-Green-Red) instead.If your brain is conditioned to think in RGB, switching to BGR is not an easy task.Before you can start solving the game you need to know the position of the pieces that were already played.And before you can find those pieces you need to know where the board is.In order to find the board you’ll need to look for a large, blue rectangle.Then you draw lines around it and find the corners.Now your application “sees” the board.  The next step is to look at the location of the played pieces.For this you need to look inside the board for concentrations of the right color.If you’re using a board that has a tray for the game pieces this tray is also part of the board and you’ll need to cut off a bit from the bottom.Of course if you’re using a different light source, colours are perceived differently by the camera (e.g. the same colour in fluorescent lighting is green while it will be matched to yellow when using a light bulb). This means that the colours found by openCV might no longer match the ones you programmed in so you’ll also have to take the white balance into account.The resulting data is then mapped into a 2-dimensional array which then can be fed to an Minimax AI solver.This solver will use a decision tree and you can define the amount of positions it needs to think ahead.This process also takes advantage of alpha-beta pinning. It uses a binary tree to decide which option is the best, but as soon as it sees that one branch scores too low it will no long check that part of the tree.All in all it’s a pretty performant bit of code: OpenCV can take the image, process it and spit out the results in about 50ms.Everything is possibleAR/VR can do anything, but that makes you wonder why it is not everywhere yet.There are quite a few reasons for this: first of all you need a business case.Companies won’t invest a lot of money into something that doesn’t have a return on investment.And even if there’s money to be made in it you still need customers.People actually have to want your product.TakeawayThe key takeaways of Frederick around image processing are:  Everything is possible  Think before you act  Just do it  You can rewatch his talk on our channel on Youtube.Testimonial TVH MyAssetPlanner by Tom VerelstTom spoke about the application MyAssetPlanner he has been working on at TVH.TVH is a company which rents out equipment and can provide you with spare parts for a lot of tools and vehicles.MyAssetPlannerTomorrowland is quite a big event and for setting up and tearing down they rent up to 160 machines (or “assets”).For every asset a contractor will need to schedule it to be available at a certain time, resulting in more than 300 phone calls which need to be handled by the asset planner.TVH wanted to improve the customer experience and decided they were going to develop a new application to help with the planning of these assets: MyAssetPlanner was born.First there was a design sprint by Clockwork in order to come up with a prototype and the most important business needs which needed to be implemented. This part took about two weeks.After this prototype, TVH started with the implementation of the first version which was put into production two months later.This first version has currently been in use for four months including for the last iteration of Tomorrowland.For development, the Spotify model was used which is focussed around:  autonomous squads  do it yourselfThe architecture is driven by events and uses Kafka as a backbone allowing you to just consume what you need, the events in which you are interested.The entire CI/CD pipeline is fully automated thanks to Spinnaker.This allows a developer to commit a feature which gets automatically deployed to an acceptance environment.As soon as the product owner approves the feature it will be rolled out to production.The teams work with Scrum Agile, it was interesting to hear about how they handle the sprint review.After every sprint, different business stakeholders receive a scenario and one person of the team is present for assistance.All the stakeholders must go through the scenario themselves.This forces your stakeholders to work with the application instead of just watching a demo, resuling in a ton of valuable feedback.What is nextFor the moment there are a 17 customers using the tool. Very soon this will scale up to 5% of all TVH customers which amounts to 3.000.After that there will be a gradual ramp up towards 20.000 customers.  You can rewatch his talk on our YouTube channel.Visualizing Mandelbrot fractals using Riff and Spring Cloud Function by Dieter HubauFunction as a ServicePivotal Function Service (PFS) is a pretty new addition to the Pivotal landscape.Riff, an open source project, is the base of PFS.The idea behind Riff is to easily create functions and move these to the cloud.Its name is derived from the guitar world as you can see here, the project lead is Mark Fisher.Initially Zookeeper and Kafka were used to send messages between functions but this did mean a lot of extra overhead to set up.Google found out about Riff and came to Pivotal with Knative which runs on Kubernetes.You can find its repository on GitHub.Knative allowed Pivotal to remove a lot of the boilerplate needed to run with Kafka and Zookeeper.Knative has four components:  Knative Serving: request/reply messages.  Knative Build: auto-detect your code and create the required containers and sidecars.  Knative Eventing: replaces Kafka and Zookeeper providing channels and subscriptions.  Fourth Knative block: makes it run really well on Google Cloud.RiffA new version was released a couple of weeks ago.Since this is a very fresh and new project, this did mean a lot of late nighters to migrate the code of the demo.Experimental projects like this do tend to make frequent API changes during their initial development phase as backwards compatibility is currently not (yet) a requirement.MandelbrotOne of the simplest mathematical functions which creates the most complex objects is the Mandelbrot set.You can keep zooming in, resulting in more calculations the deeper you go.It also tends to create pretty graphics which is nice for a demo.The codeFor a 400 x 400 image we will send a request to the backend to calculate the result for every pixel.This means that around 160.000 calls will be sent to do the calculation.It is not the most efficient way to do the calculation but this demo is about using functions and not about calculating a Mandelbrot really fast.Riff uses containers to package your business logic.There are two kinds:  UserContainer: actual container contains the actual function  InitContainer: they run before your container.You can use these to instantiate a database.For one function you need about 200MB of memory as it spins up a slimmed down Spring Boot in the backend.All in all it is still pretty performant.Dieter mentioned a talk of Dave Syer about performance in Spring which is very interesting to watch: link.It was a nice talk showing what you can do with Riff but also a warning that it still is under development and using it might have you encounter bugs.If that does happen do not hesitate to reach out to the Riff team.When working on his talk, Dieter also raised several issues.  You can rewatch his talk on our channel.Testimonial: HZIV by Pieter Van HeesPieter is currently on a project at HZIV, a government health agency which offers its members legal health and disability insurance.The old situationA lot of old applications still run with the almighty Cobol.10 years ago, HZIV wanted to rewrite these using Java 5 and Swing but that rewrite didn’t go so well.So they are still using these old beasts.A new startWith more time comes more insight and it was decided to do a new rewrite and also to give the teams more freedom in how to set up their development environment and choosing the technologies that are best suited.Chosen technologies are, most notably:  Java 8 &amp; 10  MongoDB  ELKCurrently the teams are working in an agile way, delivering fancy new features.Working @ HZIVWhat is great?You have great freedom to choose and test out new stuff. At the same time the atmosphere is relaxed, allowing you to explore without pressure.What is bad?There is the risk of Developer Anarchy, where developers just introduce fancy new stuff without properly ironing out the edges.Some managers like to micro-manage.The same managers also tend to introduce frequent scope changes which does not combine well.Why should you want to work here?Appearantly working at HZIV feels like being in a spy HQ.The combination of some old furniture and modern technology create a very particular atmosphere.Very interesting to hear about an organization who has embraced a new way of working and is currently in the process of finding their way around it.To see what goes well and what does not.It is also surprising that this can happen in a government agency as most of them in Belgium are not known to be very innovative.  You can rewatch his talk on our channel.The 10 worst mistakes your Product Owner can make by Julia MüllerA product ownerIs responsible for maximizing the value of the product and the work of the development team.1. PO doesn’t know his/her productHaving no knowledge of your product is fatal, because you are responsible and accountable for the success of your product.A good tip is that as a product owner you should be able to explain your product in 3 minutes.Use a product canvas to help you understand your product.2. Team doesn’t have a productIf a team does a lot of little tasks, it will miss context and will have to endure frequent scope changes.This means that the team will have very little focus which is not very effective.3. There is no vision for the productIt is bad when user stories describe solutions like: The user needs a button to be able to ... This causes the team to stop thinking about what would be the best possible solution.Do not forget that a team of people has more knowledge than one individual.Scrum is developing towards a goal and is about autonomous teams.The vision which you as production owner provide must provide direction and guidance for the team.A helpful tool is a “postcard from the future”.4. The non-economic dreamerEvery feature adds complexity, increases the chance of failure, and makes future changes more expensive.A product owner must be able to reason economically as every feature should add a clear benefit.A product owner should also not hide behind ‘something’ the tech team does.As those ‘things’ also determine the cost and business value of your product.5. Tech Debt not my Problem.A good technical design is important for the success of your product. For example: Investing in delivery speed might not add a direct business feature but will reduce development cost and allow you to provide new features faster in the future.Customer value is different to business value.An investment can also lead to business value.6. The YES-sayerIf you always say ‘yes’ to every feature request then your product may become a Frankenstein monster.It is better to discuss more upfront instead of implementing features you might need to remove later.Wishful thinkingDon’t fall in the trap of wishful thinking.Do not just assume that your product will be a success.Know how to validate your assumptions and make sure that you have a good feedback loop so that you know that the features you provided fulfil the business requirements.Ignoring the factsIgnoring the obvious signs that you are not going to meet your goal.So many demos are made with Powerpoint, a real sprint review should be the user starting the application and using it themselves.If they get stuck you need to ask them why, as these situations will give you great feedback.7. Obsessing over detailsThe product owner who used to be an analyst.A huge backlog is not good, a good exercise is to take the top 50 and delete the rest.In a huge backlog there is a lot ‘relevant’ information in the stories, however this information tends to become stale.Later on it will also become increasingly difficult to implement these stories as there is no certainty that the information they contain is still correct.Another big no-no is having a ‘definition of done’ being more than one page long for a user story.A nice concept is the three C’s:  Card (post it - fat pen): a story is just a single card. This is a reminder that we need to talk about this.  Criteria: Only Acceptance Criteria on the story.  Conversation: During refinement there will be a conversation with the team and then you will expand the story.Also be careful not to be a perfectionist.No first release is going to be perfect.Being fast in the market beats having a complete product.8. No presence, no communicationCommunication is one of the most important factors of failure in an organization.A product owner must be with the team and not communicate from the sidelines.9. Crossing competencesThis is not a 13, but it is much smallerIf your team says it is a 13, it is a 13.They have the knowledge to estimate and implement a story.10. Product owner is the only one filling the backlogLet the team bring their own ideas to the backlog.This will help them focus and deliver a better solution.Focus on what truly matters for your product.QuestionsThe product owner has a lot of hats, how can he manage all this?The product owner is indeed responsible for a lot of stuff.But being responsible does not mean that you need to deliver everything yourself. Learn to delegate.The product owner should also not be the only one who meets the stakeholders and not everybody should have to be a stakeholder.Learn to remove stakeholders who do not contribute to the business value.Remember the Frankenstein monster.Can a product owner ignore stakeholders wishes?A Product Owner should be able to take his own decisions and decide to run some tests on customers and collect the resulting data.It is sometimes better to ask for forgiveness later.If the product owner has no trust of the organization making him/her unable to take decisions, then that person should not be the product owner.  You can rewatch her talk on our channel.Deep learning in practice by Remco RungeMachine learning: you try to get the good algorithm to find the stuff you want to find.Deep Learning is based on biology, about how people thought the brain functioned. The assumption was proven wrong but the basic principles still work.Tensorflow has a nice playground where you can see these effects live in action.Deep learning has two flows:  Forward propagation: feed data through it.  Backward propagation: update weights so the next iteration will be better.You can define weights on input in order to make certain inputs more important.Deep learning requires a very large network with a large amount of nodes and a lot of hidden layers.The early layers allow your network to distinguish basic structures within your data.Further down in the layers the network will learn more detailed features like; eyes, ears, …It is cool that the network learns these features about how to recoginize a person just by providing it with pictures of faces. We did not provide it with the concept eyes but eventually, in one of the iterations the network started to take eyes into consideration.Deep Learning at OrdinaWorkshop: hack a drone.A small drone was equiped with deep learning to detect and identify objects by using the Deep Learning for Java.In a short demo we saw it could identify a notebook.Tic Tac ToeIn contrary to Go, Tic Tac Toe just has 360.000 possible moves making it easier to use for a demo.The network for this demo learned by playing against some random opponents without any real strategy, this made it quite dumb. You could improve this by letting it play against itself.Digital Railway SurveyThe idea is to recognize signs next to the railway and verify if these have been installed at the correct place.You Only Look Once is a very cool algorithm which gives you very nice bounded boxes around the objects on the images making it easier for your network to know where it needs to look and thus preventing you from processing unrelevant pictures.It was also nice to see that with deep learning the system can distinguish signs from a passing train even when we would be unable to see these signs, let alone see what has been written on it.QuestionsHow long does it take to get started?If you work with existing data sets and known algorithms, 15 lines of code is enough.Remco mentioned that for a brand new project it can easily take up to 55 days just to show that deep learning is possible for a specific business case.It takes a long time to gather the required data.You need to label it and make sure it is diverse enough so your model is not too focussed on one specific subset of input.  You can rewatch his talk on our channel.Innovation without asking permission by Bart BlommaertsService decompositionWhen you want to split up a monolith into smaller applications there are three patterns you can follow.  Split: define vertical functional boundaries.This does not happen to be possible in all old applications.  Strangle: extract and re-implement logic in new components.  Extend: new functionality in new microservices. Do pay attention to not create a distributed micro-**** fest, also known as a distributed monolith.Generally you will not just use one pattern but all three.Anti-corruption layerThe anti-corruption layer translates to and from different models.It allows you to enforce loose coupling between contexts.You can use different patterns for it:  Shared repository: just share the same repository between various services.  Data-synchronized repository: each microservice has one database asynchronously synchronize the data.  Event-driven synchronised repository: the main idea behind events, do not wait for data to be asked but just publish it.An event happened in the past and it contains three types of data:  Data it owns: this is data tied owned by the publisher in the event  Data it needs: this is data which can originate from other services but which is necessary to handle the event  Referenced Data: data which might be relevant for the event. For example when booking a holiday, the reference temperatures of the location to where you want to travel to.The event should contain enough information so that consumers do not need to query for additional data.Otherwise you will not think properly about your bounded contexts and you will not be able to build a loosely coupled system.In an ideal world you can talk with your business stakeholders about how these events should look like.Bounded contextsBart indicated that it is very interesting to look at the DDD lite movie from Greg Young.It may look complex and after watching it the first time you might decide that you don’t ever want to do Domain-Driven Design, but make sure to watch it a second time and things will definitely become more clear. ;-)Identify the domainWhere does your microservice start?Identify domains and subdomains.These subdomains tend to correspond with your bounded contexts.How?Look at the data model:  Talk to business stakeholders.Make the list of subdomains explicit.  Look at the data.  Look at the code, both existing and historically.If two files always get committed together, they most likely belong to the same bounded context.Integration patternsBetween bounded contexts there can be lots of integration patterns. A great source of information about these patterns can be found on the site of Gregor Hohpe: Enterprise Integration Patterns.DemoThe previous concepts were then shown in a short live demo providing a real implementation.What was interesting to see was that Apache Avro was used as a schema for the events.This shared schema repository is then the only coupling between your services.Using a schema allows your consumer services to see if they can process an event or not.APIsThere exist a lot of API guidelines which you can use as baseline for your own, for example: Paypal, Zalando, …This talk is not about API design but when you design an API please keep Postel’s law in the back of your head.Postel’s Law: Be conservative in what you do, be liberal in what you accept from others.Do make sure that you have documented your API using: Swagger, Avro, HAL, RAML.Without asking permission?Continuous experimentation is the first thing to introduce.You should really try to introduce that culture.Microservices are small and thus can be easily thrown away.Use feature toggles and monitor your users in order to gather user feedback about which business features are the most interesting for your stakeholders.With traffic routing you can make sure that only certain users get access to certain features allowing you to experiment with a very low risk.Use this data, these metrics you gathered, to convince your business stakeholders.Distributed systemsAre hard, see the eight fallacies of distributed computing.MonitoringSee the hard nature of distributed systems as a way to introduce monitoring and logging (Grafana, Prometheus)You can also provide this monitoring data to your users to give them more insights in the application, giving them direct feedback of their actions.Which can give you new opportunities.For example: They will see failures and they will want this to be resolved.MaintenanceDo not forget to take maintenance into consideration before adopting some new technology.Because you should not forget that you will still need to maintain it.  You can rewatch his talk on our channel.  You can find the slides of his talk on Slideshare.Thanks KevinMany thanks to our colleague Kevin for organizing the JOIN event.Without all of his work it would not have been as great as it was.There is much more  This was just summary of some of the talks we had on JOIN, there were many more as you can see on YouTube.Next YearHope to see you around next year. Keep an eye out for future updates on our JOIN site."
      },
    
      "development-2018-11-05-managing-translations-with-crowdin-html": {
        "title": "Managing translations with Crowdin",
        "url": "/development/2018/11/05/managing-translations-with-crowdin.html",
        "image": "/img/2018-11-05-managing-translations-with-crowdin/main-image.png",
        "date": "05 Nov 2018",
        "category": "post, blog post, blog",
        "content": "  As your application continues to grow, you may want to support more and more languages.Managing all that in the source code will become very hard and prone to errors.Luckily there’s an easy solution to that and it’s called Crowdin!Table of contents  What is Crowdin  Key features  Getting started  In app translations  ConclusionWhat is CrowdinCrowdin is an online platform that allows you to do translations through a visual interface.You can appoint people as translators for any natural language.Either an integration with your Git repository or a CLI tool can be used to get the source strings into Crowdin.After uploading the source strings, the translators can get to work.They’ll be notified whenever there’s something new to translate.When they’re done, approvers can start proof-reading with the help of some thorough quality assurance functionalities.You’ll never miss a translation again because it will clearly be marked as not being translated.Strings that are removed from the source file will also be removed from all translations.The translated files can be downloaded at any moment and will be in the exact same format as the source file.In other words, clean and up-to-date translation files!Translator view:Approver view:Key features  A large number of file formats are supported, ranging from json files to csv and properties files.It can thus be used for Angular, React, Vue … applications as well as a Spring Boot service and so on.  Crowdin has a very clever parser and detects thing such as HTML tags and variables.It will warn you if they are translated and no longer match the original tags/names.  Quality assurance checks will help the approver in verifying the translations.  Extra context can be provided for each source string to help translators understand what exactly they need to translate.  Integration with source control systems is very easy and will make sure that the source strings are up-to-date with your Git repository. Translations will automatically be pushed to your Git repository.  There’s even the possibility to integrate it with your website and have people do the translations on your website directly.That way the translators get a lot of context of what to translate.  If you’re unable to do the translation for a certain language, Crowdin allows you to easily hire professional translators!Getting startedAccountTo start using Crowdin, you of course need an account at Crowdin.com. There’s a limited free trial. After that you can choose from a variety of payment plans depending on the number of source strings and projects you’ll have.  The number of projects you need on Crowdin depends on your setup.However, it’s recommended to create a project for each Git repository.So if your front- and backend application live in the same repository, you can configure multiple source files within one project.If not, you’ll need to create two separate projects on Crowdin.Once logged in, you can create a new project. You can choose the source language and the languages you wish to translate to. Then go to the project’s settings and then to API.There you’ll find a project identifier and a API key. We’ll need those later when setting up Crowdin in our project.Setup in your projectFirst we need to install the Crowdin CLI. For Mac you can run brew install crowdin. For Windows an installer is available.Once installed, open a terminal and go to the root of your project and run crowdin generate.This will generate a crowdin.yml file that is used to configure Crowdin in your project.In that file four things need to be changed:  project_identifier: the one you find under API on the Crowdin site  api_key: also under API  source: this should point to the source translation file (e.g. in an Angular app /src/assets/i18n/en.json)  translation: the translated files will end up here (e.g. /src/assets/i18n/%two_letters_code%.json)  I don’t recommend selecting the source language as a target language, because it will then be overwritten when downloading the translations.You could put the source language file in a separate directory if you prefer the source language being editable on Crowdin as well.However, you would have to update the source file manually when there are changes.  There are more options and regexes available, but these are the only ones required to set up Crowdin in an Angular app for example.Upload and download translationsThere are two options here: either you integrate Crowdin with your Git repository or you up and download the translations manually (or automate it using CLI commands).I personally prefer integrating it with Git so we’ll get to that first.  It’s not possible to use both at the same time because you’ll end up with multiple source files on Crowdin.com.Git integrationOn the Crowdin website, go to the project’s settings again, then to Integration.Choose the source control system you’re using and click on Set Up Integration (or choose Enterprise if you’re on a self-hosted version).For GitHub and GitLab you’ll be redirected to their website to authorize Crowdin to use your account.If you chose Enterprise you’ll need to generate a token yourself and paste it into the correct field together with the URL to your self-hosted source control system.Next you can choose which branches you want to watch for changes to the source language file and choose a name of the branch to where translations should be pushed.  Which source branch you wish to use depends on your workflow. In my case, we work with short-lived feature branches and once they are done, we merge them to the master branch.So, the master branch is the one being watched by Crowdin, since it will have all the correct source strings.With this setup, each change to the source language file will be reflected on the Crowdin site.When a source string is translated and approved, Crowdin will push the translation to the target branch (e.g. i18n_master) and even open up a merge request for your review!A great benefit of this is that your stories don’t have to wait for all translations to be done.Translations are now the responsibility of the people that you appointed to do the translations.When they do, you’ll be notified with a merge request!Manual up- and downloadIf you don’t want to integrate with your Git repo, you can use the Crowdin CLI.There are two simple commands that let you upload the source files and download the translations.Upload:crowdin upload sourcesDownload:crowdin downloadIn app translationsAnother nifty feature is that you can do translations directly in you webapp.All you need to do is add a JavaScript snippet to your index.html and select a pseudo language called Acholi.Crowdin will give unique identifiers to all of your source strings and use those as translations.Because of the snippet, these IDs will be located and replaced with translated strings for the language you’re translating to.You will also be able to edit the translations inline in your application and they will immediately be reflected on Crowdin.Now, this isn’t something you want to do in your production environment.Ideally you’d only have a specific environment setup that loads the JavaScript snippet.If the snippet is loaded you’ll always see the Crowdin tools in your webapp,so it’s up to you whether you want to use it and where.More info here.ConclusionCrowdin has really helped me and my team in the way we manage our translations.At first we were sending out emails for every label to get it translated.Of course we missed quite some translations in various languages.The variables in the translated string would sometimes be translated as well which meant that they wouldn’t work.With Crowdin we now have a clear overview of what is translated and what isn’t.Instead of sending emails, translators get notified and can do the translations through a nice UI.Because of the thorough quality assurance tools, less mistakes are made and those translated variables can be fixed before approving the translation and bringing it to production.In short, it’s a must-have tool when you need to manage translations, especially for languages you do not speak yourself!"
      },
    
      "microservices-2018-11-02-inter-service-communication-html": {
        "title": "Communication in a distributed system with OpenFeign: Tips &amp; Tricks",
        "url": "/microservices/2018/11/02/Inter-service-communication.html",
        "image": "/img/intercommunication/intercomm_header.jpg",
        "date": "02 Nov 2018",
        "category": "post, blog post, blog",
        "content": "In contrast to monolithic applications, services in a distributed system are running on multiple machines. To let these services interact with each other, we need some kind of inter-process communication mechanism.With the help of OpenFeign, I will explain how we can fire off synchronous calls to another service.Table of contents  Setup  Different kinds of HTTP clients  Enabling Mutual SSL  Intercepting requests  Give it a (re)try  Securing your API  Creating SOAP clients  Handling errors with the error decoder  ConclusionCommunication with OpenFeignTo understand the basics of inter-process communication, we need to look at what kind of interactions we can do.OpenFeign, a declarative HTTP client by Netflix simplifies our way of interacting with other services. When we decide that it is time to decompose our modulith because of numerous reasons, we would have to look for a way to handle our inter-process communication.SetupTo use OpenFeign we need to add it to our classpath&lt;dependency&gt;    &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;When we inspect the dependency module, we see that there is a lot coming out-of-the-box with the Spring Cloud Starter.If you are providing your own resilience or load balancing library you can just add the necessary dependencies you need.Be aware that the syntax is different between using the Spring wrapper or OpenFeign itself.To let your Spring context know that it has to search for OpenFeign clients, we just add @EnableFeignClients. You can add this annotation to any class annotated with @Configuration, @SpringBootApplication or @SpringCloudApplicationAfter we’ve enabled OpenFeign on our classpath, we can start adding OpenFeign clients. When defining these clients, we have two solutions you can choose from. The OpenFeign library, which provides us with the basics but very customizable OpenFeign clients, and the Spring library, that adds a few extra libraries to it for cloud tooling.Spring@FeignClient(value = \"auth\", fallback = FallbackAuthClient.class, configuration = FeignConfig.class)public interface AuthClient {     @RequestMapping(method = RequestMethod.GET, value = \"checkToken\")    boolean isTokenValid(@RequestHeader(\"Authorization\") String token);}  @FeignClient: is the annotation for Spring to recognize OpenFeign clients, OpenFeign clients have to be interfaces as it is self-declarative.  value/name: is the name of the Feign client that will be used to create a Ribbon load balancer which can then be linked to the target application using service discovery or a fixed list of servers. You could also use the url attribute to point your client to the target application when you’re not using Ribbon.  fallback: if Hystrix is enabled, you can implement a fallback method.@Componentpublic class FallbackAuthClient implements AuthClient {    @Override    public boolean isTokenValid(@RequestHeader(\"Authorization\") String token) {        return false;    }}  configuration: is for extra configuration like logging, interceptors, etc… more on that below.  @RequestMapping: Spring Cloud adds support for Spring MVC annotations and for using the same HttpMessageConverter's used by default in Spring Web.OpenFeignTo create an OpenFeign client we need an interface and a Feign builder that tells the interface it is an OpenFeign client.public interface AuthClient {    @RequestLine(\"GET /auth\")    @Headers({\"Authorization: {token}\", \"Accept: application/hal+json\"})    boolean isValid(@Param(\"token\") String token);}  @RequestLine: is defining which verb and which URI path you are communicating to.  @Headers: is defining the request headers that come with the request.The builderOpenFeign provides us with a builder-like pattern for our clients.When we want to customize, we just add our own customization to the builder. To see the builder at work, let’s create a bean of our client and return a Feign builder.It’s important to let the builder know which interface he has to target for communication. The second parameter is most likely the base url where all the requests begin. Get your URLs from the yml or properties file with the help of @Value.@Value(\"${base.url}\")private String baseServerUrl;@BeanAuthClient authClient() {    return Feign.builder()            .target(AuthClient.class, baseServerUrl);}Different kinds of HTTP clientsThe default HTTP client of OpenFeign uses HttpUrlConnection to execute its HTTP requests.You can configure another client (ApacheHttpClient, OkHttpClient, …) as follows:@BeanAuthClient authClient() {    return Feign.builder()            .client(new ApacheHttpClient())            .target(AuthClient.class, baseServerUrl);}OkHttpClientOkHttp is an HTTP client that’s efficient by default:  HTTP/2 support allows all requests to the same host to share a socket.  Connection pooling reduces request latency (if HTTP/2 isn’t available).  Transparent GZIP shrinks download sizes.  Response caching avoids the network completely for repeat requests.ApacheHttpClientThe advantage of using ApacheHttpClient over the default client is that ApacheHttpClient sends more headers with the request, eg. Content-Length, which some servers expect.  Aside from these clients, there are a few more to research if you want : OpenFeign clientsEnabling Mutual SSLMutual SSL is supported in all of these clients.To achieve this in an ApacheHttpClient, we have to create an HttpClient that builds the SSL context.When the SSL context is valid, we wrap this inside an ApacheHttpClient for being compliant with OpenFeign.public ApacheHttpClient createHttpClient() throws SSLException {    HttpClient httpClient = HttpClients.custom()            .setSSLSocketFactory(createSSLContext())            .build();    return new ApacheHttpClient(httpClient);}Add it to the builder.@BeanAuthClient authClient() {    return Feign.builder()            .client(createHttpClient())            .target(AuthClient.class, baseServerUrl);}Give it a (re)tryWhen we want to build some resilience in our communication, we can setup a retry mechanism in our OpenFeign client. If the other service is unreachable, we will try again until it is healthy or until the max attempts you have set in your configuration has been reached. When we want to use the retryer of OpenFeign, we got three properties we can set.      period: How long it takes before the retry is triggered        maxPeriod: That’s what the maximum is of how long it can take before a retry is triggered        maxAttempts: How many retries may the client trigger before it fails  Example:@Value(\"${retry.period:3000}\")private int period;@Value(\"${retry.maxPeriod:30000}\")private int maxPeriod;@Value(\"${retry.maxAttempts:5}\")private int maxAttempts;@BeanAuthClient authClient() {    return Feign.builder()            .retryer(new Retryer.Default(period, maxPeriod, maxAttempts))            .target(AuthClient.class, baseServerUrl);}Intercepting requestsIf you need some basic authorization, custom headers or some extra information in every request of the client, we can use interceptors. This becomes very useful in situations where every request needs this extra information.To add an interceptor, we just add an extra method that returns the OpenFeign interceptor.private RequestInterceptor requestInterceptor() {    return requestTemplate -&gt; {        requestTemplate.header(\"user\", username);        requestTemplate.header(\"password\", password);        requestTemplate.header(\"Accept\", ContentType.APPLICATION_JSON.getMimeType());    };}To enable the customization, we add the interceptor to the builder.@BeanAuthClient authClient() {    return Feign.builder()            .requestInterceptor(requestInterceptor())            .target(AuthClient.class, baseServerUrl);}Securing your APIWhen we want to add the security layer between our services, there are a couple solutions to look at. Here are a few that can be handled by OpenFeign.BasicWhen you want to send basic credentials you can just add an interceptor for the OpenFeign client and add the username and password.BearerFor only Bearer token communication, you can just pass it down in the request header of your method call.//Spring@RequestMapping(method = RequestMethod.GET, value = \"checkToken\")boolean isTokenValid(@RequestHeader(\"Authorization\") String token);//OpenFeign@RequestLine(\"GET /auth\")@Headers({\"Authorization: {token}\", \"Accept: application/hal+json\"})boolean isValid(@Param(\"token\") String token);OAuth2This link provides a good explanation about the use of OAuth2 with OpenFeign: OAuth 2 interceptor.Creating SOAP clientsBesides JSON encoders and decoders, you can also enable support for XML.If you ever have to integrate with SOAP third party APIs, OpenFeign supports it.There is a very detailed explanation on how to use it in the documentation of OpenFeign.Handling errors with the error decoderThe OpenFeign API provides an ErrorDecoder to handle erroneous responses from servers.Since there are many kind of errors we can get, we want a place where we can handle each one of them accordingly. An OpenFeign ErrorDecoder must be added to the configuration of the client object as you can see in the code below.@BeanMyClient myClient() {   return Feign.builder()           .errorDecoder(errorDecoder())           .target(MyClient.class, &lt;url&gt;);}Rather than throwing an exception in the decode method of the ErrorDecoder, you return an exception to Feign and Feign will throw it for you.The default error decoder ErrorDecoder.Default always throws a FeignException.The problem with ending up with a FeignException is that it does not contain a lot of structure. It is a plain RuntimeException which only contains a message with a stringified response body. No way of interpreting that exception to rethrow a more functional exception eg. UserNotFoundException.Error decoderTo handle the errors, we have to look at the structure of these errors. From that structure, we build up our own exception and throw it so the ControllerAdvice class can handle our exception.public class CustomErrorDecoder implements ErrorDecoder {    @Override    public Exception decode(String methodKey, Response response) {        CustomException ex = null;        try {            if (response.body() != null) {                ex = createException(response);            }        } catch (IOException e) {            log.warn(\"Failed to decode CustomException\", e);        }        ex = ex != null ? ex : new CustomException(\"Failed to decode CustomException\", errorStatus(methodKey, response).getMessage());        return ex;    }        private CustomExceptionException createException(Response response) throws IOException {        String body = Util.toString(response.body().asReader());        List&lt;ErrorResource&gt; errorMessages = createMessage(body);        return createCustomException(body, errorMessages);    }        private List&lt;ErrorResource&gt; createMessage(String body) {        return read(body, \"$.errors\");    }        private CustomException createCustomException(String body, List&lt;ErrorResource&gt; errors) {        CustomException ex = new CustomException();        ex.setErrors(errors);        int status = read(body, \"$.status\");        ex.setStatus(Integer.toString(status));        ex.setTitle(read(body, \"$.title\"));        return ex;    }} Warning: Working with checked exceptions and Feign is a bit tricky for several reasons.Returning a checked exception is possible in the ErrorDecoder, but to avoid Java’s UndeclaredThrowableException, you’ll have to add it to the method signature in the Feign interface. Doing this however, causes Sonar to complain because there’s no actual code which throws that exception.ConclusionThese were my experiences with OpenFeign and I like the simplicity of it. If you choose for the Spring wrapper or OpenFeign, the client is an advanced tool for enabling inter-service communication.As of now, they just released a new version that is compliant with Java 11.So go experiment and learn on the way!Sources  OpenFeign Documentation  Spring Cloud OpenFeign Documentation  OAuth 2 interceptor  SOAP integration  Other HTTP clients  Ribbon  Hystrix"
      },
    
      "kickstarters-2018-10-29-kickstarter-trajectory-2018-summer-html": {
        "title": "Kickstarter Trajectory 2018 Summer Edition",
        "url": "/kickstarters/2018/10/29/Kickstarter-Trajectory-2018-Summer.html",
        "image": "/img/kicks.png",
        "date": "29 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Introduction  54 young professionals started the Ordina kickstarter trajectory this summer, on the 1st of August. JWorks counted 5 kickstarters: Sander, Steven, Ken, Wout en Michaël. All looking for a challenge and a fresh start. Some freshly graduated from school, others having multiple years of experience, IT related or not. The goal was to broaden every kickstarter’s knowledge of the fast evolving IT world. We learned the basics of every hot topic which will definitely give us enough credits for our first project.The kickstarter trajectory consisted of two parts:  One month covering all kinds of trainings: technical, non-technical, backend, frontend, DevOps…  After our minds were filled with all this information, there was a DevCase where we could put everything we learned into practice.First DayOn the first day of the kickstarter trajectory we were welcomed into Ordina and got an introduction about the structure of the company. After that we took a tour around the building, and we were told what the different workspaces are and where the different teams work. It was nice to notice that everyone we met was very friendly and helpful. This made us feel directly at ease.On the first day we also received the keys of our car and a laptop, so we were equipped to begin our journey at Ordina.SecurityIn the beginning of the trajectory we got an introduction by Tim De Grande of the most important security principles like GDPR. This is very important to Ordina because all its consultants need to keep this information in the back of their minds when working at a customer.We also followed a more technical security course which explained some of the most common attacks and how to avoid them.BackendJavaIn this lecture given by Yannick De Turck, we were introduced to all new features of Java 7, 8 and 9 aswell as Java 10.We started off with Java 7 where we learned:  Switch-statement with String values  Automatic Resource Management  Diamond Syntax  Better exception handling with multi-catch  Literal Enhancements  The new IO API  Fork Join Framework  JVM EnhancementsJava 8 also introduced some useful new features:  Lambda Expressions  Extension Methods  Functional Interfaces  Method and Constructor References  Streams and Bulk Data Operations for Collections  Removal of PermGen  New Date &amp; Time API  New Default API for Base64 Encoding  Improvements for Annotations  Performance ImprovementsJava 9 introduces:  Project Jigsaw: Modules  Project Kulla: JShell  Factory Methods for collections  Diamond operator for anonymous inner classes  Try-with-resources enhancement  CompletableFuture API improvements  Private methods in interfaces  HTTP 2.0 Client  Process API Improvements  Reactive Streams  Optional Improvements  Collectors Improvements  Stream ImprovementsLast but not least we had an introduction of Java 10 which delivers:  Local Variable Type Inference  Unmodifiable Collections  New Optional.orElseThrow() method  Performance Improvements  Container awareness  Root CA Certificates includedIn the afternoon we made a few exercises on these new features and improvements which gave us a brighter view on the possibilities within present Java development.Spring and Spring BootThe lectures of Spring and Spring Boot were given by Ken Coenen. These were spread over two days where the first day was an overal introduction to Spring fundamentals, followed by a second day where we have put everything into practice.Day 1:The first day we got an introduction to Spring and Spring Boot about the core fundamentals and concepts of JPA, Beans and application contexts. After that we went further into the features of the framework where we were introduced to Spring Web Services and Spring Security.Day 2:On the second day we made a small project where we created a backend application to fetch all information of different digital coins (cryptocurrencies). We learned how to read data from an API using DTOs and storing them into a database. At the end of the day we had a fully working backend application which fetched all information automatically and exposed it to different endpoints.MicroservicesTim Ysewyn and Kevin Van Houtte gave a brief overview of the microservices architecture. We learned when this is best applicable. This can be applied when there is a monolith that is responsible for multiple different tasks. It could be better in that case to split off these different tasks into multiple microservices. One of the advantages of doing that is the possibility to deploy the different microservices separately and the possibility to upscale the resources of the microservices that receive the biggest loads.Clean CodeDuring this course of clean code, Pieter Van Hees taught us the best practices of how to write clean code. This improves the readability and performance of our applications which is of great importance to Ordina.FrontendThe frontend courses started off with HTML/CSS/JavaScript given by Yannick Vergeylen after which we went more in-depth of other topics.Build tools:In this course given by Michael Vervloet, we started off with Node.js and its features like asynchronous programming and event emitters.Later, we learned about NPM and other package managers and how to use them inside a project like an Angular app. At the end, Angular-CLI was covered, the start of one of the most important frameworks we use at JWorks.Typescript:In the TypeScript course given by Kevin Van Houtte, we built further on Node.js and NPM. We did an exercise about school management where we used OO-programming and CRUD in TypeScript. This was pretty challenging but with each other’s help, we managed to get the final assessment done.Angular:Angular was the last frontend course given by Ryan De Gruyter. This helped us to quickly create a frontend that is connected to a backend project. Here, we went more in-depth on the SPA framework and how different components interact with each other.DevopsThe trajectory also included courses about the DevOps culture. We got some introductions to Docker, Kubernetes and CI/CD given by Tim Vierbergen.Docker &amp; KubernetesThese courses were given by Tom Verelst. He explained us the basics of containerization, more specifically how this is done by Docker.During this hands-on session we learned how to work with containers and images. We learned how to use, create and delete them. There was also an explanation of the theory behind containerization and what the advantages are of using this, especially when compared to using virtual machines.To orchestrate the containers, we received a course on Kubernetes. There we learned about the concepts of pods, secrets, and more.We practiced this in a small exercise where we needed to configure a Minikube and run a simple application.CloudThe last technical session we followed was about different cloud technologies. This was given by Dieter Hubau and Bas Moorkens.We learned about the advantages of running applications in the cloud and what the differences are between the different operation models.To make this more tangible, we went into multiple cloud platforms to see what the possibilities were. At the end we focussed on OpenShift Origin as this is one of the preferred container management packages inside JWorks.Soft SkillsBesides sharpening our technical skills we worked on our soft skills as well.In the 2-day session ‘Essentials of Communication’ we learned how to present ourselves by means of role playing games and cases that reflect real world scenarios.After an additional ‘Brand Yourself’ session we were ready to prove and present ourselves as worthy consultants to the management of Ordina.All these techniques are also useful in the Agile &amp; Scrum methodologies where we learned the importance of being prepared for change.DevCaseIntroductionDuring the second month of the kickstarter trajectory we were assigned to develop an event planner.The purpose was to have more of an overview and control of the upcoming JWorks events.In short, JWorks employees can create and approve events depending on their rights.In addition, a weekly digest of the events is sent to the JWorks Telegram chat group and a Telegram bot is made available with some defined commands.Technology &amp; MethodologiesTogether with our coaches Orjan De Smet, Axel Bergmans and Haroun Pacquée we started off with an introduction to the project.The user stories were presented on the Scrum board.After defining the sprint goal for the first upcoming two weeks we divided ourselves into a frontend and a backend group. Using Scrum methodology we held our daily stand-up meetings and as soon as a new functionality was developed a pull request was made and reviewed by our coaches.Every two weeks, at the end of the sprint, a demo was shown to our coaches followed by a retrospective and a sprint planning.By making use of Continuous Integration, code changes in Github were automatically deployed to OpenShift where a Jenkins pipeline went through different stages ranging from testing the code and code quality, to building the Docker image.Frontend tools that were used in the project:  Angular  Angular-CLI  Angular Material  JestIn the backend we made use of the following technologies:  Java 8  Spring Boot  Mockito and JUnit  Telegram API  Keycloak SecurityFor more technical details of the used tools and technologies of the DevCase, a separate blog post will follow!Personal ExperiencesKen Keymolen  The kickstarter trajectory provides the means to learn new evolutions &amp; technologies within the IT world.The DevCase gave us a good understanding on how to incorporate new technologies within an IT project. These trainings provide a solid base to continue to build our skills &amp; expertise in the different areas IT has to offer,making sure we are positioned to provide the best solutions for our customers.Sander Smets  Before the kickstarter trajectory, I did not really have an in-depth view on deployment and cloud automation. Our DevCase and trainings made sure that all of us have a complete understanding of frontend and backend technologies, cloud automation and new architecture strategies like DDD and microservices. Now I feel like a more complete developer and ready to tackle day-to-day problems at customers.Wout Meskens  I am very happy that I have been given the opportunity to follow the kickstarter trajectory. The first month updated my knowledge about a lot of interesting topics. It was especially interesting to learn about the DevOps technologies. The DevCase was very helpful to put all this new knowledge into practice. It was fun to see that we could make an exciting application with all these technologies. The kickstarter trajectory made me excited to use these technologies to help customers.Steven Deleye  The kickstarter trajectory was the chance for me to learn a lot about new technologies in a short amount of time.Putting this information into practice during the DevCase gave me more understanding in how and when we use these technologies.Michael De Wree  The kickstarter trajectory was not easy, but achievable. This made me so much more excited. Especially the DevCase was a good way to gain technical experience. Besides the possibility to learn and grow, Ordina makes me feel at home. I look forward to the next couple of years!"
      },
    
      "conference-2018-10-26-lead-dev-london-2018-html": {
        "title": "The Lead Developer London 2018",
        "url": "/conference/2018/10/26/lead-dev-london-2018.html",
        "image": "/img/2018-10-26-Lead-Dev-London-2018/lead-dev.png",
        "date": "26 Oct 2018",
        "category": "post, blog post, blog",
        "content": "London!For the fourth year in a row White October Events and Meri Williams organised The Lead Developer: a two-day conference covering topics from leadership to self-improvement and team dynamics, mixed with some technical talks. This year Ordina JWorks travelled to the Barbican Centre in London to attend this amazing and inspiring conference for the very first time and returned home energized and full of ideas!  As tech lead you have to be a great developer. But you also need to be a good team leader, be commercially astute, and stay on top of all the tech developments and trends coming over the horizon. With a balance of content across team, tools and tech, The Lead Developer conference is designed with the complexities of the job in mind.In this blogpost we will summarize some of the talks that are mainly focused on personal development of a team lead. All talks were recorded and can be found on The Lead Developer London 2018 YouTube channel. Highly recommended!‘First steps as a lead’ by Dan PersaWhy become a lead in the first place?According to Dan Persa, dealing with people problems is much more interesting than dealing with computers, but each company interprets this in a slightly different way. Some call it “tech lead”, others call it “people manager”, but in either case it is your job to understand what the role is all about and check that it fits your expectations.Becoming a lead is not a natural evolution for a developer. It’s a career switch and not a promotion; because a different skillset is required.The Don’t-Repeat-Yourself principle, which is a key principle for each software developer, does not apply when working with people. Communicate things over and over again.Repetition is key.Getting into leadership is not trivial, so ask people around you for help. How do you learn these leadership skills very fast?  Shadow a mentor to ensure a smooth transition and not feeling lost.  Don’t just read books, but put them in practice as well.  Reflect and remember good and not-so-good behaviour from your former leads.As a lead you need to create an open and safe environment with mutual trust.But how you do earn the trust and respect of your team?  Open yourself up to be vulnerable.  Have regular one-to-ones.  Be transparent in decision-making, especially around performance evaluations.  The best solutions to problems come from the team, let them take ownership of the problem.  Create a feedback culture: give feedback as soon as possible after the action in need of feedback happened, decouple development feedback from performance evaluations, and organise health checks meetings.  Celebrate successes!  “Not taking a decision is a decision: we are accountable for the decisions we take as well as for the ones we decide not to take.” @danpersa #LeadDevLondon‘Building sustainable teams to handle uncertainty’ by Jenny DuckettWhat can disrupt a team?Because we frequently make deliberate changes about how we’re working in our team, change is often a good thing. However, there’s uncertainty involved.  People can leave or join the team; team members can feel un(der)valued. The dynamics of personalities and opinions change whenever people change.  It can be hard for teams to make decisions and handle roadmaps while senior leaders move. Revisiting decisions that you thought you’d settled and making progress can be hard.  Shifting wider priorities can make a team feel frustrated and disempowered.  Re-organising teams out of existence can make people feel undervalued as individuals.  Even things like desk moves can have a significant impact on people. More about this on Lara Hogan’s blogpost.Putting people through these things more than occasionally, will result in a retention problem.The way changes make people feel is at least as important as the change itself.Emotions are critical.  “An empowered, open team who understands its context can adapt to change.” @jenny_duckett #LeadDevLondonWhat can you do to prepare?First of all, work on yourself so you can sustain through these disruptive changes and be able to support your team better as a result.You don’t need to do it all yourself: let go of details, give people your trust and assume positive intent.Set yourself up to lead sustainably.Make your team’s work ownable by stopping to split efforts and starting to define a single and clear goal. Communicate this goal over and over again.It will feel like you’re just repeating yourself and you’re boring people, but it’s reassuring just to know that the goal hasn’t changed.Give your team the background they need for each piece of work by adding context to user stories, organizing story kickoffs and running workshops.Empower your team to take ownership by embracing opportunities for positive change.Also become great at integrating new people by assigning them a mentor. Let them improve your onboarding guide and technical documentation.Make it clear that you value learning as part of everyday work, recognise that everyone is always learning.Share understanding of your work in the team, the whole context is important.This can be done by writing good commit messages (also the why not just the what), but also by documenting decisions and the reasons for it, and keeping your documentation up-to-date.Taking ownership helps people handle change.Support and grow individuals. Use every piece of work to help someone grow and start with individual needs.Effective delegation can be accomplished by being clear and explicit, explain what’s happening and why.Also set the scope and boundaries, what’s the problem that they’re trying to solve, who’s involved and when should they bring questions to you.Grow the next generation of leaders by teaching people to do your job and make yourself dispensable.Show your team where they fit into the wider organisation and how they relate to its goals.A wider view helps people adapt when things change.Don’t over-insulate your team because when something gets through your shield, it will be very disruptive.Grow people for a resilient team and organisation, and encourage your team to show off their work, to be proud of it.Make sure that your manager understands how much work it takes from everyone to build a team that works well and the time it takes to become familiar with the domain they’re working in.Ask them for support to build a sustainable team and show why all your team’s work matters.Good communication about change is vital.  “When stressful things happen, it’s easy to default back to not communicating. But that’s when it’s even more important to empower people during tough times.” @jenny_duckett #LeadDevLondon‘The hardest scaling challenge of all - yourself’ by Christian McCarrickCommunication  Be an effective communicator: as you grow, you’ll get better at getting your point across to larger and larger groups.  Communicate up to the manager, across peers and down to your team.  Personal branding and self-promotion are important: have confidence and show the value of your work.  Create a safe environment by genuinely praising other people and giving honest feedback.  Seek out for negative feedback and ask specific questions for better feedback.  The most important communication skill to learn is how to say “no” without feeling guilty.Prioritization &amp; Time management  If you don’t prioritize your life, someone else will.  The Eisenhower Matrix: How to make decisions on what’s urgent and important.  Multitasking is an anti-pattern, so write down all the things you need to do and keep a journal.  Turn off email notifications. Instead, schedule short blocks during the day when you quickly check emails and schedule longer blocks at the end of the day where you tidy things up.  Get into the zone by blocking your calendar: dedicate time to specific things.  Make yourself a priority once in a while. It’s not selfish, it’s necessary.  Obsess with the things that matter!Delegation  Move away from “How can I get things done?” to “How can this task/decision/goal get done?”.  Not delegating properly is one of the biggest anti-patterns and limits the scaling your entire team because you become the bottleneck around decision making.  Let things go!Personal development &amp; mental health  Leading takes energy and often gets lonely.  If athletes get injured, they sit on the bench. Mental fatigue and burnout are issues that affects us more than we might think.  Excercise, meditate (tip: Headspace), and read every day.  “Your job as a manager is to make your team more badass.” @cmccarrick #LeadDevLondon‘Go slow to go fast: building strong foundations for leadership’ by Alicia LiuAlicia talked about how her rapid increase in job responsibilities in a rapidly growing startup led to an increase in stress, difficulty sleeping, and ultimately depression. She wrote a great blogpost and gave a very inspiring talk about the tools she used to recover from depression and insomnia, and how she became a better leader and manager.A lot of engineers are promoted into management because they’re good engineers, regardless of their management skills.Some people are naturally good at both, but if you’re good at engineering, it might actually be harder to develop leadership skills. Coding drains a lot of mental energy, while managing others requires lots of emotional energy.Being a good engineer doesn’t make you a good leader because information is not the same as knowledge. You can’t study to be a leader, you’ll have to change and embrace discomfort to become a good leader. Be humble by listening to others, being present with each person and regularly switching context is a hard thing to do.Focus first on form then speed.Mastering others is power, mastering yourself is true strength.  “A gardener doesn’t tell plants how to grow. A gardener creates the best environment for the plants to flourish. But you still need to know how to garden, and you need to know what to weed out. Leadership is about dealing with the unknowns.” @aliciatweet #LeadDevLondon"
      },
    
      "kafka-2018-10-23-kafka-stream-introduction-html": {
        "title": "Kafka Streams introduction",
        "url": "/kafka/2018/10/23/kafka-stream-introduction.html",
        "image": "/img/kafka/kafka-logo-thumb.png",
        "date": "23 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Kafka Stream - A practical introductionThe GoalThe aim of this article is to give an introduction to Streaming API, and more specifically to the Kafka streaming API.Since I’m not really into writing huge loads of theory, I’m going to try and keep the theory to the minimum to understand the basics and dive directly into the code by using an example.That left the task to find a useful example, for which I got inspired by the work of some collegues. They created an IOT system that measures the usage of the staircase in big buildings with Lora IOT sensors (Stairway To Health).So I thought that this is indeed streaming data, the people that open the doors of the staircase are considered as being the stream.With that done let’s go to the theory …The theoryEase of UseKafka Streams is a simple client library which only depends on Kafka.So if you have Kafka, there is nothing else you will need to do in order to be able to work with Kafka Streams.GuaranteesKafka Streams provides you with guarantees making it safer for you to process records.The (intermediate) state of the stream processors is stored within replicated changelog topics allowing Kafka Streams to recover from failures and resume processing after replaying the changelog topics.A Kafka Streams stream processor will receive one input record at a time.It will apply its operation to it, like: map, filter, join and send out one or more output records to the downstream processors.Since 0.11.0, Kafka is able to process deliver messages exactly once, the same logic can be used within Kafka Streams so a record will only be processed exactly once.Just set processing.guarantee to exactly_oncewith the default being at_least_once.DSLKafka Streams provides a Domain Specific Language which is recommended for most users, especially beginners.  KStream: a KStream is created from a specified Kafka input topic and interprets the data as a record stream.   It will only receive records of a subset of the topic partitions.  All of the topics’ available partitions will be processed by Kafka Streams instances.  KTable: a KTable is also based on a Kafka topic, but is interpreted as a changelog stream.   So that for every record key only the most recent value will be returned, it will also handle a subset of partitions.  GlobalKTable: a special type of KTable, as its data will be populated with records from all the partitions of the input topic.After you have created your KStream or KTable you can start with a variety of transformations on your record stream, like: filter, map, flatMap, groupBy, …As you will see later in the example we provide, it is all quite easy to grasp.TopologySource ProcessorA Source Processor does not have any upstream processors and it will produce an input stream from one or more Kafka topics.Stream ProcessorA node within the processor topology representing a single processing step, it is used to transform data.Sink ProcessorA Sink Processor will act like a sink, it will send any of the received records to a specific Kafka topic without any further processing.StreamThis corresponds to an unbounded, continuously updating data set. Like a Kafka topic it will consist of one or more stream partitions.Local StateEvery stream task in a Kafka Streams topology can use one or more local state stores.These state stores can be a RocksDB database or an in-memory hash map.When data is persisted to a local state store Kafka Streams provides automatic recovery in the case of some failure allowing the processing to continue.WindowingTime is pretty important when dealing with streams, we distinguish the following notions of time within streams:  Event time: when the event occured.  Processing time: the time when the event was processed by the stream processing application.  Ingestion time: the time when the event was stored within a topic by Kafka.Windows will allow you to group your records with the same record key towards that time.We have the following types of windows:Tumbling time windowsThese feature fixed-size, non-overlapping, gap-less windows.Since the windows do not overlap, a data record will belong to only one window.Hopping time windowsHopping time windows feature a fixed size, but the advance interval (aka “hop”) can be different to that fixed size.These windows can also overlap, so that a data record may belong to more then one window.Session windowsThese represent a period of activity separated by a defined gap of inactivity.All events within that gap will be merged with an existing session.If the gap is too large, a new session window will be created, the size of the window itself will thus vary.The Practical PartDisclaimerThis project is intended as a first step into the world of streaming, so some shortcuts were taken, and not all design decisions are production ready. A good example is the use of strings as the content of the messages, this should be done in a more structured way (with Avro for example){:target=”_blank” rel=”noopener noreferrer”}.Setup of the projectThis is the really easy part, to use the streaming api from Kafka only 1 dependency must be added. Here is the example to do it in Maven.    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;            &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;            &lt;version&gt;1.1.0&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;And that is it. Sometimes life can be simple. :)Creation of the inputIn the real world this would be done by the IOT devices that send their data through the network to the central system. But since it is not easy for demo purposes to have a sensor and a door nearby, and even less handy to open and close it a couple of hundred times to test it out,I created a simulator that just sends data to the Kafka cluster.This simulator creates two kinds of messages:key = 0E7E346406100585, value = T_7configuration information about on which floor a certain device is located.key = 0E7E346406100585, value = pulse@1496309915each time a person opens the door, the key is the unique id of the device, and then a pulse and the time at which it occurred    List&lt;String&gt; devices = new ArrayList&lt;&gt;();    devices.add(UUID.randomUUID().toString());    devices.add(UUID.randomUUID().toString());      devices.add(UUID.randomUUID().toString());    KafkaProducer producer = new KafkaProducer(props);    //send the device information    for (int i = 0; i &lt; devices.size(); i++) {        String val = String.format(\"%s@%s@%s\", \"T\", \"\" + (i + 1), System.currentTimeMillis());        producer.send(new ProducerRecord(\"stream_in_dev\", devices.get(i), val));    }    try {        Thread.sleep(1000);    } catch (InterruptedException e) {    }    new Random().ints(10, 0, devices.size()).forEach(i -&gt; {        producer.send(new ProducerRecord(\"stream_in\", devices.get(i), \"pulse@\" + System.currentTimeMillis()));        try {            Thread.sleep(250);        } catch (InterruptedException e) {}    });    producer.close();This wil create three floors with a random device id, and afterwards it will send an event for a random door being opened 10 times.Reading of the outputFor checking what happens in the system a data dumper was created that outputs all the messages on all the topics of interest (the input, the output and the intermediate queues included).public class DumpData {    private static Logger log = LoggerFactory.getLogger(DumpData.class);    public static void main(String... args) {        ExecutorService executorService = Executors.newFixedThreadPool(2);        executorService.submit(() -&gt; {            KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(defaultProperties(\"your_client_id\"));            List&lt;String&gt; topics = consumer.listTopics().keySet().stream()                    .filter(streamName -&gt; streamName.startsWith(\"stream_\"))                    .peek(log::info)                    .collect(Collectors.toList());            consumer.subscribe(topics);            while (true) {                ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);                records.forEach(record -&gt; {                            log.info(\"topic = {}, partition = {}, offset = {}, key = {}, value = {}\",                                    record.topic(), record.partition(), record.offset(), record.key(), record.value());                        }                );            }        });    }}This will subscribe to all the topics that start with ‘stream_’ on the Kafka.The main partSo we finally arrived at the part where it all happens.Just as a recap, the goal of this stream is to transform both input streams into a stream that gives how many people took the stairs at each floor.As a start we must create a new StreamBuilder from the Kafka library    final StreamsBuilder builder = new StreamsBuilder();We need to get the data somewhere, here we get it from the same topics our data simulater writes to.    KStream&lt;String, String&gt; sourceDev = builder.stream(\"stream_in_dev\"); // which device is where    KStream&lt;String, String&gt; stream_in = builder.stream(\"stream_in\"); // the pulse messagesThe first stream (stream_in_dev) will be converted into a lookup table that is used when handling the second stream. This lookup table will contain which device is installed on which floor.    KTable&lt;String, String&gt; streamKtableDev = sourceDev        .groupByKey()        .reduce((val1, val2) -&gt; val1, Materialized.as(\"stream_ktable_dev\"));Since one of the shortcuts we took is creating all the topics with only one partition, we don’t have any problems with streams not having the data it needs.If using multiple partitions, then we should have used either the KGlobalTable or we should have made sure that the partitioning is done in such a way that we get the corresponding data from both partitions on this node.The second stream contains the pulses. Each time a person takes the stair, a message is sent, and this must be added to the counter of the people taking the stairs at that specific minute.        stream_in                .filter((key, value) -&gt; value.startsWith(\"pulse\"))                .leftJoin(streamKtableDev, (pulse, device) -&gt; device.substring(0, device.lastIndexOf('@')).replace('@', '_') + pulse.substring(pulse.indexOf('@')))                .map((k, v) -&gt; new KeyValue&lt;&gt;(v.substring(0, v.indexOf('@')), v.substring(v.indexOf('@') + 1)))                .groupByKey()                .windowedBy(TimeWindows.of(TimeUnit.MINUTES.toMillis(1)))                .count()                .toStream((k, v) -&gt; String.format(\"%s - %s\", k.key(), Date.from(Instant.ofEpochMilli(k.window().start()))))                .mapValues(v -&gt; \"\" + v)                .to(\"stream_out\");This seems to do a lot of things and this is indeed the case. But the API makes a clean chain that is not hard to follow.  .filter() We only want the inputs that start with a pulse, the real IOT devices also send battery information and so on, on the same topic.This could also be solved by sending them to different topics but it shows that filtering is possible  .leftJoin()  we join with the devices lookup table created with the previous KTable statement. This allows us to translate the device id into the location.key is ‘0E7E346406100585’ and value is ‘pulse@1496309915’ will be translated to the same key but with value ‘T_7@1496309915’  .map() we map the message into something more useful. in stead of the key ‘0E7E346406100585’ and value ‘T_7@1496309915’ format we now get a key of ‘T_7’ and a value of ‘1496309915’.  .groupByKey() we want to group these by key (which is now the floor number and not the device id like it was in the beginning)  .windowedBy() and create a tumbling window for each minute  .count() and within the window count the number of items  toStream() This means that the last three lines together change the stream into a stream that gives the number of messages per minute for a certain floor  mapValues() map the result of this into a new stream that gives the amount per minute where the key is the floor (T_7) and the value is a combination of the amount and the when (5 - Thu Oct 10 16:28:04 CEST 2018)  to() send it to the output stream"
      },
    
      "agile-2018-10-23-team-under-high-voltage-html": {
        "title": "Team under High Voltage",
        "url": "/agile/2018/10/23/Team-under-High-Voltage.html",
        "image": "/img/teams-under-high-voltage/main-image.png",
        "date": "23 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Recently a client called on us to help bring motivation back to a development team.Although the final deadline for delivery was nearing rapidly, team members started to leave the project, worn out and fed up.Need it be said that this didn’t exactly promote work joy and team spirit – or, for that matter, diminish the already unrealistic workload.How had matters come to these dire straits?It surely wasn’t the case that a dedicated multi-skilled dev team was lacking.Only, apart from this, the project seemed to be tainted with all the stains one can think of to make being Agile and rolling out Scrum difficult or impossible.When teams are immersed in a waterfall culture and stuck in the iron triangle of project management, there is little chance for Agility to be a success.Making matters worse, a belated start of the project (due to a delayed go) had gradually created a growing requirement/necessity to work quickly rather than focusing on quality.The easy answer would be… just fix Scrum and all the rest will follow.That makes sense, for indeed, Scrum can fix a whole lot, with effort, time and a great deal of customer convincing.Creating customer satisfaction by maximizing the business value, is of course the ultimate goal of every software project.But the belated launch of the project and the sheer impossibility of still attaining the definition of success – made me realize that it would be very wrong to talk about story points or speeding up velocity.It wouldn’t even be right to keep focusing on minimizing the MVP – which in this case is at a bare minimum already.We all have to choose our battles wisely.The first thing to do, I thought, is enable the team to find a common denominator, a list of preferred behaviours and shared interests that would make them feel better as a team.Primary focus is ensuring that the project mood board switches from taking to giving energy to the people working on it.This exercise makes it possible to disconnect the team members’ degree of happiness and motivation from the looming deadlines and the ever-changing backlog.This makes success somehow achievable again, albeit a more personal sort of success, focused on growth and team dynamics.I know this sounds like a long shot, but at least it could possibly lead to a solution with a long-term outcome.In working together the Agile way, we must always remember that people and interactions come first.Read the Manifesto carefully – and you will see that the psychological safety of a team is key to the whole lot – enabling flow away from the items on the right and making room for the items on the left.From there, all the rest is due to follow.Burning up story points should never be accompanied by burning up energy.What is burning down at one side should be building up at the other.Once this is ensured, progress comes at a steady pace.Details about the exerciseAre you interested in knowing how I helped the team detect these common denominators?For that I used the liberating structure 1-2-4-All, which goes like this:  Everybody gets a card and a thin marker.  In a time box of one minute, everyone by themselves writes down what comes after “I hate it when people…”(Somehow it’s easier for people to find examples of unwanted behaviour, than asking them for what they appreciate.)  Next two minutes they pair up and share their examples, thinking the value and good behaviour that could counterbalance the negative one.This value should be written on the other side of each card.  Team up to make a group of four and share your good behaviours and values during a time box of four minutes.Start clustering them when doubles or connections start to emerge.  For ten to fifteen minutes (depending on how big the group is), everybody explains to the group their preferred behaviour and the related value.Try to cluster them into groups and define a common denominator for each cluster, to make it more manageable.Make sure that everyone understands them and that nobody perceives any contradictory combinations.  Ask for commitment of the group to try and stick to these behaviours and values.Anyone who doesn’t respect them, can be made accountable and should be addressed by the others.Don’t hesitate to reach out if you have any questions regarding this topic!"
      },
    
      "development-2018-10-20-make-your-own-cli-with-golang-and-cobra-html": {
        "title": "Make your own CLI with golang and cobra",
        "url": "/development/2018/10/20/make-your-own-cli-with-golang-and-cobra.html",
        "image": "/img/make-your-own-cli-with-golang-and-cobra/banner.jpg",
        "date": "20 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Getting started  Adding functionality  Adding flags  Using environment variables instead of flags  Taking a file as input  That’s it!  There are lots of command line interfaces (CLIs) available these days, some of which are written in Golang.There’s even a big chance that you’re using one of them.Docker or Kubernetes for example, have a CLI written in Go.These were written using a framework called Cobra.And that’s exactly what I’m going to use to show you how to write your own simple CLI with Golang.I’m also going to show you ways to provide parameters like environment variables and config files to your CLI, so you can do more than just the basics.Getting startedThe framework Cobra provides a generator that adds some boilerplate code for you.This is handy because now you can focus more on the logic of your CLI instead of figuring out how to parse flags.Assuming you have Golang correctly installed, you can get the generator by doing the following:$ go get github.com/spf13/cobra/cobraThis creates an executable which you can run from anywhere, because it is located in the $GOPATH/bin directory, which is in turn added to your PATH variable if you installed Go correctly.You can go ahead and create a new folder for your Go code.The name of the folder will be used as the name of your CLI.Navigate inside this folder with your terminal and execute $ cobra init to start a new project.It should generate a main.go file and a cmd package.Sidenote: by default Cobra will add an Apache License.If you don’t want this, you can add the flag -l none to all the generator commands.It will however add a copyright claim at the top of every file (// Copyright © 2018 NAME HERE &lt;EMAIL ADDRESS&gt;).If you pass the flag -a YOUR NAME the claim will contain your name. These flags are optional though.When you look inside the main.go file, there’s not much going on. It just calls the execute function of the cmd package.This function resides in the root.go file, which is doing a lot more. For now, just focus on this part:// rootCmd represents the base command when called without any subcommandsvar rootCmd = &amp;cobra.Command{   Use:   \"hello-cobra\",   Short: \"A brief description of your application\",   Long: `A longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.`,   // Uncomment the following line if your bare application   // has an action associated with it:   //  Run: func(cmd *cobra.Command, args []string) { },}Here you can define how to use the CLI, together with a short and a long description.Adding functionalityIn the next step we need to think about which actions we want the CLI to perform.A good practice is to work with verbs like get, post, describe,… For our example we want the CLI to say hello, so I’m going to construct it as follows:$ hello-cli say helloTo do this we can leverage the generator again to add a new command for us:$ cobra add sayIt will generate a file inside the cmd package called say.go.In this file you can again specify the way you want the command to be used and describe its function.You’ll also see the execute function which gets executed every time you call hello-cli say.You’re probably never going to use it like that, except with a --help flag. If a user calls it like that, we want the user to know he needs to provide additional items to the say command.So we’re going to return an error if that happens.The Run function of the cobra command doesn’t return anything by default.You can however change Run to RunE, which expects the function to return an error if there is any:RunE: func(cmd *cobra.Command, args []string) error {    return errors.New(\"Provide item to the say command\")},The RunE function also shows the help output if there’s an error.This is to show the user how to properly use your command.At the bottom of the file you’ll see a function called init:func init() {   rootCmd.AddCommand(sayCmd)   // Here you will define your flags and configuration settings.   // Cobra supports Persistent Flags which will work for this command   // and all subcommands, e.g.:   // sayCmd.PersistentFlags().String(\"foo\", \"\", \"A help for foo\")   // Cobra supports local flags which will only run when this command   // is called directly, e.g.:   // sayCmd.Flags().BoolP(\"toggle\", \"t\", false, \"Help message for toggle\")}Here you can add subcommands and flags.By default, the say command is added to the root command, which is exactly what we want.Let’s add a sub command to say hello:$ cobra add sayhelloLike the say command, sayhello is added to the root command.In this case we want it to be added to the say command instead:func init() {    sayCmd.AddCommand(sayhelloCmd)}To make the sayhello print “Hello World”, edit the Run function:Run: func(cmd *cobra.Command, args []string) {    fmt.Println(\"Hello World!\")},Now execute the following inside your projectfolder:$ go installFollowed by:$ hello-cli say helloIf everything worked correctly, your CLI should output “Hello World!”.Adding flagsTo make your CLI a bit more interesting, we are going to add some flags.You can choose between local and persistent ones.Local flags are available only for that command, whereas persistent flags are also available for the subcommands of that command.For this example we want to greet a person by name.We’re going to do this by making a --name flag.Navigate to the init function of the sayhello.go file, and add a flag:func init() {    rootCmd.AddCommand(sayCmd)    sayhelloCmd.Flags().StringP(\"name\", \"n\", \"\", \"Set your name\")}The first string is the full name of the flag, and can be executed with two dashes like --name.The second string is the short notation, which can be executed with one dash.The third one is the default value, and the fourth is a description.To make the flag do something we need to add some logic to the Run function:Run: func(cmd *cobra.Command, args []string) {    name, _:= cmd.Flags().GetString(\"name\")   \tif name == \"\" {       \t\tname = \"World\"   \t}  \tfmt.Println(\"Hallo \"+name)},Do a quick go install, and check if it works:hello-cli say hello -n NickThe output should be “Hello Nick”.Using environment variables instead of flagsIf you don’t want to pollute your command line, or if you’re working with sensitive data which you don’t want to show up in the history, it’s a good idea to work with environment variables.To do this, you can use Viper. Viper is another dependency from Steve Francia.Cobra already uses Viper in the generated code, so why not use it as well.You can however achieve the same result by using the os package from the Go standard library.We want to make the environment variable the default value.If you remember, the default value is set in the init() function.:func init() {    sayCmd.AddCommand(sayhelloCmd)    sayhelloCmd.Flags().StringP(\"name\", \"n\", viper.GetString(\"ENVNAME\"), \"Set your name\")}That’s all you need to do to parse environment variables.Taking a file as inputFor this next part,  we want to provide multiple parameters to our CLI.You can provide flags for every single one of them, but when you have a lot of parameters, a config file can be a better option.So create a new file with a .yaml extension, and add the following contents:name: \"Billy\"greeting: \"Howdy\"If you used the generator, There will already be a flag configured that expects the path to an initial config file.You can find this flag in the root.go file under the initConfig() function.As you can see, it uses Viper again to do this.Good news! We don’t have to do it ourselves!The only thing we need to do is to extract the variables from the file:Run: func(cmd *cobra.Command, args []string) {    greeting := \"Hello\"    name, _ := cmd.Flags().GetString(\"name\")    if name == \"\" {        name = \"World\"    }    if viper.GetString(\"name\")!=\"\"{        name = viper.GetString(\"name\")    }    if viper.GetString(\"greeting\")!=\"\"{        greeting = viper.GetString(\"greeting\")    }    fmt.Println(greeting + \" \" + name)},Again use Viper in the same way as you did before with the environment variables.Try it out by doing another install and entering the following command:$ hello-cli say hello --config config.ymlThat’s it!  You have successfully written a CLI in Golang that can parse about any variable around the block! All sample code can be found on GitHub."
      },
    
      "architecture-2018-10-12-spring-boot-angular-gradle-html": {
        "title": "Integrate Angular in Spring Boot using Gradle",
        "url": "/architecture/2018/10/12/spring-boot-angular-gradle.html",
        "image": "/img/2018-10-12-spring-boot-angular-gradle/angular-spring-boot-gradle.jpg",
        "date": "12 Oct 2018",
        "category": "post, blog post, blog",
        "content": "I often found myself struggling when I had to integrate Angular into a Spring Boot project. This is something that shouldnot take a lot of your valuable development time. That’s why I want to share how I did it with Gradle in a very fast and easy way.I set up an example repo which you can find on GitHub.Application structureThis guide assumes you have a root directory that contains two child directories. One with the Angular code, and another one with the Spring Boot code. By keeping these apart from each otherit will be easier to develop within the application.We’ll make use of Gradle’s multi-project buildsto split the application into multiple modules.Because I generated my Spring Boot project with Spring InitializrI already have a Gradle Wrapper, gradlew file, gradle.bat file and settings.gradle file available. We want to move those to our root directory. Keep the build.gradle file within the Spring Boot directory.We should also add a new build.gradle file to the root directory and another one to the child directory with our Angular code.Your application structure should look like this:.├── build.gradle├── gradle│   └── wrapper│       ├── gradle-wrapper.jar│       └── gradle-wrapper.properties├── gradlew├── gradle.bat├── todo-api│   └── build.gradle├── todo-ui│   └── build.gradle└── settings.gradleYou should have three build.gradle files, we’ll check their content within a minute. But we first have to tell Gradle the name of our project and make sure it will recognize the two modules. This can be done in settings.gradle.rootProject.name = 'todo'include 'todo-api', 'todo-ui'We’ll use todo as the project name and include both the backend and frontend module by specifying their directory name.It’s important that this name matches the path of the directory, otherwise Gradle cannot find these modules.Gradle will now recognize both child directories as a subproject.AngularFor the Angular part we want to create a jar with a static directory that contains the result of our Angular build. By doing this we can include the jar in our backend module. And because Spring Boot will automatically add static web resources located within static, the Angular application will be visible when we launch the application.This can be done by using the com.moowork.node plugin.Let’s take a look at the build.gradle file in our todo-ui project.// 1plugins {  id 'java'  id \"com.moowork.node\" version \"1.2.0\"}// 2node {  version = '9.2.0'  npmVersion = '6.4.1'  download = true}// 3jar.dependsOn 'npm_run_build'// 4jar {  from 'dist/todo-ui' into 'static'}Let me explain step-by-step what we’re doing here:  We need the java plugin to have the jar task available and the com.moowork.node plugin to execute node scripts like npm_run_build.  We have to specify which node and NPM version we want to use.  Before creating the jar with the jar task we want to build our Angular project, otherwise we don’t have any static files to serve. This can be done by using the npm_run_build task.1  When we build the Angular project our static files will become available in dist/todo-ui. We want those files into static. The from 'dist/todo-ui' into 'static' command in the jar task will simply copy everything from dist/todo-ui into static.2When we build the subproject it will run npm run build and create a new jar with the build result in the static directory.We can now setup Spring Boot to include the jar.Spring BootOur Spring Boot project already has a build.gradle file generated. We just have to add one line within our dependencies to include the todo-ui module.dependencies {\timplementation(project(':todo-ui'))}Build the project, execute the generated Spring Boot jar and go to localhost:8080, you should see your Angular web application. That’s all folks!NoteThere are several ways to integrate Angular in Spring Boot using Gradle (or Maven). To me, this is the easiest and fastest way to do it. Because we’ve separated the backend from the frontend it will be a lot easier to find your way in both modules.Now go and apply this on your own projects! Don’t hesitate to contact me if you have any questions.1 by default, npm run build will execute ng build (specified in package.json)2 by default, Angular will output the build result in dist/{project-name} (specified in angular.json)"
      },
    
      "angular-2018-10-08-angular-state-management-comparison-html": {
        "title": "NGRX vs. NGXS vs. Akita vs. RxJS: Fight!",
        "url": "/angular/2018/10/08/angular-state-management-comparison.html",
        "image": "/img/2018-10-08-battle-of-the-state-managers/battle-of-the-state-managers.jpg",
        "date": "08 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Reason for comparingWhen creating a web application, one of the questions to ask is how data should be managed.If the application must be reactive, it’s best to use ReactiveX (or Rx for short) to create streams of data.The next question is how this could work performant and reliable.The current trend is to use a Redux-based storage solution, which consists of a Store, Selectors to get data from the store in the form of Observables and Actions to modify the store.This allows for a single source of truth, a read-only state and the flow of data going in one direction.There are a couple of different solutions for Angular.NGRX is by far the most popular, leaving the new kids in town, NGXS and Akita, far behind in popularity.It is, however, not always needed to have a storage framework solution.Very small applications are easy to create with plain RxJS, if you are quite skilled.In this post, I’ve stacked each of these solutions against each other to see what can be learned.Table of content  The fighting ring  The competitors  Fight          Available Tooling      Features      Boilerplate code      Community      Dependencies and size        Final scoreThe fighting ringTo compare the four competitors, I’ve set up a simple To Do-application (GitHub) with Angular CLI.The master branch holds a base of the application, which cannot run on its own.It needs a state management solution from one of the other branches.To make the comparison easier, the base application is written in such a way that each solution only adds files to a statemanagement folder and loads a service and zero or more modules into the AppModule.No other files (except package.json, package-lock.json and logo.png) are to be changed.From an end-user perspective, the application would appear and behave the exact same, no matter which state management solution is used.The logo is added to be able to differentiate which solution is running.A To Do-application is perfect to demonstrate CRUD.A FakeBackendService is provided to simulate a RESTful API backend.The idea is to load the list only once in the application’s lifetime and then update the state, without needing to fetch everything from the backend again.As such, the FakeBackendService logs its calls to the console for monitoring.The competitorsNGRX(v6.1.x) Docs  RxJS powered state management for Angular applications, inspired by Redux  @ngrx/store is a controlled state container designed to help write performant, consistent applications on top of Angular.NGXS(v3.2.x) Docs  NGXS is modeled after the CQRS pattern popularly implemented in libraries like Redux and NGRX but reduces boilerplate by using modern TypeScript features such as classes and decorators.Akita(v1.7.x) Docs  Akita is a state management pattern, built on top of RxJS, which takes the idea of multiple data stores from Flux and the immutable updates from Redux, along with the concept of streaming data, to create the Observable Data Stores model.  Akita encourages simplicity. It saves you the hassle of creating boilerplate code and offers powerful tools with a moderate learning curve, suitable for both experienced and inexperienced developers alike.Plain RxJS(v6.0.x) DocsOne of the aims of a (progressive) web application is to minimize loading time by reducing the package size.In that light, some developers opt to not use a framework, but instead use plain RxJS.To simulate a store as much as possible, I’ve used BehaviorSubjects to hold the state and pipeable operators to modify the state.Fight1. Available toolingSince this post is aimed at developers, it might be best to first evaluate the tools available for developers.A Redux Devtools plugin exists for Chrome and Firefox, or it can be run as a standalone application.It allows developers to see the impact of a Redux action and time travel between these actions.Another useful feature available to Angular developers is Angular Schematics, which allow to create pieces of code through Angular CLI.None of the solutions have these tools in their default packages and they need to be installed separately.Redux DevToolsDevTools in NGRXNGRX provides @ngrx/store-devtools for DevTools.It works as expected, displaying the latest actions with their impact and the resulting state of the store.It’s possible to jump to specific actions and even skip actions.It also allows devs to dispatch an action directly from the DevTools itself, but does not verify that action’s payload.Implementing the tools is as easy as importing the following line to the AppModule:StoreDevToolsModule.instrument()NGRX DevTools provide options for displaying a maximum age of actions, displaying a name, logging only to console, sanitizing state and actions and serializing the state.DevTools in NGXSAlthough NGXS is also modeled after CQRS, it behaves a bit differently.It provides @ngxs/devtools-plugin for DevTools.It does, however, not support all functionalities.The latest actions can be viewed with their impact and resulting state.But while it’s possible to jump to specific actions, it’s not possible to skip actions or dispatch new ones using the DevTools.Implementing the tools is just as easy as with NGRX, importing the following line to the AppModule:NgxsReduxDevtoolsPluginModule.forRoot()NGXS also provides some options for displaying a maximum age of actions, displaying a name and sanitizing state and actions.DevTools in AkitaAkita is the only solution not powered by a Redux-like pattern.That is why it also has limited functionality in the DevTools.The DevTools are available through @datorama/akita-ngdevtools.Similar to NGXS, the latest actions can be viewed with their impact and the resulting state.And similar to NGXS, it’s possible to jump to specific actions in the timeline, but impossible to skip actions or dispatch new ones using the DevTools.What’s more is that the raw action does not present the actual payload.When adding custom actions, you also have to name them with the @action decorator.Implementing the tools is, as ever, possiblie by importing the following line to the AppModule:AkitaNgDevtools.forRoot()Akita’s DevTools plugin also provides some options for displaying a maximum age of actions, a blacklist and a whitelist.DevTools in plain RxJSSince RxJS itself is no Redux-based storage solution, it obviously does not provide any support for Redux DevTools at all.SchematicsSchematics in NGRXNGRX has quite a lot of schematics available through @ngrx/schematics.It allows to create stores, feature stores, reducers, actions, container components, effects, entity stores all with a lot of options.In my To Do-applications, most of the work was done using two simple commands:ng g @ngrx/schematics:store AppState --module app.module.ts --root --statePath statemanagementng g @ngrx/schematics:entity statemanagement/TodoItem --reducers index.tsThe first command added the StoreModule and StoreDevToolsModule into AppModule and created a reducer in statemanegement/index.ts.The latter command created the following files:  statemanagement/todo-item.actions.ts, with a lot of premade actions for inserting, updating, upserting, removing one or multiple entities.  statemanagement/todo-item.model.ts, with a premade model interface I changed to just export the TodoItem interface which I created for the base application.  statemanagement/todo-item.reducer.ts, and its corresponding spec file handling the generated actions (the spec file did however only test ‘unknown action’) and providing several basic selectors, though I had to modify the following code for the selectors to work:export const selectTodoItemState = createFeatureSelector&lt;State&gt;('todoItem');export const {  selectIds,  selectEntities,  selectAll,  selectTotal,} = adapter.getSelectors(selectTodoItemState);The attempt to create extra actions using the following command was not as easy as it seemed.ng g @ngrx/schematics:action statemanagement/FilterOnly one action was created called LoadFilters, with the type [Filter] Load Filters.It would’ve been easier if one could specify the name of the action some more.I ended up extending the generated Entity store with the filter and sorting properties for which I had to add extra reducers and selectors manually.These properties could have been part of a separate state, but I opted to keep the store as simple as possible.Schematics in NGXSNGXS does not offer any schematics extensions.It does, however, offer a CLI through ngxs-cli.Although the documentation makes mention of the package @ngxs/ngxs-cli, this package was at the time of writing not available.Using the CLI, a state file todo-items.state is created, along with todo-items.actions.ts with 1 example action (add), to add an item to an array.Other than that, everything must be done by yourself, including adding importing the module into AppModule.The CLI offers some options for a name, whether or not to create a spec-file, the path and the name of the folder to create.Schematics in AkitaAkita does offer schematics through akita-schematics.It allows to separately create a store, model, query and service (meant for http), or everything together in what they call a feature.I used the following command to create a feature store:ng g akita-schematics:feature statemanagement/todoItemsThis created the following files:  statemanagement/state/todo-item.query.ts, which is used for selecting items from the state.  statemanagement/state/todo-item.model.ts, with a premade model interface I changed to just export the TodoItem interface which I created for the base application.  statemanagement/state/todo-item.store.ts, which contains the actions that can be performed.Since I didn’t use the --plain option, the state created by that command was an extension of EntityState, and already has some actions available for setting, inserting, updating and removing entities.Similar to NGRX, it was easy to extend the state with a filter and a sort property.Akita also provides a CLI tool, but I didn’t test this.Schematics in plain RxJSSince RxJS is based on operator functions, it’s nearly impossible to have useful schematics for this use case.This means that a developer must write everything by hand.Tooling summary            Tooling      Redux DevTools      Schematics                  NGRX      Yes      Yes              NGXS      Yes, limited      No, but limited CLI              Akita      Yes, limited      Yes, also CLI              Plain RxJS      No      No      2. FeaturesThis is a tough one.I didn’t have use cases in a To Do-application to research every possible feature.But, nevertheless, let’s cover the most useful features and their solutions.Feature: Asynchronous actionsWhat is meant with asynchronous actions, is that an action is dispatched to the store and the store is updated in an asynchronous way.An example of this is the use of a FetchItems action, which performs a request to the backend and dispatches one or multiple different actions when that request completes.This is especially useful when using a realtime database or Google Cloud Firestore, which opens a socket and can emit multiple events.In the example application, I’ve implemented this for a one-time fetch of items where possible.NGRX can handle this with @ngrx/effects, a separate package to be installed.Effects can be added to the root module or to a feature module for lazy loading.They can react on any Observable (not only emitted actions) and must emit a new action.If multiple actions should be emitted, these must be flatMapped.There is also a schematics extension available to generate an Effects class.NGXS allows actions to be handled asynchronously out-of-the-box.These actions can dispatch different actions, but can also modify the state directly.An Observable or Promise must be returned to notify the dispatcher that the action has been completed.Akita does not have support for asynchronous actions.The subscription to an asynchronous stream of data must be handled by yourself.While RxJS is effectively the reason asynchronous actions can exist in Angular, it is quite difficult for novices to update the store from a stream.Feature: Memoized SelectorsNGRX offers support for Selectors as constants.These can be easily chained in other selectors, making them ideal when the store is being refactored.NGXS works similar, but uses functions inside the State class.They can be chained, but it’s not as clear to understand as the NGRX solution.A neat feature within NGXS, is the so-called shared selector, which allows to create a selector that can be used with different states.Akita takes a different approach.A Query class is created, in which functions and constants can be defined.These return Observables, which can be used to obtain a part of the store and can be combined using RxJS operators.Unlike NGRX and NGXS, Akita does not easily offer selecting queries across different states in the store, without creating substates.As ever, RxJS, must throw in the towel for this. When you need something from the store, you’ll need to use some operators to get that specific item.Feature: PersistenceNGRX does not offer any persistence logic itself.There is however a 3rd party package available, ngrx-store-localstorage, which works through a meta reducer.It offers some options, one of which is setting the Storage interface and which keys to sync and how to (de)serialize those items.NGXS does have its official plugin, @ngxs/storage-plugin, a separate module that can be imported into the AppModule.It also has options for setting the Storage interface and which keys to sync and how to (de)serialize those items, but also offers migration strategies.This allows for a version with a radically changed store to not meet with synchronization errors.Akita’s main package includes a persistState() function.Including this function in the main.ts file allows the state to be stored in either localStorage or sessionStorage.Other options include, setting the key by which the state is saved, and including/excluding several aspects of the store and how to (de)serialize those items.When using plain RxJS, you’re on your own again.Other featuresThe frameworks offer even more features.I’m not going into detail for each of them.Most of them are included in the following summary.Features summary            Features      NGRX      NGXS      Akita      Plain RxJS                  Async actions      Yes, through effects      Yes      No      No              (Memoized) selectors      Yes      Yes      Yes, as queries      No              Cross-state selectors      Yes      Yes      No      No              Offline persistence      3rd party package      1st party package      Main package      No              Snapshot selection without first()      No      Yes      Yes      No              Forms synchronization      3rd party packages      1st party package      Main package      No              Router synchronization      1st party package      1st party package      No      No              WebSocket      3rd party package      1st party package      No      No              Angular ErrorHandler      No      Yes      No      No              Meta Reducers      Yes      Yes      No      No              Lazy loading      Yes      Yes      Yes      No              Cancellation      No      Yes      No      No              Side effects      Yes      Yes      No      No              Web workers      No      No      Yes      No              Transactions      No      No      Yes      No      3. Boilerplate codeIn this round the boilerplate code is evaluated.This is code that is needed for each part of the state, but differs only a little per state.I opted not to use immer for immutable state changes to give each competitor the same chances.Also within this section, there is the amount of files needed or generated for the To Do-application.Starting with NGRX, which generated 9 files through schematics.These files include the reducer file.Even though I created an Entity store, to ease the use of an entity collection, the reducer file still contained a lot of code through the adapter.A lot of which were the reducer cases for all the adapter’s actions, like set, insert, upsert and delete one or many items.Even though these cases only call the adapter’s functions, most of these methods won’t change and it would be nicer if these could have been part of the @ngrx/entity package, like the generated selectors.The same argument holds for the actions created in todo-item.actions.tsNGXS fairs a little better in this aspect.It generated only 3 files, though each action had to be written out myself and I created extra files for other actions.And even though the action functions can refer to Generic functions, it’s a shame an Entity State with specialized functions is not included in the package.Akita generated just 4 files using the schematics.Because I used an EntityState, a lot of Query functions and Actions were readily available, without them taking extra space.With RxJS I managed to create an operator function for each ‘action’ very simply.The application is quite simple, but the method is scalable enough without much overhead.Summary            Boilerplate      Files generated      Total files (*)      Boilerplate code                  NGRX      9      12      Heavy              NGXS      3      7      Medium              Akita      4      6      Low              Plain RxJS      0      6      Medium      4. CommunityBased on Google Trends of the past 12 months, NGRX is obviously the most searched for state manager.The reason is likely that it was the first Redux implementation available for Angular.When looking at the GitHub repositories, NGRX has the most stars (at over 3.5K), followed by NGXS (at 1.4K) and Akita (at around 480).Again this indicates NGRX is the most popular framework.But what about contributors?Looking at the repositories’ insights, it’s clear that the same sequence is followed.NGRX takes the lead, NGXS a solid second and Akita last.There it’s also visible that NGRX is still under very active development, looking at the commits.NGXS meanwhile stagnated and Akita has a steady pace.Community summary            feat.      Google Trends      GitHub stars      Contributors      Commits                  NGRX      1st      1st      1st      1st              NGXS      2nd      2nd      2nd      3rd              Akita      3rd      3rd      3rd      2nd      5. Dependencies and sizeState management does not come out-of-the-box with Angular. There is a need to install extra dependencies.Luckily all these dependencies are available through npm.To make the different implementations as feature-equal as possible, I’ve decided to create entity stores where possible and include dev-tools if available.Furthermore, all implementations were built with and without production mode.For comparison purposes, the base application measured in at 14.6MB without production mode, and a mere 754KB with production mode.NGRX is a heavy hitter.It included multiple dependencies for different features. @ngrx/store is the basis.@ngrx/store-devtools, @ngrx/entity, @ngrx/effects and @ngrx/schematics complement this, although the schematics are dev-only.All this gives the packages a weight of 14.9MB without production mode and 786KB with production mode.NGXS fairs a little better.It only includes @ngxs/store and @ngxs/devtools-plugin.This makes the packages weigh in at 14.8MB without production mode and 778KB with production mode.Akita also has an all-in-one package for the store.@datorama/akita holds all functionality, while @datorama/akita-ngdevtools and akita-schematics provide some development tools.Despite this, Akita overthrows NGRX with 15.4MB without production mode and matches NGXS with 778KB with production mode.The difference between NGXS and Akita in production mode was a mere 24B.RxJS is the clear winner here.It needs no extra dependencies whatsoever as RxJS already is a dependency of Angular, making the packages 14.6MB without production mode and 762KB with production mode.Dependencies and size summary            Size      Non-production (MB)      Production (KB)                  Base      14.6      754              NGRX      14.9      786              NGXS      14.8      778              Akita      15.4      778              Plain RxJS      14.6      762      Final scoreIt’s not easy to just say which solution is the all-time champion.Each of the competitors has its advantages and disadvantages.These are the podium places for each round:            Round      NGRX      NGXS      Akita      Plain RxJS                  Tooling      1st      3rd      2nd                     Features      2nd      1st      3rd                     Boilerplate code      3rd      2nd      1st      2nd              Community      1st      2nd      3rd                     Dependencies and size      3rd      2nd      2nd      1st      "
      },
    
      "cloud-2018-10-01-how-to-build-a-serverless-application-with-aws-lambda-and-dynamodb-html": {
        "title": "Create a Serverless Application with AWS Lambda and DynamoDB",
        "url": "/cloud/2018/10/01/How-to-build-a-Serverless-Application-with-AWS-Lambda-and-DynamoDB.html",
        "image": "/img/2018-10-01-How-to-Build-a-Serverless-Application/AWS-Lambda-and-DynamoDB.png",
        "date": "01 Oct 2018",
        "category": "post, blog post, blog",
        "content": "Table of content  Introduction  Serverless: What &amp; Why  What we will build  Prerequisites  DynamoDB  Lambda: scan DynamoDB  API Gateway: Access the scan Lambda  Lambda: Write to DynamoDB  API Gateway: Access the write Lambda  What is next?  Extra resourcesIntroductionReady to create a serverless application?New to AWS and curious to learn more?Read on and learn more about the AWS services by building a serverless app!  Serverless: What &amp; Why  A serverless architecture is a way to build and run your applications without having to think about infrastructure.You no longer have to maintain servers to run your applications, databases and storage systems.And most of all, it is so easy!  Yes, once you get the hang of it, it really is mind-blowingly easy.However, first you need to know the basic infrastructure to set up a serverless application.Let’s do this!What we will buildJoin me in building a serverless application in which users can give great coding tips to each other. To keep it as simple as possible we will build everything through the AWS Console and focus on the infrastructure.No need to deploy any code from your computer to AWS.DemoI could show you a frontend that uses our serverless backend to give and get coding tips.But that would be an extra layer between you and our serverless application.Here, I am triggering the app with Curl.Post a new Coding Tip to the database:curl -X POST \\    https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips \\    -H 'Content-Type: application/json' \\    -d '{    \"author\": \"Nick\",    \"tip\": \"Learn by doing\",    \"category\": \"General\"  }'A new item was added to the CodingTips database.I added a few already and can retrieve them too.View all the Coding Tips that are currently in the database:curl -X GET https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips  Architecture  The coding tip items are stored in a NoSQL database AWS DynamoDB.There are two Lambda Function in play.One to GET the coding tip items from the database and one to POST a new coding tip item to the database.The user can access these Lambda Functions through an API provided by the AWS API Gateway service.This Gateway will redirect to the right Lambda Function based on the HTTP method (POST or GET).Both Lambda Functions are connected to CloudWatch where you can view the logs of your functions.AWS IAM is used to give the services the right permissions to connect to each other.PrerequisitesTo follow along you need:  an AWS account.If you do not have one already, you can create one by following these steps from the official guidelines:Create an AWS account.You have to provide a credit card number to create an account.Don’t worry! The AWS-Free-Tier provides plenty of resources that widely exceed what you will use for this tutorial.If you ask me, AWS is really offering a fantastic amount of stuff for free.You should be grateful for this, it will give you plenty of time to get to know the AWS Services.  coding enthusiasmDynamoDB  Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.– from AWS docs: https://docs.aws.amazon.com/amazondynamodbCreate a database to store your items.Login to the AWS Console and under Services go to DynamoDB.Click on Create table.Name the table CodingTips. As primary key make a Partition key  author, type String’.Check the Add sort key checkbox and choose date, type Number as a sort key for your table.Leave the default settings checked and hit Create.  Notice the Amazon Resource Name ARN property.We will use this later to point to this DynamoDB table.You just created the DynamoDB table that the application will use.Awesome!Add elements to CodingTips tableManually add some elements to the CodingTips table.Go to the CodingTips table, open the Items tab and click Create item.Add a couple of random items to the table as shown in the image below.Notice that date is in milliseconds.These are the milliseconds that have past since the Unix Epoch 1970-01-01.For example, 1538368878527 equals Mon 1 October 2018 06:41:18.Hit Save to store the item in the database.  I added a couple of items as you see in the image below.  Notice that I did not add a coding tip yet.We will do this later by using a Lambda Function!Lambda: Scan DynamoDB  AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.– from AWS docs: https://docs.aws.amazon.com/lambdaLet’s build a Lambda that scans the DynamoDB table for our items.In the AWS Console under Services navigate to Lambda.Click the Create Function button to start creating a Lambda.Choose Author from Scratch and start configuring it with the following parameters:  Name: CodingTips_Scan  Runtime: Node.js 8.10  Role: Create a custom role  Selecting Create a custom role will take you to another page to create this new role.The role is used to give the Lambda Function the right permissions.Configure the role as shown in the image below.If everything went well you should only have to adapt the name of the role.Name it lambda_dynamodb_codingtips.The rest will be automatically generated for you.  Click Allow.Hit Create function to create the Lambda.This will open the designer view of your Lambda Function.  One thing is missing here.The Lambda Function has the authority to send its logs to CloudWatch.This authority is given by the role we just gave it.However, it is mentioned nowhere that it has the right to access the CodingTips table.We should arrange this too.Configuring the Role for the Lambda Function  AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.– from AWS docs: https://docs.aws.amazon.com/IAMUnder AWS Services navigate to IAM (Identity and Access Management).Under roles find the lambda_dynamodb_codingtips role and click it.It has one policy (for the CloudWatch logs) attached to it already.Click Add inline policy and go to the JSON tab. In the JSON tab add the following JSON to configure this new policy.Add the arn that points to your own CodingTips table!You can find this in the Overview tab of your table which we showed above.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": \"dynamodb:*\",            \"Resource\": \"arn:aws:dynamodb:eu-west-1:389795768041:table/CodingTips\"        }    ]}Click Review policy and name it Lambda-DynamoDB-CodingTips-Access.Hit Create policy.You now attached a new policy to the existing lambda_dynamodb_codingtips role.The role summary looks like this:  Go back to the designer view of the CodingTips_Scan Lambda.Now you see that the Lambda Function has the right to connect to the DynamoDB table.  Function CodeYeah, finally it is time for some code!In the configuration window of the lambda add the code in the Function code block.‘index.js has to contain the following code:console.log('function starts');const AWS = require('aws-sdk');const docClient = new AWS.DynamoDB.DocumentClient({region: 'eu-west-1'});exports.handler = function(event, context, callback){    console.log('processing event: %j', event);    let scanningParameters = {        TableName: 'CodingTips',        Limit: 100 //maximum result of 100 items    };    //In dynamoDB scan looks through your entire table and fetches all data    docClient.scan(scanningParameters, function(err,data){        if(err){            callback(err, null);        }else{            callback(null,data);        }    });}Save the Lambda Function to persist the changes.  The handler function is the function where the Lambda execution starts when the Lambda is triggered.  The event parameter contains the data from the event that triggered the function.  The scanningParameters are used to configure the scan of the table.  This function scans the DynamoDB table for the first 100 items it finds.  docClient.scan(scanningParameters, function(err,data) executes the scan and returns either the result or the error that occurred.Test write-LambdaAll right! Let’s test this thing..On the Lambda Function configuration page you see a dropdown and test button in the upper right corner.Click the dropdown and configure a new test event.I called mine Test and added an empty test event {}.  Save it and you are ready to test the Lambda.From the dropdown select your test event and hit the Test button!Nice one, this returns the items in your table:  API Gateway: Access the scan LambdaMmmh, fine.. We can trigger the Lambda Function with a test event.But we want to be able to trigger it from anywhere using a URL.In the designer view of the lambda you can still see add triggers from the left.Well, let’s add that trigger!To expose a Lambda Function AWS provides the API Gateway.Under Services navigate to API Gateway.  Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a “front door” for applications – from AWS docs: https://aws.amazon.com/api-gatewayBasically this is the Service you use to create all of your API’s.  Click Create API and name your api CodingTips  Add a description if you like  Leave the Endpoint Type to regional and Create API  The API has been created.Configure it by adding a resource.  Under Actions click Create Resource and name it tips with /tips as Resource Path  Check Enable CORS to make your API accessible from anywhere  Hit Create ResourceTime to configure the HTTP GET request.  Select the /tips endpoint  Under Actions select Create Method and select GET.  Integration type is Lambda Function  As Lambda Function provide the name of the lambda.In this case that is Codingtips_Scan.  Save the configuration.  Only one thing left: select the API and under Actions click Deploy API. You will be asked to provide a name for the stage. Name it default.In the Stages tab click the GET method and copy the Invoke URL.  This is your gateway to trigger the lambda.Since we just created a HTTP GET request you can use either your browser, Curl or Postman to do this.In a browser tab past the Invoke URL.  From the command line with Curl execute this command with your own Invoke URL:curl -X GET https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tipsEither of the above actions will return the items in the CodingTips table!Congratulations, you just created your first serverless app!Did you know:  performing a scan on a DynamoDB table will return the items in a random order  you just joined the club of serverless application developers  you should be proud of yourselfCommon errors  Missing Authentication Token:          Check the URL you are trying to invoke.  Does it have the format ‘https://{domain}/{stage}/{method}’.  Stage and method were set when creating the API Gateway.      Enable CORS for your API Gateway      Made changes to the API Gateway? Make sure to redeploy the API.        Lambda Exceptions:          Check CloudWatch for logs.  Under Services go to CloudWatch.  In the Logs tab access the Log Group /aws/lambda/CodingTips_Scan to view the logs of the Lambda.        Trouble with API Gateway:          Enable API Gateway logging      Lambda: Write to DynamoDBUsers should be able to POST new items to the table.This is possible when we create a Lambda Function to write to the database.Create write-LambdaIn the AWS Console under Services navigate to Lambda.Click the Create Function button to start creating a Lambda.Choose Author from Scratch and start configuring it with the following parameters:  Name: CodingTips_Write  Runtime: Node.js 8.10  Role: Choose an existing role  Existing role: lambda_dynamodb_codingtipsCreate the function.The CodingTips_Write Lambda Function already has access to CloudWatch and DynamoDB.This is because we gave it the existing lambda_dynamodb_codingtips role that has policies which allow these access.The designer view of the Lambda Function now looks like this:  Function CodeLet’s add the code of this function!In the configuration window of the lambda add the code in the Function code block.Enter the following code in the index.js file:console.log('function starts')const AWS = require('aws-sdk')const docClient = new AWS.DynamoDB.DocumentClient({region: 'eu-west-1'})exports.handler = function(event, context, callback){    console.log('processing event: ' + JSON.stringify(event, null, 2))    let currentMonth = new Date().getMonth() + 1     let currentYear = new Date().getFullYear()    let params =  {        Item: {            Date: Date.now(),            Author: event.author ? event.author : \"Anonymous\",            Tip: event.tip,            Category: event.category,            MonthAttribute: currentMonth,            YearAttribute: currentYear,            YearMonthAttribute: currentYear + \"-\" + currentMonth        },        TableName: 'CodingTips'    };    docClient.put(params, function(err,data){        if(err) {            callback(err, null)        }else{            callback(null, data)        }    });}Save the Lambda Function to persist the changes.What happens in this Lambda Function:  The event parameter of the handler function contains the data from the event that triggered the function.  The params are used to configure the scan of the table.  The Item object contains the data that has to be put into the table.  The Item not only contains the Date and Author, but also other attributes like the Tip itself and Category..That’s allowed because it is a NoSQL database. The MonthAtrribute, YearAttribute and YearMonthAttribute are added automatically.  docClient.put(params, function(err,data) executes the write and returns either the result or the error that occurred.Test write-LambdaConfigure a new test event called test and add the following JSON attributes:    {      \"author\": \"Nick\",      \"tip\": \"Don't hesitate to ask for help when you need it\",      \"category\": \"General\"    }Save it and test the lambda by hitting the test button.Execution result: succeeded? Go to the CodingTips table and you will see a new item that was added into your table.  API Gateway: Access the write LambdaAgain we need to expose our Lambda Function via an API Gateway so that users can post messages to it.  Under Services navigate to API Gateway.  Click the CodingTips API that we created already for the GET Request.  You need to add a HTTP POST Method to this API.Under Resources click /tips, Actions, Create Method and select POST.The Integration Type is Lambda Function.The name of that Lambda function is CodingTips_Write, which we just created.    Hit Save to create the new method.  When AWS asks you, add the permission to the Lambda FunctionWe want to pass a JSON object to this API.The API in turn has to pass on the JSON to the Lambda.To enable this, click on Integration Request and under Mapping Templates check When there are no templates defined (recommended)  Add mapping template with Content-Type application/json.Add this template and save:{  \"author\": $input.json('$.author'),  \"tip\": $input.json('$.tip'),  \"category\": $input.json('$.category')}  Under Actions click Deploy API. You will be asked to provide a name for the stage. Select the default stage and Deploy.In the Stages tab there is an Invoke URL.  This is your gateway to trigger the lambda.Since we just created a HTTP POST request you can use either Curl or Postman to do this.From the command line with Curl execute this command with your own Invoke URL:curl -X POST \\    https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips \\    -H 'Content-Type: application/json' \\    -d '{    \"author\": \"Nick\",    \"tip\": \"Learn by doing\",    \"category\": \"General\"  }'You just added a tip to the CodingTips table!This can be checked by invoking the GET method of the API Gateway we designed in the beginning of this article.Use your browser or curl to check the items in the table.curl -X GET https://k5p4u1y2we.execute-api.eu-west-1.amazonaws.com/default/tips  Common errors  Missing Authentication Token:          Check whether the Mapping Template under the Integration Request of your API Gateway is correct      Check the URL you are trying to invoke.      Made changes to the API Gateway? Make sure to redeploy the API.        Lambda Exceptions:          Check CloudWatch for logs.  Under Services go to CloudWatch.  In the Logs tab access the Log Group /aws/lambda/CodingTips_Write to view the logs of the Lambda.        Trouble with API Gateway:          Enable API Gateway logging      What is nextSome suggestions to keep you busy:  Query DynamoDB instead of scanning  Create GSI (Global Secondary Index) to query and sort  Create a frontend that uses this serverless infrastructure as backend  Deploy this infrastructure with AWS Cloudformation  Deploy using a Jenkins pipeline  Run locally with SAM LocalExtra resources  AWS Lambda: AWS Lambda Introduction  AWS DynamoDB: AWS DynamoDB Introduction  AWS API Gateway: AWS API Gateway  AWS IAM: AWS IAM Introduction  AWS CloudWatch: Getting started with AWS CloudWatch  Using the DynamoDB docClient: AWS DocClient Example"
      },
    
      "ionic-2018-09-30-e2e-testing-ionic-protractor-appium-e2e-testing-html": {
        "title": "Automated E2E (End-to-End) testing on Android and iOS with Ionic, Protractor and Appium.",
        "url": "/ionic/2018/09/30/e2e-testing-ionic-protractor-appium-e2e-testing.html",
        "image": "/img/ionic-protractor-appium.jpg",
        "date": "30 Sep 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Automated testing  Appium  Getting started with Appium  Configuring Protractor and the Ionic project  Running Tests  Conclusion  Example repositoryIntroductionMany articles related to E2E testing Cordova/Ionic applications are about applications which only run in the browser.But what if your application run’s on native mobile devices?This article will cover how to get started with E2E testing your Ionic application on native Android and iOS devices.To follow along, I recommend having a basic understanding of Javascript, Typescript, Jasmine and automated testing in general.Automated testingWhen releasing an application, we need to make sure it was thoroughly tested.Making sure of discovering any bugs before reaching production.These tests or test scenarios can be done manually, but this would consume a lot of time and resources.The more cost-effective solution would be to automatically run these test scenario’s entirely by a programmable agent.Thanks to a few technologies, we can script a bot that can perform most user interface interactions, such as clicking on a button, performing touch gestures (f.e. swiping), etc.The most popular solution for automated E2E tests is called Selenium which is based on the WebDriver protocol.While Selenium is a great solution for browsers, there is a better solution for native mobile apps called Appium.AppiumAppium is a tool for automating mobile applications and writing cross-platform UI tests. It is very similar to Selenium. The difference is that Selenium is a tool for automating browsers and web applications, whereas Appium is a tool for automating Native / Hybrid mobile applications.Appium allows developers to write UI tests for mobile applications in any number of programming language (Javascript, Java, .NET, etc.), as it uses a superset of the Selenium WebDriver specification, called the MJSONWP protocol.Cross-platform UI testingBecause we are using Ionic with Cordova, we can write our codebase using only web technologies but still build, deploy and run on multiple platforms.Our mobile application can be packaged and deployed as a native application for both iOS and Android.We can achieve the same cost-savings strategy “Write once, run anywhere” for our UI tests using Appium.To automate our UI tests, there needs to be an agent that programmatically drives the UI of your mobile application.For each platform there are different agents:  iOS: XCUITest  Android: UIAutomator, Selendroid, EspressoControlling these agents requires a developer to write platform-specific code.Appium with MJSONWP (Webdriver spec) provides an abstraction layer to drive these agents programmatically in a platform agnostic way.We will explain how to set up your Appium server and run automated UI tests in your Ionic application on Android and iOS mobile devices.Getting started with AppiumEnvironmentThe first step is to setup your environment.Because we are targeting Android and iOS we will only describe the setup for macOS, but it shouldn’t be too different compared to other platforms once you have followed the official ionic resources guide below, they have guides for all platforms.Again: On the developer resources page of the official ionic documentation, you wil find guides on how to setup your machine depending on the OS you are working on.Next:  Install appium-doctor using npm.  Run appium-doctor –ios and fix any issues  Run appium-doctor –android and fix any issuesAppium serverThere are multiple ways to start an Appium server:  Appium Desktop  webdriver-manager  npm install -g appium &amp;&amp; npm run appiumAppium desktopAppium Desktop is a graphical user interface for running an Appium server and starting sessions to inspect your applications.Note: For macOS make sure to drop the downloaded package in the /Applications folder.Otherwise you will encounter write permission issues.Appium desktop has two advantages:  It comes with an inspector to show and inspect all elements of your application  Record user actionsThe drawback is inspecting and recording user actions only supports the Native context.You cannot record actions for the Webview context.Cordova applications always run in the webview context.https://appium.io/docs/en/writing-running-appium/web/hybrid/index.htmlwebdriver-managerSelenium and appium server managerWebdriver-manager is officially supported by Angular and works well together with Protractor,the official E2E testing framework for Angular applications.Ionic up until version 3.x is built on top of Angular,from version 4 and on Ionic has decoupled from the Angular framework and recreated all of their components using StencilJS.NPMWe will be using this package to start up our Appium server.LanguageDecide in which language you want to write your tests in.You need to have a client library that can send MJSONWP / JSONWP HTTP Requests to the Appium server.For our application, we will write our tests in Typescript using Protractor since it has added support for Angular and type safety.Other webdriver javascript clients:  http://webdriver.io/guide/getstarted/modes.html  https://github.com/admc/wdClient libraries for different languagesProject setupWe are going to use Ionic 4 and the super template as our application to run our tests against.First, make sure your development machine has been set up correctly.On the developer resources page of the official ionic documentation,you will find guides on how to set up your machine depending on the OS you are working on.Once your machine is set up, install the Ionic CLI.npm i -g ionicNext, generate the Ionic Cordova application using the Ionic CLI.ionic start superApp super --type=ionic-angular --cordovaionic cordova platform add androidionic cordova platform add iosTest if you can build the application by entering the following commandsAndroidionic cordova build androidiOSNote: You will have to open your ios project in xcode first to configure your code signing identity and provision profile.ionic cordova build iosIf you were able to run these commands successfully, we can start E2E testing our application on both iOS and Android.Before continuing, make a folder /e2e in the root of your project.Configure the E2E testing tools in your Ionic projectAppium  Install Appium as a local dependency  Add the correct chrome driver  Create an NPM task in your package.json  Boot up the appium service1. Install Appium as a local dependencyJust run the following command to add Appium as a local dependency, this will allow us to work with Appium using NPM scripts.npm i -D appium2. Add the correct chrome driverTo be able to run your tests on Android devices, you need to match the correct chrome driver with the Chrome version running on the Android test devices.Here is an overview of all the chrome drivers and their respective Chrome versions.To download a chrome driver, go to the Chrome Driver Downloads page.Once you have selected your chrome driver, download it and put in the /e2e folder.3. Create an NPM task in your package.jsonBefore running Appium, you can provide the downloaded chrome driver as a cli argument:\"appium\": \"appium --chromedriver-executable e2e/chromedriver\"4. Start your Appium serverNow you should have everything configured correctly to start your Appium server.Simply run:npm run appiumProtractorProtractor will be our test runner and testing framework.Visit their website for more information on Protractor.  Install protractor as a local NPM dependency  Configure typescript configs  Create your protractor config  Create NPM script for running your e2e tests1. Install Protractor as a local NPM dependencyInstall the test runner with the following command:npm install -D protractor1. Configure TypescriptWe require a few extra tools to be able run and write our tests in Typescript.npm install -D ts-node @types/jasmine @types/nodeNext, in your /e2e folder, create a tsconfig.json file with the following configuration:{  \"compilerOptions\": {    \"sourceMap\": true,    \"declaration\": false,    \"moduleResolution\": \"node\",    \"emitDecoratorMetadata\": true,    \"experimentalDecorators\": true,    \"lib\": [      \"es2016\",      \"esnext.asynciterable\"    ],    \"outDir\": \".\",    \"module\": \"commonjs\",    \"target\": \"es5\",    \"types\": [      \"jasmine\",      \"node\"    ]  }}This will be our tsconfig for our e2e test scripts.It’s also a good idea to write your configuration files in Typescript.For our protractor configuration, we will use a different typescript configuration file.In your /e2e folder, create a file called /e2e/protractor.tsconfig.jsonThis configuration file will extend the one we created earlier, we want to overwrite the include and exclude parameters to make sure it only matchesand transpiles the protractor.config.ts file.{  \"extends\": \"./tsconfig.json\",  \"include\": [    \"**/*.config.ts\"  ],  \"exclude\": [    \"./test\"  ]}3. Configure protractorNow one of the more exciting parts, configuring Protractor!Create a file called /e2e/protractor.config.ts with the following contents:import {Config} from 'protractor';import * as tsNode from 'ts-node';const serverAddress = 'http://localhost:4723/wd/hub';const testFilePAtterns: Array&lt;string&gt; = [  '**/*/*.e2e-spec.ts'];const iPhoneXCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  app: '/Users/${user}/ordina/e2e/superApp/platforms/ios/build/emulator/superApp.app',  version: '11.4',  platform: 'iOS',  deviceName: 'iPhone X',  platformName: 'iOS',  name: 'My First Mobile Test',  automationName: 'XCUITest',  nativeWebTap: 'true'};const androidPixel2XLCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  platformName: 'Android',  deviceName: 'pixel2xl',  app: '/Users/${user}/ordina/e2e/superApp/platforms/android/build/outputs/apk/android-debug.apk',  'app-package': 'be.ryan.superApp',  'app-activity': 'MainActivity',  autoAcceptAlerts: 'true',  autoGrantPermissions: 'true',  newCommandTimeout: 300000};export let config: Config = {  allScriptsTimeout: 11000,  specs: testFilePAtterns,  baseUrl: '',  multiCapabilities: [    androidPixel2XLCapability,    iPhoneXCapability  ],  framework: 'jasmine',  jasmineNodeOpts: {    showColors: true,    defaultTimeoutInterval: 30000  },  seleniumAddress: serverAddress,  onPrepare: () =&gt; {    tsNode.register({      project: 'e2e/tsconfig.json'    });  }};To get an idea of all the configuration parameters and their description, visitThe official Protractor Github repoI will go over the points that took me the most effort to configure correctly.CapabilitiesRefers to the capabilities of a single E2E session, it describes which features a particular session should have, for example:  Platform (Android / iOS / …)  Device Name  Automation framework  Location of the Application build (.apk, .ipa)  etc.If you want to spin up multiple E2E testing settings, you need to configure the multiCapabilities property.Android Capability  Run ionic cordova build android and configure the output path in the app property  app-package should match the package name in your config.xml  app-activity is always MainActivity by default unless you have changed this in your config.xmlconst androidPixel2XLCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  platformName: 'Android',  deviceName: 'pixel2xl',  app: '/Users/${user}/ordina/e2e/superApp/platforms/android/build/outputs/apk/android-debug.apk',  'app-package': 'be.ryan.superApp',  'app-activity': 'MainActivity',  autoAcceptAlerts: 'true',  autoGrantPermissions: 'true',  newCommandTimeout: 300000};iOS Capability  Run ionic cordova build ios and configure the output path in the app property  Point to the .app file and not the .ipa if you are using simulators.  Set automationName to XCUITest instead of the deprecated UIAutomator  browserName is a mandatory parameter, but since we’re targeting Native apps, we can leave this as an empty stringconst iPhoneXCapability = {  browserName: '',  autoWebview: true,  autoWebviewTimeout: 20000,  app: '/Users/${user}/ordina/e2e/superApp/platforms/ios/build/emulator/superApp.app',  version: '11.4',  platform: 'iOS',  deviceName: 'iPhone X',  platformName: 'iOS',  name: 'My First Mobile Test',  automationName: 'XCUITest',  nativeWebTap: 'true'};4. Create an NPM script for running e2e testsIn your package.json, add the following task:\"e2e\": \"tsc --p e2e/pro.tsconfig.json &amp;&amp; protractor e2e/protractor.config.js --verbose\"Running and Writing UI testsFor Protractor to know which tests to run, you need to configure the specs property, in our case all the files that end with .e2e-spec.tsconst testFilePAtterns: Array&lt;string&gt; = [  '**/*/*.e2e-spec.ts'];export let config: Config = {  ...  specs: testFilePAtterns  ...};If you followed along, you should be able to run your tests by entering the following command:npm run e2eWriting protractor tests is out of scope in this post, but here is an example test script that you should be able to run on both iOS and Android.import {browser, by, element, ElementFinder, protractor} from 'protractor';describe('App', () =&gt; {  describe('Tutorial Screen', () =&gt; {    it('should skip to the welcome screen and have the correct button labels', async () =&gt; {      const skipButton: ElementFinder = element(by.id('skip'));      await browser.wait(protractor.ExpectedConditions.elementToBeClickable(skipButton));      const skipButtonLabel: string = await skipButton.getText();      expect(skipButtonLabel).toEqual('SKIP');      skipButton.click();      const loginBtn: ElementFinder = await element(by.id('btn-login'));      await browser.wait(protractor.ExpectedConditions.elementToBeClickable(loginBtn));      const loginBtnLabel: string = await loginBtn.getText();      expect(loginBtnLabel).toEqual('SIGN IN');      loginBtn.click();    });  });});There are seven basic steps in creating an Appium test script.  Set the location of the application to test in the desired capabilities of the test script.  Create an Appium driver instance which points to a running Appium server  Locate an element within the mobile application.  Perform an action on the element.  Anticipate the application response to the action.  Run tests and record test results using a test framework.  Conclude the test.Webview And Native contextOur example application is a hybrid application.Meaning it will be packaged and deployed as native app so we can access Native API’s.But it will acually run inside a webview. By using Cordova our webview can communicate with Native API’s (f.e. Camera).When the Camera is launched, we enter a Native Context, if we exit the Camera and go back to our Hybrid application we return to the Webview Context.Appium helps us to easily switch between these contexts since locating and interacting with UI elements are very different in both contexts.For example, there are no DOM elements in the Native Context.To locate a native UI element you need to use an Accessibility ID. At the same time, AccessibilityID’s are not available in a Webview context.TouchEvents like Tap / Swipe / Drag ‘n Drop are only supported in the Native context.You can not use them in the Webview Context.Behaviour-driven development with CucumberCucumber is a tool for BDD.You can easily integrate Cucumber with Appium using Protractor cucumber framework on NPM.A typical workflow looks like this:Describe an app feature and corresponding scenarios in a .feature file. The contents are written in GherkinFeature: As an employee, I want to access the application@AuthenticationScenario: Authenticate with AzureADGiven I am on the Login pageWhen I click on \"Login\"When I provide my credentialsWhen I click on the \"Submit\" buttonThen I should see the Dashboard page@Authentication FailedScenario: Authenticate with AzureAD failsGiven I am on the Login pageWhen I click on \"Login\"When I provide incorrect credentialsWhen I click on the \"Submit\" buttonThen I should see the Login failed pageDevelopers write an implementation for the feature in a step definitions file:Given(/^I am on the Login page$/, () =&gt; {    expect(app.getTitle()).to.equal('Login');});When(/^I click on \"Login\", () =&gt; {    LoginPage.loginButton.click();});When(/^I provide my credentials$/, () =&gt; {    WindowsAuthPage.enterCredentials(usr,pass);});When(/^I click on the \"Submit\" button$/, () =&gt; {    WindowsAuthPage.submitButton.click();});Then(/^I should see the Dashboard page$/, () =&gt; {    expect(app.getTitle()).to.equal('Dashboard');});Let protractor and Appium run the step definitions in an Automated way.The advantage of using cucumber is that non-developers can easily write their own .feature files in plain English (Gherkin).This offers:  Better collaboration between Business &amp; Developers  Feature files can act as contracts for acceptance criteria  Better reporting and readability of the UI testsCloud testing providersThe following providers offer great support for Appium tests in the cloud:  Saucelabs  TestObject, only real devices, has been purchased by Saucelabs.  Kobiton, only real devices, allows you to connect your local mobile device farm.  ExperitestConclusionWhile investigating and hands-on experiencing Appium, I noticed the following trade-offs:  Tests can be flaky (Simply rerunning a failing test can make it succeed)  Tests on iOS take a while to run  Appium is slower than for example running tests directly with Espresso or XCUITest  Documentation can be outdated and is scattered around the web  Setting up an environment for iOS and Android takes a lot of time initially  UI tests can differ for each platform  Sending key events can be very slow, which make the tests run very slow  You need a good knowledge of WebDriver API’s to write good tests  Debugging is hard, I mostly relied on console.log statements.  Testing on Android needs to happen with Chrome Browser version 54+.For Android we are limited to recent Android API’s and devices with Chrome browser version 54+, this means we can not test older devices or devices with older Android versions.Setting up a local Appium server also takes a lot of setup and configuration, but this can be circumvented if you decide to go for a cloud testing provider like Saucelabs.Still, I believe Appium offers a lot of value because  We can write UI tests both for Android and iOS using a single programming language  You can automate manual testing for multiple platforms  There are quality cloud testing providers out there to help you with all your testing needsAnd once you have everything set up, it works quite well.Example repositoryhttps://github.com/ryandegruyter/ordina-ionic-appium-protractor"
      },
    
      "spring-2018-09-28-springone-fun-with-the-functional-web-framework-html": {
        "title": "SpringOne Platform - Fun With The Functional Web Framework",
        "url": "/spring/2018/09/28/SpringOne-fun-with-the-functional-web-framework.html",
        "image": "/img/2018-09-27-SpringOne-Platform/post-image.jpg",
        "date": "28 Sep 2018",
        "category": "post, blog post, blog",
        "content": "Fun with the Functional Web Frameworkby Arjen PoutsmaThis talk is a follow-up of a talk that Arjen Poutsma has been giving a few years now, called ‘New in Spring 5: Functional Web Framework’. In his new talk he goes more in depth in some of the features that are offered by the framework.What is it?The Spring functional web framework (called WebFlux.fn) is an alternative to the annotational style web framework, Web MVC.It was introduced in Spring 5.0 and for spring 5.1 they did some refinements in the API after feedback from developers.Design goalsThe WebFlux.fn framework had three main goals.The first one was to create a web framework with a functional style.By this they mean that they wanted to leverage the new functional concepts introduced in Java 8, like Function and Stream.The second goal was to make the framework fully reactive by using the functionality from Reactor.The third goal was to act more like a library and less like a framework.The reason for this is that many people don’t like the “automagic” things the Web MVC (annotational style) framework does.Web MVC does a lot of things behind the scenes that you as a client of the framework don’t know about, unless you read up on how the framework works internally.So acting more like a library instead of a framework means that a lot of things will be more explicit, so you as a client of the library will see more clearly what is going to happen.A fourth goal, that was more a side effect than intention, is that there is no more reflection in WebFlux.fn.By not using annotations anymore to map HTTP requests to controller methods, there is no more reflection.This has the great effect that your application will take less time to start up because Spring has to do less classpath scanning.This is also useful for when you want to use GraalVM.How does it workThere are three main concepts in the WebFlux.fn framework:  The HandlerFunction  The RouterFunction  The HandlerFilterFunctionWe’ll discuss these in the following sections.The HandlerFunctionIs a function that maps a ServerRequest to a Mono&lt;ServerResponse&gt;.public Mono&lt;ServerResponse&gt; showPet(ServerRequest request) {    String id = request.pathVariable(\"id\");    return this.petRepository.findById(id)            .flatMap(pet -&gt; ServerResponse.ok().contentType(APPLICATION_JSON).body(fromObject(                    pet)))            .switchIfEmpty(Mono.defer(() -&gt; ServerResponse.notFound().build()));}You can see in this example that there are some differences with a Web MVC controller method.A big difference is that we can only get a ServerRequest as a parameter. So if we want a path variable, body or anything else from the HTTP request, we have to get it from the ServerRequest variable.Spring does not inject this information as method parameters in WebFlux.fn.The second difference is that the object we return has to be a Mono&lt;ServerResponse&gt;.In Web MVC the return type could be a lot of different things like any type of Object, a ResponseEntity, etc.The RouterFunctionIs a function that takes a ServerRequest and returns a HandlerFunction using a RequestPredicate.@Beanpublic RouterFunction&lt;ServerResponse&gt; routerFunction(PetHandler petHandler) {    RouterFunction&lt;ServerResponse&gt; html = route()            .GET(\"/pets/{id}\", accept(TEXT_HTML), petHandler::renderPet)            .GET(\"/pets\", accept(TEXT_HTML), petHandler::renderPets)            .build();    RouterFunction&lt;ServerResponse&gt; json = route()            .GET(\"/pets/{id}\", accept(APPLICATION_JSON), petHandler::showPet)            .GET(\"/pets\", accept(APPLICATION_JSON), petHandler::showPets)            .build();    return html.and(json);}The order in which you define these router functions matters.The first router function’s  handler that matches your HTTP request will be the one that is executed.This makes it a lot clearer when you read the router functions to know which one will be executed, it’s the first one that you define and matches.An advantage of the RouterFunction over the annotational style is that you can map multiple endpoints to the same HandlerFunction.This is not possible in Web MVC because you can only put one @RequestMapping on a controller method.In the WebFlux.fn framework however, you can refer to one HandlerFunction in as many RouterFunction matchers as you want.Improvements in the RouterFunction spring framework 5.1:  A router DSL with less static imports:     //5.0 version route(GET(\"/people\"), personHandler::getPeople) //5.1 version route()   .GET(\"/people\"), personHandler::getPeople)        And a new pattern matcher to resolve which HandlerFunction to call, which is a lot faster than the previous one.RequestPredicatesIs a function that maps a ServerRequest to a boolean.This is used to match your HandlerFunction to a HTTP request.Spring provides a lot of default predicates for paths, accept headers, etc.But you can also create your own very easily, with lambdas, methods, or classes.// lambdaroute().GET(\"/people\", serverRequest -&gt; serverRequest.path().endsWith(\".json\"), personHandler::getPeople)// methodroute().GET(\"/people\", this::pathEndsWithJson, personHandler::getPeople)private boolean pathEndsWithJson(ServerRequest request) {    return request.path().endsWith(\".json\");}// classroute().GET(\"/people\", new PathEndsWithJsonPredicate(), personHandler::getPeople)public class PathEndsWithJsonPredicate implements RequestPredicate {    @Override    public boolean test(final ServerRequest request) {        return request.path().endsWith(\".json\");    }}nested RouterFunctionSimilar to the class level @RequestMapping, but a lot more powerful.@Beanpublic RouterFunction&lt;ServerResponse&gt; petsRouter(PetJsonHandler petJsonHandler, PetHtmlHandler petHtmlHandler) {    RouterFunction&lt;ServerResponse&gt; html = route()            .nest(accept(TEXT_HTML), builder -&gt; { builder                .GET(\"/{id}\", petHtmlHandler::renderPet)                .GET(\"\", petHtmlHandler::renderPets);            }).build();    RouterFunction&lt;ServerResponse&gt; json = route()            .nest(accept(APPLICATION_JSON), builder -&gt; { builder                .GET(\"/{id}\", accept(APPLICATION_JSON), petJsonHandler::showPet)                .GET(\"\", accept(APPLICATION_JSON), petJsonHandler::showPets);            }).build();    return route()            .path(\"/pets\", () -&gt; html.and(json))            .build();}You can choose on what you nest, depending on the needs of your software.In WebFlux.fn you can couple HTTP requests for the same path, but different accept headers to different classes, as in the example above.Here you only define your path once which means no duplication, and it’s easier to change the path to for example /animals, because there is only one place where you have to change it.In Web MVC it would look like this.@RestController@RequestMapping(value = \"/pets\", produces = MediaType.APPLICATION_JSON_VALUE)public class PetJsonController {...}@RestController@RequestMapping(value = \"/pets\", produces = MediaType.TEXT_HTML_VALUE)public class PetHtmlController {...}The HandlerFilterFunctionIs a function that takes a ServerRequest and a HandlerFunction and returns a ServerResponse.@BeanRouterFunction&lt;ServerResponse&gt; mainRouter(PetHandler petHandler, OwnerHandler ownerHandler) {    RouterFunction&lt;ServerResponse&gt; petsRouter = petsRouter(petHandler);    RouterFunction&lt;ServerResponse&gt; ownerRouter = ownerRouter(ownerHandler);    return petsRouter.and(ownerRouter)            .filter(this::performanceLogging);}public Mono&lt;ServerResponse&gt; performanceLogging(ServerRequest request, HandlerFunction&lt;ServerResponse&gt; next) {    Instant start = Instant.now();    Mono&lt;ServerResponse&gt; response = next.handle(request);    Duration duration = Duration.between(start, Instant.now());    LOGGER.info(\"Processing request {} took {} ms \", request, duration.toMillis());    return response;}The HandlerFilterFunction is more flexible than Servlet filters because you can put a HandlerFilterFunction on a RouterFunction.This means that you can apply this filter to a subset of your routes instead of on all routes.It can be used for example for security, logging, timing, etc.Future evolutionsCurrently the functional web framework does not work with Servlets but only with Spring’s self made ServerRequest and ServerResponse.They are however looking at creating a functional web framework that works with Servlets and without Reactor.ConclusionThe functional web framework is a lot better  in what properties of the HTTP request you can match on to choose a controller function.  in reducing duplication of your matching logic  in providing a clean way to separate controller logic and routing logic  in explicitness of routing so you can easily see how your HTTP request will be bound to a controller methodIt is a very good alternative to the more common annotational style web framework Web MVC. The advantages mentioned definitely make it worth trying it out for yourself!#References  Code from the talk  Documentation"
      },
    
      "iot-2018-09-28-3d-printing-intro-html": {
        "title": "3D Printing: An introduction",
        "url": "/iot/2018/09/28/3D-Printing-Intro.html",
        "image": "/img/2018-09-28-3D-Printing/banner.jpg",
        "date": "28 Sep 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  What is 3D printing  Types of 3D printing  Getting started with 3D printing  The futureIntroduction3D printing is a term that has been hyped for a long time.It’s a technology that is in essence not that new, but now more than ever is getting better and accessible to everyone.Today we take a dive into the world of 3D printing and what it really has in store for us and the world in the coming years.What is 3D printingBasically 3D printing can be described as: Producing 3D modeled objects by printing them with a 3D printer.It offers a new way to do fast prototyping without the need to create very expensive molds or stencils.It has long been hyped to be the next big thing for:  Everyday use: recreating objects, replacing broken parts  Medical use: create patient specific casts or prostheses that fit better  Weapons: In the news, 3D print a gun which can pass through metal detector, and is not registered with the authorities.Although 3D printing has changed how prototyping works, a lot of misconceptions exist:  3D printing is slow, very slow          Thus not usable to create batches of the same object        Limited available materials (for hobby use)  3D printed parts are strong but not as strong as molded or cast parts (mainly for plastics)Types of 3D printing3D printing is not one technology, there are many different methods how a 3D print can be created.Wherein the method the print is created varies depending on the technology used.Different technologies use different methods and materials, each with their distinct advantages and disadvantages.Some of these methods include (but are not limited to):  Fused deposition modeling (FDM)  Stereolithography (SLA)  Digital Light Processing (DLP)  Masked Stereolithography (MSLA)  Direct Metal Laser Sintering (DMLS)  Selective laser melting (SLM)  Electronic Beam Melting (EBM)Below we will go into detail for some of the more used types:  FDM  SLA, DLP &amp; MSLA  SLS &amp; SLMDifferent printing methodsFDM: Fused Deposition ModelingWith Fused Deposition Modeling the printed model is created by melting a compound (this being mostly PLA/ABS/PETG) and tracing the model layer by layer, each layer thickening the model as the printer deposits more material on the model.In the image above all the main pieces for an FDM printer are visible. The nozzle is moved on the Z-axis, and the build plate is moved on the X/Y-axes by stepper motors which control the movement up to one tenth or even one twentieth of a millimeter.A spool of material is fed into the nozzle which melts it and deposits it onto the model by use of extrusion wheels/stepper motors.FDM printers are widely available both ready to use and as kits that require assembly, these are also the most affordable type of printers on the market.You can always extends your printer and even print replacements for broken components.                                SLA &amp; DLP: Stereolithography &amp; Digital Light ProcessingThese types of printers work by using either a laser or a projected image to cure a UV-reactive resin.With Stereolithography a powerful UV laser traces the object layer by layer, like the FDM printer does, however it does not deposit material itself, rather it cures the resin in the tank at the point where the laser is.Because SLA uses a laser that is moved by mirrors, it can have a very high resolution, the disadvantage is that it is as slow as an FDM printer since each layer needs to be traced.With Digital Light Processing an UV projector is used, it projects an entire layer at once, this results in a lower printable resolution for the object, but yields a significant speed increase since an entire layer is printed at once, there is no need to trace the entire layer.Masked SLA is a cheaper version of the DLP method where a LCD based photomask is used in front of an UV-LED array instead of an UV based projector.The images above show the distinct difference between these two (three) methods.The SLA method has much better quality compared to the DLP or MSLA methods.                                DMLS &amp; SLM: Direct Metal Laser Sintering &amp; Selective Laser MeltingDirect Metal Laser Sintering and Selective Laser Melting are two of the more industrial methods of 3D printing. They are used, as the name suggests, to 3D print object in metal.The main procedure for both methods is similar.A moving arm pushes metal particles across the printing area, after which a powerful laser will trace the current layer of the object being printed.This will fuse the particles together and is also where the biggest difference between the two methods exists.Once the current layer is finished, the arm moves back and forth again, bringing in a new layer of metal particles.With DMLS the particles are sintered together but the metal itself has not melted completely, is hasn’t flowed.When using SLM instead, the metal particles are actually fully molten and they fuse together much more firmly creating extremely though objects!Objects created with these methods are free of internal stresses and defects that are common and hard to prevent with other production methods.The parts can also be printed as a whole rather than needing assembly of different parts, which further reduces the possibility for failures or errors during assembly.This however comes at great cost, literally, since these methods are very expensive and require state of the art equipment!Because of this high cost they are used in industries where the fault tolerance is very low, such as aerospace.Different printing materialsAs you might have noticed in the previous section different printing methods require different printing materials.In the section below we’ll go more into detail about these materials.Polymers/plasticsFDM and DLP printers require polymer based materials.These are plastics or plastic like materials that are easy to print.PLA (PolyLactic Acid):  Biologically degradable  Melting (printing) temperature: 170 ⇔ 230 °C  Can be used to print medical supplies  Many colors available + glow in the dark  Not UV stable  Cannot withstand high temperatures, do not leave it in the sun inside a car!ABS (Acrylonitrile Butadiene Styrene):  Not biologically degradable  Melting (printing) temperature: 220 ⇔ 260 °C  Many colors available + translucent  UV semi stable  Can withstand higher temperatures than PLA!  Harder to print than PLA and requires venting since the fumes are toxic!PETG (PolyEthylene Terephthalate Glycol):  Not biologically degradable  Combines best of PLA and ABS  Melting (printing) temperature: 220 ⇔ 250 °C  Many colors available + translucent  UV semi stable  Can withstand higher temperatures than PLA!  No toxic fumes, odorlessThese three materials are commonly used in FDM printers. The most and safest to use is PLA, which is fully bio degradable (over a long long time), it also is the easiest material to print with.ABS is stronger but not biodegradable and produces toxic fumes when printing, it also requires very precise cooling during printing or the print will warp and the layer will detach, ruining the print.PETG is the best of both worlds, it is stronger than PLA and does not produce toxic fumes.There are a lot more materials available, oftentimes with very specific properties to make it act more like rubber, be more flexible, glow in the dark,…All these different materials require different print settings and some can be quite hard to get right, experimentation is key!Some materials also exist that can be used in a medical context, these are however not printed with the average FDM printer since they need to match very high standards for medical use for both internal and external use.DLP materials:  Liquid polymer that undergoes photopolymerization, meaning it is cured/hardened by exposure to UV light.  Many different types of resin are available          Biodegradable ⇔ Non biodegradable      Flexible ⇔ Sturdy      Polymer ⇔ Ceramic      Low temp ⇔ High temp        There are also resins available that cure with regular “daylight” and do not require special UV lights to cure the printed objectMetalFor the moment metal printing is not for the mere mortal, however larger companies/industries are already using 3D printers capable of printing metal.Not all methods can print all metal materials, some like SLM are limited in the metals it can ‘print’.Most printed metals:  Titanium  Aluminium  Cobalt chrome  Gold  Silver  Copper  Bras (alloy)Aerospace industries nowadays print both external and internal parts of rockets and their engines, these objects are stronger and have less internal defects and stresses.The medical industry can also print patient specific prostheses in titanium allowing for better patient recovery after injuries.Concrete/constructionIn more recent years concrete printing has caught some attention and people are experimenting with it.It has some nice advantages:  Allow cheaper construction of small buildings  Faster construction  Small area mini homes for the developing world -or- after natural disastersThe video below shows a working concept of a small 3D printed ‘home’, it is constructed in place and can be finished in very little time.    FOODFood printing has been a hype for a long time.Culinary experts have been experimenting with the concept, but also companies like NASA for astronauts.Simple items like chocolate and dough can already be 3D printed, based on the FDM method, the edible object is printed layer by layer.There are ‘robots’ that can print more complex items, but these are not actual 3D printers and more automated assembly machines, the boundary between 3D printing and assembly can become a bit blurry.At this point in time it is not yet possible to print entire meals from raw base ingredients.This will no doubt be developed further as time progresses.Getting started with 3D printingGetting started with 3D printing is not easy, it can be very daunting.There are a lot of things you need to take into accountSince it is still relatively expensive, you want to ask yourself these questions:  What type of 3D printing do I want?  What is the best (price/quality) printer available?  Will I use it enough to justify the purchase?  You could also just order a printed model online!Getting things right is hard:  Printer calibration  Model slicing &amp; print settings⇒ 3D printing remains an intensive trial and error process!Only begin if you are willing to invest the necessary time into it.If at first you don’t succeed, try again and again and again and again!Thankfully the online hobby community is very large and generally very helpful.You will need to model your object or download it from a ‘makers’ website.You will need to slice your model with settings for your specific printer, and adjust these settings based on the quality and speed you want.Online resources:  Youtube channels:          Maker’s Muse      3D Printing Nerd      Make Anything        Forums:          3D Hubs      Reddit 3D printing        These YouTube channels and forums are an invaluable resource when getting into 3D printing, they contain loads of information, do’s and don’ts.By watching videos and reading articles you can prevent mistakes gaining insights faster in how 3D printing works and what is possible.  Maker websites:  Thingiverse  Tinkercad  MyMiniFactoryThese websites provide vast libraries of models and some even offer an online modeling tool.Before diving deep into your modeling software check if someone else had the same idea and created the object you want.⇒ good artists copy, great artists steal - PicassoModeling software:  Modeling software:          3DS Max      Maya      Blender        CAD software:          Fusion 360      Autocad      Inventor      These software suites let you create and export models.All of these programs are easy to pickup (except for the CAD software) but are extremely hard to master!There are also many alternatives available, but these are the most common ones.Slicing software:  Ultimaker Cura  Slic3r  Simplify 3DThese software suites let you convert your 3D models into GCODE.GCODE is the ‘language’ that 3D printers use to print an object.It contains a set of instructions for stepper motors, extruders, fans and other peripherals that make up a 3D printer.This code is generated from the 3D model and is layer based hence the name ‘slicer’, since it slices the model into layers and translates this to the GCODE required to print.The slicer software also takes into account overhangs and infill.Overhangs are parts of the object that have no support beneath them, they float, but since material cannot just float in midair supports are generated that are removed (by the user) when the print is finished.Infill is a way to speed up printing, the inside of and object would normally be 100% filled with material, this is slow and uses a lot of print material.By setting the infill percentage a structure is generated inside sealed of parts of the model that contains a lot less material while maintaining most of its strength.By playing with this setting a lot of time can be saved!The futureThe future is now!3D printers are advancing at an extremely rapid pace, new printer models arrive on the market almost weekly.Chinese firms are producing very high quality printers and production quality is going up.The future will bring a lot more:  Better printers  Finer resolution  Lower cost  Lower maintenance  More and more versatile materials  A 3D printer in every home?  Repair stuff yourself ⇔ Throw away stuff that is broken, built to break…ResourcesGetting into 3D printing requires a lot of research, A LOT!Since the technology is advancing so fast I have not listed any specific printers, they are reviewed on youtube and forums where experienced members of the community test them and give their verdict on them.Printing methods &amp; materials:  SLA vs DLP vs MSLA  DMLS vs SLM  Types of FDM polymers/plastics  Types of DLP resinsOnline resources:  Maker’s Muse  3D Printing Nerd  Make Anything  3D Hubs  Reddit 3D printingMaker hubs:  Thingiverse  Tinkercad  MyMiniFactoryModeling software:  3DS Max  Maya  Blender  Fusion 360  Autocad  InventorSlicers:  Ultimaker Cura  Slic3r  Simplify 3D"
      },
    
      "spring-2018-09-27-springone-platform-html": {
        "title": "SpringOne Platform - A birds-eye view",
        "url": "/spring/2018/09/27/SpringOne-Platform.html",
        "image": "/img/2018-09-27-SpringOne-Platform/post-image.jpg",
        "date": "27 Sep 2018",
        "category": "post, blog post, blog",
        "content": "This year we went again to SpringOne Platform to check out the latest changes and what’s to come.In this post I’ll try to give an overview of everything that’s just been released or coming in the future.Because there were A LOT of sessions, I wasn’t able to gather or note down everything.All sessions except for the workshops were recorded and I’ll update this post to get you directed straight to the videos on YouTube when they come online!AnnouncementsI listed some of the major announcements that were made during SpringOne.You can find a link to the corresponding video at the end of each topic.AWS Service broker for PCF, open betaAmazon will bring the AWS Service Broker to PCF, making it easier for you to connect your applications to its service catalog of over 18 services.Watch the talk here.Pivotal Function ServiceNext to PAS (Pivotal Application Service) and PKS (Pivotal Container Service), Pivotal will expand PCF with PFS (Pivotal Function Service) which will be build on top of Knative.Pivotal Greenplum, which is its “Postgres for Petabytes”, will also be coming to PCF in the near future so that the same easy-to-use experience that developers nowadays already have with the platform will be extended to the data scientists and data engineers.Watch the talk here.First-timers-only issuesBecause it can be daunting for people to enter a community they are creating “first-timers-only” issues on the Spring Boot project, making it easier for newcomers to get connected.While working on these issues developers of the Spring Boot project will come alongside you to teach you to contribute back, help you and coach you.This way you will get to become part of the Spring community.Watch the talk here.Pivotal Tracker: Maestro  What happens when your organization scales, and you end up with lots of fast-moving developers?It’s really, really easy to get misaligned.And when you get misaligned, it’s really easy to ship the wrong thing.That’s why Pivotal has built a tool to help teams articulate and align around business outcomes to give organizations the high-level view so they can assure that everyone is aligned and delivering value.Watch the talk here.Pivotal Act  “Pivotal Act is a program that partners with humanitarian organizations and charities to identify, design, and develop practical solutions to pressing challenges around the world.”Together with developers, designers, and engineers from Pivotal Labs, Pivotal is going to apply the same methodology they use in other engagements with clients but tailored to the needs of the humanitarian and social impact sectors instead of just donating technology or funds.This way charitable and nonprofit organisations can use the present-day technologies and put them directly to use through the partnership with Pivotal, while building up the organisation’s technology capabilities so they can continue their work after the engagement ends.If you want to find out more, just visit https://pivotal.io/act.You can watch the announcement here.R2DBCWhen using the reactive model, people were limited to NoSQL databases because those were the only ones that had a driver which supports things like streams and backpressure.During the keynote Oliver Gierke announced the R2DBC project.This project currently consists out of a client, an SPI and a PostgreSQL implementation to bring the reactive capabilities to SQL databases.It also has an adapter to support the OpenJDK incubator project “ADBA”, a non-blocking database access API that Oracle is proposing as a Java standard.RSocketOn stage Stéphane Maldini announced the RSocket project.RSocket, built by Facebook, Netifi and Pivotal, is a binary application protocol that provides reactive streams semantics.This protocol is payload, transport AND language agnostic making it easy to send eg. JSON or protobuf payloads over TCP, UDP, WebSockets or HTTP/2 using Java, Kotlin, C++, JavaScript,…Using this protocol we can eg. easily resume our stream of events where we left off in case the connection got interrupted.To get more in-depth knowledge of the reasoning behind the protocol visit https://rsocket.io/ and look up the document that explains the motivation in detail.Spring Cloud AzureTo quickly integrate with Azure services, Microsoft has already created a couple of starters to easily connect to eg. Azure Active Directory or Azure Key Vault.Not all starters can be found on start.spring.io, to get a complete overview of all their starters and modules visit their GitHub repository.Watch the talk here.Cloud Native BuildpacksCloud Native Buildpacks is a new effort initiated by Pivotal and Heroku which aims to unify the buildpack ecosystems with a platform-to-buildpack contract.They embrace modern container standards, such as the OCI image format and take advantage of the latest capabilities of these standards.This way buildpacks can be used cross-platform.You can watch the announcement here.ReleasesSpring Framework 5.1The highlights of this release are the support for JDK 11, initial refinements for GraalVM compatibility and Reactor &amp; Hibernate got an upgrade to respectively Californium and 5.3.Improved startup times and less heap memory consumption are also some benefits you get when upgrading to this release.Check out this post by Juergen Hoeller to find out more!Spring Boot 2.1.0.M4The fourth milestone of Spring Boot 2.1 got released to incorporate the 5.1 release of the Spring framework and closes over 40 issues and pull requests.Madhura Bhave wrote this post announcing the milestone release and added some useful links to the release notes and updated reference documentation.Make sure to check them out!Spring Batch 4.1.0.RC1Mahmoud Ben Hassine announced yesterday the first release candidate of 4.1.0.This release was mainly focused on running Spring Batch correctly on Java 8, 9, 10 and 11.Their plan is to release this version by the end of October so it can be shipped with Spring Boot 2.1.Spring Data LovelaceWith the release of Spring Framework 5.1 comes a new release of Spring Data.This release notable topics are:  Support for immutable objects  Deferred JPA repository initialization  Support for MongoDB 4.0 Client Sessions and Transactions  New Spring Data JDBC module  Apache Cassandra mapping improvements for Map and tuple types, Lifecycle Callbacks, and Kotlin Extensions  Replica Reads with Spring Data RedisTo see what else has changed, check out this post by Mark Paluch.Spring Security 5.1In this new release a lot has been added regarding WebFlux support: OAuth 2, CORS and HTTPS redirection  are just a few to sum up.More than 50 issues have been resolved too!To see what else has been added, check out what’s new in the reference documentation.Spring Fu 0.0.1Spring Fu is an experimental micro-framework that makes it easy to create lightweight Spring-powered applications with functional APIs instead of annotations.It introduces Kofu (Kotlin and functional) and Jafu (Java and functional, still a proof of concept) configuration for configuring Spring Boot in a functional way and makes use of the functional bean registration.It also ships with coroutines support, GraalVM native images support and various other features.To learn more about this interesting project, visit this link!Spring Tools 4Spring Tools 4 is completely re-built from scatch after a decade of updates and improvements of Spring Tool Suite (STS).It’s a new set of IDE agnostic tools that can be installed in your favorite IDEs and editors.Currently they support Eclipse, Visual Studio Code and Atom IDE.With this release also comes the end of Spring Tools 3, but not until mid 2019!STS 3.9.x will still receive updates and will be shipped as a full distribution, and the distribution will be updated to the upcoming Eclipse releases (2018-09, 2018-12, and beyond).After mid 2019, it will no longer receive any maintenance updates."
      },
    
      "docker-2018-09-24-docker-networking-with-weave-html": {
        "title": "Docker multihost networking with weave",
        "url": "/docker/2018/09/24/docker-networking-with-weave.html",
        "image": "/img/docker-basic-networking/docker-basic-networking.png",
        "date": "24 Sep 2018",
        "category": "post, blog post, blog",
        "content": "  In my last blogpost we talked about setting up a Docker network on a single host. We talked about a very basic 3 tier application which was packaged into 3 containers.An Angular frontend, a spring boot backend and a mysql database container.  No person in their right mind would ever run an application on a single host in production. ( If you are doing this please give us a call and we will gladly come help you out ;-) )So today I will talk you through the process of setting up multi host networks with the help of Weave.Table of contents  Why multihost networking?  Weave  Basic example  Application example  Onprem cloud application example  ConclusionWhy multihost networking?In my last blogpost we ran our three containers on one single host network. This has a few obvious disadvantages which I will address first:      If you would run all your applications as Docker containers on one machine, your environment would still be very prone to outages.If something happens to the physical machine that you use, then you are pretty much done for and you have an outage.Depending on how critical your applications are, this could have disastrous results.That’s why in practice you want to spread your applications over several hosts and connect them to each other over a network.        Not every application that you package in a Docker container is the same.In our example we have a web frontend, a Spring Boot backend and a MySQL database server.A database server has fundamental different needs than a webserver.For example, you want to make sure your database container is running on a system with large hard disks with much faster access times.On the other side, a webserver has little use for large hard drives and has much more benefit from increased memory to allow for many concurrent connections.        If you use a microservice architecture and you want to scale up a certain microservice, it is recommended to run the new instance of that microservice on a different machine than the first instance.This is done for resilience of the application but also to spread the application workload over several different physical machines.This improves the scalability of your system.  So in short: multihost networking is pretty important to build resilient, robust applications that scale well.It also allows you to deploy your containers on hardware that makes most sense for each container.Now that you are convinced that we need multi host networks, at least for our production environments, let’s talk about what we can do to set this up with Docker and Weave.WeaveThere are several options to do multihost networking with Docker.Nowadays Docker itself even supports a basic form of this but we want to use all the good stuff like DNS lookups and service discovery.That’s why we are going to use Weave.NET.Now, what is Weave.NET?Weave.NET is software that is build by the company Weaveworks.It allows you to create a virtual network across multiple hosts and enables automatic discovery of hosts and containers within the network.The following features are the most important and useful to me:      It’s easy to setup. As I will show in the hands-on part of this post, the setup of Weave is done within minutes and is pretty straightforward.        It provides a virtual network on top of your existing network.In big organisations, the network setup can be quite complex with multiple VLANs and firewalls.Using Weave, your network people only need to open one port.Everything that stays within the Weave virtual network can communicate over that same port.        The virtual network is very flexible.You can use Weave to build one virtual network between your on-prem and cloud environment.Within this network, all your containers can communicate with each other as if they were living on the same machine.        An added benefit of the virtual network is that it’s pretty easy to secure.You can encrypt all the traffic on the Weave virtual network which adds a layer of security ontop of the existing security you have in place.This is especially useful if you wish to build a network which spans your on-prem and cloud environment.Since all traffic is encrypted you are not at risk when containers in the cloud communicate with containers in your on-prem environment.        The Weave network comes with its own DNS server.This allows you to do service discovery within the Weave network.This has huge benefits over addressing applications with their IP addresses.Service discovery allows you to do easy loadbalancing and provides your applications with high availablity.        Last but not least I feel obligated to point out that Weave works very well with Kubernetes.Kubernetes is basically the orchestration tool you want to be using to run your containers.A deeper tour of Kubernetes is for another time, but for now I’d like to point out that the default network provider within Kubernetes is flanel but you can replace that with Weave if you want.You then get all the benefits of Weave plus the added value that you can make a virtual network that spans several Kubernetes clusters and/or your regular Docker applications.  The main point I’m trying to make here, is that Weave.NET takes care of a lot of low level networking stuff for you.This allows you to build a more robust and scalable environment to run your applications on without having to worry much about the lower levels in the networking stack.Basic exampleIn this example, I will be using Ubuntu machines on which I have already installed Docker.Following picture shows the setup we will be build in this example:    Installing WeaveTo install Weave on an Ubuntu system you can run the following commands:sudo curl -L git.io/weave -o /usr/local/bin/weavesudo chmod a+x /usr/local/bin/weaveIf you want to install Weave on another system take a look on the weave install page.On systems which don’t support native Docker, you will have to set up something like Docker-machine to get everything to work.We will need to run the Weave install on both of the machines that we are going to use in this example.Launching WeaveNow that we have installed Weave on both machines, let’s start it up.Setting up host 1On the first host, type the following commands:weave launcheval $(weave env)The first command launches Weave. Weave runs as a set of Docker containers on your system, you can see this when you run the launch command:    If you run the docker images command you can take a look at all the images that Weave downloaded:    The second command configures the Weave environment so that containers that get launched will automatically attach to the Weave network.This means that you have to provide no additional arguments to your Docker run commands to use the Weave network.Behind the scenes Weave has setup a Docker network for you on your machine, you can take a look at this with the command:docker network ls     You can recognise the Weave network by the name Weave and as you can see it uses the weavemesh driver instead of one of the standard Docker network options.When our network setup is done we will test it out with a test container.So for now let’s start this container on the first host:docker run -d --name app_on_host1 weaveworks/ubuntuThis is just a stripped down Ubuntu Docker image.We will use it later to test our network setup.If you look at your machine’s network stack you will see that Weave has setup several different networks on your host machine in order to make it’s magic work.When I run the:ifconfigcommand on my machine, I can see that Weave added following network stacks to my machine:    What all these networks do is out of scope for this post as that operates on a pretty low level in the TCP/IP stack and we are not really interested in it in our usecase.We just want it to work, right?Setting up host 2In the last screenshot of the previous section we looked at the network stack of host 1.This is important because we need the physical IP address from that host to tell our second host to connect to host 1.Host 1 has IP address 192.168.1.18. This gives us following command to run on host 2:weave launch 192.168.1.18eval $(weave env)The syntax of this command is pretty straightforward.You tell Weave to launch and to connect to every IP you supply as an argument to the launch command.We will also setup a test container on this host:docker run -d --name app_on_host2 weaveworks/ubuntuThese are all the setup steps you need to do in order to setup a simple Weave network.Now let’s see how we can verify that everything is working.Verifying our setupThere are a few things we can do to verify that our setup is working.To start off let’s look at the status off our Weave service:weave statusThis produces following result:    You can run this command on either host, it will provide you with basically the same results.All sorts of useful stuff is available in the status overview including the range of your Weave subnet, the connected peers, the connections.As a first step to verify that our network is working, let us examine the connections and peers in more detail with following commands:weave status peersweave status connections    As you can see from the output of the commands there are now two hosts in our network and there is one connection between them.So far everything is looking good!Now let’s do the real test and see if the containers can find each other by the name we gave each container.SSH into the container on host 1 and ping the application on the other host:docker exec -ti app_on_host1 bashping app_on_host2    As we can see from the results, our container was able to resolve the other container over the Weave network by making use of the Weave DNS service and the Weavemesh network.If we reverse our test and run it on host 2 we get this result:docker exec -ti app_on_host2 bashping app_on_host1As you can see from the example outputs, both hosts are now connected, and thanks to the Weave network and DNS service, they are able to resolve each other by hostname.Another noteworthy thing is that Weave has setup a class A network (10.xxx.xxx.xxx) range for us.These are all IP addresses within the Weave network so you don’t have to worry about any IP conflicts with your existing network.If you would have the need for a specific subnet you can force Weave to use whatever subnet you like (192,168.1.XXX for example).Application exampleFor this example we will use the Weave setup we created in the previous example.I will be running the application from the basic Docker networking blogpost on our two hosts in the Weave network.You can checkout the code for this example at my github account.For this example I will be using the weave-basic-example branch.Setup databaseIn the first step, we setup our database container on one of the Weave hosts.I will be using my desktop to run the database container.First we will run the run_db.sh script to start up a new MySQL container and assign a root password to it.When that container is up and running we can initialise it with a database and some data.To initialise the database run the init-db.sh script.We can run following commands to verify that our database is up and running in the Weave network:docker psweave psThis should give you a similar output to the screenshot below:    As you can see the mydb container was automatically added to the Weave network because of the eval $(weave env) command we ran earlier in this demo.Note that the Weave ps command shows you the container id and the IP address it has allocated to that container.Now that our database is up and running, let’s switch over to our other machine and start the other services over there.Setup backend applicationLet’s install our backend service on our second host.In the backend folder we will run the following commands to compile our application and start up our Docker container.Let’s start our backend container with the test_run_backend.sh script.This script runs the container and exposes its webservice to the outside world through port 8080.We can then use our browser on our host to verify that the service is running correctly:mvn clean installdocker build -t rest-backend ../test_run_backend.shTo verify that out application has started successfully we can run the following commands:docker psweave psThis shows us that the container wass started correctly and it got an IP addresss within the Weave network.    Now that we know our backend and our database are running within the Weave network, we can query our REST service and see if everything is working as expected:    When I do an HTTP GET to my REST backend, it comes back with a greeting it got from the database container on the other host.Now that we have verified that our backend runs fine, we will stop it and start it up again without exposing a port to the outside world:docker stop rest-backenddocker rm rest-backend./run_backend.shThe Docker ps command on the backend now runs without exposing a port.Since our frontend application is calling our REST backend through the Weave network, we don’t have to expose a port to the outside world:    Setup frontend applicationOur frontend application is a bit different from the one I used in my previous demo.In the previous demo we used an Angular frontend, but I replaced it with a simple webserver I wrote in Golang.I want to demonstrate that applications can access each other through the internal Weave network by their container name.I could not demonstrate this last time with Angular code since that’s rendered on the browser at client side.Our client’s browser is not inside the Weave network so it has no way to resolve the webservice call from our frontend application to our REST service.In this example, our Go frontend app renders the HTML response on the server side.This means the code that calls the REST service is running inside the Docker container and is thus a part of the Weave network.We can also access our REST backend via following url:    The source code of the Go application is on Github in the frontend folder.Please note: I also included a compiled binary so that you can use the program even if you don’t have a working Golang installation.To start our frontend container, run following commands inside the frontend folder.Only execute the first command if you wish to compile the Go program yourself:go builddocker build -t frontend ../run_frontend.shOur Go frontend application should be up and running now so let’s check if everything is working as expected:    We can access our frontend application through port 8080 (which we exposed to the outside world with the Docker run command).In this example I gave the name “john” which has no entry in our database so our REST backend returned the standard hardcoded message “hello friend”.    In this example I gave “bas” as a parameter and since we have a message in our database for this name, we got the message “hello master” as a result.Since no Docker ports are exposed to the outside world (except for our frontend application to make it accessible from our host), this is definitive proof that our Go application can access our REST backend application through the Weave network.Likewise, our REST backend application accesses our mydb container through the Weave network as well.Oh, and by the way: You can run your containers on any host which is connected to our Weave network. Cool, huh?Now, let’s take this one step further. In this next example we will extend our Weave network from our home network into the cloud.Onprem cloud application exampleIn the final example of this blogpost I will show a setup where we have 1 Weave network that spans our onprem environment and connects to a cloud environment (AWS).In this example we will run each application container on a different host. It looks like this.    In the end result I will be running the mydb container on my desktop (database server). The rest-backend will run on my laptop (backend server) and our frontend container will run on an EC2 instance on AWS.Everything will be connected through the Weave network that we will reconfigure to encrypt our data between the cloud and onprem environment.Let’s get started.Setup EC2 serverThe first thing we have to do is create a new EC2 server on AWS that we can use for this demonstration.I’m not going to go into full detail here about how you can do that but it’s quite easy and there are a lot of tutorials out there.After our EC2 server is up and running we can login to it via ssh and install docker and weave.We will start up Weave on our EC2 server with encryption enabled.weave launch --password weavetesteval $(weave env)The password weavetest part enables the encryption for this peer. If another peer wants to connect to this peer it has to provide this password before a connection can be established.After the connection is established all traffic over the connection will be encrypted.Another thing we have to take care of is make sure that Weave on the EC2 server can connect to it’s peers.Remember, an EC2 is running in a VPC behind a firewall so we have to open certain ports for this to work.In the case of Weave we have to open the port 6783 – which is the control port.And we also have to open the port 6784 – which is the data port.You can do this by editing the launch group of your EC2 and add following inbound rules to it.    As you can see I added rules to allow TCP and UDP traffic on port 6783 from all sources. This enables the Weave control process to connect to its peers.Afterwards I added a rule to allow UDP traffic on port 6784 from all sources. This port is used by Weave to send it’s data.Note, I also opened port 8080 because our frontend uses this port to listen for web requests.    We will use the public ip4 address and DNS name later in this demo.Reconfigure the existing onprem hostsBefore you can connect Weave from your homenetwork to the cloud you have to make sure that your router ports 6783(TCP/UDP) and 6784(UDP) are opened.First lets reconfigure our database host.weave resetweave launch --password weavetesteval $(weave env)The weave status command on this host now shows that encryption is enabled.    As you can see Weave is now reporting that encryption is turned on but this Weave peer is not yet connected to any others.Let’s take care off that now. Switch over to the backend host.Now let’s run the magic Weave command that will make our setup run.weave launch 34.247.178.145 192.168.1.99 --password weavetestThis command launches Weave on our backend host and tells it to connect to our EC2 instance in the cloud and to our database server onprem.    As you can see our backend host is now connected to both our EC2 server and our database server. Let’s check our EC2 server as well.    As you can see our EC2 server now also has encrypted connections to both the backend server and the database server.Weave will automatically connect hosts on your network as long as they can be connected though 1 common peer.In this case our backend server has a connection to our EC2 server and a connection to our database server.So Weave will automatically create a connection between those 2 as well.    As you can see in the screenshot I browsed to the frontend application on the AWS server and it generated a correct response which means our setup works!ConclusionIn this blogpost we started off with a simple onprem Weave network.After testing that the network works we deployed a distributed application on that network.After this we extended our private Weave network with a peer on AWS in the cloud.The end result we achieved was that our distributed application was running spread over a cloud and onprem environment.In our final example we also enabled encryption on the Weave network so all traffic between your onprem environment and the cloud is encrypted.This can be a huge benefit for enterprises who want to move their applications into the cloud. With Weave you can setup your network in a hybrid cloud/onprem model and be sure that all communication is safely encrypted.This allows the enterprise to do a gradual move to the cloud instead of having to do a big bang approach."
      },
    
      "testing-2018-08-15-node-red-dev-ci-html": {
        "title": "Node-RED: Development and CI",
        "url": "/testing/2018/08/15/node-red-dev-ci.html",
        "image": "/img/node-red-dev-ci/node-red-dev-ci-white.png",
        "date": "15 Aug 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  What is Node-RED  Why we are using Node-RED (or an alternative)  Node-RED to the rescue  Configuration components  Running an instance  Creating your first flow  Spicing things up  JSON  Node-RED persistent config  Node-RED and Docker  Node-RED and CI  ConclusionWhat is Node-RED  Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways.” – from docs https://nodered.org/And yes, that’s all true.But we’re not using Node-RED for those things.There are two use cases for which we use Node-RED, but before we go into those, we’ll take a quick look at some other Node-RED features.Some great pros:  Node-RED is built on Node.js, taking full advantage of its event-driven, non-blocking model.This makes it ideal to run at the edge of the network on low-cost hardware such as the Raspberry Pi as well as in the cloud. (from docs)  Node-RED comes with a web based graphical user interface, where you can manage your API flows with drag and drop functionality.  Because Node-RED is built on Node.js you can just run it locally on a lot of systems (Windows, Mac OSX, Linux,…), on a lot of devices and in the cloud.The graphical user interface lets you create your endpoints and flows in an easy way.Just drag and drop your components in your flowchart and connect them by drawing a line between them.Double clicking each component will open the detail screen where you can set some variables for that component.When your flows are ready, there is an import and an export function available for storing and sharing your configs.Why we are using Node-RED (or an alternative)Node-RED and DevelopmentComing from a frontend dev background, I can confirm that nothing is more frustrating than running backends, or at least part of the backend, locally, before you can start coding and testing.Yes, Docker and Docker Compose are helping a lot but still…In some cases, backend development, for a specific feature, is planned for the same sprint as the frontend development.So even running a backend locally, will not help you.Mocking can be an option.You can start mocking the data inside the communication layer or mocking an external backend with test data.In an ideal world, teams should do some analysis of the specifications coming from business and their analysts.Based on those specification, developers can start estimating tasks (or stories and epics), and think about the architecture and design.At this point, technical specifications could already get written down, such as model designs and API contracts.It’s almost impossible to do a good job in the frontend if those things are not available, or at least specified.Why do you want good test data?If you know the characteristics, it’s easier to do some layout stuff.You would think that mockups and designs would be accurate, but most of the time they aren’t or they are not inline with the data.So what about mocking?Yes, you can mock stuff yourself and maybe skip the communication layer.Or you can mock the communication part in the communication layer by means of interceptors for example.But at some point in time, you’ll need an API and test data that is close to production data when it comes to live data specs.When mocking inside your components, would you include everything in your code base?Or would you exclude it, but still make it more accessible for your colleagues?Some frameworks include this kind of functionality such as MockBackend from Angular (More info here).That said, I really like and prefer external tools for mocking a backend.That way it’s separated from your app and code base, so it can easily get replaced or modified without triggering stuff in your frontend pipelines.You can even choose to run a tool on your local machine.Or on-premise and make it accessible for all your (frontend) developers.Node-RED and Testing (CI)Even if you have implemented your mocks inside your communication layer and it’s doing the job you expect it to do, how are you going to test your communication itself?You can use those mocks to run unit tests, but it’s harder to run integration tests that cover your communication layer as well.Using an external tool, gives you the possibility to reuse this part in your test setup.You can integrate this external mock backend in your setup and redirect your frontend calls to the mock instead of a real backend.Why you should do this, is explained in an earlier post on our tech blog.If you’re interested, you can read all about it here.Node-RED to the rescueIn both cases, Node-RED can help us providing a real API that responds with test data.For the test data, I prefer test data getting delivered by the business or the client, but if that is not an option, you can create your json data based on the API contracts and model designs.Again, I’m mentioning the API contracts and model design, because often, organisations fail at this.Let’s setup a real easy API.PrerequisitesBefore starting this tutorial, you should download or install some things:  Make sure you have a Node.js (incl npm) environment  sudo npm install -g --unsafe-perm node-red  A running Docker daemon  docker pull nodered/node-red-dockerNOTE: To be honest, you only need one of those 2 environments.For the other part, you can just read through it and then try it in the other setup.Configuration componentsAlthough we are only using Node-RED for HTTP(S) (REST) and MQTT backend mocking, Node-RED provides more options.Node-RED can even be used as a real backend, using external databases for example.So before we start our setup, let’s take a quick look at some basic features.InputsHTTP, TCP, UDP are probably the most straightforward and known by all of you.I guess no explanation is needed.WebSockets are also used a lot nowadays.WebSockets are an advanced technology that makes it possible to open an interactive communication channel/session for pushing messages between a client(s) and a server(s).MQTT is one of the standards for messaging for mobile devices.It is a lightweight messaging protocol for small sensors and mobile devices, optimized for high-latency or unreliable networks.Inject, catch, status and link are internal inputs.Inject provides a way of injecting a message(s) into a flow.This can be done manually or at a regular interval.Catch can be used to handle an error from another node.The normal from of that node will get terminated, but with catch you can handle and catch the error providing an alternative route.The status node is just reporting messages from other nodes and link lets the user create virtual links between flows.OutputsSame as above for HTTPS, TCP, UDP, WebSockets, MQTT and link but now for outputs.Debug lets you show messages in the sidebar.The debug sidebar provides a structured view of the messages it has sent, making it easier to understand their structure.FunctionsNode-RED comes with a lot of predefined function blocks.The ones we are using most are the regular function-block itself and the json- or xml-function to parse json and xml.Besides those, there are function blocks that lets you run calls to external platforms, delay, reroute or redefine messages, etc.If you need more information about one of them, Node-RED show an information screen when selecting one of them.SocialThis section provides us with functionality to intercept (input/output) email- and twitters messages.I’ve personally never used one of them, but I can imagine they can come in handy.StorageFile (input/output) is self explanatory.The tail function, lets you add a watcher to a file.As the docs describe, this cannot be used on Windows systems.It’s based on the $ tail -f fileName command on Unix systems and watches for changes.AnalysisThis component lets you analyse a received message based on a sentiment score.You can read all about it here.AdvancedLike the tail function, the watch-block watches for directory or file changes.The feedparse can be used to monitor an RSS/atom feed for new entries.The last one, the exec-block lets you run a system command and returns its output.Running an instanceFirst things first, let’s fire up a Node-RED instance.(We are going to start with the npm package version)To start using or initiating an instance of Node-RED by using the npm repositories, just run:$ node-redIf installed correctly, you should see the following log:  There are four interesting lines in this log.The first three of them are the settings file, the user dir and the flows file.The last line shows us where the service is available.We’ll come back to the config later, and we are going to take a look at the User Interface based on the uri in the last line.As mentioned before, Node-RED offers a user interface, to configure the service.Open your browser and navigate to http://127.0.0.1:1880 (default setup config).You’ll see all the components mentioned above on the left side of the screen.In the middle you have a tabbed structure for your flow chart(s), and on the right you have debug, info and detail windows.On the top right of your screen, you have a settings dropdown and a menu with deploy options.Creating your first flowLet’s use the Node-RED user interface to create a simple flow.In this case a simple http-input connected to a function that returns its data through an http-output.Start by dragging the http from the input list onto the chart.Then, drag and drop a function block from the function list and the http from the output list onto the chart.Connect the http-input with the function block by dragging a line starting from the input node, to the function block.Repeat this for the function-block to the output.  Of course this isn’t ready for being deployed just yet, cause we did not add any logic yet.Let’s configure our flow!InputDouble click the http-input block.This action will open a window on the right where you can set some properties.  Method: GET  URL: /data  Name: Get all data  Confirm these settings by clicking ‘Done’.Function blockDouble click the function-block.Again, a window will open!  Name: Retrieve data  Function:msg.payload = [  {id: 1, title: \"Title 1\"},  {id: 2, title: \"Title 2\"},  {id: 3, title: \"Title 3\"},  {id: 4, title: \"Title 4\"},  {id: 5, title: \"Title 5\"}];return msg;NOTE:The information section on the right comes in handy while configuring the function-block  A JavaScript function block to run against the messages being received by the node.The messages are passed in as a JavaScript object called msg.By convention it will have a msg.payload property containing the body of the message.This explains why we are setting the msg.payload property.  Again, confirm these settings by clicking ‘done’.OutputOpen the config for the http-response-output block.  Name: return all  Status Code: 200  As you can see, the output lets you choose the return status code.This can be used later if you would like to implement failed REST-calls for mocking purposes.Confirm these settings by clicking ‘Done’.DeployOn the top right of your screen, you have the deploy-dropdown.Clicking deploy will deploy this flow onto the running server, but you can also use the arrow to open up the menu and choose a more accurate scenario.Since this is our first deploy and we only have one flow, we don’t have a choice.  Test setupTo test our deployment and of course our flow, just navigate to http://localhost:1880/data in your browser or use curl to GET the data:  $ curl -X GET http://localhost:1880/data  Spicing things upLet’s try and implement a DELETE.We would first drag and drop an http-input, a function-block and an http-response-output onto the flow chart.  The input would look like:  Notice the :id.This way we tell Node-RED that this is a path parameter.The output would look like:  But what about the function-block ? We could come up with something like:let data = [ {id: 1, title: \"Title 1\"}, {id: 2, title: \"Title 2\"}, {id: 3, title: \"Title 3\"}, {id: 4, title: \"Title 4\"}, {id: 5, title: \"Title 5\"}];if(msg.req.params &amp;&amp; msg.req.params.id) { // the id   data = data.filter((item) =&gt; {       return item.id != msg.req.params.id;   })}msg.payload = undefined;return msg;So we’ve filtered out an element, but that’s it.Everytime we call this function block, it will start by initialising the data array.Since we are not persisting the data, requesting all the records, will still return all the data records.Persisting dataThere is a way to connect to a database, but that is outside of the scope of this post.First, let’s show some ways of persisting data.  Node context  Flow context  Global contextThe node context lets the user persist variables for that node.Whenever that node gets redeployed, the context is gone.The flow context is a context shared over all nodes in that flow chart while the Global context lets you share a context over the whole app.You can set and get a context variable with the getters and setters from the api.For a flow context this will look like:let data = flow.get('data');// do stuffflow.set('data', data);Initialising dataNow that we know how we can store data, we need to find a way to initialise the data.We could for example launch a call to trigger a function that would store data in the flow context.We could build this by using an http-input, a function-block and an http-response-output.But this doesn’t feel right.Luckily, Node-RED provides us with an inject-input.  This input will get triggered periodically, or just one time. inject-input is made for injecting messages of almost any type into the next component.This message can be a string, a number, the current timestamp, …We are going to use it as a trigger on startup to initialise our data into our flow.inject-input:  inject-function:  RefactoringNow we know how we can inject and persist data in a context, let’s refactor our GET and DELETE.And while we are at it, add a second GET so we can query one record by ‘id’.GETOur two http-inputs are exactly the same except for the parameter (id).The function-block on the other hand, will have some build-in logic:let data = flow.get('data');if(msg.req.params &amp;&amp; msg.req.params.id) {    for(let itx in data) {        if(data[itx].id == msg.req.params.id) {            msg.payload = data[itx];            break;        }    }} else {    msg.payload = data;}return msg;We are retrieve the data from the context and storing it in a local variable.Based on the existence of the id parameter, we are going to search for a single record, or return all records.Notice the == instead of ===.This is because the id in the data is a number, where the id from the params is a string.Based on the logic, we could add a query parameter to the endpoint that let’s us filter the data on the title property for example, but that is out of scope.DELETEAs shown earlier, we are going to use a simple filter function to filter out the record with the given id.We start by retrieving the data from the flow context, and then filtering this data.Don’t forget to rewrite the new data to the flow context, so other components will get updated context data.let data = flow.get('data');if(msg.req.params &amp;&amp; msg.req.params.id) {    data = data.filter((item) =&gt; {        return item.id != msg.req.params.id;    });    flow.set('data', data);}msg.payload = undefined;return msg;All togetherNow we should have an inject-input and 3 http-inputs (2 GET’s and 1 DELETE).Our flow should look like:  Let’s deploy and test this setup.First click the deploy button and wait a second.Then navigate with your browser to http://localhost:1880/data.This should display your data in your browser.  Now, let’s try to delete a record.$ curl -X DELETE http://localhost:1880/data/3And refresh your browser:  Seems like one big success! No?Not exactly.Try and delete records 1, 2, 4 and 5.Now what?Our data is gone and we don’t have a POST nor a PUT implemented.Implementing a POST and a PUT is really straightforward.POST and PUTFor the example, we are going to work with an upsert. POST and PUT will trigger the same function block that will be smart enough to update (if exists) or insert.  The upsert function-block can look like:let data = flow.get('data');let body = msg.payload;body.id = Number(body.id); // quick fix making sure it's a numberlet id = msg.req.params &amp;&amp; msg.req.params.id || body.id;let found = false;for(let itx in data) {    if(data[itx].id == id) {        data[itx] = body;        found = true;        break;    }}if(!found) {    data.push(body);}msg.payload = body; // or adjusted body if neededreturn msg;So now we have a GET, POST, PUT and DELETE.But still, we are missing something.Assume we are developing delete functionality in the frontend while using this setup as a backend.Since we only have 5 records, I should inject new data to test after my 5 deletes.Or assume we are testing a frontend, connected to this mock backend.Wouldn’t it be nice that we can refresh the data to its initial state before running a new test suite?The point I’m getting to is, we should come up with a way of refreshing our data to its initial state.Resetting dataWhen you look at your flow chart, you’ll see there is something that looks like a button on the left side of our inject-input.When you click it, you’ll notice a toast message appearing on the top of your user interface:  But what if we don’t have access to the user interface, or we don’t have control over it what so ever? For example, when running automated tests against this mock backend, somewhere on a dynamic Jenkins Docker slave? To overcome this, we are using an http-input to trigger the reset of our data.Let’s take a look at our implementation:  The http-input is listening on /rest.As you can see, we’ve added an http-response-output.REST-calls are expecting a response! Don’t forget to deploy your new setupMaking sure this works:$ curl -X GET http://localhost:1880/data   // should respond with the initial data$ curl -X DELETE http://localhost:1880/data/2   // should delete record with id 2$ curl --header \"Content-Type: application/json\" \\  --request POST \\  --data '{\"id\":\"7\",\"Title\":\"Title 7\"}' \\  http://localhost:1880/data   // should add record with id 7$ curl -X GET http://localhost:1880/data   // should give the expected result$ curl -X GET http://localhost:1880/reset  // should reset the data$ curl -X GET http://localhost:1880/data   // should show the reset is working  JSONThe last refactoring we are going to do in this tutorial is switching from an hardcoded data json object to an external json-file.As mentioned before, Node-RED provides ways of integrating storage and databases, but that is out of the scope of this post.Still, we don’t want to include real data in this setup.We should loosely couple our data provider and the backend mock implementation.If some analysts, for example, provide us with new test data, we don’t want to change anything in this setup.One way to overcome this, is to work with json-files.Assume we have a json-file, called data.json with content:[  {\"id\": 1, \"title\": \"Title 1\"},  {\"id\": 2, \"title\": \"Title 2\"},  {\"id\": 3, \"title\": \"Title 3\"},  {\"id\": 4, \"title\": \"Title 4\"},  {\"id\": 5, \"title\": \"Title 5\"}]Remember when I mentioned some important lines in the console at startup?One of those lines was referring to the User directory:User directory : /Path/To/Your/Home/.node-red (which is the default)Because we didn’t change anything in the settings.js file (or didn’t point to a custom one), Node-RED creates a context folder for the current instance.Copy your data.json to this folder.I will explain later why we are doing this.  Let’s read data from this json file instead of hardcoding the data:Add a file component from the storage sectionOn the left of the user interface, there is a storage section with an input file option.Drag this onto the flow chart.Unlink the connections from the inject-input and the reset http-input with the ‘store’ function-block and connect both with the file-storage component.Open the details and set the properties:  Do read the text on the bottom about the patch to the file! And keep in mind we have selected the ‘single UTF-8 string’ option.Add a Json parse functionIn the function section on the left, you have a JSON component.Depending on its input, it will parse or stringify a string or json object.Because we will be sending this component a ‘single UTF-8 string’, it will parse it to a JSON object.  Integrate this component in between the file-storage component and the store function-block.Refactor the store functionWe are now ready to refactor the store function-block.Remove the data and replace it with the content of the json-parse function.  Result [result]When finished, our setup should look like the following flow chart:  When the json-file gets updated, a data reset call will reload the new data into the flow context without needing to change the setup itself!Node-RED persistent configAs mentioned before, Node-RED uses a user directory, a flow.json file and a settings.js file.If those do not exist, it will create default ones for you in the default node-red path %USERPROFILE/.node-red.Going through the settings file is also out of the scope of this post.Advanced users can just read all about it in the docs or just open it and modify it.Why I’m mentioning this directory, is because of the config persistence of Node-RED.Go ahead and kill your Node-RED server (CTRL-C in terminal).And now just restart it:$ node-redAs you can see, Node-RED will just pick up its previous config because it’s available in the default directory.When more people need to work with this config, we need to share it.The easiest way to share this config, is to wrap it in a Git repository.Keep in mind that this may not be best practice to wrap the whole directory, but this way, your colleagues and continuous integration platform can just checkout the repository and run the server, including the data files.When running $ node-red --help it shows the command line params.  Look at the -s and -u option.Node-RED provides us with cli parameters to run custom configs.Let’s assume we clone our newly created Git repository in ~/repositories/node-red.$ node-red -s ~/repositories/node-red/settings.js -u ~/repositories/node-redNode-RED and DockerNode-RED also provides a Docker image at Docker Hub.This way you do not even need a Node.js environment preinstalled.You can just run it with:$ docker run -it -p 1880:1880 --name mynodered nodered/node-red-dockerWhat about our persistent config?  Node-RED is started using NPM start from this /usr/src/node-red, with the –userDirparameter pointing to the /data directory on the container.The flows configuration file is set using an environment parameter (FLOWS),which defaults to ‘flows.json’.This can be changed at runtime using thefollowing command-line flag. – from docs https://hub.docker.com/r/nodered/node-red-docker/$ docker run -it -p 1880:1880 -e FLOWS=my_flows.json nodered/node-red-dockerSo we can choose only to provide the flows.json file or we could map our user directory from our repository as a volume to the /data in the container, providing the container our context.An other option is to bake your userDir into your image.This way you can provide your CI with a ready to go Docker image for this particular case.This is probably not the best solution, but in some cases it can be very helpful.Node-RED and CIAs we now know how we can configure, dockerize and run our mock backend, we could easily include it in one of our testing stages during our Continuous Delivery pipelines setup.For those of you whom read the article about the different stages of API testing (here), the following setup will need no further explanation.  In this setup, the task at hand is running the automated tests defined in and run by our Gauge, Protractor or Nightwatch.js framework.To be able to do this, it would be nice to launch an environment at once.We can provide our Continuous Integration environment with a compose file that will launch and destroy our frontend and Node-RED mock backend in one environment.ConclusionIt’s nice to have an external tool available that can get reused for multiple purposes, in our case development and testing.Node-RED provides us with cool features and an easy to use User Interface to do so!"
      },
    
      "testing-2018-08-03-testing-angular-with-jest-html": {
        "title": "Testing Angular with jest",
        "url": "/testing/2018/08/03/testing-angular-with-jest.html",
        "image": "/img/2018-08-03-testing-angular-with-jest/jest.png",
        "date": "03 Aug 2018",
        "category": "post, blog post, blog",
        "content": "Last year I learned about Jest, a testing framework. ‘Yet another one’ was my first thought.Later that year my colleague and I were looking to extend our test coverage. We where using Jasmine to write the tests we had and Karma to run them.It worked for sure and we had a lot of tests but it was like a punishment to write them every time, repeating the same code to mock things and when it finally worked and we pushed them to the CI they would sometimes fail randomly.So we were eager to find a better way to test.Pretty quickly we started looking into Jest. It differentiated itself by not using Karma as a test runner.We liked the idea because Karma actually has some points of failure which we encountered often.Karma vs JestKarmaLet me quickly give you an overview of what it is that Karma does:  it spawns a webserver to serve your source and test code;  it starts a browser from your machine and connects to the webserver;  it spawns a reporter which has a socket connection with the webserver;  it runs every test, awaits its result and sends it to the reporter.In the end we have 3 components communicating with each other. Which components exactly, dependends on the environment Karma is running on.Our CI was a Linux machine, I had an Ubuntu to work on, my colleague was on MacOS, and other guys in the team were on Windows.So we all had different Chrome versions which gave us some issues. And our CI used PhantomJS, which is outdated, so here we also had some issues.JestHow are these issues fixed in Jest?As I mentioned before Jest does not use Karma to run the tests, it will just start a single NodeJS process which does all the work by itself:  it compiles your code;  it runs your tests with NodeJs (or JSDOM for DOM testing);  it creates a report.Just plain and simple without too many interconnected processes to break.Also, no real browser is needed on the machine since NodeJs and JSDOM are used.Therefore the only tool to keep up to date is Jest, which is managed automatically via the Yarn lockfileSet upSo how can you set it up and quickly replace all your tests (if you’re coming from Jasmine)?To make Jest available in an Angular project, you first need to install Jest and jest-preset-angular.Since Jest is made for React (backed by Facebook, remember) we need jest-preset-angular to fix some things for us.$ yarn add -D @types/jest jest jest-preset-angularSome configuration is always needed so let’s add some lines to the package.json (or export the config in a jest.config.js).First we point to the preset that we will use. Next we provide a setup-jest.ts script in which we import some necessary files (we’ll create it later on).Then we provide information about how Jest should transpile our code under the transform property. Therefore we point to the preprocessor from jest-preset-angular for our typescript and html files.And for the Javascript files we’ll point to babel-jest (which ships with Jest).The transformIgnorePatterns point to some libraries that don’t need to be transpiled for our tests. (If you get Unexpected token import issues, you might need to add some packages here)\"jest\": {  \"preset\": \"jest-preset-angular\",  \"setupTestFrameworkScriptFile\": \"&lt;rootDir&gt;/src/setup-jest.ts\",  \"transform\": {     '^.+\\\\.(ts|html)$': '&lt;rootDir&gt;/node_modules/jest-preset-angular/preprocessor.js',     '^.+\\\\.js$': 'babel-jest'   },  \"transformIgnorePatterns\": ['node_modules/(?!@ngrx|ng2-translate|@ionic|lodash|ionic-angular)'],}As mentioned previously, we create a setup-jest.ts file in which we import some code from jest-preset-angular and a global mocks file.import 'jest-preset-angular';  import './jest-global-mocks';  In the jest-global-mocks.ts we provide functionality that is not found in JSDOM but that we use in our code (and thus is found in our preferred browser).So we mock things that are globally accessible, if you use certain browser API’s you should also mock them here.For our example we needed the following code:const mock = () =&gt; {  let storage = {};  return {    getItem: key =&gt; key in storage ? storage[key] : null,    setItem: (key, value) =&gt; storage[key] = value || '',    removeItem: key =&gt; delete storage[key],    clear: () =&gt; storage = {},  };};Object.defineProperty(window, 'localStorage', {value: mock()});Object.defineProperty(window, 'sessionStorage', {value: mock()});Object.defineProperty(window, 'getComputedStyle', {  value: () =&gt; ['-webkit-appearance']});Object.defineProperty(window, '__env', {value: {env: {backendUrl: 'mocked URl'}}});As you can see, our tests use localStorage, sessionStorage, getComputedStyle and an environment property (__env) on the window.With everything set up we could run our test by running the Jest commandjestOf course not much is running yet since all our tests use Jasmine, and ‘jasmine’ (as a keyword) is unknown to Jest.To fix this we need to replace some Jasmine specific code by Jest specific code.Jasmine to jestIn Jasmine we would create a mock object using Jasmine’s createSpyObj-function and passing some parameters in it.// Jasmineconst serviceMock = createSpyObj('service', ['methods', 'you', 'want', 'to', 'mock']);In Jest we just create an object with the expected properties, and jest.fn() creates a mock function for us.This is a great improvement since you get better code hints and you can easily create more advanced mocks.// Jestconst serviceMock = { methods: jest.fn(), you: jest.fn(), want: jest.fn(), to: jest.fn(), mock: jest.fn()};Also to mock return values it is a bit different (for the better):// JasmineserviceMock.you.mockReturnValue(serviceMock.you as Spy).and.returnValue('yannick vergeylen');// JestserviceMock.you.mockReturnValueOnce('yannick vergeylen');// And you can chain multiple return values if you need itserviceMock.you.mockReturnValueOnce('yannick vergeylen')               .mockReturnValueOnce('bob')               .mockReturnValue('everyone');// Or even with a function which can execute simple logic.// But you shouldn't be implementing to much logic, since you don't want to test the tests.serviceMock.you.mockImplementation((firstname,lastname) =&gt; `${firstname} ${lastname}`);// Or provide it at initialisation which saves you a line of codeconst serviceMock = { methods: jest.fn(), you: jest.fn((firstname,lastname) =&gt; `${firstname} ${lastname}`), want: jest.fn(), to: jest.fn(), mock: jest.fn()};For the assertions you shouldn’t have to change much, since Jest uses almost the same assertion functions as Jasmine.expect(serviceMock.methods).toHaveBeenCalled();expect(serviceMock.methods).toHaveBeenCalledWith('value');// Jasmine(serviceMock.you as Spy).calls.mostRecent()[0]// JestserviceMock.you.mock.calls[0][0] // to get the first argument of the first call (firstname)serviceMock.you.mock.calls[0][1] // to get the second argument of the first call (lastname)I changed all our tests with some regexes, it is possible with some creativity, but today there are codemods which should do the hard work for you.Checkout the jest documentation to find out more.Jest really gets interesting when you use libraries and need to mock them:// Jestimport {HttpClient} from '@angular/common/http';import {CompaniesService} from './companies.service';import {Observable} from 'rxjs/Observable';jest.mock('@angular/common/http');const httpClient = new HttpClient(null);let companiesService= new CompaniesService(httpClient);test('the service should map the return value to an array of companies', () =&gt; {  httpClient.get.mockReturnValueOnce(Observable.of({companies:[{name:'C1',code:'C1'}],page:6,total:51}))  companiesService.getPage(6)    .subscribe((value)=&gt;expect(value).toEqual([{name:'C1',code:'C1'}]));  expect(httpClient.get.mock.calls[0][0]).toEqual('backendUrl/companies?page=6')});Instead of mocking HttpClient we can just import it, provide the return value we know backend will give and focus on testing the output of our getPage method.In the above example you see I have to create a instance of httpClient to get around dependency injection in Angular, but other imports can also be mocked in the same way.ConclusionSo one year later we are still using Jest and testing is still a lot more enjoyable than it was before.Not painless as Jest claims it to be, but that’s just the nature of testing I guess."
      },
    
      "agile-2018-07-31-agile-leadership-game-html": {
        "title": "Agile Leadership Game",
        "url": "/agile/2018/07/31/Agile-Leadership-Game.html",
        "image": "/img/agile-leadership-game/How-Agile-Are-You.jpg",
        "date": "31 Jul 2018",
        "category": "post, blog post, blog",
        "content": "Last week we, as the guild of agile coaches from Ordina, tried and tested the Agile Leadership Game developed by the Agile Consortium called ‘How Agile Are You?’.In this blog post, we will provide you with some more information about the game.Details  2-6 players (even though more is possible)  Intended audience is management  Takes 1.5 hours (on average)  Needs a skilled facilitatorGame PlayThis game has been designed to get discussions on agile leadership going and to ensure that people start observing behaviour that belongs (or does not belong) to an agile organisation.It is specifically intended for management teams.The game is focused on agile mindset and agile leadership: what do people and organisations need from management when engaged in an agile transformation?Participants are asked to very rapidly (! ten seconds per card on average :) !) divide fourty-two cards with ‘characteristics’ of organisations, over one of the quadrants on the board.As a participant you are to decide how the characteristic manifests itself in your organisation at this moment.The game board consists of the quadrants “LET GO, IGNORE, CREATE and KEEP” as you can see in the image below.If the characteristic on the card is behaviour that the organisation DOES HAVE at that time but that you DON’T WANT, the card gets sorted in the quadrant “LET GO”.If the participant perceives the characteristic on the card as a behaviour the organisation DOES HAVE and that you DO WANT, it ends up at “KEEP” and so on.An example of a characteristic is “management spends a lot of time putting out fires”.There is no right or wrong: the game provides insight in the current state of affairs.The characteristics are labelled in types of behaviour belonging to types of organisations.So you could end up with lots of characteristics belonging to agile organisations in the “CREATE” quadrant. :)After the time box for sorting the cards ends, the participants will discuss the characteristics that seem most relevant to them and their organisation.Each participant gets the chance to enter at least one card/characteristic into the discussion.This needs to be (well) facilitated by an experienced coach/agile master.You can take as long as you like for the discussion and could do several discussion rounds, but we would recommend not to have it last for too long (1.5 hours max).After the discussion round it is useful to add an extra step in the game as to make the outcome more actionable:the participants prioritise the top three of characteristics/topics they would like to let go or create.These are the topics they can start working with the very next day!This way you also prevent the session being perceived as just fun, irrelevant and/or without consequences.Because there are fourty-two cards in the game and you won’t discuss them all, the game is most suitable to be played again at a later time – it won’t lose its value.Outcomes retrospective:TIP:  Participants need some time to figure out how to play the game and how to work with the quadrants on the board.  An ice breaker to start with and a small warm up round increases effectiveness and actionable outcome of the game.  Making sure the topics in the discussion round are precisely those that matter most to the organisation and the participants, adds greatly to the usefulness of the session.  You need a skilled facilitator.TOP:  Very nice game to play, offering lots of insight and actionable openings to start your transformation at leadership level.  The game doesn’t take too long, can easily be played several times over time and can easily be fitted into a leadership programme, workshop, curriculum or “heidag”.  It’s a perfect self-assessment game for management teams that are wondering how agile they actually are as a team. :)If you would like to try this game but don’t yet have an experienced facilitator available, don’t hesitate to reach out to us! We’d love to help!More information about the game can be found here."
      },
    
      "culture-2018-07-03-jworks-a-culture-of-learning-html": {
        "title": "JWorks: A Culture of Learning",
        "url": "/culture/2018/07/03/JWorks-a-Culture-of-Learning.html",
        "image": "/img/jworks-culture.jpg",
        "date": "03 Jul 2018",
        "category": "post, blog post, blog",
        "content": "  People often wonder, why should we invest in people, what if they leave?Better ask yourself, what if you don’t, and they stay?The Java unit at Ordina Belgium has always been amongst the most profitable and respected units of the entire group.Yet, we’ve seen a big change in management style a couple years back.While during the financial crisis the primary focus had to be on continuity and securing revenue, the last couple of years have been revolving around the people and their careers.The Java unit has been rebranded to Ordina JWorks, a separate identity with a well-defined mission.We want to help our customer strategically, by co-creating solutions that help them disrupt their respective industry.To do that, we had to become thought-leaders in our field, recognised for our expertise by both the tech community and our customers.In today’s market, hiring only the very best profiles doesn’t scale well.Hence, allowing people to grow is the only way forward.Aside from books and online courses, internal trainings and competence centers are a great way to get people to the next level.JWorks organises at least one training or knowledge sharing session a week.During winter and summer, entire training programs are being organised both for junior, medior and senior profiles.This begs the question though, how do we keep these trainings relevant?The gap between repeating what others have said and found out, and actually performing new qualitative research, is what makes all the difference.Being able to experience learnings firsthand allows people to really dig into subjects and deeply understand the trade-offs and decisions that led to solutions we use today.Part of researching a given topic consists of talking to international subject matter experts.Thanks to the open source community we get to work in, people are very approachable.It’s quite straightforward to get to know core committers, well-known architects or developer evangelists.Yet, the best way to really pick someone’s brain is to meet them face to face.JWorks facilitates this by sending our people to conferences and events around the world.San Francisco, New York, Seattle, London, Barcelona, Las Vegas, Austin, Berlin, Kiev, Riga, Paris, Amsterdam, Budapest, Casablanca, Frankfurt, Washington DC; only a few of the cities we’ve frequented during the past couple of years.We encourage people to take a couple days extra to enjoy the city and do some sight-seeing.Conveniently labelled “learning holidays”, this type of trips really create a community feeling inside the company.If people are having fun, they’ll be much more motivated to go the extra mile.Upon return, the accumulated knowledge gets put into good use by giving input in training material, creating blogposts with differentiating content or by helping to shape customers’ vision and facilitating technical decision-making.We also encourage people to be vocal about the things they love, by giving a workshop, speaking at internal events, or becoming a rock star speaker at international conferences.By becoming public speakers ourselves, we really get to know the nitty gritty details of whatever subject we’re talking about.Taking a seat at conferences’ speaker dinners also helps to network and enter the world of the greatest minds of our industry.This is how JWorks builds a community of highly motivated thought-leaders."
      },
    
      "conference-2018-07-03-agile-devops-summit-brussels-html": {
        "title": "Agile &amp; DevOps Summit Brussels",
        "url": "/conference/2018/07/03/Agile-DevOps-Summit-Brussels.html",
        "image": "/img/agile-devops-brussels-2018/agile-devops.png",
        "date": "03 Jul 2018",
        "category": "post, blog post, blog",
        "content": "  Agile &amp; DevOps Summit Brussels is a one day conference on agile practices and DevOps.In this post, I will take you along to the various talks that were attended.Table of Contents  Agile is (not) only for IT  Agile Maturity  How to bring Agile out of software sector  What we wished we knew before signing up for Agile and DevOps Transformation  Is Spotify becoming the new Xerox?Agile is (not) only for ITThe talk started with explaining that all 12 principles of the Agile Manifesto can be applied to all layers of the organization, it’s not only for the IT department (hence the title). The Agile Manifesto enables everyone to focus on the same objective and towards the same goals.In order to get everyone to focus, it’s important to know in what situation you are. To find out, you can use the Cynefin framework. This framework explains where you belong by differentiating between 4 different domains.  Complex: non-lineair, never complete.  Complicated: analysis is needed, a lot of possibilities, multiple possible answers.  Chaotic: take action, dynamic, nothing is the same.  Obvious: predictable and repeatable, one or few answers.To take a real example: ‘Planning a birthday party for your child’:  If you leave the children all alone the entire afternoon, you have no idea what happens during your absence or what the state of your house will be when you return: Chaos.  If you plan the entire afternoon, with a timetable, to ensure that everything happens as planned: Complicated.  If the children are obedient and don’t do anything wrong at all (not exactly realistic): Simple.  If you set boundaries, rules, adjust continuously: Complex.When you’re in a complex situation, you can apply the agile principles.The other part of the talk was about the Agile Wave and how it affects other parts of your organization other than IT. If the IT department starts working according to the agile principles, there is always an impact on the other domains of your organization.For instance, if you have a team that goes agile, there’s an impact on items like budget, bonuses, costs, collaboration, estimation and so on. So the other domains need to adapt as well.So agile is not only for IT because:  Agile values and principles are applicable in all domains.  Agile helps dealing with complexity, which is present in all domains.  The roots of the agile principles originate from more domains than just the IT.  The agile journey of an organization impacts all layers, not just IT.Agile MaturityThe next talk’s subject was the issue of measuring agile maturity in an organization. Agile maturity can be answered with 3 questions:  Why am I doing it?  What am I doing?  I am convinced about its outcome?If you can answer those 3 questions as an organization, you’re already far ahead of others in terms of agile maturity.To determine the maturity, discuss the following items:  Context: You should know the context of what you’re doing.  Team: What is your motto? Know your team to avoid turnover. Important values for a team: respect, trust and humility.  Discipline: Agile requires much discipline, such as timeboxing, estimating, communicating with the customer, collaborating and so on.  Infrastructure: Must be fast to react to changes. If it’s not, it blocks various agile processes. DevOps is obviously crucial as it will enable faster reaction times.  Ownership: Taking ownership of what you’re doing. Never shift responsibility to someone else.Bring Agile out of the software sectorThis talk touched the same subject as the first talk, but had a bigger emphasis on why an organization should shift towards agile.There are some organizations that don’t like change, they’re not comfortable with it. Yet inevitably, everyone needs to adapt. In general, IT departments are used to change and quickly embrace it, but other departments might not.The biggest investment that will be needed when transitioning is people’s moods and behavior.  So it’s important to state why agile is useful and what the advantages are:  Time to volume: It’s easier to respond to changes in requirements.  Efficiency: People share and collaborate more, which decreases the amount of bottlenecks.  Engagement: You can retain and attract more talent.The process of change traditionally follows the Kübler-Ross model.What we wished we knew before signing up for Agile and DevOps TransformationUnlike the other talks, this one featured a panel discussion about what you should do if you want your Agile and DevOps transformation to fail. A discussion was had with 3 panel members and the rest of the audience.  Just rebrand your middle management          For example, instead of having a regular project manager, now you can have agile project managers.        Ignore resistance and feedback          It doesn’t matter what your employees think, the management knows better.        Discourage experimentation and fire bearers of bad news          Employees shouldn’t be allowed to try new things and if it’s bad news, everyone should ignore it.        Don’t plan for productivity loss with new people          If you add new people to a team, the team should be more productive right away, right?        Add all agile ceremonies on top of existing meetings          If we have more meetings, it means we’re discussing items and are productive.        Do no take minutes, assign no actions          Everyone remembers what was discussed and knows what to do, many days later.        Don’t take metrics          If we don’t see the problems, there aren’t any.        Don’t measure anything apart from badging hours          Obviously, there’s a direct correlation between badged hours and productivity.        Do DevOps with Ops          It’s a fancy name.      Infrastructure should still be separate so it can be controlled.        Create DevOps department to coordinate between Dev and Ops          More control and coordination is better        Ban teams from using their own tools          Management knows best what tools their teams need, all teams need the exact same things.      To be clear, this list contains a fair bit of sarcasm.Spotify becoming the new XeroxThe last talk I attended was a discussion about the agile model of Spotify that is becoming the new Xerox. The term ‘new Xerox’ is a reference to the generic trademark that Xerox became. When you talked about photocopies in the past, you would talk about Xerox as they were the leading organization in that area.Other examples are Bic, Sharpie, Croc and so on.The speaker then started talking about the agile model that Spotify uses in The Good, The Bad and The Ugly terms.The Good  It helps to maintain a team’s autonomy and keep the various teams aligned.  It helps to scale agile by using the concept of tribes, chapters, guilds, squads and so on.The Bad  It’s a mistake that it’s called a model. It’s an engineering culture which typically cannot be mapped one-on-one to another organization.  It was released 6 years ago, which is ancient in agile terms. It has evolved and might be completely different now.  It doesn’t talk about management nor budgets. While agile is less concerned about this, it should still be mentioned if you want the management to follow.  The context that was used by Spotify while creating this model is not the same as the context that is present in your organization.The UglyThe terminology that Spotify uses has some issues:  Tribes: it sounds primitive.  Squad: it sounds military.  Guild: it’s a blocked evolution from medieval times.To complete the talk, he explained the acronym CALMS that is used for DevOps transformations:  C: Culture          Empower your teams.      Roles should be used instead of functions.      T-profiles are the way to go.      Failure should be allowed as long as you learn from your mistakes.      Work together with the business to achieve the desired product.        A: Automation          Everything is code (config, code, tests and so on) and in the pipeline.      Pay attention to things that can go wrong.        L: Lean principles          Constant flow of small changes.        M: Measure to improve          Inspect and Adapt.        S: Sharing          Learning from your peers.      "
      },
    
      "iot-2018-06-27-viroreact-html": {
        "title": "ViroReact: Build cross-platform AR/VR applications for Android and iOS using React",
        "url": "/iot/2018/06/27/viroreact.html",
        "image": "/img/2018-06-26-viroreact/viroreact.jpg",
        "date": "27 Jun 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Basics of ViroReact  Demo app  Lessons learned and conclusion  Extra resourcesIntroductionAugmented Reality is a very interesting space so naturally, we wanted to do something with it.There are many frameworks available for building cross-platform AR/VR applications.We came across ViroReact which uses React and React Native to create immersive VR and AR applications using a single codebase for Android and iOS.This persuaded us to give it a try.In one day, with some tweaking afterwards, we were able to create a simple app to show the status of a meeting room.Basics of ViroReactComponentsThe main building blocks of a ViroReact app are the components.The most important component is a SceneThe scene is the digital environment the user interacts with.All other components live in this scene.&lt;ViroScene&gt;&lt;ViroARScene&gt;Just like many other components, the Scene comes in two variations: ViroScene and ViroARScene.As the name suggests ViroARScene is used for Augmented Reality applicationswhile the ViroScene is meant for Virtual Reality.A ViroSceneNavigator is used to navigate between these scenes.&lt;ViroSceneNavigator&gt;&lt;ViroARSceneNavigator&gt;The SceneNavigator holds a stack of scenes to enable simple transitions from one scene to the next or previous scene.These scenes can be populated with all kinds of componentsFor example a ViroText.&lt;ViroText\ttext=\"Hello World\"\tposition={[0,0,-5]}\tcolor=\"#ff0000\"\twidth={2} height={2}\tstyle=/&gt;Or perhaps a ViroBox, a ViroQuad or a ViroSphere.These components can be modified by setting their properties.One of the most important properties is the position.It takes an array of three numbers as the x, y and z coordinates.There are also more specialised componentsLike a Viro3DObject which lets you include custom made 3D objects in your scene.&lt;Viro3DObject\tsource={require(\"./res/spaceship.obj\")} type=\"OBJ\"\tresources={[require('./res/spaceship.mtl'),\t\trequire('./res/texture1.jpg')]}\tposition={[1, 3, -5]} scale={[2, 2, 2]}\trotation={[45, 0, 0]}\ttransformBehaviors={[\"billboard\"]}/&gt;These 3D objects can be created in your favourite 3D modelling tools.ViroReact supports OBJ or VRX (converted from FBX) formats.The billboard transform behaviour is an interesting feature to make sure the object is always facing the user.A Viro360Image can be set as the background of a scene.A ViroPortal lets the user take a peek into a different scene.It basically is a window to another world.The Viro3DObject included in it acts like the window frame.A ViroARImageMarker reacts when one of the ViroARTrackingTargets is scanned.It will show all the components inside the ViroARImageMarker tag.We have used this in our little app, more on that below.&lt;ViroARImageMarker target={\"targetOne\"}&gt;    &lt;ViroBox position={[0, 0.25, 0]} scale={[0.5, 0.5, 0.5]} /&gt;&lt;/ViroARImageMarker&gt;ViroARTrackingTargets.createTargets({  \"targetOne\" : {\tsource : require('./res/targetOne.jpg'),\torientation : \"Up\",\tphysicalWidth : 0.1 // real world width in meters  },});We wouldn’t see anything without eyes and lightThe Camera component is our eyes.&lt;ViroCamera position={[0, 0, 0]} rotation={[45, 0, 0]) active={true} /&gt;A default camera is provided at the origin position={[0, 0, 0]}.The camera always looks in the negative Z direction.So if you want an object to be visible as soon as the scene loads up, make sure to set its position with a negative Z value position={[0, 0, -5]}. If the object would have a positive Z value it would be placed behind you when you load up the scene.There is also a ViroOrbitCamera where the camera orbits around a certain position, always keeping it in view.Lights are very important components in a scene.Without any light, the user wouldn’t see anything.Luckily a default light is provided when none is defined.We didn’t need lights in our setup but if you want a more realistic or visually stunning experience, I highly recommend you to look into the different lights in the documentation.There are four lighting models: Phong, Blinn, Lambert and Constant.These are the algorithms that calculate what your objects will look like when influenced by light.By default, elements use the Constant lighting model, which means lights will be ignored and the object will show its full color.ViroReact supports four types of light.The ViroAmbientLight is the simplest light.&lt;ViroAmbientLight color=\"#ffffff\"/&gt;It lights up all the objects in the scene equally from every direction.Only the color needs to be set.A ViroOmniLight is comparable to a light bulb.&lt;ViroOmniLight    color=\"#ffffff\"    attenuationStartDistance={2}    attenuationEndDistance={6}    position={[0,-5,5]} /&gt;The light originates from a specified position and moves in all directions.The light slowly fades out if the distance is between 2 and 6, set in the attenuation properties.A ViroSpotLight is comparable to a flashlight.&lt;ViroSpotLight    color=\"#ffffff\"    attenuationStartDistance={2}    attenuationEndDistance={6}    position={[0, -5, 5]}    direction={[0 -1, 0]}    innerAngle={0}    outerAngle={45} /&gt;The light originates from a single point and shines in a cone.The direction and angle of the cone can be set as well as the attenuation start and end distances.Some other properties are available to create the shadows you want.The last type of light is the ViroDirectionalLight.&lt;ViroDirectionalLight    color=\"#ffffff\"    direction={[0, -1, 0]} /&gt;The sun would be the prime example of a ViroDirectionalLight.It shines over the whole scene in a specific direction.To make these objects look good we need MaterialsMaterials are used to place texture images on 3d objects.They can make a long box look like a brick wall or a sphere look like our planet.ViroMaterials.createMaterials({  earth: {\tshininess: 2.0,\tlightingModel: \"Lambert\",\tdiffuseTexture: require('./res/earth_texture.jpg'),  }});There are many properties available for materials.Discussing all of these is outside the scope of this article.The diffuseColor and diffuseTexture are the main color and texture of the material.These can be placed on basic 3D objects like ViroBox, ViroQuad and ViroSphere.Complex Viro3DObjects can have multiple materials.Move them around with AnimationsOur scenes are still lacking motion.That’s where animations come in handy.ViroAnimations.registerAnimations({\tanimateImage:{properties:{scaleX:1.0, scaleY:0.6},\t\t\t\t  easing:\"Bounce\", duration: 5000}});&lt;ViroImage source={require('./res/myimage.jpg')}\t\t   position={[0, -1, -2]}\t\t   animation=/&gt;Animations change the numerical value of properties over time.Typically this is used for the position and scale properties but it can just as well be used for color values or other numerical values.There are five easing types: Linear, EaseIn, EaseOut, EaseInEaseOut and Bounce.These represent how the values change over time.Control the scene with a ViroControllerWith all of the above, we can create an awesome scene but we can’t interact with it yet. The ViroController provides us the ability to interact with our scene.&lt;ViroController reticleVisibility={true} controllerVisibility={true} onClick={this._onClickListenerForAllEvents} /&gt; With the reticle (the blue circle in the middle of the view) the user can select and point to objects and call the function in the onClick property.It’s also possible to use other events like onHover, onTouch, onSwipe, onPinch, onScroll, etc…These events are very useful for UI.There are many more fun componentsApart from the ones we mentioned here, there are many more fun components to include video, sound and particles in your scene.They follow the same principles but each have their own properties.This being built on React Native we can also declare our own components of course!export default class CustomComponent extends Component {\tstatic propTypes = {customProperty: PropTypes.number}    constructor() {super() ;}\trender() {return (\t\t&lt;ViroText text={this.props.customProperty}&gt; );}\t}&lt;CustomComponent customProperty={42}&gt;Just extend from Component, define your properties in propTypes and use it in your scenes.In our demo application, we used this to create our scene but you can create small reusable components too of course!Demo applicationBefore you can build a ViroReact application you need to request an API key from ViroMedia.This should not take more than a few minutes.For our demo application, we decided to use meeting rooms as a use case.Since ViroReact is advertised as a framework for rapidly building AR / VR applications, we wanted to put this statement to the test and try to create an application in one day with no knowledge of the framework at all.Use Case: Meeting room status viewer    Sometimes people want to have a quick meeting or Skype call. They might be standing or passing by a meeting room, and have the ability to immediately check if the meeting room is available for the next 30 minutes.We created an application where the user can view the status of a meeting room in Augmented Reality by scanning the room name/picture.We used Image Recognition in ViroReact to achieve this.Aside from the Image Recognition, we were also curious how ViroReact handles  HTTP requests  UI Updates  User interactionEventually, I created three applications:  A backend application for holding meeting room state (NestJS)  A client application for changing the state of a meeting room (Ionic PWA)  A client application for viewing the state of a meeting room (ViroReact)For this blogpost, I will only discuss how we created the ViroReact application.The AR Meeting room viewer (ViroReact)    ViroReact is built on top of React, so basic knowledge of React is necessary. To get started, you can follow this great free beginner guide by Kent C. Dodds at egghead.io The beginner’s guide to ReactStep 1: Project setupWe began by following the Official Quick Start guide from ViroMedia.Using the react-viro CLI, we generated a ViroSample project.react-viro init ARMeetingRoomViewerStep 2: Create an AR scene for viewing the status of a meeting room.To be able to scan the meeting room we needed a picture of the meeting room nameplate, this will act as the marker to scan the meeting room information.    I placed the marker image inside of the **/js/res** folder.Because the file extension was in capital letters (.JPG), I had to configure this extension in the rn-cli.config.js file inside of the root folder.  getAssetExts() {    return [\"obj\", \"mtl\", \"JPG\", \"vrx\", \"hdr\"];  },Next, I created the actual scene in a file called markerscene.js in the /js/ folder.To be able to scan the image marker, we need two important APIs:  ViroARTrackingTargets  ViroARImageMarker componentWhen the scene initialises we need to setup the Tracking Target(our image marker)mr7 refers to a meeting room name.We call this method inside of the constructor.    setMarker() {        ViroARTrackingTargets.createTargets({            \"mr7\": {                source: require('./res/mr7.JPG'),                orientation: \"Up\",                physicalWidth: 1            },        });    }Then we need to define what to render. The root component will be an AR Scene: &lt;ViroARScene onAnchorFound={this._getInfo}              onClick={this._getInfo}&gt;  ... &lt;/ViroARScene&gt;We bind two events on the ViroARScene component, onAnchorFound and onClick. Every time one of these events occurs, we want to fetch the latest meeting room state.onAnchorFound gets called when the Image Marker has been detected.getInfo() {    fetch('https://rooms.meeting/rm7')        .then((response) =&gt; response.json())        .then((res) =&gt; {            const isTrueSet = (res.isAvailable === 'true');            this.setState({                isAvailable: isTrueSet,                nextMeeting: isTrueSet ? `Next meeting: ${res.nextMeeting}h` : `Free @ ${res.nextMeeting}h`,                text: isTrueSet ? `Available` : 'Not Available'            })        })        .catch((error) =&gt; {            console.error(error);        });}Inside the scene, we want to display the meeting room data when the Image Marker is scanned.We need to use the ViroReact ImageMarker component for this.&lt;ViroARImageMarker target={\"mr7\"}&gt;    &lt;ViroFlexView style={this.state.isAvailable ? styles.containerAvail : styles.containerNotAvail}                  width={3}                  height={3}                  position={[0, 0, 1.25]}                  rotation={[-100, 0, 0]}&gt;        &lt;ViroText text={this.state.text}                  width={2}                  height={2}                  style={styles.text}/&gt;        &lt;ViroText text={this.state.nextMeeting}                  position={[0, -1, 0]}                  width={2.5}                  height={2}                  style={styles.nextMeeting}/&gt;    &lt;/ViroFlexView&gt;&lt;/ViroARImageMarker&gt;The ViroARImageMarker component has a target “mr7” assigned.This refers to the ViroARTrackingTarget we defined in the setMarker() method above.When the target is successfully scanned, all the content of the ViroARImageMarker component will be rendered.In our case, two TextViews positioned with a FlexView.We bind the data we fetched from in our getInfo() method to the ViroText and ViroFlexView components.And these are the styles we defined for the ViroText and ViroFlexView.var styles = StyleSheet.create({    text: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    nextMeeting: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    containerAvail: {        flexDirection: 'column',        backgroundColor: \"#E98300\",        padding: .2,    },    containerNotAvail: {        flexDirection: 'column',        backgroundColor: \"#e91530\",        padding: .2,    }});Our final scene'use strict';import React, {Component} from 'react';import {StyleSheet} from 'react-native';import {    ViroARScene,    ViroText,    ViroConstants,    ViroARTrackingTargets,    ViroARImageMarker,    ViroFlexView} from 'react-viro';export default class MarkerScene extends Component {    constructor() {        super();        // Set initial state here        this.state = {            text: \"Initializing AR...\",            nextMeeting: \"\",            isAvailable: true        };        this.setMarker();        this._getInfo = this._getInfo.bind(this);    }    setMarker() {        ViroARTrackingTargets.createTargets({            \"mr7\": {                source: require('./res/mr7.JPG'),                orientation: \"Up\",                physicalWidth: 1            },        });    }    render() {        return (            &lt;ViroARScene onAnchorFound={this._getInfo}                         onClick={this._getInfo}&gt;                &lt;ViroARImageMarker target={\"mr7\"}&gt;                    &lt;ViroFlexView style={this.state.isAvailable ? styles.containerAvail : styles.containerNotAvail}                                  width={3}                                  height={3}                                  position={[0, 0, 1.25]}                                  rotation={[-100, 0, 0]}&gt;                        &lt;ViroText text={this.state.text}                                  width={2}                                  height={2}                                  style={styles.text}/&gt;                        &lt;ViroText text={this.state.nextMeeting}                                  position={[0, -1, 0]}                                  width={2.5}                                  height={2}                                  style={styles.nextMeeting}/&gt;                    &lt;/ViroFlexView&gt;                &lt;/ViroARImageMarker&gt;            &lt;/ViroARScene&gt;        );    }    _getInfo() {        fetch('https://rooms.meeting/rm7')            .then((response) =&gt; response.json())            .then((res) =&gt; {                const isTrueSet = (res.isAvailable === 'true');                this.setState({                    isAvailable: isTrueSet,                    nextMeeting: isTrueSet ? `Next meeting: ${res.nextMeeting}h` : `Free @ ${res.nextMeeting}h`,                    text: isTrueSet ? `Available` : 'Not Available'                })            })            .catch((error) =&gt; {                console.error(error);            });    }}var styles = StyleSheet.create({    text: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    nextMeeting: {        fontFamily: 'Arial',        fontSize: 32,        flex: .5,        color: '#FFFFFF'    },    containerAvail: {        flexDirection: 'column',        backgroundColor: \"#E98300\",        padding: .2,    },    containerNotAvail: {        flexDirection: 'column',        backgroundColor: \"#e91530\",        padding: .2,    }});module.exports = MarkerScene;Step 3: Load your marker scene on application startupNow that we have our scene, we can load it on start-up.In the root folder you can find the app.js file.Here we can define which scene to load when starting up the application.Assign your API key from ViroMedia.const sharedProps = {    apiKey: \"6E2805CC-xxxx-4Ex0-8xx0-02xxxxxxx\",};Import your marker scene.const MarkerScene = require('./js/MarkerScene');Render your AR Scene.Final result:import React, {Component} from 'react';import {ViroARSceneNavigator} from 'react-viro';const sharedProps = {    apiKey: \"6E2805CC-xxxx-4Ex0-8xx0-02xxxxxxx\",};const MarkerScene = require('./js/MarkerScene');export default class ViroSample extends Component {    constructor() {        super();        this.state = {            sharedProps: sharedProps        };        this._getARNavigator = this._getARNavigator.bind(this);    }    render() {        return this._getARNavigator();    }    _getARNavigator() {        return (            &lt;ViroARSceneNavigator {...this.state.sharedProps}                                  initialScene=/&gt;        );    }}module.exports = ViroSample;Demo        Lessons learned and conclusionWithout prior knowledge it was a bit challenging for us to get our development environment set up correctly.We had a lot of issues with debugging and cached builds. When we had issues, it was hard to tell if the problem was with React Native or ViroReact.Debugging was a big challenge for us and the react native development tools don’t seem to work well with ViroReact.The documentation is quite expansive but it was not always up-to-date.But aside from that, once we were aware of which parts of the dev tools that worked and which ones that didn’t, we were able to quickly build an AR application.Extra resources  An introduction to virtual and alternate reality  Documentation  Sample applications  Beginner’s guide to React"
      },
    
      "conference-2018-06-27-craft-conf-html": {
        "title": "Craft Conf 2018",
        "url": "/conference/2018/06/27/Craft-Conf.html",
        "image": "/img/craft-conf-2018/Craft-Conf-2018.png",
        "date": "27 Jun 2018",
        "category": "post, blog post, blog",
        "content": "  Craft Conf is a two day conference in Budapest, aimed at talks surrounding the ‘Software as a craftsmanship’ idea.JWorks was present this year on the 10th and 11th of May and we would love to give you an overview of some of the talks we attended.Table of Contents  Power Games for High-performance Team Culture, Psychological Safety, and EI - Richard Kasperowski  Perceived performance: The only kind that really matters - Eli Fitch  SWARMing: Scaling Without A Religious Methodology - Dan North  Seven (plus/minus two) ways your brain screws you up - Joseph Pelrine &amp; Jasmine Zahno  Designing a high-performing team - Alison Coward  Estimates or NoEstimates: Let’s explore the possibilities - Woody ZuillPower Games for High-performance Team Culture, Psychological Safety, and EI - Richard KasperowskiRichard Kasperowski is a speaker, trainer, coach and author focused on high performance teams.Richard is the author of The Core Protocols: A Guide to Greatness.He leads clients in building great teams that get great results using the Core Protocols, Agile, and Open Space Technology.During this talk, Richard provides an overview of some of the conditions that need to be met for the team to become a high performance team.In order for this to happen, it’s very important that companies realize that they need to focus on getting the right people together instead of focusing on achievements.Hierarchy and power distanceA company needs to achieve the best possible team culture.But what does this mean?What is culture?  The collective programming of the mind which distinguishes the members of one group or category of people from another. - Geert HofstedeRichard talks about six different culture dimensions, researched by Geert Hofstede:  Power distance:Expresses the degree to which the less powerful members of a society accept and expect that power is distributed unequally.  Individualism versus collectivism:Individualism can be defined as a preference for a loosely-knit social framework in which individuals are expected to take care of only themselves and their immediate families.Collectivism, however, represents a preference for a tightly-knit social framework in which individuals can expect their relatives or members of a particular group to look after them in exchange for loyalty.  Masculinity versus femininity:The masculinity side of this dimension represents a preference in society for achievement, heroism, assertiveness and material rewards for success.This society is more competitive.Its opposite, femininity, stands for a preference for cooperation, modesty, caring for the weak and quality of life.This society is more consensus-oriented.  Uncertainty avoidance:This dimension expresses the degree to which the members of a society feel uncomfortable with uncertainty and ambiguity.The big issue here is how a society deals with the fact that the future can never be known: should we try to control the future or just let it happen?  Long term orientation:Every society has to maintain some links with its own past while dealing with the challenges of the present and the future.Societies who score low on this dimension prefer to maintain time-honoured traditions while viewing change within the society with suspicion.Those with a culture which scores high, on the other hand, take a more pragmatic approach: they encourage efforts in modern education as a way to prepare for the future.  Indulgence versus restraint:Indulgence stands for a society that allows enjoyment of basic and natural human drives related to enjoying life and having fun.Restraint stands for a society that suppresses enjoyment of needs and regulates it by means of strict social norms.The talk further focuses specifically on the power distance dimension.Power distancePower distance entails hierarchy, the gaps between people in the company.A lot of power distance means a lot of hierarchy.Next, we did some activities that let us experience hierarchy.The first one was with a partner of our choice where one of us was the ‘leader’ and the other was the mirror.Naturally, the mirror had to imitate everything the leader did.For the next exercise one of the two persons was again the leader, but now the other person had to keep his or her nose at a distance of 10 centimeters from the palm of the hand of the leader at all times.The last exercise was with a few volunteers from the audience where one person played the CEO, a few people played the Senior Vice Presidents and some people played the Vice Presidents.The CEO stood in the middle of the room and directed the SVPs, the SVPs directed the VPs.The people who were being directed were again said to keep their noses at a distance of 10 centimeters from the palm of the hand of the person that directed them.These were very interesting activities that showed that having too much hierarchy and people who direct other people can lead to chaos when for example the leaders moved too fast for the others to be able to follow the directions.Directions are much easier to handle when there are less people involved and more trust is created.What do we need to build a high performance team?At the core, there are six building blocks for high performance teams:  Positive bias: Non-negativity, accept a positive mindset  Freedom: Every team member can choose and make decisions by themselves  Self awareness: Listen to yourself, use check-ins to discover, articulate and achieve what you want  Connection: Listen to others, connect great people into a great team and support each other towards a common goal  Productivity: Focus on results  Error handling: Ensure we are maintaining the positive bias, freedom, self-awareness, connection and productivityHierarchy and power culture erode high performance so when setting up and trying to maintain a high performance team, these building blocks are essential.To achieve high performance, increase the team’s emotional intelligence and psychological safety.This will decrease power distance and hierarchy.The Core Protocols are a way to reach this goal.More info on the Core Protocols can be found here.Perceived performance: The only kind that really matters - Eli FitchEli Fitch is a frontend developer with a passion for web performance, animation and all things 3D.When talking about performance, we need to account for both objective time and subjective time.Usually, we tend to optimize for objective time by using tools like lazy loading.But how feasible is this?Even a 20% increase in objective speed is unnoticeable to the user.So we have to aim for an increase of 30% or even more.Of course this is not easy at all, especially when taking into account that when working with multiple teams (possibly over different time zones), all the teams have to align on this.So how do we get the user to perceive an increase of performance?We focus on the subjective time!Active vs passive phase:What does a passive phase entail?Our passive phase kicks in when we are waiting for something to happen, say for our water to boil.Time spent in passive phases feels ~36% slower!There are two ways to prevent users from getting bored during a passive phase and keeping them interested enough to wait:  Keep users in an active state  Make passive states feel fasterHow do we keep users in an active state?There are three ways to achieve this:  Don’t tell users they’re waiting: Use loaders wisely because they can prematurely put users in a passive state  Respond to users immediately, even if nothing is happening  Keep users in flow with more responsive event listenersTo respond to the users immediately, we can implement an optimistic UI.99% of requests succeed so why not take advantage of this by first updating the UI and only then doing the actual request.We can also do our best to react as soon as the user signals intent.For example, why use the onclick event when the onmousedown event exists as it shows intent much earlier?This will provide you with a nice 100-150 millisecond head start.This is also usable on touch devices.Using :active animations also buy you more time.An animation that lasts ~200 milliseconds will provide you with 50 milliseconds extra time keeping the user in an active state.Onto topic two: how to ‘unsuck’ passive states.A wait of 1-4 seconds is long enough to lose people’s interest.There are three ways of preventive loss of interest:  Use the right loading animation  Adapt the loading scheme to the user  Distract with shiny objects!Uncertain waits feel longer so make sure to use progress bars and loading animations when appropriate.For example: bands with accelerating bars feel 12% faster!What about spinners?  “Meh” - Eli Fitch  Less than ~1 second wait: Don’t indicate loading  More than ~2 second wait: Use progress bars!Spinners are only useful between a 1 second and 2 second wait.Don’t forget that most progress bars are fake!This is due to the connection differences between the user’s connection and the backend.However, we can use adaptive loading.Measure the actual requests done!You do need baseline times to know how long to expect it to run.Of course this needs to be normalized for the resource or payload that needs to load.Again, adapt the loading scheme to the user that is requesting data.For example, you can check the user’s connection to give him a personally optimized experience.We can also learn a lot from game developers.Remember FIFA who made you play a mini football game while loading the game in the background?Predictive preloading:What if we could predict the user’s intent?One easy to setup option is to start loading the app and data in the background when a user has just started entering credentials in a login form.This quick win will give you a 4 second head start!Another option is to exploit behavioural quirks:  People tend to watch hover animations: Fancy hovers buy you ± 600 millisecond extra time  People slow down when approaching the target: Load on mouse deceleration!When combining above two techniques (hover animation + futurelink) we can get a ± 1.5 second head start.But: Use predictive preloading wisely!You will get it wrong some of the time. Do some dry runs first and try to mitigate risk by using metrics to improve.A quick summary  Perceived performance is low hanging fruit since you can provide the user with immediate, accurate and live feedback  Tailor your loading solution to individual users  React as soon as users signal intent  Introduce predictive loading bit by bit, implement it at the places in your app where it has the most impact  At the end of the day, what matters is how it feels.Eli also covers this talk on his website.SWARMing: Scaling Without A Religious Methodology - Dan NorthDan North has been coaching, coding and consulting for over 25 years, with a focus on applying systems thinking and simple technology to solve complex business problems.Business stakeholders, developers, infrastructure, the project management office and change groups don’t understand each other. What are they doing wrong?They are aiming at the wrong target!Wrong target: Cost accounting!Large businesses tend to look only at costs and profits since they are too big to be able to keep an eye on everything in the organization.Local performance targets are viewed as extremely valuable, such as reducing head count or sticking to the budget.Right target: Throughput accounting!The whole business creates value.People in the organization are either directly creating value, or they are at least enabling others to do so.The business tries to identify and resolve bottlenecks.They care about lead time and throughput: How quickly from identifying a need can they meet that need and how much value can be pushed through the organization?How can they reach the right target?  “Agile will save us!” - EveryoneThe two people still waiting for Godot might help you with this one.Agile is no holy grail and people desperately holding on to the agile transformation will realize this soon enough.Q: How do we always end up at, “Whatever we did was okay, but somehow it stopped working”?A: Some things are just inevitable:  Degradation: things start to wear out because of time  Dysfunction: once things wear out, they stop working  Expiry: after they stop working, things start to decay and fall awayHowever, these also stimulate positive change: degradation stimulates the idea of maintaining things and transforming them.Dysfunction drives innovation and challenging things and expiry stimulates creating and starting over.Why does this happen?  Change drives the need to adapt. If we’re not adapting, things are going to fail.  Interdependency drives the need to collaborate.  Imperfection drives the need to iterate.Surprise, surprise: adaption, collaboration and iteration are the drivers of Agile and Lean methods!What are our options?SWARM!ScalingWithoutAReligiousMethodology  religion (n): The structures, constraints and rituals we humans accrete around a kernel of faith because we crave answers and we fear uncertainty. - Dan NorthThe table stakes are:  Education: we need to learn some new tricks.  Practice: it’s not enough to learn them, we also need to implement them (possibly with a few failures in the process and trying again and again).  Time: minimum 3-5 years to have an impact.  Investment: time costs money.  Influence: we need to reach up and down in the organization. Everybody needs to participate.  Communication: needs to be in the plan from the get-go or it will not work.  External help: both Amazon Prime and Netflix run on AWS. They don’t block each other, they learn from each other.  Leadership: needs to be consistent, they need to be invested and resilient.If you don’t have these factors in place, SWARM won’t work.These are based on simple principles:  People are basically good. “Everyone is trying to help”. Assume this is always true.  Sustainable flow of value is the goal. We need to learn new metrics and techniques.  Theory of Constraints: only tackle one constraint at a time.Underlying principles:      Visualize -&gt; stabilize -&gt; optimize!You cannot change a system if you don’t know what that system looks like, so visualize!Use event storming, value stream mapping or whatever you need.Stabilizing means that even if the system is horribly bad, you want it to be consistently horribly bad because only then can you observe what impact your changes actually have.        Start small, get data.    “If we have data, let’s look at data. If all we have are opinions, let’s go with mine!” - Jim Barksdale  Learn from mistakes, iterate.Summing it up  Don’t be fooled! It’s easy to believe that this time will be different.  You can’t defeat the universe.Mastery is understanding how to work with the grain. Don’t try to fight all the universal, inevitable things.That means adapting, iterating and combining techniques for your context and the changes around you.  There is no magic formula! But there definitely is hope.  This all takes education, time, practice and other things like investment and leadership.Seven (plus/minus two) ways your brain screws you up - Joseph Pelrine &amp; Jasmine ZahnoJoseph Pelrine is a senior certified Scrum Master Practitioner and Trainer as well as one of Europe’s leading experts on eXtreme Programming.Jasmine Zahno is an agile coach who is passionate about the people side of product development. Her master’s degree in organisational psychology uniquely qualifies her to deal with the human issues that arise when the agile paradigm collides with traditional organisational structures.Joseph and Jasmine talk about a few subjects that they found to be powerful in understanding the complexity of ourselves and software craftmanship.The first of these subjects is that men find women who wear red to be more attractive.Men actually don’t realize this.A woman in red also triggers an exceptional reaction in other women.These women tend to react more aggressive towards women who wear red.Also, men who carry a guitar are found more attractive than men who carry a tennis racket.The next part of the talk will cover some subjects about our psychology that are relevant to use in our field, but also just fun and interesting topics.FACT: We use more than 10% of our brain. Even when we sleep, we use more.Willpower?Willpower is the ability to resist short-term temptation in order to meet long-term goals.Each day, we make 227 choices.These are all chances to follow your long-term goals.This has absolutely nothing to do with your intelligence and everything with willpower.Willpower determines academic successes over intelligence.Willpower deteriorates!After multiple choices that required willpower, your willpower will start deteriorating and you will start making worse decisions.This is actually something that supermarkets use.The fruit and vegetables aisles are always presented to you first whilst you pass the sugary items right before checkout, when your willpower is at its lowest.How to boost your and your team’s willpower?  Establish motivation - for example, align the action items in a retrospective to the team’s motivations  Focus on one goal at a time  Be authentic  Express your emotions - hiding your emotions deteriorates your willpower  Physical exercise - it will actually increase your willpower!  Eat regularly - for example, foresee fruit instead of Snickers to keep up the willpower since some will try very hard to not eat that Snickers bar  Mindfulness practicesRelative estimating =/= estimating!Social compliance is very important to keep in mind during planning poker.Once the numbers are out there, it will influence the people who have not yet voted.See the example of the five subjects to say which drawn lines are the same length.Among these five people there is one actual test subject while the other four are deliberately giving the wrong answer.The test subject will actually adjust his answer to match the others’ answers even though it is clearly the wrong answer.There are multiple factors that influence our estimations.Let’s discuss a few.Seven plus/minus twoThere’s actually a limit on our capacity to process information.This means that the number of objects an average human can hold in working memory is 7 ± 2.This is frequently referred to as Miller’s law.It has nothing at all to do with memory.It has everything to do with the way we make decisions.Linguistic primingThe way the Product Owner formulates the story has a big impact on how it is estimated.Let’s say for example that two cars had an accident.The following five words that were used in the experiment gave a different result:  Collided  Bumped  Contacted  Hit  SmashedThe question here is which word would give the highest estimate of speed?The answer is the following, where 1 represents the highest estimate and 5 the lowest:  Collided - 2  Bumped - 3  Contacted - 5  Hit - 4  Smashed - 1We are not as good with numbers as we think we are  We tend to ignore the base rate:If, for example, the yellow taxis take up 80% of all the taxis in Budapest and the red ones take up 20%, when we see 10 yellow taxis pass by we will assume that the next one will be red even though the actual chance is still 20%.  The law of small numbers:We tend to believe small numbers more than large numbers because it’s easier for us to process  We overlook the statistics when a story is involved because we tend to be more emotional than analyticalJoseph and Jasmine also refer to the Monty Hall problem:  Suppose you’re on a game show, and you’re given the choice of three doors:Behind one door is a car; behind the others, goats.You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat.He then says to you, “Do you want to pick door No. 2?”Is it to your advantage to switch your choice?The answer here is actually ‘Yes!’, however counter intuitive that may be.We are not good psychologistsThe Dunning-Kruger effect: The less intelligent a student was, the smarter they thought they actually were.Reduced intelligence leads to a reduction in the ability to self-reflect on their own intelligence.Working in teamsWe have the tendency to overemphasize personal characteristics and ignore situational factors when judging others’ behaviour.  When we are late, it’s because we have a good excuse  When a team member is late, he was probably just lazyIKEA effectWe have the tendency to overvalue the things we build ourselves.One example here is using pancake mix.These mixes didn’t sell properly until people were actually required to add their own set of eggs to the mix.To finish, Joseph and Jasmine showed us and discussed some more interesting research papers about the human psychology.Their talk is available on the Craft Conf website.Designing a high-performing team - Alison CowardAlison Coward is the founder of Bracket and author of “A Pocket Guide to Effective Workshops”. She is a strategist, trainer and workshop facilitator. With over 15 years of experience of working in, leading and facilitating creative teams, Alison is passionate about finding the perfect balance between creativity and productivity.Team work today is cross-functional and self-organizing.AirBnB for example has core teams and expand the team with the necessary expertise from within the organization when necessary.A team is made up of multiple individuals which creates a new team culture when putting these teams together.The team’s dynamics also tend to change on different projects.So we cannot apply a preset way of working to a new team or a familiar team working on a new project.Three principles for creating High Performance Teams  Can your team learn to work better together?With a fixed mindset you believe that talent is fixed.However, with a growth mindset, you believe that anyone can improve with practice and persistence.See the value of continuous learning.These teams were more successful in the past and tend to perform much better.They challenge each other and learn together.Don’t forget that this can actually be developed.Each team has the potential to improve.   What new ways of working can you create together?Apply a design approach and a design mindset. Design a way of working that works for that team!   How will you and your team start to work differently?Behaviour change and building new team habits. What team habits, what rituals can we create to help our teams work better together?Two factors for designing effective teamsCommunication &amp; trust.  How you work together is more important than what you’re working on and who you are working with.MIT Human Dynamics LabThe research conducted by the lab concluded the following:The most effective teams  Communicate frequently  Talk and listen in equal measure  Engage in frequent informal communication  The conversations are dynamicGoogle’s project AristotleA few years ago, Google was very interested in finding out what the factors were that made up the most effective teams.They concluded the following:  The people could see the impact of the work they were doing  Meaning of work, they could see why they were doing the work  Structure and clarity, everybody knew who was doing what and what they were working on  Dependability, they knew they could depend on each other  Psychological safety, the individuals in the team were able to make mistakes and take risks in front of each otherIt had nothing to do with IQ and individual talents of the people in the team and everything to do with how they worked together.Self-awareness95% of us think we are self-aware, while actually only 10-15% is!Self-awareness exercise:  What time do you naturally wake up?  When are your most productive hours?  When do you get your best ideas?  What does your ideal work day look like?We can use these answers to set up new ways of working within the team.We can do this during the project kickoff:  Share team expertise, share what your role is and what you can bring to the table  Clarify the roles, everyone needs to know who can do what work  Talk about how you will work together, how you will meet, when you will meet, how to communicate, etc.Organize better meetings and workshopsMake your meetings count!  What is the purpose of the meeting?  What is the best format of the meeting? How long do we need? Do we need to split up the meeting?  Set meeting rhythmsAmazon’s two pizza teams:If a team is too large to be fed by two pizza’s, then the team is just too large.Google Ventures Anxiety PartiesEvery three or four months they would get together with a list of all the things that were bothering them about their performance and would share that with their team.An example of this might be that they weren’t sure if they were spending too much time going to conferences or if they’re working too slow.They would share these concerns during the anxiety party and the team members could respond whether they felt this actually was an issue or not to them.Someone could then find out that what they saw as an issue wasn’t being considered an issue at all by the rest of the team, or perhaps even vice versa.These conversations build trust and better connections within the team.Take inspiration from workshopsGreat workshops have the following in common:  Collaboration  Creativity  Equal contribution  Good content  Clarity (on what needs to be done)  MotivationThe next paperclip exercise shows you how to make your meetings more effective:The first question Alison asked us was, ‘Come up with as many ideas as possible on what you can do with a paperclip.’This question was followed with, ‘Which idea would make a good business proposition?’Two different activities came up during this exercise:  Divergent thinking: Trying to come up with as many ideas as possible, the focus was on the quantity.  Convergent thinking: Applying criteria to your ideas and being critical.It’s very important to keep these two activities separate during your meetings.Our goal is to have productive conflict and constructive discussions.Watch out for group thinking by making sure everybody gets heard, introverts and extroverts alike.Equal contributions are very important!Don’t forget about check-in rounds! Use the first five minutes of a meeting to ask a personal question to encourage equal contributions and build trust in the team.Team habitsHow do you change behaviour?  Start small!One step at a time works best instead of a big bang!  “Implementation intention”: Make a plan!When you have these team habits in place, it still needs a constant review to keep improving.We need the following loop:  Design -&gt; Test -&gt; Iterate -&gt; RepeatStarting a new High Performance Team  Determine how you meet  Determine how you share ideas and how you can learn from each other  Social times, find ways to get to know each other  Alone time, don’t only think about collaborationSummary  Team work is changing  Having a growth mindset, design approach and behaviour change  Make your meetings count  Build a ‘workshop culture’  Create better team habitsMore information on how to run effective workshops can be found here.Estimates or NoEstimates: Let’s explore the possibilities - Woody ZuillWoody Zuill has been programming for 30+ years and works as an Agile Coach and Application Development Manager. He and his team are the originators of the Mob Programming approach to teamwork in software development and he is considered one of the founders of the “#NoEstimates” discussion.The first part of Woody’s talk is about his own personal experiences.He was involved in a project that included over 200 developers.He concluded that the same ‘lesson learned’ kept popping up after each iteration: ‘Our estimates need to be better!’It was always the same lesson learned, there was never any improvement. Woody calls this the ‘Cycle of Continuous No-Improvement’.How does one get out of this cycle? Is it safe to question the status quo?There is a bigger idea behind these questions:  “The object isn’t to make art, it’s to be in that wonderful state which makes art inevitable.” - Robert HenriIf you set up the environment for good things, then good things will happen.Woody’s suggestion was very simple:How about working with no estimates?The tweet that started it all in 2012:What is an estimate?Working definition: An estimate is a guess of the amount of time (usually work time) to create a project, a feature or some bit of work in developing software.We use estimates to help us make decisions and spark a conversation (to ultimately make a decision).Afraid of changeWhy is it that we want control and certainty over time, cost and schedule?We don’t need help to make decisions, we need help to make good decisions.Are estimates then really the only way to make them?Are we even able to draw the right conclusions?Perhaps ‘on time’ and ‘on budget’ are actually not a good measure of the results of our decision if we had to cut in our feature to be able to deliver?But why hold on to estimates?  “Fear of losing control is a big barrier for change.” - Bjarte BogsnesWe tend to hold on to what we know.A few quotes in favor of breaking the cycle:  “Alternative to estimates: do the most important thing until either it ships or it is no longer the most important thing.” - Kent Beck  “As teams progress they first struggle with estimation, then can get quite good at it, and then reach a point where they often don’t need it.” - Martin FowlerWhen starting a project, requirements are handed over to the development teams in an orderly fashion, very neatly organized and containing everything that needs to happen.Or so the stakeholders like to think.The project itself, however, usually ends up being a lot more disorganized with features being added, removed or changed with no way to tell early on what the end result will look like.Then why try to get estimates for the project if most of the requirements tend to change anyway?  “It’s in the doing of the work that we discover the work that we must do.” - Woody ZuillRather than size and smallness, look for the following qualities in a story:  Potentially valuable  Understandable  Cohesive - does it belong together  Distinct - is it clearly separate from other stories-&gt; These don’t require estimates! When these qualities are found, we have something we can work on.The Twelve Calculations example  80% of the use of the app comes from 20% of the features  80% of the use of a feature comes from 20% of its implementationThis means that with only 4% of the implementation effort (20% of 20%), we can cover the 20% of the features being actively used.Let’s try to have many, small, inexpensive attempts at value.Let’s do, discover, validate and steer.This is what being agile is all about!What do you think would have more payoff?Turning up the goods on getting our estimates better?Or turning up the goods on our ability to rapidly deliver potentially useful software?Let’s learn to control our urge to control things.Let’s quit worrying about whether we will get done in three months for now.Let’s get good at being done everyday.On Woody’s website, you can find more info on his take on estimates.Interested in more?As we were not able to attend all the talks at CraftConf, we only covered the ones we’ve attended in the blogpost above! Hope you find this blog post as interesting as we did the conference!If you’re interested in more details; all the talks were recorded and can be found on the CraftConf website.Thanks for reading, everyone!"
      },
    
      "conference-2018-06-26-spring-io-2018-html": {
        "title": "Spring IO 2018",
        "url": "/conference/2018/06/26/Spring-IO-2018.html",
        "image": "/img/spring-io-2018/spring-io.jpg",
        "date": "26 Jun 2018",
        "category": "post, blog post, blog",
        "content": "Spring IO is back!Marked in red on the calendar of every JWorks consultant: the yearly edition of Spring I/O.This year, we weren’t going to wait for the explicit approval of our manager and we ordered 27 early bird tickets as soon as we could and booked our flights to sunny Barcelona!It promised to be a special edition, since everything was gonna be bigger and better: the venue, the speaker roster, the food, the atmosphere.  The Palau de Congressos de Barcelona is a much bigger venue than the one we’re used to from previous years.This is why the organizer Sergi Almar was able to accomodate 1000 attendees this year (twice as much as the year before!), which shows how much interest there is in the Spring community.It was a really good location, it has ample space to grow in the coming years and the catering was also of good quality.Next year we’ll most likely get the same venue, but the event will probably overlap with the nearby Barcelona International Motor Show, which takes place every two years.Free test drives during a conference? Yes, please!We’ll talk about some of the presentations this year, but it is definitely not a complete list.There were so many interesting talks, we’re actually going to need quite some time to rewatch them all on Youtube!Let us know if we missed anything by filing an issue or contacting us at our general JWorks email.We will probably still update this blogpost with other talks.Links to videos and resources might still be added as they become available on the Spring I/O Youtube channel.  Michael Plöd: Implementing DDD with the Spring ecosystem  Mark Heckler: Migrating legacy enterprise Java applications to Spring Boot  Andreas Falk: Spring Security 5 Workshop  Juergen Hoeller: Spring Framework 5 - Hidden Gems  Ray Tsang: Google Cloud Native with Spring Boot  Simon Baslé: Flight of the Flux  Nakul Mishra: Spring Kafka - One more arsenal in a distributed toolbox  Oliver Gierke: Breaking Down Monoliths Into System of Systems  Tommy Ludwig: Observability with Spring based distributed systems  James Weaver: Machine Learning exposed: The fundamentals  Jeroen Sterken &amp; Kristof Van Sever: Testing every level of Spring microservices application Day 1: Talks &amp; WorkshopsImplementing DDD with the Spring ecosystem by Michael PlödAfter the keynote session, one of the first talks was given by Michael Plöd.He talked about implementing Domain-Driven Design (DDD) using the Spring ecosystem, leveraging various Spring technologies such as Spring Boot, Spring Data and Spring Cloud.Michael attributed the inspiration for and ideas around DDD to the following two books:  Domain-Driven Design: Tackling Complexity in the Heart of Software, by Eric Evans.  Implementing Domain-Driven Design, by Vaughn Vernon.Domain-Driven Design is currently a very popular way of implementing and looking at microservices.However, he immediately made an important disclaimer:  Everyone should be aware that DDD is not a silver bullet to be used in all projectsOne should not force DDD on problems that aren’t suited for it.Another important thing to remember is to model your microservices along business capabilities.If your microservices are highly coupled on a business level, all that fancy technology in Spring Boot won’t help you.We will use Strategic Design to find a solution that takes into account business capabilities.Bounded ContextEvery sophisticated business (sub-) domain consists of a bunch of Bounded Contexts.We can, for example, create linguistic boundaries using Bounded Contexts if the solution has two types of “accounts”: a BankAccount and a UserAccount.Each Bounded Context contains a domain model and is also a boundary for the meaning of a given model. We don’t nest Bounded Contexts.Inside of a Bounded Context, it’s important to not repeat yourself.On the other hand, between several Bounded Contexts, repeating yourself is allowed for the sake of decoupling.Tactical DesignSystems, and this applies to both monolithic and microservice architectures, should be evolvable.DDD offers a set of patterns, which are the internal building blocks of the Tactical Design part of DDD, that helps us in this regard.Michael now talked us through each of these concepts:  Aggregates:          Entities      Value Objects        Factories  Repositories  ServicesEntitiesEntities represent the core business objects (not data objects) of a Bounded Context’s model.Each of these has a constant identity which should not be your primary database key but rather a business key.Each Entity also has its own lifecycle.Value ObjectsValue Objects derive their identity from a combination of various attributes.As an example, Michael brought up the representer object he was holding: it costs 80 euros so this representer object could be identified by the value of 80 and the currency Euros. We do not care about which ‘80 euros’.Value Objects do not have their own lifecycle: they inherit it from Entities that are referencing them.It’s also important to note that for example a Customer can be an Entity in one Boundary Context but be a Value Object in a totally different Boundary Context.Take note that your DDD Entity is not your JPA Entity.Because the JPA Entity is a data entity while the DDD Entity is a business entity. Don’t mix these types.AggregatesAggregates group Entities and Value Objects together.The Root Entity is the entry point in terms of access to the object graph and for the lifecycle.For example, you aren’t allowed to enter a loan application form through the loan for instance: you would also have to go through the loan application form.Best Practices for architecting AggregatesSmallPrefer small Aggregates that usually only contain an Entity and some Value Objects. Don’t build big reference graphs between Aggregates.Reference by identityDo not implement direct references to other Root Entities. Prefer referencing to identity Value Objects.One transaction per AggregateAggregates should be updated in separate transactions which leads to eventual consistency.Consistency BoundariesTake a look which parts of your model must be updated in an atomically consistent manner.Best practices for implementing AggregatesCode can be found on the author’s DDD-with-Spring GitHub project where he implemented a credit loan application consisting of three Spring Boot applications.VisibilityDon’t just make everything private and expose everything with public getters and setters.This is, in Michael’s words, the “shortcut from hell” because you aren’t doing information hiding and are exposing everything to the outside world.ReferencesHow do we hook these up?  Referencing them from one Value Object to the other.  Create intermediary Value Objects to bind them together.Michael prefers Aggregates that do not reference themselves.They are hooked together with a few shared Value Objects which leads to more decoupling.There are 4 Aggregates in the application and we add a Value Object, personID, to hook AgencyResult and Applicant together.The ApplicationNumber object brings Applicant, Financial Situation and ScoringResult together.Keep your Aggregates Spring free.Aggregates should be plain old Java.PackagesWhen working with Aggregates, place each Aggregate in its own package and work with package level visibility in terms of information hiding.Creation of Aggregates: there are two options  Use the Root Entity directly.  Explicitly create an aggregate concept around your Entities and Value Objects.Make up an educated decision of your own.Builder pattern.The Builder pattern works very well with Aggregates as a substitute to the DDD factory. All Aggregates have Builders in this author’s project.Use an annotation @Aggregate and @AggregateBuilder.Why?To have a code review system in place that checks whether Aggregates are publicly visible and other non-Aggregate classes are packaged protected.Michael recommends ArchUnit, a unit testing tool for software architectures to verify visibility of classes and other architectural rules.Application ServicesThe ScoringApplicationService class holds a service that orchestrates between a lot of Aggregates.RepositoriesIn Spring Data, one uses Spring Data JPA repositories with JPA Entities. But remember these JPA entities shouldn’t be your DDD Entities.ArchitecturesThe hexagonal onion architecture is not your only option and is not suitable for everything.CRUDIf you use a CRUD architecture, Spring Data REST or a context that doesn’t run on business Entities or Aggregates may be suitable.Query Driven contextsAll the logic resides within queries.Domain EventsFor communication between Bounded Contexts there are two possible differentiations:  Orchestration.  Choreography.Orchestration is about synchronous calls going somewhere.Choreography is about events: domain events, event sourcing and event storming.Choreography turns around the call flow so for example: the credit application submits a Credit Application Submitted Event and the scoring component reacts on that Event.You model the information about your activity as a flow of discrete events.Options for Event PayloadFull Payload.Put everything we filled out in the credit application in there and work with it.REST URL.Use RESTful URLs to REST resources for the event; not the Spring Data REST repository.  Empty.  Mix.InfrastructureApache Kafka or message brokers are not the only options for infrastructure. You can also work with:  Brokers (for example Kafka or RabbitMQ): use Spring Cloud Stream.  HTTP Feeds (for example Atom): use Spring MVC with Quartz and Rome libraries.  Internal Event Bus: use Spring Application Events. Eventing within same JVM, so not using an external system.The credit application offers a HTTP feed using Atom that provides new credit agency ratings.Feed polling happens by a combination of REST with Atom: using Spring MVC and the Rome library (to create Atom feeds).At the end of the talk, Michael referenced the ddd-by-examples GitHub project as a great resource.Michael is currently writing a book, Hands-on Domain-Driven Design by example, for which you can get notified upon release by signing up on Leanpub.The slides of this talk can be found on Speakerdeck.Migrating legacy enterprise Java applications to Spring Boot by Mark HecklerMark explained how easy it can be to migrate an existing legacy Enterprise Java application to a modern, state-of-the-art Spring Boot app.Many people think that migrating these kinds of applications is impossible or very hard without rewriting the whole thing, but Mark gave us some very good pointers on how to do it quickly and efficiently:  Generate a new skeleton project from start.spring.io  Use schema.sql and data.sql data sheets to migrate and test your database  Use Kotlin to vastly simplify your code by using data classes to simplify access to members and constructors, and by moving the constructor definition in the same line as the class definition  Using Spring Data, no more need to use PersistenceContext or EntityManager  Using Spring MVC with @RestController, no more need to declare @Produces or @ConsumesBenefits  Less code: your code vastly diminishes using Kotlin and data classes  The Spring Boot + Kotlin combination greatly reduces the amount of boilerplate code  Business logic and Service Layer of the old application remains the same and is better encapsulated  Code becomes easier to maintain, easier to test  Spring Boot offers more and better deployment optionsJWorks consultants have done these kinds of migrations at multiple clients, with great success rates.Non-technical people like functional analysts, product owners and business experts are continuously amazed at the speed with which we are able to do this.Technical people that have been doing JEE development for years ask us how they can learn Spring Boot.Spring Security 5 Workshop by Andreas FalkThe target of this workshop was to learn how to make an initially unsecured (reactive) web application more and more secure step-by-step.It was a very well prepared workshop and I really enjoyed the interactivity with Andreas, he answered questions on the fly and helped us understand some of the finer details and changes in Spring Security 5, especially when using it with Spring Boot.I don’t want to diminish the excellent Spring Security workshop from Andreas Falk by copying anything from him.He deserves all the credit for his amazing work so I’m just gonna link to it here:https://andifalk.github.io/spring-security-5-workshop/Thank you Andreas for this great resource!Spring Framework 5 - Hidden Gems by Juergen HoellerSince almost every feature was backported to 4.3, most of them are already known to the general public.Though there are 7 areas of refinement within 5.0 that aren’t widely known to the public.Commons Logging BridgeSo the Spring team came up with a new dependency called spring-jcl which is actually a reimplementation of a logging bridge.It is a required dependency and is here to help streamline the logging functionality.The main difference with this way of working is that you don’t need to go through a dependency hell where you would manually add exclusions to ignore certain logging dependencies.Just add the logging library to your classpath and everything will switch to the logging implementation of your choice.It now has first class support for Log4J 2 (version 1 has reached its end of life), SLF4J and JUL.Build-Time Components IndexerThe file system traversal for classpath scanning of all packages within the specified base packages using either &lt;context:component-scan&gt; or @ComponentScan might be slow on startup.This is especially true if your application is started for a small period of time or where I/O is very expensive.Think short-running batch processes and functions, or applications being started and stopped on Google App Engine every 2 minutes.The common solution was to narrow your base packages, or even to fully enumerate your component classes so you would skip scanning all together.Starting with 5.0 there is a new build-time annotation processor that will generate a META-INF/spring.components file per JAR containing all the classes which in turn will be used automatically at runtime for compatible component-scan declarations.NullabilityThe new version contains comprehensive nullability declarations across the codebase.Fields, method parameters and method return values are still by default non-null, but now there are individual @Nullable declarations for actually nullable return values for example.For Java this means that we have nullability validation in IntelliJ IDEA and Eclipse.This allows the Spring Team to find subtle bugs or gaps within the framework’s codebase.It will also allow us, as developers, to validate our interactions with the Spring APIs.When you’re writing code in Kotlin it will give you straightforward assignments to non-null variables because the Kotlin compiler will only allow assignments for APIs with clear nullability.Data Class BindingSpring Data can now work with immutable classes.No need for setters anymore since it can work with named constructor arguments!The property names are matched against the constructor parameter names.You can do this by explicitly using @ConstructorProperties or they are simply inferred from the class bytecode (if you pass -parameters or -debug as compilation argument).This is a perfect match with Kotlin and Lombok data classes where the getter and setters are generated at compile time.Programmatic Lookup via ObjectProviderThe ObjectProvider is a variant of ObjectFactory, which is designed specifically for injection points, allowing for programmatic optionality and lenient not-unique handling.This class had the following original methods: @Nullable getIfAvailable() and @Nullable getIfUnique().With the new version of Spring these methods have been overloaded with java.util.function callbacks which empowers the developer to return a default value instead of returning null.Refined Resource InteractionSpring’s Resource abstraction in core.io has been overhauled to expose the NIO.2 API at application level, eg. Resource.getReadableChannel() or WritableResource.getWritableChannel().They are also using the NIO.2 API internally wherever possible, eg. FileSystemResource.getInput/OutputStream() or FileCopyUtils.copy(File, File).Asynchronous ExecutionSpring 5.0 comes with a couple of interface changes that will help you with asynchrous execution:  The ListenableFuture now has a completable() method which exposes the instance as a JDK CompletableFuture.  The TaskScheduler interface has new methods as an alternative to Date and long arguments: scheduleAtFixedRate(Runnable, Instant, Duration) and scheduleWithFixedDelay(Runnable, Instant, Duration).  The new ScheduledTaskHolder interface for monitoring the current tasks, eg. ScheduledTaskRegistrar.getScheduledTasks() and ScheduledAnnotationBeanPostProcessor.getScheduledTasks().Google Cloud Native with Spring Boot by Ray TsangOn one hand this workshop lowered the entry threshold for newbies.On the other hand it provided insight about what services Google Cloud has to offer.Google Spanner, Pub/Sub messaging system, CloudSQL, Runtime Config.They were all addressed.It’s quite cool to see how the team at Google managed to create decent Spring Boot Starters for all these services.They basically remove all the boilerplate code for you and offer you easy connectivity to all its cloud services.The sensible auto-configuration pre-fills most of the settings required to use:  PubSub as messaging middleware  CloudSQL as a managed relational database  Runtime Config as the backing store for your application configuration  Google Spanner as a horizontally scalable, strongly consistent, relational databaseDuring the workshop, we created a guestbook application which consisted of a frontend and some backend microservices.The workshop builds this up neatly by adding features to the application step by step.Each step introduces you to another Google Cloud service.Those of you who want to make the workshop yourself, check out the link below.  Workshop: http://bit.ly/spring-gcp-lab  Code: https://github.com/saturnism/spring-cloud-gcp-guestbook  Cloud console: https://console.cloud.google.com/Google PubSubWhat stayed with me is Google’s Pub/Sub message-oriented middleware.A publisher that creates the messages sends them to a topic.Consumers can subscribe to this topic to obtain the messages.Publishers and subscribers are decoupled. Neither of them is required to know the other one.Subscribers will either pull messages or get messages pushed from the topic.PubSub messages will be delivered at least once, but can be processed multiple times by different subscribers.Unprocessed PubSub messages are only kept for 7 days.  Ray told us a fun story about how he wanted to really explore the capabilities of PubSub and see how many messages it could handle.They created this website called https://pi.delivery/ which calculates the numbers of pi.It’s really interesting to read how hard they were able to stress PubSub (hint: BILLIONS? TRILLIONS!)Flight of the Flux by Simon BasléIn this talk Simon went deeper into the inner workings of Spring Reactor.The session started off giving a brief recap of reactive programming and reactive streamsbefore delving deeper into the machinery behind Reactor 3.Assembly time vs Execution timeWhen programming with Reactor 3 (and other functional reactive libraries like RxJava)the programming model is quite different compared to the classic imperative style.Basically your whole chain is lazy, you describe a sequence of operations and no actualprocessing happens until someone subscribes.To give an example:Assembly:    this.myFlux = Flux.just(\"foo\").map(String::length);As stated above, all that really happens when this code is called is creating a chain of operators(under the hood this phase is also used for things like operator fusion, see below).Execution:    this.myFlux.subscribe(System.out::println);When the subscribe method is called the actual chain is executed and the length of the string is printed in the console.For those familiar with Java 8 it’s basically the same as Java 8 streams,nothing really happens until a terminal operation is used (collect, reduce, count, …).One of the drawbacks of this is error handling.Since the error doesn’t happen until the subscription, it’s harder to see where the error actually happens.To alleviate this, Reactor provides a feature called assembly tracing which can be enabled with the checkpoint() operator.  Nothing happens until you subscribe.Cold and hot observablesPrevious statement holds for most observables typically encountered in a project.HTTP calls, data lookups, etc. No data is actually being produced until someone subscribes.Sometimes however an observable can be a constantly emitting event stream.When multiple subscribers subscribe on a cold observable, each of these subscriptions will trigger the whole chain from the start.A hot observable is constantly producing data and will only give the elements to the subscriber emitted after subscription time.SchedulingReactor is concurrency agnostic which means it doesn’t impose a concurrency model while it does give the developer the tools to change Reactor’s executor behavior.This is done using the scheduler abstraction: a scheduler defines the execution context, this can be the same thread, another thread or using a threadpool.The special operators publishOn() and subscribeOn() allow you to change the execution context of the current chain.The publishOn() changes the execution context of the downstream operators.For example:flux.op1().op2().publishOn(scheduler1).op3().subscribe((result) -&gt; doSomethingWith(result));op1 and op2 will execute in the original execution context(usually the thread in which the subscribe is called).op3 and the action defined in the subscribe method itself will execute in scheduler1’s execution context.The subscribeOn() changes the execution context of the subscription, meaning of the start of the execution of the chain.For example:flux.op1().op2().subscribeOn(scheduler1).op3().subscribe((result) -&gt; doSomethingWith(result));The whole chain will execute in scheduler1’s execution context.Work stealingAlthough previous topics surface easily and are simpler to demonstrate, this was perhaps the most abstract one of the session along with operator fusion.When using schedulers supporting parallel execution, Reactor uses so called ‘work stealing’ algorithms to balance the load on the different threads.If a thread is idle it can take over execution of tasks that were originally scheduled to be executed by a different thread.Under the hood, this is achieved by using a shared queue for the tasks and a drain loop.Operator fusionOne of the big advantages of having a chain of tasks defined and split up in multiple steps is that it allows the engine to identify possible optimizations in the chain.Since each individual operator also has an overhead (eg. queue for work stealing), it’s sometimes more efficient to combine some operators and execute them as one.For example:map(a).map(b).map(c) =&gt; map(abc)Reactor tries to achieve this by using a negotiation process between the operators.ConclusionThis talk gave us more insight into the more advanced parts of Reactor, arming us with knowledge to tackle potential problems in a reactive environment and helping us understand Reactor’s deeper mechanisms.For more information, Simon uploaded his presentation on Speaker Deck.(Spring) Kafka - One more arsenal in a distributed toolbox by Nakul MishraNakul started by describing Apache Kafka,a very potent messaging system which allows you very easily to act as a throughput between your applications,as long as you stay away from recreating ESB anti-patterns with Kafka.Kafka is more than a messaging queue, combining speed, scalability and stronger ordering guarantees then traditional messaging keys.In order to benefit from this ordering, it is important to choose a correct partition key.Kafka puts more emphasis on smart consumers, meaning a more client centric approach vs the broker centric approach used by Message Oriented Middleware.By designing for retention and scale, Kafka gives consumers (clients) the time to process the messages they want to process whenever they want to.It is also possible to use Kafka as a database, by having it process a stream of data in real-time using KSQL of which the results can very easily be pushed to external systems (HDFS, S3, JDBC).To be scaleable, Kafka is kept simple at its core, all data is stored as a partitioned log.This means that writes are append-only and reads are a single seek-and-scan allowing the underlying filesystem to very easily handle the storing and caching of messages.Also when reading, data is directly copied over from the disk buffer into the network buffer bypassing the JVM, ideally for flooding your network.Spring Kafka integrates Kafka with Spring giving you all the benefits of the Spring ecosystem.It also supports Kafka Streams since a few months.Testing is made easier by providing an @EmbeddedKafka and a TestUtils class:@EmbeddedKafka(partitions = 1,         topics = {                 KafkaStreamsTests.STREAMING_TOPIC1,                 KafkaStreamsTests.STREAMING_TOPIC2 })Spring Kafka also has a starter available for Spring Boot which makes it very easy to get started with Kafka and start playing around.As always, just go to start.spring.io and get the Kafka dependency.The slides of this presentation can be found at slideshare.Breaking down monoliths into system of systems by Oliver GierkeThe goal of this workshop is not to provide a clear architecture of the perfect application, but more to make you think.To let you reflect about your existing applications.There is a shorter version of this talk here while this one consumed a full two hours.This gave us the possibility to have a more in-depth look at the code that Oliver prepared and look into potential problems, which would be much harder were it a one hour session.The workshop basically is a summary of observations about the workings of monoliths and microservices.It all tends to boil down to the correct definition of bounded contexts within applications, how you can divide your application in logical modules and how these can communicate with each other.First we will observe what happens when a monolith is transformed into a microlith AKA a distributed monolith.Subsequently we will improve the design of the monolith with these bounded contexts and end up with a modulith.This modulith is still a single application, a monolith, but with different bounded contexts each having clearly defined borders allowing us to easier divide the work over various teams.From a modulith, one can go to a system of systems, a true microservice architecture.In a system of systems there are two ways you can implement the communication, either via messaging or via REST.The sample code of this workshop can be found on Github.MonolithThe monolith is reasonably ordered and the bounded contexts have been split in various packages.When building your Java application, you should make optimal use of the package options provided by Java as mentioned in this blogpost of 2013.It stated:  Make your code package protected whenever it does not need to be accessed from the outside, a good starting point is to make your repositories no longer public.  Whenever there is leakage over the bounded contexts, for example when LineItems contains Products, try to use IDs and not the actual objects of another bounded context, because whenever you update an object used within another bounded context, you will also leak into that context.It is also noted that badly structured applications tend to be built from the bottom up, from DB to the top.This means that your design is going to be way too data-centric instead of focusing on the real business interactions you are supposed to handle.Try to prevent using methods which update two bounded contexts simultaneously, as these methods have the reflex of drawing more and more code in, and they tend to grow like a cancer, killing your application from the inside out.To summarize:  Move bounded contexts into packages  Inter-context interaction is processed locally and resulting in either success or an exception (method calls in the JVM are very efficient and executed exactly once)  Avoid referencing two domain classes over bounded contexts, it’s convenient but results in problems  When you leak into other bounded contexts, there is a great risk of creating circular dependencies  When there are no clear boundaries, adding a new feature often requires you to touch other parts of the system  A monolith is easy to refactor  By its nature, it has strong consistency but this is also a disadvantage as transactions become more brittle when they fail because of related business functionalityThe monolith example code can be found here.MicrolithCreating a microlith means splitting up your systems into various smaller systems.This doesn’t mean that suddenly all your problems have been solved.If you do not correctly define your bounded contexts in order to minimize your communication, the chance is great that you have made a microlith with the following problems:  You are no longer able to use local transaction consistency  Local method invocation is transformed into RPC-style HTTP calls  You have translated the transactions of your monolith into a distributed system, needing HTTP to update each other  Remote calls are executed while serving user requests and this over multiple services  Running and testing requires the other services to be available  There is a strong focus on API contracts, which tend to be very CRUD-looking with a lack of business abstraction and hypermedia  Detecting breaking API changes is prioritized over making evolvable APIs  One tends to add more technology in order to solve issues: bulkheads, retries, circuit breakers, asynchronous calls, more monitoring systems, etcIt tends to minimize the risks of a rollback, but it does not really solve any issue, it just distributes your problems.  The first rule of distributed systems is: don’t distribute your system until you have an observable reason to.If you did not define your bounded contexts properly, it is very difficult for you to observe how to distribute your system.Example code of the microlith can be found here.ModulithWe will start using events inside our modulith, as well as more domain specific methods, like an add() on an Order.This makes everything more abstract, making your domain objects much more than glorified getters and setters.  We don’t do CQRS or event sourcing, but we just use eventing as a way to signal events over bounded contexts.These events make it relatively easy to split up the work, they can serve as either input or output for different services within the applications.Your units of work will have clear boundaries making testing and design easier.The differences with a monolith are:  Focus of domain logic has moved to aggregates  Integration between bounded contexts is event based  The dependency between bounded contexts is inverted  Side Step: Application Events with Spring DataThis is a very powerful mechanism to publish events in a Spring application.Whenever you need to send data to another bounded context, you trigger events.This has the advantage that your business services no longer needs to know about each other, they just need to trigger an event which gets picked up by the services which are interested in this event.Transactional semantics are still retained because the eventing is synchronous, by default.This also applies for JEE eventing.The @TransactionalEventListener annotation allows you to delay the execution of events,so for example, you can send out an email when an Order has truly been completed.  Side Step: Error ScenariosWhen a synchronous event listener fails, this will be handled by the transaction, so no worries.But when an asynchronous event listener fails, the transaction does not get rolled back and you will need to deal with retries.You can make use of an Event Publication Registry when you use TransactionalEventListeners as these event listeners are decorated with a log before the commit, since the system needs to know the events need to be sent out to.When the event has been processed, the log will be cleared.If it doesn’t get cleared the system can keep retrying, so you don’t lose events.Example code of the modulith can be found here.System of SystemsMessagingWhenever you make use of a message broker, you introduce a potential single point of failure, like with Apache Kafka or RabbitMQ.These brokers know about all the messages of all the systems and decide how long these messages will be retained.Coupling does exist, although not explicit, but the message format will decide which version of a service can process these messages, just as with REST.Especially if you keep your events for a long time, which is possible with Kafka, you might need to think about transforming existing events.But these messaging systems tend to be designed for scale.Pro tip: make use of JsonPath annotations for the message payload in order to make it more robust.Example code of a system of systems with messaging can be found here.REST MessagingIf you use REST you will have to deal with caching, pagination and conditional requests.Messages do not tend to be stored for long periods of time and most communication tends to be synchronous.One does have to pay attention on not to lose events as your application will immediately know if the message was processed correctly, incorrectly or timed out.REST PollingWhen using a polling mechanism, your producers do not send out messages to your consumers, but the consumers will poll the producers for new events to process.This means that:  You do not need any additional infrastructure like an Apache Kafka or a Message Bus  Event publication is part of the local transaction  The publishing system (producer), controls the lifecycle of the events and can transform these if necessary  The events never leave the publishing system  There might be a bigger consistency gap, depending on how frequently the consumers poll  It does not scale that wellThe example code with REST can be found here.ConclusionThis was a great workshop which makes you think about the design decisions you have made for your applications.If you ever get the opportunity to participate in one of these workshops, do not hesitate to join as they are much more valuable than regular talks that can be viewed online as well.Observability with Spring based distributed systems by Tommy LudwigIntroductionTommy’s talk introduced three main pillars of observability: logging, metrics, and tracing.Tommy explained that observability is achieved through a set of tools and practices that aim to turn data points and contexts into insights.Observability is something you should care about as it provides a great experience for the users of your system and it builds confidence in production where failure will happen.You ought to give yourself the tools you need in order to be a good owner in order to detect these failures as early as possible.Mean time to recovery is key here.He also quoted Werner Vogels’, the CEO of Amazon, “You build it, you run it” while also adding to it that you need to monitor it.Within a Spring Boot project, we have access to Actuator and it is awesome.It comes with a lot of goodies out of the box.There is also Spring Boot Admin that makes it easy to access and use each instance’s Actuator endpoints.Distributed systems make observing them hard by design as a request spans multiple processes.You therefore need to stitch these together in order to fully make sense of it.There are also more points of failure and adding multiple instances of the same service, for scaling reasons, will only increase the monitoring complexity.Tommy named three sides to observability:  Logging  Metrics  TracingLoggingLogs are request scoped, arbitrary messages that you want to find back later.They are formatted to give you context via things such as logging levels and the timestamp.The issue with logs is that they do not scale, concurrent requests intermingle logs, and searching through them can be cumbersome.In order to tackle these issues you can make use of centralized logging while also adding a query capability to retrieve a collection of matching logs.Within Spring Boot we can configure the logging via Spring Environment and via Actuator at runtime.Spring Cloud Sleuth is useful to add a trace ID for request correlation.MetricsMetrics aggregate time series data and have a bounded size.You can slice these based on dimensions, tags and labels.The main goal of metrics is to visualize and identify trends and deviations, and to raise alerts based on metric queries.Some examples of metrics are: response time, the response’s body size and memory consumed.In order to properly measure all this, you need to set up a metrics backend to which all applications publish their metrics data.In Spring Boot 2, Micrometer is introduced as its native metrics library.Micrometer supports many metrics backends such as Atlas Datadog, Prometheus, SignalFX and Wavefront.A lot of the instrumentation is auto-configured by Spring Boot and custom metrics are added easily.These are configurable via properties and common tags such as the application name, the instance, region, zone, and more.TracingLocal tracing happens via the Actuator /httptrace endpoint and displays the latency data.With distributed tracing you can go across process boundaries which is useful as metrics lack request context and as logs have a local context but limited distributed info.You define the sample size of the request to trace yourself as you don’t want to trace everything especially if you have a high load.This sample size is configurable at runtime, especially handy to debug errors in production.Zipkin with its UI helps you to see the timing information visually and is a good tracing backend for Spring applications.Using Spring Cloud Sleuth, the tracing instrumentation via Zipkin’s Brave is auto-configured.Via properties you can configure things such as the sampling probability and whether certain endpoints should to be skipped.It is also compatible with the OpenTracing standard that is being developed under the wings of the CNCF.Correlation everywhereHaving set up all of these, you now have correlated logging, metrics and tracing across your system, and you can find the data from each based on identifiers.Observability cycleIf an issue produces itself we can take the following steps to troubleshoot and bandage the situation:  The issue should have been reported via an alert or report  We check the metrics of our system  If needed, we check the tracing data  If needed, we check the logs  Based on the gathered information we can triage the issue and make adjustments to prevent a recurrenceKey takeawaysSystem wide observability is crucial in distributed architectures.The tools to help you with this exist and Spring makes it easy to integrate them in your system as the most common cases are covered out-of-the-box or are easily configurable.Use the right tool for the job and synergise across the different tools.Day 2: Talks &amp; WorkshopsMachine learning exposed: The fundamentals by James WeaverMachine Learning is a hot topic in tech land with all kinds of applications like predicting property prices, forecasting weather, self-driving cars, plants classification and so on. James gave a brief overview about the fundamentals of Machine Learning and its applications.But how can we define Machine Learning? Andrew Ng, Co-founder of Coursera and Adjunct Professor of Stanford University defined Machine learning in his introduction course “Welcome To Machine Learning”1 as “Machine Learning is the science of getting computers to learn, without being explicitly programmed”. An example that Andrew gave was a cleaning robot that can tidy your house. Instead you program the algorithm explicitly on how it should clean. You can for instance let the robot watch you while you demonstrate the tasks on how it should clean and learn from it.Later on he gave examples of different categories of machine learning.Categories of Machine LearningSupervised learningThis was the category where James gave the most examples of during his talk.Supervised learning is where you train your model with a dataset which contains the initial data and its correct answers.The more training data you have, the more accurate your predictions will be.Regression exampleAn example he showed us was the prediction of housing prices using regression.(From Andrew NG’s Machine learning course)In this example, the dataset consists of instances with a square footage (input) and price (output).With a regression, we can predict a continuous valued price.Classification exampleSource:Nicoguaro's Wikipedia media gallery (CC BY 4.0) An example of Supervised Learning using classification Another example of Supervised Learning is to determine a certain species of an Iris flower.The algorithm tries to determine the species of the flower with the Sepal and Petal size as input.Unsupervised LearningFor Unsupervised Learning on the other hand, you don’t give the right answers with your dataset.Your learning algorithm will try to find a structure in the given data.A method to try to find a structure, is to do it by clustering.This means the data is ‘grouped’ in clusters together with data that more or less belongs to each other.Market segment discovery and social media analysis are examples of Unsupervised Learning.Reinforcement LearningBy Reinforcement Learning, you give your algorithm rewards when it did something well.This type of learning is very popular in game playing.AlphaGo for example from Google Deepmind was taught by Reinforcement Learning.Neural networksThe second part of his talk was about Neural Networks.(Artificial) Neural Networks are computing systems that are inspired by biological neural networks.It’s made up of highly interconnected processing elements or ‘nodes’ that can process information.A Neural Network consists of different layers.An input layer, one or more hidden layers and an output layer.We can visually demonstrate how Neural Networks work with the help of deeplearning4j.You can clone and try out his example on https://github.com/JavaFXpert/visual-neural-net-server.Let’s use the flower classification example with our neural network. 1: Welcome to Machine Learning (Andrew Ng)  Testing every level of your Spring Microservices application (Workshop) by Jeroen Sterken &amp; Kristof Van SeverIntroductionThis workshop focused on testing the different levels of a microservices application.It was split up into two parts:  Testing within a single microservice  Testing the relationships between microservices with Spring Cloud contractTesting a single microservice with Cucumber and JUnitThe presentation started off with some of the new features that JUnit 5 has to offer.Since JUnit 5 supports Java 8, it allows you to use lambdas in assertions, as well as using group assertions with the assertAll() method.It’s also possible to run tests multiple times with different parameters, by annotating them with the @ParameterizedTest and @ValueSource for the arguments.Behaviour driven testing with CucumberUnit tests alone are not enough of course, you also need to test how different components work together.Usually it’s the developer who writes such tests, but it’s also possible for non-technicals to write such tests with Cucumber.How does this work exactly?Cucumber achieves this by using Gherkin, an English plain text language.It has .feature files where the different scenarios for a certain feature are described.Feature: A new empty basket can be created and filled with TapasScenario: Client creates a new Basket, and verifies it's empty    When the user creates a new Basket    Then the total number of items in the Basket with id 1 equals 0The above is a simple example of how to describe a feature and scenario.The words in bold represent Cucumber keywords, called step definitions.It’s also possible to substitute Scenario with Scenario outline in case you need to test the same scenario with different values.You can put parameters inside angle brackets (&lt;&gt;), which are substituted with values that you define in an Examples data-table.The next step is to annotate your methods with the Cucumber step definitions (e.g. @Given @When).The text that you provide the annotations with, should match the text in your .feature file so that Cucumber can glue the two together.In this case, the When the user creates a new Basket of the example above matches with:@When(\"^the user creates a new Basket$\")The annotated methods should execute what you described in the feature files, so in this case the method looks something like this:public void theUserCreatesANewBasket() {    userBasketManagement.createNewBasket();  }To try it out for yourself, go to the Workshop repo.There’s a solution branch in case you’re stuck or wish to compare your code.Spring Cloud ContractOne of the challenges of testing chained microservices is making sure that a microservice stub reflects the actual service at all times.One way this can be achieved is by using Spring Cloud Contract.Spring Cloud Contract enables Consumer Driven Contract development, where one service (consumer) defines its expectations of another service (producer) through a contract.The first step is for the consumer to write the test for the new feature, following the TDD approach.Next, add the Spring Cloud Starter Contract Verifier dependency and maven plugin to your producer.Create a base test class to the test package that loads the Spring Context.Make sure to annotate it with @AutoConfigureMessageVerifier.We should also add the contract to our resources on the producer-side:Contract.make{    description \"should return a list of all tapas\"    request{        method GET()        url \"/tapas\"    }    response{        status 200        headers {            contentType applicationJson()        }        body (            [                [                        id: 0,                        name: \"All i oli\",                        price: 1.5                ],                [                        id: 1,                        name: \"Banderillas\",                        price: 3                ]             ]        )    }}The above is an example of how a contract is defined, written in Groovy (although YAML is a possibility as well).It simply specifies that a GET request to /tapas should return the provided body as application/json.Now it’s time to create the stub.Since you’ve already added the dependency and plugin to your producer, simply run your build for the plugin to generate the stubs.The built stub artifact will be stored in your local maven repository.The plugin will also create a test class that extends the base test class we created earlier containing the necessary setup to run your tests.The next step is to add the Spring Cloud Contract Stub Runner dependency to the consumer and annotate your test class with @AutoConfigureStubRunner.By annotating your class with @AutoConfigureStubRunner and providing the groupId, artifactId and port on which the stub will run, your test class is configured to use the producer’s generated stub.Interested to try out this Spring Cloud Contract workshop?You can find the Github repo here.Got triggered?All talks were recorded by the Spring IO team. You can view them on YouTube."
      },
    
      "cloud-2018-06-01-automated-canary-analysis-using-spinnaker-html": {
        "title": "Automated Canary Analysis using Spinnaker - Codelab",
        "url": "/cloud/2018/06/01/Automated-Canary-Analysis-using-Spinnaker.html",
        "image": "/img/spinnaker/spinnaker-logo.png",
        "date": "01 Jun 2018",
        "category": "post, blog post, blog",
        "content": "IntroSpinnaker is a multi-cloud, multi-region automated deployment tool.Open sourced by Netflix and heavily contributed to by Google, it supports all major cloud providers including Kubernetes.Last month, Kayenta was open sourced, a canary analysis engine.Canary analysis is a technique to reduce the risk from deploying a new version of software into production.A new version of the software, referred to as the canary, is deployed to a small subset of users alongside the stable running version.Traffic is split between these two versions such that a portion of incoming requests is diverted to the canary.This approach can quickly uncover any problems with the new version without impacting the majority of users.The quality of the canary version is assessed by comparing key metrics that describe the behavior of the old and new versions.If there is a significant degradation in these metrics, the canary is aborted and all of the traffic is routed to the stable version in an effort to minimize the impact of unexpected behavior.        PrefaceOrdina helps companies through digital transformation using three main focus areas:      Embracing a DevOps culture and corresponding practices allows teams to focus on delivering value for the business, by changing the communication structures of the organization.Through automation, teams are empowered and capable of delivering applications much faster to production.        Having a modular decoupled architecture, our second focus area, fits well with this model.Making these changes to our architecture in combination with a culture of automation, results in a lot more moving parts in our application landscape.        Naturally, the next step is tackling the underlying infrastructure accomodate this new architecture and way of working.Cloud automation is therefore our final focus area in digital transformations.  Releasing more often doesn’t only allow new features reaching the user faster, it also fastens the feedback loops, improves reliability and availability, developer productivity and efficiency. Spinnaker plays a crucial part in all of this, as it allows more frequent and faster deployments, without sacrificing safety.Automated canary analysis, demonstrated in this codelab, is a powerful tool in that sense.Overview  Goal  Prerequisites  Introducing our Rick &amp; Morty demo  Installation  Configuration  Running the demo scenario  ConclusionGoalThe purpose of this codelab is to simplify getting up-and-running with automated canary analysis using Spinnaker on Kubernetes.PrerequisitesWe’re using Google Cloud Platform for this demonstration.Monitoring and logging will be handled by Stackdriver, which is integrated completely with GCP.The canary functionality we’re going to use in this setup requires the use of a specific cluster version with full rights:  You must be an Owner of the project containing your cluster.  You must use Kubernetes v1.10.2-gke.0 or later.Introducing our Rick &amp; Morty demoRick &amp; Morty is a television show following the misadventures of cynical mad scientist Rick Sanchez and his good-hearted but fretful grandson Morty Smith, who split their time between domestic life and interdimensional adventures.        Our demo application is a Java Spring Boot application, running on an Apache Tomcat server, packaged inside a docker container.The docker container runs on Kubernetes managed by Google Cloud Platform (GKE).The application exposes an endpoint on http://localhost:8080 and can be run locally by executing ./mvnw spring-boot:run, assuming you have a JRE or JDK (v8+) installed.The endpoint returns an HTML with a background of Pickle Rick.In season three episode three, Rick turns himself into a pickle in an attempt at escaping family therapy.        Pickle Rick will act as our initial green deployment, running stabily on production.We will try to replace it with a blue deployment.Mr. Meeseeks, featured in season one episode five, will be the protagonist of that deployment.Meeseeks are creatures who are created to serve a singular purpose for which they will go to any length to fulfill.After they serve their purpose, they expire and vanish into the air.Their motivation to help others comes from the fact that existence is painful to a Meeseeks, and the only way to be removed from existence is to complete the task they were called to perform.Meeseeks can however summon other Meeseeks to help, which could spiral out of control if the task at hand is unsolvable.        Therefore, Meeseeks are quite dangerous and a good candidate for our misbehaving blue deployment.Aside from the HTTP endpoint, our demo application also prints out a number of character names from the series.Blue Green DifferencesAside from the leading character in our two versions, there are two specific differences in the code between both versions.The following commit shows moving from the green version to the blue version: 24cc45cfFirst of all, the background image changes, which gives a clear visual indication of which version is currently deployed.Since using Meeseeks could get out of hand quickly, keeping track of how many times the Meeseeks HTTP endpoint has been hit makes a lot of sense.Hence, the blue version prints an extra Meeseeks in the logs, for every request to the endpoint.Using this setup, we should be able to consider logs as a source of information for judging the canary healthiness.Note that the Github repository can constantly switch between Pickle Rick and Meeseeks.Before starting a build and making deployments, make sure your fork is aligned with the green version.If this isn’t the case, switching to green is demonstrated in the following commit: 784e616aSetup Continuous IntegrationMaking changes to our application will be the trigger for our pipelines.Therefore, we should have a simple continuous integration flow set up.We could use Jenkins or any other build server that uses webhooks, but since our entire demo is being deployed on GCP, we can use the build server from GCP instead.First of all, fork the demo application repository.In the GCP console, open build triggers underneath the Container Registry (GCR) tab.Select Github as repository hosting, and select the forked repository to create a trigger for.Configure the trigger to activate on any branch, using a cloudbuild.yaml file located in the root of the repository.        This will run a maven build and docker build, and push the created docker image into the GCR.Installation  Throughout this guide we refer to the official documentation for individual parts of the installation already covered by the Spinnaker team.However, as reference we also compiled an exhaustive list of commands to execute based on the commands found in those articles.This means you could skip the official documentation and simply execute those commands.However, we still recommend going through the docs to get more context.The list of commands to execute can be found at the end of this chapter.Follow the guide on Spinnaker’s website: https://www.spinnaker.io/setup/quickstart/halyard-gkeCreate a cluster as mentioned here: https://cloud.google.com/monitoring/kubernetes-engine/installinggcloud components updategcloud auth logingcloud config set project &lt;PROJECT_NAME&gt;Find out the latest supported cluster version with the following command:gcloud container get-server-config --zone=$ZONECreate a cluster for your specific zone (e.g. europe-west1-d) and preferred cluster version (v1.10.2-gke.0 or later):CLUSTER_VERSION=1.10.2-gke.1GCP_PROJECT=$(gcloud info --format='value(config.project)')ZONE=europe-west1-dCLOUDSDK_CONTAINER_USE_V1_API=falseCLOUDSDK_API_CLIENT_OVERRIDES_CONTAINER=v1beta1gcloud beta container clusters create spinnaker \\  --zone=$ZONE \\  --project=$GCP_PROJECT \\  --cluster-version=$CLUSTER_VERSION \\  --enable-stackdriver-kubernetes \\  --enable-legacy-authorizationMake sure --enable-stackdriver-kubernetes and --enable-legacy-authorization are passed.Enable APIsNavigate to the Google Cloud Console and enable the following APIs:  Google Identity and Access Management (IAM) API  Google Cloud Resource Manager APIHalyard setupThis section complements official documentation with some recommendations and extras.Postpone running the hal deploy apply command until the end of this chapter.Basic SpinnakerDuring the Halyard on GKE guide on Spinnaker’s website, remember to use the right zone when creating the Halyard VM.gcloud compute instances create $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --scopes=cloud-platform \\    --service-account=$HALYARD_SA_EMAIL \\    --image-project=ubuntu-os-cloud \\    --image-family=ubuntu-1404-lts \\    --machine-type=n1-standard-4When SSH’ing into the Halyard VM, also remember to use the right zone.gcloud compute ssh $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --ssh-flag=\"-L 9000:localhost:9000\" \\    --ssh-flag=\"-L 8084:localhost:8084\"Before you perform hal deploy apply, add the Docker registry corresponding to your region. In case your project is located in Europe, add the eu.gcr.io registry as illustrated below.hal config provider docker-registry account add gcr-eu \\    --address eu.gcr.io \\    --password-file ~/.gcp/gcp.json \\    --username _json_keyhal config provider kubernetes account edit my-k8s-account --docker-registries my-gcr-account gcr-euIAM ConfigurationEnable Stackdriver access for Spinnaker in GCP’s IAM settings.Add the following roles to the member with name gcs-service-account:  Logging Admin  Monitoring AdminAutomated Canary AnalysisBefore you perform hal deploy apply, enable automated canary analysis.Follow the guide further down, but first of all, set some variables while still SSH’d in the Halyard VM.One of these variables is the Spinnaker bucket automatically created when installing Halyard.Look for the right bucket identifier in the GCP GKE buckets dashboard.PROJECT_ID=$(gcloud info --format='value(config.project)')JSON_PATH=~/.gcp/gcp.jsonMY_SPINNAKER_BUCKET=spin-48b89b5e-dd67-446a-ad9f-66e8783e9822Follow the official canary quickstart documentation.Configure the default metrics store.hal config canary edit --default-metrics-store stackdriverAnd finally execute the rollout.hal deploy applyTroubleshootingAuthenticationSometimes an issue might occur with credentials on the Halyard VM:! ERROR Unable to communicate with your Kubernetes cluster: Failure  executing: GET at: https://35.205.113.166/api/v1/namespaces. Message: Forbidden!  User gke_spinnaker-demo-184310_europe-west1-d_spinnaker-alpha doesn't have  permission. namespaces is forbidden: User \"client\" cannot list namespaces at the  cluster scope: Unknown user \"client\"..? Unable to authenticate with your Kubernetes cluster. Try using  kubectl to verify your credentials.In this case, enable legacy authentication in the GKE UI for the cluster.Cluster DebuggingYou can monitor deployment locally on your own PC by running kubectl get pods -w --all-namespaces.For this to work, kubectl needs permissions to talk to the cluster.You can use gcloud to populate your kubeconfig file with credentials to access the cluster.This can help you to look into specific logs of each Spinnaker pod or follow up on deployments handled by Spinnaker.Audit LoggingYou can find out which commands are sent to GCP by enabling audit logging.Turn on audit logging: https://cloud.google.com/monitoring/audit-logging &amp; https://cloud.google.com/logging/docs/audit/configure-data-access#exampleComprehensive list of commandsThese are all the commands we have executed in order to get everything set up.Fill in the &lt;PLACEHOLDER&gt; placeholders according to your preferences.ZONE=&lt;ZONE_NAME&gt;CLUSTER_VERSION=&lt;CLUSTER_VERSION&gt;GCP_PROJECT=&lt;PROJECT_NAME&gt;gcloud components updategcloud auth logingcloud config set project $PROJECT_NAMEgcloud container get-server-config --zone=$ZONECLOUDSDK_CONTAINER_USE_V1_API=falseCLOUDSDK_API_CLIENT_OVERRIDES_CONTAINER=v1beta1gcloud beta container clusters create spinnaker \\  --zone=$ZONE \\  --project=$GCP_PROJECT \\  --cluster-version=$CLUSTER_VERSION \\  --enable-stackdriver-kubernetes \\  --enable-legacy-authorizationHALYARD_SA=halyard-service-accountgcloud iam service-accounts create $HALYARD_SA \\    --project=$GCP_PROJECT \\    --display-name $HALYARD_SAHALYARD_SA_EMAIL=$(gcloud iam service-accounts list \\    --project=$GCP_PROJECT \\    --filter=\"displayName:$HALYARD_SA\" \\    --format='value(email)')gcloud projects add-iam-policy-binding $GCP_PROJECT \\    --role roles/iam.serviceAccountKeyAdmin \\    --member serviceAccount:$HALYARD_SA_EMAILgcloud projects add-iam-policy-binding $GCP_PROJECT \\    --role roles/container.admin \\    --member serviceAccount:$HALYARD_SA_EMAILGCS_SA=gcs-service-accountgcloud iam service-accounts create $GCS_SA \\    --project=$GCP_PROJECT \\    --display-name $GCS_SAGCS_SA_EMAIL=$(gcloud iam service-accounts list \\    --project=$GCP_PROJECT \\    --filter=\"displayName:$GCS_SA\" \\    --format='value(email)')gcloud projects add-iam-policy-binding $GCP_PROJECT \\    --role roles/storage.admin \\    --member serviceAccount:$GCS_SA_EMAILgcloud projects add-iam-policy-binding $GCP_PROJECT \\    --member serviceAccount:$GCS_SA_EMAIL \\    --role roles/browserHALYARD_HOST=$(echo $USER-halyard-`date +%m%d` | tr '_.' '-')gcloud compute instances create $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --scopes=cloud-platform \\    --service-account=$HALYARD_SA_EMAIL \\    --image-project=ubuntu-os-cloud \\    --image-family=ubuntu-1404-lts \\    --machine-type=n1-standard-4gcloud compute ssh $HALYARD_HOST \\    --project=$GCP_PROJECT \\    --zone=$ZONE \\    --ssh-flag=\"-L 9000:localhost:9000\" \\    --ssh-flag=\"-L 8084:localhost:8084\"Inside the Halyard VM:KUBECTL_LATEST=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)curl -LO https://storage.googleapis.com/kubernetes-release/release/$KUBECTL_LATEST/bin/linux/amd64/kubectlchmod +x kubectlsudo mv kubectl /usr/local/bin/kubectlcurl -O https://raw.githubusercontent.com/spinnaker/halyard/master/install/debian/InstallHalyard.shsudo bash InstallHalyard.sh. ~/.bashrcGKE_CLUSTER_NAME=spinnakerGKE_CLUSTER_ZONE=europe-west1-dPROJECT_ID=$(gcloud info --format='value(config.project)')gcloud config set container/use_client_certificate truegcloud container clusters get-credentials $GKE_CLUSTER_NAME \\    --zone=$GKE_CLUSTER_ZONEGCS_SA=gcs-service-accountGCS_SA_DEST=~/.gcp/gcp.jsonmkdir -p $(dirname $GCS_SA_DEST)GCS_SA_EMAIL=$(gcloud iam service-accounts list \\    --filter=\"displayName:$GCS_SA\" \\    --format='value(email)')gcloud iam service-accounts keys create $GCS_SA_DEST \\    --iam-account $GCS_SA_EMAILhal config version edit --version $(hal version latest -q)hal config storage gcs edit \\    --project $PROJECT_ID \\    --json-path $GCS_SA_DESThal config storage edit --type gcshal config provider docker-registry enablehal config provider docker-registry account add my-gcr-account \\    --address gcr.io \\    --password-file $GCS_SA_DEST \\    --username _json_keyhal config provider kubernetes enablehal config provider kubernetes account add my-k8s-account \\    --docker-registries my-gcr-account \\    --context $(kubectl config current-context)# Only required in case you want to use eu.gcr.iohal config provider docker-registry account add gcr-eu \\    --address eu.gcr.io \\    --password-file $GCS_SA_DEST \\    --username _json_keyhal config provider kubernetes account edit my-k8s-account --docker-registries my-gcr-account gcr-euhal config deploy edit --account-name my-k8s-accounthal config deploy edit --type distributedMY_SPINNAKER_BUCKET=&lt;SPINNAKER_BUCKET_ID&gt;hal config canary enablehal config canary google enablehal config canary google account add my-google-account \\  --project $PROJECT_ID \\  --json-path $GCS_SA_DEST \\  --bucket $MY_SPINNAKER_BUCKEThal config canary google edit --gcs-enabled true \\  --stackdriver-enabled truehal config canary edit --default-metrics-store stackdriverhal deploy applyhal deploy connectConfigurationApplication ConfigurationThis guide uses the Kubernetes V1 provider, but you can use V2 just as well.Follow the official documentation to enable the V2 provider.Visit localhost:9000 to open the Spinnaker UI.In the applications page, create a new application:        Under Infrastructure, the Clusters view should normally be opened automatically.Click the Config link on the top right and enable Canary for this project.        This should enable Canary Analysis for the project.The result should be that the Spinnaker menu for this project should be changed.Pipelines are now nested underneath Delivery, which also now boasts Canary Configs and Canary Reports.In case this is not visualised directly, you can refresh the cache by clicking on the Spinnaker logo on the top left of the page, and clicking the Refresh all caches link in the Actions drop down.        Initial ProvisioningUnder Infrastructure, switch to the Load Balancers view and create a load balancer.Fill in the stack, port, target port and type LoadBalancer.        Under Infrastructure, switch to the Clusters view and create a Server Group.                Once the server group is created, it will show up like this:        By clicking on the little load balancer icon on the right-hand side, we can now visit the Ingress IP through the load balancer view on the side of the page.        Back in the server group section, clicking on the little green chicklet will display container information on the side of the page, including logs of the application.Let’s do this for PROD as well.Follow exactly the same steps as for DEV, except use prod as Stack instead of dev.Once the PROD load balancer and server group are deployed, we’d like to make sure we never have downtime on PROD.We can set up a Traffic Guard, responsible for making sure our production cluster always has active instances.Go to the Config link on the top right of the page, and add a Traffic Guard.        Staging PipelineNow that we’ve deployed a single version of our application to DEV and PROD, it’s time to create a pipeline.This will enable us to continuously deploy new versions of our application without having to manually create new server groups every time.Head over to the pipelines view and create a new pipeline called Deploy to DEV.Under the first “Configuration” stage, configure an automated trigger.        Now add a stage to deploy our application.        We now have to add a server group as deployment configuration.We can reuse the existing deployment as a template.        Change the strategy to Highlander.It’s important to change the image being deployed, otherwise, we’d always deploy the image of our existing server group.Go down to the Container section and select the Image from Trigger.        This will automatically change the container image at the top of the dialog box under Basic Settings.        Keep all other settings as they are.Save the server group configuration, and save the pipeline.When we now select the pipelines view, we can see the newly created Deploy to DEV pipeline.We can test this by either starting a manual execution, or committing a change to our application GIT repository.Production PipelineCreate new pipeline Deploy to PROD.        Add a new Find Image from Cluster stage.This stage will allow us to look for the image we deployed to DEV, and pass that information on to upcoming stages.        Add a new Deploy stage to deploy the new DEV version into production.Under deploy configuration, add a server group based on the one in DEV.Make sure to set the right load balancer, i.e. spinnakedemo-prod.        Scrolling down to the Container section, select the image found in DEV by the Find Image stage.        Since this is a new version we’d like to push to production, it would be a good idea to build in some safety measures to protect us from unexpected failure.Using a canary release strategy allows us to limit the blast radius of potential issues that might arrise.In the Basic Settings section, set the stack as prod and the detail as canary to indicate that this deployment is our canary deployment. Also use the None strategy, since we just want to deploy this canary server group next to the one already active in production.        Now let’s test this out.Change the application to respond with PickleRicks if that’s not already the case.Otherwise, make an insignificant change to the application and push the changes to GIT (master branch).This should trigger a build, which should push a docker image to the GCR.That on its turn should trigger the deployment to DEV, which - if successful - should trigger a deployment to PROD.Once that’s done, your cluster view should look like this:        Notice the V001 on DEV, it has replaced the existing manual server group deployment using the highlander strategy.Currently our canary is registered under the same load balancer as our production cluster. This means traffic is split between the canary and production.We could test the canary manually by going to the ingress endpoint of the load balancer as we did on DEV.This could be sufficient for your needs, but Spinnaker offers automated canary analysis (aka. ACA), capable of automatically investigating traffic sent to the canary.The ACA engine Kayenta will compare certain metrics between the currently running production version, and the newly deployed canary version.Since comparing a fresh deployment with an old, potentially degraded deployment, could produce unwanted results, it’s advised to deploy both a canary and a current production instance labelled baseline, next to each other.In the Deploy to PROD pipeline configure screen, add a stage in parallel with Find Image from DEV by clicking on the outer-left Configuration stage, and adding a new stage from that point on.        From that point forward, add another Deploy stage, with the prod server group as template.        At the bottom of the Deployment Cluster Configuration, switch the Container Image to the Find Image result for prod.        Add baseline as detail, and keep the strategy as None.        Save the pipeline.We now have our basic setup of both a baseline and canary server group to perform canary analysis.Canary AnalysisOur specific demo scenario uses Meeseeks from Rick and Morty as the new version to deploy.As people who watched the series probably will know, Meeseeks can quickly become a threat to our way of living if we let nature run its course.Therefore, when switching to Meeseeks, we also write Meeseeks in the logs to keep track of them.GCP uses Stackdriver for logging and monitoring, so if we’d like to use the logs as a source of information for canary analysis, we should make a Stackdriver metric using the Meeseeks logs.In the GCP left-hand menu, under the Stackdriver section, you can find Logging and drill down to Logs-based metrics.Add a new metric using the following filter, replacing the location and project_id by the zone name and project id from earlier in this guide:(resource.labels.cluster_name=\"spinnaker\" AND resource.labels.location=\"europe-west1-d\" AND resource.labels.namespace_name=\"default\" AND resource.labels.project_id=\"spinnaker-demo-184310\" AND textPayload:\"Meeseeks”)                Back in Spinnaker, head over to the Canary Configs view under Delivery.Create a new Canary Config called Demo-config, and add a filter template.The template will filter based on the replication controller of the server group:resource.labels.pod_name:\"${scope}\"        Now we can add actual metrics to analyse.Create a new Metrics Group called Meeseeks, with one metric underneath.        Since we’d also like to know whether our CPU or memory consumption has increased, let’s add some system metrics as well.We can investigate which filters we can construct by using the GCP REST API.Add a new group called Boring System Metrics, and add the following two metrics.                The only thing left to do for this Canary Config, is to configure thresholds for the Metric Groups.The marginal is treated as a lower bound.If an interval analysis fails to reach the marginal limit, the entire canary release will be halted and no further intervals will be analysed.The pass limit is the upper bound, qualifying the analysis as a success.Anything in between will be recorded and next intervals will be analysed.        Save the Canary Config, and go back to the Deploy to PROD pipeline configuration.Join both canary and baseline deployments into the Canary Analysis stage, by using the Depends On configuration.        Configure the canary analysis stage as follows.                Rollout or RollbackAfter the Canary Analysis has run, the new version can safely replace the existing production server group.Add a stage called Deploy to PROD, copying the production server group as template, and use the red/black (aka. blue/green) deployment strategy to avoid any downtime.        At the bottom of the Deployment Cluster Configuration, switch the Container Image to the Find Image result for DEV.                Regardless whether this pipeline actually succeeds or not, we need to make sure to clean up afterwards.Add a new pipeline called Tear Down Canary, with the following trigger.        Add two Destroy Server Group stages in parallel.        Configure the first one to destroy our baseline server group.        And finally also destroy the canary server group.        Running the demo scenarioAs explained in the introduction of the demo application, we have two versions of our application.As long as we keep deploying green versions with minor changes to other parts of the application (not impacting Meeseeks logs), the whole pipeline should pass, including the canary analysis.For a canary test to be successful, we need data.The more data our test can gather, the more informed the decision will be.In our demo scenario, we can continuously refresh the page to generate more load and more Meeseeks in the logs, but we can also use a script for that.In the root of the demo repository, a script called randomload.sh can be used to generate calls to the PROD ingress endpoint at a random interval.The script uses HTTPie to make calls, but you can also replace it with curl commands.Also, make sure you change the IP address in your forked repository’s file.SuccessA successful canary release would look like this.        Meeseeks logs should occur at a similar rate in the canary and the baseline server group.        CPU and RAM metrics are also part of the comparison.In the example below, the canary CPU metrics deviated too much from the baseline, resulting in a failure for that metric group.However, the weight of those metrics were not high enough to fail the verdict, but it did cause the outcome to be labeled marginal.                FailureWhen switching to the blue Meeseeks version, the initial DEV deploy would succeed, but our canary analysis should fail after one or two intervals.A failed canary release would look like this.        The canary server group generated a noticeable higher amount of Meeseeks than our baseline server group, resulting in a failed analysis.        Even though canary CPU and RAM metrics were quite in sync with the baseline, our Meeseeks metrics were enough to fail the entire pipeline.                ConclusionPickle Rick and Mr. Meeseeks have shown us the power of automated canary analysis using Spinnaker.There are still a few considerations we have to take into account, such as the importance of choosing the right metrics and filters and iterating on those after each canary release.Yet, having a tool like this at our disposal allows us to release more often to production, without compromising safety or quality.By reducing manual and ad hoc analysis only the most stable releases are deployed to production in a highly automated way."
      },
    
      "orchestration-2018-05-22-lagom-1-4-and-kubernetes-orchestration-html": {
        "title": "Lagom 1.4 and Kubernetes orchestration",
        "url": "/orchestration/2018/05/22/lagom-1-4-and-kubernetes-orchestration.html",
        "image": "/img/lagom-kubernetes.png",
        "date": "22 May 2018",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Introduction  Upgrading to Lagom 1.4  Lightbend’s orchestration tools  Adding Kubernetes support to our Lagom Shop Scala application  Building and publishing Docker images  Locally deploying to Kubernetes using Minikube  Generating Kubernetes resources for production  Conclusion  Extra resourcesIntroductionIn this blog post we will take a closer look at the Lightbend Orchestration tools.Tools helping you deploy your Lagom application to Kubernetes and DC/OS.It was already possible to deploy Lagom applications to Kubernetes as this guide demonstrates but it involved more manual tasks and having to write the Kubernetes resource and configuration files yourself, as it usually goes.As it currently stands, the tools are only supported in combination with sbt so Maven users cannot fully take advantage of it just yet.Maven support will follow soon however as the Maven equivalent plugin is nearing its first release version.If you are new to Lagom feel free to take a look at one of our earlier blog posts on Lagom:  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom 1.2: What’s new?Before getting to that though, we will upgrade our sample application Lagom Shop Scala, which was also referred to in our previous blog posts, from Lagom 1.2 to Lagom 1.4 to demonstrate the upgrading process.The application consists of two Lagom microservices combined with a frontend written in Play Framework.Afterwards we will take a closer look at how easy it is to integrate the Lightbend Orchestration tools into our project and how we can get our project up and running on Kubernetes.Note that this blog post is not an in-depth guide on the tools themselves but more a general overview and for us to share our impressions.Upgrading to Lagom 1.4In this section we will upgrade our sample application Lagom Shop Scala, from version 1.3.4 to 1.4.x.Lagom 1.4 uses Play Framework’s latest version, 2.6 for which we will also need to change a few things in our project.Lightbend provides a migration guide for each new version they release, so in this case we followed the Lagom 1.4 Migration Guide and the Play 2.6 Migration Guide.When upgrading multiple minor versions, it is advised to upgrade one minor version at a time to smoothen the process.In our case we are only limited to upgrading a single minor version so we can just use the latest migration guide right away.Upgrade the Lagom version in project/plugins.sbt:addSbtPlugin(\"com.lightbend.lagom\" % \"lagom-sbt-plugin\" % \"1.4.5\")Upgrade the sbt version in project/build.properties:sbt.version=0.13.16Upgrade the Scala version to 2.12.4 in build.sbt:scalaVersion in ThisBuild := \"2.12.4\"Upgrade Play JSON Derived Codecs to 4.0.0 which adds Play 2.6 support:val playJsonDerivedCodecs = \"org.julienrf\" %% \"play-json-derived-codecs\" % \"4.0.0\"Replace play.api.data.validation.ValidationError with play.api.libs.json.JsonValidationError.Mix in LagomConfigComponent, HttpFiltersComponents and AssetsComponents and remove lazy val assets: Assets = wire[Assets] in the application loader class extending BuiltInComponentsFromContext in the Play frontend project.abstract class Frontend(context: Context) extends BuiltInComponentsFromContext(context)  with I18nComponents  with AhcWSComponents  with LagomKafkaClientComponents  with LagomServiceClientComponents  with LagomConfigComponent  with HttpFiltersComponents  with AssetsComponents {Change override def describeServices to override def describeService in each Lagom project’s class extending LagomServerComponents as the other one has become deprecated.override def describeService = Some(readDescriptor[ItemService])Implement CSRF security in the frontend project by utilising CSRF form fields (@CSRF.formField) or one of the other approaches.Note that Lagom’s development mode service locator now listens on port 9008 instead of 8000 although this can still be changed by overriding the default port.To see a complete list of changes we did, refer to commit bdf5ecff.Lightbend’s orchestration toolsAs we mentioned in the introduction, Lightbend offers a developer-centric suite of tools helping you deploy your Play/Akka/Lagom applications to Kubernetes and DC/OS.The tools help you create a Docker image of all your applications, help with generating Kubernetes and DC/OS resource and configuration files based on the Docker images, and they allow you to deploy your whole Lagom project to Kubernetes using a simple command which can be pretty convenient for development. The generated JSON and YAML files could be put under version control after which they can be submitted to a CI/CD integrated central repository.The suite consists of three different tools:  sbt-reactive-app, an sbt plugin that inspects your projects and builds annotated Docker images.The Maven equivalent plugin is still being worked on and is nearing its first release version.  reactive-cli, a command-line tool with which you generate the Kubernetes and DC/OS resource and configuration files.You need to install this on the device or environment from which you will deploy to Kubernetes.Install guidelines are available in the documentation.For Mac for example this is easily accomplished with Homebrew:    brew tap lightbend/tools &amp;&amp; brew install lightbend/tools/reactive-cli    reactive-lib, a library for your application that is automatically included in your application’s build by the sbt-reactive-app sbt plugin.It allows your application to perform service discovery, access secrets, define health and readiness checks, and more as it understands the conventions of the resources generated by the command-line tool.Adding Kubernetes support to our Lagom Shop Scala applicationWe start off with adding the sbt-reactive-app sbt plugin in the project/plugins.sbt file:addSbtPlugin(\"com.lightbend.rp\" % \"sbt-reactive-app\" % \"1.1.0\")Now enable the plugin for each module in the build.sbt file:lazy val itemImpl = (project in file(\"item-impl\"))  .dependsOn(itemApi)  .settings(commonSettings: _*)  .enablePlugins(LagomScala, SbtReactiveAppPlugin)  lazy val orderImpl = (project in file(\"order-impl\"))  .dependsOn(orderApi, itemApi)  .settings(commonSettings: _*)  .enablePlugins(LagomScala, SbtReactiveAppPlugin)  .settings(    libraryDependencies ++= Seq(      lagomScaladslPersistenceCassandra,      lagomScaladslTestKit,      lagomScaladslKafkaBroker,      cassandraDriverExtras,      macwire,      scalaTest    )  )  .settings(lagomForkedTestSettings: _*)  lazy val frontend = (project in file(\"frontend\"))  .dependsOn(itemApi, orderApi)  .settings(commonSettings: _*)  .enablePlugins(PlayScala &amp;&amp; LagomPlay, SbtReactiveAppPlugin)  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies ++= Seq(      lagomScaladslServer,      lagomScaladslKafkaClient,      macwire,      scalaTest,      \"org.webjars\" % \"foundation\" % \"6.2.3\",      \"org.webjars\" % \"foundation-icon-fonts\" % \"d596a3cfb3\"    ),    EclipseKeys.preTasks := Seq(compile in Compile),    httpIngressPaths := Seq(\"/\")  )If you also have a frontend module it is important to define the httpIngressPaths, as you might have seen in the code sample above, in order to have your frontend be accessible from outside the cluster.Mix in the LagomServiceLocatorComponents trait in each module’s application loader:import com.lightbend.rp.servicediscovery.lagom.scaladsl.LagomServiceLocatorComponentsclass ItemApplicationLoader extends LagomApplicationLoader {  override def load(context: LagomApplicationContext): LagomApplication =    new ItemApplication(context) with LagomServiceLocatorComponents {      override lazy val circuitBreakerMetricsProvider = new CircuitBreakerMetricsProviderImpl(actorSystem)    }}Building and publishing Docker imagesThe tool suite comes with an easy way to deploy all your services to Minikube for development so you will want to set that up first.For installation instructions, consult the Minikube documentation.Start up Minikube with a sufficient amount of memory:$ minikube start --memory 8192Starting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file.You kind of need a decent device with a decent amount of RAM.I have tested all of this on a MacBook Pro with 8GB RAM and I simply was not able to assign a sufficient amount of memory to comfortably run everything locally.Execute the following command to have our Docker environment variables point to Minikube:eval $(minikube docker-env)We can check if Minikube is up and running with the following command:$ kubectl get nodesNAME       STATUS    ROLES     AGE       VERSIONminikube   Ready     &lt;none&gt;    1m        v1.8.0To figure out your Minikube IP you can utilise the following command:$ echo \"http://$(minikube ip)\"http://192.168.99.100It is also important to enable the Ingress addon:minikube addons enable ingressWe need to launch sbt:$ sbtAfter which we can publish the images locally (you might want to grab a coffee after executing this):docker:publishLocalUnderneath, SBT Native Packager is being used.Check out its documentation for Docker related configurations.Publishing to a Docker Registry is also possible by defining the dockerRepository sbt setting in the project and after authenticating to the registry, see Publishing to a Docker Registry.After doing so you can execute the publish command:docker:publishOur Docker images should then be available:$ docker imagesREPOSITORY             TAG                 IMAGE ID            CREATED             SIZEorderimpl              1.0-SNAPSHOT        e9be41c50eb2        32 seconds ago      159MBitemimpl               1.0-SNAPSHOT        357a9d546593        9 minutes ago       159MBfrontend               1.0-SNAPSHOT        7251c13a5373        6 days ago          141MBAs for the Maven equivalent plugin, the Docker images can be build by executing:$ mvn installLocally deploying to Kubernetes using MinikubeFor development we can make use of an sbt task for deploying everything to Kubernetes using Minikube.But before that, we need to do a couple of things.We first need to set up Lightbend’s Reactive Sandbox which is a Docker image that contains the usual components used when developing reactive applications using the Lightbend frameworks:  Cassandra  Elasticsearch  Kafka  ZooKeeperWe will use Helm, a package manager for Kubernetes, to set it up.Install instructions of Helm are available on the GitHub repository.For Mac for example you can install it using brew:$ brew install kubernetes-helmWith Helm we can then install the Reactive Sandbox:helm inithelm repo add lightbend-helm-charts https://lightbend.github.io/helm-chartshelm updatehelm install lightbend-helm-charts/reactive-sandbox --name reactive-sandboxAll set up, we can now utilise an sbt command to deploy all our services to Minikube!Start sbt in the root of your project:$ sbtAnd deploy everything to Minikube using a sbt task that comes with the sbt-reactive-app plugin.The task however is not yet supported on Windows unfortunately.deploy minikubeDuring the setup we encountered a connection initialisation error of Helm:Cannot initialize Kubernetes connection: Get http://localhost:8080/api: dial tcp 127.0.0.1:8080: getsockopt: connection refusedWe found a solution for this in the following Helm GitHub issue: Tiller pods can’t connect to k8s apiserver #2464.$ kubectl --namespace=kube-system create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:defaultIf all went well your application should be accessible at https://192.168.99.100.You can use the Minikube dashboard to inspect everything at http://192.168.99.100:30000.  Note that you can also deploy a single module instead of all of them.For example if we only want to deploy the frontend, you simply switch to the specific project before running the command:$ sbt[info] Loading global plugins from /Users/yannickdeturck/.sbt/0.13/plugins[info] Loading project definition from /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/project[info] Set current project to lagom-shop-scala (in build file:/Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/)&gt; projects[info] In file:/Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/[info] \t   common[info] \t   frontend[info] \t   itemApi[info] \t   itemImpl[info] \t   lagom-internal-meta-project-cassandra[info] \t   lagom-internal-meta-project-kafka[info] \t   lagom-internal-meta-project-service-locator[info] \t * lagom-shop-scala[info] \t   orderApi[info] \t   orderImpl&gt; project frontend[info] Set current project to frontend (in build file:/Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/)[frontend] $ deploy minikube[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/frontend/target/scala-2.12/frontend_2.12-1.0-SNAPSHOT.pom[info] Packaging /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/frontend/target/scala-2.12/frontend_2.12-1.0-SNAPSHOT-sources.jar ...[info] Done packaging.[info] Packaging /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/frontend/target/scala-2.12/frontend_2.12-1.0-SNAPSHOT.jar ...[info] Done packaging.[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/common/target/scala-2.12/common_2.12-1.0-SNAPSHOT.pom[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/item-api/target/scala-2.12/itemapi_2.12-1.0-SNAPSHOT.pom[info] Wrote /Users/yannickdeturck/Documents/Git-projects/lagom-shop-scala/order-api/target/scala-2.12/orderapi_2.12-1.0-SNAPSHOT.pom13:23:14.079 [pool-7-thread-1] DEBUG com.lightbend.lagom.internal.api.tools.ServiceDetector - Loading service discovery class: FrontendLoader[info] Sending build context to Docker daemon  54.52MB[info] Step 1/9 : FROM openjdk:8-jre-alpine[info]  ---&gt; b1bd879ca9b3[info] Step 2/9 : RUN /sbin/apk add --no-cache bash[info]  ---&gt; Using cache[info]  ---&gt; 193af79a4475[info] Step 3/9 : WORKDIR /opt/docker[info]  ---&gt; Using cache[info]  ---&gt; 741d2377c4e8[info] Step 4/9 : ADD --chown=daemon:daemon opt /opt[info]  ---&gt; Using cache[info]  ---&gt; d7884eead001[info] Step 5/9 : USER daemon[info]  ---&gt; Using cache[info]  ---&gt; cdedfe6fc10c[info] Step 6/9 : ENTRYPOINT [][info]  ---&gt; Using cache[info]  ---&gt; 2db1227ffe9e[info] Step 7/9 : CMD [][info]  ---&gt; Using cache[info]  ---&gt; bd147ead79fd[info] Step 8/9 : COPY rp-start /rp-start[info]  ---&gt; Using cache[info]  ---&gt; 340110c7c251[info] Step 9/9 : LABEL com.lightbend.rp.app-name=\"frontend\" com.lightbend.rp.applications.0.name=\"default\" com.lightbend.rp.applications.0.arguments.0=\"/rp-start\" com.lightbend.rp.applications.0.arguments.1=\"bin/frontend\" com.lightbend.rp.app-version=\"1.0-SNAPSHOT\" com.lightbend.rp.app-type=\"lagom\" com.lightbend.rp.config-resource=\"rp-application.conf\" com.lightbend.rp.modules.akka-cluster-bootstrapping.enabled=\"false\" com.lightbend.rp.modules.akka-management.enabled=\"false\" com.lightbend.rp.modules.common.enabled=\"true\" com.lightbend.rp.modules.play-http-binding.enabled=\"true\" com.lightbend.rp.modules.secrets.enabled=\"false\" com.lightbend.rp.modules.service-discovery.enabled=\"true\" com.lightbend.rp.modules.status.enabled=\"false\" com.lightbend.rp.endpoints.0.name=\"http\" com.lightbend.rp.endpoints.0.protocol=\"http\" com.lightbend.rp.endpoints.0.ingress.0.type=\"http\" com.lightbend.rp.endpoints.0.ingress.0.ingress-ports.0=\"80\" com.lightbend.rp.endpoints.0.ingress.0.ingress-ports.1=\"443\" com.lightbend.rp.endpoints.0.ingress.0.paths.0=\"/\" com.lightbend.rp.sbt-reactive-app-version=\"1.1.0\"[info]  ---&gt; Using cache[info]  ---&gt; 7251c13a5373[info] Successfully built 7251c13a5373[info] Successfully tagged frontend:1.0-SNAPSHOT[info] Built image frontend:1.0-SNAPSHOT[info] deployment.apps \"frontend-v1-0-snapshot\" deleted[info] service \"frontend\" deleted[info] ingress.extensions \"frontend\" deleted[info] deployment.apps \"frontend-v1-0-snapshot\" created[info] service \"frontend\" created[info] ingress.extensions \"frontend\" created[success] Total time: 19 s, completed May 11, 2018 1:23:31 PMWe can inspect everything with kubectl:$ kubectl get podsNAME                                     READY     STATUS    RESTARTS   AGEfrontend-v1-0-snapshot-cbdbdb68b-rbrrx   1/1       Running   2          6dreactive-sandbox-74fd955ddd-cjpw8        1/1       Running   7          6d$ kubectl get servicesNAME                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGEfrontend                         ClusterIP   10.103.128.78    &lt;none&gt;        10000/TCP                       6ditem                             ClusterIP   10.109.117.169   &lt;none&gt;        10000/TCP,10001/TCP,10002/TCP   6dkubernetes                       ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                         6dreactive-sandbox-cassandra       ClusterIP   None             &lt;none&gt;        9042/TCP                        6dreactive-sandbox-elasticsearch   ClusterIP   None             &lt;none&gt;        9200/TCP                        6dreactive-sandbox-kafka           ClusterIP   None             &lt;none&gt;        9092/TCP                        6dreactive-sandbox-zookeeper       ClusterIP   None             &lt;none&gt;        2181/TCP                        6d$ kubectl get ingNAME       HOSTS     ADDRESS   PORTS     AGEfrontend   *                   80        6ditem       *                   80        6d$ kubectl get deployNAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEfrontend-v1-0-snapshot   1         1         1            1           6dreactive-sandbox         1         1         1            1           6dThere is currently no equivalent for Maven to deploy everything to Minikube in one simple command.In the next section however, we will look at how we can utilise the reactive-cli tool for generating resource and configuration files, and deploying everything to Kubernetes which is also the way to go for development in this case.Generating Kubernetes resources for productionThe following workflow could also be used for development but it is more suited for deploying to your production environment as the deploy minikube workflow in the previous section simplifies a lot of things for you.We will make use of the reactive-cli command-line tool to have it generate the Kubernetes resource and configuration files.Installing is easy, as described in the documentation.For Mac for example we can do this using Homebrew:brew tap lightbend/tools &amp;&amp; brew install lightbend/tools/reactive-cliVerifying if it was installed correctly can be done by checking the version:$ rp versionrp (Reactive CLI) 1.1.1jq support: UnavailableNow we can utilise it to generate Kubernetes resources.In the previous section we deployed our frontend and item service but we also have our order service to deploy.Let us generate the Kubernetes resources and deploy it to Minikube.$ rp generate-kubernetes-resources \"orderimpl:1.0-SNAPSHOT\" --generate-services --generate-pod-controllers --pod-controller-replicas 2 --env JAVA_OPTS=\"-Dplay.http.secret.key=simple\"---apiVersion: \"apps/v1beta2\"kind: Deploymentmetadata:  name: \"order-v1-0-snapshot\"  labels:    appName: order    appNameVersion: \"order-v1-0-snapshot\"spec:  replicas: 2  selector:    matchLabels:      appNameVersion: \"order-v1-0-snapshot\"  template:    metadata:      labels:        appName: order        appNameVersion: \"order-v1-0-snapshot\"    spec:      restartPolicy: Always      containers:        - name: order          image: \"orderimpl:1.0-SNAPSHOT\"          imagePullPolicy: IfNotPresent          env:            - name: \"JAVA_OPTS\"              value: \"-Dplay.http.secret.key=simple\"            - name: \"RP_APP_NAME\"              value: order            - name: \"RP_APP_TYPE\"              value: lagom            - name: \"RP_APP_VERSION\"              value: \"1.0-SNAPSHOT\"            - name: \"RP_DYN_JAVA_OPTS\"              value: \"-Dakka.discovery.kubernetes-api.pod-namespace=$RP_NAMESPACE\"            - name: \"RP_ENDPOINTS\"              value: \"HTTP,AKKA_REMOTE,AKKA_MGMT_HTTP\"            - name: \"RP_ENDPOINTS_COUNT\"              value: \"3\"            - name: \"RP_ENDPOINT_0_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_0_BIND_PORT\"              value: \"10000\"            - name: \"RP_ENDPOINT_0_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_0_PORT\"              value: \"10000\"            - name: \"RP_ENDPOINT_1_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_1_BIND_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_1_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_1_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_2_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_2_BIND_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_2_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_2_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_BIND_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_MGMT_HTTP_PORT\"              value: \"10002\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_BIND_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_AKKA_REMOTE_PORT\"              value: \"10001\"            - name: \"RP_ENDPOINT_HTTP_BIND_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_HTTP_BIND_PORT\"              value: \"10000\"            - name: \"RP_ENDPOINT_HTTP_HOST\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_ENDPOINT_HTTP_PORT\"              value: \"10000\"            - name: \"RP_JAVA_OPTS\"              value: \"-Dconfig.resource=rp-application.conf -Dakka.discovery.method=kubernetes-api -Dakka.management.cluster.bootstrap.contact-point-discovery.effective-name=order -Dakka.management.cluster.bootstrap.contact-point-discovery.required-contact-point-nr=2 -Dakka.discovery.kubernetes-api.pod-label-selector=appName=%s\"            - name: \"RP_KUBERNETES_POD_IP\"              valueFrom:                fieldRef:                  fieldPath: \"status.podIP\"            - name: \"RP_KUBERNETES_POD_NAME\"              valueFrom:                fieldRef:                  fieldPath: \"metadata.name\"            - name: \"RP_MODULES\"              value: \"akka-cluster-bootstrapping,akka-management,common,play-http-binding,service-discovery,status\"            - name: \"RP_NAMESPACE\"              valueFrom:                fieldRef:                  fieldPath: \"metadata.namespace\"            - name: \"RP_PLATFORM\"              value: kubernetes          ports:            - containerPort: 10000              name: http            - containerPort: 10001              name: \"akka-remote\"            - containerPort: 10002              name: \"akka-mgmt-http\"          volumeMounts: []          command:            - \"/rp-start\"          args:            - \"bin/orderimpl\"          readinessProbe:            httpGet:              path: \"/platform-tooling/ready\"              port: \"akka-mgmt-http\"            periodSeconds: 10          livenessProbe:            httpGet:              path: \"/platform-tooling/healthy\"              port: \"akka-mgmt-http\"            periodSeconds: 10            initialDelaySeconds: 60      volumes: []---apiVersion: v1kind: Servicemetadata:  labels:    appName: order  name: orderspec:  ports:    - name: http      port: 10000      protocol: TCP      targetPort: 10000    - name: \"akka-remote\"      port: 10001      protocol: TCP      targetPort: 10001    - name: \"akka-mgmt-http\"      port: 10002      protocol: TCP      targetPort: 10002  selector:    appName: orderYou could store the generated resources and tune it, but it is also possible to just generate everything that is necessary and just deploy it right away by chaining kubectl apply:$ rp generate-kubernetes-resources \"orderimpl:1.0-SNAPSHOT\" --generate-services --generate-pod-controllers --pod-controller-replicas 2 --env JAVA_OPTS=\"-Dplay.http.secret.key=simple\" | kubectl apply -f -deployment.apps \"order-v1-0-snapshot\" createdservice \"order\" createdWe can verify whether it is up and running:$ kubectl get deploymentsNAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEfrontend-v1-0-snapshot   1         1         1            1           6dorder-v1-0-snapshot      2         2         2            2           6mreactive-sandbox         1         1         1            1           6d$ kubectl get podsNAME                                     READY     STATUS    RESTARTS   AGEfrontend-v1-0-snapshot-cbdbdb68b-mwfqq   1/1       Running   1          50morder-v1-0-snapshot-5884754595-65wxd     1/1       Running   0          4morder-v1-0-snapshot-5884754595-wdbng     1/1       Running   0          4mreactive-sandbox-74fd955ddd-cjpw8        1/1       Running   7          6dConclusionUpgrading to Lagom 1.4.x and Play 2.6 went pretty smooth as the migration guides of Lightbend cover mostly everything in detail as usual.The orchestration tools make it pretty easy to test your Lagom application running in Kubernetes locally while still having the possibility to fine-tune the generated resource and configuration files.Integrating the tool suite into our project went smoothly.Kubernetes has gained a lot of popularity lately and with this, Lagom shows that it wants to embrace Kubernetes to deploy your applications onto next to ConductR.The single deploy minikube command is not yet supported on Windows but we imagine that it will be in the near future.Windows users can still utilise the reactive-cli command-line tool to generate the resource and configuration files and deploy it themselves via kubectl on their Minikube.Maven users will only need to wait a little bit longer to take advantage of most things the tool suite has to offer as the plugin is nearing its first release version.Extra resources  Our Lagom Shop Scala application GitHub repository  Lightbend Orchestration Documentation  Kubernetes instructions for the Lagom online-auction-scala application  Hello-World for Lightbend Orchestration for Kubernetes for a sample Play application  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom 1.2: What’s new?  Lagom Documentation  Kubernetes Documentation"
      },
    
      "agile-2018-04-30-scrum-vs-kanban-html": {
        "title": "Scrum VS Kanban",
        "url": "/agile/2018/04/30/scrum-vs-kanban.html",
        "image": "/img/agile/scrum-vs-kanban/boxers.jpg",
        "date": "30 Apr 2018",
        "category": "post, blog post, blog",
        "content": "Scrum VS Kanban  Scrum and Kanban are both agile frameworks that are used to deliver software, or any other sort of project for that matter, in an incremental and iterative way. Both terms have become incorrectly interchangeable. Although there are many similarities between the two, understanding the key differences between both Scrum and Kanban will bring clarity.The basicsAs with all agile frameworks, we have a product owner, a development team and an intermediate person, known in Scrum as the Scrum Master and in Kanban he or she is known as the Agile Coach.All three work together to get the most value to a client in the shortest amount of time.  Work is divided into stories and those stories are put on a product backlog. So far, so good, both Scrum and Kanban do the same at the very basic level. The way both handle these stories is where the difference lies.ScrumA Scrum team works in Sprints, a Time-boxed period in which the development team delivers a new iteration of the product.Most commonly these Sprints last 2 weeks, although 3 or even 4 weeks Sprints aren’t a rare sight.Each Sprint start with a Sprint planning meeting. In this meeting the development team, Scrum Master and Product Owner decide which stories the development team will tackle during this Sprint. These selected stories are known as the Sprint Backlog. This backlog should not change during the Sprint.On a daily basis in the Sprint, there is a Daily Standup meeting, a 15-minute standup meeting in which every member of the development team answers three simple questions: ‘What have I done yesterday’, ‘What will I do today’ and ‘Are there any problems that block me in going forward with my task’.After a Sprint there are two rituals; the Sprint review meeting, where all stakeholders are invited to a demonstration of new functionalities added to the product;and the Sprint Retrospective meeting, in which the development team discusses what went well, what went wrong and what could be improved in the next Sprint.The goal of the retrospective is to improve the way of working in the team in comparison to the previous Sprint.Some pitfalls of ScrumLet it be clear that Scrum isn’t as easy as it seems, Scrum has its pitfalls.Most of these can be avoided by making these issues clear from the start of a project.Some examples are:The Product owner not being available for questions of the teamWhen a Product Owner is always in meeting, or otherwise engaged, it could happen that the development team has to wait longer for answers on their questions, and by waiting for answers blocking development.Extending the Sprint with just a couple of daysSome teams will try to delay the end of a Sprint by a couple of days, just to make sure everything is ‘Done’. Every item that is not ‘Done’ at the end of a Sprint goes back into the Product Backlog.No exceptions. Extending it by just a couple of days would create a desync in the rhythm and possibly create a desync with another team that depends on your team.Changing the Sprint lengthChanging the Sprint length during a project is just not done. It will disrupt the team’s flow and the velocity of the team will go down. Once you’ve chosen a Sprint Length, your team should stick to it.Not having the same Sprint rhythm as another teamIn larger projects, teams will be dependent on one another. Not having the same Sprint rhythm will cause issues because you’ll have to wait for the other team to be ready for you, or vice versa, which would once again block progress.The Scrum Master being too involvedA Scrum master is a dedicated role in the team.It is very hard to be a Scrum master and develop at the same time. He or she should be like a firefighter, be idle at times, but be ready when an emergency occurs.Trying to solve problems within a Daily StandupThe Standup has to be a short time boxed meeting. Answering the questions and going forward. If there are problems, they should be handled before or after the Standup, not during.KanbanKanban is a continuous process. There isn’t a Sprint backlog. Only the columns on the board and the prioritized product backlog.The Kanban Board is crucial within Kanban. It is usually split into three columns: To Do, Doing and Done. More columns can be added to fit your needs but those three have to be there.To limit the work that can be done there is a Work in Progress limit on every column on the Kanban board which is related to the team’s capacity. For example, a team of four developers would be able to handle 6 to 8 stories at the time. The lower the limit, the faster work will pass through the board. Having it too low though will result in a bottleneck situation.Kanban has a Daily Standup meeting, a 15-minute standup meeting in which every member of the development team answers three simple questions: What have I done yesterday, what will I do today, are there any problems that block me in going forward with my task.Demo meetings are held at regular intervals.During these meetings all stakeholders are invited to a demonstration of the new functionality since the last demo.Kanban also has retrospective meetings, which have the goal to improve the way of working of the team and solve any issues that have risen since the last retrospective.In Kanban there are multiple ways you can do retrospectives.One way would be to do them on a regular basis (bit Scrum like). Although this doesn’t fit in the Kanban way of working,this provides a welcome disruption of the daily workflow and if conducted well the retrospective will result in a morale boost for the team since they have been listened to and know problems they have will be handled or have been handled during the retro.Since having retrospectives on a regular basis kind of goes against the way of working in Kanban, a simpler way of holding retrospective has been popping up the ‘Stop and Solve‘. This means that every time there is an issue (that the person who encountered it can’t solve on their own), an immediate retrospective involving the full team is called. Everyone looks into the problem, and brainstorms for possible solutions until a solution is found.There are pros and cons on both ways of working; what is chosen depends on the team.Some pitfalls of KanbanLike with every framework, Kanban has its pitfalls. Some examples:Kanban board not being up-to-dateOne of the most important pieces of Kanban is the Kanban board, if that is not up-to-date it will result in the team not having a up-to-date view of what needs to be done.You would be looking at the past.Not keeping to the WIP limitHaving too much tasks in one column will reduce the overview the team has, and tasks will get ‘lost’.A solution would be to implement physical limits to the board by drawing a line.Is there a Holy Grail?The answer to that question is quite simply: No.It all depends on the situation you’re in. Scrum would not work for teams that do not have clear business requirements. What we mean by that is a systems team or a helpdesk team. Kanban would be way more useful for these sort of teams; high priority tickets appear on the backlog and are handled as fast as possible by the team.An advantage of Scrum compared to Kanban is that the business will know what will be delivered during the next Sprint.Kanban is more dependent on the incoming requests.As said before; it all depends on the situation you’re in. Analyze the situation before deciding which agile framework will work best for your team!"
      },
    
      "spring-2018-04-28-spring-cloud-contract-meet-pact-html": {
        "title": "Spring Cloud Contract, meet Pact",
        "url": "/spring/2018/04/28/Spring-Cloud-Contract-meet-Pact.html",
        "image": "/img/2018-04-28-Spring-Cloud-Contract-meet-Pact/post-image.jpg",
        "date": "28 Apr 2018",
        "category": "post, blog post, blog",
        "content": "CDCT, or Consumer-Driven Contract TestingConsumer-driven contract tests are actually integration tests that are targetting your API, whether it’s REST-based or messaging-based.Imagine you’re working on an application that exposes its data using a REST API.Another team is using your exposed data for some functionality that they are providing.In order to guarantee that the functionality of the other team their application doesn’t break if we make changes to our API, we create a contract between the two teams.Key in this setup is that the contracts are defined by the consumer of your API instead of the developer that wrote the implementation of a certain functionality.With this aproach we can generate tests by using those consumer-driven contracts, and verify whether we’re going to break any of our consumers’ applications.You can find a couple of interesting and useful links on the bottom of this post which go further into detail.Spring Cloud Contract provides us with JVM-based libraries, allowing us to generate Groovy contracts, package them as a jar, and upload them to an artifact repository like Nexus or Artifactory.Great, but that means we can only use these contracts between two parties that are using a JVM language.What if the consumer of our API is a NodeJS or .NET microservice, or even an Angular application?For consumers or producers written in other languages, the lack of library support might be a concern.With the latest Spring Cloud release, that’s a thing of the past!Spring Cloud Contract, meet PactThe first release candidate of Spring Cloud Finchley, which was released on the 25th of April, also ships the first release candidate of Spring Cloud Contract.Our first release candidate of the Spring Cloud Finchley release train has been released.  Checkout the blog post for more information and as always feedback is welcome! https://t.co/8TK0tudkzr&mdash; Spring Cloud (@springcloud) April 25, 2018Spring Cloud Contract has updated its support for Pact and added the support to connect to a Pact broker.The Pact broker acts as a repository for sharing the Pact contracts and verification results.The most awesome feature in my opinion is the visualisation of the contracts between all known parties.In the UI you can see the last time there was a new version of a contract published, when it was verified and what status it had, and last but not least the details of the contract.                  List of contracts between parties with the timestamps and status                        Details of a contract between two parties                        A graph showing the dependencies between all known consumers and producers      Simply add the spring-cloud-contract-pact dependency, which will add the Pact Converter and Pact Stub Downloader, together with some configuration and you’re good to go!The Pact contracts will be retrieved from the Pact broker and converted to Spring Cloud Contract contracts for you, so these in turn can be used to generate stubs and tests.It’s also possible to write Spring Cloud Contract contracts, convert them to Pact contracts and upload them to the Pact broker.The upside to it is that these contracts are in fact language agnostic as opposed to Spring Cloud Contract, the downside to it is that it lacks some functionality regarding messaging compared to Spring Cloud Contract.Currently Pact contracts up until v4 are supported, which means that both request-response and messaging contracts can be used.Note that not all functionality is supported though, as is described in this section of the reference documentation.A typical workflow to define such contracts could look like this:  The consumer of the data writes a test for the feature by doing TDD.In this test the contract is defined which will be used as a stub.  Next up the missing implementation is written.  When we push our changes to Git our CI pipeline is triggered, in which we’ll upload all defined contracts to a central artifact repository, eg. a Pact broker, after all our tests have been run.  After our code’s been pushed, we’ll clone the producer’s application.  We create a test that uses the newly added CDC and file a pull request.  The team that works on the producer’s application can take over the pull request and add changes where necessary.In case you want to go the extra mile: you can also use the same approach for consumer-driven changes within the producer’s application!If you want a new REST endpoint or a message that will be delivered through some messaging system like Kafka or RabbitMQ, you can add an initial implementation of the missing functionality in the pull request.The other team then only needs to review the changes and add some adjustments if needed.Both teams can really benefit from such workflow: the consumer’s team doesn’t need to wait until the producer’s team has time to create the implementation, while the producer’s team can keep their focus on what’s on their product backlog.Useful linksThe three-second or three-minute tour give you a nice very brief walkthrough of Spring Cloud Contract.Consumer Driven Contracts and Your Microservice Architecture by Marcin Grzejszczak (@mgrzejszczak) and Adib Saikali (@asaikali).A step-by-step guide to Consumer Driven Contracts with Spring Cloud Contract.Implementation of the consumer driven contract library Pact for Javascript, from creating a test to generating the contract and uploading it to the Pact broker."
      },
    
      "spring-2018-04-16-spring-boot-2-0-anniversary-meetup-html": {
        "title": "Spring Boot 2.0 Anniversary Meetup",
        "url": "/spring/2018/04/16/spring-boot-2-0-anniversary-meetup.html",
        "image": "/img/spring-boot-2/051-thumb.jpg",
        "date": "16 Apr 2018",
        "category": "post, blog post, blog",
        "content": "  On March 1st, Spring Boot reached GA on its second major version.To celebrate this, we invited Spring Boot legend Stéphane Nicoll to give us an in-depth view on what’s new in Spring Boot 2.He talked about the new features while migrating a Spring Boot 1.x application to Spring Boot 2.0.Spring Boot 2.0Stéphane gave us an overview of the new features in Spring Boot 2.0.It was kind of a summary of Phil Webb’s announcement post Spring Boot 2.0 goes GA.Code says more than a thousand words.And like every talk I attended from Stéphane, he started live coding quite quickly.We migrated a Spring Boot 1.x application to Spring Boot 2.0.The migration process is very simple.In short, these are the steps you have to follow:  Change Spring Boot parent version number in your pom.xml  Replace deprecated property keys with the help of the spring-boot-properties-migrator module  If you’re working with passwords, define a PasswordEncoder  When you use a lot of hookpoints and Spring Boot classes directly, eg. SpringBootServletInitializer, migrating is slightly more work.The reason behind this is that a lot of the Spring Boot API’s have changed.Change version numberChange the Spring Boot Starter Parent version number in your pom.xml to the new Spring Boot version.&lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;2.0.1.RELEASE&lt;/version&gt;&lt;/parent&gt;As from April 5th, this is the 2.0.1.RELEASE.Stéphane released it on the train to the Ordina HQ in Mechelen.Spring Boot 2.0.1 available nowhttps://t.co/WpYx0Pwff8@springboot @springcentral @ProjectReactor&mdash; Stéphane Nicoll (@snicoll) 5 april 2018Property keys and the properties migratorProperty key changesWhen you upgrade, you will get compilation errors saying there are unknown properties in your properties files.Some property keys have been deprecated and won’t work anymore.            Old property      New property                  spring.datasource.initialize      spring.datasource.initialization-mode              endpoints.health.path      management.endpoints.web.path-mapping      All major IDEs, eg. IntelliJ, Netbeans and STS, will inform you about the newer property key.Properties migratorYou can add the spring-boot-properties-migrator module to your Maven project.&lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-properties-migrator&lt;/artifactId&gt;  &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;The Spring Boot Properties Migrator looks for configuration properties it can replace and provides the programmer with feedback.For example, endpoints.health.path will automatically be replaced by the new management.endpoints.web.path-mapping property.Spring Boot 2.0 will also tell you you’re using an old key when you provide old keys as environment variable.It will not tell you the line on which you defined the property, but it will give an appropriate message.Some properties cannot be fixed automatically:  The security auto-configuration is no longer customizable  Replacement key 'spring.datasource.initialization-mode' uses an incompatible target type — String was replaced by an enumerationSpring Security changesAs from Spring Security 5.x, a lot of security features were redesigned.Some features are made more strict.Since Spring Boot 2.x now uses Spring Security 5.x under the hood, the Spring Security autoconfiguration has been redesigned as well.Previously with Spring Boot 1.x, you could have Spring Security configuration spread accross your application.With Spring Boot 2.0, if you want to know what the security configuration of your application is, you only have to look at one file, your SecurityConfiguration class.No password encoderWhen you haven’t defined a PasswordEncoder bean, Spring will throw the There is no PasswordEncoder mapped for the id \"null\" error when creating your application context.As from Spring Security 5.x, Spring Security enforces you to use a password encoder.Spring Security enforces this by activating the default DelegatingPasswordEncoder, which looks for PasswordEncoder beans.By adding a BCryptPasswordEncoder, the DelegatingPasswordEncoder will return that instance to encrypt passwords.@Beanpublic BCryptPasswordEncoder passwordEncoder() {    return new BCryptPasswordEncoder();}  If you really want to, you can override password encoding by adding {noop} to the password value.This will treat the password by activating the NoOpPasswordEncoder instead of the default DelegatingPasswordEncoder and will treat your password as plain text.Please note that this is not recommended if you deploy your app to a production environment!Actuator endpointsSome Actuator security endpoint settings aren’t modifyable by properties anymore.For example, management.security.roles=HERO won’t be picked up anymore.Only two endpoints are being exposed by default, /info and /health.You can include or exclude management endpoints by using the new property management.endpoints.web.exposure.include.            Old property      New property                  endpoints.health.path      management.endpoints.web.path-mapping      Other enhancementsConfiguration processorWhen you add the spring-boot-configuration-processor Maven module to your project, your IDE will be able to interpret your @ConfigurationProperties class and autocomplete the properties files.There were some other enhancements too.You can now use the Duration type for properties directly.private Duration delay = Duration.ofSeconds(3)The hello.delay autocompletion now also shows the unit behind the value, eg. hello.delay=3s.Spring Boot Dev Tools enhancementsDevTools is a feature in Spring Boot which adds nice development features.Spring created its own small LiveReload server, with a reload function.The communication protocol of LiveReload is open source.When you start the app, it also starts the LiveReload server.Something in your application watches the classpath for changes.If you change a template or a configuration property, Spring Boot will pick up the change, restart the Spring context and notify LiveReload.The restart only takes about 1 to 3 seconds, because the JVM is still hot.Spring Boot Dev Tools is not a new feature from Spring Boot 2, but the Spring developers have added some enhancements.One of those new features is that you get a delta of what changed and what triggered the LiveReload functionality.MicrometerIn Spring Boot 1.x, there was a metrics system with which you could register gauges etc.There’s an Actuator endpoint with which you could view those metrics.You could export those metrics to Prometheus or some other system.Micrometer is comparable to what SLF4J is for logging.You get an API for metrics that is independent of any vendor.You can record values and expose those values with a registry system to the outside world.&lt;dependency&gt;    &lt;groupId&gt;io.micrometer&lt;/groupId&gt;    &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;    &lt;version&gt;1.0.2&lt;/version&gt;&lt;/dependency&gt;For all projects that are supported by Micrometer, the metrics are exposed, eg. HikariCP.Spring Boot detects that you use HikariCP and automatically exposes those metrics to the different registries.To get more into detail we recommend you to check out this post, which covers Micrometer more in-depth.Q&amp;AHow should I convince my manager to upgrade?Because of some security issues in previous versions, Pivotal will stop providing support for those version.Another reason to upgrade is if you’re going to add new features to an application, eg. add metrics.When will Spring Cloud be ready to support Spring Boot 2?You can check http://start.spring.io/actuator/info in the spring-cloud section to see whether there are Spring Cloud versions which use Spring Boot 2.{  \"Angel.SR6\": \"Spring Boot &gt;=1.2.3.RELEASE and &lt;1.3.0.RELEASE\",  \"Brixton.SR7\": \"Spring Boot &gt;=1.3.0.RELEASE and &lt;1.4.0.RELEASE\",  \"Camden.SR7\": \"Spring Boot &gt;=1.4.0.RELEASE and &lt;=1.4.999.RELEASE\",  \"Edgware.SR3\": \"Spring Boot &gt;=1.5.0.RELEASE and &lt;=1.5.11.RELEASE\",  \"Edgware.BUILD-SNAPSHOT\": \"Spring Boot &gt;=1.5.12.BUILD-SNAPSHOT and &lt;2.0.0.M1\",  \"Finchley.M2\": \"Spring Boot &gt;=2.0.0.M3 and &lt;2.0.0.M5\",  \"Finchley.M3\": \"Spring Boot &gt;=2.0.0.M5 and &lt;=2.0.0.M5\",  \"Finchley.M4\": \"Spring Boot &gt;=2.0.0.M6 and &lt;=2.0.0.M6\",  \"Finchley.M5\": \"Spring Boot &gt;=2.0.0.M7 and &lt;=2.0.0.M7\",  \"Finchley.M6\": \"Spring Boot &gt;=2.0.0.RC1 and &lt;=2.0.0.RC1\",  \"Finchley.M7\": \"Spring Boot &gt;=2.0.0.RC2 and &lt;=2.0.0.RC2\",  \"Finchley.M9\": \"Spring Boot &gt;=2.0.0.RELEASE and &lt;=2.0.0.RELEASE\",  \"Finchley.BUILD-SNAPSHOT\": \"Spring Boot &gt;=2.0.0.BUILD-SNAPSHOT\"}Does Spring Boot 2 support Java 10?Yes.However, the Java 10 build step for Spring Boot 2.0.1 failed after Stéphane released it on the train to Mechelen.Eric De Witte posted a tweet about it during the meetup.I see @concourseci is being used pervasively at @pivotal I.e. @springboot tests against java 8, 9 and 10 pic.twitter.com/o2Z5tqX4ka&mdash; Eric De Witte (@vEDW) 5 april 2018Stéphane tweets the day after that the Java 10 build was fixed.Does @springboot 2 support Java 10?A picture is worth a thousand words pic.twitter.com/5jpG27gJnE&mdash; Stéphane Nicoll (@snicoll) 6 april 2018The meetupThe Spring Boot 2.0 Anniversary Meetup event started at 18h at the Ordina headquarters in Mechelen.Since you cannot learn on an empty stomach, we started off with Belgian French fries.Stéphane’s presentation started at 19h.He took us on a two hour ride through the Spring Boot landscape.After the presentation, a huge celebration cake was carried in.Our bakery had done its best to transform the Spring Boot logo into an immense nice looking cake.We think he did a good job!Check out Stéphane’s tweet.Hey @Lifeatordinabe, thank you so much for organizing this event and that @springboot 2 celebration cake was delicious! 🤗Looking forward to seeing the pictures pic.twitter.com/MvNhncLneI&mdash; Stéphane Nicoll (@snicoll) 5 april 2018A professional photographer took some atmospheric photos of the event.You can see all pictures on Elke.photos.Useful linksThe event was streamed live on YouTube.Subscribe to our channel for more Java- and JavaScript related videos.You can also consult the Spring Boot 2.0 Migration Guide for more information."
      },
    
      "nodejs-2018-04-10-complete-introduction-to-nodejs-html": {
        "title": "Complete Introduction to NodeJs",
        "url": "/nodejs/2018/04/10/complete-introduction-to-nodejs.html",
        "image": "/img/complete-introduction-node.png",
        "date": "10 Apr 2018",
        "category": "post, blog post, blog",
        "content": "What Is nodeNodeJs is a program that let’s you run JavaScript code on your machine without the need of a browser.Underneath the surface of node is the V8 JavaScript runtime which is the engine that allows your browser to run JavaScript code.On top of that, node adds some extra functionality to create server side applications(for example fs to interact with the file system, http or https to send and receive http calls, net for tcp streams, and many more).Use cases:Real time applications (chat, stocks, IoT)The event based nature of NodeJs and ‘keep-alive’ connections makes it ideal for real time applications, whenever an event occurs,for example a chat message being received or a stock price being updated, it can emit an event on its connected sockets to update the client’s chat screen or stocks chart.REST APIsThis will be a topic on its own, but with frameworks built on top of NodeJs like Express or Nest it is really easy to get a REST API up and running in no time at all.Serverless:NodeJs is supported with almost any serverless provider (Amazon Lambda, Azure functions, Google Cloud functions, …).So developers can focus on their code and business logic instead of maintaining and setting up complicated server architectures.File uploading: When writing applications that depend a lot on network access and accessing files on the disk we have to keep an eye on how the data is being transferred back and forward.For ultimate efficiency, especially when dealing with large sets of data, we need to be able to access that data piece by piece.When that happens, we can start manipulating that data as soon as it arrives at the server.Instead of holding it in memory until all chunks have arrived and writing it to disk, node can for example create a writable stream on the disk and write the chunks directly to the files without keeping them in memory and without blocking the entire application.This way it can also receive multiple files at the same time.Benefits of Javascript across the stackNot only does it make development quite a bit faster and easier by having a large community with lots of reusable code for your application (npm).It also lowers the barriers between frontend and backend developers by using the same programming language over the entire stack.So more efficiency and faster, leaner development which in turn means lower development costs.Also worth noting is that JavaScript is currently THE most popular programming language According to StackOverflow,so more developers will be able to easily understand and contribute to the application if needed.Another important criteria: when it comes to cloud hosting,RAM is probably the main influencing factor when it comes to pricing and since node is designed and encourages developers to write programs to use as less memory as possible it is often a cheaper alternative.MultithreadingThis is usually a big issue/talking point when it comes to node.In short: each NodeJs process is single threaded. If you want multiple threads, you have to have multiple processes as well.You could say that because of that, NodeJs encourages you to implement microservices when dealing with these larger and complicated applications.Which is a good thing since it makes not only your entire application but also each process individually very scalable.The downside is that this might introduce some added complexity to your application.But with Node’s lively modular ecosystem (npm) you can imagine there are already solutions to make setting this up a lot easier, (i.e. Moleculer, Seneca, …).An important characteristic of microservices is “shared nothing”.Node has a shared-nothing model:  A shared-nothing architecture (SN) is a distributed-computing architecture in which each node is independent and self-sufficient,and there is no single point of contention across the system. More specifically, none of the nodes share memory or disk storage.The advantages of SN architecture versus a central entity that controls the network (a controller-based architecture) include eliminating any single point of failure,allowing self-healing capabilities and providing an advantage with offering non-disruptive upgrade.A shared-nothing architecture (SN) is a distributed-computing architecture in which each node is independent and self-sufficient,and there is no single point of contention across the system. More specifically, none of the nodes share memory or disk storage.The advantages of SN architecture versus a central entity that controls the network (a controller-based architecture) include eliminating any single point of failure, allowing self-healing capabilities and providing an advantage with offering non-disruptive upgrade.(src: Wikipedia)Additionally, node has some other features to make use of multiple cores like for example the cluster:a single instance of NodeJs runs in a single thread. To take advantage of multi-core systems, the user will sometimes want to launch a cluster of NodeJs processes to handle the load.The cluster module allows easy creation of child processes that share server ports and automatically load balances across these processes.Blocking vs. Non-BlockingAs we’ve said before, NodeJs encourages you to take advantage of non-blocking code (like JavaScript promises).To demonstrate how this works, I’ll give you an example in pseudo code for reading a file from the filesystem.Blocking:    read file from filesystem,    print content    do something else`Non-Blocking:    read file from filesystem        Whenever we're complete, print contents (callback)    do something elseDifference:When reading two files, the blocking code starts reading the file.In case of a large file, let’s say this takes 5 seconds.After the file has been read, it logs its content. Then it starts reading the second file which again takes around 5 seconds and the content gets logged.In the non-blocking code, we tell the processor to start reading the file, and when it’s done, to “let us know” (resolve promise) so that we can do more stuff with it.At the same time since there is another file to be read, we start reading the second file and again tell the processor to notify us when it is ready so that we can do stuff with it.Whenever a ‘Promise’ of reading a file resolves, its callback (in our case, the pseudo code: print contents) gets executed(this also means that, when file #2 takes less time to be read, it will be resolved and printed first which is something you might want to keep in mind).V8 Runtime EngineNode uses Google Chrome’s V8 runtime engine to run JavaScript code, we’ve shown this video in one of our previous blog posts before,but since it might be useful to know how it works under the hood I’ve included the video once more.When it comes to node development there are some differences, since we don’t get events from the DOM.In node we can get them from the NodeJs event emitter but the way it works stays the same.It has some useful tips like avoiding to block the call stack.Philip Roberts: Help, I'm stuck in an event-loop.Installing NodeJsDownload the installer for your OS at https://nodejs.org/en/download/.Or, if you are a Mac user and have brew installed you can install it with brew.Open a terminal and run the following commands:Install nodebrew install nodeVerify if node was successfully installed (should output your node’s version number)node -v Node ModulesPreviously I talked about one of the benefits of node being its vast ecosystem of open source code that you can exploit.To avoid having to write the same common logic over and over again, node’s greatest feature is probably its modularity.You can put common logic in a node module that you can reuse over and over again in different components of your projects or even reuse them in other projects.NPMNPM or ‘Node Package Manager’ is an online registry for node modules. When you’ve written a useful module, why not share it with the world.Whenever you’ve implemented some common logic that can be reused across projects, it is a common practice in the world of JavaScript development to make it Open Source and share it with other developers who might want to implement the same logic.This way they don’t have to write it themselves which saves times and headaches.Just like using other people’s modules might do for you.Here are some useful npm commands to get you started:npm init  Start a new project. This creates a package.json file that keeps track of the installed modules (if you save them).npm install &lt;module_name&gt; Downloads a module that is registered (by name) on the npm registry.To see which modules modules are available, simply visit npmjs.com and search for whatever you need.The downloaded code will be saved in the node_modules directory. (Unless global install)npm install &lt;module_name&gt; —save  installs and also updates your project’s dependencies in package.jsonnpm install &lt;module_name&gt; —save-dev  installs and updates your project’s development dependencies in package.json (dependencies that you don’t need at runtime, i.e. testing frameworks like Jasmine or Karma, build frameworks like gulp or webpack, …)npm install &lt;module_name&gt; --global  installs the package globally, packages with command line interfaces like gulp-cli, angular-cli are installed globally.npm uninstall  uninstalls packages from your projectnpm update  updates your packagesFor a full overview of npm commands and further documentation of npm, check out this page: https://docs.npmjs.com/Node REPLOnce you have installed node you can open a terminal window and run node.This will return a node REPL where you can run JavaScript code. For example:function add(a, b){ return a + b } &lt;enter&gt;add(4, 7) &lt;enter&gt;// Returns 11To terminate the REPL hit CTRL + C.Hello WorldUsing the REPL can be useful sometimes, but when we want to make some persistent programs,we might want to write our code in a file and run the content of that file.In this example we’ll create a helloworld.js file.Create it in your favourite IDE or run touch helloworld.js and open it in your IDE or Vim/nano/…To keep it simple for this first project, we’ll simply make a program that logs ‘hello’ and ‘world’ in your terminal,we’ll log them separately just to show you that there’s different ways to log data with node.helloworld.js// process is a global variable that refers to the current node process you are running,// it has a stdout property that has a write method which we can call to output data.process.stdout.write('hello\\n'); // the \\n creates a new line in the terminal// or a bit simpler, the one we are used to from the browser, console.logconsole.log('world');Hello World AsyncAn example of non-blocking code// after 2 secs, print worldsetTimeout( ()=&gt; { console.log('world'); }, 2000);console.log('hello\\n');// prints  'hello' first, then 'world'When you’ve watched the video about how the V8 Engine works, you’ll know why ‘hello’ gets logged first and ‘world’ second:What happens is:  setTimeout is added to the call stack  setTimeout has a timer and a callback, this fires up  V8’s timer Web API  now that the Web API is taking care of the setTimeout, it gets removed from the call stack  console.log is added to the call stack, it logs ‘hello’, and removes console.log from the call stack  once the timer has completed, it pushes the callback to the task queue  since there are no more function calls on the call stack, the event loop adds the callback to the call stack  finally, ‘world’ is printed.There’s no need for an additional thread to pause the program for 2 seconds and after that log ‘world’.The V8 Engine handles this for us just like it does with any other async functionality in the browser.So this is a very simple example of how non-blocking code works in NodeJs, the timeout did not block our code, ‘hello’ got logged right away.Hello ModuleNow lets give you an example how to create a node module (a very simple and not a very useful one) but just to give you an idea of how you can export your code and use it in other files.We’re going to create a module that has a log function which takes a parameter (name) and logs ‘hello ' to the console.Then we'll import it in another file and call the function from there.log.js// create our custom hello functionconst log = (name) =&gt; { console.log(`Hello ${name}`); }// export this functionalitymodule.exports = log;hello.js// the way we import another module into our file is by using require(), require is a global module for node// when requiring local modules (not the ones we install with npm), we give it the path relative to the current file,// no need for extensions, since node looks for a .js filevar log = require('./log');// now that we have our functionality available in the log variable, we can use itlog('Mike');// logs 'hello Mike' to the consoleTo import modules that you’ve installed with npm, don’t specify a path, but give it the package’s name.For example:// this imports the express module from the express package if we installed it`require('express');To learn more about how to use require go check out this useful url: Requiring Modules.Hello ServerNow that we know how to require other modules, let’s create a basic server application.We’re not going to install any server frameworks (like Express) yet, instead we’ll require a module that comes with node.Node has some built-in modules that you can use like http (HTTP server), https, fs (file system), net (TCP sockets), … (a list can be found here: https://www.w3schools.com/nodejs/ref_modules.asp)For this program, we’ll use node’s http module.hello-server.js// import the http module (docs: https://nodejs.org/api/http.html)const http = require('http');// we create a server that will send plaintext 'Hello World' back to the client and put it in a variable serverconst server = http.createServer((req, res)=&gt; {  res.writeHead(200, {'Content-Type': 'text/plain'});  res.end('Hello world\\n');});// we tell the server to start listening on port 3000server.listen(3000, ()=&gt;{    // once the server has successfully started listening on port 3000    // (if the port isn't already in use)    // we'll log this to the console    console.log('Server running at localhost:3000/');});You can now open our browser, visit localhost:3000 and should see our ‘Hello world’ response from the server.Event EmitterAnother great feature that comes with NodeJs is the event emitter.The event emitter allows us to emit and listen for named events,whenever the EventEmitter emits an event all the functions attached to the named event are called synchronously.It’s real simple, let us show you with an example:// first we require the 'events' module that comes with nodeconst events = require('events');// next we'll create a new instance of the events module's event emitterconst eventEmitter = new events.EventEmitter();// we'll tell the event emitter that we are going to listen for the 'hello' event// and give it a callback function that gets called when the event is triggeredeventEmitter.on('hello', (data) =&gt; {  // as you can see, our callback function accepts a data parameter,  // we'll check if the event was emitted with data and has a 'name' property, if so we log 'Hello name'  if (data != null &amp;&amp; data.name) {    console.log(`Hello ${data.name}`);  } else {    // if no data was passed to the callback, we'll simply log 'Hello world'    console.log('Hello world');  }});// now that we are listening for the 'hello' event, we'll emit the event, once with data, and once without dataeventEmitter.emit('hello', {name: 'Mike'}); // logs 'Hello Mike'eventEmitter.emit('hello'); // logs 'Hello world'StreamsThere are many ways that you can utilise readable/writable streams with NodeJs, for example the file system to read/write to files.But to give you a simple example, let’s reuse our code from our hello server.Since the request object is a readable stream and the response object is a writable stream,we can create an application that pipes the data from the request, back to the response.Let’s see it in action with an example:streams.jsconst http = require('http');const server = http.createServer((request, response)=&gt; {  res.writeHead(200, {'Content-Type': 'text/plain'});  // The request is a readable stream, this means that the connection isn't immediately closed,  // the connection stays open for as long as the client keeps sending data.  // Streams inherit from the event emitter, so we can listen for the request stream's 'readable' and 'end' events  // the readable event is triggered whenever the request has sent a chunk of data that can be read.  req.on('readable', ()=&gt;{      let chunk = null;      // as long as we can read chunks from the request, we write those chunks to the response      while(null !== (chunk = request.read())){         // we can keep writing to the writable response stream as long as the connection is open,         // so we keep piping the readable data to the response          response.write(chunk);      }      // you can test it with curl from the terminal by sending a 'hello' string as data,      // simply run: curl -d 'hello' http://localhost:3000      // the 'hello' string is being sent back to the client  });  req.on('end', ()=&gt;{      // once the request stream is closed, we also close the response stream      response.end();  });  /*\tI've written it out completely to show you what is does,\tbut in fact we can use the pipe method to refactor the request's 'on readable' in some much simpler code\treq.on('readable', ()=&gt;{             request.pipe(response)        })   */});server.listen(3000, ()=&gt;{    console.log('Server running at localhost:3000/');});ClusterA single instance of NodeJs runs in a single thread.To take advantage of multi-core systems, the user will sometimes want to launch a cluster of NodeJs processes to handle the load.// require the cluster moduleconst cluster = require('cluster');// we'll set up a http server on all cpus and load balance between themconst http = require('http');// we need to know the amount of cpus our machine has available,// so we do this with the 'os' module's cpus method which returns an array of cpus, to get the amount we get the array's lengthconst numCPUs = require('os').cpus().length;if (cluster.isMaster) {  // the cluster will first start up a master process that forks itself onto the other cpus and handles the load balancing between these workers  console.log(`Master ${process.pid} is running`);  // fork this process to a worker for every cpu that is left (note the &lt; and not &lt;=)  for (let i = 0; i &lt; numCPUs; i++) {    cluster.fork();  }  // when a worker dies, it emits an exit event  cluster.on('exit', (worker, code, signal) =&gt; {    console.log(`worker ${worker.process.pid} died`);  });} else {  // if the cluster is not master, it (in this case) sets up our http server  // workers can share any TCP connection  http.createServer((req, res) =&gt; {    res.writeHead(200);    res.end('hello world\\n');  }).listen(8000);  console.log(`Worker ${process.pid} started`);}More information on clusters on https://nodejs.org/api/cluster.htmlFinallySo that’s it for this blog post. I hope it was useful to you.If you have any suggestions or feel like I’ve forgotten to mention some important stuff, feel free to comment below.I’m currently working on some follow-up tutorials:  Building REST APIs with NestJs (TypeScript),  Microservices with NodeJs (Moleculer),  Serverless with NodeJsOnce these are finished I’ll add the links below, so stay tuned!"
      },
    
      "angular-2018-03-30-angular-security-best-practices-html": {
        "title": "Angular Security Best Practices",
        "url": "/angular/2018/03/30/angular-security-best-practices.html",
        "image": "/img/angular-security-best-practices.png",
        "date": "30 Mar 2018",
        "category": "post, blog post, blog",
        "content": "Angular Security Best Practices  Software security is a hot topic nowadays.We, web developers, need to be up-to-date with all latest security issues that we could encounter when developing a web application.In this blog we’ll check what kind of best practices we should have in mind when building an Angular app so we limit the amount of security issues we could have.Up-to-date Angular librariesThe angular team is doing releases at regular intervals for feature enhancements, bug fixes and security patches as appropriate.So, it is recommended to update the Angular libraries at regular intervals.Not doing so may allow attackers to attack the app using known security vulnerabilities present within older releases.1. Preventing cross-site scripting (XSS)XSS enables attackers to inject client-side scripts into web pages viewed by other users.Such code can then, for example, steal user data or perform actions to impersonate the user.This is one of the most common attacks on the web.1.1. Sanitization and security contextsTo systematically block XSS bugs, Angular treats all values as untrusted by default.When a value is inserted into the DOM from a template, via property, attribute, style, class binding, or interpolation, Angular sanitizes and escapes untrusted values.This is the declaration of the sanitization providers in the BrowserModule:export const BROWSER_SANITIZATION_PROVIDERS: Array&lt;any&gt; = [  {provide: Sanitizer, useExisting: DomSanitizer},  {provide: DomSanitizer, useClass: DomSanitizerImpl},];@NgModule({  providers: [    BROWSER_SANITIZATION_PROVIDERS    ...  ],  exports: [CommonModule, ApplicationModule]})export class BrowserModule {}The DOM sanitization serviceThe goal of the DomSanitizer is to clean untrusted parts of values.The skeleton of the class looks like this:export enum SecurityContext { NONE, HTML, STYLE, SCRIPT, URL, RESOURCE_URL }export abstract class DomSanitizer implements Sanitizer {  abstract sanitize(context: SecurityContext, value: SafeValue|string|null): string|null;  abstract bypassSecurityTrustHtml(value: string): SafeHtml;  abstract bypassSecurityTrustStyle(value: string): SafeStyle;  abstract bypassSecurityTrustScript(value: string): SafeScript;  abstract bypassSecurityTrustUrl(value: string): SafeUrl;  abstract bypassSecurityTrustResourceUrl(value: string): SafeResourceUrl;}As you can see, there are two kinds of method patterns.The first one is the sanitize method, which gets the context and an untrusted value and returns a trusted value.The other ones are the bypassSecurityTrustX methods which are getting the untrusted value according to the value usage and are returning a trusted object.The sanitize methodIf a value is trusted for the context, this sanitize method will (in case of a SafeValue) unwrap the contained safe value and use it directly.Otherwise, the value will be sanitized to be safe according to the security context.There are three main helper functions for sanitizing the values.The sanitizeHtml function sanitizes the untrusted HTML value by parsing the value and checks its tokens.The sanitizeStyle and sanitizeUrl functions sanitize the untrusted style or URL value by regular expressions.How can we disable the sanitization logic?In specific situations, it might be necessary to disable sanitization.Users can bypass security by constructing a value with one of the bypassSecurityTrustX methods, and then binding to that value from the template.An example:import {BrowserModule, DomSanitizer} from '@angular/platform-browser'@Component({  selector: 'my-app',  template: `    &lt;div [innerHtml]=\"html\"&gt;&lt;/div&gt;  `,})export class App {  constructor(private sanitizer: DomSanitizer) {    this.html = sanitizer.bypassSecurityTrustHtml('&lt;h1&gt;DomSanitizer&lt;/h1&gt;&lt;script&gt;ourSuperSafeCode()&lt;/script&gt;') ;  }}  Be careful: If you trust a value that might be malicious, you are introducing a security vulnerability into your application!1.2. Content security policy (CSP)Content Security Policy (CSP) is an added layer of security that helps to detect and mitigate certain types of attacks, including Cross Site Scripting (XSS) and data injection attacks.These attacks are used for everything from data theft to site defacement or distribution of malware.To enable CSP, configure your web server to return an appropriate Content-Security-Policy HTTP header.You can find a very detailed manual how to enable CSP on the MDN website.To check if your CSP is valid you can use the CSP evaluator from google.1.3. Use the offline template compiler (aka AOT-compiler)Angular templates are the same as executable code: HTML, attributes, and binding expressions (but not the values bound) in templates are trusted to be safe.This means that if an attacker can control a value that is being parsed by the template we have a security leak.Never generate template source code by concatenating user input and templates.To prevent these vulnerabilities, use the offline template compiler, also known as template injection.If you use the Angular CLI, it’s easy to enable AOT:ng build --aotng serve --aotMore info can be found on the Angular Guide website.1.4. Avoid direct use of the DOM APIsThe built-in browser DOM APIs don’t automatically protect you from security vulnerabilities.For example, document, the node available through ElementRef, and many third-party APIs contain unsafe methods.Avoid interacting with the DOM directly and instead use Angular templates where possible.1.5. Server-side XSS protectionInjecting template code into an Angular application is the same as injecting executable code into the application.So, validate all data on server-side code and escape appropriately to prevent XSS vulnerabilities on the server.Also, Angular recommends not to generate Angular templates on the server side using a templating language.2. HTTP-level vulnerabilitiesAngular has built-in support to help prevent two common HTTP vulnerabilities, cross-site request forgery (CSRF or XSRF) and cross-site script inclusion (XSSI).Both of these must be mitigated primarily on the server side, but Angular provides helpers to make integration on the client side easier.2.1. Cross-site request forgery (XSRF)Cross-site request forgery (also known as one-click attack or session riding) is abbreviated as CSRF or XSRF.It is a type of malicious exploit of a website where unauthorized commands are transmitted from a user that the web application trusts.In a common anti-XSRF technique, the application server sends a randomly generated authentication token in a cookie.The client code reads the cookie and adds a custom request header with the token in all subsequent requests.The server compares the received cookie value to the request header value and rejects the request if the values are missing or don’t match.This technique is effective because all browsers implement the same origin policy.Only code from the website on which cookies are set can read the cookies from that site and set custom headers on requests to that site.That means only your application can read this cookie token and set the custom header.Angular HttpClient provides built-in support for doing checks on the client side. Read further details on Angular XSRF Support.2.2. Cross-site script inclusion (XSSI)Cross-site script inclusion (also known as JSON vulnerability) can allow an attacker’s website to read data from a JSON API.The attack works on older browsers by overriding native JavaScript object constructors, and then including an API URL using a &lt;script&gt; tag.This attack is only successful if the returned JSON is executable as JavaScript.Servers can prevent an attack by prefixing all JSON responses to make them non-executable, by convention, using the well-known string \")]}',\\n\".Angular’s HttpClient library recognizes this convention and automatically strips the string \")]}',\\n\" from all responses before further parsing."
      },
    
      "kickstarters-2018-03-29-kickstarter-trajectory-2018-light-html": {
        "title": "Kickstarter Trajectory 2018 Light Edition",
        "url": "/kickstarters/2018/03/29/Kickstarter-Trajectory-2018-light.html",
        "image": "/img/kicks.png",
        "date": "29 Mar 2018",
        "category": "post, blog post, blog",
        "content": "The Ordina Kickstarter trajectory is a collection of courses tailored and designed by the senior consultants of Ordina.These courses are created to give the beginning software developer a broad knowledge base while also providing an in-depth view on several technologies and best practices.This year the Kickstarter trajectory spanned 15 days, with topics ranging far and wide: backend to frontend, Spring Data JPA to TypeScript and everything in between.All of these courses will make sure the candidates will be able to hit the ground running on their first project as Ordina consultants.This post will summarize the training and experiences we’ve had while following the Kickstarter trajectory of JWorks, Ordina’s Java/JavaScript unit.BacklogWhat we have doneIntro Kickstarter Trajectory &amp; Dev EnvironmentOn our first day we got a brief introduction to Ordina JWorks by Yannick De Turck. We have learned that JWorks consists of 10 competence centers: Agile &amp; DevOps, API &amp; Microservices, Application Security, Cloud Native Platforms, Continuous Integration &amp; Delivery, Big Fast Data, Internet of Things &amp; Machine Learning, Javascript &amp; Hybrid Mobile, JVM Languages and Software Architecture. Every competence center is responsible for organizing workshops and presentations. As an Ordina consultant it’s possible to contribute to one of these competence centers. For example by writing a blog post on the JWorks Tech Blog, writing an article on the JWorks Docs, or assisting with a workshop.We have received two books to read: Oracle Certified Associate Java SE8 (to obtain the Java 8 certificate) and Clean Code. These books are an absolute must for every junior developer!Yannick showed us the preferred stack of JWorks for the backend and frontend.Later we have seen that Ordina operates according to the secure-by-design principle which means that every consultant will be trained to achieve certifications in security. Security should be inside every consultant’s DNA.In the afternoon we have learned about the following topics:  Integrated Development Environments (IDEs) and how important it is to choose the right one  Build tools: Maven, Gradle and npm  Version Control System: Git  Continuous integration  Scrum and Agile  Improving your productivity as a developerJavaOn Tuesday we got our first learning session from Yannick De Turck.In the morning we talked about all the new features since Java 7.Especially streams in Java 8 and the var keyword in Java 10 caught my attention.Streams and lambdas were the biggest change in Java 8 and severely altered the way we write code, I believe var will do the same for Java 10. But it will take some time before we can use it in production environments of course,since Oracle doesn’t provide long-term support for versions 9 and 10.In the afternoon we did some fun exercises where we had to implement methods to make pre-made tests pass.Of course most of these were about Java 8 features because…GitDuring the second day of our kickstart trajectory we’ve got a brief explanation by Yannick again.He explained us that Git is an open-source, distributed version control system that keeps track of your files and history.Basically this means Git offers us tooling to collaborate on code bases without overwriting each others changes so easily.We saw which workflow JWorks uses in Git and which commands we can use to do so.This way we learned how to create Git repos and create separate branches for features or different releases.And we even saw the different ways to merge these branches.One thing we’ll definitely won’t forget that fast is to rebase when pulling to keep a clean non-spaghetti like history,something that is preferred by many co-workers at JWorks! At least that is what Yannick has told us. ;)Spring and Spring bootThe lectures on Spring and Spring Boot were given by Ken Coenen and were spread over two days.During the first lecture, we got a recap of the concepts of JPA, beans, application contexts and other things that Spring uses in its core fundamentals. Next, we dug deeper into the framework and introduced ourselves with Spring Web Services and Spring Security and created a small backend application during the second lecture.Both lectures were rather theoretical, but very informative and elaborate with a lot of examples. So you have got everything you need to get familiar with Spring. You can find the course material on Ken’s Github page.MicroservicesThe workshop on microservices was lectured by Kevin Van Houtte and powered by Spring Cloud and Netflix OSS. It went pretty fast and at the end of the workshop, we had acquired a great overview of all the important aspects regarding the microservices architecture! The most important thing to know is that each microservice takes care of only one specific business function.Currently, monolith architectures are still being used a lot within companies, but as an up-to-date IT consultant it’s essential to know about microservices and where to use it.We learned about the 12-factor app methodology, which defines all the important aspects of microservices architecture: codebase, dependencies, configs, backing services, etc.In a hands-on approach we learned how to create a microservice, how to register it in a service registry (using Eureka), how to externalize our configuration (using Spring Cloud), how to create routing logic (using Zuul) and finally how to test the implementation using the Feign HTTP client.Unit testing and mockingOn Thursday we got a course about testing from Maarten Casteels, who works as a consultant for Ordina at Belfius.  The first part of the day was a very passionate and interactive theory session about the following subjects:  Goals of testing  What to test  Fixtures  Mocks  AssertionsAfter the lunch break we did some exercises together that showed us how to mock out dependencies and which pitfalls we should pay attention to.This gave us a better understanding of the theory we saw that morning.All in all it was a great course explaining the big picture of testing but also showing us the ropes in day-to-day working with tests and code.The open atmosphere enabled us to ask a lot of questions which Maarten always answered thoroughly.Frontend EssentialsAt the end of our first week we went over some of the frontend essentialsbefore diving deeper into the frontend frameworks and build tools the next week.This workshop was given by Yannick Vergeylen.Our colleagues from the VisionWorks department accompanied us since they use the topics covered in this workshop as well.After a theoretical recap about HTML, CSS and JavaScript we learned how to use HTML to create web pages and how CSS is used to style these pages and its components. We also used some JavaScript and learned how it is used to modify some of the HTML-components.During the workshop we were given an exercise in which we had to recreate a given example page with the above technologies.This way we had some hands-on experience straight away!Build ToolsWe started the second week with a solid introduction of frontend build tools. The topics of this workshop (given by Michael Vervloet) were Node.js, package managers and build systems &amp; generators (gulp, webpack and Angular CLI). After every topic we got the chance to put this newly acquired knowledge into practice. This started from scratch by installing Node.js and at the end we created an Angular project.AngularOne of the must see frontend frameworks is Angular of course.We’ve been introduced to it by Ryan De Gruyter.Ryan did a very good job and gave us a good base to get started with Angular.He taught us what Angular components are and how we can display data inside these components with the different types of data-binding.We also saw how we can let these components communicate with each otherand pass data from child components to its parent component and vice versa.On top of that, we saw how Angular directives are used to loop over objects to show multiple elements. And how we can use the *ngIf directive to hide/show elements and many more of these directives.But that’s not all, he also taught us about modules, services, dependency injection and much more.It was a very educational session for sure.Ryan did a good job on giving us some theoretical information about the different parts of Angular.After each theoretical part we made some exercises.And the cool thing about it?All these parts combined we made ourselves a small crypto currency listing application with real data!    DevOps &amp; CI/CDWe learned that developers should share the responsibility of looking after the system they build. And not just hand the release over to operations. The development team can also simplify the deployment and maintenance for the operation team. This could be done by introducing a DevOps culture. Yes, it’s not a role. It’s a culture. We have learned that DevOps aims to remove the isolation between operations and software developers by encouraging collaboration. It should also be easier to change your code and push it to production by using Continuous Delivery (CD).Continuous Integration (CI) is the process of automating the build and testing of code every time a team member commits changes to version control. We have learned how we can configure a CI tool. We had the chance to have some hands-on experience with GoCD.This workshop was given by Tim Vierbergen.SecurityNowadays, security is a hot topic and it’s important to handle sensitive (personal) information in a secure manner.Because it’s not only a PR nightmare for your business, it’s also a financial disaster because of GDPR that will take effect this May 2018. This fascinating lecture was presented by Tim De Grande on our last day of the Kickstarter trajectory.We discussed basic security fundamentals, common web vulnerabilities, and tips and tricks on how to secure your own applications.DockerWe started with a recap of the theory behind creating images and spinning up containers.Soon after, we were creating our own images and learned how to run our applications in a container. On the way we experienced the advantages of Docker and how it fits nicely in the processof CI/CD. Thanks Tom Verelst for guiding us into the Docker world!RecapNick: “Ordina has given me the chance to increase my knowledge by involving me in the Kickstarter program.         It was great to learn about the top-notch Ordina stack from our own experts.        This is exactly what I was looking for.         The Kickstarter program just ended and I’m eager to start using my knowledge in an enterprise environment again.        Ordina also provides plenty of other learning opportunities.         Since I arrived, I could join an interesting seminar or workshop every week.”Sam: “The opportunity you get at Ordina to learn from experienced developers is something you can’t miss.        The Ordina Kickstarter trajectory is the perfect example of how it should be.        You’re presented some of the newest technologies from really kind, helpful and experienced developers.        It’s the perfect program to get you started in your career as developer!”Dries: “As a backend developer with a few years experience I followed the program to see where I had gaps in my knowledge.         Thanks to the great atmosphere and experienced teachers, I was able to fill those gaps.         The sessions were very interactive which enabled me to ask any question, they were always met with well-founded answers.         The courses on JS and Angular also sparked my interest in frontend work, which will be very useful in my further career.”Maarten: “The courses were tons of fun and taught me a lot.          All of our teachers were competent individuals who made sure we learned as much as we could during the time we had.          I didn’t have any experience in frontend development, so the frontend courses were an eye-opening experience.          To anyone that’s having doubts about this Kickstarter trajectory: go for it!          I can definitely recommend it!”Johan:  “I’m really eager to learn more about new technologies and the Kickstarter course suited my needs. It challenged me in a good way on both a personal and technical level. Ordina really knows how to kick start newcomers into the astonishing world of technology.”Yunus: “Ordina gave me the opportunity to put my academic knowledge in practice and learn about the latest technologies. I’ve learned the best practices from our seniors during this intensive Kickstarter trajectory. Every graduate needs to have participated in a kickstart trajectory like Ordina’s, it’s the best way to start your career.”Yen: “The Kickstarter program at Ordina really got me fast on track with the latest technologies being used in the field. I’m fresh out of school where I had a focus on backend and here at Ordina this got greatly improved upon. It was also interesting to get in depth on frontend tech! You notice all the coaches are experienced programmers and it was a privilege to learn from them. And if you need any help after a workshop, they are always quick to help. To summarize, I really recommend this Kickstarter program to accelerate your career!”About Ordina’s kickstarter trajectoriesInterested in knowing more about Ordina’s kickstarter trajectories?More information is available on Ordina’s website."
      },
    
      "conference-2018-03-15-secappdev-2018-html": {
        "title": "SecAppDev 2018",
        "url": "/conference/2018/03/15/SecAppDev-2018.html",
        "image": "/img/secappdev-2018/secappdev_wide.png",
        "date": "15 Mar 2018",
        "category": "post, blog post, blog",
        "content": "  Last February I was able to attend the 2018 edition of SecAppDev.It’s a training/security conference in Leuven that lasts a week and which hosts top-notch speakers.It’s organised by Philippe De Ryck of imec-DistriNet, KU Leuven.SecAppDev for me was a week filled with learning, I’ll recap a few of the sessions I attended in this post.Security model of the web - Philippe De RyckThe most basic security control of the modern internet is the ‘Origin’.This was thought up over 20 years ago and, at the time, was adequate for its purpose.Nowadays however, origin is a poor security constraint: we load scripts from CDNs, we include frames from other providers, …Because of this, more security controls have been bolted on in the last years.In this talk, Philippe De Ryck explored some of these.We learned how to use X-Frame-Options and Content Security Policy (CSP) settings to limit who can include our pages in a frame.Next up he explained how to limit the power of other sites which you might need to frame in yours using the sandbox attribute, which was introduced in HTML5.Once you’ve limited what the frame can do, you can open up communications between your page and the frame through the Web Messaging API.Once you’re past frames, we come to scripts.Nowadays, we load scripts from all over the place, often knowing nothing more than a name.These scripts run within the context of your page and can do everything the current user can do.To make sure these scripts aren’t tampered with you’d ideally investigate them first and then use subresource integrity (SRI) to make sure they aren’t modified.Most CDNs nowadays offer this as a service: they provide you with the correct hashes for the scripts they host.That does mean you need to trust your CDN to host a non-malicious file at the time you include it.After a quick look at CSP, we came to the cookies.As we all know, cookies are not the best solution: they’re sent over both HTTP and HTTPS and they can be read and modified by (malicious) scripts.This allows for some interesting attacks like session hijacking and session fixation.An attempt was made to fix this through Secure and HttpOnly flags.Recently a new spec tries to restrict cookie behavior based on prefixes: __Secure- and __Host-.Because browsers send your cookies on all requests to your domains, this opens you up to an attack called Cross Site Request Forgery (CSRF).We discussed a few methods that can be used to mitigate this risk: hidden form tokens, “transparent” tokens, checking the origin header and samesite cookies.The session ended with a look at JSON Web Tokens (JWT).Contrary to popular belief, these represent data, not a transport mechanism.It’s perfectly fine to store a JWT in a cookie, rendering the whole cookie vs. tokens debate a bit useless.Putting your token on an Authorization header, does protect against CSRF, but introduces some other complexities.OWASP’s top 10 proactive controls (Jim Manico)In this session we had a quick look at version 2 of the OWASP proactive controls.These are the things every developer should do in order to harden their code.The full list has 10 items, but because of some very interesting discussions, we only managed to cover the first 5.1. Verify for security early and oftenThis is not an easy thing in today’s DevOps world as code is deployed to production a lot more often.Etsy deploys more than 25 times per day, while Amazon manages a deploy every 11.6 seconds!Make sure that security testing is part of the build process, doing that ensures that your security testing actually happens.There are several tools available that can help you out here (e.g. OWASP ZAP or Nessus) and you can combine them for increased coverage.Make sure you don’t end up on the “hamster wheel of pain” where you focus on the specific bugs they reveal, rather than the class of bugs.2. Parameterize queriesWe should all know by now that queries should never be built using string concatenations.Use parameterized queries instead to prevent SQL injections.Use parameters for everything: not just the user-supplied input, but configurations and hard-coded values as well.This can give you a performance boost as well, since parameterized queries are compiled by the database only once and then reused.3. Encode data before use in a parserThe best known vulnerability here is Cross Site Scripting (XSS).Allowing someone to inject HTML tags in your HTML pages gives them nearly unlimited power over your application.Make sure to encode all user input before feeding it to a parser (a browser is basically a very powerful HTML parser) to prevent these kinds of issues.For Java applications, you can use the OWASP Java encoder project to handle your HTML encoding.They also have tools available for other languages (.NET, PHP, …).4. Validate all inputsAnd don’t just do it client-side.Client-side validations are easily bypassed, so you need to repeat them server-side as well.If your users need to be able to post HTML, you need to sanitize it.For that you can use the OWASP HTML Sanitizer.Keep in mind that even valid data can cause issues: ' OR '1'='1'; -- is a perfectly valid password, and the Irish people will be grateful that you allow the use of ' in name fields.What about files?You also need to do this if your users are allowed to upload files.Files create even more risks: you need to make sure that the uploaded files are safe.First validate the file name, file type and decompressed size (preferably before decompressing).Run it through a virus scanner on a separate machine to protect against exploits against your antivirus.For images, you need to enforce size limits and you’ll want to verify that you’re actually dealing with an image.The easiest way to do that is to rewrite the image (e.g. using ImageMagick).Once again, you want to do this on a separate machine to prevent malicious images to take over your application.5. Establish authentication and identity controls.  Don’t limit the password (within reason). Don’t enforce arbitrarily short passwords or limit the type of characters that can be used.You do want to limit the length, if only to prevent DOS attacks, but 100+ characters shouldn’t be an issue.  Check the chosen password against a list of the 100k most common chosen passwords  Use a strong, unique salt.Each credential should have its own salt, and don’t skimp on the length.64 or 32 characters (depending on the hashing algorithm) should be the norm.  Impose a difficult verification on both attacker and defender.Use a hashing algorithm that’s appropriate, such as PBKDF2, scrypt or bcrypt.Alternatively, you could use HMAC-SHA-256( [private key], [salt] + [password] ) to only make it hard on the attacker.However, this introduces a lot more complexity in your system.Other authentication best practices should also be applied, such as two factor authentication, a proper lockout policy, …A practical introduction to OpenID Connect &amp; OAuth 2.0 (Dominick Baier)Dominick Baier gave a very interesting talk on OpenID Connect and OAuth 2.0.An important distinction he made at the start is the difference between a user and a client.Users are people (carbon based life forms) while the word “client” refers to applications (or silicon based life forms).OAuth2.0 is a protocol meant for client authentication while OpenID is the successor to SAML (and as such meant to authenticate users).OAuth is not meant for user authentication, even though it’s commonly (ab)used for that through various incompatible, proprietary extensions.OpenID ConnectOpenID Connect piggy backs on OAuth2.0.It adds support for logging out and key rotation.More importantly, it’s an open standard and it publishes a list of certified implementations.Compliance with the spec is guaranteed through a set of tests.EndpointsAn OpenID Connect server (or token service) has to implement a set of endpoints:  A discovery endpoint to discover where the other endpoints are.  An authorize endpoint (for users)  A token endpoint (for machine to machine processes)Discovery endpointAn example of a discovery endpoint is at https://accounts.google.com/.well-known/openid-configuration.It returns an unsigned JSON document: for security OpenID Connects relies entirely on HTTPS.The issuer must be the URL where the document is located.Authorize endpointThis endpoint handles authentication for web applications and is found in the authorization_endpoint field of the discovery endpointThe client (in this case the browser) makes a request to the authorize endpoint and passes along a few required parameters:  The callback url: the token service will verify that this url is allowed and perform a callback to this url after the user is logged on.  A nonce (number used once) which will be echoed to the client so it can verify server responses.  And a scope which needs to include openid.The server will then authenticate the user and show a consent dialog.This dialog shows the logged in user, the application that requests access and the access that’s being requested.When the user allows this request, the token service sends  response to the client containing a JWT based identity token as well as a cookie.This means that the token service will remember the user for future logon requests to other applications.Identity token validationWhen you use an identity token to authenticate to an application, the application needs to validate this token.It does this by making sure that:  The issuer name matches the value of the iss claim  The aud must contain the client-id that was used to register the application.  The proper signing algorithm must be defined in alg.  The current time must be before exp  If the token is too old (as defined in iat or “issued at”), it can be rejected  nonce must match what client sent  And you need to validate the signature. For that you check the kid field in the header and use find that key in the document you find at the jwks_uri field from the discovery endpoint.Session managementSince the token service places a cookie in the user’s browser, this means that you have one logon session active.When you access another application that uses the same token service, it just needs to show you the consent dialog, without asking you to log in again.This is called “Single Sign On” (SSO).OpenID Connect also supports “Single Sign Out”.When you log out of the token service (by calling the /end_session endpoint), it will try to sign you out from all applications.It support three different ways of doing this:Javascript based notificationIn order to use this, your application should always contain a specific iframe.The source of this iframe is defined in the check_session_iframe field of the discovery config.This frame is loaded in the same origin as the token service and it will do a JS call to the parent page to log out.Front-channel notificationEven though the spec calls this a “best-effort approach”, it’s still the method that’s most common.It requires each client to implement a clean-up endpoint.When the user logs out, the token service will render an HTML page that containing an invisible iframe for each client.These iframes will call the clean-up endpoints.Normally, these iframes will contain the session id in the url to prevent “signout spam”.Otherwise it would be too easy for a malicious site to add an image to their pages signing you out of your sessions, causing a DOS.The reason this approach is “best-effort” is that the browser might not be able to call all endpoints before the user navigates away from the log out page.Back-channel notificationThis is the safest option, as it guarantees that the user will be signed off from all applications.Unfortunately it’s also the most complicated to implement.In this method, the token service will call a server-endpoint on all client applications.This means that the application server will need to implement the clean up endpoint.Besides that, you also need to be sure that a network connection is possible between the ID provider and all application servers."
      },
    
      "iot-2018-03-14-stairway-to-health-2-html": {
        "title": "Stairway to Health 2.0 (the Ordina version)",
        "url": "/iot/2018/03/14/Stairway-To-Health-2.html",
        "image": "/img/stairwaytohealth2/banner.jpg",
        "date": "14 Mar 2018",
        "category": "post, blog post, blog",
        "content": "Harder, Better, Faster, StrongerHere we are again, another blog post about Stairway to Health.Why? Well, we’ve created our own Ordina version of the Stairway to Health application.There are quite a few interesting bells and whistles, among others, here are a few of the new features:  New (and awesome) frontend design, with Ordina theming obviously  Upgraded from Angular 4 to Angular 5  Material Design  Nest.js in stead of Express.js (still Express underneath, but cleaner code!)  Backend e2e tests with Mockgoose  Deployed on OpenShift  New type of sensors  Cheers feature, users can motivate and support each otherStairway to Health @ OrdinaAs you might have read in our previous post about Stairway to Health, the purpose of the application is to improve worker health in a fun and engaging way. With the app we try to encourage employees to take the stairs instead of the elevator.We’ve put up some sensors that can detect how much the stairs are used on a per floor basis and how many people take the elevator.In the app they can see the results and thus they can do an extra effort if they are falling behind.New in the Ordina version is that employees can now also cheer and motivate each other since we’ve added a chat feature to the application.Internet of ThingsThe Stairway to Health project is a simple yet great example to show what the Internet of Things can do:  LoRa sensors detect door openings, these are installed on the doors of the staircases  These sensors communicate via the LoRa network to report their status  In our case, sensor data is sent to the Proximus MyThings platform which processes the data  The data gets sent to the Stairway to Health application  The Stairway to Health application interprets and visualises the dataIn summary: We install sensors on the doors (things) to measure usage and we analyse the data to persuade people to move more.The result is a good example of how IoT can influence our daily lives.For more on this topic, check the application’s About pageDive into the technical detailsThe reason of us writing this blog post is mainly because we want to explain some of the technical changes and improvements we’ve madesince we’ve updated (pretty much rewritten) the application. So let’s get started.The APIExpressJs to Nest.js: The main difference here is that we’ve rewritten the application to use the new framework inf favour of the old implementation with ExpressJs.Migrating from Express to Nest is not that difficult, since Nest is a wrapper on top of the Express framework.It provides you with some nice TypeScript decorators which makes your code a lot cleaner, more compact and easier to read.ExpressJs exampleexport class EntityApi extends CoreApi {    private entityController: EntityController = new EntityController();    constructor() {        super();    }    // the create function would that have to be executed by the main server while bootstrapping the application    public create(router: Router) {        router.get( '/auth/entities',                    this.authenticate,                    this.requireAdmin,                    (req: Request, res: Response, next: NextFunction) =&gt; {                        this.entityController.getEntityList(req, res, next);                    });    }}NestJs example// automatically registered to the server by nest// all /auth routes require user to be logged in (doesn't come standard with Nest)@Controller('/auth/entities')@UseGuards(RolesGuard)export class EntitiesController {    constructor(private readonly entitiesService: EntitiesService) {}    @Get('/')    @Roles('admin')    async findAll(): Promise&lt;IEntity[]&gt; {        return await this.entitiesService.findAll();    }}Websockets with NestJsWorking with sockets is also a lot easier and cleaner when using Nest.We can utilise the @WebSocketGateway to create a new route/gateway, @SubscribeMessage to listen for certain events and @OnGatewayConnection or @OnGatewayDisconnect to know when users connect or disconnect to the server.There wasn’t any straight forward solution for broadcasting to all clients. Once a user sends a message, we want to update the messages for everyone that has the client open. So we solved this by pushing all connected clients to an array and when we receive a ‘cheer-created’ event, we loop over the array of clients and emit an event to them one by one.import {\tWebSocketGateway, SubscribeMessage, OnGatewayConnection, OnGatewayDisconnect,\tWsResponse} from '@nestjs/websockets';@WebSocketGateway({namespace: 'events/cheers'})export class CheerEventsComponent implements OnGatewayConnection, OnGatewayDisconnect {\tpublic clients = [];\tconstructor() {\t}\thandleConnection(client: any) {\t\tthis.clients.push(client);\t}\thandleDisconnect(client) {\t\tfor (let i = 0; i &lt; this.clients.length; i++) {\t\t\tif (this.clients[i].id === client.id) {\t\t\t\tthis.clients.splice(i, 1);\t\t\t\tbreak;\t\t\t}\t\t}\t}\t@SubscribeMessage('cheer-created')\tonEvent(): WsResponse&lt;void&gt; {\t\tthis.broadcast('cheer');\t\treturn;\t}\tprivate broadcast(message: string) {\t\tfor (let c of this.clients) {\t\t\tc.emit(message);\t\t}\t}}Optimising chart data and countsOn Stairway to Health we used mongo aggregations to get our chart data from the database. Once we hit 1.5 million logs, these calls put a lot of stress on our servers and took a long time to load, so in stead we now keep track of daily, weekly, monthly, yearly and total logs in their own collection.Whenever we receive a log from the MyThings stream we update all these collections. For example the daily logs collection contains documents that look like this:{\"date\": {    \"$date\": \"2017-12-20T21:49:15.532Z\"},\"friendlyName1\": \"C\",\"friendlyName2\": \"1\",\"hour\": 22,\"identifier\": \"20-12-2017\",\"counts\": 55}So when we want the hourly data from a certain day, we query the collection for the date we want and and simply return an array with all the different hours, if an hour doesn’t exist, we assume it didn’t send any logs/counts.When we receive a log, we check if there is an entry that has “date” and “hour” equal to the log’s date. If so, we update, otherwise we create a new entry (upsert).We still store the log in a “logs” collection, so that if ever our daily, weekly, … collections get corrupted, we can run a script that populates these collections with the correct data.async create(log: ILog, stream?: boolean): Promise&lt;ILog&gt; {    try {        // We insert the log into our logs collection        let item = await this.logModel.create(log);        // the identifiers so we can easily query for them        let dailyIdentifier = `${item.day}-${item.month}-${item.year}`;        let weeklyIdentifier = `${item.week}-${item.year}`;        let monthlyIdentifier = `${item.month}-${item.year}`;        let yearlyIdentifier = `${item.year}`;        // sensors send all their containers to us, we only need to update the collections        // if they are 'counters' and they have a numeric value        if (item.container === 'counter' &amp;&amp; item.numericValue) {            // update all collections            // by putting them in a variable, they all get executed without having to wait for each one to complete,            // and we have no 'callback hell', below te do a Promise.all so that we know when they are all done.            let dailyCountPromise = this.dailyCountsModel.update({                identifier: dailyIdentifier,                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                hour: item.hour            }, {                // increment, not overwrite the counts                $inc: {counts: item.numericValue}            }, {                // upsert makes sure that if the entry we try to update doesn't exist, we create one                upsert: true            });            let weeklyCountPromise = this.weeklyCountsModel.update({                identifier: weeklyIdentifier,                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                day: item.day            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            let totalCountPromise = this.totalCountsModel.update({                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            let yearlyCountPromise = this.yearlyCountsModel.update({                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                month: item.month,                identifier: yearlyIdentifier            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            let monthlyCountPromise = this.monthlyCountsModel.update({                friendlyName1: item.friendlyName1,                friendlyName2: item.friendlyName2,                week: item.week,                identifier: monthlyIdentifier            }, {                $inc: {counts: item.numericValue}            }, {upsert: true});            // once all collections are updated, we emit a 'stream-received' event,            // which will reload the charts on the client application            Promise.all([ dailyCountPromise,                          weeklyCountPromise,                          totalCountPromise,                          yearlyCountPromise,                          monthlyCountPromise]).then(() =&gt; {                              if (stream) {                                  socket.emit('stream-received');                              }                          }, (err) =&gt; {                            console.log(err);                          });        }        return item;    } catch (error) {        throw new HttpException(error.message, HttpStatus.BAD_REQUEST);    }}The Visible partsThe main changes we’ve made on the frontend are:  Changing the colours, we created a dark theme with Ordina branding  Used material design for a smoother user experience  Replaced Highcharts library with @swimlane/ngx-charts  Migrated to Angular 5                                                                                                            Since users should now be able to register to the application to cheer for and motivate each other we added these new screens and functionality.                                                                        Deploy on OpenShiftSince we’ve separated our frontend and backend code we used 2 separate Git repositories. The nice thing about deploying to OpenShift is that we can add a webhook to GitHub so that every time we merge a pull request from our develop branch to ourmaster branch to our Git remote, it builds and deploys the new code immediately.    The new sensors: Proximus MySenseFor the previous version of Stairway to Health we used Magnetic door sensors,these use a magnet mounted on the door frame and the sensor mounted on the door itself, when the door is closed the magnetmakes contact with the sensor and the sensor detects the door is closed. This means you need to mount at two places,and it needs to be carefully placed to align. This makes it not an ideal solution.A solution for this is the MySense sensor. This is a LoRa sensor programmable with JavaScript.The MySense is a small LoRa device containing multiple sensors.It contains a temperature sensor, a button, …But the most important sensor for our case is the accelerometer.Using the accelerometer we can detect when the door is moving. After detecting a motion we will blackout the sensorfor 30 seconds to allow the door to be closed again and not count multiple motions.To save battery we do not send on every motion,but count the amount of motions for 15 minutes and then send the counter,also when the counter is 0 we will not send to save battery.ConclusionWe made some major improvements when it comes to performance, maintainability and functionality.By deploying our application to OpenShift, we also improved our workflow and made it a lot easier to deploy our changes.By using the MySense as our sensor we only have to mount one piece per door. An extra advantage is that this sensor is a lot cheaper.Interesting Links  Stairway to Health 2.0  Blogpost Stairway to Health 1  Nest.js  OpenShift"
      },
    
      "security-2018-02-12-hpkp-deprecated-what-now-html": {
        "title": "HPKP is deprecated. What now?",
        "url": "/security/2018/02/12/HPKP-deprecated-what-now.html",
        "image": "/img/security/padlock_code.jpg",
        "date": "12 Feb 2018",
        "category": "post, blog post, blog",
        "content": "HPKP is deprecated. What now?  Recently Google announced their intent to deprecate support for Public Key Pinning (HPKP).Let’s have a look at the reasons for this and what technologies we can use to replace it.Deprecated? Why?As mentioned in the previous blog post, HPKP carries some very strong risks.It only takes a small mistake to render your site completely inaccessible, but that’s only 1 of the reasons Google mentions for deprecating support for HPKP.The other risks they call out are that it’s hard to build a pin-set that’s guaranteed to work and the risk of hostile pinning.Hostile pinning hasn’t been observed yet, but it’s an attack that allows someone to take your site hostage should they somehow be able to obtain a valid certificate for your domain.Because of the first 2 reasons, adoption rates of HPKP have remained very low and browser vendors have been looking for a viable replacement.Expect-CTOne of the new headers thought up to replace HPKP is Expect-CT (Expect Certificate Transparency).This tells the browser to check the Certificate Transparency (CT) logs to make sure the presented certificate is properly logged.Certificate TransparencyCT is a project by Google that provides a framework for monitoring and auditing SSL certificates in (almost) real-time.One of the reasons for its existence is the 2011 hack of the Dutch CA Diginotar. This resulted in the hackers being able to issue more than 500 fake SSL certificates (including for sites like facebook.com and google.com).In turn, these certificates could then be used by the attacker to perform a Man-in-the-middle (MitM) attack against these sites, without alerting the user that anything fishy was going on.CT is a tool that allows you to detect when a fake certificate has been issued. When a CA participates in the program, it has to log all certificates they issue in a publicly searchable log.These logs are monitored by applications which can report to you whenever a new certificate for one of your domains is issued.If the certificate was issued in error (or maliciously), you can immediately take steps to have it revoked.How does Expect-CT help me out here?Expect-CT tells the browser that you only want it to trust certificates signed by CAs that have Certificate Transparency enabled.When the server presents a certificate that’s not issued by such a CA, the browser will reject it and display a warning to the user.If you combine these 2 points, you can see how this protects your users:  By monitoring the CT logs, you can quickly identify any fraudulent or misissued certificates for yur domains and have them revoked.  If the certificate is issued by a CT that doesn’t pop up in these logs, it’s simply rejected by the browser.How to monitor CT logsOf course the whole premise of this solution is that you actually monitor the CT logs for your domains.If you don’t do this, you’re still at risk of someone obtaining a fraudulent certificate and impersonating you.Fortunately, there are plenty of companies and tools out there that can help you out with this.  SSLMate offers an open source tool called Certspotter  If you don’t want to run it yourself, you can pay them to do it for you.  For smaller (personal) projects, you can use Facebook’s monitor.  Or you use one of the other APIs or services that are available.What’s important is that you get the reports quickly so you can immediately take action.Use the headerSince (to my knowledge) browsers don’t have psychic powers (yet?), you still need to tell it that you expect the CA to have CT enabled.For that you’ll need to add the Expect-CT header on your responses.Obviously it will only look for these on an HTTPS URL, since on a simple HTTP connection it can easily be added or removed by a MitM.The header looks like this:http requestExpect-CT: enforce, max-age=31536000, report-uri=\"https://example.com\"This tells the browser to enforce the CT rule and to do so for the next year.Any infractions will be sent to the report-uri you mentioned.As with all headers that contain a report-uri, you can also use report-uri.io to aggregate these logs.As with most things that stand a chance of rendering your site inaccessible, it pays to be cautious when adding this header.Typically you don’t want to start by adding this header as defined above.Instead, you want to deploy it first without the enforce directive (and preferably a very low max-age such as 0)Doing so will tell the browser that you don’t want it to block connections with a bad certificate, but just to send the error to the report-uri.This setup allows you to test without impacting your users: you can now monitor this for a while to see if everything still works as expected.After that, enable the enforce directive and slowly increase the max-age to the point you want it to be.RisksThe risk of adding this header is quite low, if you follow the procedure above.You should only make sure that your CA actually uses CT.However, since October 2017 Chrome has made this a requirement in order for CAs to be in the trust-store.The main risk lies in not monitoring the CT logs properly. If you don’t monitor the alerts or don’t have a procedure to deal with misissued certificates, you’re still at risk of impersonation.CaveatsUnfortunately, there’s one major caveat to using this header.At the time of writing, only Chrome fully supports Expect-CT.Mozilla has also indicated that they will support it, but Microsoft so far doesn’t seem to be following suit.Should you use it?Yes. The risk is minimal, the only downside at the moment is the lack of browser support.At the very least, a large percentage of your users enjoys added protection against MitM attacks.Certificate Authority AuthorizationThe downside of Expect-CT compared to Public Key Pinning (HPKP) is that you need to make sure that your monitoring is handled correctly.If you don’t notice on time that a certificate has been issued, an attacker may be able to impersonate you for some time.You can make this a lot harder on the attacker by using Certificate Authority Authorization (CAA).CAA is a way for you to indicate exactly who is allowed to issue certificates for your domain.How to implement?Since the issuance of certificates is not limited to websites, CAA is not implemented through HTTPS response headers, instead it’s a record that you need to put in your DNS settings.You simply add the correct record to your DNS like this:            Name      Type      Value                  example.com.      CAA      0 issue \";\"        Note: the example above prevents all CAs from issuing certificates for your site. Don’t just copy-paste this.You can have multiple CAA records and the value of these tells the CA exactly what you want.Since this is a bit cryptic, lets look a bit more in detail at what’s happening here.The value above consists of 3 parts:  the flag (0)  the tag (issue)  the value (\";\")The combination of tag and value can be referred to as “the property”.The whole of CAA is governed by RFC 6844FlagsCurrently, flags can have 2 values: 0 or 128.A value of 0 means the property is non-critical, while a value of 128 means that is is critical.If a property is marked as critical, the CA must completely understand it before it proceeds.Generally it’s correct to use 0, so it’s advised to always use that value.There is support for customized flags in the RFC, but that’s beyond the scope of this post.TagsThe current specification has 3 tags you can define:  issue specifies which CA is authorized to issue certificates  issuewild indicates which CA is authorized to issue wildcard certificates (e.g. for *.example.com)  iodef similar to report-uri you can use this to get reports on invalid requests (either to an email address or to an http endpoint)issueThis tag specifies which CA is allowed to issue certificates for the domain and its subdomains.This includes the wildcard subdomain (meaning that the certificate would be valid for all subdomains).A value of “;” indicates that no issuance is allowed.You’re allowed to define multiple CAs, but you’ll need to use a new DNS record for each one:            Name      Type      Value                  example.com.      CAA      0 issue \"ca1.com\"              example.com.      CAA      0 issue \"otherca.net\"      issuewildThis one is used explicitly for wildcard certificates.If issuewild is present, any values in issue may not be used in the issuance of a wildcard certificate.You can use this in case you never want a wildcard certificate to be issued or when the list of CAs that are allowed to issue wildcard certificates differs from the original list.iodefYou can use this tag to report invalid certificate requests:            Name      Type      Value                  example.com.      CAA      0 iodef \"mailto:certificates@example.com\"              example.com.      CAA      0 iodef \"https://certificate.example.com/endpoint\"      As you can see, you can either have these reports sent by email, of have them delivered to an HTTP endpoint.The report is sent in the IODEF format, which also means that your endpoint needs to be RFC 6546 compliant.The easy wayTo help you in creating your CAA, SSL Mate has released a useful tool: CAA Record Helper.It can help you create a CAA record and will tell you how to set it up in your DNS service.RisksA badly implemented CAA record can mean that your CA is not allowed to issue your certificate.The other risk is that it relies on DNS: DNS records can be spoofed and this might allow an attacker to trick a CA into issuing a fraudulent certificate.Because of this, the RFC recommends implementing DNSSEC (Domain Name Security Extensions).Should I use it?I’d suggest you do. While having an incorrect policy can prevent the CA from issuing a certificate, this situation can be rectified quickly and shouldn’t put your users at risk.It will make it a lot harder for an attacker to obtain a certificate."
      },
    
      "docker-2018-02-12-azure-draft-html": {
        "title": "Azure Draft",
        "url": "/docker/2018/02/12/Azure-Draft.html",
        "image": "/img/2018-02-12-Azure-Draft/draft-logo.png",
        "date": "12 Feb 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Installing Draft  Setting Sail with Draft  ConclusionIntroductionDraft is an open-source tool from Microsoft Azure.It attempts to make the development for Kubernetes clusters easier, by getting Docker and Kubernetes out of the way.Developers no longer require Docker,and can just push their applications to a remote Kubernetes clusters using Draft.Draft accomplishes this by using only two simple commands.The first command is draft create.This tool detects the application language, and writes out a Dockerfile and a Kubernetes Helm chart in the source tree.These files are generated based on Draft “packs”.These packs are simple scripts that only detect the languageand write out the Dockerfile and Helm charts.The idea is based on some features of PaaS systems like the CloudFoundry’s buildpacks.The only difference is that the build and deployment descriptors are stored in the source tree.The second command is draft up.First, all source code will be uploaded to any Kubernetes cluster, local or remote.Then, the application is built on the cluster using the generated Dockerfile.Finally, the built image is deployed to a dev environment using the Helm Chart.Draft does not support many languages yet, but it currently supports most of the popular languages like Java, Python, Golang, JavaScript, Ruby, Swift, PHP, C# and Clojure.It also has support for Gradle and Maven projects.You can see all packs here.Installing DraftBefore you can start using Draft,there are quite a few components that need to be set up.However, if you are using a remote Kubernetes cluster,you will only need to do the setup once for multiple developers.Other developers will only need to install the Draft client to benefit.For this example, we will be using Minikube, a local Kubernetes cluster.The total list of tools required is the following:  Minikube: a local Kubernetes cluster  kubectl: the CLI tool for working with Kubernetes  Tiller: the Helm agent running on the Kubernetes cluster which manages installations of your charts.  Helm: the Helm client  Draftd: the Draft agent running on the Kubernetes clusterLet’s get started!Downloading all dependenciesWe will start by installing the latest release of Minikube using Homebrew.If you do not have Homebrew,you can check how to install Minikube here.$ brew cask install minikube==&gt; Satisfying dependenciesAll Formula dependencies satisfied.==&gt; Downloading https://storage.googleapis.com/minikube/releases/v0.25.0/minikube-darwin-amd64==&gt; Verifying checksum for Cask minikube==&gt; Installing Cask minikube==&gt; Linking Binary 'minikube-darwin-amd64' to '/usr/local/bin/minikube'.minikube was successfully installed!After Minikube has been installed,we can install Azure Draft!First, we add the Azure Draft repository by adding a Homebrew tap.$ brew tap azure/draftNow that we have added the repository,we can install Draft!$ brew install draft==&gt; Installing draft from azure/draft==&gt; Downloading https://azuredraft.blob.core.windows.net/draft/draft-v0.10.1-darwin-amd64.tar.gz/usr/local/Cellar/draft/0.10.1: 5 files, 45.9MB, built in 1 secondIf you do not use Homebrew,you can download the latest release of Draft here.You will have to unzip the download and add it to your PATH manually.Starting MinikubeNow we have downloaded all required dependencies,we can start setting up our cluster.Let’s start our Kubernetes cluster.$ minikube startStarting local Kubernetes v1.9.0 cluster...Starting VM...Downloading Minikube ISOGetting VM IP address...Moving files into cluster...Downloading localkube binaryConnecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file.The cluster is up and ready. As you can see from the output,Minikube also configured our kubectl client by automatically creating a .kubeconfig file.$ kubectl cluster-infoKubernetes master is running at https://192.168.99.100:8443To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.Enabling the Docker RegistryTo be able to use our Draft agent on the server,we will need to enable the embedded Docker registry on the cluster.Minikube makes this straightforward using an addon.We only need to enable it!$ minikube addons enable registryregistry was successfully enabledInstalling HelmNow that we have our Minikube up and running,we can install the Helm server agent (Tiller) and the Helm client.$ helm init$HELM_HOME has been configured at /Users/tomverelst/.helm.Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Happy Helming!Even though Tiller is installed now,you must wait for it to be deployed.Wait until there is one instance ready!$ kubectl -n kube-system get deploy tiller-deploy --watchNAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEtiller-deploy   1         1         1            1           4mInstalling DraftAll requirements are set up now for Draft.Let’s install the final component: Draft!$ draft init --auto-acceptInstalling default plugins...Installation of default plugins completeInstalling default pack repositories...Installing pack repo from https://github.com/Azure/draftError: Unable to update checked out version: exit status 128Error: exit status 1Uh, oh! Seems like Git cannot clone the Draft pack repo.According to this GitHub issue,this happens with Git version 2.16+.If you have this error, the workaround currently is to manually add a specific version of the pack repo.$ draft pack-repo add https://github.com/Azure/draft --version v0.10.0Installing pack repo from https://github.com/Azure/draftInstalled pack repository github.com/Azure/draftWe manually installed the Draft pack repo now. Let’s try to set up Draft again.$ draft init --auto-acceptInstalling default plugins...Installation of default plugins completeInstalling default pack repositories...Installation of default pack repositories complete$DRAFT_HOME has been configured at /Users/tomverelst/.draft.Draft detected that you are using minikube as your cloud provider. AWESOME!Draftd has been installed into your Kubernetes ClusterHappy Sailing!Great.The workaround works! As you can see, Draft is still in alpha and will not properly work yet.This setup is of course for local development.If you want to have a production ready, RBAC enabled, Draft setup on a remote Kubernetes cluster,you can take a look at the Advanced Installation guide.Setting Sail with DraftIf you managed to get to this point,you either went through all the effort to set everything up,or you skipped to this part!We can now start drafting up some applications. Since I am a fan of Go, I will start with drafting up a Go application.Here is a simple Go application that listens on port 8080 and returns “Hello Draft!”.package mainimport (    \"fmt\"    \"net/http\")func handler(w http.ResponseWriter, r *http.Request) {    fmt.Fprintf(w, \"Hello Draft!\")}func main() {    http.HandleFunc(\"/\", handler)    http.ListenAndServe(\":8080\", nil)}Let’s run it to see if it works.$ go run main.go# Open a separate terminal$ curl localhost:8080Hello, Draft!The application works. Now we can let Draft create the Dockerfile and the Helm chart.$ draft create--&gt; Draft detected Go (100.000000%)--&gt; Ready to sail$ lsDockerfile\tcharts\t\tdraft.toml\tmain.go$ ls charts/goChart.yaml\tcharts\t\ttemplates\tvalues.yamlDraft detected that it was a Go application, It generated a Dockerfile and the Draft deployment descriptor,and it also copied the Go pack to the charts directory.This is great, as it enables the possibility to customize the pack for this specific application.Let’s take a look at the generated Dockerfile.$ cat DockerfileFROM golang:onbuildENV PORT 8080EXPOSE 8080The official Golang onbuild image is used.This image is great for development purposes,but I would not recommend using this image for production purposes,as it is around 700MB, while the application is only a few lines of code.For demo purposes, let’s continue to use this generated Dockerfile,and try to deploy our application on Kubernetes using Draft.$ draft upDraft Up Started: 'goapp'goapp: Building Docker Image: SUCCESS ?  (60.1681s)goapp: Pushing Docker Image: SUCCESS ?  (63.0775s)goapp: Releasing Application: SUCCESS ?  (0.5346s)goapp: Build ID: 01C653GK70A7SR2FMT2325TBHDBuilding and pushing this application took around 2 minutes,which seems pretty long,but that is highly likely because of the 700MB base Docker image.This image first needs to be downloaded.Then it needs to be pushed to the registry.We can connect to the application using draft connect.$ draft connectConnecting to your app...SUCCESS...Connect to your app on localhost:50066Starting log streaming...+ exec appLet’s see how the application is installed on our cluster.$ kubectl get deploymentNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEgoapp-go   2         2         2            2           5m$ kubectl get svcNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGEgoapp-go     ClusterIP   10.103.78.13   &lt;none&gt;        80/TCP    4mkubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   1h$ kubectl get podsNAME                       READY     STATUS    RESTARTS   AGEgoapp-go-88f4b7bc7-4cltn   1/1       Running   0          4mgoapp-go-88f4b7bc7-wt7kx   1/1       Running   0          4mAs you can see,our application has successfully been deployed to Kubernetes,and is deployed using a Kubernetes Deployment resource.The services are not exposed by default,so we will need to either use kubectl port-forward &lt;pod&gt; 8080,or SSH into our cluster.$ minikube ssh$ curl 10.103.78.13Hello Draft! If you want to expose your applications automatically using Draft,you can use a Kubernetes Ingress Controller for this.You will need to enable an Ingress Controller in Kubernetes (minikube addons enable ingress),and initialize draft with the --ingress-enabled flag.More information about this can be found here.Deploying changesDraft is meant to be used during development,so it is important we can push changes.Let’s make a change to our application.func handler(w http.ResponseWriter, r *http.Request) {    fmt.Fprintf(w, \"Bye Draft!\")}Now that we have made some changes,let’s try to deploy our new version.This is done using the same command.$ draft upDraft Up Started: 'goapp'goapp: Building Docker Image: SUCCESS ?  (12.0163s)goapp: Pushing Docker Image: SUCCESS ?  (16.0110s)goapp: Releasing Application: SUCCESS ?  (0.2311s)goapp: Build ID: 01C65507WTBX5EAJKWWR53T652The build time has gone down from 2 minutes, to 28 seconds.This is because the Golang Docker image no longer needs to be downloaded and/or pushed to the Docker registry.The deployment is updated with the new version of the application.Old pods are taken down by Kubernetes and new ones are started.$ kubectl get podsNAME                        READY     STATUS              RESTARTS   AGEgoapp-go-6fb684d887-2kq69   0/1       ContainerCreating   0          23sgoapp-go-6fb684d887-qmth6   1/1       Running             0          23sgoapp-go-88f4b7bc7-wt7kx    0/1       Terminating         0          19m$ minikube ssh$ curl 10.103.78.13Bye  Draft! Our changes are now deployed to the Kubernetes cluster!ConclusionDraft is great for local development using Kubernetes.It is meant to be used before committing and pushing your code.Applications can be deployed to Kubernetes within minutes,without requiring to write Dockerfiles and/or Kubernetes resource files.Azure Draft is still experimental for now, but the development team is active, and I have not run into many issues yet.It brings one of CloudFoundry’s best features, namely build packs, to Kubernetes.It’s definitely worth a try!Resources  Draft website  Draft GitHub  Helm"
      },
    
      "testing-2018-02-05-writing-tests-in-kotlin-with-mockk-html": {
        "title": "Mocking in Kotlin with MockK",
        "url": "/testing/2018/02/05/Writing-tests-in-Kotlin-with-MockK.html",
        "image": "/img/writing-tests-in-kotlin-with-mockk/mockk.png",
        "date": "05 Feb 2018",
        "category": "post, blog post, blog",
        "content": "Table of contents  Introduction  Mockito and its shortcomings in Kotlin  The idiomatic mocking framework for Kotlin  Summing it all up  Other useful linksIntroductionI have been pretty excited about Kotlin since JetBrains released the first official version on the 15th of February 2016.It did take me a while before I managed to get my hands dirty, which was in between the version 1.1 and 1.2 release.Besides developing in Java, which I’m doing full time as a senior Java consultant, I’ve also been dabbling in Scala for quite some years with Lightbend’s Play Framework.Everyone knows how verbose Java is, and how it lags a bit behind the newer, fancier programming languages.It still misses features such as pattern matching, case/data classes and local-variable type inference.Starting from Java 8 with the introduction of Lambdas, we have finally been given the option to add a more functional programming flavour to our code which was due in time.Scala felt very refreshing for me back then, when I started to use it which was shortly before the JDK 7 release.It felt clean and powerful, bringing the best of both worlds of object-oriented programming and functional programming.At the same time, Scala houses a lot of complexity since there are so many ways and styles to tackle problems.You could compare it a bit to having the toolkit available to build a space rocket when you only plan on building a small airplane. And this is where Kotlin comes in, being very similar to Scala but with a focus on practicality and simplicity.Coming from the industry instead of academia, it focuses on solving problems faced daily by programmers.I’m a big fan of Test-Driven Development and thoroughly testing the behaviour of my code by making use of mocks.Mockito has been my mocking framework of choice combined with PowerMock for mocking constructors, static and private methods, and more.As Kotlin also runs on the JVM, it can make use of the huge Java ecosystem.It was a no-brainer for me to immediately add these testing libraries to my Kotlin project for writing my tests.And thus I set off, creating a new Kotlin project to see how it fared.Mockito and its shortcomings in Kotlin    I started off with adding the Mockito dependency to my Kotlin project.&lt;dependency&gt;    &lt;groupId&gt;org.mockito&lt;/groupId&gt;    &lt;artifactId&gt;mockito-core&lt;/artifactId&gt;    &lt;version&gt;2.13.0&lt;/version&gt;&lt;/dependency&gt;And wrote a first simple test in which I wanted to test a Service class that uses a Generator to generate a record and a Dao for persisting it.class ServiceTest {    class Generator { fun generate(): String = \"Random String that's not random\" }    class Dao { fun insert(record: String) = println(\"\"\"Inserting \"$record\"\"\"\") }    class Service(private val generator: Generator, private val dao: Dao) {        fun calculate() {            val record = generator.generate()            dao.insert(record)        }    }    val generator = Mockito.mock(Generator::class.java)    val dao = Mockito.mock(Dao::class.java)    val service = Service(generator, dao)    @Test    fun myTest() {        val mockedRecord = \"mocked String\"        Mockito.`when`(generator.generate()).thenReturn(mockedRecord)        service.calculate()        Mockito.verify(generator).generate()        Mockito.verify(dao).insert(mockedRecord)        Mockito.verifyNoMoreInteractions(generator, dao)    }}Writing the test went pretty smooth although the code looks a bit funky.When I ran it, I stumbled on this nice error:org.mockito.exceptions.base.MockitoException: Cannot mock/spy class be.yannickdeturck.HelloTest$GeneratorMockito cannot mock/spy because : - final class — anonymous classes — primitive typesAs all classes and methods are final by default in Kotlin, using Mockito appears to be a bit problematic due to how Mockito creates its mocks.If you are interested in how Mockito’s mocking works internally you should checkout this response on StackOverflow that roughly sums it up.I did a bit more research on using Mockito and stumbled upon this slightly tuned version for Kotlin, wrapping some of Mockito’s functionalities, providing a simpler API.I decided to try that one out and replaced my Mockito dependency with it:&lt;dependency&gt;    &lt;groupId&gt;com.nhaarman&lt;/groupId&gt;    &lt;artifactId&gt;mockito-kotlin&lt;/artifactId&gt;    &lt;version&gt;1.5.0&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;I rewrote my test a bit in order to make use of the cleaner syntax the library had to offer.Note how both defining and using the mocks is a bit more elegant:class ServiceTest {    class Generator { fun generate(): String = \"Random String that's not random\" }    class Dao { fun insert(record: String) = println(\"\"\"Inserting \"$record\"\"\"\") }    class Service(private val generator: Generator, private val dao: Dao) {        fun calculate() {            val record = generator.generate()            dao.insert(record)        }    }    val generator = mock&lt;Generator&gt;()    val dao = mock&lt;Dao&gt;()    val service = Service(generator, dao)    @Test    fun myTest() {        val mockedRecord = \"mocked String\"        whenever(generator.generate()).thenReturn(mockedRecord)        service.calculate()        Mockito.verify(generator).generate()        Mockito.verify(dao).insert(mockedRecord)        Mockito.verifyNoMoreInteractions(generator, dao)    }}Sadly, we still have the Mockito error.As I said, in Kotlin all classes and methods are final by default which Mockito cannot deal with.You would have to explicitly make your classes inheritable using the open modifier.Another approach would be to add interfaces to everything.Changing your code just for the sake of being able to write good tests is something I’m not exactly fond of, and in this case we are also getting around one of the key features of Kotlin.Starting from Mockito version 2.0.0 it did became possible to mock final classes although it is an incubating, opt-in feature.This however, requires a bit of a setup really.It basically consists of creating a file called org.mockito.plugins.MockMaker with as content mock-maker-inline and placing it under resources/mockito-extensions.It felt a bit hacky but apparently this is only a temporary way to set it up.Although there are supposed to be plans to make it more straightforward.Hadi Hariri wrote an extensive blog post on setting this up and you should check it out if you would like to learn more about it.Good, so this makes it possible to create mocks without having to add the open modifier to all your classes and methods!It does’t appear to be completely compatible with Mockito Kotlin even though the library depends on Mockito version 2.8.9.Trying to run the test resulted in the following error:org.mockito.exceptions.base.MockitoInitializationException: Could not initialize inline Byte Buddy mock maker. (This mock maker is not supported on Android.)At the time of writing there is a version 2.0.0 in alpha for Mockito Kotlin so I tried to switch to it to see if it changed anything.Note that the dependency is a bit different and you need to use the classes in the com.nhaarman.mockitokotlin2 package:&lt;dependency&gt;    &lt;groupId&gt;com.nhaarman.mockitokotlin2&lt;/groupId&gt;    &lt;artifactId&gt;mockito-kotlin&lt;/artifactId&gt;    &lt;version&gt;2.0.0-alpha02&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;It got rid of the above error.I did ran into some unexpected behaviour where my mocks’ behaviour was rather unexpected when I also added partial mocking using spies.I spent some time to get my head around it and during my quest for answers I stumbled upon this library called MockK, created by Oleksiy Pylypenko.I decided to check it out as I became a bit annoyed with Mockito in Kotlin so far.The idiomatic mocking framework for Kotlin    Although I am a huge fan of Mockito for mocking in Java, using Mockito in Kotlin just feels a bit too Java-ish when you have this elegant Kotlin code all around in your project. MockK’s main philosophy is offering first-class support for Kotlin features and being able to write idiomatic Kotlin code when using it.Adding MockK is as simple as ever as you only have to add the dependency to your project and you are set to go.Maven:&lt;dependency&gt;    &lt;groupId&gt;io.mockk&lt;/groupId&gt;    &lt;artifactId&gt;mockk&lt;/artifactId&gt;    &lt;version&gt;${mockk.version}&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;Gradle:testCompile \"io.mockk:mockk:${mockkVersion}\"The available MockK documentation provides a really nice overview of all the different features with a lot of examples, making it very easy to get started.If you have used a mocking framework before such as Mockito, everything should come natural as you usually do when writing tests with mocks.You have the same three parts in which your tests are divided:  Preparing the test data and setting up the mocking  Executing the logic that you want to test  Performing the necessary validation and verification checks to see if the result and behaviour matches your expectations.The test from above is written as follows:class ServiceTest {    class Generator { fun generate(): String = \"Random String that's not random\" }    class Dao { fun insert(record: String) = println(\"\"\"Inserting \"$record\"\"\"\") }    class Service(private val generator: Generator, private val dao: Dao) {        fun calculate() {            val record = generator.generate()            dao.insert(record)        }    }    val generator = mockk&lt;Generator&gt;()    val dao = mockk&lt;Dao&gt;()    val service = Service(generator, dao)    @Test    fun myTest() {        val mockedRecord = \"mocked String\"        every { generator.generate() } returns mockedRecord        every { dao.insert(mockedRecord) } just Runs        service.calculate()        verifyAll {            generator.generate()            dao.insert(mockedRecord)        }    }}Going over the example, everything should feel familiar but more elegant.You may however, wonder what the every { dao.insert(mockedRecord) } just Runs line is doing exactly.By default in MockK, mocks are strict so you need to provide some behaviour for them.If we were to omit the line, the test would fail as we would run into the following error:io.mockk.MockKException: no answer found for: Dao(#2).insert(mocked String)A feature I was immediately fond of as I like to write strict tests.Note that you can also define the mock as being a relaxed mock in order to avoid this strict behaviour:val dao = mockk&lt;Dao&gt;(relaxed = true)Mockito has something similar for verifying with Mockito.verifyNoMoreInteractions(generator, dao) which I also extensively use to enforce that all calls of mocked objects are verified.Of course, the above example is only the tip of the iceberg as MockK houses a ton of other features such as annotations to simplify creating mock objects, spying to mix mocks and real objects, partial argument matching, capturing arguments, verification order support, matchers, coroutine mocking support, and so much more.After fiddling with Mockito, I happily continued using MockK for my little Kotlin project.Summing it all upMockito for me felt a bit hacky/unnatural and too Java-ish when using it in a Kotlin project.I imagine it will probably become better in the near future.MockK, being a newer library specifically targeted at Kotlin, felt clean and pleasant to use with excellent documentation.Oleksiy is also actively working on the library as you can see in the repo’s releases section.I highly recommend checking out MockK for mocking in a Kotlin project as it is currently a better option in my humble opinion.Oleksiy is also very active on the MockK Gitter and he helps you out quickly should you have any questions.He is also open to feedback and enjoys being challenged in order to make MockK an even better library.If you want to learn more about MockK, you should definitely read Oleksiy’s blog post series in the next section.Other useful links  MockK documenation  MockK GitHub  MockK Gitter  Oleksiy Pylypenko’s Twitter  Blog post by Hadi Hariri: Mocking Kotlin with Mockito  Blog post by Oleksiy Pylypenko: Mocking is not rocket science: Basics  Blog post by Oleksiy Pylypenko: Mocking is not rocket science: Expected behavior and behavior verification  Blog post by Oleksiy Pylypenko: Mocking is not rocket science: MockK features"
      },
    
      "architecture-2018-01-27-visualizing-your-spring-integration-components-and-flows-html": {
        "title": "Visualizing your Spring Integration components &amp; flows",
        "url": "/architecture/2018/01/27/Visualizing-your-Spring-Integration-components-and-flows.html",
        "image": "/img/2018-01-27-Visualizing-your-Spring-Integration-components-and-flows/post-image.jpg",
        "date": "27 Jan 2018",
        "category": "post, blog post, blog",
        "content": "  This post can be useful for everyone who wants to have insights on their application’s internal architecture when integrating with other systems using Spring Integration.From the developer that just started in your team and who wants to have an overview, to the seasoned team member that needs to troubleshoot a problem in production.Currently we are working on the phased roll out of a microservices architecture at one of our clients.To ensure that everything works as it’s supposed to we are using a lot of Enterprise Integration Patterns to stitch both the old and the new landscape together.The best way to achieve a solution when using Java, is to use Spring Integration.A few days ago we wanted to have clear insights on how certain microservices are communicating with the existing systems.Creating your graph of Spring Integration components and flowsA first step to expose your Spring Integration components and flows is to add an IntegrationGraphServer bean to your application.This class resides in the o.s.i.support.management.graph package, between all the required classes to collect, build and render the runtime state of Spring Integration components as a single tree-like Graph object.Exposing the graphUsing the Spring Integration HTTP module you can easily expose the IntegrationGraphServer functionality as a REST service.Just add the @EnableIntegrationGraphController annotation to your application, and you’re good to go!Or, in case you are using XML config, add the &lt;int-http:graph-controller/&gt; XML element to your setup.Be sure to edit the allowedOrigins attribute of the annotation in case you’re accessing the endpoint between 2 domains.Sidenote: your application needs to be deployed on a web container, or it needs to use an embedded web container in case you are building on top of Spring Boot.Visualizing the exposed graph of components and flowsWith D3.js we are able to visualize everything within our graph.In this quick &amp; dirty gist I created, you can find a simple example of a possible visualisation.Download the index.html file, point the script to the correct endpoint by editing the graphEndpoint variable, open the file in your browser and you should see every component and flow!As Gary Russell pointed out in his reply to my tweet you can also use Spring Flo for the visualization.Spring Flo is an Angular based, embeddable graphical component for pipeline/graph building and editing.This is used as the basis of the stream builder in Spring Cloud Data Flow.You can find the sample application here.Taking it one step furtherIf you add the @EnableIntegrationManagement annotation or the &lt;int:management /&gt; XML element to your setup, the graph will even expose all the metrics of your Spring Integration components.This will definitely help you out when you want to monitor your components and flows, enabling you to troubleshoot problems even faster in case something goes wrong in production."
      },
    
      "tech-2018-01-20-jworks-tech-blog-html": {
        "title": "The JWorks Tech Radar",
        "url": "/tech/2018/01/20/JWorks-Tech-Blog.html",
        "image": "/img/techradar/TechRadar.png",
        "date": "20 Jan 2018",
        "category": "post, blog post, blog",
        "content": "  The JWorks Technology Radar is intended to showcase our opinion on the most important tech trends we see today.It is based on the Thoughtworks Technology Radar and uses its open source framework for visualization.We adopted the mechanics and methodology from Thoughtworks, as described below.Our tech radar can be reached through this link.The RadarThe Radar is a document that sets out the changes that we think are currently interesting in software development - things in motion that we think you should pay attention to and consider using in your projects.It reflects the idiosyncratic opinion of a bunch of senior technologists and is based on our day-to-day work and experiences.While we think this is interesting, it shouldn’t be taken as a deep market analysis.QuadrantsThe quadrants are a categorization of the type of blips:  Programming Languages and Frameworks. Quite straight-forward, languages and software frameworks.  Tools. These can be components, such as databases, software development tools, such as Versions Control Systems; or more generic categories of tools, such as the notion of polyglot persistence.  Platforms. Things that we build software on top of such as mobile technologies like Android, virtual platforms like the JVM, or generic kinds of platforms like hybrid clouds.  Techniques. These include elements of a software development process, such as experience design; and ways of structuring software, such as microservices.We don’t make a big deal out of the quadrants - they’re really just a way to break up the Radar into topic areas. We don’t think it’s important which quadrant a blip goes into, unlike the rings - which generate a lot of discussion.RingsThe metaphor of a radar says that the closer a blip is to you, the sooner it will be on top of you. Like most metaphors, you can’t take it too seriously, but there’s an essential sense to it.Our Radar has four rings, which we’ll describe starting from the middle:  The Adopt ring represents blips that we think you should be using now. We don’t say that you should use these for every project; any tool should only be used in an appropriate context. However we do think that a blip in the Adopt ring represents something where there’s no doubt that it’s proven and mature for use.  The Trial ring is for blips that we think are ready for use, but not as completely proven as those in the Adopt ring. So for most organizations we think you should use these on a trial basis, to decide whether they should be part of your toolkit. Typically we’re happy to use trial blips now, but we realize that most readers will be more cautious than us.  The Assess ring are things that you should look at closely, but not necessarily trial yet - unless you think they would be a particularly good fit for you. Typically, blips in the Assess ring are things that we’re currently trialling, on our projects.  The Hold ring is for things that are getting attention in the industry, but we don’t think are ready for use. Sometimes this is because we don’t think they’re mature enough yet: sometimes it means we think they’re irredeemably flawed. We don’t have an “avoid” ring, but we do throw things in the Hold ring that we wish our clients wouldn’t use.Unlike the quadrants, we do have some quite passionate arguments about which ring a blip should go into. We don’t tend to have angry debates, but rings are what generate the most energetic discussions. Over the course of making the Radar we’ve come up with some useful rules of thumb to help us put things into rings.We can only put blips into the Trial ring when we have experience of that blip on a real project. This can mean we sometimes look behind the technology curve, because we may like the look of a technology but haven’t yet persuaded a client to try it out - and until we do that blip cannot pass into Trial.For the Adopt ring, we only include items when we think it would be a poor and potentially irresponsible choice not to use them given the appropriate project context.More information can be found on the Thoughtworks website."
      },
    
      "testing-2018-01-05-gauge-automated-testing-html": {
        "title": "Automated testing with Gauge",
        "url": "/testing/2018/01/05/Gauge-automated-testing.html",
        "image": "/img/2018-01-05-gauge-automated-testing/gauge.png",
        "date": "05 Jan 2018",
        "category": "post, blog post, blog",
        "content": "After finishing a new feature in your web app, you test whether it works.However, while working on that new feature, you may have broken something else.If you want to know for sure, you have to test everything.Clicking around, filling in forms, … will cost you some valuable time.So, you’ll probably end up not doing it thorough enough or even not at all, assuming everything else still works fine.That’s exactly why you should have automated tests do it for you!Gauge is yet another test automation tool that serves that purpose.The founder is ThoughtWorks, the company that also created  Selenium and GoCD.It’s still in beta at the moment of writing, but it already works terrific!Gauge is comparable with Protractor or JUnit extended with Cucumber (if you haven’t heard of these, they are worth checking out).In this blog we’ll be mainly talking about automating browser testing using Selenium WebDriver, although that isn’t the sole purpose of Gauge.If you don’t know WebDriver, it’s what allows you to interact with the browser to traverse the DOM, click on elements and so on.Now, unlike Cucumber, Gauge itself only supports three languages at the moment: Java, Ruby and C#.Others may be supported by the community like JavaScript.You can define steps written in either one of those languages.These are the actual tests and can be identified by a sentence you can choose yourself.By combining these sentences, you can write test scenarios which are written in Markdown.That means you don’t need any programming experience to write test scenarios.You could create some sort of dictionary with the step sentences that others can use for the scenarios or the other way around.In other words, anyone is able to read and write test scenarios.The actual implementation of the steps does require some technical knowledge.Table of contents  Setup  Writing tests  Running the specs  Environments and configuration  Reports  ConclusionSetupGauge offers an installer which can be downloaded here.During the install, you can select which core plugins to install.In this blog the tests are written in Java, so we would need the Java core plugin.After the install, you’ll be able to run gauge from the command line.It can, for example, be used to install more plugins (as well as core plugins gauge install java).Next, in order to use WebDriver in our tests, we’ll need the Java SDK and Maven.When you’re a Java developer, you probably already have those installed.If not, you’ll find enough on Google on how to install those.To start a new project, create a new folder and run gauge init java in it.This will setup a basic Gauge project.Then we need to add a pom.xml file ourselves, because we need some dependencies such as Selenium WebDriver.Next, WebDriver needs to be set up in our test code.A good example of how you could do that can be found here.They created a DriverFactory so you can easily switch between browsers using environment variables (we’ll get to that).Gauge hooks are used to start and stop WebDriver when running the test suite.Just copy those pieces of code into your own project.Or, you could start from the Maven + Selenium example provided by Gauge which can be found on GitHub.That way you’d have some examples to start from.Now we can start writing tests.Writing testsOn the lowest level we have our Java functions that control the browser using WebDriver commands.To these functions we can assign a @Step annotation to be able to identify it.It’s usually a sentence describing the action being performed.The sentences can then be used to write the scenarios of our tests (or specs).If you’d like to combine multiple steps into once sentence, you can do so by creating so called “concepts”.A typical folder structure for a Gauge setup using WebDriver and Java is as follows:    Writing the specsThe specs are written in Markdown.Each spec file starts with a title and is underlined with ===.Next, some steps can be defined that will be run before each scenario.When listing steps, you need to prefix each step with an asterix (*) as in a Markdown list.After that, the actual scenarios can be written.They start with a title and are underlined with ---.Again, the steps for a scenario should be listed as in a Markdown list.You can also add some tags which can be used to only run certain specs and to search in the HTML reports.Here’s an example specification:Customer sign-up================* Go to sign up pageCustomer sign-up----------------tags: sign-up, customer* Sign up a new customer with name \"JWorks\" email \"jworks@ordina.be\" and \"password\"* Check if the sign up was successfulWriting the stepsThe sentences we wrote in the specs still need to be linked to Java functions.We can do so by simply adding a @Step annotation to a Java function.It doesn’t matter in which class you put the functions, you’re free to choose how to organize them.As long as they are under the src/test/java folder.You could, for example, group them per page or feature.public class CustomerSignup {    @Step(\"Sign up as &lt;customer&gt; with email &lt;customer@example.com&gt; and &lt;password&gt;\")    public void registerCustomerWith(String customer, String email, String password) {        WebDriver webDriver = Driver.webDriver;        WebElement form = webDriver.findElement(By.id(\"new_user\"));        form.findElement(By.name(\"user[username]\")).sendKeys(customer);        form.findElement(By.name(\"user[email]\")).sendKeys(email);        form.findElement(By.name(\"user[password]\")).sendKeys(password);        form.findElement(By.name(\"user[password_confirmation]\")).sendKeys(password);        form.findElement(By.name(\"commit\")).click();    }    @Step(\"Check if the sign up was successful\")    public void checkSignUpSuccessful() {        WebDriver webDriver = Driver.webDriver;        WebElement message = webDriver.findElements(By.className(\"message\"));        assertThat(message.getText(), is(\"You have been signed up successfully!\"));    }}As shown in the example above, you can easily pass parameters to steps.You simply have to wrap the keywords in &lt;&gt; in the @Step annotationand list the same keywords as parameters in the actual Java function.Obviously, you can then use them in your Java code.Page object patternA clean way to organize all your code would be to use the page object pattern.This means that for every page in your web app, you should create a class.Such a class contains all code to interact with that specific page.The example we saw earlier, could be transformed into this:Page object:public class SignUpPage {    public WebElement usernameField;    public WebElement emailField;    public WebElement passwordField;    public WebElement passwordConfirmField;    public WebElement commitButton;    SignUpPage() {        WebDriver webDriver = Driver.webDriver;        WebElement form = webDriver.findElement(By.id(\"new_user\"));        this.usernameField = form.findElement(By.name(\"user[username]\"));        this.emailField = form.findElement(By.name(\"user[email]\"));        this.passwordField = form.findElement(By.name(\"user[password]\"));        this.passwordConfirmField = form.findElement(By.name(\"user[password_confirmation]\"));        this.commitButton = form.findElement(By.name(\"commit\"));    }}Step definition:public class CustomerSignup {    private SignUpPage signUpPage = new SignUpPage();    @Step(\"Sign up as &lt;customer&gt; with email &lt;customer@example.com&gt; and &lt;password&gt;\")    public void registerCustomerWith(String customer, String email, String password) {        this.signUpPage.usernameField.sendKeys(customer);        this.signUpPage.emailField.sendKeys(email);        this.signUpPage.passwordField.sendKeys(password);        this.signUpPage.passwordConfirmField.sendKeys(password);        this.signUpPage.commitButton.click();    }}The great benefit of this approach is that you can reuse a lot of the code.You only have to locate the elements once instead of in every step.Functions can be added to the page objects as well.Suppose you have a dropdown, you first have to click to open it and then select an option from the list.You can write a function doing all that.In your step definition, you then simply have to call that function to select something from a dropdown.It’s a good way to avoid too much code duplication.ConceptsIf you find yourself repeating the same sequence of steps over and over,you could combine those steps into one step using concepts.These are also written in Markdown and you can pass arguments the same way as in the Java @Step annotations.They should be placed in the /specs/concepts folder and use the *.cpt extension.# Sign up a new customer with name &lt;name&gt; email &lt;email&gt; and &lt;password&gt;* Sign up as &lt;name&gt; with email &lt;email&gt; and &lt;password&gt;* Show a message \"Thank you for signing up! You are now logged in.\"Cucumber only offers this feature in some implementations, they don’t in Cucumber.js for example.They refuse to implement it because they believe this creates too much abstraction and makes you lose sight of the overall picture.Gauge does offer it, so it’s up to you whether you want to make use of it or not.Running the specsSince the project is setup with Maven, the tests can be run with mvn test.However, if you want to pass any arguments, you’ll need to use mvn gauge:execute instead.TagsYou may have noticed in the spec files that tags can be added.They can be used to run only certain specs.mvn gauge:execute -DspecsDir=specs -Dtags=\"sign-up &amp; customer\"ParallelSo far, Gauge didn’t stand out from other automated testing solutions.Although there’s one thing that really does stand out and that’s how easy it is to use parallel execution!mvn gauge:execute -DspecsDir=specs -DinParallel=trueRunning this command will start up a stream for each CPU core your computer has.For each stream it will open a browser window and execute the specs.So if you have four CPU cores, four browser windows will be opened.You can overwrite the number of parallel executors, but it’s recommended not to exceed the number of CPU cores.mvn gauge:execute -DspecsDir=specs -DinParallel=true -Dnodes=2Now, some specs may take longer to run than others.By default, specs are divided dynamically over the streams.So when a spec has finished, it will take the next one from the list of specs that still need to be executed.It’s possible to change this so the specs are divided on startup, but the command is deprecated and will be removed.Making tests independentTo make use of this parallel execution, you’ll have to make sure that your tests don’t rely on each other.I think it’s pretty clear why you shouldn’t do that.Anyway, suppose you test the sign up and sign in.If your sign in relies on the user being signed up through a previous test, these tests should be run synchronously.However, if you would want to test whether a user can sign in after having signed up, you should do so in one test.That immediately solves our problem and we are safe to use the parallel execution!Environments and configurationThe config files are located under env/default.You should have three files in that folder: default.properties, java.properties and user.properties.In the example by Gauge, they have an APP_URL parameter in that last file.I recommend using that approach as well, you can get parameters in your Java code using System.getenv(\"APP_URL\").It’s possible to create different environments by simply creating a new folder.There you can add *.properties files containing additional properties or properties overwriting the ones in the default folder.So, that means you don’t need to copy the whole configuration. Gauge will always load all the default properties. Then it will load those defined for the environment you wish to use. If a property is defined for an environment and it already exists in the default configuration, it will use the one defined for that environment instead.Now, suppose you want a different configuration for your CI-tool, you can create a folder named ci.When running the specs, simply pass an argument stating the environment.mvn gauge:execute -Denv=\"ci\"Environments can also be used to run gauge with another browser like in the example by Gauge.Create a folder named firefox for example and add a file called browser.properties.In that file you add browser = FIREFOX.When you then run the tests with the firefox environment, it will use FireFox as a browser instead.(This only works if you have your project set up like in the example, the Driver and DriverFactory files are required here.)ReportsTo get a HTML report, the plugin has to be installed first: gauge install html-report.That’s about it!After running the specs, a nice HTML report will be outputted to the /reports folder.It shows which tests succeeded and which failed with some additional graphs.In the report you’ll even find how long it took to run a test and each of its steps.There’s also a search functionality to quickly find a certain spec.    ConclusionIt’s a good idea to write automated tests.If you do it well, you don’t have to spend a lot of time manually testing your application.The chance of something being broken by your changes will decrease dramatically.Also, be sure to use the page object pattern and create functions for repetitive actions.It avoids code duplication and having to update the same code in multiple places.If you’re starting a new project or starting from scratch with writing browser tests, you should consider using Gauge.Even though it’s still in beta.With ThoughtWorks as the main force behind this tool, it’s here to stay!"
      },
    
      "testing-2018-01-04-3-stages-api-testing-html": {
        "title": "3 Stages of API testing",
        "url": "/testing/2018/01/04/3-stages-api-testing.html",
        "image": "/img/3-stages-api-testing/overview.png",
        "date": "04 Jan 2018",
        "category": "post, blog post, blog",
        "content": "  Continuous Integration with automated testing is more and more incorporated in the culture of software delivery companies.Running tests in different stages is a big part of it.In this post, we’ll have a look at our three stages of API testing we are promoting at Ordina.Table of contents  About the setup  Stage 1: Unit testing  Stage 2: Testing against a mock-end  Stage 3: Testing full environment  ConclusionSetupThe example we are going to use is part of a bigger microfrontend/microservice setup.The front-end part of this example is the actual header of this microfrontend setup. It’s the top bar, developed as a separate front-end application.This header provides the user with a search input field, where the user can search our database of competence centers.This part is written in Angular (5).The app gets dockerized after the unit tests (and build) are successfully completed.It is served by a simple Express server inside a Docker container.The back-end part provides the data of the competence centers.It’s nothing more than a simple REST API written in TypeScript using the Nest.js framework.The data provided by this service is a JSON file.Its content is parsed into memory and is exposed through this REST API.    The front-end (header) is providing the user with an input field.This field allows the user to perform a search on our back-end service.It also provides a clear button, so the user can remove the content from the input field and reset the local cache of search results.A second button is the filter button.When pressed, it will emit an event that can be listened to by other microfrontends.public filterCCs(): void {  if(isPlatformBrowser(this.platformId)) {    const event = new CustomEvent('filterCCs', { detail: { needle: this.needle} });    window.dispatchEvent(event);  }}public filterStats() {  if ( this.needle === '' ) {    this.data = [];  } else {    this._updateData();  }}private _updateData() {  this._ccService.getStats(this.needle).subscribe((response) =&gt; {      this.data = response;  });}public resetSearch(): void {  this.needle = \"\";  this.data = [];}When resetting the content with the clear button, we’re not sending a request.When there’s nothing to search for, the result would be an empty array.So we’re just resetting our local data to an empty array.    The back-end API is exposing three endpoints:@Get()async getAllCCs(@Response() res) {    const ccs = await this._ccsService.getAllCCs();    res.status(HttpStatus.OK).json(ccs);}@Get('/search')async searchCCs(@Response() res, @Query('needle') searchString) {    let filtered = this._ccsService.searchCCs(searchString);    res.status(HttpStatus.OK).json(filtered);}@Get('/:id')async getCC(@Response() res, @Param('id') id) {    const cc = await this._ccsService.getCC(+id);    res.status(HttpStatus.OK).json(cc);}The header will always trigger the endpoint at search with a needle.When the needle is undefined (or an empty string), the search endpoint will return an empty list.Stage 1: Unit testing the communication layer (front and back-end)    The first stage is unit testing each component. This step is almost always part of the component’s build. Let’s take a quick look at each component.Unit testing the communication part of the UI    For unit testing our front-end Angular 5 application, we are going to use the import { MockBackend } from '@angular/http/testing'; from Angular itself to mock our back-end....beforeEach(async(() =&gt; {    TestBed.configureTestingModule({        ...      providers: [        CCService, // our service that is handling the communication        {provide: XHRBackend, useClass: MockBackend} // our mock      ]    })    .compileComponents();  }));...describe('Should query ccs with an observable', () =&gt;    it('Should return data', inject([CCService, XHRBackend], (ccService, mockBackend) =&gt; {      mockBackend.connections.subscribe((connection) =&gt; {        const ccs: Array&lt;CC&gt; = [          {              ...          },          {              ...          }        ];        connection.mockRespond(new Response(new ResponseOptions({          body: JSON.stringify(ccs)        })));      });      ccService.getStats('tim').subscribe((ccs: Array&lt;CC&gt;) =&gt; {        expect(ccs.length).toBe(2);        expect(ccs[0].clEmail).toBe('tim.vierbergen@ordina.be');       });      })    )  );The Angular framework handles the communication part.We want to mock the $http call and see how we are handling the result.The mockBackend is working inbetween our own code and the provided $http part.Every call is triggering the mockBackend.connections, so the subscribers are triggered.We are providing our own data and returning it as the response of the $http call.Unit testing the communication layer of the service    For unit testing our back-end, we are using Jest.Jest is a testing framework by Facebook.If you are interested in Jest, make sure to watch this blog, as a post around this topic is in the making.Just as with our front-end, we trust the framework to correctly handle the communication itself.Our unit tests will start at the controller level of our API.describe('searchCCs', () =&gt; {    it('should return a filtered array of competence centers', async () =&gt; {      await ccsController.searchCCs(mockResponse, 'tim');      let data = JSON.parse(mockResponse.data);      expect(data.length).toBe(2);      expect(data[0].clEmail).toBe('tim.vierbergen@ordina.be');    });});Where mockResponse is exactly that.A simple mock of the response object.Purpose of these unit testsThe goal of these unit tests is to make sure that the functions inside the components are working as expected.This way the next stage of testing can only fail due to errors from outside this component.However, to make sure this is the case, the mock data should be as close to production data when it comes to data specific characteristics.This is more important for your front-end because you have less control over the data itself.Your back-end API is only responding to request params that are defined in the specs.So it’s easier to control them, or ignore unknowns.Stage 2: Testing against a mock-end    In our second stage we are going to use a mock service to test against.This means that we are going to mock ‘the other’ component by replacing it with an easy to use solution.Although we are mocking some parts, this can be seen as an end-to-end test for each component itself.We want to test our component by means of external services, just as it is supposed to work in a complete environment.Mocking our back-end to test our front-end.    For our front-end component (user interface), we are going to mock the back-end.Some front-ends are performing calls even without human interaction.However, in most cases, front-end communication is depending on human interaction.To end-to-end test this part, we are also in need of a framework to mock this user interaction.Gauge, Protractor and Nightwatch.js are some examples of these frameworks.Most of them depend on ‘Selenium WebDriver’.Node-RED for back-end mockingWe are using Node-RED for our back-end mock because it’s so easy to setup and dockerize.  Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways.Node-RED is much more than just a tool to mock a back-end or any other service.It comes with a great User Interface to define your flows and to deploy them on your server.For this example we will mock our three endpoints and return test data.This test data can come from different sources.Node-RED provides multiple ways of working with data.You can include a simple MongoDB in your setup and read (even write) data from it.Or you can just use functions where you hard code your data.To keep it simple, we will use the latter in our setup.    A simple mock for a http-call consists out of three parts:  The entry point definition itself (input)  The function that handles the data (can be static or database or …)  The response definition    User interface automationIn this setup we are using Protractor for the e2e tests.The user input is limited to an input field to trigger the search REST-call and two buttons, one button for clearing the input and one for sending the search string to other microfrontend components.Some of the use cases, such as ‘clearing’ the input, are already covered in the unit tests.Depending on the effort you can always retest them in these e2e tests, but for this example, those are not important.We want to trigger the search REST call by sending the search string tim to the input field, and testing the outcome in the user interface.    ...describe('Searching with Tim should show 2 results', () =&gt; {    page.setSeachText('mySearchString').then(function() {        it(\"Input field should contain 'tim'\", function() {            expect(page.getSearchText()).toBe('tim');        });        describe(\"Result should show 2 entries\", function() {    \t    it(\"will show the number 2\", function() {                expect(page.getResultNumber()).toBe('2');            });            it(\"will show a dropdown with 2 results\", function() {                expect(page.getResultList().length).toBe(2);            });        });    });});...Conclusion for our front-endThis mock e2e test is depending on a mock back-end and a user input automation system.We are running these tests on our GoCD setup with dockerized elastic agents.To run these tests, we are in need of a go-agent that can run these e2e test with Protractor, but we also need an environment where we can serve this front-end and the mocked service.We are doing this with a go-agent that first spins up a docker-compose (for our front-end and mock-end), runs the protractor tests to this new environment and then brings down the environment when tests are finished.Mocking our front-end to test our back-end.    For testing our back-end service, we only need one other service.This service will need to fire REST calls to our back-end service and analyse the response.We can use Postman to set this up.  Postman is the complete toolchain for API developers, used by more than 3 million developers and 30000 companies worldwide.Postman makes working with APIs faster and easier by supporting developers at every stage of their workflow.It’s available for Mac OS X, Windows, Linux and Chrome users.You can use Postman for more than just API testing.In our setup, we need to create a testing scenario and just run it against our back-end service.Postman provides a user interface to do so.However, because we are running our tests on a cloud elastic go-agent, we need to find a way to automate this step.Luckily, Postman also provides a command-line tool called Newman.Newman let’s you run your test scenarios from your command line.You can first configure everything through the user interface and then just export the scenario so you can use it through the CLI.  You can read more about Postman and Newman in our blogpost API Testing with Postman and NewmanBelow, you can find a part of the exported JSON configuration.This part will send a GET request to the search endpoint, providing the search string tim.It will then analyse the response and check if the resulting array contains 2 entries and verifies the data....    {\t\"name\": \"Search existing ccs\",\t\"event\": [\t\t{\t\t\t\"listen\": \"test\",\t\t\t\"script\": {\t\t\t\t\"type\": \"text/javascript\",\t\t\t\t\"exec\": [\t\t\t\t\t\"var jsonData = JSON.parse(responseBody);\",\t\t\t\t\t\"var firstResult = jsonData[0];\",\t\t\t\t\t\"\",\t\t\t\t\t\"tests[\\\"Status code is 200\\\"] = responseCode.code === 200;\",\t\t\t\t\t\"tests[\\\"4 results returned\\\"] = jsonData.length === 2;\",\t\t\t\t\t\"tests[\\\"First result contains id \\\"] ='id' in firstResult;\",\t\t\t\t\t\"tests[\\\"Cl name contains tim vierbergen\\\"] = firstResult.cl === \\\"Tim Vierbergen\\\";\"\t\t\t\t]\t\t\t}\t\t}\t],\t\"request\": {\t\t\"url\": {\t\t\t\"raw\": \":/ccs/search?needle=tim\",\t\t\t\"host\": [\t\t\t\t\"\"\t\t\t],\t\t\t\"port\": \"\",\t\t\t\"path\": [\t\t\t\t\"ccs\",\t\t\t\t\"search\"\t\t\t],\t\t\t\"query\": [\t\t\t\t{\t\t\t\t\t\"key\": \"needle\",\t\t\t\t\t\"value\": \"tim\",\t\t\t\t\t\"equals\": true,\t\t\t\t\t\"description\": \"\"\t\t\t\t}\t\t\t],\t\t\t\"variable\": []\t\t},\t\t\"method\": \"GET\",\t\t\"header\": [\t\t\t{\t\t\t\t\"key\": \"Content-Type\",\t\t\t\t\"value\": \"application/json\",\t\t\t\t\"description\": \"\"\t\t\t}\t\t],\t\t\"body\": {},\t\t\"description\": \"\"\t},\t\"response\": []}...        In our continuous integration system, we are running these tests on a simple go-agent that can run these Newman tests.This agent spins up our service container, runs these tests and bring down that container.Conclusion for our back-endTesting this back-end service with a mock front-end is pretty easy.Since our data is included in this service and it is limited to a simple JSON file, we are not running performance tests.However, when your back-end needs to communicate with a database and/or make calculations, you can and should already include some performance tests in this stage.You can, for example, include some Gatling tests in this stage and put some load on this service to check response times.The goal of this stage is to test the whole component, including the frameworks we are using for the communication.It is still a decoupled system.Failures in this stage will show you that some integrations are failing and you know exactly where to look for the errors.Stage 3: Testing full environment    We now want to end-to-end test the whole system.We can use docker-compose to spin up this environment and then again run tests against the user interface.It will communicate with the real back-end to query its data and show the results in the user interface.Again, we want to run those tests on an elastic go-agent, so we are in need of an automated system.Right, we already used this in the previous stage where we were testing the user interface against a mocked back-end.    This elastic go-agent must be able to run the Protractor tests (obviously). It will first need to spin up this environment, run the tests and tear down the environment. Same goes for performance tests in this stage.You can use some frameworks to put extra load on your front-end to see how it’s behaving when it needs to handle more REST-calls for different parts.Or you can run more instances of the user interface, resulting in more load on the back-end service.ConclusionDecoupling your system and running tests in different stages will make it easier to debug when something is going wrong.Finding errors earlier will also save you some time and resources.Being able to find bugs before taking it to the next stage and spinning up complete environments will reduce the cost of your system (cloud).Yes, writing tests can be more expensive in time and resources in the short run, but it will save you a lot more time and resources in the long run."
      },
    
      "conference-2018-01-03-js-conf-budapest-day-2-html": {
        "title": "JSConf Budapest 2017 Day 2",
        "url": "/conference/2018/01/03/JS-Conf-Budapest-day-2.html",
        "image": "/img/js-conf-budapest-2017/header.png",
        "date": "03 Jan 2018",
        "category": "post, blog post, blog",
        "content": "From JSConf Budapest with loveThis year’s edition of JS Conf Budapest returned to the first venue at Urania National Movie theater.  Uránia Cinema in the middle of the city, near the party-district.Designed by Henrik Schmahl in the late 1890’s, the interior is characterized by the ornamental motifs of the Venetian Gothic and Moor styles.The place is listed as the world’s 3rd most beautiful cinema on Bored Panda.Many tech conferences were hosted here recently, such as TEDx and Strech Conference, because of the unique atmosphere.JS Conf Budapest 2017 is hosted by Glen Maddern and Charlie Gleason.At 10:00 the second day of the conference started. Enough time to drink great coffee and enjoy the breakfast.Day 2: Talks  Don Burks: MVC - What a web app and a Mozart Violin Concerto have in common  Opher Vishnia: Web Animation: from Disney to SASS  Imad Elyafi: Migrating Pinterest profiles to React  Laura Carvajal: YES, your site can (and should) be accessible too. Lessons learned in building FT.com  Nikita Baksalyar: Exploring the P2P world with WebRTC &amp; JavaScript  Vaidehi Joshi: Goldilocks And The Three Code ReviewsDay 2: MorningDon Burks: MVC - What a web app and a Mozart Violin Concerto have in commonYou can find him on Twitter using the handle @don_burks.The presentation can be found here.Composers and developers of applications have more in common than you might think. In his talk, Don Burks pointed out the similarities in structure between music and apps.An app has a certain structure, being the architecture or the configuration of how all the different components interact with each other. The same applies to a composition, where the structure represents the different themes in the composition. When you look at an application, you see a system of databases, web servers, load balancers, etc. All these components are part of the system. Composers have the same vision on their composition.When creating the flow of an application, developers start from a certain point and move forward from one step to another, where each step describes an action or event in the app. These steps are like a melody in music that moves everything forward. Developers also think vertically to visualize the architecture of the application. All the technologies that are used, are represented in a vertical stack.The goal of the application is to deliver a complete package, where the horizontal problem-solving and the vertical architecture should be transparent to the user. In music it is important that every tune is played on time. The musician must deliver the right tune at the right time in order to create the music like the composer intended it to be. An app should run the same way: it has to deliver the right “tune” at the desired time to provide the best user experience.The talk gave us more insight about various aspects in development, seen from a musician’s perspective. Composing music is an art and so is coding.Opher Vishnia: Web Animation: from Disney to SASSYou can find Opher on Twitter using the handle @opherv.The presentation can be found here.Opher is a designer and developer. In his talk, Opher shared his vision on how animations come to live.After we got introduced to Opher’s cute dog named Blizzard, which grew up to a direwolf that can be casted directly in a TV show about dragons and an upcoming winter, Opher started his talk with some child nostalgia from Disney. He shared his amazement about the animations used in old animated movies such as The Lion King, especially since they were hand drawn, and how these animation bring the characters and surroundings to life.Now, how do animations come to life? Two of the influencing factors are Follow Through and Overlapping Actions, which are part of the 12 basic principles of animation. The Follow Through principle defines that not every part of a moving entity moves the same way and stops the same way. Rather when one part of an entity stops, the other parts will follow through and come back. To illustrate this, Opher showed us an animation of a moving carrot that stopped suddenly, causing the leaves to go further before they stopped moving as well.The Overlapping Action principle means that when an entity is moving, its looser parts start moving later. The animation of the moving carrot clearly showed that the carrot itself moved in a smooth way, while the leaves were being dragged behind the carrot.These animation principles are applied by design specifications of huge companies such as the Material Design specs of Google. By taking these principles into account, you can give components and also the flow of your application more realism, for example when responding to events such as user input.Now, how can we implement these animations in our app? Opher discussed three implementations with us by means of an animation he has been working on:  CSS  GSAP, GreenSock Animation Platform  WAAPI, Web Animation APICSSWith the CSS implementation, Opher’s animation performed well across different devices and was directly understood by the browser. However, he stumbled upon the limitations of CSS, which made it tricky to implement complex animations and dynamic animations were even a complete no-go. Besides that, debugging was not a joy.GSAPGSAP provides a great, robust API to implement animations and even dynamic animations. It also deals with inconsistencies of browsers, which makes the life of a developer easier. Unfortunately, there are some downsides too. One of them is that you depend on an extra lib. Additionally, the JavaScript where GSAP is based on is implementation-sensitive and more advanced features of GSAP are not free.WAAPIWAAPI provides a native JavaScript API for animations. Basically you query for the desired elements in JS and call the animation function of those elements where you configure the animation. The animate function accepts two parameters: the keyframes and the duration of the animation. The keyframes should be an array of objects, where each object defines the state of the object at a certain time. The second parameter can also be replaced by an options object which enables you to configure the animation way better, such as adding delays or repeating the animation infinitely. The downside is that WAAPI is not supported by many browsers, but there is nothing a good polyfill cannot fix.So, which one should you use in your next project? As for most situations, this depends on your specific requirements and your expertise with the different implementations. It is recommended to keep these principles in mind when implementing animations in the future. With a little effort you can bring your own app to life, just like the animators of Disney did in their fairy tales.Imad Elyafi: Migrating Pinterest profiles to ReactYou can find Imad on Twitter using the handle @eelyafi.The presentation can be found here.A similar presentation was recorded on PolyConf 2017 and can be found on YouTube.A detailed write-out of that presentation can be found on MediumImad Elyafi is a software engineer at Pinterest. In this talk Imad tells you the story of how Pinterest migrated to React, explaining the techniques they tried and challenges they faced.With the current availability of fantastic modern frameworks, Pinterest decided to migrate from their outdated Denzel framework to React.Why React?Imad started off by saying they had a list of requirements for the new framework.  Large developer community  Design patterns that are compatible with the existing stack to make the migration easier  Isomorphic rendering, therefore being able to reuse templates on server- and client-side  Performance  Developer ExperienceRoad to ReactRewriting the whole app from scratch would be risky and expensive.Also, Pinterest did not want to freeze code and stop shipping new features.So they had to migrate a service that is constantly changing. A very complicated challenge Imad compares to changing the engines of an airplane while mid-flight.The solution was to rewrite the app piece by piece. That resulted in creating a so called hybrid app where two frameworks can exist together for the time it takes to migrate from the old framework to the new one.The very first step they had to take to make this hybrid app was to change their infrastructure and enable server-side JavaScript rendering.Before they used the Jinja templating engine for server-side rendering in Python and the JavaScript equivalent Nunjucks for client-side rendering.By also enabling Nunjucks rendering on a stand-alone NodeJS server, they now achieved pure isomorphic rendering with JavaScript on the server and on the client.Secondly, Pinterest had to render React components in their old Denzel framework. So they added React-specific bindings to Nunjucks’ templating language with a new keyword (component), to represent the “bridge” between Denzel and React.An example of a Nunjucks template with a React-Denzel bridge:{% if in_react %}  {{ component('MyReactComponent', {pinId: '123'}) }}{% else %}  {{ module('MyDenzelComponent', pinId='123') }}{% endif %}Lastly, they had to create adapters for the old data resources. To do so, they used a technique called High-Order Components (HOC).A HOC is a function that takes a component and returns another component.This technique allowed them to compose components with a resource.When the component is added to the DOM, it will create the resource and send a request to the API.A callback function will update its state and trigger the rendering of the given component. With this approach, you can keep your components and data in sync all the time.You can read more about HOCs hereUI experimentsImad explained that they used an A/B testing framework to measure the impact of the migration.By doing this they managed to see consistent performance and engagement improvements, both of these metrics have improved by 20 percent.Last but not least, migrating to React was also great for the developers: less duplicated code, a single language on client and server and a large developer community Imad was definitely happy to be a part of.Laura Carvajal: YES, your site can (and should) be accessible too. Lessons learned in building FT.comYou can find Laura on Twitter using the handle @lc512k.The presentation can be found here.Laura Carvajali works at the Financial Times. She's responsible for the accessibility of their website so that even blind people are able to use it.In her talk, she explained how to achieve this.Accessibility doesn’t happen by accident, you have to make it happen yourself.A good starting point is to install pa11y (with npm).It checks your HTML and points out where you can improve.Color contrast issues, no or bad alt text for images and no related label for input fields are very common issues pa11y reports on.Pa11y-ci can be used to integrate it with your CI and can break the build when there are errors.Next, there are some extra steps that can be taken.The most expensive one is getting an external audit to get more feedback.They have people that test with voice control, keyboard only mode, text to speech and other tools.A cheaper option is to do customer research and user testing with users with various disabilities.Instead of having other people doing the testing, you could learn how to use the tools for people with disabilities yourself.This is of course the cheapest option.A MacBook for example already has a lot of tools built in for people with disabilities!Day 2 afternoonNikita Baksalyar: Exploring the P2P world with WebRTC &amp; JavaScriptYou can find Nikita on Twitter using the handle @nbaksalyar.A similar presentation as the one given at JSConf can be found here.Nikita Baksalyar is a Software Engineer at MaidSafe. During his talk he explained how we could use newer and not so new technologies to decentralize the web to its former state.The web becomes increasingly centralized. We trust our private data to be stored in data centers despite news about data leaks. We exchange our messages and they are handled to three-letter agencies without you knowing about it. Can we do better and return the web to its decentralized roots? A combination of proven and emerging technologies like WebRTC can help us.What is WebRTC?Whenever you visit a webpage, you’d typically enter a web address or click a link to view a page. A request is made to the server and that server provides the webpage you’ve requested. The key here is that you make a HTTP request to a locatable server and get a response back.Let’s say that you want to do a video chat with mom. Mom’s computer is probably not a web-server, so how will she receive my audio and video data? Enter WebRTC.WebRTC stands for web real-time communications. It is a very exciting, powerful, and highly disruptive cutting-edge technology and standard. WebRTC leverages a set of plugin-free APIs that can be used in both desktop and mobile browsers and is progressively becoming supported by all major modern browser vendors.The primary benefit of WebRTC is real-time peer-to-peer audio and video communication. In order to communicate with another person (i.e., peer) via a web browser, each person’s web browser must agree to begin communication, know how to locate one another and transmit all multimedia communications in real-time.Decentralized networksWhen you think of networks you immediately start thinking of network providers, hubs and the likes. We are moving away from the initial idea of the internet, which was supposed to be a decentralized network. Now what is a decentralized network? A good example of a decentralized network is BitCoin. Data is shared over multiple nodes and those nodes get updated by sending update events.The way forwardThe Internet started as a way to have data spread across the world to make sure that in case of a disaster, natural or human made, data would be preserved.Peer 2 Peer communication is key in both a decentralized and the internet of old. We are making moves towards this redecentralization with the power of WebRTC and other more commonly known technologies such as BitTorrent for file sharing, Distributed git for code, etc.Vaidehi Joshi: Goldilocks And The Three Code ReviewsYou can find Vaidehi on Twitter using the handle @vaidehijoshi.The presentation can be found here.A similar presentation was recorded on RailsConf 2017 and can be found on YouTube.A detailed write-out of that presentation can be found on Medium.The original intent behind code reviews was that they would help us take collective ownership in the creation of our software.In other words, we’d each be stakeholders in our development process by having a hand in controlling the quality of our products.While code reviews are generally understood as being a necessity to catch problems at the “lowest-value” stages (the time at which the least investment has been made and at which the cost to correct the problem is the lowest), Vaidehi Joshi asks whether they actually work and, if not, how can we try to improve upon the process.Based on Code Complete by Steve McConnell, she identified 3 major formats of code review:1. InspectionsInspections are longer, deeper code reviews that typically catch about 60% of defects in a program.2. WalkthroughsA walkthrough is shorter and is usually intended to provide teaching opportunities for senior developers to newer programmers, while giving junior developers the chance to change old methodologies.Typically, they catch about 20 to 40% of the defects in a program.3. Short code reviewsShort reviews are faster, but still in-depth. They focus on small changes, including single-line changes, that tend to be the most error-prone.McConnell’s research uncovered the following about shorter code review:An organization that introduced reviews for one-line changes found that its error rate went from 55 percent before reviews to 2 percent afterward.A telecommunications organization in the late 80’s went from 86 percent correct before reviewing code changes to 99.6 percent afterward.But what do developers think of code reviews?To know this, Vaidehi did a survey on Twitter and got about 500 responses.The survey had questions with a scale of 1 to 10, where 1 was strongly disagree and 10 was strongly agree.These are the stats:The quantitive dataThe question “Code reviews are beneficial to my team” had a clear answer.The average score was around 9 for most languages, with the top 3 containing Swift at an average of 9.46, Ruby at an average of 9.19 and JavaScript at an average of 9.1.Another question was “How many pull requests are reviewed”, on which the majority answered that all pull request were reviewed.However, about 10% of the answers indicated that pull requests where only reviewed when someone was explicitly requested to review.The qualitative dataSo, most developers think code reviews are needed and state that all code is being reviewed.But what do they think of the quality of code reviews?Ultimately, what seemed to make or break a code review experience depended upon two things: how much energy was spent during the review process and how much substance the review itself had.A code review was bad (and left a bad taste in the reviewer’s and reviewee’s mouth) if there wasn’t enough energy spent on the review, or if it lacked substance.On the other hand, if a code review process was thorough and time was spent reviewing aspects of the code in a substantive way, it left a much more positive impression overall on both the reviewer and the reviewee.ENERGYOn the question “Who all is doing the review and how much time are they spending on it?”, a lot of things could be learned.  A developer blindly thumbs-up everything or the second or third reviewer is more likely to agree when already seeing an approval.This makes the code review a formality, which doesn’t carry any weight.  A review is performed different depending on who submits.Seniors get no feedback, while juniors are picked to death.The reviews are unfair and can break confidence.  Commits are too big, which cause long review time, which in turn has a bad effect on future branches/PRs/merges.Long review times take too much energy, which causes them to be postponed.SUBSTANCEThe question “What exactly is someone saying, doing, or making another person feel while they review their code?” brought these answers.  An assessor who takes all the feedback for his own account, having a mentality of “see red squiggle, fix red squiggle”.They just change the code without second thought, as long as it makes the reviewer happy.  A reviewer’s comment is not clearly explained.The reviewee just has to change their code to the reviewers vision.  A reviewer is unable to distinguish between stylistic preference and functional difference, which causes nitpicking at syntax.Multiple reviewers might even have conflicting visions.  Words matter, an unkind review might break confidence.How can one do better?A bad code review almost made me leave the company. A great code review leaves me feeling better equipped to tackle future projects.  Use PR templates.Github provides some default templates for a PR, in which a couple of questions need to be answered short and clearly.  Include screenshots/gifs, providing more context on what is changed and why.  Use linters to eliminate style and syntax nitpicking.  Encapsulating PRs into small packages, aiming for small commits.  Assign specific reviewers, so they may provide valuable input and/or teach or learn something.But even more important  Review everyone: it’s a good horse that never stumbles.A senior developer is not infallible and might even be overconfident.  Develop empathy: call out the good stuff, too.Make people feel less vulnerable, push for a culture that values vulnerability — both in actions and in words.  Most importantly, iterate: start a conversation when feeling that the code review flow doesn’t work well.Give everyone the chance to propose their suggestions.This survey answer summarized the importance of the last part perfectly:I love code reviews in theory. In practice, they are only as good as the group that’s responsible for conducting them in the right manner.Afterparty at EXTRA Budapest by EPAMEPAM invited everyone to chill, have some drinks and games at the EXTRA ruinpub after JSConf Budapest. Beer and a selection of soft drinks and juices are on the house.After Movie    Got triggered?All talks were recorded by the JSconf team. You can view them here.Read our full report on day 1 of JS Conf Budapest 2017 here!"
      },
    
      "conference-2018-01-03-js-conf-budapest-day-1-html": {
        "title": "JSConf Budapest 2017 Day 1",
        "url": "/conference/2018/01/03/JS-Conf-Budapest-day-1.html",
        "image": "/img/js-conf-budapest-2017/header.png",
        "date": "03 Jan 2018",
        "category": "post, blog post, blog",
        "content": "From JSConf Budapest with loveThis year’s edition of JS Conf Budapest returned to the first venue at Urania National Movie theater.  Uránia Cinema in the middle of the city, near the party-district.Designed by Henrik Schmahl in the late 1890’s, the interior is characterized by the ornamental motifs of the Venetian Gothic and Moor styles.The place is listed as the world’s 3rd most beautiful cinema on Bored Panda. Many tech conferences were hosted here recently, such as TEDx and Strech Conference, because of the unique atmosphere.JS Conf Budapest 2017 is hosted by Glen Maddern and Charlie Gleason.First thing to do when entering the building was getting our badges.Then we could have breakfast at some standing tables on the first floor.For the coffee lovers, professional baristas served the best coffee possible. With a nice heart drawn on top if it.At 9:45 the conference would officially start so we went to the conference room.Day 1: Talks  Bodil Stokke: You Have Nothing To Lose But Your Chains  Stefan Judis: Watch your back, Browser! You’re being observed  Jonathan Martin: Async patterns to scale your multicore JavaScript… elegantly  Madeleine Neumann: Impostor syndrome - am I suffering enough to talk about it?  Eirik Vullum: JavaScript Metaprogramming - ES6 Proxy Use and Abuse  Sandrina Pereira: How can Javascript improve your CSS mixins  Kacper Sokołowski: You use Content Security Policy, don’t you?  Dan Callahan: Practical WebAssembly  Luke Bonaccorsi: How I ended up automating my curtains and shouting at my laptopDay 1: MorningBodil Stokke: You Have Nothing To Lose But Your ChainsYou can find her on Twitter using the handle @bodil.The presentation she gave can be found at her personal website.A talk about the open source movement and the Free Software movement it grew out ofThe talk started with a story about Richard and a Xerox printer. Richard is a developer suffering from a minorusability flaw in the Xerox printer at his office. Like the good developer he is, he wants to fix the issue and shareit with the world for everybody’s benefit. Therefore, he needs access to the code. However, it turns out that Xerox’ code for that particular printer is not publicly available. So, Richard can’t fix the issue. He will have to live with the inconvenience, as well as everyone at the office and even everyone using that same printer. The clue here is that a minor fix has to wait until someone at Xerox finds the time to solve the issue. Considering the minor status of the issue, it’s not even likely to happen… ever. With open source software this fix could be done by a motivated user in a few moments.This little intro sets the mood for the talk. One can consider it a bit opinionated, but there are with no doubt some powerful arguments for open source software. The talk also covers the free software movement that all started it and from which the open source movement branched of. The difference though is just in its philosophy. First of all, a commonmisunderstanding is that free software does not mean one can get it with zero cost. It says that anyone can get the code and is free to do with it as pleased. Modify, change, sell or use it for another purpose. On the other hand, open source software provides some restrictions. Therefore, open source software is more popular and used more widely, because it gives control.For example: a concurring company might purchase your proprietary software and then have access to the code. They could copy your product and sell it for a lower price. This can be done with Free software, but open source software has some licenses defined to prevent this. In the talk some of these licenses are covered. I took the liberty to list some of them here with a short explanation. Click trough to see how easily it is to use them and have a legal basis to rely on.The most popular and widely used licenses are:Apache License 2.0Designed to be simple and easy to comply with, but more extensive than the previous versions. One can use the licensed software for any purpose, to change and redistribute. Changes can be distributed under other licenses, but unchanged code needs to be distributed under the same license.3-clause BSD licenseDesigned to be simple and short. It allows unlimited redistribution for any purpose as long as its copyright notices and the license’s disclaimers of warranty are maintained. The license also contains a clause restricting use of the names of contributors for endorsement of a derived work without specific permission. In the 2-clause version that restriction is left out.GNU General Public LicenseSoftware under GNU GPL is free (as in: do with it as you please). The main restrictions defined by this license are thatyou should always mention the authors of the software and it must always stay under the GNU GPL license.MIT LicenseCreated by Massachusetts Institute of Technology. It has one simple rule: the copyright statement should stay in anycopy of the software. ‘Copyright (c) &lt;year&gt; &lt;copyright holders&gt;’Mozilla Public License 2.0Code under MPL can be copied or changed, but must stay under MPL. The code can be combined with closed source files.Open source should be considered by many companies, since many can benefit from open source. SpaceX for example benefits from open source software, non the less their own code is closed. Another company by the same founder, Elon Musk, has its code publicly available: Tesla, the electric car manufacturer. Here’s a part I found on  Tesla’s own blog. At Tesla we felt compelled to create patents out of concern that the big car companies would copy our technology and  then use their massive manufacturing, sales and marketing power to overwhelm Tesla. We couldn’t have been more wrong.  The unfortunate reality is the opposite: electric car programs (or programs for any vehicle that doesn’t burn  hydrocarbons) at the major manufacturers are small to non-existent, constituting an average of far less than 1% of  their total vehicle sales. While the competition might benefit from sharing your code, so does the world. This counts for Tesla in particular. While there might be a huge market for electric vehicles, we also need them as fast a possible. Open source software can help us achieve that goal.Stefan Judis: Watch your back, Browser! You’re being observedYou can find Stefan on Twitter using the handle @stefanjudis.The presentation can be found on speakerdeck.To get information from a browser, you always had to do a pull. However, it's now also possible to ask the browser to push this information to you when something has changed by using observables!Verifying whether an element has become visible in the viewport is a very common use case. If you have to pull that information from the browser, it’s also a very heavy one since the piece of code doing that verification, is run each time a scroll event is fired. A better way would be to have the browser letting us know when an element has reached the viewport. Therefore, browsers offer a so called IntersectionObserver through JavaScript. When creating an IntersectionObserver you can pass it a callback function which will be fired when the observed elements enter or leave the viewport. Optionally you can also pass some options such as how much of the element should become visible/hidden in the viewport.Unfortunately Safari doesn’t support this feature yet, but luckily, it’s polyfillable.There are several more observers such as:  MutationObserver - fires when an attribute of an observed element has changed (supported by all major browsers)  ResizeObserver - fires when an element is resized (behind a flag in Chrome, not yet supported in other major browsers)  PerformanceObserver - emits metrics about the performance of the web page (e.g. time to paint, mark statements, navigation time…) (supported by all major browsers except Edge)Another great benefit of these observers is that all functions RxJS offers us (e.g. skip, pairwise, filter …), can be used as well!The emitted values of the observers are collections so we can use functions such as map, filter and reduce there as well.As mentioned in the presentation, these two combined gives us “Collection super powers!”.Jonathan Martin: Async patterns to scale your multicore JavaScript… elegantlyYou can find Jonathan on Twitter using the handle @nybblr.The presentation can be found on speakerdeck.“JavaScript is single-threaded, so it doesn’t scale. JavaScript is a toy language because it doesn’t support multithreading.”Outside (and inside) the web community, statements like these are common.In a way, it’s true: JavaScript’s event loop means your program does one thing at a time.This intentional design decision shields us from an entire class of multithreading woes,but it has also birthed the misconception that JavaScript can’t handle concurrency.In fact, JavaScript’s design is well-suited for solving a plethora of concurrency problemswithout succumbing to the “gotchas” of other multithreaded languages. You might say that JavaScript is single-threaded…just so it can be multithreaded!Before diving into solving concurrency problems, Jonathan explained how the (V8) JavaScript runtime actually works and reacts under the hood.Next, he told us how the call stack, event loop WebAPIs and the callback queue works and how it handles synchronous (blocking) and asynchronous (non-blocking) code.Explaining that would be an entire blog post on its own. Luckily he gave us a great link to a video that explains it very clearly, so I’ll add that instead.Philip Roberts: Help, I&#039;m stuck in an event-loop.What is concurrency, multi threading and parallelismSo if you’ve just watched the video above, you know that JavaScript has one call stack (a single thread) and executes the functions in sequence.With multithreading, as the word says, we have multiple threads.This means that the program can assign these tasks to multiple stacks so that multiple tasks get executed at the same time.In a computer with a single processor and single core, to do multi threading,the processor would alternate between these tasks really fast so that they appear to be happening at the same time.Back in the early days of computing, this was the only option we had. This is called concurrency.Around 2005 Intel, AMD and the other chip manufacturers started creating processors with multiple cores. This meant it could actually do multiple things at the same time, since it had multiple “brains”.Processors could now assign different tasks to different cores and they would run at the same time. This is what we call parallelismJavaScript multi threading: impossible?Although your JavaScript code is single-threaded and only does one thing at a time, the JavaScript Runtime and Web APIs are multithreaded!When you pass a callback function to setTimeout() or start an AJAX request with fetch(),you are essentially spinning up a background thread in the runtime. Once that background thread completes and the current call stack finishes executing, your callback function is pushed onto the (now empty) call stack and run-to-completion.So your JavaScript code itself is single-threaded, but it orchestrates legions of threads!ES2017 async functionsThe title of his talk contained the word ‘Elegant’ and this is where the ES2017 async/await functionality comes in.This is a great alternative for dealing with promises in JavaScript. If you’re a JavaScript developer you probably know what ‘callback hell’ is, or at least heard of it.When writing complex programs, we could find ourselves in a situation where we would have to create multiple nested Promises to make sure we have the results of one call to continue with the next and so on.Async - declares an asynchronous function (async function someName(){...}).  Automatically transforms a regular function into a Promise.  When called, async functions resolve to whatever is returned in their body.  Async functions enable the use of await.Await - pauses the execution of async functions. (var result = await someAsyncCall();).  When placed in front of a Promise call, await forces the rest of the code to wait until that Promise finishes and returns a result.  Await works only with Promises, it does not work with callbacks.  Await can only be used inside async functions.// Promise approachfunction getJSON(){    // To make the function blocking we manually create a Promise.    return new Promise( function(resolve) {        request.get('https://myurl.com/example.json')            .then( function(json) {                // The data from the request is available in a .then block                // We return the result using resolve.                resolve(json);            });    });}// Async/Await approach// The async keyword will automatically create a new Promise and return it.async function getJSONAsync(){    // The await keyword saves us from having to write a .then() block.    let json = await request.get('https://myurl.com/example.json');    // The result of the GET request is available in the json variable.    // We return it just like in a regular synchronous function.    return json;}If you’re a beginner with async functions and want to learn more this topic, check out this videoFor further reading on how Jonathan used async patterns for multicore JavaScript, he has written an elaborate blog post about it. We suggest you go check it out!Madeleine Neumann: Impostor syndrome - am I suffering enough to talk about it?Madeleine is a front-end developer at 9Elements. She’s also a conference organiser of RuhrJS.You can find Madeleine on Twitter using the handle @maggysche.The presentation can be found on slideshare.The reason we struggle with insecurity is because we compare our behind the scenes with everyone else’s highlight reel.Madeleine wanted to share her life experience with us. While she attended secondary school, Madeleine was the creepy loner. ‘What’s wrong with me?’, ‘What did I do wrong?’ she asked herself on several occasions. ‘My behaviour must be wrong, I have to change’. So she decided to take up programming in high school and felt truly belonged.After Madeleine graduated high school, she started to work as a front-end developer where she was learning a lot, very quickly! However, she soon discovered that the speed at which she was learning gradually stagnated. She had mixed feelings about her profession and abilities, thinking she did not belong there and had no idea what she was doing.So, she decided to work even harder. All of her friends and colleagues congratulated her for her effort and hard work, but Madeleine still wasn’t satisfied. Shortly after, she learned about “the imposter syndrome”.Here are some common signs that someone might experience, where one feels like an imposter:  Does everyone overestimate you?  Do you tend to discuss yourself?  Do you compare your ability to those around you and think they’re more intelligent than you are?  Does the fear of failure freak you out?  Sometimes you’re afraid others will discover how much knowledge you really lack?  You can’t understand the compliments you receive?  You feel bad when you’re not ‘the best’ or at least ‘very special’?  You avoid evaluations if possible and have a dread of others evaluating you?  Do you focus more on what you haven’t done?Madeleine discovered that her answer to all the previous questions was ‘yes’ and came to the conclusion she sabotaged herself. Now, how do you escape the ‘imposter zone’?  You aren’t born to live a life of another person  Learn to be a healthy perfectionist  Answer on the following question ‘What would I do, if I was not afraid?’  Ask for help  Mentor people what you’re doing  It’s a good thing to know, what you don’t know  Talk about it  Bribe your friends  Being wrong doesn’t make you a fraud  Focus on providing value and write it down  Keep a file of nice things someone has said about you  Stop commenting compliments  And finally, take time for yourselfMadeleine learned that sometimes, it’s not that bad to be an imposter. Because if you are an imposter, you are an overachiever and you can surprise people with your talent.Day 1 afternoonEirik Vullum: JavaScript Metaprogramming - ES6 Proxy Use and AbuseYou can find Eirik on Twitter using the handle @eiriklv.The presentation can be found here.This very interesting talk handles metaprogramming in JavaScript. Recently a new feature in ES6 was added to all major browsers, making JavaScript even more exciting! First of all, what is metaprogramming? According to wikipedia: ‘The ability to read, generate, analyse or transform other programs, and even modify itself while running’. That is clear enough in my opinion.In metaprogramming one can define 2 branches. The first branch could be described as macros to extend your language. This happens during compile/transpile time. The second branch is called reflection and happens at runtime. There are three forms of reflection:  Introspection: the ability to examine itself  Self-modification: the ability to alter its structure  Intercession: the ability to alter its behaviourIn JavaScript they are possible by default. Lets call it a perk of this beautiful scripting language. However, it seldom results in readable code and you’ll probably need to write a lot of code for something we can now achieve in a much easier way.This talk covers some of the possibilities of proxies. Proxies couldn’t be used until recently, because it isn’tpolyfillable. It’s a feature that needs to be supported by the engine, where reflection truly happens. Therefore, nor typescript, nor babel, nor any other JavaScript preprocessor could solve that for you. By the way, preprocessors extend your language through macros, since their magic happens at transpile time.So what is this proxy I am so exited about? It’s called a proxy after the same principle we use in networking.A proxy is middleware that intercepts interaction with an interface. Therefore, it has access to the communication stream and it needs access to the interface it’s routing to. That’s very similar to how we can use proxies in JavaScript. We can wrap any object with a proxy and define a handler. That handler is an object containing traps. A trap is a function that ‘mocks’ a property or function from the object that is being proxied. The proxy then knows which actions will be performed (before they are actually performed) and can choose how to handle them. It could do something totally different or even nothing at all.let handler = {    set: (object, prop, value) =&gt; {        // do what you desire (alter the value for example);        object[prop] = value; // this wil execute the default setter        return true; // to indicate success    },        get: (object, prop) =&gt; {        let value = object[prop]; // this wil execute the default getter        // do what you desire        return value;    }};let mySquare = new Square(10,10);let myProxySquare = new Proxy(mySquare, handler);The above handler will intercept all get and set calls to a proxied class. get and set methods here are so called traps. For what purposes can we use this? One of the main purposes is to create developer friendly interfaces. In the slides you’ll find some nice examples of great uses. My favorite is the url builder, it’s glorious in its simplicity, check it out here.Now to wrap it all up, proxies are awesome, we can create powerful abstractions, be lazy and write less code and addfunctionality in a transparent way. Even though it might seem like magic for anyone else than yourself and despite a small performance cost, it’s still perfect if you want to create clean interfaces for others to enjoy.Sandrina Pereira: How can Javascript improve your CSS mixinsSandrina is UI Developer at Farfetch. You can find Sandrina on Twitter using the handle @a_sandrina_p.All code can be found on her GitHub page and the slides here.To write good tests, you have to know exactly what you need to do. When you know what to do, you do less.When you do less, you can do better!CSS and JavaScript work together more than ever these days. Using the good parts of both worlds ensures us that we can get better in web development. One of the reasons is because CSS primarily doesn’t have logic behind it. It’s simple and straightforward. However, when you have to start using logic in your CSS, you can for example add a loop with SCSS.When you find yourself reusing the same CSS code over and over, you can write a mixin.However, at the end of the day, things can get ugly. Therefore, many programmers use PostCSS to write logic in their CSS code. There are more than half a million downloads per month of PostCSS plugins!Here’s how you write a mixin in CSS:// index.css @define-mixin size $value { \twidth: $value;\theight: $value;}.avatar { \t@mixin size 20px;}This is how it works in JavaScript:// size.jsmodule.exports = (mixinNode, value) =&gt; ({ \twidth: value,\theight: value,})// postcss.config.jsmodule.exports = {    // ...   \tplugins: [\t\trequire('postcss-mixins')({ \t\t\tmixindsDir: '../src/mixins/',\t\t}, \t\t// ... \t]}// index.css.avatar {\t@mixin size 20px;}Now, we can’t test logic in CSS, but in JavaScript we can!// size.test.jsimport size from '../src/mixins/size.js';test('Size returns width and height', () =&gt; {  expect(size(null, '24px').toEqual({      width: '24px',      height: '24px'    });});So you started to use CSS mixins with JavaScript and ended up with a folder full of mixins to improve your CSS. Instead of using a series of mixins in the CSS file itself that only improve your project, we can create a custom property with the PostCSS plugin called ‘Boilerplate’.Using that, we can do the following:// index.css.avatar {  size: 20px;}// index.jsconst postcss = require('postcss');postcss.plugin('postcss-size', () =&gt; css =&gt; {  // let’s transform CSS with JS  css.walkDecls('size', decl =&gt; {      // 1. get the size value       const value = decl.value;      // 2. add “width” &amp; “height” properties      decl.cloneBefore({ prop: 'width', value });      decl.cloneBefore({ prop: 'height', value });      // 3. remove “size” property       decl.remove();  });});// index.test.jsconst plugin = require('./index.js');const postcss = require('postcss');function run(input, output) { ... };test('Sets width and height', () =&gt; {  return run(    '.foo { size: 1px; }',    '.foo { width: 1px; height: 1px; }'  );});After you execute the command npm publish in the console, you aren’t only going to improve your own project, but everyone’s projects.You can find other popular PostCSS plugins here.That’s why I came here today. To share something that improved my project and might improve yours as well. I believe sharing is what make us better.Kacper Sokołowski: You use Content Security Policy, don’t you?Kacper is a front-end developer for Codewise.He’s also a speaker and community organiser for KrakowJS.You can find Kacper on Twitter using the handle @kaapa_sThe presentation can be found hereEveryone knows that security is important right?The biggest companies like Facebook and Google spend tons of money on bug bounty programs to ensure that their products are secure.But is there a way that we can make our website prone to some of the most popular attacks?There is one security mechanism which can help, but yet not everyone knows and uses it.It’s called Content Security Policy.Kacper started his presentation with an example to demonstrate why security is hard.In 2005, Kamkar released the Samy worm, the first self-propagating cross-site scripting worm, onto MySpace.The worm carried a payload that would display the string \"but most of all, Samy is my hero\" on a victim's profile and cause the victim to unknowingly send a friend request to Kamkar.When a user viewed that profile, they would have the payload planted on their page.Within just 20 hours of its October 4, 2005 release, over one million users had run the payload, making it the fastest spreading virus of all time.XSSCross Site Scripting (XSS) was used to inject and spread the virus.It’s a technique to inject and execute any JavaScript code in the context of the page.What can you do with XSS?  Steal cookies  Steal localstorage data  Break the layout and style of the page  Whatever you can do with JavaScript…You can find a lot of information about XSS and other vulnerabilities on this website: https://www.owasp.orgHOW TO BE SAFE?!CSPContent Security Policy (CSP) is an added layer of security that helps to detect and mitigate certain types of attacks, including Cross Site Scripting (XSS) and data injection attacks.Inline code is considered harmful so don’t use something like this:&lt;script&gt;alert('hello JSConfBP!');...&lt;/script&gt;Instead externalise your code and do something like this:&lt;script src=\"...\"&gt;&lt;/script&gt;HTTP HEADERSWhen you have externalised your scripts, you need to make sure your site only loads these scripts.To enable CSP, you need to configure your web server to return the Content-Security-Policy HTTP header.Specifying your policy:Content-Security-Policy: script-src ‘self’ http://google.com …Specifying your directive(s):Content-Security-Policy: script-src ‘self’ http://google.com …Specifying the URL list:Content-Security-Policy: script-src ‘self’ http://google.com …Other directives you can use:  connect-src  img-src  script-src  style-src  …You can use the fallback directive for other resource types that don’t have policies of their own: default-srcConclusionMany parts of your website will probably break when you CSP for the first time.So, start using it as early as possible!Dan Callahan: Practical WebAssemblyYou can find Dan on Twitter using the handle @callahad.In this talk Dan explained what WebAssembly is all about. How it works, what it's for, the features that are already there and which features are yet to come.WebAssembly, what is it?Well, according to http://webassembly.org/:\"WebAssembly or wasm is a new portable, size- and load-time-efficient format suitable for compilation to the web.\"A compiler for the web:  Low-level, binary format for programs:  WebAssembly is a fast, portable, compact, cross-browser binary format for compilation to the web.  It’s an open standard supported by all major browsers. caniuse.com  Direct successor of asm.js  General purpose virtual architecture  It allows new types of applications and heavy 3D games to run efficiently in browsers.Why?Performance!WebAssembly is a binary format for JS.It has 2 major benefits:  The JS engine can skip the parsing step  It’s much more compact than the JS original sourcePortabilityAt the moment of writing this blog, there are two languages that can compile into wasm, those are C/C++ and Rust.This is great for portability since code written in C works on Mac, Linux and Windows.Is JavaScript dead?JavaScript is alive, but its client-side monopoly is dead.WebAssembly doesn’t replace JavaScript, but does expand the web and complements JavaScript:  High Level (JS) vs. Low Level (WASM)  Text (JS) vs. Binary (WASM)Unity SupportWhen it comes to creating 3D games, Unity also has experimental support for WebAssembly.Check out this demo of an in browser gameUnreal EngineThis is a video of Epic’s “Zen Garden” demo running in Firefox.The demo is built with WebAssembly and WebGL 2, both emerging standards that enable amazing video games and applications in the browser.    What about older browsersUse asm.js as a fallback.When using Binaryen with Emscripten, it can load the compiled code using one of several methods.By setting -s BINARYEN_METHOD='..' you can specify those methods, as a comma-separated list. It will try them one by one, which allows fallbacks.By default, it will try native support. The full list of methods is:  native-wasm: Use native binary wasm support in the browser.  interpret-s-expr: Load a .wast, which contains wasm in s-expression format and interpret it.  nterpret-binary: Load a .wasm, which contains wasm in binary format and interpret it.  interpret-asm2wasm: Load .asm.js, compile to wasm on the fly and interpret that.  asmjs: Load .asm.js and just run it, no wasm. Useful for comparisons or as a fallback for browsers without WebAssembly support.Can I compile JS to WASM?Don’t do that!Browsers will still have native JavaScript VM along-side wasm.There’s no reason to compile JS to wasm because you would also have to include a whole JavaScript VM.The resulting code would be huge and slower than the JS VM natively provided.Interesting Links:            Here you can translate C/C++ to WebAssembly and see the machine code generated by the browser.                YouTube video on what WebAssembly means for React        Tanks demo Unity game on webassembly.orgLuke Bonaccorsi: How I ended up automating my curtains and shouting at my laptopYou can find Luke on Twitter using the handle @lukeb_uk.The presentation can be found hereBeing lazy can lead to some great out of the box thinking and finding innovative solutions for common everyday stuff.Luke talks about how he created a chatbot that automates things for him to make them a bit less common and/or boring.WoodhouseHe would have named it Jarvis, but since this would be a far worse butler than Jarvis was, he named it Woodhouse after the butler character from the tv-series 'Archer'.Around mid-2014 he started working on a chatbot that does little bits in his house. Basically, he put together a Raspberry Pi running JavaScript code that actually serves as a router with some core functionality built in like:  Broadcasting  Preference storage  SchedulingThere’s two types of modules that make it up:  Plugins do all the heavy-lifting so you can interface with hardware (as long as it’s possible with JavaScript or the node ecosystem) or get it to send a message. You could for example let it connect to API’s to get it to do your builds on your CI tools.  The interfaces which are basically chat systems. They are the way to talk to the chat bots. If the system has a way for you to build stuff for it in JavaScript, you can connect to it and let it do stuff on for example: Facebook, Slack, HipChat and many more…Open sourceAll of it is open source (MIT) and is avaiable on GitHub. It’s written in JavaScript and runs on NodeJs.Automating Lamps    Sending a message in a chat application that gets picked up by Woodhouse and he/it then turns on his lamps at home.So, as he walks down the street getting to his house, instead of coming home and stumbling over things searching for the light switch in the dark,he can just send a message and the lights will be on when he gets there.Lamp plugs (they are from China, so super safe, right?). Maybe not, but they cost about £15 and are great for poking around.After doing so, he found out that there was a Google group that had been hacking around with them and found the SSH password for it.It turned out it runs OpenWrt which is a router firmware based on Linux.So, after being able to SSH into it and work with the Linux installed on it, you can run basic scripts on it (it has limited memory so you can’t just install everything you like on it).But most importantly, it’s got a web server built into it, so you can hit an endpoint and make the relay turn on. That’s how his relay works.There’s an endpoint on the plug and when he goes to that endpoint, it switches it on or off depending on a parameter.Automating the curtainsFor giving talks about the application, he wanted to add something new to the application and so… he automated his curtains.    The setup and parts for it are very basic and simple. It’s basically some string, plastic wheels, a servo and an esp82266.The esp8266 is a wireless Arduino type board, but the cheap Chinese version so you can buy loads of them and connect themto your network. So for about £2 each you can control stuff over your network from anywhere. It runs Mongoose OSwhich lets you write JavaScript on your hardware, it takes away a lot of the complexities of the lower level code and lets you use a language you know.Shouting at his laptopNot out of frustration or anything like that. Besides the chatbot, he wanted to add voice control to the application so that he could tell his laptop to open/close the curtains or turn on/off the lights.    It uses a NodeJs library for offline ‘hot words’ detection. So instead of having it constantly listening to him,he can just shout ‘Woodhouse’ which will make it reply to say that it’s listening. The rest of the complex speech to text is done by Google,since they have a lot more data than him. There are open source systems for doing speech to text, but you would have to train it yourself and well, we’re doing all this because we want to be lazy…So he created a few of these voice control units and spread them around the house and let them connect to one central instance. So he can activate it from wherever he is in the house.ConclusionSo instead of being lazy, he admits to being the stupid kind of lazy. He has spent about hundreds of hours coding for it to do simple stuff.So it’s not really about being lazy, but more being not driven to do those simple things.Party feat Live:JS by SINNERSCHRADERAfter a long day of JavaScript fun we were invited to a rooftop party at Corvinteto located near the venue.Imagine a party with awesome visuals, music &amp; beats, and lights - all powered and created by JavaScript!More info about the concept can be found here: LiveJS    Day 1: ConclusionAt the first day of the conference we were already inspired by some good talks. Wondering what day 2 would bring.Read our full report on day 2 of JS Conf Budapest 2017 here!"
      },
    
      "microservices-2017-12-30-secure-your-architecture-part2-html": {
        "title": "Securing your cloud-native microservice architecture in Spring: part 2",
        "url": "/microservices/2017/12/30/Secure-your-architecture-part2.html",
        "image": "/img/microservices/part2/part2logo.jpg",
        "date": "30 Dec 2017",
        "category": "post, blog post, blog",
        "content": "Since the rise of the digital era, most enterprises keep their data in a digital format.But if their sensitive data lacks security, it can cause the data to be unreliable, unstable and unavailable to their business.We have to be prepared if an attacker breaches into our network and tries to hack our sensitive data.Whether it is in motion or at rest, encrypting our data and using the proper protection mechanisms will make it worthless for the hacker to use.Overview  Securing your cloud-native microservice architechture in Spring: Part 1  Cryptographic Algorithms  Key Mechanics  Cloud-hosted Key management service  Spring Cloud Config ServerCryptographic AlgorithmsWhen implementing our application, every programming language will provide us with a set of known libraries for cryptographic algorithms. A big flaw is implementing an algorithm by yourself, the known algorithms have been reviewed, patched and been known for their excellent security. These are the most used types that you can use for encryption at rest:Symmetric EncryptionThe key used in encrypting data at rest is used for both encrypting and decrypting the data.This key becomes very vulnerable if anyone gets a hold on it.  Well known: Advanced Encryption Standard encryptionAsymmetric EncryptionIn asymmetric encryption, a pair of keys are used. A public key that is exposed and encrypts your data and a private key that is only known by the owner that decrypts your data.This key-pair can also be used to sign your data, so the application knows that it can trust the source of the data.  Well known:  Rivest–Shamir–Adleman encryptionKey MechanicsEncryption keys are another aspect of encryption, handling the keys becomes just as sensitive as the data itself. That’s why we need mechanisms on how keys are stored and shared so attackers can’t get a hold on them.Key rotationEncryption key rotation will provide protection especially when the certificate expires, is corrupted or the key management admin is no longer part of the company. Lets say, you got a good eye at detecting patterns and detect that the same key is being used for encrypting data.To avoid this, you rotate your keys, and every time the same data field is encrypted it will result in a different encrypted message.JSON Web Key (Set)We discussed in the previous post about retrieving a JWK(S) to verify our JSON Web Token in our microservice. A JWK is a JSON object that represents a cryptographic key that consists of information to verify a JWT. If you like to dive into signing JSON documents you can check out this blog post on Digitally signing your JSON documents.JWKS example:{\"keys\": [  {    \"alg\": \"RS256\",    \"kty\": \"RSA\",    \"use\": \"sig\",    \"x5c\": [      \"MIIC+DCCAeCgAwIBAgIJBIGjYW6hFpn2MA0GCSqGSIb3DQEBBQUAMCMxITAfBgNVBAMTGGN1c3RvbWVyLWRlbW9zLmF1dGgwLmNvbTAeFw0xNjExMjIyMjIyMDVaFw0zMDA4MDEyMjIyMDVaMCMxITAfBgNVBAMTGGN1c3RvbWVyLWRlbW9zLmF1dGgwLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMnjZc5bm/eGIHq09N9HKHahM7Y31P0ul+A2wwP4lSpIwFrWHzxw88/7Dwk9QMc+orGXX95R6av4GF+Es/nG3uK45ooMVMa/hYCh0Mtx3gnSuoTavQEkLzCvSwTqVwzZ+5noukWVqJuMKNwjL77GNcPLY7Xy2/skMCT5bR8UoWaufooQvYq6SyPcRAU4BtdquZRiBT4U5f+4pwNTxSvey7ki50yc1tG49Per/0zA4O6Tlpv8x7Red6m1bCNHt7+Z5nSl3RX/QYyAEUX1a28VcYmR41Osy+o2OUCXYdUAphDaHo4/8rbKTJhlu8jEcc1KoMXAKjgaVZtG/v5ltx6AXY0CAwEAAaMvMC0wDAYDVR0TBAUwAwEB/zAdBgNVHQ4EFgQUQxFG602h1cG+pnyvJoy9pGJJoCswDQYJKoZIhvcNAQEFBQADggEBAGvtCbzGNBUJPLICth3mLsX0Z4z8T8iu4tyoiuAshP/Ry/ZBnFnXmhD8vwgMZ2lTgUWwlrvlgN+fAtYKnwFO2G3BOCFw96Nm8So9sjTda9CCZ3dhoH57F/hVMBB0K6xhklAc0b5ZxUpCIN92v/w+xZoz1XQBHe8ZbRHaP1HpRM4M7DJk2G5cgUCyu3UBvYS41sHvzrxQ3z7vIePRA4WF4bEkfX12gvny0RsPkrbVMXX1Rj9t6V7QXrbPYBAO+43JvDGYawxYVvLhz+BJ45x50GFQmHszfY3BR9TPK8xmMmQwtIvLu1PMttNCs7niCYkSiUv2sc2mlq1i3IashGkkgmo=\"    ],    \"n\": \"yeNlzlub94YgerT030codqEztjfU_S6X4DbDA_iVKkjAWtYfPHDzz_sPCT1Axz6isZdf3lHpq_gYX4Sz-cbe4rjmigxUxr-FgKHQy3HeCdK6hNq9ASQvMK9LBOpXDNn7mei6RZWom4wo3CMvvsY1w8tjtfLb-yQwJPltHxShZq5-ihC9irpLI9xEBTgG12q5lGIFPhTl_7inA1PFK97LuSLnTJzW0bj096v_TMDg7pOWm_zHtF53qbVsI0e3v5nmdKXdFf9BjIARRfVrbxVxiZHjU6zL6jY5QJdh1QCmENoejj_ytspMmGW7yMRxzUqgxcAqOBpVm0b-_mW3HoBdjQ\",    \"e\": \"AQAB\",    \"kid\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\",    \"x5t\": \"NjVBRjY5MDlCMUIwNzU4RTA2QzZFMDQ4QzQ2MDAyQjVDNjk1RTM2Qg\"  }]}Explanation properties:  alg: is the algorithm for the key  kty: is the key type  use: is how the key was meant to be used. For the example above sig represents signature.  x5c: is the x.509 certificate chain  e: is the exponent for a standard pem  n: is the modulus for a standard pem  kid: is the unique identifier for the key  x5t: is the thumbprint of the x.509 cert (SHA-1 thumbprint)Cloud-hosted Key management serviceKMS is a fully managed service that allows you to manage your encryption keys in the cloud.Most of these KMSs offer the best way for encryption and generate, rotate and destroy your keys. But the KMS is vendor lock-in so all your keys will stay on the platform.To avoid vendor lock-in, we can implement our own open source version for managing our encryption keys.A few examples to get an idea of KMS:  Google Cloud KMS  AWS KMS  Azure VaultA few examples of open-source variants:  Spring Cloud Config Server  HashiCorp’s Vault  KeywhizSpring Cloud Config ServerThe Spring Cloud Config Server provides a centralized external configuration management backed optionally by a Git repository or database.Using a REST API for external configuration, Config Server supports encryption and decryption of properties and yml files. First step is downloading the Java Cryptography Extension on our local pc.  JCE provides a framework and implementation for encryption, key generation, key agreement and message authentication code algorithms. You’re not installing JCE itself, because it’s packaged within the Java SE binary.However, you do need to update its policy files from time to time.Downloads are available for Java 6, 7 and 8.This will allow the config server to use the encryption tool of the JCE.After the download, the next step will be securing the config server by adding Spring Security to the classpath and configuring your Basic/OAuth2 authentication.&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt;Key ManagementThe config server supports encryption and decryption with a symmetric key or an asymmetric key-pair.The choice of which key you will need is within your security terms. The symmetric key is the easiest way to set up but less secure than the asymmetric one. To set up a symmetric key, you just assign a string to the key holder: encrypt.key=&lt;key&gt;To configure these asymmetric keys, we will need a keystore created by the keytool utility from the JDK.The public key will encrypt and the private key will decrypt your data.To create a keystore you can do something like this in your command line:$&gt; keytool -genkeypair -alias &lt;keyname&gt; -keyalg RSA -keysize 4096 -sigalg SHA512withRSA \\  -dname \"CN=Config Server,OU=JWorks,O=Ordina,L=Mechelen,S=State,C=BE\" \\  -keypass &lt;secret&gt; -keystore config-server.jks -storepass &lt;password&gt;This will generate a keystore for the config server to use. Place it in your repository project and configure it in your yml. Warning: Be aware if you package your keystore within your application jar/war file, the same encryption keys will be used across all of your environments!Example yml in the config server:encrypt:  key-store:    location: classpath:config-server.jks // resource location    password: &lt;password&gt; // to unlock the keystore    alias: config-server-key // to identify which key in the store is to be used    secret: &lt;secret&gt;EncryptionTo encrypt the data, start up your config server locally and enter this in your command line.$&gt; SECRET=$(curl -X POST --data-urlencode secret http://user:password@localhost:&lt;port&gt;/encrypt)$&gt; echo \"datasource.password=$SECRET\" &gt;&gt; application-dev.ymlWhen the encryption is done, we get an encrypted piece of data in your configuration in the form of:spring:  datasource:    username: dbuser    password: '{cipher}FKSAJDFGYOS8F7GLHAKERGFHLSAJ'What to store where?When designing your config server, you have different options on where and to which our config server has access.Using a Git repositoryThe default and most common way most of us use is via private Git repositories where we store our sensitive data where the config server can fetch it.Be aware, never put configuration inside your code repository, it violates the twelve-factor app which requires strict separation of config from code. Config varies substantially across deploys, code does not.Health checksYou can enable the health check to the config server within the application.If you do this, always look at which version control would be the best fit, always check when they go into maintenance. It could be that they host it in another timezone, which could lead to a cascading failure.In my opinion, you can just disable the health checks with spring.cloud.config.server.health.enabled=false and avoid further failures. If you expect that the config server might go down temporarily when your client app starts, please provide a retry mechanism after a failure. To enable a retry, first add spring-retry to your classpath with @EnableRetry annotation and spring.cloud.config.failFast=trueUsing JDBCNew to this list is the support for JDBC. This enables us to store configuration properties inside a relational database. By switching the active spring profile to JDBC and adding the dependency of spring-jdbc to your classpath, Spring Boot will configure the datasource you included on the classpath.To store the data you will need to set up new tables in your database.For more information: using JDBCUsing VaultHashiCorp’s Vault provides a centralized external management server. Vault can manage static and dynamic secrets such as username/password for remote applications/resources and provide credentials for external services such as MySQL, PostgreSQL, Apache Cassandra, MongoDB, Consul, AWS and more.Spring supports using the Vault as a backend for Spring Cloud Config.If you are using Spring Boot, a quick way to enable Vault is to set your spring profile to vault. Spring Boot’s conditionals will activate all the auto configuration for a connection with the Vault server.Using File SystemSo when you’re working locally on your machine, you can always look at the native profile to activate the file system as your “backend”.But I don’t recommend it for use in a deployment environment since it comes with various problems and extra setup.One of those problems would be high availability, unlike Eureka, the config server doesn’t have the concept of peers.The obvious option is to use a shared file system but it requires extra setup.ConclusionWith the latest technologies coming up, you can expect that our data will be stored in an immutable ledger that is secured by cryptography.But we have to be aware of the arrival of quantum computers. This could make the best encryption algorithms useless. But as always, we will find a way to protect ourselves…Sources  Advanced Encryption Standard encryption  Rivest–Shamir–Adleman encryption  Google Cloud KMS  AWS KMS  Azure Vault  The twelve-factor app  Using JDBC  HashiCorp’s Vault  Vault as a backend  Spring Cloud Config Server  Keywhiz  JCE 6  JCE 7  JCE 8"
      },
    
      "architecture-2017-12-27-deploying-web-applications-with-environment-specific-configurations-html": {
        "title": "Deploying web applications with environment-specific configurations",
        "url": "/architecture/2017/12/27/Deploying-web-applications-with-environment-specific-configurations.html",
        "image": "/img/2017-12-27-Deploying-web-applications-with-environment-specific-configurations/post-image.jpg",
        "date": "27 Dec 2017",
        "category": "post, blog post, blog",
        "content": "The problemRecently one of my colleagues came across a problem when he wanted to create an Angular application which needed to have different configuration values between environments.In 2016, Jurgen Van de Moere wrote a blogpost which explained how to create environment-agnostic applications with AngularJS.A year later, Rich Franzmeier explained in his blogpost a solution for Angular applications which was based on Jurgen’s post.Although both solutions work perfectly, they both have some downsides to it.Nowadays we want to push devops to all teams.This implies that our applications should be immutable, and that everything should be automated (as much as possible).If we want to deploy our application we have to overwrite our configuration file manually, so this was the main disadvantage of the solution they proposed.The solutionOur solution is build on top of theirs, and should be able to work for all kinds of frameworks (AngularJS, Angular, React, Vue,…).Here is how we did it.Code setupWe started with an env.js file which contains all environment-specific configuration.This file will expose the data as global variables.One could set the URL where our API is hosted like this:(function (window) {  window.__env = window.__env || {};  // API url  window.__env.apiUrl = 'http://dev.your-api.com';}(this));Next up is to expose this variable to our web application, which in our case is an Angular app.We created an injectable Configuration class, which will have a getter to retrieve our API URL.import {Injectable} from '@angular/core';function _env(): any {  // return the native window obj  return window.__env;}@Injectable()export class Configuration {    get apiUrl(): any {    return _env().apiUrl;  }  }All we need to do now is to inject an instance of the Configuration class in the class where we need the info.Deployment setupAfter building our Angular app, we need to package everything together.Since we want to make everything immutable, the obvious choice here is to use Docker.While starting a container, we can specify environment variables.This is where the second part of our solution starts.Using the envsubst bash command we’re going to convert a template file into an env.js file which contains all our data.Our env.js.tpl file looks like this:(function (window) {  window.__env = window.__env || {};  // API url  window.__env.apiUrl = '$API_URL';}(this));To perform this conversion we need to create a startup.sh script which will actually execute the envsubst command.envsubst &lt; /usr/share/nginx/env.js.tpl &gt; /usr/share/nginx/html/env.jsnginx -g 'daemon off;'Now we need to create a Dockerfile which in turn will be used to create our Docker image.FROM nginx:1.13-alpineCOPY dist /usr/share/nginx/htmlCOPY env.js.tpl /usr/share/nginx/env.js.tplCOPY startup.sh /usr/local/bin/startup.shENTRYPOINT [\"/bin/tini\", \"--\", \"/usr/local/bin/startup.sh\"]ConclusionThis is how we managed to build our application only once, and deploy across numerous environments.Although it is not as immutable as we want it to be, at least we have automated the process to get rid of most human errors.And this way we can easily create a separate environment if there is a bug in production."
      },
    
      "architecture-2017-12-22-tech-lead-html": {
        "title": "Twelve tips to become an awesome Technical Lead",
        "url": "/architecture/2017/12/22/Tech-Lead.html",
        "image": "/img/tech-lead/world.png",
        "date": "22 Dec 2017",
        "category": "post, blog post, blog",
        "content": "What is Technical LeadershipA Technical Lead has the responsibility to help the team move forward. The person assigned to the role, is someone who has sound technical experience and strong communication skills. He or she will be accountable for the technical direction of the project or product and serve as the go-to person for cross-team interactions.  When it comes to medium to large teams it is quite common to have a full-time Tech Lead present, responsible for important leadership activities such as\tGuiding the project technical vision.\tEg. what technology are we going to use, how are we going to deliver the project, what patterns will we use, etc.\tAnalyzing risks and cross-functional requirements.\t\t\t\t\t\tAnalysing risk means mitigating risk: can we chose a certain approach or does it have too many unknowns.\t\t\tWhat will the impact on the project be when taking a certain risk. \t\t\tEg. introducing new technology you saw at a conference.\t\t\t\tCoaching less experienced people.\tYou most likely will have mixed experience in your team. \tThis makes a lot of sense when it comes to cost of a project, mixing and matching skills and experience; thus educating less experienced people.\tBridging communication between stakeholders and the team.\tBusiness stakeholders are often less technical in nature then developers. \tThey will use a different language and the Tech Lead will need to mitigate that.Do we need a Technical Lead?Some people argue against the role; claiming a team of well-functioning developers can make decisions and prioritise what is important to work on. Even when these perfect conditions would exist, during which team members talk to each other openly, discussing pros and cons before arriving at an agreed solution, it doesn’t take much to upset this delicate balance.The Tech Lead role is just that – a role. Instead of focusing on whether the role should exist, it is better to focus on ensuring all Tech Lead responsibilities are met. As with every leadership position, a bad leader can make things worse. With these tips I would like to help you to make sure that doesn’t happen.Two sides to a story  As the job title implies, Technical  Lead  is a job with mixed responsibilities: there is a technical and a leadership side to the story.I will share tips for both sides, although the distinction is not always clear.It is very unlikely these sides will be equally divided. More on this in Tip 4.1. Advocate for Change  Advocate for change, means installing a mindset of positive evolution.When a proces is slow or cumbersome … try to turn that around and make it better.One way of doing this, is by using OODA loops: Observe, Orient, Decide, Act.More information on OODA, can be found in this earlier blogpost.In order to observe slow or cumbersome processes correctly, it is important to be part of the team and experience the same pain as everybody else on the team.You should adopt a state of mind that continuously wants to improve a certain situation. The Japanese call this “Kaizen”. In our case, the situation you want to improve is the efficiency and happiness of the team and the delivery of a software project.Seek out the issues that prevent good teamwork.2. Work through Failure and SuccessThings will failThings will fail. Don’t worry too much about failure.Builds will fail. Deploys will fail. Schedules will be missed. Crashes will happen. If you prepare for failure, it will be easier to cope with it.When things fail, don’t look for someone to blame. You are the Tech Lead. Take responsibility and use your energy to fix the problem at hand and learn from it. Of course, don’t fix the same bug twice. If you need to fix the same bug twice, then you made a wrong decision.Learning from failure, will shape your orientation and make for better decision-making in the future.Celebrate successWhen the team has a sense of achievement, they will be happy and motivated, to be the best they can. It’s important to celebrate smaller achievements, like a successful sprint or a completed feature. I did a project once, where we delivered a system and the customer was really happy with it … Unfortunately, the vision of the customer changed, and the project never made it to production. If that’s the moment you’ve been waiting for …When someone comes up with a new idea, maybe an approach or framework they saw at a conference, and if the idea delivers, it is important that whoever came with the new idea, should be credited. This is very rewarding and will lead to more cooperation, creativity and out-of-the-box thinking.A drink on Friday evening, a small lunch, maybe a team building are all good ideas to get a happy and motivated team. Oh, and it’s fun.3. Stay TechnicalA Tech lead has a lot of non-coding responsibilities, but it is very important not to neglect the hands-on technical activities:  Write code, do proof-of-concepts, define interfaces, … Depending on the maturity of the team your involvement will be different.  Do code reviews and have your code reviewed. When new people arrive at the project, I tend to do most of their code reviews and I will be pretty strict: I will write tests that cause NullPointerExceptions, I will ask them to adhere to conventions, to use the Single Responsibility Principle, to be careful about packaging and naming, etc etc.I will also elaborate on the reasoning for these remarks and for the choices that were made.This might challenge existing ways of working and increase the maturity of the codebase.The number of changes they have to do (after a review) will quickly become less.  Insure a technical vision exists and is shared by the team. This vision needs to be in line with the customers needs. Customer needs will lead to important constraints, eg. regarding reuse (a throwaway project for marketing vs. a multiyear enterprise endeavour … but be aware that this type of constraint might also change).Sharing how you got to this vision with your team, will have theyhuge impact its adoption. Try to involve the team to arrive at the technical vision. And make sure they know how they contribute in reaching that vision.  Keep an eye on the evolution of the code: after a while, the amount of actual coding you do might be lower, but you need to stay up to date on the evolution of the code. You need te maintain awareness of the system and its technical constraints.Most (if not all) developers will be happy to define frameworks, to advocate certain methodologies, etc. But some non-functional requirements (also called quality attributes) such as networking, security, deployment and consistency are often overlooked.4. Always Available  As a Tech Lead, you should always be available for your team; for questions, for support, for guidance or to make decisions. I started this blogpost by saying the technical leadership role has two important aspects and combining these is never easy. Something that makes a lot of sense (for me) is writing down the amount of effort you expect to put into certain tasks, eg.  Technical design: preparing work for the team (that includes you). Making sure it is clear what needs to be implemented and how. This will often take a lot of quality attributes like networking, security, … into consideration.  Business: talking to the customer, looking at their needs and goals and matching these with the technical vision of the project.  Project Management: defining user stories, estimating, follow-up.  Code: writing code, doing code reviews, etc.The assigned percentages will obviously vary for everyone and for every project. It’s also important to look at the actuals, because these will help you understand on what you are spending time.5. Be a mentor for your team    Mediator: A Tech Lead should be a mediator, that facilitates discussion.When people have different opinions, you should embrace this. Because it means they care enough about something to discuss about it. In the end we work towards the same goal. Everybody can learn from the opinion of others.Get input from the team and try to reach a consensus.If reaching a consensus is really impossible and a decision is necessary, decide. Not deciding will always lead to more discussion.  Mentor: A technical lead should be a mentor for developers. Be a teacher. When you review code or when you explain certain conventions, be sure to clearly explain the reasoning why you are doing something in a particular manner.  Effective Delegation: After a while, your team will adopt certain best practices and less (strict) reviews will be necessary or more people will do reviews. That’s the point where you can also give ownership of user stories to more developers. By transferring ownership to developers, they will be highly motivated to do a great job. A tech lead should not try to own all of the responsibilities. The tech leads needs to make sure responsibilities are taken by someone.  Match goals: match the individual goals of the developers with the larger goals of the project and the organisation. This is specifically targeted dynamic coaching. Dynamic, because goals can change. Communication is very important when it comes to matching goals: it will make people feel valued.  Optimise for the group: Individuals in a team are extremely important, but when it is difficult to find consensus, it’s the team you should focus on. Teams that collaborate well, will perform better and members of a well-performing team are happy members.A good Tech Lead  knows when to give input  knows when to make decisions  knows when to step back and allow the team to take more ownership.Share responsibility, give ownership … but stay accountable.6. Surround yourself with other Tech LeadsThere are many reasons to surround yourself with other Tech Leads. On a personal level, it presents an opportunity to learn from your peers: how do they provide input for their team and how do they divide their time between the different responsibilities of the role.On an organisational level, you should verify if there is a clearly understood overarching goal.If this is the case, you might want to investigate whether cross-organisational coordination is required to meet objectives.It is important to keep track of architectural guidelines to make sure your product will play along nicely with other components and to make sure the larger system is consistent.Chances are there will be dependencies on the product of other teams or on the members of other teams.Make sure these are taken into account when composing a sprint.This kind of coordination is a genuine problem at many (larger) organisations or customers.Investing time in networking, is necessary to avoid surprises beyond your control.7. Think Big, Bias for Action.Think Big and Bias for Action are two of the twelve leadership principles at Amazon.Thinking big, means creating and communicating a bold direction for the project or the product. This will inspire results, because people are working on something big. Something that makes a difference.Focus on the opportunities that might arrive in the future. Make decisions that are not limiting. An excellent book on this, is Liminal Thinking by Dave Gray.A Bias for Action means acknowledging that many actions and decisions are reversible and don’t need extensive study.Getting things done … matters.When you put a flywheel in motion, it will keep rotating. Focus on simple things to get the flywheel to move. It will encourage people to deliver as initial hurdles have been taken.8. Interviewing potential new team membersKnow what you are interviewing for.Are you looking for someone for the longer term or are you looking for someone for a short assignment?When you look at a resume, look for patterns: eg. duration of an assignment. Does this match with your needs? If it doesn’t, make sure you ask the candidate if he or she has certain preferences. Some people like long-term projects, others don’t. This does not have to be a blocking issue. But it is something to talk about. Also look at used languages, libraries and frameworks. Do these match with your current choices? When you are looking for a long-term team member, experience with certain tools is less important than the will, ability and eagerness to learn.I always try to focus on the mindset of a developer: thinking logically, identifying multiple approaches to tackle a certain problem.Personally, I strongly discourage using Stack Overflow to find questions.It is more important to ask questions that are relevant for your project.My personal pattern for conducting an interview is as follows:  Comfort  Offer options  Build on the responses  Show interest  Bonus questionOf course, always, stay polite. If the candidate doesn’t match with your specific goals, don’t send them home with a bad feeling.Beware: we still will need to get things done, even when we don’t have the time, resources or influence to fix the team composition.9. Embrace cultural differencesDiversity is invaluable.All people are different and live different lives.This is incredibly valuable, because your users will also be different.Surround yourself with passionate people.Nowadays most (if not all) teams use some kind of instant messaging.When working with teams in different time zones, this becomes even more valuable as it enables asynchronous communication and broadens the potential answers.I mentioned this before: everybody is part of the team and everybody’s opinion should be valued.10. Estimating is hard  Douglas Hofstadter  Hofstadter’s Law: It always takes longer than you expect, even when you take into account Hofstadter’s Law.Estimating is hard. When you do it more often, you will get better at it … but you still will get it wrong from time to time.In agile projects, the entire team can participate in a planning poker meeting. Planning poker can expose unknowns when estimating a user story.In general, there are two approaches to cope with these unknowns: doing a technical design before starting with the user story (eg. by defining a spike) or accepting the risks, together with your business stakeholders.As a technical lead, you will likely also need to do estimations before the team is actually building something or when responding to RFPs (request for proposals). This can be to give business stakeholders an idea of the potential cost, to decide on priorities or to evaluate staff.To achieve this, I suggest using three point estimates, where you do an optimistic, a best guess and a pessimistic estimate and use this formula: (O + 4BG + P) ÷ 6  to get the weighted mean.Depending on the nature of the estimation, the number of unknown unknowns might be large: the project can be very similar to other projects or completely different. Factor these in. Estimate for the team that will do the implementation: you are probably estimating a real project. This is not the fastest time you can possibly do something, in the best possible conditions. The estimations represent the ability to execute for a team; not your ability to do the implementation yourself. Also make sure, you know your deliverables. This can be more then code and deployment artefacts, eg. code quality assurance reports, manuals, …Document assumptions.Mastering estimation is a lifelong journey. It will set you apart. And your colleagues will associate you with professionalism, stability, and quality work.11. Interfacing with the outside world  The language used by non-technical stakeholders might be very different then that of the development team.A Tech Lead must find a way to communicate ideas in ways non-technical people can understand. Eg. by using analogies and using terms others can easily relate to.In a DDD world, this means establishing a ubiquitous language.Work closely with customers, try to detect requirements from them and continuously map their requirements with the on-going implementation.As a technical lead, I don’t think you should be the Single Point of Contact. Because then you introduce a potential liability in the project: a strong dependency on you. Include your team in certain discussions, but make sure you prevent continuous interruptions of your team members … So don’t be the Single Point of Contact, but try to be the First Point of Contact.12. Facilitate (agile) team workI would urge all Tech Leads to facilitate agile team working. Of course this works better, when the business is involved as well. But even when they are not involved, assign a proxy product owner. Chances are, this will be you.It doesn’t really matter if you use scrum, kanban or something else, but aim for short development cycles, feedback loops, etc.ConclusionYour team’s strength is not a function of the talent of individual members. It’s a function of their collaboration, tenacity, and mutual respect.If you’d like more information on Technical Leadership, you can check my slides on SlideDeck or this video on YouTube of my talk at Devoxx."
      },
    
      "iot-2017-12-20-virtual-reality-html": {
        "title": "An introduction to virtual and alternate reality",
        "url": "/iot/2017/12/20/Virtual-Reality.html",
        "image": "/img/virtualreality/banner.jpg",
        "date": "20 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  A look into the wonderful and exciting world of virtual, alternate and mixed reality.IntroThe concept of virtual reality is not a new one, neither is the one of augmented reality.It has been around for quite a long time already.Perhaps mixed reality will be the next big thing? I’ll leave that up to you to decide.Ever since we have been able to create visual representations of our own or other worlds, mankind has been fascinated by transferring the sensory perception into this world.It is in the recent years that technological advancements have made it possible to do this in ever increasing realistic and engaging ways.A trip down memory lane…In 1962 some crazy person built what is generally believed to be the first virtual reality experience.This machine pictured above is called a Sensorama. The user of the Sensorama sits on a tilted chair and watches a short film with stereoscopic 3D images in wide-angle view with stereo sound and even added effects such as wind and aroma. A predecessor of those 4D movies, available in cinemas and theme parks these days.This, of course, cannot be compared with the more advanced implementations we have today.In the 70’s and 80’s virtual reality really started blossoming in certain specialized areas.In 1968 Dr Ivan Sutherland created the Sword of Damocles at MIT.This head mounted display (or HMD) was so heavy it needed to be suspended from the ceiling.It was able to track head movements and show rudimentary 3D images.During the 90’s and 00’s: more sophisticated military, flight and combat simulators started showing up.In the privatized industry: simulators for airline pilots, doctors and surgeons.Different ways of interacting with this virtual world also started appearing:  Head tracking  Touch screens  Gloves  Entire flight simulators  …This is also when video game companies first tried to capitalize on this exciting new technology and bring it to a wider audience.But the technology was still too rough and did not catch on.Some of these early attempts include:  Nintendo Virtual Boy  Virtuality (arcade system)  iGlasses  VFX-1                                                                But then came 2012.The year in which the Oculus Rift was kickstarted. This sparked a renewed interest in virtual reality which, thanks to technological innovations, was looking much, much better it ever had!All kinds of realitiesAlright, so we have been talking about virtual reality for a while.But what exactly are virtual reality (VR), augmented reality (AR) and mixed reality (MR) and how do they relate.Above you see the reality - virtuality continuum.Let’s start on the left side.The left side depicts reality or the Real World. With the Real World we mean the world where you, as the reader of this article, are sitting in a chair looking at your computer screen or holding your smartphone.For the sake of this article we assume that the world is real and that there are no matrix-like shenanigans going on.Simply put, the world as you know it and experience it every day, the world you and I live in.On the other end of the spectrum we see the Virtual Environment.Everything in the virtual environment is virtual, so ‘not real’.It is the imaginary world that you see on a screen.The user is locked out of the real world.An extreme example we all know of would be The Matrix, in which the person lives their life inside The Matrix not knowing the entire thing is not real.Augmented Reality (AR) and Augmented Virtuality (AV) are situated in between these two ends of the spectrum.As their names suggest, AR is more closely aligned with the real worldwhile AV is closer to the virtual world.In other words AR consists of a mostly real world with a few virtual elements augmenting the experience.AV exists mostly in the virtual world with a few elements of our real world visible.Mixed or Merged reality (MR) is a term used to describe anything between reality and virtuality.This means it contains both elements of the real world and a virtual world.It can go all the way from a mostly real world with a bit of virtual world sprinkled on top to mostly virtual world with only trace elements of the real world remaining visible.With MR as opposed to AR and AV, the virtual world is aware of the real world and can interact accordingly.We see the term X-realities being used to describe any of the VR, AR, MR and any other realities.These terms are pretty vague and some have even changed meaning over the years.They outline the boundaries of the different areas in the spectrum, but they tend to bleed over into one another very easily, and get mixed up a lot!Virtual RealityWe call something virtual reality when the user is emerged in a virtual world and is completely blocked out from the real one.The simplest example of this is a 360° video experience.More advanced implementations have the user take part in a fully virtual world where their movements and actions are tracked and translated into actions in the virtual world.Some examples of virtual reality implementations are:  Games, both recreational and educational  Photo content: photo spheres  Video content: video spheres, 360 videos  Creative applications like 3D painting, sculpting, DJ applications and many others…Currently the most popular application for VR is video games. There are a lot of games with VR support on the market already, and more are getting released almost every day!Shooting games and simulators seem to be an exceptional match for the platform.Although mostly used for gaming purposes, a VR headset can also be used for viewing 360 scenes and videos.Thanks to head tracking, the viewers perspective is adjusted automatically when the head is moved.The more advanced headsets like the Oculus Rift and the HTC Vive even have external sensor arrays to track user movement, and have hand based controllers for interaction.Although you have to deal with additional setup compared to a phone where you can just drag the viewport or use your phone’s motion sensors.Augmented RealityLast year, the world exploded with people young and old crowding the streets in search for Pokémon.Pokémon Go is a fine example of an AR application.It is augmented reality because the application is still grounded mostly in the real world.The virtual elements are layered on top of the real world, but they do not interact with it.There are 4 types of augmented reality applications.  Marker  Inverse Marker  Markerless  ProjectionMarker basedMarker based AR apps use image recognition to recognize a specific pattern or marker.The marker is detected and replaced or covered with a virtual object.These types of applications are very simple and used for showcasing or displaying additional information.Inverse marker basedInverse marker based AR is very similar to regular marker based AR.These applications are used in conjunction with large screens with cameras where the user only has to control the marker.MarkerlessThese markerless applications use positional tracking and GPS to determine where to show things by mapping the environment and creating a spatial awareness to track objects when moving.The popular Pokémon Go and Ikea apps are perfect examples of this.Projection basedProjection based AR projects images rather than showing them on the screen.This requires hardware capable of projecting so is not as widespread.It is used more in manufacturing.The great thing about AR is that any recent smartphone is capable of running AR applications.Many AR apps only need a camera to work.While specialized depth sensing sensors exist, they are not as widespread and are used more in specialized industries.Mixed RealityMixed reality takes the best parts of both virtual and augmented reality.What makes mixed reality special is that it understands the environment, it can interact with and respond to changes or events happening in the real world.Hardware that supports MR takes the form of glasses and headsets with cameras and other sensors.These sensors are used to map the physical 3D space so the virtual objects know where they are situated and know what is in the real world.This is where you get the holographic experiences, even tough they aren’t real holograms, they look and feel exactly as you would expect them to.Hardware and current pain pointsCurrent devices are a big step up from what we had even a few years ago, but there are still many improvements to be made.The high-end VR devices require beefy computers equipped with high-end graphics cards.The headsets requiring a smartphone offer a more mobile experience but are still limited in the visual effects department.For a virtual reality experience to feel smooth and natural it needs to be rendered at a minimum of 90 frames per second.If the device can not keep up, the user might experience VR sickness, similar to motion sickness.Because our eyes are so close to the screen and the resolution our eyes can perceive is so high, pixels become visible, detracting from the experience.When looking closely, the screen looks like a raster.This can be solved with a higher resolution but that again requires more computational power.This much content at such a high pace also requires a lot of bandwidth and low latency.Another deterrent for VR headsets is the extensive setup required to make them work.High-end devices require base stations or tracking cameras to accurately measure movement.There are devices with sensors built in so that they are not dependent on external base stations, this technique is called inside-out tracking.Creating realitiesMaking software that is AR/VR/MR enabled can be quite different from traditional business application development.These type of applications are often heavily dependent on visual elements and can require extra artistic knowledge.For virtual and mixed reality game engines can be used to create “games” to create these experiences.These are some of the most popular game engines that are free for personal user:  Unity  Unreal Engine  CryEngineThink of these as a sort of photoshop for software.We can leverage the editors to see what we’re working on and make changes immediately.These tools are ideal for virtual reality.Both Unity and Unreal Engine have plugins available for augmented reality.The more business targeted applications are often augmented reality enabled applications.Both Apple and Google have made big efforts to support this on their respective OSes and basically every smartphone with a camera is now AR enabled.Apple’s ARKit and Google’s ARCore are the respective augmented reality APIs for their IOS and Android platforms.Since the inception of these two APIs the amount of AR apps has greatly increased, Google is even discontinuing its Tango project in favor of ARCore.There are many other SDKs available, many of which even have their own Unity plugin.Vuforia and ARToolkit seem to be the most popular ones.Microsoft created the Windows Mixed Reality framework for mixed, virtual or augmented reality apps to run on windows 10 based computers.Business cases &amp; ExamplesSo, as software developers, what can we do with all this fancy tech?Our Dutch colleagues already made an awesomely detailed VR application to help train personnel of the Royal Dutch Navy.Make sure to have a look since it is a truly impressive feat of software engineering.    Virtual reality is widely used for training purposes.Whether it is for a boat, an aircraft, a machine or a human body, a virtual space can prepare someone for situations that are otherwise very costly or difficult to simulate.Augmented and mixed reality can show information when and where you need it.On a mobile phone screen, through glasses or projected on a surface, having the correct information at hand is always useful.Visualizing a product before it is manufactured can enhance the design process by discovering points of improvement much earlier on.Visualization like Ikea’s app for trying furniture or a tour through historic Bruges or even business cards with an AR marker for increased memorability.For us at Ordina, we see most potential in AR and MR.Mobile based AR applications that can be used for showcases, in the IoT world, or even just augmented digital signage with inverse marker based AR.ConclusionThe years to come will bring even more advancements to the wonderful world of virtual, augmented and mixed realities.A lot of things are still shaping up and being developed, ever improving on earlier versions.A lot of solutions and standards are still being figured out.Which means that now is a great time for us to try stuff out, and get a feel for the technology, whilst preparing and working on proof of concept applications."
      },
    
      "conference-2017-12-19-xpdays-benelux-2017-html": {
        "title": "XP Days Benelux 2017",
        "url": "/conference/2017/12/19/XPDays-Benelux-2017.html",
        "image": "/img/xpdays-benelux-2017/XPDays-Benelux-2017.png",
        "date": "19 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  XP Days Benelux is a two day conference on agile software development for and by agile practitioners.In this post, I will take you along to the talks and sessions I attended and participated in.  Table Of Contents        The 8 Stances of a Scrum Master - Barry Overeem      Do Not Deal with Resistance! - Remi-Armand Collaris &amp; Linda Dorlandt      Motivate Your Team with Gamification - Jean-Jacques Courtens      An Integral View on Agile - Frederik Vannieuwenhuyse &amp; Johannes Schartau      The Art of Hosting Conversations that Matter - Johan Decoster &amp; Jef Cumps    The 8 Stances of a Scrum Master - Barry OvereemBarry Overeem is a freelance Scrum Master and Professional Scrum Trainer at Scrum.org. He’s an active member of the Scrum community and shares his insights and knowledge by speaking at conferences, facilitating workshops and writing blog posts.It is very common for a Scrum Master to get tasks assigned, that aren’t actually tasks a Scrum Master should be performing in the first place.The job description of a Scrum Master comes with a lot of other tasks than people, sometimes including the Scrum Master, assume.The session starts with an overview of the eight misunderstood stances of a Scrum Master:  Scrum Police: Scrum isn’t a hard set of rules to be followed. There’s absolutely nothing wrong with a bit of flexibility and empathy based on the team’s situation.Time boxing for example is very important but keep an open mind when the team is having a very valuable conversation after the time box has ended.Flexibility regarding time boxing is of course not meant to happen too often. So keep an eye on recurring discussions to see if they need to be addressed individually.  Hero: Managing and solving all problems and impediments like nobody’s business! While an important task for the Scrum Master, make sure not to get too focused on being the team’s impediment super hero.  Scribe: “Hey John, you’re taking notes again for this meeting, right? Thanks!”. Do you recognize this exchange? Congratulations, you’re the team’s personal scribe.This is especially to be avoided during retrospectives since this creates a false sense of ownership of the issues and actions towards the Scrum Master.  Admin: The Scrum Master is the workflow master. Need to add a board in Jira? Ask the Scrum Master. Need to start a sprint in Jira? Ask the Scrum Master. Now, hold up right there! The Scrum Master can of course perform these tasks but yup, so can the other team members.  Secretary: The Scrum Master plans all the work in the team members’ agendas. Keep up-to-date with everybody’s holidays, sick days and toilet breaks. (Can you sense the sarcasm?)  Chairman: While this is often the case, the Daily Scrum isn’t a meeting where the team members report back to the Scrum Master. It’s a meeting by and for the development team. In absence of the Scrum Master, the Daily Scrum must still happen.  Team Boss: The Scrum Master is the boss of the team. He/she decides who is in and who is out. A sick day? You’re fired! Buy the Scrum Master a chocolate cake? Here, have a raise! Oh, what a world it would be!  Coffee Clerk: Please do get your team members some coffee once in a while. But no, to everyone’s surprise, fetching coffee day-in day-out is not part of the Scrum Master’s job description.So, what are the eight preferred stances of a Scrum Master then?Glad you asked!The eight preferred stances are:  Teacher: There is much the Scrum Master can teach the Scrum Team. The Scrum Master must ensure that Scrum is understood and implemented properly by the entire team.He or she makes sure the team stays on track of the Agile practices and principles.  “The art of teaching is the art of assisting discovery.” - Mark Van Doren  Impediment Remover: An impediment is a problem that goes beyond the self-organization of the Development Team. Make sure the team understands and uses their own ability to solve problems and be self-organized. It might also create an opportunity for the team to come up with creative ideas to solve the impediments themselves.  Facilitator: Being a Scrum Master also means facilitating the team in transparency, inspection and adaptation. It is also through great facilitation that the Scrum Master succeeds in getting more value out of every event.  Coach: It’s important to stay away from the solution and ask questions in order to facilitate discovery of solutions. Of course, the Scrum Master may offer new perspectives to help the team reach a solution.  Servant Leader: Remember that the Scrum Master serves others. Being able to read the room, manage conflict and facilitate resolutions within the team is a very important responsibility of the Scrum Master. Make sure to lead by example and make others feel comfortable with failing.  Mentor: The difference between coaching and mentoring is that for mentoring, having in-depth knowledge is crucial. A mentor helps the team understand the practices and principles of Agile and transfers his or her knowledge of the subject.  Manager: The Scrum Master manages a whole bunch of things: the culture, the Scrum process, team health, Scrum values, impediments and boundaries of self-organization. These boundaries need managing because boundaries that work for one team might not work for another.  Change Agent: It’s essential to try to influence the company culture to open up to Scrum so the Scrum Team can flourish and thrive.More info on the 8 Stances of a Scrum Master can be found here.Do Not Deal with Resistance! - Remi-Armand Collaris &amp; Linda DorlandtRemi-Armand Collaris is a team and organisation coach who uses Agile, Scrum and LEAN ideas to find new ways to help people, both within teams as well as among teams, communicate and collaborate better.Linda Dorlandt is a mentor in change processes that help teams collaborate and reach a common goal. In order to do this, she uses and teaches methods for coaching and process management.The session by Remi-Armand and Linda started with two exercises. For the first exercise, we had to pair with one of our neighbours where one of us was the coach and the other the coachee.The coachee had to open up about an issue they’re facing that they cannot solve by themselves and the coach had to try to resolve it by suggesting solutions. After a few minutes, we had to switch roles. Both people then had to show to the group whether they felt better about the problem or not.The second exercise dealt with the same problem, but instead of immediately trying to suggest solutions, the coach asked questions to get to the bottom of the issue and figure out the goal of the coachee. Again, after this round, both people had to show to the rest of the group how they felt about their problem, if the feeling was better, worse, or stayed the same.The goal of these exercises was to show that trying to push someone towards a solution for their problem isn’t the best way to handle the situation since there will always be a ‘but’ coming from the person with the issue. Asking questions and trying to get the person to thoroughly think about their situation and problem will help them reach a solution themselves that they understand and accept.As an example of someone trying to actively deal with resistance, they showed the YouTube video called ‘It’s Not About the Nail’.The conclusion is easy and was mentioned in the paragraph above. Pushing someone towards a solution they don’t see fit will not give you the outcome you hope for.Remi-Armand and Linda discussed a series of steps that will help facilitate the conversation with a person that has a problem and is unable to come to a solution by themselves.  First step: Make a connection. Acknowledge the person’s issue and make them feel comfortable opening up to you.  Second step: “What is bothering you?” Try to figure out what the actual root cause is of their issue. Get to the bottom of it and focus the complaints.  Third step: “What do you think will help you?” Guide the person towards a change in language. It’s important they can see that a change in the situation is actually possible.  Fourth step: “When?” Create concrete plans for change. Don’t postpone taking action. Plan it and make it happen.More information can be found on the Dutch website ‘Praktisch op weg naar Teamresultaat’.Motivate Your Team with Gamification - Jean-Jacques CourtensJean-Jacques Courtens is the founder and managing partner at Adsdaq where he introduced gamification as a means of motivating the teams to deliver working products consistently.This was an Open Space session where Jean-Jacques discussed the way he implemented gamification in his company. At Adsdaq, they work with a reward system where points can be earned by completing certain tasks.They assign points to three sections: Timesheets, demos and sprint objectives.  Timesheets: Filling in the timesheet in time is worth 1 point  Demos: Doing a demo is worth 2 points  Sprint objectives: Each completed story point is worth 1 point. For sprint objectives, they also use a point multiplier. X0 when the sprint has failed, X1 when 90% of their main goal of the sprint was achieved, X2 when 100% of the main goal was achieved and X3 when the entire sprint was a complete success.For all three sections, they also assign badges. For example, the team can earn the golden badge when 15 sprints in a row are a complete success.The earned points are actually converted to Euros that are spent on team celebrations where 1 point is worth 1 Euro:  70% of the earned money goes towards team activities, such as team building, team lunch, etc.  30% is spent on personal rewards like cinema tickets, dinner for two, etc. This is only meant for team members that were part of the sprint.Jean-Jacques noted that the company probably spends around 3.500-4.000 Euros every year for this reward system, for a team of 8 people.It’s also important to note that the team is still being rewarded when the sprint fails and not being reprimanded for it. The reward is just a lot bigger when the sprints succeed.An Integral View on Agile - Frederik Vannieuwenhuyse &amp; Johannes SchartauFrederik Vannieuwenhuyse is a multidisciplinary generalising specialist and is continuously on a journey of discovery and learning how to grow effective, resilient and agile organisations.Johannes Schartau is an Agile Coach, consultant and professional Zombie Scrum fighter from Hamburg, Germany. He is passionate about creating environments for real collaboration and joyful creativity.The first part of this exercise consisted of reading six case studies and writing down on post-it notes how you would handle each situation, what questions you would ask.These post-it notes were used in a later exercise, after an explanation of the so-called Integral Theory:Integral Theory was created and shaped by Ken Wilber and consists of the idea that anything can be viewed as a thing by itself or as part of a larger group (Individual VS Collective).Anything can also be seen from within and from the outside (Interior VS Exterior). Based on this, Ken suggests that any kind of knowledge or experience can be assigned to any of the four quadrants shown above.  Leadership and Engagement: Interprets people’s interior experiences and focuses on “I”  Culture and Shared Vision: Interprets the collective consciousness of a group of people and focuses on “We”  Behavior and Metrics: Observation of the behavior of people and focuses on “It”  Organizational Architecture and Systems: Focuses on the behavior of a group of people as functional entities seen from outside: “They”The following image gives an overview of the questions that help organize knowledge into the quadrants:After explaining Integral Theory, we picked up our post-it notes and organized them into the quadrants that were put up across the room.When the post-it notes were assigned to their respective quadrant, Frederik and Johannes asked us to move towards the quadrant in the room that we felt we had the most experience in or we felt we were good at.Next, we had to discuss in our ‘quadrant groups’ whether the post-it notes that were put up in the quadrant were a good match for it or whether they belonged in another quadrant.The final exercise consisted of moving with your group to the next quadrants and listening to one person, that stayed behind from the initial group, explain the conclusions they reached during the initial discussion.The presentation of Frederik and Johannes can be found here.The Art of Hosting Conversations that Matter - Johan Decoster &amp; Jef CumpsJohan Decoster is an Agile coach and trainer trying to make a difference in the lives of the people he works with by uncovering everyone’s unique potential and looking deeper into the essence behind any theory or concept.Jef Cumps is an experienced coach and trainer supporting organisations in their transformation towards more agility and a more engaging, humane and effective way of looking at work.The session by Johan and Jef started with a small exercise. We paired up with our neighbour and asked each other the question, ‘Why are you really at this conference, and what has it meant to you so far?’The point of the interview was asking questions to guide the interviewee in telling their story. After these interviews, the person interviewing had to share what they had learned with the rest of the table and the others had to take notes of the summary.The steps for executing this exercise are covered in this picture:After this exercise, it was clear that asking appropriate and considerate questions is a key element of hosting a good and constructive conversation.This kind of conversation is built upon four key elements:Make sure the conversation stays polite and has a good flow of dialogue, but also take note of the two other sections.It’s very important to respect the differences in ideas between you and your conversation partner. It’s okay to voice your opinion, but be considerate and respectful and make sure to not overpower the other person.Listen often and do not judge too quickly. The other person might not be good in voicing their opinion clearly so suspending judgement is always a good idea. Things may become clearer later on in the conversation.Right before the break, people were invited to write down questions they had, where they could use the help of others to solve it.Eventually, the top 8 questions were picked and each question owner was going to be the conversation host for one of the tables.After the break, we joined a question owner at their table with a maximum of 4 persons per table. The other people helped the question owners answer their question by having three conversations based on the following questions:  What is the quest behind the question? Try to get into the question as deep as possible and figure out what the real reason is behind the question.  What is missing? Is anything from the previous conversation still missing? Are there any other additional reasons behind the question?  What actions can be taken? Decide on concrete actions that can be taken.Each question was assigned a time box of 15 minutes and you were only allowed to talk about that specific question during that time box.After each question, the other people at the table moved around to another table to help with someone else’s issue.So, after the exercise, every non-question owner was able to help three different persons by helping with a different question each time.This exercise is called a Pro Action Cafe, which is based on World Cafe and Open Space.The outcome of the exercise was a group of people, happy to have some concrete actions to start working with to try and solve their issues.More info on the Art of Hosting can be found in the following image and on the mentioned website."
      },
    
      "iot-2017-12-16-meeseeks-box-diy-guide-html": {
        "title": "Building a Meeseeks Box!",
        "url": "/iot/2017/12/16/Meeseeks-Box-DIY-Guide.html",
        "image": "/img/meeseeks/wallpaper.jpg",
        "date": "16 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  Ooh, I’m mister Meeseeks, look at me!A Meeseeks Box?For those of you who are already familiar with the show Rick and Morty, the Meeseeks Box should be a well-known object!For those who do not know Rick and Morty, go watch it now, I’ll wait!In short: The Meeseeks Box is a technological/magic box crafted by Rick. When the button on top is pressed, a Meeseeks is spawned.The Meeseeks can be given one assignment (like a wish) and he will try to fulfil said request.The Meeseeks will only disappear when the task has been completed.One caveat, existence is painful for the Meeseeks, the longer it lives, the more sanity it loses.Our colleague Dieter Hubau made a fully operational Rick and Morty themed example to demonstrate Spring Cloud Stream. You can read this excellent story about it on our tech blogBe sure to check it out, it’s a good read!This blog post will go into detail on building your own Meeseeks Box, which I integrated to work with the above Spring Cloud Stream demo.What should it do?The Meeseeks Box is intended to complement the Spring Cloud Stream demo mentioned above.If the button on top is pressed, like in the series, a Meeseeks is spawned in the demo application. (A new instance, see the Spring Cloud Stream blog post)The Meeseeks will then search for the Szechuan sauce until it is found.For the demo a maximum of three Meeseekses can be spawned, as to not overwhelm the people with Meeseekses, because they tend to get annoying if they live for too long.The hardware setupThe setup for the box is as follows:  A box (container)  Raspberry Pi 3 with GPIO connected to a button with an LED  Internal battery pack to power the boxSince the Raspberry Pi 3 has built in WiFi and Bluetooth it is possible to make the box fully wireless.The Pi has Node installed on it (the latest version) and is connected to the WiFi.The WiFi can be easily configured by placing the SD card in your computer and placing a file name wpa_supplicant.conf file in the root of the boot volume.This file contains the configuration for the WiFi network the Pi should connect to.wpa_supplicant.confcountry=BEctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1network={     ssid=\"SSID-goes-here\"     psk=\"key-goes-here\"     key_mgmt=WPA-PSK}Make sure you do not omit any of the first lines or your Pi’s WiFi will cease to function until a corrected version of the file is used!The build!The original idea was to make the box itself from wood or thick cardboard. But since I wanted to try something new that would entail less manual work with getting all the insets correct on the sides of the box, I decided to go for a 3D printed version.The box:You can download the file here.The lid:You can download the file here.These two 3D models were originally obtained from the Thingiverse but I’ve adapted and scaled them properly.I ordered the 3D prints via 3D Hubs and was surprised it was finished in one day.When I went to get the printed versions I was a bit concerned that they might not have turned out as I had hoped.And was I right:A rookie mistake, I didn’t check the model dimensions once I uploaded them into the online tool for processing. As that seemed to have converted up the measurements I used and set them to millimeters instead of centimeters.An easy fix and the second printed version was in the correct size, but printed by a colleague to keep costs down.Next came the task of painting the thing. As time was short and I only had cheap non-spray water based paint available, I decided to proceed anyway.I did apply a spray can based primer first, to make the box white again as the orange color was far from perfect to apply other colours.Many layers and hours later the box was painted.Nowhere near perfect but good enough for a first try at painting 3D printed models. The big issue with these paints and 3D printed models is that the paints tends to get in between the printed ‘lines’ and thus requiring a lot more paint without actually getting a nice result.                                The button on top was attached by very carefully drilling a hole in the top lid and pushing the base of the button through.The gap was tight enough for the button to stay firmly in place by friction alone, allowing it to be removed later on.The Raspberry Pi was attached to the underside of the lid with some standoffs and super glue.The lid fits on the box and is held in place by magnets.This prevents any moving parts that might fail due to material fatigue or attaching hinges, since attaching these to the box and lid would be cumbersome, as screws can’t easily take hold in the 3D printed material.A future, more elaborate version of the box could include cutouts for the lid in the box.The code behind itThe code behind the Meeseeks Box is a simple NodeJS application.As it is run on a Raspberry Pi we need to make use of raspi-io to make use of the GPIO on the board.I also use Johnny-Five as an abstraction layer. More information about Johnny-Five can be found on their extensive website.main.jsvar Raspi = require('raspi-io');var five = require('johnny-five');var http = require('https');var board = new five.Board({    io: new Raspi(),    repl: false});board.on('ready', function() {    var ctx = this;    var prevValue = 1;    this.pinMode(0, five.Pin.OUTPUT);    this.pinMode(7, five.Pin.INPUT);    this.digitalWrite(7, 1);    this.digitalRead(7, function(value) {        //console.log(value);        if(value == 1) {            //Enable this to disable the LED when the button is released!            //ctx.digitalWrite(0, 0);        } else if(value == 0 &amp;&amp; prevValue == 1) {            ctx.digitalWrite(0, 1);            doCall('POST');        }        prevValue = value;    });});The above code is very simple, it makes a new Board instance which we pass a new Raspi instance telling the Johnny-Five library that we are actually running on a Raspberry Pi and that it does not need to search for any other connected boards (like Arduinos).What you also might notice, for those who have used Johnny-Five in the past, is that we do not make use of the full power of Johnny-Five. We are not using the LED or Button classes and instead are taking a more lower level approach by controlling the IO pins directly.This has a very good reason.The Node application is run at startup, when the Raspberry Pi boots, as a Linux service.Starting it automatically breaks the REPL functionality of Johnny-Five which results in the application exiting after a good second, making it unusable.This is why the Board config has the repl parameter set to false, this prevents the REPL from starting and makes it so the application does not exit unexpectedly.This unfortunately also prevents us from using the full abstraction power of the Johnny-Five framework.The actual code is very simple. We wire up a pin as input for the button and another pin as output for the LED.We put the input pin to high, this prevent the input from flickering between high and low (essentially a pull-up to vcc).We then bind a function to the digitalRead which gets executed every time the state of the input pin changes (high to low -or- low to high).Since we do a pull-up to vcc our button will actually be connected to the GND which will result in the signal of the input pin going to low when the button is pressed and back to high when it is released.Please also be sure to wire up the LED with a correct resistor to prevent it from drawing too much current, as that might damage the IO pin it is connected to!Calculating such a resistor is an easy feat. If the LED needs 3 volts to function and uses 20 milliamps doing so: R = U / I = 5V (pin out) - 3V (LED) /  0,02A = 2V / 0,02A = 100ΩThis means that a 100Ω resistor needs to be put in series with the LED to prevent it from causing any damage to the IO pin/circuitry.main.jsfunction doCall(method) {    var request = http.request({        host: 'rnm-meeseeks-box.cfapps.io',        port: 443,        path: '/',        method: method,        headers: {            'Content-Type': 'application/json',            'Content-Length': 0        }    });    request.write('');    request.end();}The code above is a simple snippet used to make a call with no contents to the remote server.When the URL is called via the POST method, a Meeseeks is created.When the URL is called via the DELETE method the currently active Meeseekses are destroyed (for testing).You can edit this to perform any action you like.A video showing a fully operational Meeseeks Box:  Running Node as a systemd service on LinuxAs the Meeseeks Box needs to be simple to use, the application should automatically start when the Pi does.The best option was to make a systemd service and run it on system startup.First we need to create the systemd service file:sudo nano /lib/systemd/system/meeseeks.serviceThis will create a new file (if one does not exist yet).Place the contents below in this file and save it.[Unit]Description=Meeseeks Box serviceAfter=network.target[Service]Type=simpleUser=your-user-hereExecStart=/usr/bin/node /home/meeseeks/main.jsRestart=on-failure[Install]WantedBy=multi-user.targetThis file tells systemd what the service is and does, with what executable and which user.The After=network.target tells the service daemon that this service should only start if the network stack has already loaded!To test the service, first execute: sudo systemctl daemon-reloadThis reloads the daemon so it knows of the newly created service.Now we can manually start/stop/reload the service by using: sudo systemctl start meeseeks where you swap out start with the action you want to perform.To make the service run at startup use: sudo systemtl enable meeseeks and to disable it again, use the same command but swap out enable for disable.A far more detailed explanation about this matter can be found here.Meeseeks at DevoxxThe entire purpose of the Meeseeks Box was to be part of our booth at the well-known Devoxx conference in Belgium.Our booth drew quite the crowd this year, mostly because of the nachos and the totally real Szechaun sauce to go with them. Have a look at a couple pictures below:                                ConclusionThis was a fun side project to work on, even though the ‘deadline’ was a bit tight and I would have liked to have done some things differently, all in all everything turned out really well.A few lessons learned though:  Check measurements before ordering a 3D print  Non-spray water based paints are not the best match for painted 3D printed models  If you mess up the WiFi on the Pi it can be a real pain to debug it!  When starting Node as a service on Linux the Johnny-Five REPL does not work  Super glue is not always so super ;)"
      },
    
      "docker-2017-12-15-docker-basic-networking-html": {
        "title": "Docker basic networking",
        "url": "/docker/2017/12/15/Docker-basic-networking.html",
        "image": "/img/docker-basic-networking/docker-basic-networking.png",
        "date": "15 Dec 2017",
        "category": "post, blog post, blog",
        "content": "  Containers are all the rage at the moment so I guess everybody knows how to build and run a container by now.But what use is one container by itself?In this post, I will show how you can create networks within Docker and what they are used for.  Afterwards, I will guide you through a step-by-step example on how to create a Docker network and add containers to it.This way, we will end up with a multi-tier application that is running on Docker containers in a basic network.Table of contents  Preface  Network setup  Database container  REST Backend container  Frontend container  ConclusionPrefaceIn the current application landscape, we see a strong rise of distributed applications.This is done by implementing a microservice architecture and deploying these applications in Docker containers.It’s important that these containers are able to communicate with each other, after all, what good is a microservice that is isolated?In order to achieve this, a couple of patterns are used.In this post I will demonstrate two of these patterns to you.  Communication within a Docker network between containers  Communication outside the Docker network by exposing internal portsOur final application setup looks like this:    All of our Docker applications will be deployed on one host machine.We will have a custom Docker network running with three containers attached to that network:  The database container is just a MySQL database running within Docker.  The backend container is a Spring Boot application that connects to the MySQL DB container and provides a REST service to the outside world.  The frontend container is an AngularJS application that consumes the REST service from the backend container.You can find all the code examples on Github.The only thing needed to complete this guide is a working Docker installation.Ready? Set. Go!Network SetupTo start this guide, let’s have a look at the Docker networks that are available on our machine:docker network ls  When you run a Docker container and you do not provide any network settings, it is by default attached to the bridge network.Containers that are connected to this default bridge network can communicate with each other by using their internal Docker IP address.Docker does not support automatic service discovery on this network.We want to be able to access our containers by using their container name instead of the internal IP address so we are going to create our own network:docker network create --driver bridge basic-bridgeThis creates a user defined network with the bridge driver that is called basic-bridge.If we look at the Docker network stack, we see that our user defined bridge network is added:    You can look at more details of the network by using following commanddocker network inspect basic-bridge    The basic-bridge network can use IP addresses from the 172.18.0.0/16 range and will use 172.18.0.1 as its default gateway.Database containerNow that we have created our own network, we can start attaching our containers to that network.Let’s start off by creating the MySQL database container.The following command pulls the MySQL image from the Docker repository and starts it as a container that is attached to our network:docker run --name=mydb --network=basic-bridge -p 3306:3306 -e MySQL_ROOT_PASSWORD=test -d MySQL:8.0.3That’s easy right?This container is attached to the basic-bridge network that we created in the previous step.Run the following command to look at the container in detail:docker inspect mydb    We can see in the output that it has gotten the 172.18.0.2 IP address and that it’s using the default gateway of the network that we created.Now we should set up the database in our container with the schema for our REST application.mysql -h 127.0.0.1 -P 3306  --user=root --password=testThis will connect a MySQL shell onto our localhost:3306.We can access this port because we exposed it when we started the container by using the -p flag.Note that this is done for convenience only, so we can access the container from our host and set up a schema.Now run following SQL commands in the MySQL shell.create database greeting;use greeting;create table greeting (    id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,  greetername varchar (50),  greeting varchar (255));  insert into greeting (greetername, greeting) values ('bas', 'Hello master');  insert into greeting  (greetername, greeting) values ('Jack', 'Hello slave');  select * from greeting;Now that we have setup our MySQL container and initialized the schema we are ready to create our REST backend service.Backend containerYou can find the code and Dockerfile for the backend container on the Github link earlier in the post under the /backend folder.The backend application itself is pretty simple.It just listens on port 8080 for requests on the path /greeting.You can pass the name request parameter to this path to get a customized response.The application fetches its greetings from the database container we set up earlier.I added following properties under src/main/resources to connect it to our database:    The important part here is that I referred to our database by using mydb:3306.We are able to do this because we will launch this backend service on the same docker network as the mydb container.This way, they will be able to resolve each others name by using the basic-bridge network we created.Next up, we will build the container from the Dockerfile I created.To do this run the following command in the /backend folder:docker build . -t rest-backendOnce the Docker image is built, let’s run it in a container:docker run --name=rest-backend --network=basic-bridge -p 8090:8080 -d rest-backendBy using the --network=basic-bridge, we attach the container to the basic-bridge network that we created earlier.You can look at the network details of this container by using:docker inspect rest-backend    We can see in the output that it is attached to the basic-bridge network just like our mydb container.We also used the -p 8090:8080 flag to expose the inner 8080 port (the port our Spring Boot application uses) to the outside world via the 8090 port.We can now curl or browse to localhost:8090/greeting to verify that everything is working:    We can see that our service successfully returned a response.Frontend containerWe can now create our frontend AngularJS application that consumes the REST service we just created.You can find the code for this application under the /frontend folder in the Github repository.Create the Docker image for the frontend container by running:docker build . -t angular-frontendNow let’s run the container by using:docker run --name=angular-frontend --network=basic-bridge -p 8080:80 -d angular-frontendAngularJS renders in the browser so it’s not rendered inside a Docker container.This means that our angular application would not be able to contact the REST backend container as that container is only known within the docker network.To work around this inconvenience I will explain 2 alternatives:  connect it to the internal Docker IP address of the rest-backend containerIn this case we connect the angular application to the ip address of our backend container. This only works because the ip address is known to our browser.The ip address is known because the machine on which our browser runs is also running the Docker network.Note that this is not very portable.If you would deploy the container somewhere else or you would browse to the application from another host this would break.      connect it to the public exposed port of the REST backend container.When we set the application up like this the angular application can access the backend container through the publicly exposed port 8090.Note that in this case the application would break as well if you deploy the frontend container somewhere else or if you would access it from another host.    The result in both cases is that our angular application can contact the REST backend and serves up a good response.    As I mentioned before both these alternatives have pretty obvious flaws in them.In a real world setup you would like to make your REST service publicly available on a webserver so that consuming applications would be able to connect to it by using that public URL.If there is interest in this kind of setup I can cover it in a later blogpost.Our application setup is now complete and our full setup looks like this:    ConclusionAs we saw in this guide it is actually pretty simple to create a single host Docker network and enable containers to communicate with each other over this network.When we created our Angular application, we saw that this approach has its limitations.Another limitation of this setup is that this kind of network is limited to a single host as it will not work over multiple hosts.Of course nobody wants to run a distributed application in containers on one host.I will make a follow-up blogpost where we look into Docker multi-host networks by using weave.NET.docker stop bas"
      },
    
      "conference-2017-11-15-javaday-ukraine-2017-html": {
        "title": "JavaDay Ukraine 2017",
        "url": "/conference/2017/11/15/JavaDay-Ukraine-2017.html",
        "image": "/img/javaday-ukraine-2017/javaday-ukraine-2017.png",
        "date": "15 Nov 2017",
        "category": "post, blog post, blog",
        "content": "  JavaDay Ukraine is an annual international two-day conference in Kyiv with more than 60 global speakers with various topics on Java, software architecture, machine learning, data science, and more.In this blog post we will go over some of the talks that we have attended.  Table Of Contents      Developing Microservices with Kotlin    Going Reactive with Spring Data - Christoph Strobl    Spring Boot 2.0 Web - Stéphane Nicoll    The API Gateway is dead! Long Live the API Gateway! - Spencer Gibb    Continuous Deployment to the Cloud using Spinnaker - Andreas Evers    10 tips to become an awesome Technical Lead - Bart Blommaerts    Hands-on introduction to CQRS and Event Sourcing with Axon Framework - Steven Van Beelen    Spring Cloud Stream — a new Rick and Morty adventure - Dieter Hubau    8 Steps To Becoming Awesome With Kubernetes - Burr Sutter  Developing Microservices with Kotlin - Haim YadidHaim Yadid is a developer, architect and group manager currently working as head of backend engineering in Next Insurance.In his search for a better programming language, he compared different strongly and loosely typed JVM languages such as Scala, Ceylon, Groovy, JRuby, Clojure and Kotlin.The chosen language would have to be concise, safe, versatile, practical and interoperable.Being a fan of strongly typed languages, Groovy and JRuby were no option.Scala was a good option but due to the complexity of the language, the long compilation times and lack of backwards compatibility assurance, it was also dropped.Kotlin proved to be the winner as it contained all of the above listed characteristics.It is also able to make use of the huge Java ecosystem and, being backed by Jetbrains, was very assuring.It also helped that Google made Kotlin the official language for Android Development. Not to mention it was the subject of 9 different talks at JavaOne 2017.In his talk, he wanted to share his findings and experiences when developing in Kotlin which he labeled as a huge success.The project he worked on contains a microservices backend over DropWizard deployed to AWS together with serverless endpoints in AWS Lambda.Used technologies, frameworks and libraries are amongst others Maven, DropWizard, AWS Lambda, PDFBox, XMPBox, Flyway, Stripe and Mockito Kotlin.Building the project was done via the Kotlin Maven plugin.He started with version 1.0.2 and immediately upgraded to every release which always went very smooth; even the migration to 1.1.0, which included Java 8 support, went without any issues.Onboarding new Java developers is never a hassle as they are capable of developing in Kotlin by the time they get to know the architecture.Haim really liked extension methods, which allow you to add functionality to an existing class or interface.The null safety, which is very similar to the null safety of Apple’s Swift - where nullability is part of the type of an defined object - was also well-appreciated.He also pointed out to us that Java open source libraries work extremely well with Kotlin. All you need to do is add the dependency to your build file and you are good to go.Data classes, similar to case classes in Scala, offering a concise way to define simple classes for holding data, were used for all their DTOs.Also worth mentioning is that IntelliJ has a converter functionality for converting a Java class to Kotlin.Obviously it’s mostly used as a starting point when migrating existing Java classes.We really liked Haim’s talk as we are very eager to try out Kotlin in a project.Haim’s presentation is available on SlideShare:   Building microservices with Kotlin  from Haim Yadid Going Reactive with Spring Data - Christoph StroblChristoph Strobl is a developer at Pivotal and is part of the Spring Data team.Starting from Spring Framework 5, reactive support was added to all the core Spring Framework projects.In a reactive architecture, it is important that your system is reactive from top to bottom in order to take advantage of the full performance gain, the persistence layer is no exception to this.During the talk, Christoph went over the classic imperative approach of a Spring application where Spring MVC is used and the performance problems that can arise when all threads are in use.A reactive architecture makes better use of server resources but in turn adds more complexity to your architecture.The publish-subscribe mechanism is heavily used in this architecture where, how can you guess it, publishers publish messages to which subscribers can subscribe.The mechanism also comes with back pressure for the subscribers, allowing them to define how many messages they want to handle next in order to avoid being overrun.It is important to note here that the reactive publish-subscribe mechanism is based on the push model. The subscriber will not actively fetch the data but will instead receive the data from the publisher who pushes the new messages to the subscriber when they’re available.In the other part of the session, Christoph went over several features of Spring’s Project Reactor, Spring Data Kay and Spring WebFlux.The publish-subscribe mechanism in Reactor is based on the Reactive Streams specification and there are two reactive types: Flux, an Asynchronous Sequence of 0-N items, and Mono, an Asynchronous 0-1 result.Spring WebFlux is the reactive brother of Spring MVC and uses Project Reactor under the hood for building reactive endpoints.Spring Data Kay is the newest version of Spring Data which now contains reactive repositories and reactive templates.At the time of writing this is only usable for MongoDB, Redis, Couchbase and Cassandra as the other databases lack a reactive JDBC driver.In the final part of the talk, Christoph held a demo of a Spring Boot 2 reactive application showcasing all the reactive features.All in all, a very interesting talk about building a reactive application using Spring.The demo code is available on GitHub.The presentation is available on Speaker Deck:Spring Boot 2.0 Web - Stéphane NicollStéphane Nicoll joined the core Spring Framework development team early 2014, being one of the main contributors to both Spring Framework and Spring Boot since.Stéphane’s session was all about Spring Framework 5 and Spring Boot 2.0.Spring 5 comes with Spring WebFlux which is the reactive brother of Spring MVC allowing you to build non-blocking APIs.He explained that there is always the issue of supporting all the different clients like desktops, laptops, smartphones and tablets, and their different internet speeds.Smartphones often have access to the slowest internet speeds and thus require the most optimal solution regarding bandwidth and performance.All the different concepts of building a reactive application with Spring Framework 5 and Spring Boot 2.0 were explained with a demo application called Smart Meter.Basically, you have all these different data inputs via sensors being gathered by an aggregator and then streamed to a dashboard.The frontend is written in Thymeleaf 3.0 which is the version in which reactive support was added.Besides the frontend needing reactive support, the persistence layer of the backend also needs it.In Spring Data Kay, reactive support exists for Redis, MongoDB, Couchbase and Cassandra.The other main databases such as Oracle, PostgreSQL and MySQL aren’t there just yet as they lack a reactive JDBC driver.In the demo, MongoDB is used.Stéphane also demonstrated some new additions to Spring Boot Actuator such as a unified way to implement custom endpoints, better output format,separate status endpoints (you now have /status and /health) and a simplified security model to specify who has access to (for example) status and info as users with a certain role may be allowed to see more.Properties in Actuator now also display the properties file in which they have been declared and the exact position.Stéphane concluded the talk by announcing that the release candidate was foreseen somewhere at the end of November.However, a recent tweet of his announced a small change to the release schedule:Spring Boot is having an extra milestone and RC1 is scheduled early December now.See https://t.co/6kOGdPMtfp&mdash; Stéphane Nicoll (@snicoll) November 9, 2017The demo code is available on GitHub.The presentation is available on Speaker Deck:The API Gateway is dead! Long Live the API Gateway! - Spencer GibbSpencer Gibb, Spring Cloud co-founder and co-lead, started by talking about the responsibilities of an API gateway.He started by revisiting Netflix’s Zuul which is servlet based and thus has blocking APIs, and referred to Mikey Cohen’s presentation on Netflix’s Edge Gateway Using Zuul in which Zuul 2 is also mentioned.Zuul 2 was supposed to be integrated in Spring Cloud but as it still hadn’t been released, Pivotal went with their own solution: Spring Cloud Gateway.It is built upon Spring 5, Reactor and Spring Boot 2.In order to have the gateway be non-blocking, there is a single event loop similar to how it is in Node.js.In another section, Spencer talked about the internals of Spring Cloud Gateway and the Spring Reactor features it uses.This involves the usage of classes such as HandlerMapping, WebFilter, Predicate, ServerWebExchange, PathPatternMatcher, RoutePredicateHandlerMapping and many more.As a filter to rewrite paths was commonly requested before, this was the first filter that they have written when implementing the Spring Cloud Gateway.Spencer also mentioned that they were also focusing on providing a simple API to write filters.Also neat to mention is that route configuration is now possible in YAML.In the final part, Spencer demonstrated an implementation of a Gateway showcasing the different ways of how to use the API to define different byhost, rewrite, hystrix and limited routes.He started off by visiting the legendary Spring Initializr webpage and created a Spring Boot 2 application with the Gateway dependency.httpbin is something that he is a big fan of, as it is really useful for testing whether for example the correct rerouting is happening and the right headers are being added to the requests.The presentation is available right here.Continuous Deployment to the Cloud using Spinnaker - Andreas EversAndreas Evers, principal Java consultant and Solution Expert at Ordina Belgium, held a session on Spinnaker for doing Continuous Deployment to the Cloud.Digital transformations usually require embracing a devops culture and adopting microservice architectures since without microservices, it is harder to go faster to the market.Moving your infrastructure to the cloud is possible via either IaaS or PaaS.With microservices, your deployment frequency explodes as it is way more flexible.Netflix for example deploys over 4.000 times per day and that number is still increasing.Andreas explained that cloud deployments are complex and that it is important to be able to do easy rollbacks.There is also the fact that we want to plan our deploy at the right time frame, preferably when traffic is lowest to have the least amount of users impacted.Andreas talked about a couple of other principles such as making sure that infrastructure is immutable, repeatable and predictable across the different environments through baking images or building containers by using for example Docker.Equally important are the deployment strategies like (rolling) blue/green (or red/black if that’s how you roll (pun intented). Looking at you, Netflix!).Using the blue/green deployment strategy you can deploy the new version right next to the old version. What happens next depends on how the strategy has been configured.Either the load balancer will reroute all traffic from the old version to the new version, or (if the rolling strategy has been configured) the traffic will gradually get rerouted to the new version. The last option is great for canary testing or smoke tests.A third principle is doing automatic deployments by defining a pipeline which is always possible by just scripting all of this yourself but this is usually rather brittle.This is where Spinnaker comes in to help you out with all of that.The internal structure of Spinnaker consists of a couple of microservices written in Spring Boot.Spinnaker fulfills all the principles we have just summed up and more:  It allows you to specify the pipelines together with all the different environments  It allows you to plan your deployments  You can configure and tune your deployment strategies  It has support for Chaos Monkey which allows you to test your system on how resilient and recoverable it is as VMs get taken out  It has canary analysis  Configuring, installing, and updating Spinnaker is done via HalyardSpinnaker is still heavily being worked on and there are a couple of nice features coming up:  Canary strategies  Rolling blue/green strategies  Declarative Continuous Deployment (config as code)  Operation monitoringFinally Andreas did a demo of Spinnaker using a simple application based on Rick &amp; Morty which is also available on GitHub.During the demo he demonstrated how you can set up pipelines, the deployment strategy, the traffic guard and canary testing in Spinnaker.People attending the talk were able to participate by going to the url to which the application was deployed in order to show how only a part of the traffic was routed to the new version.The slides of Andreas’ talk are available on Speaker Deck:10 tips to become an awesome Technical Lead - Bart BlommaertsBart Blommaerts, Application Architect at Ordina Belgium, presented a talk with tips on how to become a better and more awesome technical lead.Spoiler alert, there were actually 12 tips instead of 10!  Advocate for change:You should experience the same pain as your team.Try to work together more closely with the people in your team to see if there are any pain points or issues that your team members are experiencing.  Work through failure and success:It is inevitable that some things will fail now and then.It is important to prepare for failure and to take responsibility.Don’t finger point!Failing is an opportunity to learn and should be embraced.Success on the other hand, should be celebrated as early and as often as possible, and not only after the end of a project.You should celebrate for example the successful delivery of a sprint or when a feature has been completed.Congratulate your team and individuals often as it is good for the moral.  Stay technical:Reserve the right amount of time to code and review code.It is important to hold on to that technical vision and to see how your project’s code base evolves.At the same time it is important that you still grasp the technical aspect of your project as it will help you to make the right decisions for your project and your team.  Always available:You should always be available and easily approachable for your team members.According to Bart, your time should be spent about 35% on technical design, 25% with the business, 15% on project management and 25% on code.  Be a mentor of your team:As you have a key position in your team you should avoid being a strict ruler and decision maker, and try to make the best decisions for your team.Instead, try to be a mediator and a mentor for your team members.Effective delegation is important and try to hand out responsibilities to your team.Know when to give input, when to make decisions and when to step back.  Surround yourself with other technical leads:Each person is different and everybody has a different way to approach things.There is a lot to be gained by making use of cross-pollination and learning from other technical leads about how they approach and deal with things.It is important to be open for other approaches and to widen your vision.  Think big, bias for action:You should think big and differently.Try to focus on opportunities and to create a bold direction.Don’t be afraid to undertake action as actions are reversible.You don’t always need to do that time-consuming, extensive study before undertaking action as speed matters.  Interviewing potential new team members:Be prepared for interviewing potential new team members and be sure to go through the resumes.The mindset of a potential team member is more important than their knowledge of the tooling.You want the person to be eager to learn and to fit in your team.As for actually taking the interview, don’t blatantly copy questions from StackOverflow and expect the interviewee to come up with the exact same solution.Instead, first comfort the interviewee, offer them different options during the interview and try to build upon the responses they are giving.Show interest in the person and be sure to offer them a bonus question.  Embrace cultural differences:Everybody is different and diversity is invaluable.Have respect for everybody’s opinion and try to surround yourself with them as they offer you different points of view.Don’t forget that everybody in your team has the same shared end goal.If you are working with an offshore team, take the time difference into account.You can try to change your work hours to be more available to them.Focus on good communication and be sure to document the work and tasks well.  Estimating is hard:Bart quoted Hofstadter’s Law: “It always takes longer than you expect, even when you take into account Hofstadter’s Law”.In order to make more educated guesses, doing a planning poker can be useful.Define a sequence, set a baseline and allow reasoning.Don’t be afraid of uncertainty as it is inevitable.Bart suggests using the following formula: (O + 4*BG + P) / 6 where O is the optimistic estimate, BG the best guess estimate and P the pessimistic estimate.You should add 20% to the guess for properly testing, debugging, polishing, documenting and random wtf moments.Don’t forget, any estimate is better than no estimate, and make sure to share and review estimates.  Interfacing with the outside world:Adapt the way and the language when you communicate with non-technical people.Try to be the go-to-guy/girl for the management, the customer and other stakeholders.And very important: don’t be afraid to say “no”!  Facilitate (agile) team work:Be agile, use a prioritised backlog.Plan your sprints, use burn down charts and do sprint retrospectives.Your team’s strength is not a function of the talent of individual members but rather of their collaboration, tenacity and mutual respect.In short, facilitate an awesome team.The slides of Bart’s talk are available on SlideShare:   JavaDay 2017: 10 tips to become an awesome technical lead (v4)  from Bart Blommaerts Hands-on introduction to CQRS and Event Sourcing with Axon Framework - Steven Van BeelenSteven Van Beelen, Software Engineer at AxonIQ, held a hands-on session on CQRS and Event Sourcing using the Axon Framework which helps developers to focus on application functionality rather than the non-functional requirements of an application.The main advantage of event sourcing is that there is less info loss as you are storing all the different events, leading to the final state of records whereas in a classical example you only hold on to the final state of a record.By using event sourcing you also get a reliable audit log right out of the box.At the same time there is also a performance increase as events are processed in the background asynchronously, leading to faster response times.With event sourcing you mostly make use of a cache, as replaying events when looking up records can be time consuming.This is further compensated by making use of snapshots every 100 events for example.Some of the cons are that events are readable forever and that it is a lot of work if you decide to rewrite the event model and that you also have to think of versioning your model.Sourcing the model from lots and lots of events takes time but this is also resolved by making use of snapshots.The Axon Framework is open source (Apache 2 license) and supports concepts like DDD (Domain-Driven Design), CQRS (Command and Query Responsibility Segregation) and EDA (Event Driven Architecture).The framework helps you to focus on the business functionality as it takes care of the plumbing for you.The majority of the time was spent with live coding.Steven created a Spring Boot app with Kotlin.Useful to mention is that Axon has support for Spring Boot AutoConfiguration by adding the axon-spring-boot-starter dependency to your project.Axon will automatically configure the basic infrastructure components (Command and Event Bus) as well as any component required to run and store Aggregates and Sagas.Kotlin was chosen as it provides a concise way to write code, the data classes especially are very useful for writing commands and events as these are immutable POJOs.The application was about creating conferences and talks in order to demonstrate the framework.For the command model, it came down to marking aggregate classes using the @Aggregate annotation.In the aggregate classes, the identifier gets annotated with AggregateIdentifier.Your command handler gets annotated with @CommandHandler.This is the class where all the logic resides on how to handle all the different commands for the specified aggregate, usually resolving into events.Similar to the command handler, there is also an event sourcing handler annotated with @EventSourcingHandler containing the logic for processing the created events of the specified aggregate.Furthermore you have a controller and a command gateway.As for the query model, there is an @EventHandler that processes any events, updating your query models.The demo application is available on GitHub.For more information on the framework, be sure to consult the well-written reference guide.Spring Cloud Stream — a new Rick and Morty adventure - Dieter HubauDieter Hubau, principal Java consultant and competence lead Cloud &amp; PaaS at Ordina Belgium, presented his cool Spring Cloud Stream application featuring Rick and Morty.Spring Cloud Stream allows you to create message driven microservices and is based upon Spring Integration and builds upon Spring Boot.Briefly summarising the talk wouldn’t do it justice so instead we will link you to the blog post he has written on the topic available right here.The presentation is available right here:There is also a recorded video available on our JWorks YouTube channel, be sure to check it out!\t8 Steps To Becoming Awesome With Kubernetes - Burr SutterButter Sutter, Director for Developer Experience at Red Hat, gave a cool presentation about Kubernetes.After a lengthy introduction to DevOps, the challenges of creating and running microservice architectures and Kubernetes, we could dive into some of the more technical features Kubernetes has to offer.Burr introduced us to his eight step program to become awesome at Kubernetes:Step 1: InstallationBurr showed us the many ways you could setup a Kubernetes cluster, including Minikube or Minishift.There are plenty of guides on the web for deploying Kubernetes on any of the major infrastructure providers (AWS, Azure or Google Cloud), but there’s also the Kubernetes-as-a-service offerings from Google and Microsoft which can get you going very quickly.Running Openshift can be as easy as running oc cluster up on your local workstation, which sets up a local Openshift cluster for you using Docker.Step 2: Building imagesThere are several ways to build Docker images for Kubernetes or Openshift. We like the following ones:  The classical way would be to docker build a Docker image, push it to your local Docker registry and run it on a Kubernetes cluster using kubectl run ... or by creating a Kubernetes deployment using kubectl create -f deployment.yml  For Java applications, the Fabric8 Maven plugin can be used to build, run and debug Docker images on a Kubernetes cluster  You can use Helm Charts - think of Helm as yum/apt/brew for Kubernetes  If you’re used to Docker Compose and you have a lot of those config files lying around, you can use Kompose to convert them to Kubernetes config files  Openshift provides a way to create an image straight from your source code called Source to ImageStep 3: kubectl exec or oc execIf you want to find out what is going on inside of these black-box containers, you really should use the exec command which allows you to SSH into them so you can snoop around and learn about the internals of your applications.It’s very handy to debug applications, to figure out issues you might be having and to identify bugs or problems in advance.Step 4: LogsLooking at logs can make troubleshooting so much easier.Kubernetes allows the user to look at console logs for any pod in the cluster using kubectl log &lt;name-of-the-pod&gt; but it gets quite tiresome rather quickly.Luckily for us, the community has come up with another handy tool called KubeTail which allows us to tail multiple pods at the same time, with colorized output.Step 5: Service Discovery and Load BalancingIn Kubernetes, pods can be exposed as services, which are internal to the cluster but are really handy in inter-service communication.Inside of the cluster, services can address each other through DNS via their service names. A service called producer running on port 8080 can be called with the following URL: http://producer:8080.When creating a deployment containing a service inside of Kubernetes with two replicas, a ReplicaSet will be created for you, and all traffic to that deployed service will be load balanced automatically over those two replicas.For more specialized load balancing (for example when doing Canary Deployments) you can make use of multiple deployments.Step 6: Live and ReadyThis step is really helpful when just starting with Kubernetes, especially when you can’t figure out why your app keeps restarting over and over.Burr taught us about the existence of and differences between the liveness and readiness probes in Kubernetes.Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations.Sometimes, applications are temporarily unable to serve traffic.For example, an application might need to load large data or configuration files during startup.In such cases, you don’t want to kill the application, but you don’t want to send it requests either.Kubernetes provides readiness probes to detect and mitigate these situations.A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.Step 7: Rolling updates and Blue/Green deploymentsAs mentioned above for Canary Deployments, Kubernetes offers the possibility to balance load over different versions of your application.This makes it a great tool to manage rolling updates of new versions of your application, as well as Blue/Green deployments.Although I like the possibilities of the framework, I prefer to use other tools for this like Spinnaker or a service mesh like Istio.These tools will use Kubernetes for us, so that we don’t have to worry about changing and updating configuration files all the time.Step 8: DebuggingAs we’ve said before, debugging in Kubernetes is completely viable and there are several possibilities here.  Using kubectl exec to get inside of the container and live debug the application  Using the Fabric8 Maven plugin to debug the application  Even Visual Studio Code now has the possibility to live debug a running Java application  Red Hat JBoss Developer Studio can be used to debug Kubernetes or Openshift applicationsAs a bonus for his talk, Burr also explained one of the new service meshes out there called Istio.It promises to deliver an open platform to connect, manage, and secure microservices.Using a sidecar proxy inside each pod called Envoy and some governing tools such as Istio Pilot and Mixer, it solves many of the problems that microservice architectures pose, such as secure inter-service communication, service discovery, circuit breaking, intelligent routing and load balancing, etc…This seems like a very promising technology inside of the Kubernetes and Openshift world and we will keep a close eye on it.There was a talk from Ray Tsang at Devoxx about Istio which was very interesting and entertaining, as always.You can find Burr’s presentation right here on Google Docs."
      },
    
      "architecture-2017-10-20-ordina-architecture-kata-html": {
        "title": "First edition of the Ordina Architecture Kata",
        "url": "/architecture/2017/10/20/Ordina-Architecture-Kata.html",
        "image": "/img/kata/kata-6-thumb.jpg",
        "date": "20 Oct 2017",
        "category": "post, blog post, blog",
        "content": "  On the 18th of October 2017, Ordina Belgium organized the first Ordina Architecture Kata.The session was presided by Bart Blommaerts, cross-unit Competence Manager Architecture.A group of sixteen senior consultants, with different areas of expertise, were gathered in Mechelen to practice software architecture.What is a Kata?Kata is a Japanese word most commonly known for the presence in martial arts.The English term for Kata is form and it refers to the detailed choreographed patterns of movements practiced either solo or in pairs.You might know the saying practice makes perfect, and Architectural Katas are exactly that: practicing.These Katas were born out of a simple desire — Software architects need a chance to practice being software architects.  “So how are we supposed to get great architects, if they only get the chance to architect fewer than a half-dozen times in their career?” - Ted NewardPragmatic Architecture Today - RecapIn his conference talk and blog post Pragmatic Architecture, Today, Bart Blommaerts discusses the need to think about Software Architecture.Since this is very relevant to this Architecture Kata, we recap quickly what we learned back then.Why do we need an architecture?We need to build a system.A system is build for stakeholders.Customers, users, developers, … are all stakeholders of a particular system.Those stakeholders need to have a clear view on what needs to be built.Every system has an architecture, even those where architectural decisions weren’t formally made.An architecture is described in an Architectural Description.This description is also particularly useful for the stakeholders.An Architectural Description uses views, which are shaped by perspectives.OODAOODA is a re-entrant feedback loop, that consists of four stages:  Observe: Listen to customers, gather requirements, available resources, …  Orient: Assess comparable systems, use your experience to make sense of your earlier observations.  Decide: From the orientation stage, multiple alternatives might need to be considered. In the decision stage, we take a decision.  Act: Act on your decision, implement.An exercise that can help you in the different stages, is to start with some bullet points and then writing them out explicitly.Comparing the full text with the bullet points, will often be very insightful.To reach consensus when taking decisions, share these with customers, peers, … and verify if they share your ideas.Visualization of the architecture  “One cannot see the ocean’s currents by studying drops of water” — Grady Booch.To a certain amount, you can derive business logic from the code.One might say that the code is the truth, but not the whole truth.Goals of visualizing your architecture:  Consistency  Reporting — Architecture needs to be in the heads of the stakeholders  Checking and validating — Share the architecture with your different stakeholders  Share information — Other people might have experience with certain challengesUnified Modeling Language (UML)Using a language like UML can be useful, especially when doing model-driven development. Also, be very aware that this way of working can become very inefficient.When you are not doing MDD, UML can still be used, if there is shared understanding of the created diagrams.Boxes and linesBoxes and lines are a possibility too, and Bart recommends this more pragmatic approach.Don’t make things more complex than they need to be, boxes and lines are fine.Just make sure to be consistent and always provide a legend.Also make sure your stakeholders understand what you’re drawing.A legend will really help with getting the message across.It’s important that you can discuss a matter while speaking a common language.Avoid fluffy diagrams and mixed abstractions.Don’t mix eg. user interaction information with data flow information.Decision logDocument your decisions and alternatives in a Decision log, also known as Architecture Decision Record (ADR).It will prove itself useful in the future and requires you to think about a decision.There’s no need to invent the wheel here.There are several templates for different use cases available on the internet, for example in this ADR repo on Github.Only document what’s useful.ViewpointsViews help you to make architectural decisions.Bart explained the different views with sharp-cut examples.Context View — Describes the relationships, dependencies and interactions between the system and its environment.Added in the second print of the book.Bart thinks this might be the most important view of them all.Every component is a part of the greater system.Functional View — Defines the architectural elements that deliver the systems functionality.It documents the systems functional structure.You can make decisions on a functional level eg. two components are doing similar things.Should they be separate components?Information View — Models the system data and its state.The purpose of many applications today is capturing data.  Sidenote: Data modeling can be a long and complex process.As an architect, you need to do data modeling at an architecturally significant level of detail.Go to the level of detail that is needed for your team of developers.Concurrency View — Describes the concurrency structure of the system, mapping functional elements to concurrency units to clearly identify the parts of the system that can execute concurrently eg. process a file in blocks.You can solve a lot with specific language constructs and asynchronous messaging.If you want to dig deeper and want to know the nitty gritty details of messaging, a must-read is the book Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf.Development view — Describes the architecture that supports the software development process.When you have an experienced development team, this can be very high-level.Make sure you include the senior developers in the team, when constructing the development view.They have the experience and on top of that… they will be more motivated to be part of the decision making process and technical vision.Deployment view — Describes the physical environment into which the system will be deployed, including the system dependencies on its runtime environment.Make sure you include all information relevant for deploying the application, eg. OS, Apache HTTPD, Tomcat, etc.Operational view — Describe how the system will be operated, administered, and supported when it is running in its production environment.You can use a state chart to describe the operations process.PerspectivesPerspectives shape the views for non-functional requirements.When you introduce perspectives, you’ll have to make trade-offs.An architectural decision will favour certain perspectives and at the same time, hinder other perspectives.For example, strong encryption favours security but hinders performance.Here’s a list of very plausible non-functional requirements:  Accessibility — Ability of the system to be used by people with disabilities.  Evolution — Ability of the system to be flexible in the face of the inevitable change that all systems experience after deployment, balanced against the cost of providing such flexibility.  Location — Ability of your system to overcome problems brought about by the absolute location of your system’s components.  Performance and scalability — Ability of the system to predictably execute within its mandatory performance profile and to handle increased processing volumes.  Regulation — Ability of the system to conform to local and international laws, quasi-legal regulations, company policies, and other rules and standards.  Security — Ability of the system to reliably control, monitor and audit who can perform what actions on what resources and to detect and recover from failures in security mechanisms.  Usability — The ease with which people who interact with the system can work effectively.The KataOur kata for today — AM.I.SCK  Nurses that answer questions from patients via a chat platform.  250+ nurses  Access to medical histories  Assist nurses in providing medical diagnosis  Reach local medical staff, even ahead of time  Enable parts of the system for direct patient usage  Conversations are not considered medical recordsThe sixteen attendees were divided in groups of four.Each team had fifteen to twenty minutes to brainstorm about the case and create the first four views on a whiteboard together.Afterwards, each team had to present their views to the entire group.Bart challenged our opinions and gave practical tips on how to improve our thinking.After a second theoretical deep dive about how perspectives can have an effect on your views, we did the same excercise for the last three views.TakeawaysThe different viewpoints really complement each other.When drawing a view, you’ll notice that you might be able to add more information to another view and vice versa.When drawing a context view, focus on the interactions with other systems.Don’t be tempted in drawing eg. a frontend and a backend component for your system, unless these are separated by external systems.That granularity is not important for the context view.One view can contain several diagrams (eg. you can have multiple state diagrams in the Information View), additional text, tables containing data, etc.Use the experience of every team member to draw the diagrams.Think of similar projects and previous professional experiences.Ordina Accelerator 2018This course was part of the Ordina Accelerator program.With Accelerator, Ordina offers its employees the necessary tools to develop themselves further.Not only technical-, but also social- and organizational skills are included in the program.Medior and Senior experts get the chance to literally accelerate their career by extensively following courses and workshops over a period of two years.AssignmentAt the end of the kata, the participants received their assignment.Upon succesful completion of this assignment, they received an internal Architect Associate certificate.This badge is a pre-requisite to participate in the professional and master programs.The following people have earned the Architect Associate badge:  Andreas Evers  Benjamin Haentjes  Frederick Bousson  Geert Clissen  Jochen Van de Voorde  Johan Buntinx  Jos Clijmans  Ken Coenen  Kevin Bosteels  Koen Vanden Bossche  Mario Van Hissenhoven  Martin Kwee  Sebastiaan Tempels  Stijn Dierckens  Sven BovensThe list will be updated, when more people receive the badge.Their results, will help shape to future of the Ordina Architecture Katas.Links and resources  Recommended reading: Software Systems Architecture by Eoin Woods and Nick Rozanski.In this book, they discuss Viewpoints and Perspectives  Architectural Katas on neilford.com  https://archkatas.herokuapp.com/  ArchitecturalKatas Google User Group  Liminal Thinking  Enterprise Integration Patterns"
      },
    
      "tech-2017-10-18-javaone-html": {
        "title": "JavaOne 2017",
        "url": "/tech/2017/10/18/javaone.html",
        "image": "/img/j1.jpg",
        "date": "18 Oct 2017",
        "category": "post, blog post, blog",
        "content": "The last time I visited JavaOne was back in 2014.So, I was very excited to go back to San Francisco this year.TakeawaysJDK9JDK9 was the “big topic” of JavaOne, with Jigsaw getting a lot of attention.Jigsaw might break some code (eg. code that uses internal Sun APIs), but Java modularity will surely help further adoption of Java.Many libraries and frameworks already work together nicely with JDK9.We saw this in a nice demo of IntelliJ where the IDE does a lot of the Jigsaw heavy lifting.Another interesting change, is that Java will have a new release every six months from now on.These releases will also introduce a new numbering scheme, comparable to what Ubuntu has been using.While I applaud the idea to deliver faster, it will definitely come with a learning effort, for developers.The Good Cop/Bad Cop Guide to Java 9A very entertaining talk on JDK9 was the one from Simon Maple and Oleg Šelajev from Zeroturnaround where they discussed pros and cons of JDK9 modules, G1GC, JShell and other API updates.It was probably one of my favorite talks, because it was both funny and educational.FN ProjectDuring the first Java keynote, the FN Project was announced and open-sourced.The FN project is a container native serverless platform that you can use on any platform supporting Docker.This also means that local development becomes very easy, which isn’t always the case with other serverless solutions.It has out of the box support Java, Go, Ruby, Python, PHP, and Node.jsI definitely look forward to playing with it.Microservices BOFIn the microservices BOF on Monday evening, Chris Richardson presented what must be the perfect summary of the current state of everything going on in the microservices landscape.He created a pattern language for microservice architectures at microservices.io that I find particularly interesting.After Chris, Josh Long proved that he can bring a 45 mins talk in 15 mins with only live coding which was very amusing to watch.JavaOne surely showed us that microservices are still hot!ACID Is So Yesterday: Maintaining Data Consistency with SagasWhen creating a distributed system with microservices, using a database per microservice is generally seen as a best-practice.A drawback of this approach is that transaction management becomes a big hurdle and ACID (Atomicity, Consistency, Isolation, Durability) is no longer a desirable option.An alternative for ACID, is BASE: Basically Available, Soft state, Eventual consistency.BASE can be realized with sagas.A saga is a sequence of local transactions.Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga.If a local transaction fails because it violates a business rule then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.Clouds and containersAs expected, a lot of talks focused on cloud deployment and operation.One of the more interesting ones, compared the different container orchestration options and runtimes like Docker Swarm Mode, Kubernetes, Amazon ECS, Mesos/Marathon, Rancher and Triton.The talk demoed some differences between (for example) Docker Stack and Kubernetes, like the scaling of pods vs. the scaling of services.To make a correct choice between all options, it is imperative to take consistency, portability, build integration with CI / CD, community and transferability of skills into consideration.Source code of the comparison is available here.My TalkMy talk: 10 Tips To Become An Awesome Technical Lead was scheduled on Thursday in the exquisite Marriott Marquis.From the start, I had a lot of interaction with the attendees and that obviously is very motivating.Thanks a lot for that!I uploaded my slides on Slideshare.Networking EventsSilicon ValleyAfter landing in San Francisco, my colleague Andreas, picked me up at the airport and we went to visit some of the most famous tech companies in Silicon Valley.Silicon Valley, in the southern San Francisco Bay Area of California, is home to many start-up and global technology companies.Apple, Facebook and Google are among the most prominent.The size of some of these companies really was remarkable.Visiting the HP garage, the Android statues etc. was a really great way to cope with jetlag :)      Oracle OpenWorld Benelux Bike Tour 2017Sunday morning, Oracle Benelux organised a Bike Tour for the Benelux attendees, which I liked a lot.We biked the typical San Francisco route: Fisherman’s Wharf, Fort Mason, the Marina, Golden Gate Bridge, Sausalito and then back by ferry.Three years ago, I did a similar tour, but did not take the ferry back.The ferry was actually very nice: passing Alcatraz and watching the San Francisco skyline pop up.  Social events and partiesThere are a lot of parties happening during JavaOne.One of the highlights, for me, was PartyOne 2017 by ZeroTurnaround, Hazelcast, Tomitribe and BainCapital.This was an incredible opportunity to talk to some of the really big names of the Java industry in a very relaxed atmosphere as you can see in the following picture :)  Another fun event, was Oracle Cloud Fest: a concert of Ellie Goulding and The Chainsmokers in the AT&amp;T Park.We teamed up with our Ordina colleagues from the Netherlands to enjoy our last night in San Francisco.  "
      },
    
      "iot-2017-10-12-stairway-to-health-html": {
        "title": "Stairway to Health with IoT and the MEAN stack",
        "url": "/iot/2017/10/12/Stairway-To-Health.html",
        "image": "/img/stairwaytohealth/stairway-to-health.jpg",
        "date": "12 Oct 2017",
        "category": "post, blog post, blog",
        "content": "  Healthier at the office with the ‘Internet of Things’.What is Stairway to HealthIn an effort to improve worker health in a fun and engaging way, Proximus wanted to encourage their employees to take the stairs instead of the elevator.This is when the idea of a little game between the three towers came along. On different dashboards across Proximus and on the Stairway to Health website, the employees could see which tower had the most employees taking the stairs.They can also get a more detailed look of how many people taking the stairs where and when, with drilldown views for monthly, weekly, daily, and even hourly statistics.What does it do?The Stairway to Health project is a simple yet great example to show what the Internet of Things can do:  LoRa sensors detect door openings, these are installed on the doors of the staircases  These sensors communicate via the Proximus LoRa network to report their status  Sensor data is sent to the Proximus MyThings platform which processes the data  The data gets sent to the Stairway to Health application  The Stairway to Health application interprets and visualizes the dataIn summary: We install sensors on the doors (things) to measure usage and we analyse the data to persuade people to move more. The result is a good example of how IoT can influence our daily lives. Proximus was able to provide us with all the necessary building blocks to offer a complete end-to-end solution!MyThings and Stairway to HealthMyThings is the Proximus IoT platform for onboarding, managing, configuring and monitoring IoT sensors. By registering (onboarding) our sensors to the platform, we can let MyThings take care of decoding the messages and set up a stream to our application.This way every time a log comes in from the sensor, we get the decoded data posted to our designated endpoint.The Requirements  The usage of the stairways is measured and the results should be visualized on large screens in the towers.  These screens should have a QR code so that employees can easily visit the application on their mobile devices.  When visiting the website, they should be able to click on the results to get a more detailed view of the data.  The frontend application should be available in Dutch and French and the dashboard should switch between these languages every minute when viewing it on the large screens.  Admins should be able to manage locations (towers) and chart timespans.  It should have an info page with some information about the project and its purpose.So technically this translates to build an application that:  Has an endpoint to receive logs from the MyThings Application,  Stores the data to its own database,  Show the data in charts that have multiple layers to see more/less details,  Shows the ratio of the results per tower,  The frontend dashboard data has to reload automatically (since it is shown on some big screens @ Proximus),  Add multi-language (automatically switch languages when viewing on tower’s large screens),  Is performant (able to handle many logs coming in and calculate the data to be displayed in the graphs),  CRUDs for managing timespans and locations,  Use the timespans / locations when displaying data.Oh, and did we mention we were only given four weeks to complete this mission…The IngredientsSo given all the requirements listed above and the fact we didn’t have a lot of time to waste, we chose to use a MEAN (TypeScript) stack. MEAN stands for MongoDB Express Angular and NodeJS. It’s possible to use the mean stack with plain JavaScript, we chose to implement it with TypeScript since we wanted some strong typings on the backend application and we were going to use Angular 4 on the frontend which comes with TypeScript as well.NodeJs:Write event driven applications with asynchronous I/O powered by the ultra fast Google V8 Engine. Mostly known for running your local dev environment and automating build tasks for frontend developers. NodeJS is probably one of the best and easiest options out there for real-time applications (with socket.io), which is exactly what we needed for our application.MongoDB:Great to work with when dealing with JavaScript Objects. Good driver support with Mongoose for NodeJs. Document based structure, which makes it really flexible when it comes to modelling and it’s extremely scalable. We also took advantage of the very performant aggregation functionality for dealing with large amounts of data.ExpressJS:A node framework that comes with some great functionality for setting up your node server and makes it easy to create routes, middleware, handling requests/responses, serving files from the filesystem, configuring static files, easy connections to the database, and much more.Angular(4):A TypeScript-based open-source frontend web application platform led by the Angular Team at Google and by a community of individuals and corporations to address all of the parts of the developer’s workflow while building complex web applications.Socket.IO:Socket.IO enables real-time bidirectional event-based communication. It works on every platform, browser or device, focusing equally on reliability and speed. To trigger events on our frontend application we used this great library to be able to detect when new data has been received and refresh the dashboard.Highcharts:Interactive JavaScript library for creating dynamic charts. Highcharts is based on native browser technologies and not reinvent the wheel. Thousands of developers have contributed their work for us to use in our own projects. Also backwards compatible for IE.JavaScript across the stackNot only does it make development quite a bit faster and easier by having a large community with lots of reusable code for your application (npm), it also lowers the barriers between frontend and backend developers by using the same programming language over the entire stack, so more efficiency and faster, leaner development which in turn means lower development costs. Also worth noting is that JavaScript currently is THE most popular programming language, so more developers will be able to easily understand and contribute to the application if needed. And probably the most important criteria: when it comes to cloud hosting, RAM is probably the main influencing factor when it comes to pricing. NodeJs uses less RAM than comparable Java applications.Source and more about these testsNow that I’ve listed some of the pros of full-stack JS, I should also mention that it might not be the best solution for computation-heavy backend applications.For projects like machine learning or heavy mathematical calculations the single CPU core and having only one thread that processes one request at a time might be easily blocked by a single compute-intensive task. Yet, there are numerous ways to overcome this limitation. By simply creating child processes or breaking complex tasks into smaller independent microservices.Let me just note that the comparison with Java above here is not because we are claiming that one is better than the other, it’s just to demonstrate that they both have their use cases and can be equally worth considering when choosing a technology for your application.Some great use cases for JavaScript across the stack are:  real-time chat,  Internet of Things,  real-time finance (stocks),  monitoring applications,  event-driven applications,  server-side proxies,  many more…Blocking vs. Non-BlockingIn NodeJs you can take advantage of JavaScript promises. One of the benefits of this is that we can write non-blocking code.To demonstrate how this works, I’ll give you an example in pseudo code for reading a file from the filesystem.Blocking:read file from filesystem, set equal to \"contents\" print content do something elseNon-Blocking:read file from filesystem &nbsp;&nbsp;&nbsp;&nbsp;Whenever we're complete print contents (callback)   do something elseSetting up our dev environment / buildThe frontend part of this was really easy. We used angular-cli to generate a new project. In the future this also gave us the advantage of generating new components, services, pipes, testing and much more. Also for the charts and translations we choose for easy to use libraries like Highcharts and ngx-translate (previous ng2-translate).For the backend we decided to go with gulp. We added some tasks to transpile our server site TypeScript files to JavaScript so that node can execute it. For local serving we created a sequence task that combines running ng build from the angular-cli and a gulp task to use nodemon for running our server and restarting on changes. When working on the frontend, doing an ‘ng build’ was a bit too slow, therefore we added a --standalone flag, to the serve task so that we could just build the backend application and do the frontend serve with ng serve which is a lot more performant than having to do a ‘ng build’ on every change.Since we are using TypeScript throughout the application, it only felt right to use the TypeScript version of gulp as well. It takes a little effort to get used to, but once you get the hang of it, it makes writing gulp tasks a lot more fun and less error prone.Using the provided decorators, our gulp tasks look something like the following:@Task()    public environment(cb) {        return gulp.src('./dist/app/server/config/mongo.connection.js')                   .pipe($.if((yargs.env === 'prod'), $.replace('mongodb://localhost:27017/stairway', require('./secrets').mongoUrl)))                   .pipe(gulp.dest('./dist/app/server/config'));    }and create sequence tasks with:@SequenceTask()    public mocha() {        return ['buildApp', 'runMochaTests'];    }Now that we have a gulpfile.ts file, we need to ensure that the gulpfile gets transpiled as well, we did this by adding an npm script, so that we can use TypeScript compiler with the tsc command to transpile the file and make sure we are using the latest changes every time we use gulp.(to get the tsc command, install typescript globally with npm)Building Stairway to HealthAfter setting up our dev environment, database and getting a simple application up and running it’s time to start implementing our first features.Receiving data from MyThingsSo first things first, on MyThings we took a look at how we were going to structure the data that was going to be streamed to the Stairway to Health application.In the MyThings application every sensor can have a friendlyName1 and friendlyName2, we used these to specify which tower and which floor they represent. The sensors send a lot more data than just the magnetic pulse counts, therefore we needed the container field, to be able to filter on counter logs only (however, we store the other messages as well, maybe for future use). The value field is the amount of times the sensor was triggered, in other words, the actual counts. And of course a timestamp since we will show the data in time based graphs.The timestamp represents the time that the sensor has sent its message to the MyThings application, we also wanted to keep track of when our application has received the log, so before saving we added one extra field to store this in our database.After we defined our model/schema of our logs, it was simply adding an endpoint to our express router and our first feature was ready. Well not exactly, we needed to trigger an event to refresh the data on our dashboard, but we’ll get back to this later.The DashboardSince we created an Angular(4) application, we took advantage of the great features of angular-cli which makes it really easy to get a new project up and running and generate new components, services, tests and much more. We started by adding all the components needed for the application and adding the Proximus styling to the project. After that we imported the Highcharts library from npm to first make the charts on the homepage and later making the charts for the detailed views. All the charts were first made with mock data so that we could perfectly say from the backend what data we needed and in which format. From now on we knew how our JSON for the charts had to be made and we could implement the api endpoints for the dashboard and the details page. Finally after adding all the charts we started on adding the different languages to the application. Here we got our biggest ‘lesson learned’, it is much faster to start with I18N then to end with it, this is because you have to find all the normal text in the HTML files and copy them to the JSON-files. ALso we had to quickly create a translation list that the business could translate for us.Mongo AggregatesAs for displaying the daily, weekly and total counts below the buildings, we had to get this data from the database, keeping in mind that we would have to iterate over millions of sensor logs (at the time of writing this blog post, 1.4 million over 4 months). We had to make sure it was performant. This is where the Mongo aggregates come in handy. Instead of looping over the results and adding them up, we let Mongo take care of this with the $sum operator which in code looks like the following:this.sensorLogModel.aggregate([                {$match: {container: 'counter', value: {$ne: 0}}},                // group them by fn1 (tower) and add up all 'value' fields                {$group: {_id: '$friendlyName1', total: {$sum: '$value'}}}            ]);Remember, we store all the logs, but we only need counter logs. So for a little more performance, we leave out the ones with value 0 (a lot of them in the weekends), that’s what the $match is forThe result: an array with objects that have an _id field with friendlyName1 as value and a total field with the sum of all (counter) values per tower. We repeat this for daily and weekly, but add a start and end date (which we simply create with TypeScript). $match then looks something like this:{$match: {container: 'counter', value: {$ne: 0}, timestamp: {$gt: start, $lt: end}}}Later on we added some more calls to get the data by time span and location for the more detailed chart data, but you get the idea, we simply edit the timestamps or friendlyName1 (also by friendlyName2 on the hourly chart, which displays the hourly data per floor).Socket.IONow that we have data that can be retrieved and displayed on the frontend, time to implement some way to let our frontend application know when we receive some new data, so that it in turn can do a request for that new data.For this one to be clear we’re going to skip ahead in time and show a high level scheme of how the application is made up.    The bin (js) file is where we create our http, https and socket servers. To communicate between them, we use the node event emitter. The server.ts file (let’s call it the app) gets bootstrapped onto these servers and when creating the app, we pass the created event emitter to it. This enables us to listen and broadcast events back and forward. The event emitter emits events between the backend services and the socket.io server emits events to our frontend application.So in our case, to let the frontend know when the sensor-log endpoint has received a message, we emit a log-received event on the node event emitter. In the socket IO server we are listening on this event and we broadcast a data event to every connected frontend application. The frontend applications are listening for this data event and refresh their data by calling the dashboard endpoints.However, since we have about 60 sensors sending data, this event was triggering quite a lot and with the chart rendering animations on our frontend application we had to wrap the log-received in a timeout so that we would only refresh it once every 30 seconds (if a log was received).I’ve picked a few lines of code from our bin file to demonstrate how we pass the eventEmitter when bootstrapping our application on to the http and https services from node.const server = require('../dist/app/server');const http = require('http');const https = require('https');const events = require('events');const eventEmitter = new events.EventEmitter();const httpServer = http.createServer(server.Server.bootstrap(eventEmitter).app);const httpsServer = https.createServer(options, server.Server.bootstrap(eventEmitter).app);After that, we bootstrap the created https server on to the socket.io application. It too gets the same EventEmitter instance passed into its constructor.const io = require('socket.io')(httpsServer);const sockets = require('../dist/app/sockets');const ioApp = sockets.Sockets.bootstrap(io, eventEmitter).io;In our sockets file, the method that gets executed will listen on the logsReceived from our passed EventEmitter, and emits a data event on our io instance.public sockets(eventEmitter, io){    eventEmitter.on('logsReceived', (logs) =&gt; {        io.of('/socket/').emit('data', logs);    });}Configuration CRUDSince we did not want our configuration to be hard coded, we added some configuration screens to be able to change the time spans and entities (towers).                                    By the way, ‘gewicht’ in the first image stands for weight. To make sure the ratios are fair, we made sure that every tower has a ‘weight’ to multiply its log values by. These weights are calculated by the amount of employees/tower, with the largest tower having a weight of 1.Let’s take a look at how we set up our backend structure for creating crud endpoints.In our /routes directory we keep all files that define the urls and methods of every endpoint, and tell it which controller and method to use:timespan.route.tsrouter.get('/timespan/', (req: Request, res: Response, next: NextFunction) =&gt; {    this.timespanController.getTimespanList(req, res, next);});router.post(('/timespan/', this.authenticate, (req: Request, res: Response, next: NextFunction) =&gt; {    this.timespanController.createTimespan(req, res, next);});next under our /controllers directory we have our controllers where all our functionality/logic istimespan.controller.tspublic getTimespanList(req: Request, res: Response, next: NextFunction) {    return this.timespanModel.find({}, [], {sort: {start: 1}})    .then((result) =&gt; {        res.json(result).status(200);    }, (err)=&gt;{        res.statusCode = 500;        res.statusMessage = err;        res.send();    });}AuthenticationTo prevent everyone from changing these configurations of course we had to add some authentication functionality. As you can see in the router code above, we created an authentication middleware so that on every route that we want the user to be authenticated, we can simply add this.authenticate() to the route. This checks a JWT token in the headers. We check the token to be valid. If it’s not valid, we send an unauthorized response, and if it is valid, we decode it and add it as a user object on the request. This way we can access it in the controller and do some logic depending on its role, etc.this.authenticate is a method we added to the core.route.ts.Every route extends this super class so that we can put common code and middleware in this file.JWT stands for JSON Web Token and is a JSON-based open standard for creating access tokens that assert some number of claims. For example, a server could generate a token that has the claim logged in as admin and provide that to a client. The client could then use that token to prove that he is logged in as admin.DeployFinally we deployed it to the Proximus data center and watched the Proximus employees take on the challenge.                                ConclusionAfter four hard weeks of working and writing many lines of code, we delivered our project to Proximus and the contest could start. Things we would have done differently:  Use mongo indexes and aggregation for large amounts of data  Use javascript date in stead of timestamps in mongo, easier to create aggregate with dates  Dockerize! So far, the most work has gone into getting the application deployed  Implement I18N translations at the beginning, as it is better to add translations while working on the component  Also we learned how complicated it can be to have one component with multiple switching charts. Instead of switching components."
      },
    
      "conference-2017-10-09-perconalive2017-html": {
        "title": "Percona Live 2017 Dublin",
        "url": "/conference/2017/10/09/perconaLive2017.html",
        "image": "/img/percona-live-2017/logo2.png",
        "date": "09 Oct 2017",
        "category": "post, blog post, blog",
        "content": "    Percona Live Europe is a yearly conference on open source database organized by Percona.We had the opportunity to attend this year’s conference in the beautiful city of Dublin.Beside enjoying the local brews and drafts we attended several sessions out of which we highlight some sessions in this blog post.MongoDB Shootout: MongoDB Atlas, Azure CosmosDB and Doing It YourselfWhen running MongoDB in the cloud, you have several options. David Murphy compared MongoDB Atlas, CosmosDB and the good old DIY.Using Atlas, you get monitoring, automation and the possibility to pay for backups.You pay per instance and you have a wide variety of instances and regions to choose from on AWS, GCP and Azure.The biggest downside is that the cost is about 44% more than running you own servers in the cloud.This means that, in contrast to DIY, you continuously pay more instead of writing off your initial investment and paying less in the end.The monitoring is really good but the problem here is that if you have have a polyglot environment with your own monitoring and alerting, you can not integrate it with Atlas so you end up with yet another tool.Upgrading is really easy and just a click of the button thanks to the automation.Backups need to be paid for per GB and are taken continuously.CosmosDB is offered by Microsoft on Azure and claims to be MongoDB compatible. This is not completely true because they have no support for the aggregation framework.So you can only use it for simple CRUD operations.The pricing is based on a pay-per-operation model which means it’s really hard to figure out what your cost will be and how it will evolve over time.Here you also have the downside of continuously paying more than DIY.The monitoring is very basic and of no help when you run into problems or strange behaviour, it’s like a black box.Upgrading is done behind the scenes which means you don’t have to worry unless the upgrade means your code is no longer compatible, then you are stuck.Backups are very basic because they are taken approximately every four hours and only the latest 2 backups are stored.DIY has the most power to offer IF you have a mature, and complete DevOps team.With DIY you pay a high price up-front for hardware and people.You need to implement your own monitoring, i.e. with the Elastic stack or Percona’s PMM.Also, automation is a big part of the effort.And last but not least, you need to implement your own backup strategy.The biggest upside is that you have full control over what you implement and how you do things.You choose the cloud service provider or the hardware you want to use.You choose what you want to monitor and how you alert.You choose how often and when you backup.But of course, it’s all up to you.Visualize Your Data With GrafanaGrafana is used to build monitoring dashboards based on time series data.Grafana supports a wide range of data sources to get its data and generate the dashboard.There are already a lot of pre-built dashboards you can use and customize for your own needs.You can build your own dashboards with panels.Each panel is fed with data from a datasource based on a query.Grafana has created a query editor with support for different data sources, like PromQL for Prometheus, to make it easier to build the queries you need.The end result can look like this :Database Reliability Engineering: What, Why and How?The DBA from the old days, hidden in a basement behind closets, performing magic on that mysterious thing called a database, is dead.Enter the Database Reliability Engineer (DBRE, loosely based on SRE), a person who embraces the new paradigms in the IT-world.He is an advocate of how data should be treated and used, he teaches his colleagues, he takes part in pair-programming, he is an active team member in cross-functional teams.A DBRE’s knowledge is not confined to a single system, he can support polyglot persistence.He can support these systems on premise and in the cloud.He automates as much as possible and uses tools of the trade including source control systems and helps creating infrastructure as code.The DBRE enables his organisation to apply known principles of the software engineering world to the database world.In this role he applies principles from Database Reliability Engineering, like designing for scale, availability, operations and performance.Also visibility, alerting and database change and release management are just a few parts of the tasks to do.For more detailed information make sure to check out the book Database Reliability Engineering, a must read for everyone in the field.MongoDB Security ChecklistMongoDB has been in the news lately due to MongoDB ransomware attacks. This might make you wonder whether or not MongoDB is secure.Well, rest assured it is very secure.But you need to turn security on, at least until the next major release where security will be on by default.MongoDB has a plethora of security features in their community edition and the commercial offering provides even more goodies like LDAP integration and baked in encryption-at-rest.It starts with simple username/password authentication and moves on to x.509 certificates based authentication.Once authenticated you have authorization with either build-in or user-defined roles and privileges, so you can fine-tune which users have access to which database or collection and which actions they can perform on them.You can further lock-down your MongoDB by fixing the network interface it is listening to so it’s not open to the internet, or encrypting the communication between replica-set or sharded-cluster members.If you are running MongoDB, then reading the security checklist is a must!Improvements to MongoRocks in 2017MongoRocks is MongoDB using RocksDB as the underlying storage engine.From the MongoRocks website :\"RocksDB is a key-value library based on Log Structured Merge Trees. It is maintained by the Facebook Database Engineering Team, and is based on LevelDB, by Sanjay Ghemawat and Jeff Dean at Google\"MongoRocks differs from WiredTiger in the way it stores data.WiredTiger uses a B-tree where MongoRocks uses a LSM-tree structure.Both have pros and cons of course.An LSM-tree structure favours space and insert efficiency over read efficiency.An B+ tree structure favours update and read efficiency over space efficiency.So depending on your workload you can choose which might suit you better.Of course, nothing beats measuring what the effect of your workload is on performance of the choosen storage engine.Because, you know, silver bullets and such…So testing and measuring is key in deciding which engine you should choose.Nevertheless, MongoRocks is showing nice improvements over previous versions and has several interesting benefits over WiredTiger.Certainly when storage endurance is an issue or if your working set does not fit into memory.Automatic Database Management System Tuning Through Large-Scale Machine LearningThis is probably the most stunning talk of the conference.OtterTune is a tool developed by students and researchers at Carnegie Mellon to automatically tune your database.This is done by making clever use of previously collected data of other tunings and applying machine learning to it.The presented results showed that for the given workload, OtterTune was at par with DBAs which had double digit years of experience.Looking at this from the bright side, OtterTune would help DBAs to focus on areas other than figuring out which combination of the multiple settings they should use to tune their database.It would definitely help to do better Database Reliability Engineering.ConclusionThis is just a small portion of the huge amount of sessions at Percona Live, but of course, one needs to choose.It’s really great to see this conference putting open source on the foreground and displaying the wealth of choice and diversity of technologies in the open source database world.We see that this space is continuously expanding and that the future is looking even more promising than the present.Good times ahead!Useful links &amp; further reading  Percona Live Europe Dublin  MongoRocks  Grafana  MongoDB Atlas  Elastic  MongoDB security checklist  Database Reliability Engineering"
      },
    
      "kickstarters-2017-10-05-kickstarter-trajectory-2017-html": {
        "title": "Kickstarter Trajectory 2017",
        "url": "/kickstarters/2017/10/05/Kickstarter-Trajectory-2017.html",
        "image": "/img/kicks.png",
        "date": "05 Oct 2017",
        "category": "post, blog post, blog",
        "content": "  This year, 45 young professionals started the various Ordina Kickstarter trajectories.Five of those, participated in the Kickstarter trajectory of JWorks.Each of them looking for a challenge and a fresh start!For some, it was a transition between school and work, and Ordina handled this very well.The main goal of this trajectory was to widen every student’s knowledge of the IT world.They taught us the basics of every topic that’s hot at the moment.This will definitely come in handy during our first project.First impressionsWhen we arrived the first day, we were welcomed with breakfast and afterwards, we got a tour around the building.The corporate culture here is truly a plus for Ordina and we immediately felt at home as everybody is really friendly and helpful.We got our equipment consisting of a car and either a Windows laptop or a MacBook Pro depending on our preference.It’s clear that, starting from day one, Ordina makes sure that their consultants are well equipped to work successfully.What is the Kickstarter trajectory?The Kickstarter trajectory consisted of intensive training spanning two months.During this time, courses on different frontend and backend technologies, methods, tools and soft skills were given by senior consultants and external lecturers. This trajectory is ideal for graduating students and people who want to make a switch to IT.The constant guidance and support made this a good preparation for our first project.During the fourth week we had a team building event with all the kickstarters from the different units. The goal was to get to know the other kickstarters and strengthen our team spirit by shooting each other with bow and arrow!During the last week of the Kickstarter trajectory, the ‘JOIN Event’ was held.This day was split into two parts: the unit meeting where the whole JWorks unit gathers for a year report where every competence center presents its past and upcoming activities, and the completed and current projects are highlighted. And in the afternoon the JOIN Event itself. There were talks given by members of JWorks as well as by external speakers about upcoming or commonly used technologies such as Docker and Spinnaker, but also about Scrum and User Experience.In addition to gaining knowledge about these subjects, this event provides a unique opportunity to meet most members of the unit or at least see them, since there are over a hundred.Other events we could participate in were the CC-meetings and the Ordina Boardgame Night.These also provided opportunities to get to know both our and other units as we deepened our understanding of the presented subjects.As you can see Ordina employees are very involved.TrainingWe kicked off our career at Ordina in the JWorks unit.The main technologies used in this unit are Java &amp; JavaScript.Most of the workshops in the Kickstarter trajectory are based on these two technologies.Small side note, don’t be fooled by these names: Java &amp; JavaScript have no underlying connection.In general, the development of modern applications is divided into two main groups: the frontend part and the backend part.Java dominates in the backend part, while JavaScript is located in the frontend part.For two months, we’ve had the luxury to deepen our knowledge in both domains, in order to obtain a comprehensive understanding of the cutting-edge technology stack that’s being used in JWorks.Below you’ll find a brief overview of the technologies we explored and applied in the workshops.  Frontend          HTML5, CSS3, BEM, SASS      JavaScript, TypeScript      Angular (2+), Ionic      Npm, Bower, webpack        Backend          Newest features in Java 7/8/9      Java EE, Spring, Spring Boot with Maven      JPA, Hibernate      Unit Testing: JUnit, Mockito, etc.      MongoDB      Cloud solutions (PaaS, IaaS, SaaS): Pivotal Cloud Foundry and OpenShift      Docker, Git        Design principles and methodologies          Microservices      Agile, Scrum      DevOps, Continuous Integration      DDD, BDD, TDD      Rather than learning these technologies in isolation, we learned to create applications by combining them.For example, we used TypeScript, CSS and Ionic to build mobile apps, we wrote backend logic by uniting the forces of Spring Boot, Java and unit testing.We also combined Angular, TypeScript, SASS, HTML and npm to create a web application.In our projects, we learned to utilize the Version Control System Git to collaborate with a team and share projects and code progress.One thing we’ll never forget is that if we push breaking changes to the master branch and break the build, we’ll have to buy “boterkoeken” for the unit!Last but not least, we were introduced to the wonderful world where clean code and microservices are the heroes that kick spaghetti code and monoliths in the butt.During the trajectory we also had a couple of “free” days during which we had to read the Clean Code book and prepare ourselves for the Oracle Certified Associate, Java SE 8 Programmer I exam.Memorable moments“The soft skill sessions were real eyeopeners and gave me a better understanding regarding introducing myself, giving presentations and the Agile methodology.The introduction to microservices was very interesting since moving away from monoliths is the way to go.” – Ken“The tips received during the communication essentials sessions are a backpack full, good enough to present myself in a proper way.Diverse technologies used in JWorks e.g. turning a monolith to microservices, a different type of database like MongoDB, running your applications in Docker containers and such.” – Michiel“Learning about the existence of ‘microservices’ versus ‘monoliths’.Writing unit-tests to see if the code does what it should do before deploying the application in production.And the introduction to the MongoDB database and how to use it in the command line interface.” – Jef“The communication sessions were worth their weight in gold!How often do you get a chance to practice communication in the most awkward situations?I guess every day, but at least in these sessions there were no real repercussions :)” – Simon“Learning about all those different technologies.There really is too much to choose from.But if I have to pick something, it’s the Spring Boot session.I previously experimented with Spring Boot at my internship, where nothing seemed to make sense.But after this session, everything I was struggling with became clear and fell into place.” – NickThe new JWorks colleagues"
      },
    
      "angular-2017-10-04-testing-angular-with-karma-html": {
        "title": "Testing Angular with Karma 101",
        "url": "/angular/2017/10/04/Testing-angular-with-karma.html",
        "image": "/img/2017-10-02-testing-angular-with-karma/unit-tests.png",
        "date": "04 Oct 2017",
        "category": "post, blog post, blog",
        "content": "Testing your code is as important as writing the code itself. This also counts for frontend applications such as Angular apps. Unit testing is one way to do so. The goal of these kind of tests is to isolate classes and verify the output of its functions to be what you expect when they are called.We also need a tool to run our tests written in TypeScript.Karma is the one we’ll be using to run tests described in this blog.It will open a browser, execute pieces of JavaScript and report the results back to you.Now, I must admit that I’m not too fond of writing tests myself. However, I do strongly believe they help a lot towards improving the quality of the code. Writing unit tests can be quite a hassle, but with an application that is continuously growing and changing, they are an efficient way to prevent bugs getting to production.Table of contents  Setup  Writing tests  What to test  Tips and tricks  ConclusionSetupLet’s take a look at how it’s done in an Angular app using Karma.If you’re using the Angular CLI, you’re in luck because setting up the unit tests is easy. It’s already done! All you need to do is run ng test (or npm test). It will transpile your tests and run them using Karma. If you’re not using the Angular CLI yet, I recommend creating a new project with the CLI and copying your existing project to it.It will make your life a lot easier.Running ng test will run the tests in watch mode, meaning that every time you save a change to a file, it will automatically rerun your tests. Additional flags can be passed like --single-run to make it run only once. When passing the --code-coverage flag, it generates a report in HTML. By default it’s found under coverage/index.html and it indicates which parts of your code were covered by your unit tests.    Writing testsStructureNow that the setup is done, let’s look at how to write the tests themselves.First of all, test files should be named after the .ts file you’re testing, but with .spec added to the file name (e.g. when testing login.component.ts, the test file should be named login.component.spec.ts). It’s best practice to keep the spec file in the same folder as the ts file. So mostly, for a component, you’ll end up with a HTML, scss, spec.ts and ts file in one folder (unless you like to inline your HTML and CSS).    Next up, the content of a test file. The Jasmine spec is used to format the tests (more info).This means that individual tests are grouped together in a describe block. A test itself starts with it. Besides tests, you can also add other blocks to a describe, like beforeEach, beforeAll, afterEach, afterAll… What these blocks do, is quite self-explanatory. Here’s an example how it could be used: when testing a class, you’ll want to create an instance of that class for each test, so instead of writing the same code in each test to create an instance, you could put that code in the beforeEach clause. Simply pass a function (in lambda notation) to beforeEach containing the code you want it to run.Within a test itself, the class’ public functions can be called and assertions can be made. Assertions are made using the expect function. You can give it a variable or a call to a function and tell it what you expect the result to be with toBe, toEqual, toBeTruthy, toBeFalsy, toBeNull…Here’s an example:describe('NAME_OF_YOUR_CLASS', () =&gt; {    let component;    beforeEach(() =&gt; {        //initialize        component = new AppComponent();    });        //Actual tests    it('should have a car selected', () =&gt; {        //assertions        expect(component.carSelected).toBeThruthy();    });    it('should find my favorite car brand', () =&gt; {        //assertions        const carBrand = component.getFavoriteCarBrand();        expect(carBrand.name).toEqual('Mazda');    }); }); As you can see, you can pass some text as an argument in the describe call. This is usually the name of the class you’re testing and it’ll be shown when running the tests. For the tests themselves, you can also pass some text which will be shown. These are mainly used for you to be able to identify failing tests. The text should describe what’s being tested, for example “It should get the brand of the car”, could be written as it('should get the brand of the car', () =&gt; ....Writing the actual testsThere are multiple ways to write unit tests for an Angular app. Either you use the Angular TestBed, the ReflectiveInjector or you simply call the constructor of the class directly. ReflectiveInjector and TestBed have a similar approach, so I’ll only be discussing TestBed here.It’s something pretty cool Angular came up with in order to test your components. TestBed can create components and injects all its dependencies.The instance of the component that is returned can then be used for testing.Accessing the view is also possible.Now, although I said there are multiple ways to unit test an Angular app, there’s actually only one correct way: calling the constructor.Since TestBed loads the view as well as any components, directives… used in the view, you’re actually also testing how the class integrates with them.In other words, you’re entering the domain of integration testing, which is also important, but out of scope for this blog post.The unit tests you would write using the constructor approach, could practically look the same when you would use TestBed to instatiate the components. However, there are some problems with using the Angular TestBed for unit tests which I’ll be explaining below.1. TestBedSetting up the TestBed configuration for a component kind of looks like a module definition. You should list all components, directives and services that are used by the component you’re testing directly or by importing a module that includes them. Calling createComponent will return a ‘fixture’ which can be used to access the view and also get the instance of the class linked to it. With the fixture you can find HTML elements and perform actions on them, verify their content and attributes…The instance of the class can be used to test its public functions (unit test).describe('AppComponent', () =&gt; {    let component: AppComponent;    let fixture: ComponentFixture&lt;AppComponent&gt;;    beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [CarBrandService],            imports: [CommonLogicModule]        })        .compileComponents();    }));    beforeEach(() =&gt; {        fixture = TestBed.createComponent(AppComponent);        component = fixture.componentInstance;        fixture.detectChanges();    });    it('should test the class', () =&gt; {        //use component to test the class itself        const carBrand = component.getFavoriteCarBrand();        expect(carBrand.name).toEqual('Mazda');    });    it('should test the view', () =&gt; {        //use component to test the class itself        const carBrand = component.getFavoriteCarBrand();        expect(carBrand.name).toEqual('Mazda');        //use fixture to access the HTML (e.g. get h1 element)        const de = fixture.debugElement.query(By.css('h1'));        const el = de.nativeElement;        expect(el.textContent).toContain('Mazda');    });});MockingIn unit testing, we are only interested in testing the class itself and try to isolate it as much as possible. We also want to be able to easily control the output of all dependencies of our class, such as services.spyOnOne way to do so is by creating spies for all calls to functions of those dependencies. That’s where the spyOn function comes into play:describe('AppComponent', () =&gt; {    let component: RequestPopupContainer;    let fixture: ComponentFixture&lt;AppComponent&gt;;            beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [CarBrandService],            imports: [CommonLogicModule]        })        .compileComponents();    }));    beforeEach(() =&gt; {        fixture = TestBed.createComponent(AppComponent);        component = fixture.componentInstance;        const carBrandService = fixture.debugElement.injector.get(CarBrandService);        spyOn(carBrandService, 'findAll').and.returnValue(Observable.of([            { name: 'Mazda', country: 'Japan' },            { name: 'BMW', country: 'Germany' }        ]));            fixture.detectChanges();    });    ...});In the example above, you can see when the AppComponent would call carBrandService.findAll(), instead of making a HTTP call, an Observable is returned with a list of car brands which is defined in the test itself. This is pretty cool, but also very error prone. If you forget to place a spy on a certain function, it will perform the actual call, possibly a HTTP call.That’s something we do not want at all.Mock classesTo prevent forgetting to spy on a certain function, you could create mock classes and inject them instead of the actual classes:class MockCarBrandService {    findAll(): Observable&lt;CarBrand[]&gt; {        return Observable.of([            { name: 'Mazda', country: 'Japan' },            { name: 'BMW', country: 'Germany' }        ]);         }   }describe('AppComponent', () =&gt; {    let component: AppComponent;    let fixture: ComponentFixture&lt;AppComponent&gt;;            beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [{provide: CarBrandService, useClass: MockCarBrandService}],            imports: [CommonLogicModule]        })        .compileComponents();    }));    ...});Again we see that findAll() will return an Observable containing a list. By using this approach, you’ll get an error when you forgot to define a function in the mock class. This may solve our previous problem, but now we have created another one. Karma allows us to assert whether a function was called using toHaveBeenCalled and toHaveBeenCalledWith. The problem here is that we don’t have any spies, so those functions can’t be used.We can again add spies like in the first approach, but you can imagine that this is a lot of work and will get quite messy.Jasmine spy objectsSo, the first two approaches have some issues. Luckily there’s a better way, Jasmine spy objects:describe('AppComponent', () =&gt; {            let component: AppComponent;    let fixture: ComponentFixture&lt;AppComponent&gt;;     const mockCarBrandService = jasmine.createSpyObj('carBrandService', ['findAll']);    mockCarBrandService.findAll.and.returnValue(Observable.of([        { name: 'Mazda', country: 'Japan' },        { name: 'BMW', country: 'Germany' }    ]);    beforeEach(async(() =&gt; {        TestBed.configureTestingModule({            declarations: [AppComponent],            providers: [{provide: CarBrandService, useValue: mockCarBrandService}],            imports: [CommonLogicModule]        })        .compileComponents();    }));    ...});The first argument of jasmine.createSpyObj is the name for the object and will be used to mention it in the console.This is usually the name you gave the instance of the corresponding class in the constructor. The second argument is an array containing all function names of that corresponding class that are called from the class being tested. In other words, not all functions offered by the class that’s being mocked have be listed, only the ones actually being used.  Also note that in the providers list, we have to use useValue instead of useClass since jasmine.createSpyObj already returns an instance.Using spyOn isn’t needed, a spy object is already being spied upon (hence the name) and you can call the toHaveBeenCalled and toHaveBeenCalledWith functions on it....it('should call the findAll method' () =&gt; {    component.getFavoriteCarBrand();    expect(mockCarBrandService.findAll).toHaveBeenCalled();}); ...I think it’s obvious to say that using Jasmine spy objects is the way to go. If you forget to define a function, you’ll get an error when it’s called. The functions that are defined, are also spied upon. So all the problems with the first and second approach are solved. There’s even another benefit when using spy objects. The implementation (returnValue or callFake) can be changed at any time, even in the middle of a test!Issues with unit testingA side effect of using TestBed is that when the component is loaded, the ngOnInit, ngAfterViewInit… lifecycle events are called automatically.This means you have less control over them.Getting all the imports, providers and declarations setup can be quite a struggle too. If there’s any subcomponent in the HTML of the component you’re testing, they should either be imported through a module or added in the declarations of the TestBed configuration.If you don’t feel like doing all that, you can also tell Angular to skip elements it doesn’t recognise by adding NO_ERRORS_SCHEMA to the TestBed configuration:TestBed.configureTestingModule({    declarations: [ AppComponent ],    schemas: [ NO_ERRORS_SCHEMA ]})It’s very likely that you’ll be using the Angular router in some of your components, so you’ll have to account for that too. You could mock the router dependency using a Jasmine spy object or you can add RouterTestingModule as an import instead of the RouterModule itself. The routes that are relevant can then be defined in the RouterTestingModule:imports: [RouterTestingModule.withRoutes([/*List mock routes here*/])]  To learn more about writing tests using Angular TestBed, I recommend reading this guide: https://angular.io/guide/testing.2. Calling the constructorA much better way to do unit testing is to simply call the constructor of the class you want to test.You should get an instance of each dependency that’s needed in the component’s constructor.Of course we want to mock these classes and as we saw in the Angular TestBed section, the Jasmine spy objects are the way to go.describe('AppComponent', () =&gt; {            let component: AppComponent;    const mockCarBrandService = jasmine.createSpyObj('carBrandService', ['findAll']);    mockCarBrandService.findAll.and.returnValue(Observable.of([        { name: 'Mazda', country: 'Japan' },        { name: 'BMW', country: 'Germany' }    ]);    beforeEach(() =&gt; {        component = new AppComponent(mockCarBrandService);    });    ...});Without the TestBed, you don’t have access to the view. However, your tests will run much faster as there are less things to load. When using TestBed, you’ll probably be including lots of dependencies just to make it work, giving you less control. This is something you do not want in unit testing as you want to isolate the class as much as possible. Another difference with TestBed is that you have to call the lifecycle events yourself, again giving you more control over the code you’re testing.it('should find the car brand', () =&gt; {    component.ngOnInit();    const carBrand = component.getFavoriteCarBrand();    expect(carBrand.name).toEqual('Mazda');}); Async, fakeAsync, tickAngular is full of Observables and writing tests for them is a little trickier. You might also be using the setTimeout and setInterval functions. To cope with all that, Angular provides the async and fakeAsync functions. You can simply wrap your test in an async and it should only finish after all async calls are finished. If you want to have more control, you can wrap the test in a fakeAsync instead. Then the tick() function can be called to advance time with one tick. By passing an argument to it, time can be advanced by more ticks at once: tick(500).Suppose we have this class:export class TimeoutExample {    counter = 0;    updateCounterWithDelay() {        setTimeout(() =&gt; {            this.counter++;        }, 100);    }}And this test:  it('should increase the counter with a delay', fakeAsync(() =&gt; {    const component = new TimeoutExample();    expect(component.counter).toBe(0);    component.updateCounterWithDelay();    tick();    expect(component.counter).toBe(0);    tick(10);    expect(component.counter).toBe(0);    tick(90);    expect(component.counter).toBe(1);}));It clearly shows how the tick function manipulates the advancement of time, although it isn’t really a useful test,ObservablesNow, what if you want to test a function that returns an Observable? Well, simply subscribe to it in an async block and check the result!it('should return a list of cars' async(() =&gt; {    service.findAll().take(1).subscribe(        (result) =&gt; {            expect(result.length).toBe(9);        },        (error) =&gt; {            expect(true).toBeFalsy();        }    );}));The error clause may seem strange. However, what if the findAll() call returns an error and you don’t have the error clause in your test? You’ll simply think that your test has passed because it appears green in the console. With code coverage enabled, you may notice that the part of the code you were testing isn’t marked as covered. By adding expect(true).toBeFalsy(); to the error clause, your test will fail because it shouldn’t get there!What to testNow that we know a little on how to test, let’s have a look at what to test.For starters, you don’t have access to private and protected variables/functions, so all you can do is test the public ones. All variables that are accessed by the view should be public, so those are the ones you can use for your tests. The constructor and all lifecycle events can be called as well as they are public. You should never ever set a variable or function to public in order to test it. If you can’t test it because it’s private, you’re doing something wrong. You should be able to get to it through other functions.Generally, you give an input and assert the output, it’s as simple as that. Your different inputs should also make sure that all branches are tested (e.g. an if gives you two branches, one where the if resolves to true and one to false).Unit tests in Karma also allow you to assert whether a function has been called and optionally with which parameters (toHaveBeenCalled and toHaveBeenCalledWith). This can be useful when for example testing a void function that calls a mocked function. That way you can still assert the output. So, think of possible scenarios for the functions to test, provide the input and assert the output using expect. Also try to cover other paths than just the happy paths!Testing getters and setters usually isn’t needed, unless they are more complex. In most cases they’re not and it’s quite pointless to call a setter and then assert whether it has been set correclty. Most of the time, these will be called indirectly when testing other functions.The code coverage report can help you find functions that aren’t fully tested yet. However, your goal shouldn’t be to get a 100% coverage. Getting a 100% isn’t that hard, simply calling all functions with some different inputs will get you there.It won’t mean that your code is fully tested.To give you an example, suppose you have a function that sorts a list.You write some tests with different inputs so all branches are covered and you get a 100% coverage.The ordering of the list could still be completely wrong and not what you expect, although it’s fully covered.By using expect to verify that the output is correct, you’ll be doing a way better job.Even then there may be scenarios that aren’t tested despite the coverage report stating that part of the code is covered.So try to think of the various possible scenarios (both success and error scenarios) and translate those to tests.Tips &amp; tricksOnly run certain testsWhen your test base begins to grow, you don’t always want to wait for all tests to have run when only testing a certain class or function. Therefore, you can choose to only run specific describe blocks or tests (it) by adding an f (which stands for focus) in front of them, such as fdescribe and fit. To exclude certain describe blocks or tests, you can prefix them with an x (exclude), like xdescribe and xit. This will certainly come of use.Nesting describe blocksDescribe blocks can also be nested. If you want for example different beforeEach blocks for your tests when testing a class, you can create a nested describe block for each case.describe('AppComponent', () =&gt; {    let mockCarBrandService = jasmine.createSpyObj('carBrandService', ['findAll']);    describe('Happy path', () =&gt; {        beforeEach(() =&gt; {            mockCarBrandService.findAll.and.returnValue(Observable.of([                { name: 'Mazda', country: 'Japan' },                { name: 'BMW', country: 'Germany' }            ]));        });        it(...);        ...    });        describe('Error path', () =&gt; {        mockCarBrandService.findAll.and.returnValue(Observable.throw('Error'));        it(...);        ...    });});Using the injectorDependency injection is used all over Angular meaning that it isn’t possible to simply call new for certain classes. Normally, you simply put the dependencies in the constructor of your class and Angular takes care of the rest (e.g. constructor(private formBuilder: FormBuilder)).When calling the constructor of a class in a test, you don’t always want to mock those dependencies, so you’ll need to get instances of them somehow. For example when using Angular’s FormBuilder or when you need it to create a FormGroup to use in your test. In that case, you can use Angular’s ReflectiveInjector which takes care of getting an instance for you.Here’s an example how:const injector = ReflectiveInjector.resolveAndCreate([FormBuilder]);const formBuilder = injector.get(FormBuilder);As you can see, you can simply pass the class name and it will return an instance of that class. That instance can then be passed in the constuctor of the class you’re testing.ConclusionWhen writing unit tests, it’s better to call the constructors direcly and not to use Angular TestBed. It will give you more freedom and more control, run the tests much faster and allow you to completely isolate classes. You should also write integration tests and TestBed will serve that purpose very well.To mock classes, Jasmine spy objects are simply the way to go.Changing their implementation or return value is easy and can be done at any time!Code coverage reports can be very useful to find parts of uncovered code. However, getting a high percentage of code coverage shouldn’t be your goal. Write useful tests and also, don’t limit your tests to the happy path!"
      },
    
      "spring-2017-10-04-spring-cloud-stream-rick-and-morty-adventure-html": {
        "title": "Spring Cloud Stream - A New Rick and Morty Adventure",
        "url": "/spring/2017/10/04/Spring-Cloud-Stream-Rick-And-Morty-Adventure.html",
        "image": "/img/spring-cloud-stream.png",
        "date": "04 Oct 2017",
        "category": "post, blog post, blog",
        "content": "IntroductionOne of the most interesting aspects of the Spring Framework and its ecosystem is abstraction.The Spring project maintainers and contributors have always succeeded in hiding complexity from the application developer, by adding different layers of abstraction.For example, the way a Spring Bean of a certain Interface can be autowired and how Spring will find a suitable implementation class at runtime, is a very obvious example of the Liskov Substitution Principle or how to abstract away implementation from specification.A second, higher level example is the Spring Data project which offers a common programming model for SQL as well as NoSQL databases, abstracting away the specifics of the database internals.Another great example of abstraction is the one I’ll be discussing in this blog post.DISCLAIMER: a big part of this blog post will explain how Spring Cloud Stream works by using heavy references to the animated series Rick and Morty, with the intention to be hilarious and informative at the same time.If you don’t know the show or have no sense of humor, this blog post will be informative only 😉I will ignore the obvious third option: this blog post might not be funny at all.Spring Cloud StreamI just can’t start explaining something without a definition, that would be cruel and irresponsible:  Spring Cloud Stream is a framework for building message-driven microservicesIt provides an opinionated configuration of message brokers, introducing the concepts of persistent pub/sub semantics, consumer groups and partitions across several middleware vendorsThe last part is what I like the most.Spring Cloud Stream abstracts away the complexity of connecting to different middleware solutions.It does this the Spring Boot way: by automatically configuring sensible defaults and allowing the developer to adapt the parts he wants.It might be surprising, but Spring Cloud Stream is not that new in the Spring Cloud ecosystem.The project was called spring-bus during its prototype phase and the first real commit was on May 28th 2015.Dave Syer performed the commit that changed it to its current name on July 8th 2015, so I will call that the birth of Spring Cloud Stream!The most active contributor up until now is probably Marius Bogoevici.Questions about the project can be directed to the most active contributors and community in the Spring Cloud Stream Gitter channel.Application ModelAs is described in the very detailed documentation, the following image details how a typical Spring Cloud Stream application is structured:An application defines Input and Output channels which are injected by Spring Cloud Stream at runtime.Through the use of so-called Binder implementations, the system connects these channels to external brokers.So once again, the difficult parts are abstracted away by Spring, leaving it up to the developer to simply define the inputs and outputs of the application.How messages are being transformed, directed, transported, received and ingested are all up to the binder implementations.Binder ImplementationsCurrently, there are two official Binder implementations supported by Spring, for RabbitMQ and Kafka.Next to those, there are several community binder implementations available:The current - non-exhaustive - list:  JMS (ActiveMQ, HornetQ, IBM MQ,…)  AWS Kinesis  Google Cloud Pub Sub  Redis  GemfireRick and MortyAs I have said earlier in the post, I will explain Spring Cloud Stream using a somewhat different approach, but I feel it helps to capture the power of the project.Behold, our first character appears on stage:RickThis is Rick Sanchez.He is Morty’s grandfather, a genius mastermind, inventor of inter-dimensional travel, the Microverse, a butter-passing robot and much, much more.He is also an asshole.Rick’s ObsessionIn the first episode of Season 3, Rick expressed his obsession with the 1998 Mulan Szechuan Sauce.The saying goes that a picture is worth a thousand words, so that means this video below will explain, like, a bajillion words or something:So now we know that Rick really wants this Szechuan sauce.Now, we have a purpose:  We will create a Spring Cloud Stream application, called Rick, which sole purpose is to retrieve Szechuan sauce from McDonalds!As with every Spring based application these days, it’s as easy as going to the happiest place on earth (next to production): https://start.spring.io.As our dependencies, we pick Spring Web MVC to create some handy web endpoints and Stream Rabbit since we want to send our messages over a RabbitMQ broker.We end up with the following dependencies:&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;So how does a basic Spring Cloud Stream application look like? Well, it’s actually not that different from a regular Spring Boot application:@SpringBootApplication@EnableBinding({ InputChannels.class, OutputChannels.class })public class RickApplication {\tpublic static void main(String[] args) {\t\tSpringApplication.run(RickApplication.class, args);\t}}Looks pretty familiar, doesn’t it? That’s because the only new thing in the snippet above is the @EnableBinding annotation, which automagically converts your application into a full-fledged messaging beast!The InputChannels and OutputChannels interfaces are specific to my application.Very simply explained, we can describe the Rick microservice with the following diagram:As you can see, we have defined one input channel called rick and one output channel called microverse.These are implemented in a Spring Cloud Stream application like this:public interface InputChannels {\t@Input\tSubscribableChannel rick();}public interface OutputChannels {\t@Output\tMessageChannel microverse();}  Holy sh*t Rick, this almost seems like it’s too easy!Well Morty, erm dear reader, that’s because it is!Didn’t I tell you that Spring is awesome at abstraction?Yeah, this is why.The only thing that is left for us to do, is write our “business logic”, or in our case: the part where we try to find our beloved Szechuan sauce!Since Rick is very lazy and an arrogant genius, he’s not gonna look for the sauce himself.I mean, he’s got adventures to go on, inventions to invent and generally be a pain in the ass of the Galactic Federation.Let’s add another output channel to our interface:public interface OutputChannels {\t@Output\tMessageChannel meeseeks();\t@Output\tMessageChannel microverse();}Meeseeks?! What the hell is a meeseeks?Patience my dear reader, all will be explained shortly.First, let me show you the evil, brilliant piece of code which is gonna get us the Szechuan sauce:@Componentpublic class SzechuanSauceFinder {    private static final String C_137 = \"C-137\";    private static final int minimumRequestIntervalInMillis = 50;    private static boolean SEARCHING = false;\tvoid findThatSauce() throws InterruptedException {\t\tif (!SEARCHING) {\t\t\tSEARCHING = true;\t\t\tint requestIntervalInMillis = 5000;\t\t\twhile (SEARCHING) {\t\t\t\tthis.outputChannels.meeseeks().send(buildMessage(I_WANT_MY_SZECHUAN_SAUCE, C_137));\t\t\t\tThread.sleep(requestIntervalInMillis);\t\t\t\trequestIntervalInMillis = Math.max(minimumRequestIntervalInMillis, requestIntervalInMillis - 200);\t\t\t}\t\t\tSEARCHING = false;\t\t}\t}\tvoid stopSearching() {\t\tSEARCHING = false;\t}}Isn’t that some of the most evil code you’ve ever seen?Nothing more evil than static variables controlling state of an application, or precisely placed Thread.sleep() commands.Okay, we’ve got a messaging microservice, pumping out messages at an increasing rate (up until 20 per second).How will we know if our meeseeks, whatever that is, has found the szechuan sauce?The rest of the code in this class will illustrate how an input channel can handle incoming messages:@Autowiredpublic SzechuanSauceFinder(InputChannels inputChannels, OutputChannels outputChannels) {    this.outputChannels = outputChannels;    inputChannels.rick().subscribe((message -&gt; {        GlipGlop glipGlop = (GlipGlop) message.getPayload();        if (glipGlop.getQuote() == ALL_DONE) {            stopSearching();            this.outputChannels.microverse().send(buildMessage(WUBBA_LUBBA_DUB_DUB, C_137));        }    }));}private Message&lt;?&gt; buildMessage(RickAndMortyQuote quote, String instanceId) {    return MessageBuilder.withPayload(new GlipGlop(quote, instanceId)).build();}Since the rick input channel is a SubscribableChannel, we can subscribe to it.Well duh Sherlock!A message can be of any type but we do need to cast it to our own format, a GlipGlop, but Spring has ways to make this easier for us.We could have created a method annotated with the new @StreamListener annotation, which would look like this:@StreamListener(InputChannels.RICK)public void handle(GlipGlop glipGlop) {    ...}Alright, so now we know what Rick wants, and how he intends to get it, we move to the next piece of the puzzle:Mr MeeseeksRick is such a genius, he invented a box that can spawn as many “Aladdin’s genies” as you want.Use with caution though, you have been warned!Meeseeks are creatures created to serve a singular purpose for which they will go to any length to fulfill:Finding the sauceSo our next task will be to create a Mr Meeseeks microservice.If I were to draw a very simple diagram of this application, it would look something like this:Same story as with the Rick microservice.We need a very simple Spring Cloud Stream application with one input channel called meeseeks.In this case, we want to send GlipGlops to McDonalds, Rick and the Microverse, so we’re gonna need three output channels.The only thing we really need to put some effort in - if you can even call it effort, I’ve had more effort tying my velcro shoes the other day - is the business logic:@Componentpublic class MrMeeseekRoutine {\tprivate final OutputChannels outputChannels;\t@Value(\"${INSTANCE_INDEX:${CF_INSTANCE_INDEX:0}}\")\tprivate String instanceId;\t@Autowired\tpublic MrMeeseekRoutine(InputChannels inputChannels, OutputChannels outputChannels) {\t\tthis.outputChannels = outputChannels;\t\tinputChannels.meeseeks().subscribe(message -&gt; {\t\t\tGlipGlop glipGlop = (GlipGlop) message.getPayload();\t\t\tif (glipGlop.getQuote() == I_WANT_MY_SZECHUAN_SAUCE) {\t\t\t\tthis.outputChannels.microverse().send(MessageBuilder\t\t\t\t\t.withPayload(new GlipGlop(RickAndMortyQuote.OOOH_YEAH_CAN_DO, instanceId))\t\t\t\t\t.build());\t\t\t\tthis.outputChannels.mcdonalds().send(MessageBuilder\t\t\t\t\t.withPayload(new GlipGlop(RickAndMortyQuote.PLEASE_GIVE_ME_SOME_SZECHUAN_SAUCE, instanceId))\t\t\t\t\t.build());\t\t\t} else if (glipGlop.getQuote() == YOU_ARE_A_WINNER) {\t\t\t\tthis.outputChannels.rick().send(\t\t\t\t\tMessageBuilder.withPayload(new GlipGlop(RickAndMortyQuote.ALL_DONE, instanceId)).build());\t\t\t}\t\t});\t}}It’s getting quite boring already, this is child’s play.What I’m obviously doing here, is:  subscribe to the meeseeks input channel  fetch the incoming GlipGlop  if its from Rick, comply and send a GlipGlop to the McDonalds channel requesting some Szechuan sauce  if its from McDonalds and a confirmation that we have just won some sauce, we let Rick know our task has been fulfilledLet’s see what our McDonalds microservice looks like.McDonalds: where the sauce isMore recently, to my surprise, McDonalds announced they were actually bringing back the now infamous 1998 Mulan Szechuan Sauce.So I guess my demo just got a bit more relevant and my powers of clairvoyance are proven once again.At this point, it’s just more of the same. Let me show you the diagram:  Spring Cloud Stream application  one input channel mcdonalds  one output channel meeseeksYou get it by now.Here’s the code yawn:@Componentpublic class McdonaldsCashier {\tprivate static final int ODDS_AT_FINDING_SZECHUAN_SAUCE = 500;\tprivate static final Random RAND = new Random();\t@Value(\"${INSTANCE_INDEX:${CF_INSTANCE_INDEX:0}}\")\tprivate String instanceId;\tprivate int luckyNumber;\t@Autowired\tpublic McdonaldsCashier(InputChannels inputChannels, OutputChannels outputChannels) {\t\tthis.luckyNumber = RAND.nextInt(ODDS_AT_FINDING_SZECHUAN_SAUCE);\t\tinputChannels.mcdonalds().subscribe(message -&gt; {\t\t\tGlipGlop glipGlop = (GlipGlop) message.getPayload();\t\t\tif (glipGlop.getQuote() == RickAndMortyQuote.PLEASE_GIVE_ME_SOME_SZECHUAN_SAUCE) {\t\t\t\tint randomInt = RAND.nextInt(ODDS_AT_FINDING_SZECHUAN_SAUCE);\t\t\t\tif (randomInt == luckyNumber) {\t\t\t\t\toutputChannels.meeseeks().send(\t\t\t\t\t\tMessageBuilder.withPayload(new GlipGlop(RickAndMortyQuote.YOU_ARE_A_WINNER, instanceId))\t\t\t\t\t\t\t.build());\t\t\t\t} else {\t\t\t\t\toutputChannels.meeseeks().send(\t\t\t\t\t\tMessageBuilder.withPayload(new GlipGlop(RickAndMortyQuote.SORRY_NO_LUCK, instanceId)).build());\t\t\t\t}\t\t\t}\t\t});\t}}Oh man, I think I’m getting bored even writing this.Stick with me, the demo is gonna be worth it.Don’t scroll to the bottom just yet!There’s only one piece of the puzzle left.Morty  “Aw djeez” - MortyMorty is a young teenage boy.He has short brown hair that he wears straight and neatly combed around his head.He wears a yellow shirt, blue pants, and white shoes.He’s cute and adorable and is always along for the ride.He gets to see all the incredible things that happen in the universe - and microverse - so he’s the perfect character to represent our frontend.I just want to clarify that I’m in no way a great frontend developer.I dabble in HTML, CSS and the occasional JavaScript, but my designer skills are abysmal.That’s why I love a framework like Bootstrap: easy, intuitive and fast to create a semi decent web application.So that’s why I choose to work with Bulma: the even easier, more intuitive version of Bootstrap.You can check out my horrible frontend code in the Git repository.The Morty microservice is a bit different than the others, since it needs to collect all the input messages and transfer them to a browser.We do this using server-sent events or SSE.Spring MVC has had support for SSE for a while and it’s actually very easy to use:@GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)public SseEmitter events() {    SseEmitter emitter = new SseEmitter();    return emitter;}As you can see, even Jerry could figure this stuff out.In the example above, nothing is actually being emitted.When someone browses to the endpoint, it opens an HTTP connection and waits for messages.It’s up to the server to actually start sending data messages from this emitter, which will trigger an onMessage JavaScript event at client-side.Let’s see how we implemented this for our Morty microservice:@Slf4j@RestController@RequestMapping(\"/events\")public class EventController {\tprivate final List&lt;SseEmitter&gt; emitters = new ArrayList&lt;&gt;();\t@Autowired\tpublic EventController(InputChannels inputChannels) {\t\tGlipGlopHandler glipGlopHandler = new GlipGlopHandler();\t\tinputChannels.rick().subscribe(glipGlopHandler);\t\tinputChannels.meeseeks().subscribe(glipGlopHandler);\t\tinputChannels.mcdonalds().subscribe(glipGlopHandler);\t\tinputChannels.microverse().subscribe(glipGlopHandler);\t}\t@GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE)\tpublic SseEmitter events() {\t\tSseEmitter emitter = new SseEmitter();\t\temitters.add(emitter);\t\temitter.onCompletion(() -&gt; emitters.remove(emitter));\t\temitter.onError(throwable -&gt; emitters.remove(emitter));\t\temitter.onTimeout(() -&gt; emitters.remove(emitter));\t\treturn emitter;\t}\tclass GlipGlopHandler implements MessageHandler {\t\t@Override public void handleMessage(Message&lt;?&gt; m) throws MessagingException {\t\t\tGlipGlop glipGlop = (GlipGlop) m.getPayload();\t\t\temitters.forEach(emitter -&gt; {\t\t\t\ttry {\t\t\t\t\temitter.send(glipGlop);\t\t\t\t} catch (IOException e) {\t\t\t\t\temitter.complete();\t\t\t\t\temitters.remove(emitter);\t\t\t\t\tlog.error(\"IOException when trying to send event\");\t\t\t\t}\t\t\t});\t\t}\t}}DISCLAIMER: this code is not production-ready and can probably cause instant brain damage when observed.This code is for demo purposes only.A quick explanation of the code:  we subscribe to the four input channels and attach the same message handler since we want to handle all the GlipGlops equally  when a client performs a GET request to the /events endpoint, it is assigned an SseEmitter which is added to a list  whenever a GlipGlop on any of the four input channels is received, it is sent to all the registered SseEmitters  exactly nothing is done when errors occur - totally intentionalThe MicroverseEverything that is described in this post, is transpiring inside the miniature dimension called The Microverse:the MicroverseIn all seriousness - yeah, seriously - we are deploying our microservices on the Pivotal Cloud Foundry (PCF) platform.In this case, I’m using a paid account on Pivotal Web Services, their online version of PCF.Inside this powerful Platform as a Service (PaaS) offering, there’s this concept of organizations and spaces.Inside of our Ordina JWorks organization, I have created a space called microverse to house all of the applications in my demo.This way, my wacky adventures cannot interfere with any of our actually useful applications.Through the powerful service broker mechanism, I provisioned a RabbitMQ service and bound it to my applications.This means the freshly created RabbitMQ instance’s connection details are automatically shared inside of my application’s containers as system properties.Since Spring Boot kicks ass at taking system properties and ramming them inside some auto-configuration, we don’t have to worry about anything remotely resembling boilerplate code.SummaryBefore I go over to the demo, I wanted to share my grand clarification of the Microverse and all things which lie within:As you can see, I have drawn multiple Meeseeks instances in this diagram.That’s because I want to spawn multiple Meeseeks to perform my task.Without any extra configuration, every Meeseeks instance will pick up every GlipGlop posted to the meeseeks input channel.This means adding additional Meeseeks instances won’t help us very much (it will increase the total number of GlipGlops in the system and probably overload the server even faster).We want every separate Meeseeks instance to pick up a unique message on that input channel.This can be accomplished by putting the Meeseeks application inside of a consumer group.Only one property is required to do this:spring:  cloud:    stream:      bindings:        meeseeks:          group: szechuan-finderThis indicates we want the meeseeks message channel to be part of a consumer group called szechuan-finder.DemoThis could be quite anti-climactic, but you’re gonna have to touch Pickle Rick to see the demo.Go on… Touch him…    Press the Rick and Meeseeks image in the demo and enjoy the show!Resources  Github Repository with all the code: https://github.com/Turbots/szechuan  Slides about this topic: http://slides.com/turbots/spring-cloud-stream-rick-morty  Spring Cloud Stream documentation: https://docs.spring.io/spring-cloud-stream/docs/current/reference/htmlsingle/  My presentation on our yearly JOIN event: https://www.youtube.com/watch?v=Nl9OIuNRYwI  Try out Pivotal Cloud Foundry on your local workstation: https://pivotal.io/platform/pcf-tutorials/getting-started-with-pivotal-cloud-foundry-dev/introductionImprovements  Better error handling on Morty - too many browser connections when sending 150 messages a second over event streams is quite demanding apparently  Addition of Spring Cloud Data Flow in the mix - registering the applications and dragging around inputs and outputs should be fun - also, scaling!  Improved UI - obviously  Complete event-based demo instead of endpoints to force certain operations (spawning/killing Meeseeks, waking up Rick, …)"
      },
    
      "iot-2017-09-28-end-to-end-iot-html": {
        "title": "Building end-to-end IoT demos with LoRa",
        "url": "/iot/2017/09/28/End-to-end-IoT.html",
        "image": "/img/end-to-end-iot/booze-5.jpg",
        "date": "28 Sep 2017",
        "category": "post, blog post, blog",
        "content": "  To showcase end-to-end LoRa applications we built simple yet fun, real world demo applications. These applications show a full end-to-end implementation of the LoRa technology leveraging the Proximus MyThings Internet of Things platform.    The Booze-o-meter V2 at Devoxx Belgium 2016.Building end-to-end LoRa Iot SolutionsBuilding an enterprise IoT solution is challenging. Devices need to be enrolled, monitored and maintained.You can roll your own network and handle all of this yourself, this however will require quite the backend system to facilitate all of this.The Proximus LoRa network in combination with their MyThings platform takes away most of this and allows us to focus on the actual applications.Technologies overviewFor our rapid prototypes and small to medium applications we have chosen the following technical stack:  Proximus LoRa network for LoRa connectivity  Proximus MyThings platform for device management  NodeJS with TypeScript on the backend  Angular on the frontend (The older versions are still on AngularJS)We will look into each item in full detail below:  1. LoRaLoRa, short for LoRaWAN is a LPWAN (Low Power Wide Area Network) is meant for wireless battery powered devices or ‘things’.It offers a low power, low bandwidth secure network to transceive information across large distances. The network is laid out in star topology and can easily be extended by placing more base stations also called LoRa gateways.Some network parameters:  Range of 5 to 15 kilometers (3,1 to 9,3 miles) depending on the conditions and signal strength.  Data rate of 0,3 kbps to 50 kbpsMore detailed information and the full specifications can be found on the LoRa Alliance website.  2. Proximus MyThingsProximus MyThings is a LoRa device onboarding and management platform. It is used to enroll devices and sensors, to map their data to specific endpoints and provide tools for device management.The platform consists of three main parts:  MyThings Builder: Charts and sensor values (containers)  Mythings Manager: Online device onboarding and user management  MyThings Scanner: Offline (in the field) device onboarding  3. Node.js &amp; TypeScriptMost people should be familiar with Node.It is the JavaScript runtime built upon the V8 engine that Google Chrome uses.It is a lightweight and efficient runtime that uses an event-driven, non-blocking I/O model.This combined with the added type safety that TypeScript provides makes this an excellent choice for rapid prototyping.Some of our own demo applications make use of the Node Simple Server (NSS) application, while others just use Express. This depends on the needs of the project.If you are interested in the NSS project, we have a blog post about it here and it is on GitHub too!  4. AngularLike with Node, most people should be familiar with Angular (or the older AngularJS).Angular is a development platform for building modern single page web applications.It is a complete rewrite of the older AngularJS and therefor has some big changes in how things work.Angular is easy to set up and use, it also is fully cross platform/browser compatible.Our demo applicationsAll our demo applications are publicly available in the GitHub project of NSS.These demo applications are ever evolving as we are currently porting them from the older AngularJS to Angular with TypeScript.Our demo applications have been showcased and used at several events including internal Proximus events as well as conferences like Devoxx, Techorama and The Belgian IoT convention in Mechelen.Below we will go into detail about each application and how it came to be, as well as the iterations they went through.Aside from the Slotmachine and the Booze-o-meter we’ve also developed the Stairway to Health application for Proximus. A blog post about this will be available in October.For the impatient, the IoT talk at the annual JWorks JOIN event covered this topic already and can be viewed on YouTube.1. The SlotmachineThe Slotmachine application does mostly what its name suggests, but with a twist.The idea is simple:If required, the player registers him or herself in the application.A simple push button sends a signal to the backend application. The application dispatches an event via a websocket to the frontend application which turns the Slotmachine. The Slotmachine can either result in a win or a loss. A maximum of three attempts are possible per player, after which a new player registration is required to play again.The player registration can be disabled depending on the requirements of the event/conference.The light effects are also controlled by the application.If the user has registered the gentle fading switches to a running light effect and if the user wins, the effect changes to a carrousel of different colors.The effects are controlled the same way the button is controlled but in the opposite direction. The frontend application sends a websocket event to the backend application which controls the Arduino and the LEDs.      The Slotmachine V1 test setup.V1The first version was not LoRa enabled and used a push button and Arduino integration via Johnny-Five to allow interaction. This meant that an Arduino always needed to be connected to the server or laptop that was used as a server.    The Slotmachine V1 at Devoxx Belgium 2015.V2The second version of the Slotmachine application swapped out the Arduino and the required wired connection with a LoRa enabled push button.This allowed us to demonstrate the capabilities of the LoRa network in a fun and engaging way.The application remained unchanged for the user, and was adapted to be more configurable:Setting a win chance (up to 100%) and different images/styling for different events.2. The Booze-o-meterThe Booze-o-meter application is a drink dispenser that relays liquid fill level in the dispenser.It is a fun example to demonstrate how measuring the fill level of a container can be achieved.This idea can be applied to container in a whole range of different industries and use cases. From oil tanks to garbage cans and to containers.The application setup is extremely similar to the Slotmachine application. The sensors relay their data via the MyThings platform to our backend, which in turns dispatches an event on a websocket so the frontend application can display the change.      The Booze-o-meter V1 test setup with regular water.V1The first version of the Booze-o-meter used three sensors that can detect a liquid through a thin plastic container. This allowed us to represent the level in the container in a coarse way:  FULL (initial state)  HIGH (sensor)  MEDIUM (sensor)  LOW (sensor)The sensors have a simple binary readout, true if liquid is detected, false if not.This data gets represented on the frontend application as the four states as mentioned above.V2      The Booze-o-meter V2 at Devoxx Belgium 2016 with actual liquor!The second version of the Booze-o-meter application allowed us to get a more detailed reading of the remaining fluid level in the container thanks to the addition of an ultrasonic sensor.This sensor can measure the distance between itself and a surface, in this case the surface of the liquid in the container.The application was updated to support this more granular approach that is able to show the level in the container accurately to 1%.ConclusionOur demo applications have served us well in bringing across the idea of LoRa to customers and other interested developers. We will continue to evolve our demo applications by adding new features, technologies and keeping them up to date.Useful links &amp; further reading  LoRa Alliance  Proximus MyThings  Node Simple Server on GitHub  StairWay to Health JOIN Presentation  Angular  NodeJS  Express"
      },
    
      "microservices-2017-09-26-secure-your-architecture-part1-html": {
        "title": "Securing your cloud-native microservice architecture in Spring: part 1",
        "url": "/microservices/2017/09/26/Secure-your-architecture-part1.html",
        "image": "/img/microservices/part1/securitylogo.png",
        "date": "26 Sep 2017",
        "category": "post, blog post, blog",
        "content": "When developing cloud-native microservices, we need to think about securing the data that is being propagated from one service to another service and securing the data at rest. So how much security is enough to secure our architecture? Is it the user that identifies itself and decides what data he has access to?Overview  Our cloud-native architecture  Authentication &amp; Authorization Principle  Using the OAuth2 Protocol  Understanding JSON Web Tokens  Using a User Authentication &amp; Authorization Server  Securing your microservice  Securing data at restOur cloud-native architectureIn this blog series we will cover these questions and guide you in applying the security layer to your cloud-native blueprint.With this blueprint, we are going to use the Spring ecosystem throughout the series.Solving the following problems is crucial for building a cloud-native microservices architecture, but it should be technology-agnostic:  User Authentication &amp; Authorization Server: Spring Cloud Security OAuth2  Load Balancer &amp; Routing: Spring Cloud Zuul  Communication client: Spring Cloud Feign  Externalized Config: Spring Cloud Config Server  \t        Where our journey begins…When it comes to users interacting with our system, we want to verify that the person can identify him- or herself.Most of the time this appears in a login form where you enter your credentials, or in a login page from a third party application (Facebook, Google, etc).  \t          If you like more secure systems, you can add another level of complexity on top of it.Most commonly used is Two-factor-authentication, where the client will use an external provider (Google Authenticator for example) to issue a token for your registered application.AuthorizationAuthorization is the mechanism that uses the user’s data to verify what he is allowed to do.  For instance, who has access to which resources and what are his access rights (eg. read or write) to those resources.To use these two mechanisms in our system, we will be using a security protocol that fits our microservices architecture.Since we don’t want everyone to have an account for each (micro)service, we aim to have one single identity per person so that the user needs to authenticate only once.Using the OAuth2 ProtocolWhen searching for a security protocol, we don’t want to reinvent the wheel and look at what is supported by the Spring framework.Obviously, it depends on the use case of the applications that require resources from our system.Is it a third party application like Facebook or a first party like your own application? Or both? I will explain both OAuth2 and JSON Web Token and how they solve these requirements.The OAuth2 delegation protocol allows us to retrieve an access token from an identity provider and gain access to a microservice by passing the token with subsequent requests.When introducing the OAuth2 framework to our system, we will be using four grant types.These grant types are different ways to obtain an access token, some clients are more trusted than others.OAuth2 Grant TypesThird party applications: Authorization Code grant type and Implicit grant typeAuthorization Code is the most common used grant type for third party applications, where user’s confidentiality can be maintained.The user won’t have to share his credentials with the application that is requesting resources from our backend. This is a redirection-based flow, which means that the application must be capable of interacting with the user’s web browser.  The frontend (application) makes a request to the User Authentication &amp; Authorization server (UAA) on behalf of the user  The UAA server redirects to a permission window of a third party for the user to grant permission, the user authenticates and grants permission  The UAA server returns an authorization code with a redirect url  The frontend uses the authorization code and an application identification to request an access token from the UAA server  The UAA verifies the authorization code and returns an access token  Implicit grant type follows the same principle as the Authorization Code type but does not exchange an authorization code to issue an access token.First party applications: Password grant typeThis grant type is best used for first party applications,where the user is in a trust relationship with the application.The application authenticates on behalf of the user and receives the proper JWT.  The user provides his credentials to the frontend, commonly done with a login form  The frontend assembles a POST request with the credentials to the UAA server  The UAA validates the user and returns a valid JWTTrusted Service to Service communication: Client Credentials grant typeThe trusted service can request an access token using only its client-id and client-secret.When the client is requesting access to the protected resources under its control, it is very important that the client credentials grant type MUST only be used by confidential clients.  Zuul authenticates with his client-id and client-secret  The UAA validates the credentials and returns a valid JWTOAuth2 ScopesOAuth 2.0 scopes provide a way to limit the amount of access that is granted to an access token.If the scope is not defined, the client is not limited by scope.  An access token issued to a client can be granted READ or/and WRITE access to protected resources.If you enforce a WRITE scope to your API endpoint and it tries to call the endpoint with a token granted a READ scope, the call will failJSON Web Tokens (JWT)JSON Web Tokens (JWT) is a compact URL-safe means of representing claims to be transferred between two parties.The claims in a JWT are encoded as a JavaScript Object Notation (JSON) object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or MACed and/or encrypted.The suggested pronunciation of JWT is the same as the English word “jot”.The payload consists of some standard attributes (called claims), such as issuer, subject (the user’s identity), and expiration time.The specification allows these claims to be customized, allowing additional information to be passed along.Be careful when passing additional information, if you like to go deeper on this topic with a real use case, you can read Using JWT for State TransferJwt.io provides a quick way to decode your JWT.One of the challenges in a microservice-based architecture is identity propagation.After the authentication, the identity of the user needs to be propagated to the next microservice in a trusted way. JWT is used here to carry along information of the user.Based on a token, your microservice needs to be able to create a principal object. This principal object needs to contain all the necessary info so the system can decide whether or not the request should be executed or not.  \t        Dealing with timeWhen propagating the identity of the user, you don’t want it to last for a infinite amount of time. That’s why JWTs have an expiration time.When expired, the JWT will be invalid and the client needs to request a new JWT with the refresh token.These refresh tokens carry the needed information to issue a new JWT.Refresh tokens can also expire but are rather long-lived.JWTs have three fields that relate to time and expiry, all of which are optional.In most cases, you should include these fields and validate that the token:  is not expired (exp)  was created before the current time (iat)  should not be used before the current time (nbf)All of these times are expressed as UNIX epoch timestamps, and are best checked in the order as described above.Signed JWTsSigning a JWT helps establish trust between services, because it gives a recipient reason to believe that the message was created by a known sender and that the message was not altered in transit.JWTs are being signed by a public/private key pair.Almost all of the JWT libraries support signing. To check if yours supports it, visit JWT Libraries.For a deeper dive into signing JWT, check our tech post about Digitally signing your JSON Documents  The user requests a resource  The frontend assembles a request with an Authorization header and a Bearer token inside, fires off the request to Zuul  Zuul verifies the token in communication with the UAA server  If the token is valid, Zuul redirects the frontend to the correct resource on the proper microservice  The microservice checks for authorization to the resource, if access granted, the correct resource is returnedStatelessSince we are working with cloud-native applications, we can’t have any state within them.Because we have all the necessary information and create a new principal object for each request, the token eliminates the risk of having in-memory session state in the microservice.Using a User Authentication &amp; Authorization Server (UAA)The UAA server is an identity provider. It adds authentication to applications and secures services with minimum fuss.It’s primary role is that of an identity provider, issuing tokens for client applications to use when they act on behalf of users. It can also authenticate users with their credentials, and can act as an SSO service using those credentials.There are some options available as a UAA server:  Using a third party for issuing tokens (ex. GitHub, Facebook). Tutorial Github social login  Using KeyCloak, an open source solution aimed to make it easy to secure your application. Tutorial on how to use KeyCloak in Spring  Using Okta, a commercial OAuth2, SAML and general identity management service in the cloud.  Implementing your own UAA is not really best practice since other providers cover most of the use cases. Explanatory video of the UAA serverEnabling Single Sign-OnNow that we have a way to achieve Authentication and Authorization by applying OAuth2 and JWT, we still have one problem.Having multiple frontends in our architecture, the user will have to log in to each of these applications.With Single Sign-On (SSO) we can eradicate this problem just by using the existing user session and requesting an access token.Enable OAuth2 SSO flow on Zuul serviceThe @EnableOAuth2Sso  and @EnableZuulProxy annotation on our Zuul service will forward OAuth2 tokens to the services it is proxying.Sensitive HeadersZuul secures your sensitive headers by blocking these headers downstream (microservice).Since the default settings for sensitive headers blocks the Authorization header, we have to open this setting and send these headers downstream.You can choose to set the sensitive header per route or globally.How it works: Sensitive HeadersZuul FilterBased on Netflix’s Zuul, Spring’s implementation also brings a filter mechanism.Filters are capable of performing a range of actions during the routing of HTTP requests and responses.This can help you customize security on your incoming and outgoing traffic.Review the Zuul filter guide from Netflix about how filters work.Securing your microserviceWhen enabling security in your service, the most common issues are developer-induced.Either there is a lack of built-in or easy security controls, or we make trade-offs for functionality over security.Still, we have to think about who can access this functionality and what they can do with it.We got an access token, our gateway performed a coarse grained verification and proxied it to our microservice.We are in a ‘downstream service’, where data is being load-balanced from Zuul. The next questions are:  How do we decode this JWT?  How can we secure our code with the help of Spring Security?Assembling the PrincipalIt is the responsibility of a microservice (Resource Server) to extract information about the user from the access token.Decoding the token allows the extraction of the user’s information.With this information Spring Security will assemble a Principal object containing eg. the username and the user’s roles, and puts it in the security context. Using the security context the AccessDecisionManager will be able to make a decision whether or not the request should be performed.To enable this, we need to add spring security to our class path and add the @EnableResourceServer annotation to our application.Best practices with keysThe problem that might occur is that every microservice would need to connect with the UAA server for verification on every request.Zuul verificationObviously, we don’t want every microservice to depend on the UAA servers availability regardless of startup / testing / CI. The solution is to disable exposure of your microservices to the outer network and handle only incoming traffic via the gateway (eg. Zuul, HAProxy, nginx,…).Zuul will verify the token as a trustworthy client of the UAA server and will propagate the token to the downstream services.But what if a hacker gets inside of your platform?JSON Web KeysTo solve this issue, we need an extra validity check on the microservice.When verifying a token’s validity, it comes down to verifying if the token was issued by the UAA server.This can be done by requesting the public key used for signing the JWT. This is called a JWK or JSON Web Key.Basically, you can restrict the dependency on the UAA server to one single REST call, where the JWK is fetched from a public URI.Once a microservice has a cached JWK, it can be used to verify any JWT completely by itself.This greatly reduces network calls to the UAA server and still secures all of your microservices.When you want to rotate your private/public key pair, you can use JWKS. We will go deeper into detail in one of our next posts.Securing API endpointsAt last we’re going to secure our resources.Spring Security gives us a variety of tools to secure your application at class and method level.The one that’s used most often enables method security, which you enable by adding @EnableGlobalMethodSecurity(prePostEnabled = true) to your configuration.AuthorityFor the authorization, Spring Security provides us with authorities, extracted from the access token.The authorities are placed inside a Principal, which will be used throughout the existing security context of your application.You can then reference them using Spring Expression Language (SpEL) to secure your methods.There are plenty of options you can use for method security, but we’ll highlight the most common ones.You can find a complete list in the Spring documentation@PreAuthorizeMost commonly used, @PreAuthorize will decide whether a method can actually be invoked or not.  When a user logs in and you want the user to only access his detail information, or everyone’s data in case he’s an admin, you can use the @PreAuthorize annotation.@PreAuthorize(\"(authentication.principal.uuid == #uuid.toString()) or hasRole('ADMIN')\")User findByUuid(@Param(\"uuid\") UUID uuid); @PostAuthorizeLess commonly, you may wish to perform the access-control check after the method has been invoked.The returnObject is the returned value of that method.  A user can only view his own details and not those of someone else, but an administrator can.You validate this by checking if the user has the admin role or if the principal’s UUID is the same as the one of the returned user object.@PreAuthorize(\"hasAnyRole('ADMIN','USER')\")@PostAuthorize(\"returnObject!=null or hasRole('ADMIN') or returnObject.uuid.toString() == authentication.principal.uuid\")User findOne(@Param(\"uuid\") UUID uuid); Next stepIn the next post we will cover how to secure your data at rest.Sources  Spring Cloud Eureka  Spring Cloud Hystrix  Spring Cloud Zuul  Spring Cloud Feign  Spring Cloud Config Server  Spring Cloud Security OAuth2  Two-factor-authentication  OAuth2 Scopes  Josh Long UAA intro  Tutorial Github social login  KeyCloak  Tutorial on how to use KeyCloak in Spring  Okta  Spring OAuth2 developers guide  Sensitive Headers  Zuul Filters  Spring Expression Language  Authorities  JWT decoder  JWT Libraries  JWK or JSON Web Key  JWKS"
      },
    
      "conference-2017-09-18-browser-security-features-html": {
        "title": "Browser Security Features",
        "url": "/conference/2017/09/18/Browser-security-features.html",
        "image": "/img/security/padlock_code.jpg",
        "date": "18 Sep 2017",
        "category": "post, blog post, blog",
        "content": "Browser security featuresUpdate: due to the deprecation of HPKP, we’ve posted an update to this blog post.Browsers nowadays come with a ton of security features built-in.These features are there to protect the people using your application as well as protecting your application from malicious others.Most of these features are quite easy to implement, however for some of them (such as key-pinning) you have to be careful not to break your site.It’s this danger, combined with the lack of knowledge, that prevents people from taking full advantage of them.Table of contents  Transport Layer Security  HTTP Strict Transport Security  Public Key Pinning  Content Security Policy  Subresource Integrity  Cookie Protection  ConclusionTransport Layer SecurityThe first layer of defense is not a new one at all: Transport Layer Security (TLS).TLS is sometimes (incorrectly) referred to as SSL (Secured Socket Layer). In reality SSL is an obsolete technology, with TLS being its successor, but the name stuck.Having said this, why should you use TLS? First of all, most of the features described below only work when you’re on a secured connection.Besides that, it guarantees the end user that the site they’re communicating with is actually the site they think it is.It also provides the guarantee that the content they see was not tampered with while travelling over the network.Another thing TLS brings to the table is speed: it used to be true that a secure connection was slower than an unsecured one.Modern hardware however is more than up to the task of handling this efficiently for you.Besides that, HTTP/2 is only available over a secure connection and it allows for faster page loads.Have a look at HTTP vs HTTPS for a demo of the difference.Since speed should no longer prevent you from switching to HTTPS, there’s only cost.Even that is no longer true: a simple Domain Validation certificate can be obtained for free.But even if you need more protection, an Extended Validation certificate can be had for as little as $300 per year.How hard is it?The main issue is that all resources you use on your site should be served over HTTPS.This means that all third parties should use TLS as well.Furthermore, it depends on the complexity of your site.Nick Craver wrote an extensive blog post on their road to switching to HTTPS.Should you activate this on your site?Absolutely! Modern browsers are shifting from notifying users that a page is secure to warning them that it isn’t.On top of that, Google gives a slight ranking boost to HTTPS sites.HTTP Strict Transport SecurityOnce your server is properly configured to use TLS, your next step is to redirect your users to the secure version.You could do this by simply adding a redirect-rule in your web server for the non-secure pages.This means that users will still first connect to your non-secure site, allowing a potential attacker to intercept the request and do his nefarious deeds.Wouldn’t it be nice if you could tell the browser to just go straight to the secure version?That’s the thinking behind the HSTS (HTTP Strict Transport Security) header.HSTS simply tells the browser that you’re expecting it to use HTTPS for a certain time.As a result, the browser will automatically replace http:// with https:// before making the call.This means that even following a link that explicitly defines http:// will instead be called using a secure connection.The configuration of HSTS is as easy as can be: you simply add the following header to your response:Strict-Transport-Security: max-age=31536000; includeSubDomainsThis will tell the browser that for the next 365 days, it should connect to your domain using HTTPS.The includesubdomains directive tells the browser that your subdomains should also be called using https.Setting the max-age to 0 tells the browser that you no longer wish your domain to be HSTS-enabled.HSTS preloadOf course in this scenario, the user’s first connection will still take place over an unsecured connection.This would offer an attacker a brief period in which he can still hijack the connection.To prevent this, most major browsers (Chrome, Firefox, Safari, Edge, IE11 and Opera) offer an HSTS preload list.Domains on this list will automatically be loaded over HTTPS from the start, without having to go through the HTTP -&gt; HTTPS redirect.If you want your domain to be included in this list, you should add the preload directive to the HSTS header.Strict-Transport-Security: max-age=31536000; includeSubDomains; preloadAfterwards, you can register yourself for the HSTS Preload List.Are there any risks?Activating HSTS does offer some risks:  If you include the includesubdomains directive, you tell the browser that all subdomains need to be retrieved over HTTPS.If your internal applications are on a subdomain (e.g. internal.example.com), you’ll block access to those that haven’t enabled TLS yet.  Adding the preload directive is even more dangerous because this tells browser makers to hardcode your HSTS settings.If you’ve made a mistake in the setup, it can take a long time to be removed from the list. Since this list is in the browser, you’ll affect both your existing and your new users.These risks can be mitigated through extensive testing and conservative settings. Start with a short max-age and slowly increase its length, don’t include subdomains if you’re not 100% sure that all subdomains need to be includedand perhaps most importantly, don’t activate preload unless you’re 100% sure that everything works as intended.Should I activate HSTS?For those (subdomains) where TLS is enabled, you should start rolling out HSTS (while keeping in mind the warnings above).Public key pinningUpdate: HPKP is deprecated from Chrome 67, have a look at this post to see what next.Alright, now you’ve secured your site with HTTPS, and you’ve made sure your users can’t fall victim to a Man in the Middle attack.Or have you?It’s true that HSTS will make sure that the user only connects using a secured connection, but that doesn’t mean the HTTPS connection is actually made to your server.Over the last couple of years, there were several incidents where malicious actors were able to generate valid certificates for domains they didn’t control.When this happens, your users will think they’re safe (as their browser shows the green padlock), but the attacker can still manipulate your content.To protect yourself against this, there’s a mechanism called “HTTP Public Key Pinning” (HPKP [1]).With HPKP you “pin” the public key of your TLS certificate to the browser.In the future, that browser will compare the public key that’s actually used for the TLS connection, with the pinned one and, if they don’t match, refuse the connection altogether.An HPKP header looks like this:public-key-pins:pin-sha256=\"YLh1...uihg=\";pin-sha256=\"9dNi...Dwg=\";pin-sha256=\"Vjs...eWys=\";max-age=2592000;includeSubdomains;report-uri=\"report-uri\"The max-age directive tells the browser for how long these pins are valid. You can use report-uri to get a report when an invalid certificate is used.includesubdomains makes sure that the policy also applies to your subdomains.Finally, there are the actual pins.You need to pin at least 2 fingerprints: 1 that should be active at the moment and 1 that isn’t.What to pin?First of all, you need to pin at least one of the keys in your certificate chain.While you can pin the key of the actual certificate, that might not be the best idea.Doing this means that you need to update the keys every time your certificate is changed or you will risk your users being unable to visit your site.Alternatively, you could pin the key for the root certificate of your CA (Certificate Authority).While this is a lot safer, it does mean that if your CA, or any of its intermediates is compromised, they could issue valid certificates for your site.Finally, you have the option to pin the key to the intermediate certificate. Doing so limits the attack surface to that intermediate, while it also allows you to roll out new certificates whenever you need to.Of course you can’t control when your CA will change their intermediate certificate, so that’s a danger in its own.Besides that you also need to have a second key pinned that’s not in your current certificate chain, otherwise your HPKP header will be ignored.Fortunately, you don’t have to have certificates ready for this.It’s enough to pin the public key of a CSR (Certificate Signing Request).Obviously, you can’t use the CSR of your current certificate (as that would be valid for this chain), so you’ll need to create a backup CSR.You’ll need to keep this CSR and the associated private key in a secure location, because you don’t want these to be compromised together with the original.report-uriThe report-uri directive is used to tell the browser where to send reports if it encounters an invalid certificate.The browser will POST a JSON message to the URL you specify here.If you don’t want to implement your own processing of these reports, have a look at report-uri.ioIt will process the reports from your site and display the results in a nice format, allowing you to take action when you see something that’s wrong.Report-onlyBesides the normal HPKP header, there’s also the report-only variant: Public-Key-Pins-Report-Only.This header has the exact same specifications, but it won’t block access to your site if there’s no valid pin.As the name says, it will simply report violations to the report-uri.Obviously, this header isn’t meant to increase the security of your site on its own, rather it’s a way to help you on your way to a full HPKP implementation.DangersHPKP is quite a dangerous header: it’s quite easy to commit “pinning-suicide”.Pin the wrong certificate, have a CA change keys on you or have something else go wrong and your site is inaccessible until your users’ max-age expires.Be careful rolling out this one as it’s way too easy to shoot yourself in the foot.Should I use it?This header has some serious dangers associated with it.It’s not enough to know that the current configuration is correct, you also need to be sure that you’re equipped to deal with certificate updates without breaking the site.And then you need to be sure that you’ve got a backup in place in case you ever want to switch CAs.Unless you’re 100% sure that this won’t be an issue, hold off for now as it’s too easy to DoS your own site.Content Security PolicyEven though your connection is secured with TLS, that doesn’t mean that the content can’t be tampered with in other ways (such as cross-site scripting (XSS)).An attacker could use these kinds of attacks to load malicious content.The Content Security Policy (CSP) header is designed to prevent this kind of attacks.It allows you to specify exactly what content your site is allowed to load through a load of directives.Fetch directivesFirst of all, you can define what source content can be fetched from. There’s a specific directive for each resource type and a fallback directive default-src.You define the sources where the content can be loaded from as follows:  self only load content from the same origin as the page  none don’t load any content of this type  unsafe-inline lets you use inline javascript and CSS (although it’s preferable to use a nonce)  unsafe-eval allows the use of eval(), setTimeout(String), setInterval(String) and new Function(String). There’s a reason it has “unsafe” in its name though: these functions are typically used as attack vectors for XSS.  https: allow content loaded from anywhere, as long as it’s served over HTTPS  example.com allow content loaded from anywhere on example.com, both HTTP and HTTPS  You can also use wildcards to control which origins are allowed.  E.g. *://*.example.com:* will allow resources to be loaded from all subdomains of example.com, using any scheme and port. Note that it won’t allow you to load resources from example.com itself.  nonce-... Allows you to specify a nonce. Scripts or styles that have this nonce are then allowed to execute.It’s also important to note that you can pass multiple values to these directives: self https://example.com will allow resources to be loaded both from the domain itself as well as from https://example.com.You can use these to define default-src, but CSP gives you more fine-grained control over where each type of resource can be loaded from.For that you need to use the following properties instead:  script-src - Javascript  style-src - CSS  img-src - images  font-src - fonts  object-src - objects (e.g. &lt;object&gt;, &lt;embed&gt;, …)  media-src - media such as &lt;audio&gt; and &lt;video&gt; elements  connect-src - where the page can connect to using XmlHttpRequest, WebSocket or EventSource. The browser will immediately return a 400 status code when your page attempts to connect to a non-valid domain.  frame-src - Specify which locations can be embedded in a &lt;frame&gt; and &lt;iframe&gt;  worker-src - Worker scripts  child-src - Is either deprecated or serves as fallback for frame-src and worker-src, depending on the browser and the CSP level implemented.  manifest-src - defines which manifest can be applied to the resource. (This is still experimental though).Navigation directivesThese directives tell the browser what kind of navigation is allowed:  form-action limits to where forms can be submitted  frame-ancestors specifies who may embed pages using elements such as &lt;frame&gt;, &lt;iframe&gt;, &lt;embed&gt;, &lt;object&gt; and &lt;applet&gt;Other directivesBesides these, CSP allows for quite a few other directives:  report-uri works the same way as the report-uri directive of HPKP  require-sri-for allows you to force the use or Subresource Integrity ((SRI)[#Subresource-integrity]) for stylesheets, scripts or both.  Allowed values are script and style (or both).  base-uri defines which URLs you can use in the &lt;base&gt; element  sandbox to enable a sandbox for requested resources  (have a look at Mozilla’s documentation for more information)NonceA nonce, pronounced “/nɒns/” (nance), is a term that means “number (used) once”.It allows you to load specific inline scripts without allowing all of them. Only those scripts that have a nonce attribute that matches the value specified in the CSP header will get executed.Keep in mind though that you should never hardcode the nonce or use a value that can be guessed.It’s best to generate a new nonce for each request and add it to those scripts you need to execute.E.g. if you have the following CSP setting:Content-Security-Policy: script-src 'nonce-randomValue'will only execute scripts that have the nonce attribute defined like this:&lt;script nonce=\"randomValue\"&gt;    // ... script contents&lt;/script&gt;Multiple policiesYou are allowed to specify multiple CSP policies simply by specifying the header multiple times.If you do this however, it’s important to keep in mind that subsequent CSPs are not allowed to loosen the rules, only to tighten them.Report-onlyAs with HPKP, CSP also supports a report-only variant with Content-Security-Policy-Report-Only.Once again the specifications are exactly the same but it won’t block loading or execution of disallowed resources and simply report violations.You can then use the reports it generates to decide what you need to allow in your actual CSP header, before you deploy it (and break your site).Should I use it?CSP has some risks: it can break your site’s functionality, but overall it’s relatively easy to test it. The report-uri directive allows you to monitor if there are any issues and you can use the report-only version of the header to easily validate the setup you’re planning in the wild.If you have a system that relies a lot on third party content, it might not be for you.For everyone else, try out the report-only header and see if you get any issues.Subresource integrityWhen you’re developing a web application, you’ll often depend on some JavaScript frameworks such as Angular or jQuery.Loading these files from a CDN can speed up load times from your application, since it’s quite likely that the user already has a cached version of the script available.Of course it’s a good idea to be careful about the content of these scripts.Whenever you’re loading resources that aren’t under your control, you’re depending on someone else to make sure that they aren’t tampered with.To make sure that they aren’t changed without your knowledge, you can use subresource integrity (SRI).With SRI, you add an integrity attribute to your &lt;script&gt; or &lt;link&gt; tag.This attribute contains the hash of the file you expect.Your browser will then download the file, hash it with the same algorithm and compare the results.If the hash matches, the resource will be used; otherwise it will be ignored and an error will be shown in the console.&lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"     integrity=\"sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=\"     crossorigin=\"anonymous\"&gt;&lt;/script&gt;As you can see in the example above, the integrity of the script will be checked using a ‘SHA256’ hash. You’ll also notice the crossorigin attribute: this attribute is required when loading SRI validated resources from a different origin.Possible values are use-credentials and anonymous, indicating whether a request will have the credentials flag set.If you’re using a CDN, you’ll probably want to use anonymous.Note that you only need to add the crossorigin attribute if you’re loading the resource from a different origin.For resources coming from the same origin, you can omit the crossorigin attribute.Calculating the SRI value.In order to add the integrity attribute, you need to know the correct hash of the file. The easiest way to calculate it is by simply specifying a random value and checking the resulting error in your browser.Should I use it?Most likely. If you’re depending on third party scripts, you should make sure that they aren’t changed without your knowledge.This does mean that you shouldn’t just include the latest version of a script (e.g. example.com/library/latest/) as that will change whenever a new version is released.Cookie protectionMost websites nowadays use a variety of cookies for different purposes.These too can be a source of problems: session cookies grant the user access to certain content or allow them to perform certain actions.If this cookie can be intercepted or altered, the consequences can be enormous.Because of this, it’s a good idea to protect your cookies as much as possible.Since you’re already running your site on HTTPS, it’s a good idea to make sure the cookies aren’t sent on insecure requests.You can easily do this by adding the secure flag to the cookies you send.To make the cookies even more secure, you’ll also need to prevent them from being read/modified by scripts running in the page.In most cases there’s no reason for a script to have access to these cookies, so you can simply mark them as HttpOnly.Should I use this?Yes.Your session cookies should not be available to scripts, so the HttpOnly flag should be set on those.If you’re using TLS (and you should) you should definitely set the secure flag as well.ConclusionBrowsers nowadays support a wide array of security features you can use to keep your users safe.But, as with all things, powerful tools require you to wield them carefully.If you apply them without proper thought, you can easily make your website inaccessible or render it unusable.Because of that, you need to be really careful when you implement (most of) these measures.Do proper testing and (where possible) use the Report-Only variant for a while to spot possible issues before they become real problems.Make sure you really understand what you’re doing and what the consequences are of getting things wrong.When you have all that, don’t be afraid to experiment, just make sure you do so safely.[1] Public Key Pinning with Spring Security"
      },
    
      "microservices-2017-09-17-monitoring-your-microservices-with-micrometer-html": {
        "title": "Monitoring your microservices with Micrometer.io",
        "url": "/microservices/2017/09/17/monitoring-your-microservices-with-micrometer.html",
        "image": "/img/2017-09-17-monitoring-your-microservices-with-micrometer/post-image.jpg",
        "date": "17 Sep 2017",
        "category": "post, blog post, blog",
        "content": "When we want to instrument our application, we don’t want to worry about which monitoring system we want to use, now or in the future.Nor do we want to change a lot of code throughout our microservice because we need to change from system X to system Y.Meet Micrometer!So what is Micrometer you ask?Basically, it comes down to this:  Think SLF4J, but for metrics.Micrometer provides a simple facade over the instrumentation clients for the most popular monitoring systems.It allows you to instrument your code with dimensional metrics with a vendor-neutral interface and decide on the monitoring system as a last step.Using this interface, we can support multiple monitoring systems and switch easily to an other system with little to no hassle.It already contains built-in support for Prometheus, Netflix Atlas, and Datadog, while InfluxDB, statsd, and Graphite are on their way!Using Micrometer in your applicationStarting with Spring Boot 2, more specifically since milestone M4, Micrometer becomes the defacto instrumentation library that will be powering the delivery of application metrics from Spring.Luckily for us, they also backported this functionality to Spring Boot 1.x through an additional library dependency!Just add the micrometer-spring-legacy module together with the additional monitoring system module, and you’re good to go!In Gradle:compile 'io.micrometer:micrometer-spring-legacy:latest.release'Or in Maven:&lt;dependency&gt;  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;  &lt;artifactId&gt;micrometer-spring-legacy&lt;/artifactId&gt;  &lt;version&gt;${micrometer.version}&lt;/version&gt;&lt;/dependency&gt;Creating metricsThere are a couple of ways to create meters.We will cover all different types, when to use them, and furthermore how to implement them.Dimensions/TagsA meter is uniquely identified by its name and dimensions (also called tags).Dimensions are a way of adding dimensions to metrics, so they can be sliced, diced, aggregated and compared.For example, we have a meter named http.requests with a tag uri.With this meter we could see the overall amount of HTTP requests, but also have the option to drill down and see the amount of HTTP requests for a specific URI.CountersCounters are a cumulative metric that represents a single numerical value that only ever goes up.They are typically used to count requests served, tasks completed, errors occurred, etc.Counters should not be used to expose current counts of items whose number can also go down, gauges are a better fit for this use case.        MeterRegistry registry = ...Counter counter = registry.counter(\"received.messages\");    counter.increment();GaugesA gauge is a metric that represents a single numerical value that can arbitrarily go up and down.Gauges are typically used for measured values like current memory usage, but also “counts” that can go up and down, like the number of messages in a queue.        MeterRegistry registry = ...AtomicInteger currentHttpRequests = registry.gauge(\"current.http.requests\", new AtomicInteger(0));Queue&lt;Message&gt; receivedMessages = registry.gauge(\"unprocessed.messages\", new ConcurrentLinkedQueue&lt;&gt;(), ConcurrentLinkedQueue::size);Instead of returning a gauge, the gauge method will rather return the thing that is being observed.This allows us to have quick one liners that both create the object to be observed and set up metrics around it.TimersTimers measure both the rate that a particular piece of code is called and the distribution of its duration.They do not record the duration until the task is complete.These are useful for measuring short-duration latencies and the frequency of such events.long startTime = System.nanoTime();MeterRegistry registry = ...Timer timer = registry.timer(\"timer\");    // this will record how long it took us to get a registry and create a new timertimer.record(System.nanoTime() - startTime, TimeUnit.NANOSECONDS);Or we could just annotate a method with @Timed and let Micrometer do the rest for us@Timedpublic void doSomethingWhichShouldBeFastButIsActuallyReallySlow() {}Long task timersThe long task timer is a special type of timer that lets you measure time while an event being measured is still running.To time a long running task we use the same @Timed annotation, but we set the property longTask to true.@Timed(longTask = true)@Scheduledpublic void doSomethingWhichCanTakeALoooooongTime() {}It is up to the application framework to make something happen with @Timed.In case it isn’t able to do that, you can still use the long task timer.MeterRegistry registry = ...LongTaskTimer looooongTimer = registry.more().longTaskTimer(\"sync\");private void doSomethingWhichCanTakeALoooooongTime() {    looooongTimer.record(() =&gt; {        // actually do some synchronization which takes a loooooong time    });}Distribution summariesA distribution summary is used to track the distribution of events.It is similar to a timer but more general in that the size does not have to be a period of time.Usually it is used to sample observations of things like response sizes.MeterRegistry registry = ...DistributionSummary summary = registry.summary(\"response.size\");Summary statisticsMicrometer provides quantile statistics computed at instrumentation time and histograms for use in calculating quantile statistics at query time for monitoring systems that support this.QuantilesQuantiles are cutpoints dividing the range of a probability distribution into contiguous intervals with equal probabilities, or dividing the observations in a sample in the same way.Timers and distribution summaries can be enriched with quantiles computed in your app prior to shipping to a monitoring backend.Depending on the size of your deployments, computing quantiles at instrumentation time may or may not be useful.It is not possible to aggregate quantiles across a cluster.Four quantile algorithms are provided out of the box with different tradeoffs:  WindowSketchQuantiles - The importance of an observation is decayed as it ages.This is the most computationally costly algorithm.  Frugal2UQuantiles - Successive approximation algorithm that converges towards the true quantile with enough observations.This is the least costly algorithm, but exhibits a higher error ratio in early observations.  CKMSQuantiles - Lets you trade computational complexity for error ratio on a per-quantile basis.Often, it is desirable for higher quantiles to have a lower error ratio (e.g. 0.99 at 1% error vs. 0.5 at 5% error).This algorithm is still more computationally expensive than Frugal.  GKQuantiles - Lets you trade computational complexity for error ratio across all quantiles.This is used inside of WindowSketchQuantiles.HistogramsA histogram measures the statistical distribution of values in a stream of data.It samples observations, like HTTP request durations or database transaction durations, and counts them in buckets.They can be used to compute quantiles or other summary statistics like min, max, average or median.Because histograms buckets are exposed as individual counters to the monitoring backend, it is possible to aggregate observations across a distributed system and compute summary statistics like quantiles for an entire cluster.Naturally, the error rate of the computed summary statistic will be higher because of the lossy nature of putting data in buckets.BindersBinders define a collection of meters and are used to encapsulate best practices for monitoring certain types of objects or a part of the application’s environment.For example, the JvmThreadMetrics binder which gauges thread peak, number of daemon threads, and live threads.Micrometer ships with a basic set of binders:  JVM and system monitoring  Cache monitoring  Executor and ExecutorService monitoring  Logback monitoring"
      },
    
      "conference-2017-06-21-devoxx-pl-html": {
        "title": "Devoxx Poland 2017",
        "url": "/conference/2017/06/21/Devoxx-pl.html",
        "image": "/img/2017-devoxx-pl/devoxx-poland.jpg",
        "date": "12 Jul 2017",
        "category": "post, blog post, blog",
        "content": "Devoxx Poland 2017Krakow in the ICE Krakow Congress Centre.We started off day 1 with the keynote in the absolutely, phenomenal main room:Table Of Contents  Keynote: Speed without Discipline: a Recipe for Disaster  Feature Branches And Toggles In A Post-GitHub World  A reasonable overview of Java 9 and how you could think of it  The Language of ActorsKeynote: Speed without Discipline: a Recipe for Disaster (Venkat Subramaniam)Venkat kicked off the keynote, talking about a paradigm shift, that is happening right now in software development:In the nineties, everybody was doing imperative programming, using objects to implement functionality.Nowadays, this style of software development is shifting towards a more declarative approach.In imperative programming, developers focus on both what they want to do and how they want to do it. In declarative programming on the other hand, developers focus on what they want to do and use tools and libraries to facilitate their goal.Venkat went on to state that programming in a functional style is declarative, but that not all declarative code is functional.Functional style = declarative style + higher order functionslet names = [\"Dieter\", \"Tom\", \"Andreas\", \"Ken\", \"Yannick\", \"Tim\", \"Bart\"];let count = 0;for(const name of names){  if(name.length === 4)    count++;}console.log(count);console.log(names.filter(name =&gt; name.length === 4).size);Declarative vs ImperativeVenkat told the audience that he doesn’t like driving cars.He compared driving a stick shift to imperative programming.His goal is going from point A to point B and he does not want to be involved in changing the gears (Manipulating the DOM).A car with an automatic drive train, is a step in the right direction, but still requires too much focus on how he wants to reach his destination (Using a library like JQuery).Using the auto pilot functionality in certain modern cars is another step in the correct direction, but what he really wants is a car with a dedicated driver, like Uber or Lyft offer (Abstracting the DOM and using frameworks like Angular).In this comparison the ride-sharing service is the declarative approach.Testing  I automate my tests, not because I have a lot of time, but because I don’t.After an introduction to declarative programming, Venkat switched to the topic of testing.To really be agile, we need to be confident that implementing new features won’t cause failure.We can achieve this confidence by automating our tests and making sure they are repeatable.If we are really confident, we might even be able to ship software, without running the application.Writing software without writing tests is described as JDD: Jesus Driven Development. Pray that it works.Obviously, TDD (Test Driven Development) makes a lot more sense.Software development: a profession where people get paid to write poor quality code and get paid more later to cleanup the mess.&mdash; Venkat Subramaniam (@venkat_s) 27 september 2015Testing vs verificationTesting and verification are two different things.Verification is the process that checks if the code (still) works.This is not something anyone should do manually, verification is exactly what should be automated.Testing is the process that checks if a feature is correctly implemented.Code represents what you have typed, not what you might have wanted the system to do.It is the act of gaining insight in the application and the business.This could well be a manual task.Unfortunately, most of our industry has neglected this important difference.The maturity of software verification can be categorized in three maturity stages. Projects without verification automation are in denial, they are building up an increasing technical debt.The second stage describes projects that have some automated verification on the UI level. Venkat describes tools using WebDriver for UI level verification as a pathway to hell automation.This test method can be represented in the ice-cream cone anti-pattern. For projects with the right level of automation, the pyramid pattern is a good representation.The last maturity stage contains these projects with the right measure of automated verification.DisciplineVenkat drew a comparison with 1820, where patients died regularly within three weeks after being operating.Doctors (Joseph Lister, Louis Pasteur) started cleaning their tools after surgery and noticed a positive trend in survival.Analogous to the doctors back then, we need to discipline ourselves in software engineering.This discipline is needed to keep up to speed and to stay agile, so that teams can react rapidly to customer requests. To build up this discipline, automated verification can be seen as the software equivalent of exercising.  We’re practicing a beautiful craft, let’s go turn it into a wonderful profession. Focusing on quality and creative things.Feature Branches And Toggles In A Post-GitHub World (Sam Newman)Sam told us about his experience at a project where the team was having trouble merging branches.The release branch for the next release was called R3, but for a large refactoring, branch R4 was created.Afterwards, he described merging the branches as a car crash.They even needed to introduce a dedicated R3-R4 merge bug fix team.Later on, they set up Continuous Integration in order to prevent the merging issues.The code, pushed by the developers, would get automatically validated by the CI setup.The problem with the R3-R4 release was that validation was done only for a branch and not on the integrated branches.  The integration should be validated every day and when the build breaks, fix it!For unfinished work, we can wait until it is ready before checking in.This exposes us to the risk of losing work when it’s only on the developer’s computer.Feature BranchAn alternative would be to create a feature branch, which brings us back to the problem of merging branches.  Pain of merge = fn(size_of_merge, duration_since_last_merge)Merging branches can be a difficult task and might lead to a commit race, offloading the effort to a colleague.Trunk-based developmentA third option would be to ‘check in anyway’, called trunk-based development.Every commit integrates to the trunk and developers should integrate their local changes daily.Small changes and integrating often makes it easier to merge new code.New half-finished features can be hidden with feature toggles.These toggles can be managed using flags or configurations (eg, in Zookeeper, Consul, …).  A flag should be set and evaluated in as few places as possible, preferably only once each.Flags should be removed when the new implementation is done.More info: Trunk-based developmentChanges to an existing functionality can be done by providing an abstraction above the existing functionality.The new functionality can then be developed for the abstraction and when it is done, changed to the new implementation.Branch by abstraction has the side-benefit that it can be used for A/B and canary releasing.The Continuous Delivery book tells us to treat every check-in as a possible release candidate.Developers start with the assumption that it is worthy, the CI tool decides whether it truly is.Deploy frequently with small changes, making it easier to rollback and lowering the risk of running into problems.GitAnd then there was Git, developed by Linus Torvalds with the goal to merge a patch in less than three seconds.In Git, branches are much more lightweight and every local repository contains the full source history.In 2008, GitHub was founded and introduced pull requests.If you wanted to contribute to open source projects before pull requests you had to:  Develop it locally  Generate a patch file  Mail it over to the project ownersThis feature contributed to GitHub’s success as three years later in 2011, they passed SourceForge and Google Code in popularity.Sam made the remark that pull requests use branches, which might bring problems. On top of that GitFlow was introduced.Because GitFlow introduces even more branches, it is in controversy with fast deployment and small changes cycle.With tools like Split and LaunchDarkly, GitFlow is not needed, if merged frequently.The conclusion was that experimental and release branches, that might even never get merged, still have their uses.The pull request mechanism works well in open source projects.Except for experiments, releases and pull requests, Sam recommends to prevent branches and to keep batch sizes small, integrate often and ship often.A reasonable overview of Java 9 and how you could think of it (Oleg Šelajev - Slides)Since Java 9 does not seem to have a codename and Java 10 is called Project Valhalla, Oleg proposed codename Java 9 the Fury Road, a Mad Max reference.  Java 9 Release date: September 21st 2017JShellJShell is the new REPL (Read-Eval-Print Loop) for Java.It can be used to run commands and get results immediately.For user-friendliness, the semicolons can be omitted after the instructions in JShell.Example command:jshell&gt; List.of(1).getClass()$1 ==&gt; class java.util.ImmutableCollections$List1OptionalsSeveral improvements will be added to the Optional class.Optionals can be turned into streams and have filter, flatMap and map methods.For eager evaluation these functional methods can be applied directly to the Optional.jshell&gt; Optional.of(1).map(x-&gt;x*3)$2 ==&gt; Optional[3]When using stream() in front of the functional methods a ReferencePipeline is returned.This can be used for lazy evaluation.jshell&gt; Optional.of(1).stream().map(x-&gt;x*3)$3 ==&gt; java.util.stream.ReferencePipelineAn or() method will be added to chain a supplier to empty Optionals.jshell&gt; Optional.empty().or(()-&gt;Optional.of(\"Devoxx rocks!\"))$4 ==&gt; Optional[Devoxx rocks!]StreamsTwo new methods will be added to the Stream interface, dropWhile and takeWhile.For ordered streams, these methods drop or take elements while the predicate is true.In unordered streams, dropWhile returns a subset of elements starting from the first predicate match, takeWhile returns a subset of elements matching the predicate.Stream&lt;T&gt; dropWhile​(Predicate&lt;? super T&gt; predicate)Stream&lt;T&gt; takeWhile​(Predicate&lt;? super T&gt; predicate)jshell&gt; IntStream.range(1,10).takeWhile(x-&gt; x&lt;5).boxed().collect(Collectors.toList())$5 ==&gt; [1, 2, 3, 4]jshell&gt; IntStream.range(1,10).dropWhile(x-&gt; x&lt;5).boxed().collect(Collectors.toList())$6 ==&gt; [5, 6, 7, 8, 9]ConcurrencyCompletableFuture will be extended with a copy.The copied CompletableFuture is a defensive copy and completing it doesn’t complete the original CompletableFuture.jshell&gt; CompletableFuture.runAsync(()-&gt;{while(true){}})$7 ==&gt; java.util.concurrent.CompletableFuture[Not completed]jshell&gt; $7.copy()$8 ==&gt; java.util.concurrent.CompletableFuture[Not completed]jshell&gt; $8.cancel(true)$9 ==&gt; truejshell&gt; $8$8 ==&gt; java.util.concurrent.CompletableFuture[Completed exceptionally]jshell&gt; $7$7 ==&gt; java.util.concurrent.CompletableFuture[Not completed, 1 dependents]A new ProcessHandle interface will be added, it can be used to get information and control processes.Bits and piecesThe underscore will become a keyword, so assigning a value to _ does not work.This is probably a feature for the future where _ will be used for matching arguments of any type.Assigning a value to __ will keep on working.In Java 8, default methods were added to interfaces, in 9 they can be private.Property files will support UTF-8 and there is already Java 9 support in several IDEs.There will be several changes to improve String performance, for example using a more space-efficient internal representation for Strings.Javadoc will get an improved search, HTML5 compliance and more info on the module where the class or interface comes from.The use of agents will be more flexible, a process can attach an agent to itself and a JAR can contain multiple agents.ModulesThe Java Platform Modules System (JPMS) allows modularization for Java applications.A module can define dependent modules with requires, to provide an API the exports key word is used.To give access to everyone the opens keyword can be used.There is a method getAccessable().You should be aware though about using it for determining if a module is usable or not since it actually just returns the value of setAccessable(), a toggle that you have to set yourself.Actually it just returns the value of setAccessable(), a toggle that you have to set yourself.To make a smooth transition to the JPMS, any JAR on the classpath will become an automatic module.By default, --illegal-access=permit is the default mode for JDK 9, allowing modules access to all automatic modules.As a migration strategy, Oleg proposes to wait for dependencies to modularize before modularizing yourself.Otherwise you might need to modularize twice to align with the dependencies.Java 9 with Maven is complicated, many plugins need to be upgraded and a lot of functionality is not yet fully integrated with JPMS.Gradle releases fixes more often and currently supports more features.A multi-release JAR containing multiple versions for the same file, in the same JAR, is a new feature that should be used with caution.Garbage collectionThe G1 Garbage Collector (G1GC) will become the default in Java 9.Previous Garbage Collectors were not as scalable nor predictable.The G1GC promises a more scalable and more predictable system with few modification options.By default, a quarter of the physical ram will be allocated to the heap, unless the size is specified with the -mx flag.Due to its new heap division system, it might run into problems with large chunks of data.It is recommended to feed streams of data directly to parsers without first capturing it in a byte array, this also applies to JSON parsing and database operations.Another improvement is using immutable objects wherever possible.Using a StringBuilder instead of concatenating Strings will reduce heap usage. For more info, Oleg referred to a talk on Moving to G1GC by Kirk PepperdineHTTP/2 ClientThe JDK 9 will contain an incubator package with a HTTP/2 client with a fluent API.The modules in the incubator package are non-final APIs that can be finalized or removed in future releases.HttpClient.newHttpClient().send(        newBuilder(URI.create(\"https://google.com\"))            .GET()            .build(),        HttpResponse.BodyHandler.asString())    .body();)Oleg concluded by recommending the audience not to touch multi-release JAR, jlink and Unsafe, unless you are 100% sure what you are doing.For now, he recommends to upgrade your IDE and tools and upgrade Spring to version 5.0. Then add the --illegal-access=warn startup option and fix the easy fixable warnings and then wait a year or more until the classpath and the libraries, you depend on, are upgraded.Yannick, a colleague at Ordina did a nice presentation on Java 9: A first look at Java 9 by Yannick De Turck.Presentation and sources are available at GitHub.The Language of Actors (Vaughn Vernon - Slides)Vaughn started his talk by introducing Rear Admiral Grace Hopper to the audience.In the American Navy, she was a Computer Scientist and wrote software for a long time.She was really into not wasting cycles and emphasised on not wasting nanoseconds.Then Vaughn introduced Donald Knuth, another legend in Computer Science.Knuth is known from the quote Premature optimization is the root of all evil.But that is not exactly what he said, the full quote says:  We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.Another quote was shared of Donald Knuth:  People who are more than casually interested in computers should have at least some idea of what the underlying hardware is like. Otherwise the programs they write will be pretty weird.To further build his point, Vaughn told about a project that was written in Cobol.The code was across 5 diskettes thus user interaction was needed to run the application.To improve the usability of the application, it was rewritten in C, allowing the software to fit on just one diskette.With this introduction, Vaughn wanted to emphasize how hard it is to optimize software for resource usage.Threading is hardIn 1973, academics discovered the Actor Model. 13 year later in 1986 Joe Armstrong rediscovered the approach.Armstrong designed and implemented a programming language on this model, ErlangIn 2008, Jonas Bonér came up with Akka for the Java Virtual Machine and in 2011 José Valim came up with another Actor based language called Elixir.Because the Actor Model is Message Driven, it inherently is Reactive.Now is the time for the Actor Model, with the decreasing expense of memory, network and chips.Processors are having a lot of cores these days, Intel Xeon units go up to 88 cores, Intel Xeon Phi can have more then 200 coprocessors.The actor model allows us to embrace latency. If we design for latency, it will not have a blocking impact on the design.We are not at Google scale, why use actors?With the Actor Model you can do more with less. The total number of nodes can be reduced to just a few, several million actors per machine is not a problem.The actor model uses the essence of Domain Driven Design (DDD), the bounded context and ubiquitous language.DDD is excellent way to make complexity surrender, by knowledge crunching.Actors help us reason better by having less moving parts.This allows us to focus on business aspects, instead of the architecture around it.How to do DDD in projects:  Talk with customer (iterate)  Write some scenarios (iterate)  Strategic Event Storming (iterate)  Tactical Event Storming (iterate)  Implement acceptance tests and model (iterate)This concludes our recap of this amazing edition of Devoxx Poland."
      },
    
      "spring-2017-07-11-springio17-summary-html": {
        "title": "Spring I/O 2017 Recap",
        "url": "/spring/2017/07/11/SpringIO17-Summary.html",
        "image": "/img/springio2017.jpg",
        "date": "11 Jul 2017",
        "category": "post, blog post, blog",
        "content": "  On the 18th and 19th of May, we had another great edition of Spring I/O, brought to us by organizer Sergi Almar.In this blog post, we will go over some of our favourite sessions of the conference.  DISCLAIMER: we could not include ALL the talks from Spring IO in this blogpost. We provide an extensive summary of our favorite talks and created a curated list of all the talks and resources at the bottom of this post.  Table Of Contents      Keynote: The Only Constant Is Change    Bootiful Database-centric Applications with jOOQ    Google Spanner    Easily secure and add Identity Management to your Spring(Boot) applications with Keycloak    Spring Cloud Streams (Data Services)    The Road To Serverless: Spring Cloud Function    Reactive Spring Data    The Future Of Event Driven Microservices With Spring Cloud Stream    New in Spring 5: Functional Web Framework    Spring Auto REST Docs    References  1. Keynote: The Only Constant Is Changeby Andy Wilkinson and Stéphane NicollObviously, the keynote presentation was filled with announcements, interesting tidbits and two great presenters.  The biggest topic revolved around how Spring has always tried to enable developers to adapt their applications rapidly.This capacity of adapting to change increased dramatically when Spring Boot was released, which explains part of its success.They also reported the release of Spring Boot 2.0.0 M1.This release was announced very shortly after Spring Framework 5.0 went RC1 on the 8th of May.The keynote can be found here and the demo code here.2. Bootiful Database-centric applications with jOOQby Michael SimonsAs a personal fan of @rotnroll666, I went to see his talk, fully expecting to be impressed by the level of quality and code.As always, Michael delivered: he made me realize there are still plenty of PL/SQL developers out there in the enterprise landscape, who hold on to their code like kids to their favorite stuffed animal before bedtime.Luckily for us, someone created a library called jOOQ:  jOOQ generates Java code from your database and lets you build type safe SQL queries through its fluent APIThere are many use cases where jOOQ can prove useful, but the example that Michael used described the enterprise environment at his current employer:He was working for a big utility company in Germany, who developed applications on power and gas usage consisting of lots of time-series data.Almost all of the data was stored in SQL databases with a big layer of PL/SQL on top.They also developed some desktop GIS applications using Oracle Spatial to visualize the data.So the question they asked themselves at a certain moment:  Should we approach all of our data using plain PL/SQL? Should we use an ORM tool like Hibernate? Or can we use something in between?Of course, as with any good question, the answer is:  It depends. There is no silver bulletThere are many options in the Java space to approach this problem:  Plain JDBC  Using a JDBCTemplate  JPA with JPQL and / or Criteria APIMichael’s team solved most of their problems using Hibernate, and while that comes with several advantages, it doesn’t mean you have to use it for everything.One of the improvements they made was using Hibernate combined with jOOQ:  Use Hibernate for the regular database queries and day-to-day manipulations of your database  Use jOOQ for your complex queries and database migrationsTo briefly summarize what jOOQ can do for you:  jOOQ is SQL-centric which means jOOQ infers information from the actual database, not from the ORM model  It exposes a typesafe meta-model generated from your SQL  The Query Builder framework uses a very legible DSL (in a much more concise way than the Criteria API)  It generates a Java-based schema (using Maven or Gradle)  It can reverse-engineer an existing DB and generate SQL  Spring provides integration with Spring Boot through the spring-boot-starter-jooq dependencyAn ideal scenario to use jOOQ would be to:  Run your database migration with Flyway or Liquibase first  Run the code generator to generate the Java DSL context (this happens in the Maven generate-sources lifecycle phase)  Use the DSL context to write your typesafe queries, for example:BookRecord book = create.selectFrom(BOOK).where(BOOK.ID.eq(1)).fetchOne();For more information about jOOQ, you can check out their website.  Question: What’s the difference between the jOOQ Query API and using JPA Criteria API with the Hibernate ModelGen, which is also typesafe?  It resembles the native SQL much better  jOOQ provides standardization since it performs SQL transformations that work for any SQL dialect  It should make it easier to migrate existing PL/SQL applications  There is a much more extensive collection of SQL functions and possibilities  jOOQ provides POJO mappers which are also generated from the code generator  As much as you hate them, it supports calling Stored Procedures!The code from Michael’s talk can be found on Github.3. Google Spannerby Robert KubisGoogle Spanner is a globally distributed relational database service that provides ACID transactions and SQL semantics, without giving up horizontal scaling and high availability.When building cloud applications, you are no longer forced to choose between traditional databases that guarantee transactional consistency, or NoSQL databases that offer simple, horizontal scaling and data distribution.Cloud Spanner offers both of these critical capabilities in a single, fully managed service.With Spanner, your database can scale up and down as needed, and you only pay for the amount you use.Spanner keeps application development simple by supporting standard tools and languages in a familiar relational database environment.It supports distributed transactions, schemas and DDL statements, SQL queries and JDBC drivers and offers client libraries for the most popular languages, including Java, Go, Python and Node.js.As a managed service, Cloud Spanner provides key benefits to DBAs:  Focus on your application logic instead of spending valuable time managing hardware and software.  Scale out your RDBMS solutions without complex sharding or clustering.  Gain horizontal scaling without migration from relational to NoSQL databases  Maintain high availability and protect against disaster without needing to engineer a complex replication and failover infrastructure.  Gain integrated security with data-layer encryption, identity and access management and audit logging  4. Easily secure and add Identity Management to your Spring(Boot) applications with Keycloakby Sébastien BlancI must say, this was one of the funniest talks of the conference.Sébastien knows how to entertain the crowd and he kicked off with a great quote which, of course, I immediately stole and tweeted:Similar to a quite from @sebi2706 last week at #springio17 : forget about companies, it&#39;s all about community and code! 🙌&mdash; Dieter Hubau (@dhubau) May 26, 2017First of all, let’s forget that Keycloak was created by Redhat and that it is written in Java EE.The following aspects of Keycloak are more important:  It’s Open Source)  Redhat provides support through their commercial fork called Redhat SSO)  Great Spring Boot Integration through the use of a Spring Boot Starter)  Seamless Spring Security Integration  Supports OAuth 2.0, SAML 2.0, OpenID Connect  Integration with Active Directory, LDAP and even Kerberos (start drooling enterprise users!)It’s actually quite easy to setup Keycloak:  Download the Keycloak standalone server  Extract and run it  Start the server and create an admin user  Create a new realm  Create a new application  Add roles to your application  Create a user to authenticate with  Create a Spring Boot application at The Happiest Place On Earth) and include the Keycloak starter  Add the Keycloak properties to your application.yml:          server URL      realm      resource name of your application      security constraints for your users        Run!  There are many additional features for power users:  Automatic registration of applications should be possible using a one-time token (coming soon?)  Centralized User Management  CORS support for Single Page Applications  Social Login Integration  Registration and Forgot Password functionality, all out-of-the-box, configurable at runtime  UI Customization of all pages is possible through theming (start drooling designers!)All in all, the setup and demo went very smooth and I genuinely feel this product is about to become very popular, partly because of the Spring Boot integration, but also because it just seems very solid and user-friendly.There might be a dedicated blogpost coming soon about Keycloak, so stay tuned and check our blog regularly or subscribe to our RSS feed!5. Spring Cloud Streams (Data Services)by Michael MinellaMichael gave a summary about all the new projects in the Spring ecosystem that process data and / or messages very well.He explained that there are lots of big data frameworks out there (Hadoop, Spark, …), which can handle BIG amounts of data very well.However, they are usually too bulky / difficult / inappropriate for handling smaller volumes of data.Also, for quickly setting up something like Hadoop or Spark, the learning curve is too high and the effort doesn’t justify the benefits.Solution: data microservices  Developed and tested in isolation, also easier to test  Independently scalable depending on data processing load  Familiar development model, just like regular cloud-native Spring microservices  Easier to govern for Ops  So the need for data / app integration / composition arises  Which means the need for orchestration and operational coverage arises (lots of plumbing required)Spring Cloud Stream  Streams are thin wrappers around Spring integration  Supported binder for integration between services: Kafka, RabbitMQ, …  Source, Processor, Sink model is easy to comprehendSpring Cloud Task  Tasks are finite microservices, built around Spring Batch  “Microservices that end”  Contain Task repository which tracks run/start/end of the tasks  Has Spring Batch integration (partition steps using workers)  Has Spring Cloud Stream integration (eg. launch Tasks via Streams)  Simple annotation @EnableTask  Use cases: batch jobs, scheduled one-off processes, ETL processing, data scienceSpring Cloud Data Flow  AKA the new and Improved Spring XD  Data flow orchestrator  Use a shell or the UI which goes over REST endpoints  Has custom DSL  All the components are regular Spring Boot apps (Data Flow server, Shell, …)  Data Flow server has datastore for task repository, batch repository, configuration, …  Data Flow server does not do any of the actual work  We will be publishing a fun blogpost about Spring Cloud Streams soon, so stay tuned or subscribe to the RSS feed!6. The Road to Serverless: Spring Cloud Functionby Dr. Dave SyerFaaSIn recent years we’ve seen the emergence and evolution of following cloud abstraction layers in order of abstraction level:  Virtual Machines (IaaS)  Containers (CaaS)  Applications (PaaS)  Functions (FaaS)The Goal of each of these is raising the value line; in other words, the purpose of each of these is to abstract away various concerns that are of no business value(e.g. setting up and maintaining infrastructure, the environment the code has to run in…).The latest and most extreme level of these is FaaS (or ‘serverless’).Basically all the programmer should do in a FaaS environment is write a Function and hand it over to the platform.The platform takes care of:  Making sure the function is executed on demand  Deploying and undeploying (often on demand!)  Scaling up the amount of instances quickly and in parallel if the need arises (is easier with functions since they are simpler in nature than applications)  Managing integrations with other systems  The naming scheme of “serverless” is unfortunate; of course you’re gonna have servers, you just don’t care about themProblem  By now there are a lot of FaaS solutions out there; AWS Lambda, Google Cloud Function, Azure Function, IBM Openwhisk, Fission, Kubeless, …  Deploying and programming functions is different for each platform because you have to use their native APIs and they have their own platform to deploy on.  Running and testing these functions locallyEnter Spring Cloud FunctionThe purpose of the new project called Spring Cloud Function is to solve these problems by:  Keeping all advantages of serverless/functions, but with all the possibilities that Spring offers (DI, integration, autoconfig, build tools)  Providing a low entry level for Spring devs to jump on the FaaS model  Providing a low entry level for FaaS people without having knowledge of Spring  Making it possible to run the same business logic as web endpoint, stream processor or a task  Introduce an uniform programming model across providers and able to run standalone (not on a IAAS or PAAS).  Support a reactive programming model (Flux, Mono) as wellThe project will try to achieve this by:  Supporting the familiar Java 8 Function types:        @SpringBootApplication    public class Application {        @Bean        public Function&lt;String, String&gt; uppercase(){           return (value) -&gt; value.toUpperCase();        }        public static void main(String[] args) {            SpringApplication.run(Application.class, args);        }    }          As well as the Reactive Types Flux and Mono :        ...    public Function&lt;Flux&lt;String&gt;, Flux&lt;String&gt;&gt; uppercase() {        return flux -&gt; flux.map(String::toUpperCase);    }    ...          Building/deploying the Function as a web endpoint, a task or a stream  can be done by merely altering dependencies, for example:          Deploying the function as a web endpoint can be done by adding the dependency spring-cloud-function-web        In a similar fashion, it will also possible to build for a platform like AWSConclusionIn a way the goal of FaaS is similar to the Spring framework; allowing the developer (or the IT department) to focus on writing code that has real value.The purpose of FaaS is to help us with infrastructure, scalability etc for the functions we write while Spring cloud function will allow us to write and deploy these functions in an (almost) platform agnostic fashion.At the same time, it will enable the programmer to leverage the Spring Framework with it’s various features that helps the programmer to focus even more on his main purpose; programming things that deliver real value: business code!PS: Kenny Bastani has just published a VERY detailed blogpost about Spring Cloud Function on AWS Lambda.It’s a follow-up of his earlier blogpost about Event-driven Microservices using CQRS and Serverless.I would highly recommend his blog!Spring Break  7. Reactive Spring Databy Christophe StroblBiggest changes of Spring Data Kay M3  Java 8 baseline  ID classes don’t need to be Serializable anymore  breaking change: No more null in the repositories (arguments or return values)  breaking change: More consistent naming (eg. findOne -&gt; findOneById)  Composable interfaces (separate Readable / Insertable and make custom repositories as composable interface as well)  Builder style Template APIs  Kotlin extensions are coming in M4Data Store specifics  MongoDB:          breaking change: MongoDB driver baseline to 3.x      Introduction of ReactiveMongoTemplate      Enhanced aggregation support      Collation support        Cassandra:          breaking change: Update to Cassandra 3.2      No reactive native driver –&gt; mimicking reactive driver with thread pool (and blocking) underneath (with ReactiveCassandraTemplate)        Redis:          JRedis discontinued      Upgraded to Lettuce 5 (not GA yet though) supports native reactive driver        Gemfire:          Lucene index support      Off-heap, Redis Adapter, Security annotation config        Solr:          Upgrade to Solr 6        Spring Data REST:          CORS config mechanism      Improved headers      Improved media type support      8. The future of event driven microservices with Spring Cloud Streamby Kenny BastaniEvolutionMonolith applicationThere are some cultural problems with monoliths.One big application slows down the velocity of getting into production.Everyone has to use a shared deployment pipeline.For large code-bases it is harder for new engineers to get up to speed.The engineers that were there from the beginning, who designed the application, are busy explaining the history of the application to new engineers joining the project. These developers are creating change but might get blocked by DBA and Ops teams.Monolith organizationCentralized authority for operations, database and change management slows progress.These coordinated releases batch many changes together from different teams.Usually operations drives the runtime environment of applications because they take all operation responsibility including upgrades of virtual machines.The key problem is that everything is deployed at once or nothing at all.Move towards SOAWith Service-oriented Architecture the application is split up in components which can be deployed individually but now the key problem are the shared libraries.Releasing a change in an object that is not shared can be done separate, but still a problem for the shared ones.Now we arrived at MicroservicesThere are a lot of improvements but Microservices also adds the complexity of running a distributed system.Small, two pizza (5-8 members), teams organized around business capabilities with responsibility of running their own services.We gain independent deployability because each team produces and consumes rest APIs to communicate.Team also have more freedom to chose the best tool for the job they are facing.Microservices brings the challenge of eventual consistency, in a monolith you could rollback a transaction at the database level if something went wrong. Now eventual consistency is not guaranteed, inconsistency happens all the time. Rolling back transactions across multiple services is not easy.Is it a Monolith or Microservice?  If it takes longer than a day for an engineer to ramp up its probably a monolithIn a typical architecture the front-end teams integrate directly with the microservices.This is an anti-pattern in distributed systems. Consumers should not have to worry which instance of the replicated services they have to go to.You could use Spring Cloud, it allows you to centralize authentication with OAuth tokens and routing through an  API Gateway. The front-end does not need to be concerned with all these services, for them it looks like consuming a monolithic API.Splitting the monolithThe popular route from monolith to a microservice architecture is slicing off bits of functionality.This is hard in practice, splitting up a schema is usually the complex part.Refactoring out functionality and tables to new services can be hard because of foreign key constraints for example.Why we need event-driven microservicesThe problem with microservices is that there are no foreign key constraints between services.Furthermore, distributed transactions are brittle and distributed systems are hard.  You will drown in problems you didn’t know existed!Without event-driven microservices and an audit trail you will never know why something went wrong.This audit trail allows you to reason about what went wrong and roll back state.Rules for Event-driven microservicesA lot of these rules are from reference applications and Kenny’s work with Chris Richardson.Domain events are a first class citizen, every time you change some piece of data, domain events should be exchanged.These events can be used as an audit trail to determine why state changed in a system.Each domain event contains a subject with the project aggregate and a payload with immutable data.@Entity@EntityListeners(AuditionEntityListener.class)public class ProjectEvent {    @Id    @GeneratedValue(strategy = GenerationType.AUTO)    private Long eventId;        @Enumerated(EnumType.STRING)    private ProjectEventType type;        private Project entity;        private Map&lt;String, Object&gt; payload;To make this way of working accessible to the developers, hypermedia APIs need to expose links on the aggregates.A traversal list of command and a log of events that happened should be accessible.Command handlers trigger commands on aggregate and then the command is going to generate events.Every domain event applies a state transition to an aggregate.Event handlers are going to subscribe to an event and apply changes to an aggregate to change the state.In a graph representation, event handlers would be the nodes and events the edges.CQRS is used to create materialized views from streams of events.With CQRS you will have a command side and a query side.For example the commands might be written to an Apache Kafka event store.Then an event processor could be using Spring Cloud Stream to retrieve these events and create a data model.The data model is then written to a Data Store like MySQL, where the query side reads the data.An API gateway, like Spring Cloud Netflix Zuul, can be put in front so it looks for the consumer like a regular microservice.For deploying this application you can combine these components together or scale them independently.ServerlessChanges the pricing model for the execution on a cloud provider.With Serverless you are going to have a function in the cloud and you are going to pay for each execution.It is an event driven model, so if data is fed to for instance a AWS Lambda function this can invoke other functions in Python for example.Kenny concluded with a demo and recommended Dave Syer’s Talk on Spring Cloud Functions for more info about serverless.Spring break  9. New in Spring 5: Functional Web Frameworkby Arjen PoutsmaIn the keynote Andy Wilkinson and Stéphane Nicoll mentioned that the Spring framework and especially Spring Boot is all about providing choices to developers.The framework provides us with tools to tackle problems in multiple ways. In the light of this, starting from Spring 5 there will be a new functional alternative to handle incoming web requests.This new functional web library is an alternative to annotation driven approach that is broadly applied in current applications.  Arjen Poutsma states that some people are not happy with magic that happens behind the scenes when you use annotations like @RequestMapping or the newer @GetMappingThis was one of the reasons that made Spring develop this new library.In the next sections we show a quick introduction to what was shown at Spring IO about what this new framework has to offer.Handler function exampleThe following UserHandler class is the replacement of the Controller class that we would have annotated in the regular web framework. In this new functional style the way we handle requests is a bit different.We define functions that have a ServerRequest as parameter and we return a Mono with a ServerResponse.The request contains all the information we need.It contains the body of the request, pathvariables, request headers, …So no more injecting pathvariables and body objects, we have everything we need in this ServerRequest.What we return is the ServerResponse in which we can easily put all the information we want to give back to the client.And Spring provides us with an easy builder to create such a response as it already did with the ResponseEntity builder.You can see that these new objects and builders provide us with an easy and declarative way to handle requests and create responses, without the “magic” that we used previously with the annotations.public class UserHandler {    public UserHandler(UserRepository repository) {        this.repository = repository;    }        public Mono&lt;ServerResponse&gt; getUser(ServerRequest request) {        int userId = Integer.valueOf(request.pathVariable(\"id\"));        Mono&lt;ServerResponse&gt; notFound = ServerResponse.notFound().build();        Mono&lt;User&gt; userMono = this.repository.getUser(personId);                return userMono                .flatMap(user -&gt; ServerResponse.ok().contentType(APPLICATION_JSON).body(fromObject(user)))                .switchIfEmpty(notFound);    }        public Mono&lt;ServerResponse&gt; createUser(ServerRequest request) {        Mono&lt;User&gt; user = request.bodyToMono(User.class);        return ServerResponse.ok().build(this.repository.saveUser(user));    }        public Mono&lt;ServerResponse&gt; listUsers(ServerRequest request) {        Flux&lt;User&gt; people = this.repository.allUsers();        return ServerResponse.ok().contentType(APPLICATION_JSON).body(users, User.class);    }}We have defined how we want to handle requests and how we translate it to a response.What we need next, is a way to say which requests will be handled by which handler function.In Spring MVC, this was done by adding an annotation that declared some parameters, for example, to couple a request path to a controller method.The functional web framework does this by creating RouterFunctions.This RouterFunction is a function that takes a ServerRequest and returns a Mono&lt;HandlerFunction&gt;. To choose which requests get handled by which HandlerFunction, Spring again provides us with some builder functions.That way we can easily bind the handlers we just created with a path as shown in the next code example.Router examplepublic RouterFunction&lt;ServerResponse&gt; routingFunction() {    PersonHandler handler = new PersonHandler(userRepository);        return nest(path(\"/person\"),            nest(accept(APPLICATION_JSON),                    route(GET(\"/{id}\"), handler::getPerson)                    .andRoute(method(HttpMethod.GET), handler::listPeople)            ).andRoute(POST(\"/\").and(contentType(APPLICATION_JSON)), handler::createPerson));}Creating a Tomcat serverNow that we have declared which routes are handled by which functions we have to let our server know this.In the next code example we show how to create a Tomcat server and how to bind the RouterFunction to our server.public void startTomcatServer() throws LifecycleException {    RouterFunction&lt;?&gt; route = routingFunction();    HttpHandler httpHandler = toHttpHandler(route);        Tomcat tomcatServer = new Tomcat();    tomcatServer.setHostname(HOST);    tomcatServer.setPort(PORT);    Context rootContext = tomcatServer.addContext(\"\", System.getProperty(\"java.io.tmpdir\"));    ServletHttpHandlerAdapter servlet = new ServletHttpHandlerAdapter(httpHandler);    Tomcat.addServlet(rootContext, \"httpHandlerServlet\", servlet);    rootContext.addServletMapping(\"/\", \"httpHandlerServlet\");    tomcatServer.start();}And then in the main method we only have to start our Tomcat server and we’re up and running:public static void main(String[] args) throws Exception {    Server server = new Server();    server.startTomcatServer();}ConclusionThe new functional web framework gives us a more declarative and functional way to create a server and handle web requests. In my opinion this code is a lot clearer because you have a direct link between routing and handling requests.This code may also be easer to test than the annotation driven web request handling because we don’t necessarily need to fire up our spring context to test the routing.We can just create a unit test for our RouterFunction and verify our routes are correct.What I do still wonder about is how this integrates with Spring security.How can we define which users can access which handler.Do we still do this with annotations or will we get a new way to do this as well?The Spring functional web framework is an interesting new development and we will be following it closely to see how we can use it in our new projects.Spring break  10. Spring Auto REST Docsby Florian BenzSpring Auto REST Docs is an extension on Spring REST Docs (our post on Spring REST Docs can be found here).This extension helps you to write even less code by including your Javadoc into the Spring REST Docs.For a more detailed overview on what is possible and how to start using this extension, please visit the official documentation here.Imagine you have the following method in your controller:@RequestMapping(\"users\")public Page&lt;ItemResponse&gt; searchItem(@RequestParam(\"page\") Integer page, @RequestParam(\"per_page\") Integer per_page) { ... }With the following POJO:public class User {    private String username;    private String firstName;    private String lastName;        ...}And the test that generates Spring REST Docs:this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk())\t.andDo(document(\"users\",     requestParameters(         parameterWithName(\"page\").description(\"The page to retrieve\"),         parameterWithName(\"per_page\").description(\"Entries per page\")     ),    responseFields(            fieldWithPath(\"username\").description(\"The user's unique database identifier.\"),            fieldWithPath(\"firstName\").description(\"The user's first name.\"),            fieldWithPath(\"lastName\").description(\"The user's last name.\"),    )));When using Spring Auto REST Docs, this could be replaced by adding Javadoc to the method in the controller:/** * @param page The page to retrieve * @param per_page Entries per page */@RequestMapping(\"users\")public Page&lt;ItemResponse&gt; searchItem(@RequestParam(\"page\") Integer page, @RequestParam(\"per_page\") Integer per_page) { ... }And adding Javadoc to the POJO fields:public class User {    /**    * The user's unique database identifier.    */    @NotBlank    private String username;        /**    * The user's first name.    */    @Size(max = 20)    private String firstName;        /**    * The user's last name.    */    @Size(max = 50)    private String lastName;        ...}And then removing the requestParameters and responseFields from the test:this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk());You notice that I added the annotations @NotBlank and @Size in the POJO, these annotations will also be reflected in the resulting documentation.You could also create your own annotations.Result:            Path      Type      Optional      Description                  username      String      false      The user’s unique database identifier.              firstName      String      true      The user’s first name. Size must be between 0 and 20 inclusive.              lastName      String      true      The user’s last name. Size must be between 0 and 50 inclusive.      Because the description of the POJO is now added on field level, it is guaranteed that this description will be the same everywhere this field is used, meaning less maintenance is needed.11. ReferencesYoutube PlaylistAll the talks of Spring IO 2017 are available on Youtube.Talks: Day One            Topic      Presenter(s)      Resource(s)                  KEYNOTE - The Only Constant Is Change      Stéphane Nicoll, Andy Wilkinson                    Reactor 3, the reactive foundation for Java 8 (and Spring 5)      Simon Baslé                    Architecture Deep Dive in Spring Security      Joe Grandja                     The Spring ecosystem in 50 minutes      Jeroen Sterken                    Bootiful Development with Spring Boot and Angular [WORKSHOP]      Matt Raible                    Spring Boot at AliExpress      Juven Xu                    Database centric applications with Spring Boot and jOOQ      Michael Simons                    Testing for Unicorns      Alex Soto                    Front Ends for Back End Developers      Matt Raible                    The Beginner’s Guide To Spring Cloud      Ryan Baxter                    Microservices, but what about the UI      Marten Deinum                     Making the most of Spring boot: adapt to your environment! [WORKSHOP]      Arjan Jorritsma, Erwin Hoeckx                     New in Spring 5: Functional Web Framework      Arjen Poutsma                    Deep Learning with DeepLearning4J and Spring Boot      Artur Garcia, Dimas Cabré                     Easily secure and add Identity Management to your Spring(Boot) applications      Sébastien Blanc                     The Future of Event-driven Microservices with Spring Cloud Stream      Kenny Bastani                    Container orchestration on Apache Mesos - DC/OS for Spring Boot devs      Johannes Unterstein                     Building Spring boot + Angular4 apps in minutes with JHipster      Deepu K Sasidharan                     Hands-on reactive applications with Spring Framework 5 [WORKSHOP]      Brian Clozel, Violeta Georgieva                    DDD Strategic Design with Spring Boot      Michael Plöd                    Awesome Tools to Level Up Your Spring Cloud Architecture      Andreas Evers                    Surviving in a Microservices Team      Steve Pember            Talks: Day Two            Topic      Presenter(s)      Resource(s)                  Reactive Spring      Mark Heckler, Josh Long                    Spanner - a fully managed horizontally scalable relational database with ACID transactions that speaks SQL      Robert Kubis                     Reactive Spring UI’s for business      Risto Yrjänä                     Hands-on reactive applications with Spring Framework 5 [WORKSHOP]      Brian Clozel, Violeta Georgieva                    Data Processing With Microservices      Michael T Minella                    Protection and Verification of Security Design Flaws      Marcus Pinto, Roberto Velasco                     Experiences from using discovery services in a microservice landscape      Magnus Larsson                     Harnessing the Power of Spark &amp; Cassandra within your Spring App      Steve Pember                    It’s a kind of magic: under the covers of Spring Boot      Andy Wilkinson, Stéphane Nicoll                     Introducing Spring Auto REST Docs      Florian Benz                    Leveraging Domain Events in your Spring Boot Microservices [WORKSHOP]      Michael Plöd                    Functional web applications with Spring and Kotlin      Sébastien Deleuze                    Setting up a scalable CI platform with jenkins, docker and rancher in 50 minutes      Wolfgang Brauneis, Rainer Burgstaller                      The Road to Serverless: Functions as Applications      Dave Syer                     TDD with Spring Boot - Testing the Harder Stuff      Sannidhi Jalukar                     Splitting component containers to simplify dependencies      Eugene Petrenko                    Build complex Spring Boot microservices architecture using JHipster [WORKSHOP]      Deepu K Sasidharan                    Caching Made Bootiful      Neil Stevenson                    Getting Thymeleaf ready for Spring 5 and Reactive      Daniel Fernández                     Developing a Spring Boot Starter for distributed logging      Carlos Barragan                     Reactive Meets Data Access      Christoph Strobl                    Building on spring boot lastminute.com microservices way      Luca Viola, Michele Orsi                     Growing Spring-based commons, lessons learned      Piotr Betkier                     CQRS with Spring Cloud Stream [WORKSHOP]      Jakub Pilimon                     Develop and Run your Spring Boot application on Google App Engine Flexible      Rafael Sánchez                    Manage distributed configuration and secrets with Spring Cloud and Vault      Andreas Falk                    From Zero to Open Source Hero: Contributing to Spring projects      Vedran Pavic            "
      },
    
      "paas-2017-06-29-openshift-an-introduction-html": {
        "title": "OpenShift: An introduction",
        "url": "/paas/2017/06/29/Openshift-an-introduction.html",
        "image": "/img/Openshift.png",
        "date": "29 Jun 2017",
        "category": "post, blog post, blog",
        "content": "        Why my first blog about OpenShiftWhen I started as a developer, the cloud ecosystem started expanding and became the next big thing.So obviously I wanted to see what all the fuzz was about and started taking a deeper look at it.Soon, I got introduced with so many new technologies I was not familiar with: microservices, containers, pods, Kubernetes, load balancing, Docker, PaaS,…To be honest, for me it was really overwhelming.I told myself, I would never have the time to become a guru in all these technologies to start with cloud native development.So I just sat in a corner crying about why I became a dev in a time where things never looked more complicated and changed faster than ever before.But actually it’s not all that complicated.To be honest, deploying your containers in the cloud and managing things are easier than ever before with PaaS and OpenShift.A couple of months ago, I got introduced to OpenShift and got really excited about it!Recently, Ordina gave me the chance to visit the Red Hat’s partner conference and my excitement for OpenShift reached new heights.With my body being unable to contain all that excitement for OpenShift, I had to funnel it into a blog post or otherwise I would spontaneously combust.I do have to mention that if you want to work with OpenShift, you still need to have a basic understanding about containers, PaaS and Kubernetes if you want to understand some of its magic.If you have no idea what Docker containers are or what Kubernetes is, don’t panic! There are some great blogposts on the JWorks blog explaining more about them.What is OpenShiftOpenShift is a PaaS. For those who don’t know what a PAAS is, stop reading now, take a timecab to the year 2011 and check it out because PaaS is awesome.Gartner calls OpenShift a Cloud Enabled Application Platform (CEAP).For those who are not sure anymore, here is a quick reminder.  Platform as a service (PaaS) or application platform as a service (aPaaS) is a category of cloud computing services that provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an appThere are multiple PaaS providers available.For example, you have OpenShift, Cloudfoundry, Heroku, Google App Engine and more.Most of these platforms offer a lot of the same solutions, each with their own pros and cons, but today we are going talk about OpenShift specifically.When I look up OpenShift in Google (since that’s the first thing we do these days), it gives me the following explanation:  OpenShift Container Platform (formerly known as OpenShift Enterprise) is Red Hat’s on-premise private platform as a service product, built around a core of application containers powered by Docker, with orchestration and management provided by Kubernetes, on a foundation of Red Hat Enterprise Linux.Well that explains it!I never thought writing my first blog post would be that easy!Obviously you wouldn’t be reading this blog post if this was my only explanation since my pull request would never be accepted.If I would try to explain it with my own words to someone who never heard of OpenShift, I would define it like this.  OpenShift container Platform is a platform as a service you can deploy on a public, private or hybrid cloud that helps you deploy your applications with the use of Docker containers.It is build on top of Kubernetes and gives you tools like a webconsole and CLI to manage features like load balancing and horizontal scaling. It simplifies operations and development for cloud native applications.Okay, I know this is still pretty vague and it can do so much, so why don’t we simply start with seeing where OpenShift fits in.  As you can see in the image, the IT landscape has evolved a lot in recent years.We now have DevOps, Microservices, Containers, Cloud and Kubernetes.OpenShift combines all of those things in one platform you can easily manage.So it actually fits right on top of all of that.Overview  SELF-SERVICEDevelopers can quickly and easily create applications and deploy them.With S2I (Source-to-Image), a developer can even deploy his code without needing to create a container first.Operators can leverage placement and policy to orchestrate environments that meet their best practices.It makes your development and operations work fluently together when combining them in a single platform.POLYGLOT, MULTI-LANGUAGESince it deploys Docker containers, it gives you the ability to run multiple languages, frameworks and databases on the same platform.You can easily deploy microservices written in Java, Python or other languages.AUTOMATIONBuild automation:OpenShift automates the process of building new container images for all of your users.It can run standard Docker builds based on the Dockerfiles you provide and it also provides a “Source-to-Image” feature which allows you to specify the source from which to generate your images.This allows administrators to control a set of base or “builder images” and then users can layer on top of these.The build source could be a Git location, it could also be a binary like a WAR/JAR file.Users can also customize the build process and create their own S2I images.Deployment automation:OpenShift automates the deployment of application containers. It supports rolling deployments for multi-containers apps and allows you to roll back to an older version.Continuous integration:It provides built-in continuous integration capabilities with Jenkins and can also tie into your existing CI solutions.The OpenShift Jenkins image can also be used to run your Jenkins masters and slaves on OpenShift.ScaleWhen you want to start scaling your application, whether it’s from one replica to two or scale it to 2000 replicas, a lot of complexity is added.OpenShift leverages the power of containers and an incredibly powerful orchestration engine to make that happen. Containers make sure that applications are packed up in their own space and are independent from the OS, this makes applications incredibly portable and hyper scalable. OpenShift’s orchestration layer, Google’s Kubernetes, automates the scheduling and replication of these containers meaning that they’re highly available and able to accommodate whatever your users can throw at it.This means that your team spends less time in the weeds and keeping the lights on, and more time being innovative and productive.OpensourceThere are multiple versions of OpenShift (spoiler: it’s going to be the next topic in this blog post) but they are all based on OpenShift Origin.Origin provides an open source application container platform. All source code for the Origin project is available under the Apache License (Version 2.0) on GitHubOpenShift landscapeThere are a few different OpenShift releases depending on what you need.As of this writing, the OpenShift landscape looks like this:  OpenShift OriginIt’s the upstream community project used in OpenShift Online, OpenShift dedicated and OpenShift container Platform.It’s build around Docker and Kubernetes cluster management.Origin is augmented by application lifecycle management functionality and DevOps tooling.Origin updates as often as open source developers contribute via Git.Sometimes as often as several times per week.Here you get the new feature the quickest but at the cost of stability.OpenShift container platformFormerly known as OpenShift Enterprise.It’s the platform software to deploy and manage OpenShift on your own infrastructure of choice.It integrates with Red Hat Enterprise Linux 6 and is tested via Red Hat’s QA process in order to offer a stable, supportable product with may be important for enterprises.OpenShift dedicatedOpenShift dedicated is the latest offering of OpenShift.It’s OpenShift 3 hosted on AWS and maintained by Red Hat but it is dedicated to youOpenShift onlineOpenShift Online is managed by Red Hat’s OpenShift operations team, and quickstart templates enable developers to push code with one click, helping to avoid the intricacies of application provisioning.You can view it as OpenShift delivered as a SaaS (Software as a Service)Benefits for developersBefore I show you how easy OpenShift is for a developer, let me quickly explain Source-to-Image (S2I).Let’s see how easy your life can be with the following image:  Source-to-Image (S2I) is a toolkit and workflow that creates a deployable Docker image based on your source code and add it to the image registry. You don’t even need a Docker file anymore.It combines source code with a corresponding builder image from the integrated Docker registrySo now that you know S2I, let’s take a look at the next picture        Code:If you’re a developer I assume you know how to code and push it to Git, so nothing new here…        Build:The developer can push code to be built and run on OpenShift through their software version control solution or OpenShift can be integrated with a developer’s own automated build and continuous integration/continuous deployment system. Here is were S2I can get useful.        Deploy:OpenShift orchestrates where application containers will run and manages the application to ensure it’s available for end users.        Manage:With your app running in the cloud you can monitor, debug, and tune on the fly.Scale your application automatically or allocate capacity ahead of time.  A deeper lookTime to get a little bit more technical and take a deeper look at how it works.I already talked about the developer part of the picture below, so let’s focus on the rest!  InfrastructureOpenShift runs on your choice of infrastructure (Physical, Virtual, Private, Public).OpenShift uses a Software-Defined Networking (SDN) approach to provide a unified cluster network that enables communication between pods across the OpenShift cluster.This pod network is established and maintained by the OpenShift SDN, which configures an overlay network using Open vSwitch (OVS).  The OVS-subnet plug-in is the original plug-in which provides a “flat” pod network where every pod can communicate with every other pod and service.  The OVS-multitenant plug-in provides OpenShift Enterprise project level isolation for pods and services. Each project receives a unique Virtual Network ID (VNID) that identifies traffic from pods assigned to the project. Pods from different projects cannot send packets to or receive packets from pods and services of a different project.However, projects which receive VNID 0 are more privileged in that they are allowed to communicate with all other pods, and all other pods can communicate with them.In OpenShift Enterprise clusters, the default project has VNID 0.This facilitates certain services to communicate with all other pods in the cluster and vice versa.NodesA node provides the runtime environment for containers.Each node in a Kubernetes cluster has the required services to be managed by the master. OpenShift creates nodes from a cloud provider, physical systems, or virtual systems.Kubernetes interacts with node objects that are a representation of those nodes.A node is ignored until it passes the health checks, and the master continues checking nodes until they are valid.In OpenShift nodes are instances of RHEL (Redhat Enterprise Linux).PodsOpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason.OpenShift treats pods as largely immutable, changes cannot be made to a pod definition while it is running.It implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated.RegistryIntegrated OpenShift Container Registry:OpenShift Origin provides an integrated container registry called OpenShift Container Registry (OCR) that adds the ability to automatically provision new image repositories on demand.This provides users with a built-in location for their application builds to push the resulting images.Whenever a new image is pushed to OCR, the registry notifies OpenShift about the new image, passing along all the information about it, such as the namespace, name, and image metadata.Different pieces of OpenShift react to new images, creating new builds and deployments.Third Party Registries:OpenShift Origin can create containers using images from third party registries, but it is unlikely that these registries offer the same image notification support as the integrated OpenShift Origin registry.In this situation OpenShift Origin will fetch tags from the remote registry upon imagestream creation.MasterManaging data storage is a distinct problem from managing compute resources.OpenShift leverages the Kubernetes PersistentVolume subsystem, which provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed.The Kubernetes pod scheduler is responsible for determining placement of new pods onto nodes within the cluster.It reads data from the pod and tries to find a node that is a good fit based on configured policies.The Management/Replication controller manages the lifecycle of pods.For instance when you deploy a new version of your application and create a new pod, OpenShift can wait until the new pod is fully functional before downscaling the old pod leading to no downtime.But what if the master node goes down? That’s no high availability … You can optionally configure your masters for high availability to ensure that the cluster has no single point of failure.Service layerOn top of the domain and persistence layer sits the service layer of the application.A Kubernetes service can serve as an internal load balancer.It identifies a set of replicated pods in order to proxy the connections it receives to them.Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address.Persistant storageManaging storage is a distinct problem from managing compute resources. OpenShift Origin leverages the Kubernetes Persistent Volume (PV) framework to allow administrators to provision persistent storage for a cluster.Using Persistent Volume Claims (PVCs), developers can request PV resources without having specific knowledge of the underlying storage infrastructure.PVCs are specific to a project and are created and used by developers as a means to use a PV.PV resources on their own are not scoped to any single project; they can be shared across the entire OpenShift Origin cluster and claimed from any project.After a PV has been bound to a PVC, however, that PV cannot then be bound to additional PVCs. This has the effect of scoping a bound PV to a single namespace (that of the binding project).OpenShift.ioSo before ending this blog post, I have to quickly mention OpenShift.io.As of this moment, it’s not yet available but you can try to register for the preview.I haven’t had the chance to play with it, as I haven’t received my access just yet.Basically it’s an online development environment for planning, creating and deploying hybrid cloud services.It provides the following features:  Hosted, integrated toolchain  Planning tools for managing and prioritizing work  Code editing and debugging tools built on Eclipse Che  Integrated and automated CI/CD pipelines  Dashboards and reporting toolsConclusionOf course there is so much more to tell you and show about PaaS and OpenShift.I hope that with this post you got a nice introduction to OpenShift itself and some of the benefits it offers.If you enjoyed the post, I intend to write another post later this year about OpenShift, so make sure to regularly check our JWorks blog!May the PaaS be with you."
      },
    
      "architecture-2017-06-21-pragmatic-architecture-today-html": {
        "title": "Pragmatic Architecture, Today",
        "url": "/architecture/2017/06/21/pragmatic-architecture-today.html",
        "image": "/img/prag-arch/arch.png",
        "date": "21 Jun 2017",
        "category": "post, blog post, blog",
        "content": "Software development has evolved. Agile is now the de facto standard. The role of an architect in an agile project is very different from the typical role in a more classic waterfall approach. This article presents an updated interpretation of viewpoints and perspectives and will demonstrate how to make rapid, agile delivery sustainable in a constantly changing world. These viewpoints and perspectives can be linked to easy-to-produce models that can be used immediately. A good agile architect needs to strive for consensus and buy-in.Content  What?  Why?  How?What?  Architecture exists, because we want to create a system. A system is the combination of all the different components that together define an application.These components can be loosely coupled, eg. using Microservices; it can be a monolithic application or any other combination of runtime components that fulfill certain business needs.This is a different scope than a system of systems.That would be the goal of Enterprise Architecture where the focus is on the strategic vision of an enterprise.A system is built for its stakeholders. And stakeholders are diverse: the customer (who is paying for the system), the users, the developers, … I believe, sharing a crystal-clear vision with these stakeholders and getting buy-in from them, is necessary to create a successful system.Every system has an architecture, even when it is not formally defined. The architecture of a system is typically described in an Architectural Description.The architectural description documents the system for the stakeholders and needs to make architectural decisions explicit.The goal of the architectural description is to help in understanding how the system will behave.  Following the approach in the book Software Systems Architecture by Nick Rozanski and Eoin Woods, an architectural description is composed of a number views.These views describe what is architecturally significant: info that is worth writing down because the system can not be successful without it or because stakeholders say it is significant.Deciding what to put in these views, means making decisions.Woods and Rozanski identified the following viewpoints:  Context View  Functional View  Information View  Concurrency View  Development View  Deployment View  Operational ViewThese viewpoints will assist in the writing of an architectural description.The website of the book contains a nice summary of these viewpoints.The views are shaped by perspectives. These are the cross-cutting concerns that have an impact on the views. Sometimes perspectives are also called quality properties or non-functional requirements:  Accessibility  Availability and Resilience  Development resource  Evolution  Internationalisation  Location  Performance and Scalability  Regulation  Security  UsabilityAgain, summaries are available on the website of the book.If you want a more in-depth explanation, I really recommend reading the book.In today’s agile world, I believe the Evolution perspective is a key differentiator in any architectural description.Generally, perspectives shape the architecture and deserve the necessary attention.Example  This is the 2017 Mercedes F1 W08 Hybrid. It weights 702kg and has between 750 and 850 horsepower. It is made out of approximately 80.000 different components. The price of the car is an estimated 10 million Euro. That is just for the car, not for the R&amp;D that made the car possible.Looking back at the viewpoints from above, it is easy to identify how these relate to the construction of the car:  A Formula One car needs a very specific factory (Development View).It is not built in the same factory Mercedes builds its trucks.  The cars need to be transported all around the world (all the Formula One cars travel over 100.000 miles in the air).This can be documented in the Deployment view.  Maintaining a Formula One car during a race has a huge operational cost and requires a lot of coordination (Operational View).Just count the number of engineers during a pitstop.  …In the 2015 and 2016 season, the predecessors of this car won the Formula One World Championship.At the moment of writing, the 2017 car is also leading the championship.This pattern is quite common in Formula One.The older cars however, are currently up for display in a museum.They are rarely used anymore.This throw-away approach can also be noticed when comparing to other industries like smartphones or smartwatches.A lot of the success of the car, must be its architecture then.More specifically, its ability to change: to adapt to new rules, competitors and market change.If the architecture of a system, has the ability to change, it immediately has a competitive advantage.This is especially true in agile projects.  Grady Booch  Architecture represents the significant design decisions that shape a system, where significant is measured by cost of change.Often, it is very difficult to get a system right from the beginning.That is why creating a system, that has the ability to evolve, is important.Things are changing all the time: known change and unknown change.Within this evolving system, it is the responsibility of the software architect to make sure the system remains consistent.Multiple architectural patterns exist to support this:In the past, many systems were built with a configurable metamodel. Nowadays, loosely coupled, replaceable services are favoured.  When creating a 10 million Euro car, many teams (with different responsibilities) are involved.The people who design the engine are different from the people who design the brakes.Creating the best engine, creating the best brakes, … does not imply you will have the best car.Everything needs to work together.The integrity of the system is very important.This point is again proven by Formule One: other teams can buy the current Mercedes engine.They might win some races, but they haven’t won the world championship  Russell L. Ackoff  A system is more than the sum of its parts; it is an indivisible whole. It loses its essential properties when it is taken apart.To ensure system integrity, the software architect needs to be part of the project team.He must make sure that he enables the right people to collaborate on the system architecture.Being part of the team does not mean not taking responsibility.It is great to give ownership to members of the team, but in the end, the architect needs to stay accountable.When collaborating, an architect should not enforce all technical decisions.Part of working as a team, is accepting to be challenged and embracing this challenge.When team members have spirited discussions, it shows they are passionate enough about their craft to care.Disagreeing and discussing alternatives is a great way to come to a better solution and to learn from each other.Being part of the team, as an architect, will lead to a system with a consistent vision, where the implementation matches the architectural description.This also implies that an architect should be involved in the codebase of the system: writing code, performing code-reviews, doing proof-of-concepts, supporting others, …By being involved in the codebase, you can make sure that the architectual description is understood by the developers.Visual?  While code is a very important asset of a working system, code alone is not enough to have an accurate and lasting description of a system.  Grady Booch  One cannot see the ocean’s currents by studying drops of water.The goal of visually representing a system, through the architectural description, is to make sure the architecture of the system is in the stakeholders’ heads.The visual representation can be used to check for consistency, reporting, validation and sharing information.Some ground rulesWhile UML has its merits, often it is not necessary to create an extensive UML model for the architecture.It will be time-consuming and, unfortunately, it is often the case that UML is not correctly understood by stakeholders.An alternative to UML is to use plain boxes and lines.However, when using boxes and lines:  Be consistent (especially when collaborating on the architecture).Try to be consistent over multiple projects. Templates offer a good start, but not every architecture needs the same viewpoints.  Avoid mixed responsibilities.  Avoid fluffy diagrams. Documents should not be vague. They should be about one abstraction.  Always provide a legend.Explain what a certain line or box means. Don’t make stakeholders guess.  Don’t be afraid to add text to a diagram.  Don’t model what nobody needs. Eg. if you are not using a data store, do not create an Information View.  Make sure your stakeholders understand what you are documenting.Whatever your preferred visualisation approach is, keep a decision log.Document your decisions, the considered alternatives and the timing a decision was made.Since the system will (very likely) evolve, a decision log will keep track of the reasoning behind a certain decisions.Decisions might need to change, so keeping track of the rationale behind a decision is valuable.Why?  Up-front designSome up-front design is necessary to start efficiently and to prevent too much rework.This means thinking about the big picture:  Used technology  Automation  Architectural patterns  Layering  Evolution  …  Simon Brown  Just enough up-front design to create firm foundations for the software product and its delivery.But what does just enough mean?Just enough depends on a lot of variables like budget, scope, team, … The approach will also be different for greenfield projects or for existing projects.When you are working on a greenfield project, it is important to start with a high-level view of all components in the application.These components are all the pieces necessary for a system to operate.Other components and details can be added later.Working with existing systems benefits from a slightly different approach, where you can start with an accurate high-level diagram of the current architecture of the existing application.Once this diagram is available, identify the domain-of-change of the architecture: the reason people are working on the system.On top of that, adding extension points will enable evolvability.Communication  In the inception phase of a project, you will need to talk to all the different stakeholders and make sure that their desired product will be built.Aligning requirements from different stakeholders, will often be a challenge.  In the implementation phase, it is important for the team to share a technical vision.All team members need to collaborate to the same end-goal, which requires strong communication skills.Including team members in defining the technical vision is useful to make sure everybody knows how they, individually, are contributing to the technical vision on a day-to-day basis.PoliticsThe architecture of a system will have a large impact on the implementation, delivery and usage of the system.Systems generally consist of multiple parts and it is the responsibility of the architect to focus on system integrity, creating a system that has a built-in ability to respond to change.When the system lacks integrity, it will rapidly become a system nobody wants to touch.Unfortunately many enterprises have this fear of change embedded in their culture and it will take strategy and sound people skills to prevent this from happening.Influence Maps present an interesting way to map relationships between people and to visualise who influences who, in an enterprise.Being aware of these relationships might be a game-changer.How?  One way of creating an architectural description is OODA: Observe, Orient, Decide, Act.OODA can be compared with PDCA, also known as the Deming Cycle or with Discovery Activities.  Any architectural model introduces abstraction and removes noise.This model should be well-understood and feedback loops can help with this.As an example, comparing a written down version with bullet points of a certain idea, will help in verifying that the message hasn’t changed.This insight should be mapped on the model.  Observe: Observing both external and internal circumstances or dependencies of your systems.          Collect up-to-date information from different sources: stakeholders, competitors, similar systems, other viewpoints,…        Orient: Using your past experience to make sense of these observations.          Analyse the observed information and use it to update your current reality. View events, filtered through your own experiences and perceptions.        Decide: Deciding on a response, because there might be multiple alternative solutions.          Determine a course of actions.        Act: Execute the selected decision.          Follow through on your decision.      This is not a linear process. This process benefits from continuous feedback loops.Feedback loops imply that certain decisions may lead to new observations etc.The OODA process can be used as a means of creating an architectural description.Consequently, significant decisions will become part of it.Since the creation of (significant parts of) the architectural description, starts with (runtime) observations, capturing data and measuring stakeholder value will help to achieve better observations of the system."
      },
    
      "nodejs-2017-06-20-rest-api-nodejs-koa-html": {
        "title": "Creating a REST API with NodeJS, TypeScript and Koa.",
        "url": "/nodejs/2017/06/20/REST-api-NodeJS-koa.html",
        "image": "/img/nodejs-typescript-koa/koa-logo.png",
        "date": "20 Jun 2017",
        "category": "post, blog post, blog",
        "content": "  This article assumes you already have some knowledge of npm and JavaScript development in general. It will not be a detailed tutorial about how to write a REST API, it’s more of an extra explanation for the application I made, the libraries I used and my experience with them.Why?I started this little project because I wanted to be able to quickly write backends for small personal projects with little overhead. Coming from the Java backend world, I have been writing almost only JavaScript for close to 2 years now, but only frontend. I had tried NodeJS in the past for a small project with plain old JavaScript and had a very bad time. Now however, with my new experience in JavaScript, the arrival of ES6 and TypeScript, I wanted to give it another shot.What exactly did I make?The idea was to write a backend for an application called MovieListr. It’s a simple application to track movies you have watched or want to watch. The API allows you to create, delete, update and see movies and directors. A movie also has a one-to-one relation with a director.You can find the code on Github.SetupSetting up a node project with TypeScript doesn’t require a lot of effort, the following commands are enough to get started.mkdir &lt;project-name&gt;cd &lt;project-name&gt;mkdir srcnpm init //follow the setupnpm install --save-dev typescript tsc //install TypeScript and the TypeScript compilertsc --init //generates a `tsconfig.json`, a config for the TypeScript compilerThis is it, you still have to tinker with the tsconfig.json to get it to your liking, but after that you can just start writing code.Using async / awaitI want to start with talking about the async / await features. They were what really made this code so fast to write and easy to read. The async keyword marks a function that will always return a promise. The await keyword will automatically unwrap the value from the promise and continue the code when the promise has been resolved. A small example:      const promiseFn = (): Promise&lt;string&gt; =&gt; {\t    return Promise.resolve(\"Hello World\");    }     // Old way:     const asyncFn = () =&gt; {\t    promiseFn()\t\t    .then(value =&gt; {\t\t\t    console.log(value);            });    }     // With async / await     const asyncFn = async () =&gt; {\t    const text = await promiseFn();\t    console.log(text);    }You can see how readable it is with the async / await syntax. You can write asynchronous code in a synchronous way and I used it heavily everywhere in my code. I think this is one of the things that will really make writing JavaScript fun. No more callbacks, no more boilerplate code, just the important bits. For error handling you can rely on try catch statements to catch errors and act on them.To use the async / await syntax, you can have to add esnext.asynciterable to the lib array in the tsconfig.json file.The libraries I usedKoaKoa is a small node library to create REST APIs. It was made by the guys who created Express. It takes advantage of the new ES6 feature of generator functions and it allows you to write very readable code by using the async / await features (that are based on the generator functions). For a full understanding of koa and generator functions, I suggest the Koa course on Pluralsight from Hammarberg.Koa relies heavily on middleware, so for every “step” of the process we need middleware. For instance koa-bodyparser middleware will parse the request body to json, the koa-logger middleware will log all the incoming requests and the koa-router middleware will make it easy for us to configure the url mapping to certain actions. These middlewares are installed apart from the Koa framework or you can write them yourself.typescript-iocTo make testing easy, I started looking for a dependency injection framework for TypeScript. I wanted to more or less copy the way I wrote unit tests in Java, which is using dependency injection in your actual code and just creating an instance in your unit test while passing mocks instead of the dependencies. The first dependency injection framework I found, was Awilix. I got Awilix to work, and it worked quite well, but there was still a lot of boilerplate code to write to actually register the services to the container and to get it working. You can also pass folder names so it will register all the services in that folder, but I didn’t find this optimal. I was also using Webpack in the beginning (which I write about later in the article) to build my application and bundle my code, by bundling the code, the paths of the folders obviously didn’t work out anymore in the compiled code, so Awilix was no good for me. I kept searching and I found the library typescript-ioc. This library was based on annotations, so there is barely any configuration overhead and it worked much more like I was used to in Java. typescript-ioc requires you to set experimentalDecorators and emitDecoratorMetadata to true in the tsconfig.json file. You can then just write code like      import { Container } from \"typescript-ioc\";        class Foo {        doSomething(): string {        \treturn \"Hello World\";        }\t    }        class Bar {        constructor(@Inject private foo: Foo) {        }\t        \tdoAnotherThing(): string {    \t\tthis.foo.doSomething();        }    }        const bar:Bar = Container.get(Bar);    bar.doAnotherThing();typeormAt first I just saved the movies and directors in the services as an in-memory array for testing purposes, but in a real application you will want persistence of some sort, so I needed a database. I decided on a regular old MySQL database and an ORM library to do the mapping between the database records and my TypeScript model classes. For ORM I used typeorm. It’s pretty easy to use. It also uses the annotations like typescript-ioc, which makes code very readable. The experience with this library was more or less pain free, so I really recommend it. To check a real example from my repository, check the Movie model.TestingUnit testingFor unit testing I used the classic combination of Mocha, Sinon and Chai. Since I was using dependency injection, I also needed a good way of mocking my dependencies, for this I found ts-mockito. Ts-mockito is more or less a clone of the Mockito library in Java. It allows you to create mocks of classes, make functions return certain values and verify that calls have been made. This made it super easy to write tests. For examples check the tests folder in my repository. To execute the TypeScript tests, I used ts-node. Ts-node compiles the TypeScript and keeps the compiled JavaScript in memory while it executes it. This way you don’t have to create an additional folder to compile the tests to and execute them. You can then easily create an npm script like this:mocha -r ts-node/register test/**/*.spec.tsThis tells Mocha  to require the ts-node/register module (this is what the -r ts-node/register) means and then it just passes the path of the test files to it. This also worked pretty much painlessly.end-to-end testingI wanted to be able to do some real end to end testing. So I wanted to be able to spin up my application, pass some HTTP requests to it and then verify the output of the requests. The first question was how to pass the requests to my application. For this I found the library SuperTest. You can just start you Koa app and pass the HTTP server (the return value of the app.listen function) to the agent and it will make sure the app is started and you can do some requests and check the results. This worked pretty well.The second problem was a test database. I needed a database that was as close to the real one as possible. I ran the real database in a Docker container with a volume that mapped the /var/lib/mysql (the configuration / data folder for MySQL) to a host directory, so I could recreate the container without losing data. I figured I could more or less copy the Docker configuration for the database for a test database, only without the volume. Without the volume, the data would just be saved to the container itself, so it would be lost every time the container was recreated, which is perfect for end-to-end tests, because we want to start the tests with the exact same dataset, so we can make sure our assertions keep working.So I created an npm script to start the Docker and to do healthchecks to the Docker container until it told me that the entire container was up and running and MySQL was ready to take connections. Then I wrote a script to start the actual end to end tests, which was simply the same mocha call I wrote earlier, only pointing to the e2e folder instead of the test folder. At last I wrote an npm script to stop the Docker container and remove it. You can check these scripts here. I made heavy use of the shelljs package. This npm package allows you to execute shell commands, which I used to start Docker containers from JavaScript.NOTE: this setup works well, but the starting of the Docker container takes ~30 seconds, which is quite long, considering that the tests take maybe a few seconds. In a continuous integration build, this doesn’t matter as much, but when you are trying to fix tests, it does take a lot of time if you have to wait about a minute for each test run.Task runnerWebpackWhen I started this project, I was looking up some best practices for node. I came across an article that suggested you should use Webpack for backend too. I already have some experience with Webpack from frontend development, so at first it seemed logical to use it for backend too. When I was trying to get the dependency injection to work with Awilix, I realized that I could not pass any paths to libraries, because when my code was bundled, the paths would be invalid. Then I started to actually wonder why I was bundling my code. In frontend you bundle your code to make it as small as possible so you don’t waste the user’s bandwidth and make you website load faster, but in backend, that does not matter, since the code does not have to be sent anywhere. At this point I decided I didn’t need Webpack at all and I could just use npm scripts’ functionality to create tasks.npmNpm is actually the only build tool you need. If what you want to do is more than a single line command, you can just write scripts in either TypeScript (you can execute them with ts-node), bash, JavaScript, … whatever you like. I wrote my scripts in TypeScript, because to me it makes more sense to use TypeScript for everything, but I could just as well have written them using bash. Npm also gives you pre and post task hooks. So if you write a task with the name “e2e” as I did, you can also add a task with the “pre” prefix or the “post” prefix that will automatically be executed before and after the task is executed. This way I could easily separate the starting of the Docker container, the executing of the tests and the stopping of the Docker container into different scripts. I could then just execute npm run pree2e to check if my script to start the Docker worked. I really like this approach and the fact that I don’t need another tool to learn like gulp or Webpack.DebuggingApplication codeI had some trouble at the beginning with debugging my TypeScript. For some reason in the Chrome Devtools I could not get my sourcemaps working (even though they were inline sourcemaps). Then I tried the Visual Studio Code debugger and that worked much better. To get this to work, I did the following:tsconfig.json      {        \"compilerOptions\": {            \"inlineSourceMap\": true,            \"inlineSources\": true,        }    }package.json      \"scripts\": {        \t    \"start:debug\": \"ts-node --inspect=5858 --debug-brk --ignore false src/index.ts\",            }.vscode/launch.json      {\t    \"configurations\": [\t        {                \"type\": \"node\",                \"request\": \"launch\",                \"name\": \"Debug Application\",                \"runtimeExecutable\": \"npm\",                \"windows\": {                    \"runtimeExecutable\": \"npm.cmd\"                },                \"runtimeArgs\": [                    \"run-script\",                    \"start:debug\"                ],                \"outFiles\": [],                \"protocol\": \"inspector\",                \"sourceMaps\": true,                \"port\": 5858            }        ]    }The npm script will start the execution of the index.ts with ts-node in debug mode on port 5858 and the --debug-brk tells it to break on the first line of code. The launch configuration will just execute this npm script and attach it to the debugger.Test codeDebugging the test code is more or less the same as the application code, there is just a small caveat. When you create breakpoints in Visual Studio Code, they will appear gray as if they cannot be reached. But when you execute the code, it will break on the breakpoints and then they will become red like a normal breakpoint.package.json      \"scripts\": {\t    \"test:debug\": \"mocha --inspect --debug-brk --not-timeouts --compilers ts:ts-node/register test/**/*.spec.ts\",    }.vscode/launch.json  {    \"configurations\": [        {            \"type\": \"node\",            \"request\": \"launch\",            \"name\": \"Debug Tests\",            \"runtimeExecutable\": \"npm\",            \"windows\": {                \"runtimeExecutable\": \"npm.cmd\"            },            \"runtimeArgs\": [                \"run-script\",                \"test:debug\"            ],            \"outFiles\": [],            \"protocol\": \"inspector\",            \"sourceMaps\": true,            \"port\": 9229        }    ]}ConclusionI really had a good time making this project. I really love readable and compact code and with TypeScript and the async / await syntax, I really got what I asked for. My previous experience with node.js and regular old JavaScript was really bad, mostly because of the loose typing, which forces you to constantly write a lot of tedious checks on parameters. With TypeScript that is all in the past. Apart from that, the enormous amount of npm packages available, makes it very easy to find some package that does what you need. If for some reason you can’t find something, you can easily write it yourself and publish it to npm.I always used to use Java for my backends, but the setup is always a bit of work and you have to write more boilerplate code than with TypeScript. If I make more small projects in the future, I will probably use TypeScript and Node, but for me at this point, it’s hard to tell if NodeJS will hold up in bigger projects. I would assume so, since the structure for me at this point, is very similar to Java, just a more concise syntax."
      },
    
      "spring-2017-06-07-spring-io-2017-the-spring-ecosystem-html": {
        "title": "Spring IO 2017: The Spring Ecosystem",
        "url": "/spring/2017/06/07/Spring-IO-2017-The-Spring-ecosystem.html",
        "image": "/img/spring.png",
        "date": "07 Jun 2017",
        "category": "post, blog post, blog",
        "content": "  When I was at Spring IO back in May, I was intrigued by a presentation given by Jeroen Sterken.There he talked about the Spring Ecosystem in 50 minutes.Since he only had 50 minutes, he could not focus on all the projects Spring boasts.I wanted to get a feel of what the Spring team has to offer in all its glory, by getting to know all of the main projects.Jeroen Sterken (@jeroensterken) is a Java and Spring consultant from Belgium. He’s a certified Spring instructor and currently employed at Faros Belgium. His slides of his talk The Spring Ecosystem in 50 minutes can be found here.The Spring EcosystemThere are many ways to divide the Spring portfolio.One way could be based on architecture, another way could be based on popularity. Jeroen divided the Spring Ecosystem in three categories: classic, popular and other.Before we dive into the Spring ecosystem, let’s take a look at which projects our own JWorks unit have been using the most over the past two years. Here’s the JWorks top 10, beside the Spring Framework.        Spring Boot is currently at the top. Other notable mentions are Spring Session, Spring Social and Spring Cloud Data Flow.But what’s even more interesting are the Spring projects that aren’t that widely used: Spring Mobile, Spring for Android, Spring Kafka, Spring Statemachine, Spring Shell, Spring Flo and Spring XD.ClassicThe classic projects are showing a range of the many beloved portfolio projects, where for instance Spring Security and its LDAP module will help you build your secure applications at ease.Or where the Spring IO platform will show you the insights in its development.        Spring FrameworkThe core of Spring, currently at its fifth revision.It provides key components for dependency injection, web apps, transaction management, testing, messaging, model-view-controller, remote access, data access and more.Just add the modules you need and start programming.In the fifth version, the focus lays on reactive programming with reactive streams, as well as other features and revisions like support for JUnit 5.Spring 5 will require at least JDK 8 but is already being built continuously on JDK 9.The release is planned for the end of the year, regardless whether Java 9 is released or not.Spring IO PlatformThe Spring IO Platform is built on Spring Boot and is mainly used in combination with a dependency management system.It provides dependencies that work well together.It’s basically a library on the classpath of your application which gives developers production-ready features.It does this by providing a bill-of-material Maven artifact.The libraries used in the BOM file are all curated and normalized, so they work greatly together.But if that is not to your liking, you can easily just use your own versions.The platform supports JDK 7 and 8 and is still being updated frequently.Spring SecurityNowadays you can’t ignore problems of security failures and the importance of privacy.Spring Security provides your application with authentication and authorization.It will also protect your application against a handful of possible attacks.Spring Security supports many popular authentication protocols and services like OpenID, LDAP, HTTP, … and support is extended through the available third party modules.The fifth version of Spring Security will add OAuth 2.0 support.Spring LDAPSpring LDAP hides a lot of the boilerplate code for LDAP interactions.It makes sure all the connections are created and correctly closed.This library helps out with the looping through the results and filtering those.It’s also possible to manage your transactions with a client-side transaction manager.If you’re working with this Lightweight Data Access Protocol, this might definitely be worth your while.Spring IntegrationWhen an architecture revolves around events or messages, you can get the help of Spring Integration.This project focuses on the implementation of Enterprise Integration patterns.When you want to send something from point A to point B, there could be a lot of different network protocols or restrictions in between.Spring Integration minimizes the boilerplate code needed by implementing those patterns.It just makes it easy to send events and messages throughout different endpoints.Spring BatchWith Spring Batch it is possible to write an offline batch application using Java and Spring.It makes it very convenient when you’re used to the Spring Framework to execute a bunch of jobs.It features a possibility to read and write your resource and a way of dividing data for processing and much more.There is also support for a transaction manager, job processing statistics, job status changes and much more.Spring Web FlowThe Spring Web Flow was created to help users navigate through the different views of a stateful web application.A common example could be when shopping online.The process has a clear starting and finishing view, but in between, it can change state or views dynamically.Through guided navigations, the user makes changes and it should register those changes as well as the possibility to finalize those changes through a confirmation.All this is possible with Spring Web Flow.Although this project is listed with the main projects, there hasn’t been any progress over the last years, and will be removed when Spring 5 hits the shelves.Spring Web ServicesThere are several ways to develop a web service, one of which is used in combination with SOAP.Spring Web Services helps with creating contract-first SOAP web services which are flexible by manipulating the XML contents.But due to the popularity of the architectural style of REST, the interest in SOAP has diminished.This is noticeable in the maintenance of this Spring project which hasn’t had any significant version updates.Version 2.4.0 was released on August 26th 2016 and only brought some CI jobs that are built for every commit for Spring 4.2, 4.3, and 5.0.PopularWhen you look at modern applications and their infrastructure, you’ll see the power of the Spring portfolio coming to its use.With the easy of use of Spring Boot, you can quickly start the development of a secure application and use Spring Cloud to help you with the deployment and integration for your online service provider.        A modern application might look like this:        Spring BootBeing built onto the Spring Framework, the popular Spring Boot project provides an easy to use way for creating stand-alone Spring applications without code generation and configuration of XML files.If you want to get started quickly without too much hassle, Spring Boot is the way to go by adding the dependencies you need.Spring Beans don’t need to be defined in XML or Java, as they are mostly configured automatically by Spring Boot.This way, there is no need to find and configure libraries for your specific Spring version, Spring Boot tries to do that for you.However, if you wish, you can fine-tune the auto-configuration to your own needs by adding the library to the classpath of the application, setting some properties, or adding some annotations.When you want to deploy your Spring Boot application, there’s no need to build a WAR file, since you can build self-contained JAR files with an embedded servlet container such as Jetty, Tomcat or Undertow.Spring Boot also features a command line tool for quick prototyping with Spring.The easiest way to get started with Spring Boot is to go to the Spring Initializr and add the dependencies to the project.The Spring team is maintaining the Spring Boot project regularly as it’s becoming the de facto way of using Spring.Spring CloudSpring Cloud is an umbrella project which lets you build distributed systems by implementing many best practice patterns.It consists out of many sub-projects.With the use of Spring Cloud Config Server you can setup a server with a repository, like Git, as its data store and view the changes made in the configuration.Spring Cloud Contract allows you to write Consumer Driven Contract Tests with ease.Many of the Netflix OSS components are wrapped into Spring Cloud, which makes it a lot easier to deal with the complexity of microservice architectures.And of course with a cloud service there’s often a lot of security involved which is provided by the Spring Cloud Security.You can easily integrate this with Amazon Web Services or Cloud Foundry, through their related subprojects.Spring Cloud Security is build on OAuth2 and Spring Boot which provides single sign-on, token relay and token exchange.One of the latest projects in the Spring Cloud umbrella is Spring Cloud Function.It offers an extreme convention-over-configuration approach which can leverage all of Spring Boot’s capabilities while writing only a single function.The full list of sub-projects are available here.Spring Cloud Data FlowSpring Cloud Data Flow used to be know as Spring XD and is part of Spring Cloud.It’s an updated and revised toolkit for cloud-native message-driven microservices.The change was made by the Spring team after their experience with Spring Boot.Spring Cloud Data Flow is suitable for processing, analyzing and predicting data.Through streaming it can consume data from an HTTP endpoint and writes the payloads to a database of your choice.It also manages to scale the data pipelines to your liking without any interruptions.After development, an application can be easily executed in Cloud Foundry, Apache YARN, Kubernetes or Apache Mesos, but with the Service Provider Interface you can deploy your application to other runtimes.Spring DataWhether you’re working with relational or non-relational databases, Spring Data will soothe your needs.As an umbrella project it will ease your way into data access.It abstracts the complexity of data access layers by allowing the developer to simply extend an interface.Some of the related sub-projects will help you develop quicker for your favorite database, like Spring Data Mongodb, Spring Data JPA, Spring Data for Apache Cassandra or Spring Data for Apache Solr.And through the help of some community modules this is extended to several others.With Spring Data REST you can expose your Spring Data repository automatically as a REST resource.As usual with Spring projects, they provide an excellent base but can be customised to your own needs.A full list of sub-projects and community projects are available here.Spring HATEOASHATEOAS stands for Hypermedia As The Engine Of Application State.It enables the server to update its functionality by decoupling the server and client.With Spring HATEOAS it’s easy to create a REST resource implementation using the HATEOAS as an underlying principle.It helps the client by returning a response in combination with more information on what to do next.If the state of the resource changes, the information on the next steps will also vary throughout the application.As this is a subconstraint one of the core principles of REST, the uniform interface, using Spring HATEOAS you can achieve ‘the glory of REST’.Spring REST DocsWhen you develop a RESTful service, you’ll probably want to document it so it’s easy for other developers to implement your API.Spring REST Docs helps you with the documentation process to make it more accurate and readable.It does this by running integration tests, which generate guaranteed up-to-date request and response snippets when those tests succeed.Those snippets can be included in Asciidoctor templates, which are then converted to HTML output.Alternatively it can be configured to use Markdown.The advantage here is that the documentation is always up-to-date with your code, since the integration tests will fail otherwise.There are also options for you to customize the layout of the documentation.A more in-depth look at Spring REST Docs was presented at Spring IO 2016 by JWorks colleague Andreas Evers: Writing Comprehensive and Guaranteed Up-to-date REST API Documentation.Spring SocialSpring Social lets you connect your application with Facebook, Twitter and LinkedIn.But through its many community projects it’s possible to connect to dozens other like Google, Instagram, Pinterest, …The full list is of supported third-party APIs is available hereSpring SessionWhen someone uses your web application, they will be using an HTTP session underneath.Spring Session allows you to manage those sessions separately, outside of the servlet container.It supports multiple sessions at once and can even send the sessions in the header.Spring sessions isn’t specifically tied to any container.Although the project is quite popular and has very interesting features, the project hasn’t had any major changes over the past year.OtherThese projects are mainly focused on one specific (niche) part of an application.Some wil help you with the development of specific front-end applications, while others will help you implement specific patterns.        Spring AMQPAMQP is an abbreviation for Advanced Messaging Query Protocol which Spring AMQP implements.It helps you with routing, queuing, exchanging and bindings.Additionally, there’s a listener available when sending messages asynchronously.Spring AMQP also provides a template service for sending and receiving messages.In the upcoming second version of Spring AMQP it uses version 4.0.x of the library which has been developed by RabbitMQ.Spring MobileSpring Mobile is the Spring team’s attempt at making it easier to develop mobile web applications with the use of Spring MVC.Spring Mobile implements a way of detecting the type of the device used to view the url and tries to adjust its view accordingly.Unfortunately the project isn’t that well maintained as significant updates are several years ago.Spring for AndroidAnother project without any recent updates is Spring for AndroidSpring for Android brings some of the key benefits of using Spring to Google’s mobile operating system, Android.It has a REST API client for Android with authentication support.For your social media authentication, you can use Spring Social in conjunction with Spring for Android.But there’s no use of Spring’s dependency injection, transaction manager or some other useful Spring features.Spring ShellThe Spring team provided a way for building command-line applications.Through the use of Spring you could build a full-featured shell application with your very own commands or just use the default commands that are already implemented.Or you could get access to an exposed REST API.The Spring Shell hasn’t been updated with new functionality in more than 3 years.Spring XDSpring XD is the predecessor of Spring Cloud Data Flow and therefore hasn’t been maintained.End of support will be in July 2017.Spring FloThis JavaScript library was a foundation for the stream builder in Spring Cloud Data Flow.It provides a basic embeddable HTML5 visual builder.Spring Flo is especially focused on pipelines and simple graphs.It’s built using Grunt where the commands can be ran directly or indirectly through Maven.With the use of a drag and drop interface it’s easy to create real-time streaming and batch pipelines.Additionally you can also choose to use the shell instead of the GUI interface.Spring KafkaThis is Spring for Apache Kafka, an open-source streaming processing platform.Spring Kafka provides an interface for sending messages for Kafka-based applications.It also supports a listener container and a way of sending message-driven POJOs.Spring StatemachineSome applications may require state machine concepts being implemented.Spring Statemachine provides a framework that helps with that.It provides a lot of useful things for making complex configuration easy, but also provides listener states and much more.Spring RooSpring Roo gives you the possibility to easily build full Java applications.This is a tool for rapid development of Java applications that are fully written in Java.It is focused on using the new Spring projects, like Spring Boot and Spring Data, as well as other common Java technologies.However, since the introduction of Spring Boot, Spring Roo has become less of a necessity, as Spring Boot hides a lot of the boilerplate code Spring Roo was designed to generate.Spring ScalaWhen developing applications in Scala, you can make use of Spring through Spring Scala, a community project.This brings a lot of Spring technologies to the Scala programming language.This is one of the two presented community projects by the Spring team on their main project page, the other one being Spring Roo."
      },
    
      "blockchain-2017-05-10-blockchain-introduction-html": {
        "title": "Blockchain introduction",
        "url": "/blockchain/2017/05/10/Blockchain-Introduction.html",
        "image": "/img/blockchain/blockchainHeaderImagePNG.png",
        "date": "10 May 2017",
        "category": "post, blog post, blog",
        "content": "  A lot of people are talking about blockchain these days.They’re talking about blockchain as the next big thing after mainframes, computers, the internet and social networking.This introduction is the first part in a series of blockchain posts.TopicsIn this first article about the innovative blockchain technology, we’ll cover the following topics:  Blockchain and its relation to Bitcoin  What is blockchain  Types of blockchain networks  The consensus process  Smart contracts  Valid blockchain business cases  Existing platforms  Thinking decentralized  Conclusion  Recommended readingBlockchain and its relation to BitcoinFirst of all, Bitcoin and blockchain are two different things.People tend to use both words by each other in three different contexts:              1. Digital cryptocurrency                    2. Protocol and client for executing transactions        \t            3. The blockchain which stores all Bitcoin transactions    So when talking about Bitcoin or blockchain with people, it’s important to mind this terminology.Here’s a funny quote I read in the book Blockchain: A Blueprint for a New Economy,which describes this ambiguity very well:It's as if PayPal called the internet PayPal on which the PayPal protocol was run to transfer PayPal currency.In January 2009, the Bitcoin network came into existence.Bitcoin isn’t the first attempt to digital currency, but it’s the first one that uses a peer-to-peer network to create a platform for executing transactions without depending on central authorities who validates them.You should see Bitcoin as the first platform that implemented blockchain technology.What is blockchain?So forget about Bitcoin now.That’s not what this post is about.People say blockchain is as important as the introduction of the internet. The internet is a worldwide network to share information with one another, but it is far less suitable for transferring value.If you send someone a file, it is always a copy of your file, which means you and the receiver are both in possession of the file.As we already stated, that is ideal for sharing information, but not applicable for money, certificate of ownership, and so forth.And the latter is exactly what blockchain enables: digitalizing and transferring such values.Let’s take a look at the underlying decentralized ledger technology.We believe blockchain’s definition is a good starting point:  “Blockchain is a type of distributed database that stores a permanent and tamper-proof ledger of transaction data.”TL;DR versionBlockchain is a decentralized immutable data structure.In short the blockchain is a network of computers, called nodes. Every node has the same copy of the database which they manage together. A transaction is encrypted and signed on a mathematical way. When a transaction is saved in the blockchain, it is duplicated across all nodes in the network.That’s why we talk of blockchain as distributed ledger technology, a ledger of transactions, distributed across a computer network.Transactions are bundled in one block before they are validated by other nodes.Once the network reached consensus about the validity of these transactions, the block is appended to the existing chain of blocks.The block stores the validated transactions together with a hash and a reference to the previous block.Stored transactions cannot be undone, as this would invalidate all hashes in the chain.Now a little more in detail…Transactions are broadcasted to the network for miners to mine. They assess the non-validated transactions on the memory pool by solving a mathematical puzzle. A miner builds a block containing all transactions, a proof of work that the puzzle was solved (also known as the block root hash, which is also the ID of the block) and a hash to the previous block.A block also contains the following items:  A timestamp  a nonce  and a merkle root hashA merkle root does not verify transactions, but verifies a set of transactions.Transaction IDs are hashes of the transaction, and the merkle tree is constructed from these hashes.It means that if a single detail in any of the transactions changes, so does the merkle root. It also means that if the exact same transactions are listed in a different order, the merkle root will also change.So the merkle root is cryptographic proof of the transactions in the block together with the order they are in.The nonce number is a field whose value is set so that the hash of the block will comply with the predefined network rules (eg: a run of leading zeros in Bitcoin). Miners increase the nonce until the hash is valid. Sha-256 is used to hash.The miner appends the block to the blockchain. And the majority of the other nodes, 50% + 1, double-check by verifying the proof of work in the block.It sometimes occurs that miners will validate two blocks at the same time and they will be appended to the chain. When this occurs, which doesn’t happen often, the principle of Longest Chain Wins will be implemented.The longest chain remains and the conflicting chain will be discarded.The transactions of the discarded chains will be put back in the memory pool to be mined another time.You now have a basic understanding of why we call it the blockchain.  \t        Types of blockchain networksPublic blockchains (aka. permissionless)This is a blockchain that everyone in the world can view, write transactions to, expect that these transactions will be validated and added to the blockchain.In this type of blockchain network, any connected node can contribute the consensus process.This process is used to determine if a block is valid or not.You can read more about the consensus process further in this blogpost.The public blockchain is generally a complete peer-to-peer network. Its characteristics are:  The users from the chain get protected from the creators of the chain, because there are actions to the network that even they cannot perform.Developers are not the owners of the network and don’t have more or less privileges than normal users.  These chains are transparent because everyone can see what is happening inside the chain.\"In some cases, public is clearly better; in others, some degree of private control is simply necessary. As is often the case in the real world, it depends.\" - Vitalik Buterin of EthereumConsortium blockchainsIn this type of blockchain network, the consensus process is executed by a predetermined group of nodes in the network.Let’s take a consortium of fifteen financial institutions as an example, each with a node.From this group of fifteen, there are ten nodes that need to sign each block before it is valid.You could say that these ten take ownership of the data in the blockchain.They decide which transactions are valid and which ones are not.Read rights can be public or restricted to the members of the network, eg. we can limit public view to a set number of times.Public and consortium blockchain networks are decentralized, with the difference that the consortium network is not completely peer-to-peer, because not everyone is equal.Private blockchains (aka. permissioned)There is only a small difference between consortium and private blockchain networks: write rights are with one organization instead of multiple.The read rights can be the same as with a consortium blockchain.The following characteristics apply for a private blockchain network:  The company that controls the private chain can alter the rules of the chain. In some cases this can be necessary.  The nodes that confirm a block are known, so there can’t be a majority by a mining farm with a 51% attack.  Transactions are cheaper than with public chains. This is because they need to be verified by less nodes.  Nodes are well connected and errors can be fixed quickly with manual interaction.This means that these networks give a faster confirmation and they will always be faster than public networks.  Private blockchains are just better at privacy because the access to the blockchain can be limited.From a legal point-of-view, this characteristic can have significant impact on the type of blockchain network you’ll pick.The consensus processAs we mentioned before, the network must reach a consensus of 50%+1 for a transaction to be written to the blockchain.There are a few ways a blockchain network will do this.We will be discussing the two most used.Ronald Chan wrote a nice article about consensus mechanisms in Consensus Mechanisms used in blockchain.Proof-of-WorkThis is used to deter people from tampering with the blocks and launching (d)dos attacks. We let them do a feasible but not insignificant amount of work to get a consensus. For example in the blockchain they need to find the correct nonce number that is part of the block to create a hash that fits the predetermined rules. A rule can be that the hash must start with six zeros.Proof-of-StakeIn this case you don’t need to find a nonce number but you just need to proof that you have a certain stake in the network.The bigger your stake, the more you can mine from the network.Smart ContractsThe term smart contract has no clear and settled definition.So what is it?Smart contracts are traditional contracts and official documents, but written in code.As such, the contract is understandable for everyone across the globe, irrespective of the jurisdiction it is related to. Smart contracts are like If This Then That statements, only they tend to be a lot more complex.The different definitions usually fall into one of the following two categories:  Sometimes the term is used to identify a specific technology.Pieces of code that are stored, verified and executed on a blockchain.For example, a hello world program.  The term can also be used to refer to a specific application of that technology: as a complement, or substitute, for legal contracts.  \t        Valid blockchain business casesIt’s important to understand that blockchain isn’t a solution to all of your business problems.Like in any other project, you shouldn’t make critical technology decisions on hyped buzzwords.Instead you should focus on the business value it delivers.When we translate the blockchain characteristics to business values, it can potentially solve business problems in the following five key elements:  Transparency  Operation harmonization  Business continuity  Permanence  Security  DecentralizedWe’ll discuss each element in detail and explain why blockchain technology can be an answer to that business problem.TransparencyIn a public blockchain network, by default every member of the ecosystem can access all transactions stored in the chain.They can even access smart contracts.An example of improved transparency is in the supply chain. Documenting a product’s journey across the supply chain reveals its true origin and touchpoints, which increases trust and helps eliminate the bias found in today’s opaque supply chains.Manufacturers can also reduce recalls by sharing logs with OEMs and regulators.Another potential use involves the recording of patents and intellectual property. Due to blockchain’s 100% transparency and its unforgeable nature, the information cannot be altered.Because transactions are easily trackable, it’s the perfect solution for recording ownership of patents and properties.  You can only achieve 100% transparency if you setup a public, permissionless blockchain network.In a consortium- or private blockchain network, you can define access rules to say which members can query certain information, which reduces its transparent nature.Operation harmonizationBecause business logic is implemented as smart contracts, and smart contracts are replicated over the different nodes that execute them, you have decentralized business logic.This allows you to use the same open source technology in all departments of your business.As a result, business processes are joint together, in contrast to Enterprise BPM, where business logic reuse is limited due to single enterprise data silos.Business continuityBy using blockchain technology, you have less dependency on a central infrastructure.That is because all nodes can execute transactions.When one node goes down, other nodes take over the processing.You can say that in a blockchain network, you have automatic failover.PermanenceWe already talked about the fact that activities in a blockchain cannot be undone.They are immutable.Because of this characteristic, there’s an audit trail of what happened in the system.You could say that this audit trail has a lot of similarities with the architectural pattern Event Sourcing.With Event Sourcing, all changes to application state are stored as a sequence of events.This is comparable to how transactions are stored in the blockchain.It could be interesting to combine both blockchain technology and Event Sourcing principles in a project.If you want to learn more about Event Sourcing, make sure to visit the following pages:  There’s an excellent article on Martin Fowler’s blog  Our colleague Yannick De Turck also has a chapter on Event Sourcing in his blogpost about Lagom  Ken Coenen has written about CQRS and Event Sourcing too after Ordina JWorks was present at DDD Europe back in 2016  Please note that you can only achieve full immutability if you setup a public, permissionless blockchain network.In a consortium or private blockchain network, transactions can be altered because you know the nodes that validate them.SecurityBlocks are timestamped and protected with cryptographic technology that is considered unbreakable.If a block is added it can’t be removed or altered.If you change a single bit of a transaction, the hash of this transaction will be completely different. So the merkle root hash (Merkle trees are explained in the section What is blockchain) won’t be the same, the nonce number will then be wrong and the block will be considered invalid.In this way transactions are secure once chained to the blockchain.The cryptographic technology works with the principal of public and private keys, but hashing is also a part of this technology.The private key is linked to the public key, but you cannot find out the private key if you have the public key. The private key allows you to verify that you are the owner of the public key. To make transactions, you’ll need a unique key (private key) to make a digital signature to prove that you are the owner.The private key is stored in your wallet.  Your wallet doesn’t always need to contain money, it can also hold your identity.The network is also protected from (d)dos attacks because of the distributed nature of blockchain. If a hacker wants to take down the blockchain they would need to take down every node in the network. The proof-of-work can also help deter these attacks and spam because of the high costs of mining. Even if a hacker is able to penetrate one network and attempts to steal funds, there are multiple redundant copies of the same ledger stored around the world. If one is tampered with, the others could be used as a backup to prove what funds actually belong in each account.Existing platformsWe will now discuss a few platforms that can be used to set up a blockchain and also compare Bluemix and Azure.The first one is Ethereum, a public blockchain. Ethereum looks like the Bitcoin blockchain, but it uses Ether as the currency.It is faster than Bitcoin with a transaction taking seven seconds instead of ten minutes. We can also put smart contracts on the chain, with bitcoin you can only put transactions on there.Another big one we have is Hyperledger. This is a open source collaborative effort created by The Linux Foundation. Another big partner in Hyperledger is IBM because they helped them with development and donated some patents.Hyperledger is also more focused on private networks.The fun part is that you can run Hyperledger locally on your computer and try out the technology.That brings us to IBM Blockchain.IBM’s Bluemix platform focusses on private blockchains.It empowers businesses to digitize their transaction workflow through a highly secured, shared and replicated ledger.The current technology possibilities weren’t cutting it in terms of privacy so they added their code and patents to the Hyperledger project.The next one is Multichain. Multichain is an open source private blockchain, which is Bitcoin compatible.Next up is Openchain. Openchain is a little bit special because it doesn’t use the concept of blocks but the transactions are directly chained with one another, which makes it a lot faster.Openchain is an open source private blockchain. It also doesn’t use proof-of-work but proof-of-authority.BigChainDB is not really a complete blockchain but it is more a database with blockchain features like: decentralization, immutability, public/private and consensus.BigChainDB is also open source.Last but not least, we have Microsoft Azure blockchain left to discuss.As you may have guessed, Azure is the complete opposite of IBM’s Bluemix.Azure focuses on being public, although this does not mean they don’t believe in the private model.Microsoft has said that private networks will still be important for the commercial adaptation of the blockchain technology.Microsoft also don’t dedicate their platform to one type of technology like Hyperledger for Bluemix but they support many different technologies like Ethereum, Hyperledger and more. They do have a preference though for Ethereum because they joined the Enterprise Ethereum Alliance.  \t            Table: IBM is part of Hyperledger and Microsoft is part of Enterprise Ethereum Alliance.    Thinking decentralizedLast year, Ken Coenen gave a presentation about the popularity of APIs, and how companies team up to create innovative solutions.Data is freed from their silos and made available through APIs.It’s consumable for other departments and even other companies.However, when you think about it, all of this data is centralized and we need extra effort to expose it to other parties.When working with blockchain technology, your data is decentralized by nature.It’s funny when you think about it…Why do we want to store the data somewhere centralized in a silo and then make an extra effort to expose it?Isn’t is easier to start decentralized from the beginning and give access to the people who need it?What have we been doing all these years?We’ll give an example.All applications implement their own user profile functionality.All of this user data - your profile information - is duplicated across many companies.It’s already a big improvement that applications allow you to use another platform’s credentials.Logging in with your Facebook or Google account is becoming a habit.This gives the end user a way to minimize his/her digital footprint.Don Tapscott explains this really well in summer 2016’s TED Talk How the blockchain is changing money and business.Of course, blockchain technology is still in its early stages.It’s not even sure whether the technology will last.Although these statements are purely hypothetical, we find much food for thought in them.  \t        ConclusionWhen talking with people about the possibilities of blockchain, it quickly becomes clear that we still have a long way to go.People aren’t waiting for yet another technological revolution.Instead, we need to start small.Blockchain and distributed ledger technology in general will have to evolve naturally.Blockchain solutions like IBM Blockchain or Microsoft Azure Blockchain-as-a-Service make the technology very accessible to companies in an early stage.We believe that a private blockchain network is the best way to start for a company because of the following reasons:  Throwing all your data at the world is still a very scary idea  You have to take all legal aspects into account (think of the EU’s new General Data Protection Regulation)  You can start small and expose some transactions by defining permissionsCompanies are starting to develop applications on their proprietary Bluemix- or Azure platform, without exposing everything to the outside world.Get inspired by visiting State of the Dapps.Recommended readingYou can read the following books if you like to get a grasp on possible use cases which can be implemented using blockchain technology.Please note that neither of these books will deep dive into the technical aspects.  Blockchain: Blueprint for a New Economy by Melanie Swan  Blockchain Revolution: How the Technology Behind Bitcoin Is Changing Money, Business, and the World by Don Tapscott, Alex Tapscott"
      },
    
      "conference-2017-05-04-saturn-html": {
        "title": "SATURN 2017",
        "url": "/conference/2017/05/04/saturn.html",
        "image": "/img/saturn/denver.jpg",
        "date": "04 May 2017",
        "category": "post, blog post, blog",
        "content": "SATURN is the leading international conference for software architecture practitioners who look beyond the details of today’s technologies to the underlying trends, techniques and principles that underpin lasting success in our fast-moving field. SATURN offers a unique mix to learn, exchange ideas, and find collaborators at the leading edge of modern software architecture practice.2017 marks the 13th edition of the SATURN conference, organised by the Software Engineering Institute of Carnegie Mellon University.This edition was held at the Hilton Denver Inverness in Englewood, Colorado, set against the backdrop of the majestic Rocky Mountains.Pragmatic Architecture, Today  I was honoured to talk about Pragmatic Archtecture at SATURN 2017. I had done this talk a couple of times at developer conferences, so I was very anxious to compare the feedback to certain statements at an architecture conference.As it turns out, feedback was (surprisingly) similar:  Architects nor developers are very fond of UML. Both emphasize the importance of communicating the architecture of a system to stakeholders, in a clear and understable manner.  Virtually all architects have a developer background and want to be actively involved in code.  Architects understand the necessity to play the political game. Developers might also understand this, but prefer not to participate in these, sometimes frustrating, discussions.In general, I am very happy with this feedback and it shows that the two communities can (and have to) really work together nicely.Software is details (Kevlin Henney)On the first day of SATURN 2017, Kevlin Henney presented a very interesting keynote on details in software stating that everything is a detail, depending on the level of abstraction of a certain problem.  Marissa Mayer  Geeks are people who love something so much that all the details matter.And these details can be very important.In Architecture, abstractions are necessary to focus on specific parts of a system.Modern software systems are often too complex to grasp all at once so we restrict our attenton to a small number of the software system’s structures.To avoid the obvious discrepancy between abstraction and detail, Kevlin quoted Tom Gilb stating that Architecture is a hypothesis, that needs to be proven by implementation and measurement.This illustrates the importance of collaborating as a team.We will need to make decisions, given our current understanding of a certain problem.Implementing these decisions, might lead to new beliefs and insights for the architecture. If a plot works out exactly as first planned, we might not be working loosely enough to give room to imagination and instincts.This encourages looking at things from more than one point of view.An indepth look at event sourcing with CQRS (Sebastian von Conrad)While Event Sourcing is not new, it never gained wide adoption and often is a source of confusion.However, now that microservices really have become mainstream, Event Sourcing is back on everyone’s radar.Event Sourcing ensures that all changes to application state are stored as a sequence of events. Not just can we query these events, we can also use the event log to reconstruct past states.So event sourcing offers a different way of storing information, by expressing data as events.Sebastian went to great lengths to explain that this is how the real world works.  Sebastian von Conrad  Delete sucks.Event sourcing is commonly combined with the CQRS (Command and Query Responsibility Segregation) pattern by performing the data management tasks in response to events, and by materializing views from the stored events.Zooming in on CQRS, we can use the following definitions and best practices:Reading with Queries  Clients never query the events directly.  Clients query denormalised projections that are optimised for querying.  Projections are build with projectors that process the event stream.  Projectors are decoupled from each other and don’t share any state.  Projections are cheap and easy to build and rebuild.Writing with Commands  Clients never write the events directly.  Clients express an intent to do something via commands.  If replaying gets slow, performance can be improved by snapshots.  Commands are validated by Aggreggates, which is a concept borrowed from DDD.          Aggregates fetch events from the Event Store, and replay them to reconstitute their current state.      If the Aggregate accepts the Command, it results in an event.      Event Sourcing with CQRS  Event Sourcing makes you store business facts as the source of truth.  Event Sourcing makes the system deterministic.  CQRS and the Circular Architecture work well with Event Sourcing.  Asynchronous reactors process the event stream and react to events according to business logic, outputting more events.ConclusionEvent Sourcing potentionally brings a lot of value, but it’s important not to impose Event Sourcing on a team that:  Lacks buy-in to try it  Lacks stakeholder support  Lacks intestinal fortitude  A related talk at SATURN, by Paul Rayner, provided a way to visualise large scale complexity using EventStorming. In EventStorming, developers and business experts use sticky notes to map out an event-based story of how a software system behaves.He recommended Alberto Brandolini’s book on EventStorming, to learn more on the concept.Paul’s slides are available here.How to Gain Influence as a Software Architect (Adi Levin)As a software architect, you need to deal with people: it’s important to encourage collaboration.As such, a software architect will need leadership skills and will need to know where he or she is on the leadership journey.John C. Maxwell talks about this journey to great length in his book “The five levels of leadership”.A nice summary is also available on his website.Adi shared a couple of really great tips:  Express your trust in people  Show your commitment          Never say I don’t care      Share responsiblity        Admit your mistakes  Let people know you understand them          Seek first to understand, then to be understood      Acknowledge people’s position      Encourage others to contribute to the design      Listen to people who disagree      "
      },
    
      "conference-2017-04-19-dockercon-linuxkit-and-moby-html": {
        "title": "DockerCon 2017: LinuxKit and Moby",
        "url": "/conference/2017/04/19/DockerCon-LinuxKit-And-Moby.html",
        "image": "/img/dockercon2017/thumbnail.jpg",
        "date": "19 Apr 2017",
        "category": "post, blog post, blog",
        "content": "Batteries included, but swappable.That has always been the philosophy of Docker.Since the incubation of Docker four years ago,the project has undergone many evolutions.Over the years, it has split up parts of Docker into smaller reusable components,which moved to their own projects.Docker, instead of being just one project,can now be considered a composition of multiple projects.We got runC, VPNKit, containerd, SwarmKit, InfraKit and so on.These projects are now used by many other projects other than Docker.Docker Inc. now open-sourced two new projects,called LinuxKit and The Moby ProjectLinuxKitLinuxKit is a toolkit to create small, lean Linux subsystems.The difference with other Linux distributions is the fact that you can create a distribution that only contains exactly what is needed.All system services are containers and can be removed or replaced at will.Docker partnered with multiple companies like Intel, HPE, ARM, IBM and Microsoft and the Linux Foundation,to create this new component.The minimal image size is only 35MB!These portable distributions can be used to run Linux on platforms that do not support Linux out-of-the-box.For example,LinuxKit is now being used to run Linux containers on Windows Server,using Hyper-V isolation techniques.This means you can run both Windows and Linux containers side-by-side,and create Linux/Windows hybrid clusters!The Moby ProjectUsers have been asking for the Docker-native experience on their favorite platform.These requests have not gone unheard.We received Docker for Mac, Docker for Windows, Docker for AWS and Docker for Azure.All these tools are built by composing the same open components that are used for Docker.Docker now, in total, has a library of over 80 containerized components.The problem here is,a lot of work is duplicated,to compose all these components together.Each project has its own assembly system.To fix this problem,Docker Inc. looked at the automotive industry,and copied the idea of common assemblies.Just like cars can be completely different,they can share the same chassis.And this is how the Moby project was born.It attempts to bring a set of standards and best practices together.Instead of spending months of work tying all these loose components together,you can now build a tool with the components you need within a few hours.Docker stays true to its battery philosophy.You can choose which version of the kernel you want,you can choose to use which components you want.It is all up to you!An example shown at DockerCon was “RedisOS”.They composed LinuxKit, containerd and Redis,and exported it to different formats that can run on Mac, Windows and GCP.Since these distributions are so small and portable,it is possible for companies to use this for IoT, cloud, desktop and many more platforms.You can find some examples in the LinuxKit examples repository.Trying it outI wanted to try it out myself,so I went to the LinuxKit Github repository and cloned it.$ git clone git@github.com:linuxkit/linuxkit.gitThen I ran make to build the moby tool:$ makeThis created the moby tool in the bin directory.I added this to my PATH:$ export PATH=$PATH:$(pwd)/binNow that I have the Moby tool available and on my PATH,I can build LinuxKit.Let’s take a look first at the YAML file that is used to build it.$ cat linuxkit.ymlkernel:  image: \"mobylinux/kernel:4.9.x\"  cmdline: \"console=ttyS0 console=tty0 page_poison=1\"init:  - linuxkit/init:42fe8cb1508b3afed39eb89821906e3cc7a70551  - mobylinux/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9  - linuxkit/containerd:60e2486a74c665ba4df57e561729aec20758daed  - mobylinux/ca-certificates:eabc5a6e59f05aa91529d80e9a595b85b046f935onboot:  - name: sysctl    image: \"mobylinux/sysctl:2cf2f9d5b4d314ba1bfc22b2fe931924af666d8c\"    net: host    pid: host    ipc: host    capabilities:     - CAP_SYS_ADMIN    readonly: true  - name: binfmt    image: \"linuxkit/binfmt:8881283ac627be1542811bd25c85e7782aebc692\"    binds:     - /proc/sys/fs/binfmt_misc:/binfmt_misc    readonly: true  - name: dhcpcd    image: \"linuxkit/dhcpcd:48e249ebef6a521eed886b3bce032db69fbb4afa\"    binds:     - /var:/var     - /tmp/etc:/etc    capabilities:     - CAP_NET_ADMIN     - CAP_NET_BIND_SERVICE     - CAP_NET_RAW    net: host    command: [\"/sbin/dhcpcd\", \"--nobackground\", \"-f\", \"/dhcpcd.conf\", \"-1\"]services:  - name: rngd    image: \"mobylinux/rngd:3dad6dd43270fa632ac031e99d1947f20b22eec9\"    capabilities:     - CAP_SYS_ADMIN    oomScoreAdj: -800    readonly: true  - name: nginx    image: \"nginx:alpine\"    capabilities:     - CAP_NET_BIND_SERVICE     - CAP_CHOWN     - CAP_SETUID     - CAP_SETGID     - CAP_DAC_OVERRIDE    net: hostfiles:  - path: etc/docker/daemon.json    contents: '{\"debug\": true}'trust:  image:    - mobylinux/kerneloutputs:  - format: kernel+initrd  - format: iso-bios  - format: iso-efiThis YAML file contains all the necessary information to create a distribution,that will be available in multiple formats.In this case,a random number generated and nginx are added as services.Let’s build it!$ moby build linuxkit.ymlExtract kernel image: mobylinux/kernel:4.9.xAdd init containers:Process init image: linuxkit/init:42fe8cb1508b3afed39eb89821906e3cc7a70551Process init image: mobylinux/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9Process init image: linuxkit/containerd:60e2486a74c665ba4df57e561729aec20758daedProcess init image: mobylinux/ca-certificates:eabc5a6e59f05aa91529d80e9a595b85b046f935Add onboot containers:  Create OCI config for mobylinux/sysctl:2cf2f9d5b4d314ba1bfc22b2fe931924af666d8c  Create OCI config for linuxkit/binfmt:8881283ac627be1542811bd25c85e7782aebc692  Create OCI config for linuxkit/dhcpcd:48e249ebef6a521eed886b3bce032db69fbb4afaAdd service containers:  Create OCI config for mobylinux/rngd:3dad6dd43270fa632ac031e99d1947f20b22eec9  Create OCI config for nginx:alpineAdd files:  etc/docker/daemon.jsonCreate outputs:  linuxkit-bzImage linuxkit-initrd.img linuxkit-cmdline  linuxkit.iso  linuxkit-efi.isoYou can see that a few init, onboot and service containers were added,and a configuration file was added for Docker.Finally,you can see the tool was outputted in multiple formats.Let’s try to run it:$ moby run linuxkit# ...# Lots of boot information# ...Welcome to LinuxKit                        ##         .                  ## ## ##        ==               ## ## ## ## ##    ===           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~           \\______ o           __/             \\    \\         __/              \\____\\_______// # [    2.464063] IPVS: Creating netns size=2104 id=1[    2.464434] IPVS: ftp: loaded support on port[0] = 21[    2.490221] tsc: Refined TSC clocksource calibration: 1993.943 MHz[    2.490613] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x397ba967053, max_idle_ns: 881590807276 ns[    2.713076] IPVS: Creating netns size=2104 id=2[    2.713560] IPVS: ftp: loaded support on port[0] = 21[    3.503395] clocksource: Switched to clocksource tsc/ #This image booted in just a few seconds!Now let’s see which service containers are running using runC:/ # runc listID          PID         STATUS      BUNDLE                        CREATED                        OWNERnginx       542         running     /run/containerd/linux/nginx   2017-04-19T12:34:42.1852841Z   rootrngd        601         running     /run/containerd/linux/rngd    2017-04-19T12:34:42.3200486Z   rootAs you can see,all the service containers are up and running.Within just a few minutes,I created a Linux distribution and got it up and running,with everything running in a container.If you would like to learn more about LinuxKit and the Moby Project,you can check out the following resources:  The Moby Project  Docker Blog: Anouncing LinuxKit  Docker Blog: Introducing Moby Project  GitHub: LinuxKit  GitHub: Moby"
      },
    
      "conference-2017-04-18-dockercon-multi-stage-builds-and-more-html": {
        "title": "DockerCon 2017: Multi-Stage Builds and More",
        "url": "/conference/2017/04/18/DockerCon-Multi-Stage-Builds-And-More.html",
        "image": "/img/dockercon2017/thumbnail.jpg",
        "date": "18 Apr 2017",
        "category": "post, blog post, blog",
        "content": "DockerCon 2017 has kicked off, and Docker has changed a great deal since last year’s edition.The authors of Docker have constantly stated that they wish to make the Docker experience as simple as possible.Nothing is less true if you look at some of the new features they released in the last few months,which are being presented now at DockerCon.I have compiled a list of the most useful changes and features.These will certainly help you when building your own Docker images!Multi-Stage BuildsBuilding an image is often done in multiple stages.First you compile your application.Then you run your tests.When the tests succeed, you package it into an artifact.Finally, you add this artifact to an image.You could put all these steps within one Dockerfile,but that would result into an image that is bloated with stuff that is not required for the final product, like the compilation and build frameworks.The Docker images would also be huge!A solution to this problem is to build the application outside of Docker,or to use multiple Dockerfiles.You can build the artifact with one build,extract the artifact,and use that artifact for a final build.However,this whole build process is often tied together with a script that has been hacked together,and does not truly feel like the Docker way of doing things.Docker has often been sceptical about adding new features or making changes to the Dockerfile syntax,but finally decided to tackle this build problem with a simple and elegant solution.Introducing multi-stage builds,it is now possible to define multiple stages by using several FROM statements.# First stage to build the applicationFROM maven:3.5.0-jdk-8-alpine AS build-envADD ./pom.xml pom.xmlADD ./src src/RUN mvn clean package# Final stage to define our minimal runtimeFROM FROM openjdk:8-jreCOPY --from=build-env target/app.jar app.jarRUN java -jar app.jarEach time FROM is used,you define which image is used for that stage,and in subsequent stages you can use the COPY --from=&lt;stage&gt; to copy artifacts from a previous stage.The final stage results in the image,which can contain the minimal runtime environment and the final artifact.Perfect!Using arguments in FROMUsing arguments isn’t a new thing with Dockerfiles.You could already use ARG statements to pass on arguments to the build process.These arguments are not persisted in the Dockerfile,and are frequently used to pass on versions,or secrets like SSH keys.Now it is also possible to use arguments in the version of the base image.ARG GO_VERSION=1.8FROM golang:${GO_VERSION}ADD . /srcWORKDIR /srcRUN go buildCMD [\"/bin/app\"]For above Dockerfile,I could build an image with another Go version!$ docker build --arg=GO_VERSION=1.7 .Cleaning up DockerA comment I often hear is that Docker takes a lot of space.This can be true,if you never clean up!Docker has added the docker system subcommand a while ago,You can use this subcommand to check the disk usage,and to free up space!The following command outputs the disk usage:$ docker system dfTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLEImages              7                   5                   1.247GB             769MB (61%)Containers          7                   2                   115.9MB             99.23MB (85%)Local Volumes       1                   1                   85.59MB             0B (0%)You can then use prune to clean up all resources that are no longer needed:$ docker system pruneWARNING! This will remove:\t- all stopped containers\t- all volumes not used by at least one container\t- all networks not used by at least one container\t- all dangling imagesAre you sure you want to continue? [y/N] yIt is also possible to prune certain subsystems:$ docker image/container/volume/network pruneWhich Ports?People often have trouble understanding or defining the published ports of a container,since the syntax can be confusing.Here’s a list of all possible formats that you can use to define which ports are published on a container:ports: - 3000 - 3000-3005 - 49100:22 - 9090-9091:8080-8081 - 127.0.0.1:8001:8080-8081 - 6060:7060/udpThis syntax is okay when using the CLI,but when you have to define a lot of them in a Compose file,it is no longer readable.To counter this issue,you can now use a more verbose format to define ports:ports:  - target: 6060    published: 7060    protocol: udpThis Volume Is Mounted Where?Just like the ports,volumes have a similar syntax.volumes:  - /var/lib/mysql  - /opt/data:/var/lib/mysql  - ./cache:/tmp/cached  - datavolume:/var/lib/mysql  - ~/configs/etc/configs/:roA verbose syntax has been added as well for volumes:volumes:  - type: bind    source: ~/configs    target: /etc/configs    read_only: trueTo be continuedThese few changes,especially the multi-stage builds,will certainly make your life as developer easier!I am curious what DockerCon has to offer more today and tomorrow!"
      },
    
      "architecture-2017-04-14-bredemeyer-html": {
        "title": "Bredemeyer: Architects Architecting Architecture",
        "url": "/architecture/2017/04/14/bredemeyer.html",
        "image": "/img/bredemeyer.jpg",
        "date": "14 Apr 2017",
        "category": "post, blog post, blog",
        "content": "Software architecture is getting a lot of attention.It looks beyond the details of today’s technologies to the underlying trends, techniques, and principles that underpin lasting success in our fast-moving field.It is critical to today’s business success; yet it requires technical, business and organizational talents and skills that warrant their own path of career development, education, and research.Earlier this month, I was lucky enough to participate in a four-day workshop organized by Bredemeyer Consulting in the Netherlands.The goal of the workshop: helping good architects become great architects.Bredemeyer Consulting intends to inspire and encourage architects to greatness, by helping them to visualize what is possible, and see how to get there.They aim to achieve this by providing the tools and techniques to help architects be successful.The training covers the essential technical tasks of architecting (architecture modeling and specification, architecture tradeoff analysis, component design and specification, etc.) as well as architectural leadership skills.Topics  Architecture  Strategy  Conceptual Architecture  Logical Architecture  Leadership1. ArchitectureWhatDefining architecture is never easy, as many different definitions are used by different organizations.Dana Bredemeyer defines architecture as a set of decisions that have multiple uses.These uses can span time, projects and places.Of course, this makes it easy to have too much or too little architecture.Finding the right balance, is difficult.To illustrate this difficulty, the following properties should be kept in mind when validating architectural decisions.An architectural description can be:  Good          Technically sound (eg. having well-defined interfaces)      Well-documented      Elegant        Right          A solution to the problem        Successful          When the system realizes value      It is up to the architect to make sure value is realized.HowDuring the architectural process, it is of extreme importance to look both at strategy and at implementation, as an architect generally moves between these two worlds.This process is comparable to the elevator approach from Gregor Hohpe.His book “37 Things One Architect Knows About IT Transformation” is  approachable to read and contains a large number of useful tips for aspiring and seasoned architects.Most certainly a recommended read.To facilitate decision-making, Dana Bredemeyer suggests using a re-reentrant discovery activity.In this method, decisions are taken early (and written down) because at an early stage it is still cheap to change them.However, when decisions become expensive (eg. impacting implementation), they should be taken at the last possible, responsible moment.The re-entrant nature of this model, facilitates change, dialogue and consensus.It is very similar to OODA (Observe, Orient, Decide, Act) and PDCA (Plan, Decide, Check, Act), also known as the Deming cycle.PrinciplesTo kickstart an architectural description, it is useful to define a strong foundation with a set of principles.These principles must be clear, unambiguous and actionable.They can not conflict with each other and must be followed.The reason to define these principles early, is to provide confidence when solving hard problems and to constrain decisions that get made numerous times.A couple of examples:  Ebay  Bredemeyer2. StrategyFor an architecture to be successful, it must support the business strategy of the system and the organization.One might define architecture as the translation of strategy into technology.An approach to achieve this is:  Clarify the business concept(s)  Brainstorm (with business stakeholders to look at possible and alternative business concepts)  Define high-level requirements  Define high-level architecture (From Conceptual Architecture to Logical Architecture)  ValidateAn architect must understand what business is trying to achieve and understand what an organization needs to be good at to realize success (or what the system needs to be good at).From idea to strategyWhen business concepts are clear, but the strategy isn’t (fully) established, shaping strategy from business concepts can be achieved by:  Writing down business concepts as a bulleted list  Translating this bulleted list into plain text  Looking at the difference between the list and the textThis will, more often than not, refine the business concepts and identify what is strategically most important.Another way to refine business concepts, is using the Business Model Canvas.3. Conceptual ArchitectureThe goal of the Conceptual Architecture, is to define components (subsystems) and the interaction between these components.The Conceptual Architecture must remain high-level, because in this phase, the architect wants to explore alternatives.Adding too much detail to the Conceptual Architecture will become expensive.White board sketching can be a useful method to determine to Conceptual Architecture: talking with the business users next to a white board and drawing the system together.Dana shared a couple of tips and tricks to increase participation from business users:  Make the diagram a bit sloppy. This will prevent participants from thinking it’s finished.  Have fun. Let the ideas flow.  Use colors and icons. Make it visual.Another tool to formalize components are CRC-R Templates. These are typically half a page narratives that define a component, its responsibility, its collaboration with other components and the rationale behind its responsibilities.The Conceptual Architecture can be used to validate the feasibility of alternatives.For example by going over use-cases or by identifying if the system can be in a state that renders the architecture invalid.4. Logical ArchitectureThe Logical Architecture details out full responsibilities per component and all the interfaces per component.It adds precision, providing a detailed blueprint from which component developers and component users can work in relative independence.These components ought to be derived from the Conceptual Architecture.By selecting the core behavior of the components and defining the data that moves between components (eg. in a sequence diagram), the architecture becomes actionable and ready for implementation.In larger systems, the sequence diagram can aid a component owner in understanding how his component lives in the bigger system.When the state of the data (moving between components) evolves, it might also be useful to draw a state diagram.A very interesting attention point to creating a logical architecture is that the value of a system is not the sum of its parts, but the sum of the interaction between the parts.This is beautifully explained by Russell Ackoff in this video on YouTube.5. LeadershipBeing successful as a leader often depends on the ability to influence others: getting from a lot of (possibly good) ideas to a shared vision: a philosophical harmony of values.Getting this buy-in from stakeholders only strengthens the importance of interaction and collaboration.The activities of a leader range from inspiring, mentoring, listening and setting directions.Settings directions means formally defining what a system must do or what a strategy wants to achieve.When a leader empowers his (or hers) team, trust will be established and more value will be realized.Passion and Discipline: Don Quixote’s Lessons for LeadershipWhy Don Quixote?What lessons can we learn from the fictional 16th-century gentleman who careered around the Spanish countryside tilting at windmills and challenging sheep to battle?  James G. March  We live in a world that emphasizes realistic expectations and clear successes.Quixote had neither.But through failure after failure, he persists in his vision and his commitment.He persists because he knows who he is. The critical concerns of leadership are not technical questions of management or power, they are fundamental issues of life.  – Source: Insights by Stanford BusinessBelieving in something can make others believe in it."
      },
    
      "angular-2017-04-04-optimising-performance-of-your-enterprise-angular-application-html": {
        "title": "Optimising performance of your Enterprise Angular Application",
        "url": "/angular/2017/04/04/optimising-performance-of-your-enterprise-angular-application.html",
        "image": "/img/optimising-performance-of-your-enterprise-angular-application.png",
        "date": "04 Apr 2017",
        "category": "post, blog post, blog",
        "content": "This blog post contains best practices that helped us optimise performance of our Enterprise Angular (v2+) Application we created for one of our clients.The project has been created in under 6 months with a dedicated team of 7 people of which 4 people are from the JWorks unit (2 front-end, 2 backend) and consists of two Angular Applications that use modules and components from a shared library.The use of Angular Universal does not apply (yet) for this project.Topics  Lazy loading  Code splitting and commons chunk plugin (webpack)  ChangeDetectionStrategy: OnPush  Reusable CSS with BEM and Sass  GZIP  AOT1. Lazy loadingLazy loading your project modules can greatly enhance performance.After each successful navigation, the router looks in its configuration for an unloaded module that it can preload.Whether it preloads a module, and which modules it preloads, depends upon the preload strategy.The Router offers two preloading strategies out of the box:  No preloading at all which is the default. Lazy loaded feature areas are still loaded on demand.  Preloading of all lazy loaded feature areas.We implemented the PreloadAllModules strategy in its default configuration, but know that it is possible to create your own custom preloading strategy.To do so, include the preloadingStrategy in your @NgModule like so:@NgModule({    ...    imports: [        RouterModule.forRoot(ROUTES, { preloadingStrategy: PreloadAllModules })    ]});And define your routes like this:{    path: 'performance',    loadChildren: 'performance.module#PerformanceModule',    canLoad: [AuthGuard] // Optional}Note that when using guards, the CanLoad guard blocks loading of feature module assets until authorised to do so.If you want to both preload a module and guard against unauthorised access, use the CanActivate guard instead.Want to get started with lazy loading?Maybe create a custom preloading strategy?Check out the talk Manfred Steyer gave at NG-BE 2016 about improving start-up performance with lazy loading or view the Angular docs.2. Code splitting and commons chunk plugin (webpack)Code splitting is one of the most compelling features of webpack.It allows you to split your code into various bundles which you can then load on demand — like when a user navigates to a matching route, or on an event from the user.This allows for smaller bundles, and allows you to control resource load prioritization, which, if used correctly, can have a major impact on your application load time.There are mainly two kinds of code splitting that can be accomplished with webpack: “Vendor code splitting” and “On demand code-splitting” (used for lazy loading).The CommonsChunkPlugin is an opt-in feature that creates a separate file (known as a chunk), consisting of common modules shared between multiple entry points.By separating common modules from bundles, the resulting chunked file can be loaded once initially, and stored in cache for later use.This results in pagespeed optimisations as the browser can quickly serve the shared code from cache, rather than being forced to load a larger bundle whenever a new page is visited.Among other optimisations the extra async commons chunk allows us to drastically improve performance by moving common modules out of the parent so that a new async-loaded additional commons chunk is used, which decreases initial load time.This is automatically downloaded in parallel when the additional chunk is downloaded.new webpack.optimize.CommonsChunkPlugin({  children: true,  // (use all children of the chunk)  async: true,  // (create an async commons chunk)});3. ChangeDetectionStrategy: OnPush3.1 The problemUnlike using a Virtual DOM, like ReactJS, Angular uses change detection to update the actual DOM presented to the user.Each component in an Angular application has its own change detector and in order to guarantee the latest data is always presented to the user, the default change detection strategy on an Angular component is set to always update.This means that any time JavaScript finishes executing, Angular will check for changes in all components.This usually works fast in small applications.However, when a component has a large subset of components (e.g.: a list with several items in which every row is presented by a component), performance may take a hit, even when (almost) nothing changes.The reason is that, due to the default change detection, Angular will also check for updates on a component when a change occurs on its siblings or ancestors or child components, while this is not needed in most cases.3.2 The solutionNext to trying to have less DOM, the solution is to use the OnPush strategy for change detection.The OnPush strategy will let the change detector run only in the following situations:  when an event handler is fired in the component  when one of its input properties changes  when you manually request the change detector to look for changes (using ChangeDetectorRef’s function markForCheck())  when a child’s change detector runs3.3 Setting up OnPushThere are two ways to set up the OnPush strategy3.3.1. Immutable input objectsThe simplest way is to use only immutable objects.   import { Component, Input, ChangeDetectionStrategy } from '@angular/core';   @Component({       selector: 'my-sub-component',       template: `{{ item.name }}`,       changeDetection: ChangeDetectionStrategy.OnPush   })   export class MySubComponent implements OnInit {       @Input() item: {name: string};       constructor() {}   }   The change detector will only run when the input property ‘item’ changes.   The key here is to update the reference to the object.   The change detector won’t run when something inside the object (e.g.: property ‘name’) changes.3.3.2. Observable input objectsAnother way is to use observables as inputs.   import { Component, Input, ChangeDetectionStrategy, ChangeDetectorRef } from '@angular/core';   @Component({       selector: 'my-sub-component',       template: `{{ myItemName }}`,       changeDetection: ChangeDetectionStrategy.OnPush   })   export class MySubComponent implements OnInit {       @Input() itemStream:Observable&lt;any&gt;;       myItemName: string;       constructor(private changeDetectorRef: ChangeDetectorRef) {}       ngOnInit() {           this.itemStream.subscribe(i =&gt; {               this.myItemName = item.name;               this.changeDetectorRef.markForCheck();           });       }   }   The change detector will run when the itemStream emits a new item.As change detection is run from top to bottom components, start by setting OnPush on the leaf components and work your way up.This allows to skip change detection in entire subtrees.4. Reusable CSS with BEM and SassSass has been an all-time favorite for writing structured and maintainable CSS for large projects.We combined this with the BEM methodology which helps to create extendable and reusable interface components.We used this approach on our project to create a style guide in the shared module that includes all working components used in the application.Once most of the components were available we could simply start including them in the modules that needed to be built.This greatly decreased the time needed to build the functional module.Things like colors, typography, utilities, etc. are bundled in separate files that can be included where needed.This prevents writing the same CSS over and over again and keeps the code base small(er).5. GZIPGzip is a file format and also a method of compressing files (making them smaller) for faster network transfers.It allows your web server to provide files with a smaller size that will be loaded faster by your browser.Compression of your files with gzip typically saves around fifty to seventy percent of the file size.You can easily enable gzip compression on your server by editing your .htaccess file:#Set to gzip all outputSetOutputFilter DEFLATE#exclude the following file typesSetEnvIfNoCase Request_URI \\.(?:exe|t?gz|zip|iso|tar|bz2|sit|rar|png|jpg|gif|jpeg|flv|swf|mp3)$ no-gzip dont-vary#include the following file typesAddType x-font/otf .otfAddType x-font/ttf .ttfAddType x-font/eot .eotAddType image/x-icon .icoAddType image/png .pngAddType image/svg+xml .svgAddOutputFilterByType DEFLATE text/plainAddOutputFilterByType DEFLATE text/htmlAddOutputFilterByType DEFLATE text/xmlAddOutputFilterByType DEFLATE text/cssAddOutputFilterByType DEFLATE application/xmlAddOutputFilterByType DEFLATE application/xhtml+xmlAddOutputFilterByType DEFLATE application/rss+xmlAddOutputFilterByType DEFLATE application/javascriptAddOutputFilterByType DEFLATE application/x-javascriptAddOutputFilterByType DEFLATE image/svg+xml#set compression levelDeflateCompressionLevel 9#Handle browser specific compression requirementsBrowserMatch ^Mozilla/4 gzip-only-text/htmlBrowserMatch ^Mozilla/4.0[678] no-gzipBrowserMatch bMSIE !no-gzip !gzip-only-text/html# Make sure proxies don't deliver the wrong contentHeader append Vary User-Agent env=!dont-vary6. AOTAt the time of writing, the application still runs using the just-in-time (JIT) compiler.But we are looking into how we can integrate AOT.JIT compilation incurs a runtime performance penalty.Views take longer to render because of the in-browser compilation step.The application is bigger because it includes the Angular compiler and a lot of library code that the application won’t actually need.Bigger apps take longer to transmit and are slower to load.Compilation can uncover many component-template binding errors.JIT compilation discovers them at runtime, which is late in the process.The ahead-of-time (AOT) compiler can catch template errors early and improve performance by compiling at build time.AOT ensures  Faster rendering  Fewer asynchronous requests  Smaller Angular framework download size  Earlier detection of template errors  Better securityConclusionWhile Angular states it’s performance driven out of the box, it is very important to optimise, where possible, especially when building a large Enterprise Angular Application.As you can see it’s not that difficult to integrate, so why wouldn’t you?Every bit of data that doesn’t end up downloading to the device of your users is a bless.Hat tip: Try to integrate these changes when setting up your project.It would be a shame to end up refactoring your code when halfway into development."
      },
    
      "ionic-2017-02-02-ordina-becomes-ionic-trusted-partner-html": {
        "title": "Ordina becomes Ionic Trusted Partner",
        "url": "/ionic/2017/02/02/Ordina-becomes-Ionic-Trusted-Partner.html",
        "image": "/img/ordina-becomes-ionic-trusted-partner.png",
        "date": "02 Feb 2017",
        "category": "post, blog post, blog",
        "content": "Ordina becomes Ionic Trusted PartnerWithin the JWorks unit of Ordina, Jan De Wilde has been following Ionic framework since 2014.Jan goes way back in the world of frontend development and Ionic allowed him to use his knowledge of JavaScript, HTML and CSS to create hybrid mobile applications.Along the way of experimenting, evangelizing Ionic within the unit and promoting Ionic at our clients we have had the opportunity to build some amazing Ionic (1 &amp; 2) applications for our clients.We strongly believe that Hybrid and Progressive Web Apps are the future and keep investing time in getting better in it and giving training to our employees.Some time ago Ionic opened up the Trusted Partner Program and after applying with a motivational letter and a description of projects we did for our clients, we have been selected as a Trusted Partner.  What does Ionic say about Trusted Partners?  Ionic Trusted Partners are certified consulting agencies that we connect with businesses looking to jumpstart their Ionic app development.  – Source: https://ionic.io/trusted-partners  Currently we work with both Ionic 1 and Ionic 2.We use technologies such as:  Firebase for realtime communication and data synchronization,  TypeScript and Webpack to organize our code,  and Karma, Jasmine and Protractor for testing.As mobile devices are increasingly used in almost all organisations, it is important for businesses to better protect these systems through Enterprise Mobility Management.By pushing Custom Device Policies to mobile devices via the EMM platform, administrators can control how these devices should behave within the organization, taking into account the already existing security rules.This can reduce the risk of data loss, unauthorized access and unapproved software installs on mobile devices with access to the company network.Mobile security is not just something for very large enterprises, but is relevant to all types of businesses.So it is important to take this into account from the very beginning.Want to work with us for your next mobile project? Or train your people to use Ionic?Contact Jan De Wilde.A selection* of our projectsProximus MyThings LoRa IoT PlatformFor Proximus, a big telecom operator in Belgium we have created a portal to manage LoRa connected IoT sensors and an Ionic 1 application for the field engineers to onboard and configure sensors. First part of the application allows the field engineer to identify and install/onboard a sensor using a QR code scanner or via manual input of the MAC address.Sensor type, company, location with lat/lng detection is provided in the application.Second part of the application is the ability to adjust or replace an existing sensor. Third part is a view where active/inactive sensors are presented together with their properties and last sent data. All data is exposed using web services created in Spring Boot and data is stored in a Mongo DB cluster and MySQL instance.This application is publicly available in the app store, but only accessible for field engineers.  Arcelor Mittal slab quality applicationFor Arcelor Mittal a steel processing company in Belgium we have created an application in Ionic 2 for quality assurance of the steel slabs that need to be processed. The application allows the quality assurer to look up a slab using a unique slab ID and check different properties such as: measurement, removal of edges, scarfing and cutting.The application integrates with web services provided by Arcelor Mittal and is privalely hosted by Arcelor Mittal.  4411 Parking Application for the city of CharleroiFor the city of Charleroi we created an Ionic 1 application that allows the parking guard to identify which vehicles are not parked according to their neighborhood subscription.The parking guard needs to identify himself in order to look up license plate numbers in the zone they are active.As a additional POC we integrated with a service that enabled the parking guard to take pictures of a parked car to identify the license plate number, car type, color, etc.This application is privately hosted by 4411.*Due to copyright, we are not able to display all Ionic projects."
      },
    
      "microservices-2017-02-01-lagom-1-2-html": {
        "title": "Lagom 1.2: What's new?",
        "url": "/microservices/2017/02/01/Lagom-1-2.html",
        "image": "/img/lagom.png",
        "date": "01 Feb 2017",
        "category": "post, blog post, blog",
        "content": "Table of Contents  Looking back at the first version  Lagom 1.2  Message Broker API &amp; Kafka implementation  Generic read-side support, sharding support and automatic offset handling  JDBC support  Migrating from 1.0 to 1.2  Looking at the rest of our initial feedback  Lagom 1.3 preview  Conclusion  Extra resourcesLooking back at the first versionShortly after the first MVP version of Lagom was released we wrote a blogpost in which we wrote down our first impressions and in which we did an initial comparison to Spring Cloud and Netflix OSS.We also did an introduction presentation on Lagom which is available on YouTube.Lightbend let us know that they really appreciated our feedback.They definitely understood some of our remarks and suggestions and they were willing to work on some of them.One of our majors remarks was that Maven support should be added to Lagom in order to properly target Java developers.We weren’t the only ones with this feedback and Lightbend took note of it.In September 2016, Lightbend released Lagom 1.1. In this first new minor version they introduced Maven support which also includes support for running the Lagom development environment in Maven.More information about setting up a Lagom project in Maven is available in the documentation.A few months later they released Lagom 1.2 with a couple of new features which is what this blogpost will be about.We will also revisit our initial comparison against Spring Cloud and Netflix OSS.Lagom 1.2In this new minor version of Lagom, the read-side has been overhauled.Other notable additions are the following:  Message Broker API &amp; Kafka implementation  JDBC support  Generic read-side support, sharding support and automatic offset handlingNaturally, at the same time, a couple of existing issues and bugs were also resolved. A full list is available on Github.Message Broker API &amp; Kafka implementationThe introduction of message broker support is the most notable feature of Lagom 1.2.By adding message broker support, Lagom now allows both direct streaming of messages between services but also through a broker.With version 1.2, Lagom comes with out-of-the-box support for Apache Kafka, a very popular scalable message broker for building real-time data pipelines and streaming application.The message broker API has been designed to be independent of any backend meaning that support for other brokers may be added in the future.Compared to the existing Publish-Subscribe mechanism where messaging is only available intra-service, message broker communication happens between one service to many other services.Another key difference between the two is that with Publish-Subscribe it is possible that messages might get lost, for example due to network issues or a restart of a service, message broker based communication can provide at-least-once and at-most-once delivery semantics even if the subscriber is down.In Publish-Subscribe messaging, a subscriber will only receive a message after its subscription has been accepted by the Publish-Subscribe infrastructure.The message broker on the other hand will allow the subscriber to consume all the messages since the last message it has consumed, even if the subscriber was offline or down, thanks to its decoupling of services producing events from others consuming them.The Publish-Subscribe mechanism in Lagom is provided by the Akka Cluster underneath a Lagom service whereas message broker support is provided by a third party product such as Kafka.Lagom takes care of publishing, partitioning, consuming and failure handling of messaging and when executing the runAll command, Lagom automatically runs a Kafka server with you along with Zookeeper for you.Next to ServiceCall for communicating with other services mapping down onto HTTP, there now exists a Topic abstraction that represents a topic that one service publishes and that other services can consume after subscribing.In order to make use of it you need to add the Lagom Kafka Broker module to the dependencies for both the service publishing to a topic and the service subscribing to a topic.Note that Lagom Kafka Broker module requires an implementation of Lagom Persistence so you need to add either Lagom Persistence Cassandra or Lagom Persistence JDBC to the dependencies.In our simple demo project Lagom Shop we define a new method in the item-api project’s ItemService that will return a Topic of item creation events to subscribe on and we also add it to the descriptor:Topic&lt;ItemEvent&gt; createdItemsTopic();@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").withCalls(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).publishing(            Service.topic(\"createdItems\", this::createdItemsTopic)    ).withAutoAcl(true);}In the item-api project we define a new ItemEvent interface for external use in other services.An ItemEvent already exists in the item-impl project but you want this one to be solely used within the implementation project.It is considered a best practice to have a separate definition for external use as it would otherwise cause you to break clients if you would apply internal changes.@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, property = \"type\", defaultImpl = Void.class)@JsonSubTypes({        @JsonSubTypes.Type(ItemEvent.ItemCreated.class)})public interface ItemEvent {    UUID getId();    @JsonTypeName(\"item-created\")    final class ItemCreated implements ItemEvent {        private final UUID id;        private final String name;        private final BigDecimal price;            @JsonCreator        public ItemCreated(UUID id, String name, BigDecimal price) {            this.id = id;            this.name = name;            this.price = price;        }            @Override        public UUID getId() {...}            public String getName() {...}            public BigDecimal getPrice() {...}            @Override        public boolean equals(Object o) {...}            @Override        public int hashCode() {...}            @Override        public String toString() {...}    }}In the item-impl project we add the implementation to ItemServiceImpl that will publish all ItemCreated events to the topic:@Overridepublic Topic&lt;be.yannickdeturck.lagomshop.item.api.ItemEvent&gt; createdItemsTopic() {    return TopicProducer.singleStreamWithOffset(offset -&gt; {        return persistentEntities                .eventStream(ItemEventTag.INSTANCE, offset)                .filter(eventOffSet -&gt; eventOffSet.first() instanceof ItemCreated)                .map(this::convertItem);    });}private Pair&lt;be.yannickdeturck.lagomshop.item.api.ItemEvent, Offset&gt; convertItem(Pair&lt;ItemEvent, Offset&gt; pair) {    Item item = ((ItemCreated)pair.first()).getItem();    logger.info(\"Converting ItemEvent\" + item);    return new Pair&lt;&gt;(new be.yannickdeturck.lagomshop.item.api.ItemEvent.ItemCreated(item.getId(), item.getName(),            item.getPrice()), pair.second());}We now want to make use of this in our order-impl project.Using the injected ItemService instance we can now subscribe on the topic and act on each message.In this case we log something whenever we receive a new message.@Injectpublic OrderServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,                        ItemService itemService, PubSubRegistry topics, CassandraSession db) {    ...    itemService.createdItemsTopic()            .subscribe()            .atLeastOnce(Flow.fromFunction((be.yannickdeturck.lagomshop.item.api.ItemEvent item) -&gt; {                logger.info(\"Subscriber: doing something with the created item \" + item);                return Done.getInstance();            }));}As mentioned earlier, you have the option to make use of either the atLeastOnce or atMostOnceSource delivery semantic on the Subscriber instance.If we run the application, create an item and afterwards check the logs, we see that the order service is receiving messages:2016-12-23 22:04:22,337 INFO  b.y.l.i.i.ItemServiceImpl - Creating item: CreateItemRequest{name=newItem, price=15}2016-12-23 22:04:22,349 INFO  b.y.l.i.i.ItemEntity - Setting up initialBehaviour with snapshotState = Optional.empty2016-12-23 22:04:22,357 INFO  b.y.l.i.i.ItemEntity - Processed CreateItem command into ItemCreated event ItemCreated{item=Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}, timestamp=2016-12-23T21:04:22.357Z}2016-12-23 22:04:22,359 INFO  b.y.l.i.i.ItemEntity - Processed ItemCreated event, updated item state2016-12-23 22:04:22,412 INFO  b.y.l.i.i.ItemEntity - Processed GetItem command, returned item2016-12-23 22:04:22,413 INFO  b.y.l.i.i.ItemServiceImpl - Looking up item d370993c-dd75-4a88-bf8b-d9dba1820feb2016-12-23 22:04:25,472 INFO  b.y.l.i.i.ItemEventProcessor - Persisted Item Item{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}2016-12-23 22:04:25,776 INFO  b.y.l.i.i.ItemServiceImpl - Converting ItemEventItem{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name=newItem, price=15}2016-12-23 22:04:25,883 INFO  b.y.l.o.i.OrderServiceImpl - Subscriber: doing something with the created item ItemCreated{id=d370993c-dd75-4a88-bf8b-d9dba1820feb, name='newItem', price=15}Generic read-side support, sharding support and automatic offset handlingUp until now, the read-side processor API was specific to Cassandra and required you to do the necessary offset tracking.This existing API, while still usable, has been declared depricated and instead a new specific implementation for constructing Cassandra read-sides and a more generic one for JDBC read-sides have been added.The read-side can now also be sharded by tagging persistent entity events with sharded tags.Instead of declaring only one tag, read-side processors now declare a list of them.The processing of these tags across the cluster is handled for you by Lagom.To compare the two of them, here is a sample of using a single tag:public interface ItemEvent extends Jsonable, AggregateEvent&lt;ItemEvent&gt; {        AggregateEventTag&lt;ItemEvent&gt; TAG = AggregateEventTag.of(ItemEvent.class);        @Override    default AggregateEventTag&lt;ItemEvent&gt; aggregateTag() {        return TAG;    }}And an example of using sharded tags:public interface ItemEvent extends Jsonable, AggregateEvent&lt;ItemEvent&gt; {    int NUM_SHARDS = 20;    AggregateEventShards&lt;ItemEvent&gt; TAG = AggregateEventTag.sharded(ItemEvent.class, NUM_SHARDS);    @Override    default AggregateEventShards&lt;ItemEvent&gt; aggregateTag() {        return TAG;    }}In your ReadSideProcessor you will have to override the aggregateTags() abstract method differently.When using a single tag:@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {    return TreePVector.singleton(ItemEvent.TAG);}And when using sharded tags:@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {  return ItemEvent.TAG.allTags();}Lagom now also provides automatic offset tracking, which until now, you had to do yourself in your read-side processors by explicitly loading and persisting offsets.This allows us to get rid of quite a bit of code and as less code means less bugs, this is definitely a good thing.In the section dealing with migrating to Lagom 1.2 there is a part that shows the code that we got rid of.That the existing Cassandra read-side support API still exists but it has been deprecated and might be removed in an upcoming version so it is a good idea to migrate as soon as possible.It isn’t really that much work to migrate to the new API, the exact work required is described in the Migrating from 1.0 to 1.2 section.JDBC supportJDBC support has been added to ease the introduction of Lagom into the existing organisation of potential users in order to provide support for using their existing relational database infrastructure.In Lagom 1.3.0, support for JPA will also be added which should become the preferred choice.A relational database is less preferred when setting up a non-blocking and reactive architecture but by providing support for it, it would allow potential users to start making use of the Persistent Entity API without the necessity of having to switch over to Cassandra.To make use of JDBC support you need to add the Persistence JDBC module to your project while also having to add the jar for your JDBC driver.Lagom makes use of akka-persistence-jdbc to persist entities to the database.At the time of writing only four relational databases are supported:  PostgreSQL  MySQL  Oracle  H2akka-persistence-jdbc uses Slick for mapping tables and managing asynchronous execution of JDBC calls.Slick requires you to configure it to use the right Slick profile for your database.An example of a Slick configuration in application.conf:db.default {  driver = \"org.postgresql.Driver\"  url = \"jdbc:postgresql://database.example.com/playdb\"}jdbc-defaults.slick.driver = \"slick.driver.PostgresDriver$\"A table and journal table are required by the akka-persistence-jdbc plugin and by default, Lagom is able to create these automatically for you.This automatic generation functionality can be disabled which you will probably want for any environment higher than development.lagom.persistence.jdbc.create-tables.auto = falseThe definition of the tables differ for each database, here is an example of the table definition for PostgreSQL:DROP TABLE IF EXISTS public.journal;CREATE TABLE IF NOT EXISTS public.journal (  ordering BIGSERIAL,  persistence_id VARCHAR(255) NOT NULL,  sequence_number BIGINT NOT NULL,  deleted BOOLEAN DEFAULT FALSE,  tags VARCHAR(255) DEFAULT NULL,  message BYTEA NOT NULL,  PRIMARY KEY(persistence_id, sequence_number));DROP TABLE IF EXISTS public.snapshot;CREATE TABLE IF NOT EXISTS public.snapshot (  persistence_id VARCHAR(255) NOT NULL,  sequence_number BIGINT NOT NULL,  created BIGINT NOT NULL,  snapshot BYTEA NOT NULL,  PRIMARY KEY(persistence_id, sequence_number));The definition scripts for each database are available here.If you are unaware of CQRS, Event Sourcing and the Persistent Read-Side in Lagom you could take a look at the CQRS and Event Sourcing section in our previous blogpost on Lagom.It might be a bit out of date regarding the API in Lagom but it should give you an idea.The new API for Cassandra read-side support, while very similar compared to the existing API, still differs in a few places.In the upcoming section we describe the migration process of upgrading to Lagom 1.2 and the changes we had to do to our read-side, so the required code changes can be read in that section.The API for the JDBC read-side support is rather similar compared to the Cassandra read-side support.Instead of using a CassandraSession for querying, you use a JdbcSession to retrieve a connection which in turn you will use for the execution of queries.The JDBC read-side API however, unlike the Cassandra read-side API, is not fully non-blocking which will lead to a performance difference.It could be an option to switch to Cassandra later on in the project if you want to get better performance out of it.But at least by having an API right now for these four databases it might be easier to integrate a new Lagom application within an existing architecture having these kinds of databases.Migrating from 1.0 to 1.2A migration guide is available with the steps necessary to upgrade your project to Lagom 1.2.At Ordina Belgium we streamed and recorded an introduction video on Lagom 1.0 in which we demoed a shop application.For the purpose of this blogpost, let us see how much work it is to upgrade to 1.2.First of all, we have to upgrade the Lagom version itself. Lagom 1.0 only supported sbt, so our demo is still using that.We change the version to be used in project/plugins.sbt:addSbtPlugin(\"com.lightbend.lagom\" % \"lagom-sbt-plugin\" % \"1.2.2\")For the Lagom Persistence module, Cassandra support has been pulled into its own module so you need to update the lagomJavadslPersistence dependency to lagomJavadslPersistenceCassandra in the build.sbt file.We also have to update the Scala version in build.sbt:scalaVersion in ThisBuild := \"2.11.8\"The ConductR version in project/plugins.sbt:addSbtPlugin(\"com.lightbend.conductr\" % \"sbt-conductr\" % \"2.1.16\")The descriptor() in the service interfaces needs to be updated since the .with(...) has been replaced by withCalls(...).We replace the existing code:@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").with(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).withAutoAcl(true);}With the following:@Overridedefault Descriptor descriptor() {    return Service.named(\"itemservice\").withCalls(            Service.restCall(Method.GET,  \"/api/items/:id\", this::getItem),            Service.restCall(Method.GET,  \"/api/items\", this::getAllItems),            Service.restCall(Method.POST, \"/api/items\", this::createItem)    ).withAutoAcl(true);}These are the necessary changes for us to successfully compile our project. Subsequently, we need to update our read-sides to make use of the new API.Starting with replacing the deprecated CassandraReadSideProcessor:public class ItemEventProcessor extends CassandraReadSideProcessor&lt;ItemEvent&gt; {With ReadSideProcessor:public class ItemEventProcessor extends ReadSideProcessor&lt;ItemEvent&gt; {Next step is to inject an instance of CassandraSession and CassandraReadSide via the constructor:private final CassandraSession session;private final CassandraReadSide readSide;@Injectpublic ItemEventProcessor(CassandraSession session, CassandraReadSide readSide) {    this.session = session;    this.readSide = readSide;}All code related to handling offsets can be deleted since Lagom now handles this for us.We delete the following:private PreparedStatement writeOffset = null; // initialized in prepareprivate void setWriteOffset(PreparedStatement writeOffset) {    this.writeOffset = writeOffset;}private CompletionStage&lt;Done&gt; prepareWriteOffset(CassandraSession session) {    logger.info(\"Inserting into read-side table item_offset...\");    return session.prepare(\"INSERT INTO item_offset (partition, offset) VALUES (1, ?)\").thenApply(ps -&gt; {        setWriteOffset(ps);        return Done.getInstance();    });}private CompletionStage&lt;Optional&lt;UUID&gt;&gt; selectOffset(CassandraSession session) {    logger.info(\"Looking up item_offset\");    return session.selectOne(\"SELECT offset FROM item_offset\")            .thenApply(                    optionalRow -&gt; optionalRow.map(r -&gt; r.getUUID(\"offset\")));}After having our ItemEventProcessor class extend from the ReadSideProcessor abstract we are prompted to implement two methods: buildHandler() and aggregateTags().aggregateTags() simply replaces aggregateTag() where as buildHandler() will contain the setup needed for our ReadSideHandler.The prepare(CassandraSession session) and defineEventHandlers(EventHandlersBuilder builder) methods that used to be overridden are now both implemented in buildHandler().The logic from the existing prepare() is split up into a setGlobalPrepare(), for creating Cassandra tables (note that these tasks should be idempotent), and a prepare(), for preparing statements, in buildHandler().We start by deleting the old prepare(CassandraSession session) and the defineEventHandlers(EventHandlersBuilder builder):@Overridepublic CompletionStage&lt;Optional&lt;UUID&gt;&gt; prepare(CassandraSession session) {    return            prepareCreateTables(session).thenCompose(a -&gt;                    prepareWriteOrder(session).thenCompose(b -&gt;                            prepareWriteOffset(session).thenCompose(c -&gt;                                    selectOffset(session))));}@Overridepublic EventHandlers defineEventHandlers(EventHandlersBuilder builder) {    logger.info(\"Setting up read-side event handlers...\");    builder.setEventHandler(ItemCreated.class, this::processItemCreated);    return builder.build();}Finally we perform the necessary refactoring to implement both buildHandler() and aggregateTags(), and we clean up the existing processing logic for the read-side:private CompletionStage&lt;List&lt;BoundStatement&gt;&gt; processItemCreated(ItemCreated event) {    BoundStatement bindWriteItem = writeItem.bind();    bindWriteItem.setUUID(\"itemId\", event.getItem().getId());    bindWriteItem.setString(\"name\", event.getItem().getName());    bindWriteItem.setDecimal(\"price\", event.getItem().getPrice());    logger.info(\"Persisted Item {}\", event.getItem());    return CassandraReadSide.completedStatements(Arrays.asList(bindWriteItem));}@Overridepublic ReadSideHandler&lt;ItemEvent&gt; buildHandler() {    CassandraReadSide.ReadSideHandlerBuilder&lt;ItemEvent&gt; builder = readSide.builder(\"item_offset\");    builder.setGlobalPrepare(() -&gt; prepareCreateTables(session));    builder.setPrepare(tag -&gt; prepareWriteItem(session));    logger.info(\"Setting up read-side event handlers...\");    builder.setEventHandler(ItemCreated.class, this::processItemCreated);    return builder.build();}@Overridepublic PSequence&lt;AggregateEventTag&lt;ItemEvent&gt;&gt; aggregateTags() {    return TreePVector.singleton(ItemEventTag.INSTANCE);}This leaves us with some refactoring to be done in our service implementations for registering the read-side.The sole thing that needs to be done is replacing the injected CassandraReadSide, which is now deprecated, with ReadSide.So going from:@Injectpublic ItemServiceImpl(PersistentEntityRegistry persistentEntities, CassandraReadSide readSide,                        CassandraSession db) {    this.persistentEntities = persistentEntities;    this.db = db;    persistentEntities.register(ItemEntity.class);    readSide.register(ItemEventProcessor.class);}To:@Injectpublic ItemServiceImpl(PersistentEntityRegistry persistentEntities, ReadSide readSide,                       CassandraSession db) {    this.persistentEntities = persistentEntities;    this.db = db;    persistentEntities.register(ItemEntity.class);    readSide.register(ItemEventProcessor.class);}This concludes migrating our read-side logic to the new API.Since Lagom now supports multiple persistence backends, and not only just Cassandra, TestKit no longer starts with Cassandra enabled by default.This requires us to add a single line .withCassandra(true) to our server setup:@BeforeClasspublic static void setUp() {    server = ServiceTest.startServer(ServiceTest.defaultSetup()            .withCassandra(true)            .withConfigureBuilder);}All in all, migrating the code base to Lagom 1.2 took about fifteen minutes.Looking at the rest of our initial feedbackIn the previous blogpost on Lagom 1.0 we, like many other Java developers, made the point that only offering sbt as the build tool would repel many potential Java developers.We are glad that they addressed this quickly in the first new major version (1.1) they released.Regarding using Lagom in production without using ConductR, the bare minimum for this was to write your own service locator.Jonas Bonér started a service locator project for ZooKeeper and Consul for this purpose.Lightbend also plans to offer a free limited use evaluation license which will allow developers to start working with the Reactive Platform’s commercial features from the beginning.Including not only ConductR but other goodies such as monitoring for Lagom circuit breakers and Akka actors.In the blogpost we also wrote down several impressions compared to Pivotal’s Spring Cloud and Netflix OSS, currently the most popular choice of doing microservice architectures in Java.A strong point for Lightbend is that they want to distinct themselves from Pivotal by providing extensive commercial support if you get the Reactive Platform license.Pivotal also offers commercial support but only if you buy the commercial Pivotal Cloud Foundry.A key advantage of Lagom remains that it is non-blocking down to the core, starting from the persistence layer up to the endpoints, while Spring isn’t just yet.In the upcoming Spring Framework 5, of which a milestone version (M3) is already available, Pivotal are integrating their Spring Reactive initiative providing core reactive functionality and reactive web endpoint support.On the topic of Spring Reactor, a colleague of ours, Tom Van den Bulck, recently wrote a blogpost on Reactive Programming with Spring Reactor.In the blogpost, Tom writes about the presentation of Stephane Maldini at Ordina’s JOIN 2016 event, a small one-day conference hosted by Ordina, on reactive programming which has also been recorded and is available on YouTube. So it will be interested to see whether Spring Framework 5 allows them to catch up on being fully non-blocking and reactive, which up until now, remains a stronger point of Lagom.Lagom’s CQRS and Event Sourcing integration remain another advantage of Lagom, the out-of-the-box integration is easy to work with and they continue to improve on it, now with JDBC support and soon JPA support.In a Spring Cloud application a common solution for this is making use of the Axon Framework although it requires you to do the necessary gluing yourself.There is also the Eventuate framework written by Chris Richardson.Polyglot support is something that still lacks a bit on the side of Lagom.Spring has Sidecar for this purpose.Lagom services map down to ordinary, standard HTTP and WebSockets and as for other frameworks calling Lagom services, an integration client exists for JVM frameworks while others will have to invoke the Lagom services directly via REST.As for Lagom consuming other REST services, you define an API Service and implement the descriptor() to describe the external API. Documentation for this is currently lacking but an example with Httpbin and Slack’s Messages API exists.In order to further address the polyglot support, Lightbend is planning on implementing a solution for this probably based upon Akka Streams and Alpakka.The idea is that you should be able to generate a Lagom service interface from specifications, allowing transparent integration.Having binary coupling was another remark of ours and to address this, support for Swagger will be added in version 1.4.ConclusionWhile there are still a couple of important things in need of being addressed we believe that we can conclude that Lightbend is carefully listening to the feedback given by the community.They continue to improve Lagom with new features and to offer better user experience for the developers.At the same time, Pivotal is working on providing better reactive support with their upcoming Spring Framework 5.Lightbend and Pivotal make some nice rivals to each other which is nice since this will have a positive impact on both frameworks.Pivotal clearly still has an advantage over Lightbend due to how mature and well-known Spring is although Lagom seems to develop nicely and continues to improve on its strongest points while also trying to address weak points.We think that Lagom is worth adopting if you plan on getting a Reactive Platform license.Now that Maven support is available, a big hurdle for Java developers has disappeared.Without the Reactive Platform, development should be fine but you will probably miss the useful goodies it has to offer for running your system into production such as monitoring and service orchestration which you get all for free if you go with Spring Cloud and Netflix OSS.We are sure that using Lagom without the Reactive Platform will become more interesting given enough time for the community to come up with solutions for this.Scala developers will also be eagerly awaiting the release of 1.3 after which they can finally set their teeth into Lagom with the Scala API that it introduces.Lagom 1.3 previewThe first issue, created after Lagom was released, was the need to implement a separate Scala API.Something many users of Lightbend’s technologies were craving for.Until now it was already possible to use Scala with Lagom by using the Java API, see the following seed created by Mirco Dotta.Lightbend made work of it and at the time of writing, a release candidate for 1.3.0 has been made available which finally includes the Scala API for Lagom!Other features to be expected in 1.3.0 include JPA support and new test APIs for testing message broker integration.Extra resources  Lagom: First Impressions and Initial Comparison to Spring Cloud  Lagom in Practice by Yannick De Turck  Lagom documentation  Lagom Twitter  Lagom Gitter  Lagom mailing list  Online Auction: Lagom 1.2 example"
      },
    
      "iot-2017-01-21-node-with-typescript-html": {
        "title": "Node with TypeScript",
        "url": "/iot/2017/01/21/Node-with-TypeScript.html",
        "image": "/img/node-with-typescript/node-ts.jpg",
        "date": "21 Jan 2017",
        "category": "post, blog post, blog",
        "content": "  NodeJS is a fantastic runtime to quickly and easily make projects.However as these projects tend to grow larger and larger, the shortcomings of JavaScript become more and more visible.This blog post will take a look at using TypeScript to write your Node application making it much more readable, introducing more OO like concepts whilst also making your code less error prone.NodeJS and its use cases  NodeJS has many use cases.It is an easy to pickup and use runtime.It uses Google’s V8 JavaScript engine to interpret and run JavaScript code.The user does not have to worry about threading.This is taken care off by the runtime.You write your code and make use of the many asynchronous operations provided by Node.This will take care of any multithreading for you.However, as you will read later in this blog post, making use of multiple Node instances to divide work is still possible!More on that later!Node can be used for a variety of tasks:  Small yet efficient web server  Code playground, test something quickly  Automation and tooling, instead of using ruby/python/…  IoT, Raspberry pi’s and other devices that can run Node!However, you should not use node for computationally heavy tasks!While the V8 engine is highly performant, there are other much more performant options available for computationally heavy operations!This blog post is not meant for people who have no NodeJS experience!Below are some resources for those that are new to the platform:  The main NodeJS website  The Node Package Manager  Code school intro to NodeJSThe old way, using plain JavaScript  Since NodeJS uses Google’s V8 JavaScript engine, it speaks for itself that node interprets and runs regular JavaScript code.This has some pros and cons.While it is an easy language to pick up, it can be hard to master.Javascript has always had some quirks and getting to know and how to avoid these can be tricky!It also does not require any compilation, which makes running your code very easy.However, this also removes any help from the compiler as no compile time checks are performed.No type checking, no checking for illogical structures or things that will just not work.Code for Node can be run by simple opening a command prompt or terminal window and typing    node    This will start a Node instance and present you with an interpreter.You can now type commands and press return to execute them.This can be handy to test something quickly.It is also possible to run a JavaScript file directly.This can be done via:    node path/to/javascript-file.js    However, most of the time you will not be using this way of running code.Most of the time you will use npm to install your dependencies and start the node instance:    npm install    npm start    This reads the package.json file and executes the scripts contained inside it.  Extensive documentation about the package.json file can be found on the NPM websiteTypeScript you say!?  TypeScript has been around for some years now.TypeScript is a superset of JavaScript.It uses the same syntax but adds among other things compile time type checking.It also adds a more Object Oriented model.A detailed explanation of the differences of the prototype based JavaScript and a more Object Oriented language can be found on theMozilla Developer websiteTypeScript developed mainly by Microsoft and is completely open source!This means developers can make suggestions and report bugs (and even fix these bugs if they want).  TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. Any browser. Any host. Any OS. Open source.TypeScript is very well documented and getting started with the language is fairly easy.A lot of common development tools have support for TypeScript syntax checking.These include, but are not limited to:  Intellij  Webstorm  Atom  Visual Studio Code  …As Node applications regularly use other NPM dependencies it is required for the TypeScript compiler to know about these dependencies and what types they use.You could make or generate these typings yourself.However, you can easily find these typings on TypeSearch website.The most commonly used dependencies have their typings available here!You can add the typings to the dependencies in the package.json file.    \"dependencies\": {        \"typescript\": \"2.0.8\",        \"@types/node\": \"0.0.2\",        \"@types/mime\": \"0.0.29\",        \"@types/johnny-five\": \"0.0.30\",        \"@types/serialport\": \"4.0.6\",        \"mime\": \"1.3.4\",        \"johnny-five\": \"0.10.6\",        \"serialport\": \"4.0.7\"    }    Making it all work: An exampleA few years back I started working on my own server application to host some web content and provide REST services.The code was written in JavaScript and ran on a Raspberry Pi 2 (by now a pi 3).For those of you that are interested the old code can be found on the following Github repositories:  WeatherGenie This was the initial implementation, a simple weather web application for checking the weather conditions for any city in Belgium  LoRa-IoT-Demo The second, extended iteration, based on the code from the WeatherGenie application.Because with the advent of IoT we needed a simple to extend/run/maintain solution to create IoT demos for clients.  NodeSimpleServer The third and current iteration. Written from the ground up in TypeScript and completely reworked to work better and be more maintainable.This is the application that will be detailed below!Node Simple Server: High level architecture  The Application starts in app.ts under the main src folder.This is the entry point for the application.This file contains the actual master instance code.The master instance is in charge of forking the workers and reviving them if they die.The master is also used to pass messages between the workers For this a specialized MessageHandler singleton is used.This MessageHandler instance (one per worker) is used to relay messages.The master instance itself will not execute any application logic.Its purpose is to manage the other workers and be the message bridge.    /**     * Forks the workers, there will always be one DataBroker and one IntervalWorker.     * HTTPWorker will be created based on the number of cpu cores. If less than two cores are available     * two http workers will be created.     */    private forkWorkers = (): void =&gt;{        //Fork data broker.        this.databroker = cluster.fork({name: 'broker', debug: this.isDebug});        //Fork interval worker.        this.intervalWorker = cluster.fork({name: 'interval', debug: this.isDebug});        //Fork normal server worker instances. These will handle all HTTP requests.        let cores:number                = os.cpus().length;        let numberOfHttpWorkers:number  = cores - 2 &gt; 0 ? cores - 2 : 1;        console.log('There are ' + cores + ' cores available, starting ' + numberOfHttpWorkers + ' HTTP workers...');        for (let i:number = 0; i &lt; numberOfHttpWorkers; i++) {            let worker = cluster.fork({name: 'http', debug: this.isDebug});            this.httpWorkers.push(worker);        }        //Revive workers if they die!        if(!this.isDebug) {            cluster.on('exit', this.reviveWorker);        }    };    The master will create a number of workers:  HttpWorker: Each HttpWorker is an endpoint for requests to be received.There will always be a minimum of two HttpWorkers created.If more CPU cores are available, more HttpWorkers are created.  DataBroker: For the application there is one DataBroker worker instance.This worker handles CRUD operations for data (for now in memory only).  IntervalWorker: For the application there is one IntervalWorker instance.This worker can run code periodically and is used to connect to other devices such as Arduino’s and the Raspberry Pi I/O pins.These workers are created by a WorkerFactory, as the master forks new Node instances, a process variable is set, the factory uses this to see which type the node instance should become.Each type of worker instance implements the basic NodeWorker interface.Each implementation will be detailed below.Handling HTTP requests: The HttpWorkerEach HttpWorker instance will create a Server instance.This instance will be used to receive HTTP requests.Node will automatically load balance requests between all instances that register a server on the same port.Simply put all HttpWorkers compete for the next request, the least burdened process (depending on OS/CPU process affinity) will be given the next Http request to handle.The Server class will also register the endpoints that are known to the application and can be handled.The EndpointManager is used to register endpoints.An EndPoint has a path, a method to execute and optional parameters.A Parameter is provided with a Generic type for compile time type checking, a name which should be used in the url, a description that provides information what the parameter should contain and an optional ParameterValidator.A ParameterValidator is used to validate the Parameter at runtime.If the check fails an error is shown to the user.    /**     * Maps the default endpoints.     * Endpoints can always be added at any other location and point in time.     * This can be done by getting the instance of the EndPointManager and calling the registerEndpoint method.     */    private mapRestEndpoints = (): void =&gt; {        this.endpointManager.registerEndpoint(            new EndPoint(                '/',                GenericEndpoints.index,                null            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/endpoints',                GenericEndpoints.listEndpoints,                null            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/helloworld',                GenericEndpoints.helloworld,                [new Parameter&lt;string, null, null&gt;('name', 'string field containing the name', new HelloWorldValidatorImpl())]            )        );        this.endpointManager.registerEndpoint(            new EndPoint(                '/arduino/setArduinoMethod',                ArduinoEndpoint.setArduinoMethod,                [new Parameter&lt;string, null, null&gt;('method', 'string field that contains the method used for adruino implementations', new ArduinoMethodValidatorImpl())]            )        );    };    The Server instance forwards all requests to the Router instance.As the name suggests this will perform the routing.It will see if a resource is requested or and endpoint has been called.If a resource is requested it will be served if found.If an endpoint has been called, that endpoint will be executed and passed the parameters that were entered, but only after the correct amount of parameters has been passed and they are all valid.Handling data: The DataBrokerThe DataBroker is the Node instance in the application that will save and retrieve data.For the time being it is sufficient to only have in memory ‘caches’ on which basic CRUD operations can be performed.All methods on the DataBroker are called by sending an IPCRequest with the data that needs to be saved of the instruction for what data should be retrieved.The DataBroker will reply to the original worker by sending an IPCReply with the result of the operation.The DataBroker for now only has a concept of caches.A cache has a name, type and values (of said type).Values can be retrieved, added, updated and deleted from the caches.Caches can be retrieved, added and deleted at runtime.Handling asynchronous tasks: The IntervalWorkerThe IntervalWorker as its name suggest performs tasks at a certain interval.It is also used for other asynchronous workloads, such as connecting to an Arduino and running Arduino/Raspberry pi Johhny-Five scenarios.The IntervalWorker is handy when you need for example to update the content of a cache every so often.It can also run Arduino scenarios.These are Implementations that contain logic to perform actions on the Arduino or in response to something that happens on the Arduino.The IntervalWorker picks up what type of Arduino Scenario you want to run and starts the logic.    /**     * Sets up the connection to the Arduino and starts the desired Arduino Scenario.     */    private setupArduino = (): void =&gt; {        if(this.config.arduino.enableArduino) {            if(this.config.arduino.useSerialOverJohnnyFive) {                this.arduino = new ArduinoSerial(                    this.config.arduino.serialPortName,                    this.config.arduino.serialPortBaudRate,                    new PingScenario()                );            } else {                this.arduino = new ArduinoJohnny(new BlinkScenario());            }            this.arduino.init();        } else {            console.log('Skipping arduino setup, disabled in settings!');        }    };    There are two Arduino implementations available.Both can execute a Scenario.The first and simplest implementation is the Johnny-Five Arduino implementation.This allows you to make use of the Johnny-Five framework to write dynamic code for the Arduino that can change at runtime.This is possible because it uses the StandardFirmata firmware.Johnny-Five supports a lot of components and peripherals.Their website has extensive documentation and very clear examples.Johnny-Five also supports the Raspberry PI I/O pins.This allows it to be used on a Raspberry pi also.The second Arduino implementation uses no framework and communication is done via regular serial.In the type of scenarios you have to handle all the serial communication yourself.You also have to write Arduino firmware and thus it cannot be dynamically updated at runtime.Use this Arduino implementation if some component is incompatible or not supported by Johnny-Five.Inter Process Messaging: Communicating between different Node instancesHaving all these different worker instances is quite handy.However they are of not much use if there cannot be any communication between them.Each Node instance has its own allocated memory and cannot access variables or call methods on other instances.The Node cluster and process framework provide the option to send messages between Node instances.The IPCMessage instances that are sent exist in two forms.  IPCRequest: This is the initial message that is sent to a target.  IPCReply: This is the response (if any) from the target back to the original caller.This allows for easy two way communication and identification whether the message was a reply to an earlier message.Messages can be sent with or without a callback.The callback is executed when a reply to the original message is received.Because only basic data types can be sent across Node instances the MessageManager instance of the caller stores the callback reference and generates an unique id for said callback.This allows the application to send the callback ID across Node instances and execute it when it arrives back at the caller.    /**         * MessageManager singleton class.         * This class has an array of tuples of string and Function.         * The string field is the callbackId and the Function is the actual callback.         * The message manager is a per worker instance that can only execute callbacks on the same worker.         * The integration with the IPC framework allows messages to be sent to other workers and replies to be sent back to the original worker.         * It is important that the original worker is called to execute the callback since a function cannot cross a node instance!         *         * This singleton can be used to manage IPC messages.         */        export class MessageManager {            private static instance: MessageManager         = null;            private callbacks: Array&lt;[string, Function]&gt;    = null;            private workerId: string                        = null;            /**             * Private constructor for the singleton.             */            private constructor() {                this.callbacks = [];                this.workerId = cluster.worker.id;            }            /**             * Use this method to get the instance of this singleton class.             *             * @returns {MessageManager} The instance of this singleton class.             */            public static getInstance(): MessageManager {                if(!MessageManager.instance) {                    MessageManager.instance = new MessageManager();                }                return MessageManager.instance;            }            /**             * Sends an IPCMessage of the subtype IPCRequest to the given MessageTarget (one of the three worker types).             * A target function is also given and contains the name of the function that will be executed on the target.             * The target should implement a specific handler or switch statement to handle these different target function names.             * This message is sent without a callback. This means that when the target function has finished no reply will be sent to inform the caller.             *             * @param payload The payload for the target, can be of any kind.             * @param messageTarget The MessageTarget, being one of the three types of workers.             * @param targetFunctionName The name of the function to be executed on the target. This value is NOT evaluated by eval for security reasons.             */            public sendMessage(payload: any, messageTarget: MessageTarget, targetFunctionName: string): void {                let message: IPCMessage = new IPCRequest(this.workerId, null, payload, messageTarget, targetFunctionName);                process.send(message);            }            /**             * Sends an IPCMessage of the subtype IPCRequest to the given MessageTarget (one of the three worker types).             * A target function is also given and contains the name of the function that will be executed on the target.             * The target should implement a specific handler or switch statement to handle these different target function names.             * This message is sent with a callback. The callee sends a new IPCMessage of the subtype IPCReply to inform the caller and provide it with new information if needed.             * A reply can be sent by using the sendReply method on this class.             *             * @param payload The payload for the target, can be of any kind.             * @param callback The function that should be called when a reply has been received.             * @param messageTarget The MessageTarget, being one of the three types of workers.             * @param targetFunctionName The name of the function to be executed on the target. This value is NOT evaluated by eval for security reasons.             */            public sendMessageWithCallback(payload: any, callback: Function, messageTarget: MessageTarget, targetFunctionName: string): void {                let callbackId: string = process.hrtime()  + \"--\" + (Math.random() * 6);                this.callbacks.push([callbackId, callback]);                let message: IPCMessage = new IPCRequest(this.workerId, callbackId, payload, messageTarget, targetFunctionName);                process.send(message);            }            /**             * Sends and IPCMessage of the subtype IPCReply to the sender of the original message.             *             * @param payload A new payload to provide to the original sender.             * @param originalMessage The message the sender originally sent.             */            public sendReply(payload: any, originalMessage: IPCRequest): void {                let reply: IPCMessage = new IPCReply(this.workerId, payload, originalMessage);                process.send(reply);            }            /**             * For a given callbackId execute the callback function.             *             * @param callbackId The callbackId for which to execute the callback function.             */            public executeCallbackForId(callbackId: string) :void {                for (let callbackEntry of this.callbacks) {                    if(callbackEntry[0] == callbackId) {                        callbackEntry[1]();                        return;                    }                }            }        }    &lt;br/&gt; &lt;br/&gt;    /**     * MessageHandler singleton class.     *     * This singleton can be used to handle IPC messages.     */    export class MessageHandler {        private static instance: MessageHandler         = null;        private dataBroker : cluster.Worker             = null;        private intervalWorker : cluster.Worker         = null;        private httpWorkers : Array&lt;cluster.Worker&gt;     = null;        public emitter: EventEmitter                    = null;        /**         * Private constructor for the singleton.         */        private constructor() {        }        /**         * Use this method to get the instance of this singleton class.         *         * @returns {MessageHandler} The instance of this singleton class.         */        public static getInstance(): MessageHandler {            if(!MessageHandler.instance) {                MessageHandler.instance = new MessageHandler();            }            return MessageHandler.instance;        }        /**         * Initialises the MessageHandler for being a handler for the master NodeJS process.         *         * @param dataBroker The DataBroker worker instance.         * @param intervalWorker The IntervalWorker worker instance.         * @param httpWorkers The HTTPWorker worker instance.         */        public initForMaster = (dataBroker: cluster.Worker, intervalWorker: cluster.Worker, httpWorkers: Array&lt;cluster.Worker&gt;): void =&gt; {            this.dataBroker     = dataBroker;            this.intervalWorker = intervalWorker;            this.httpWorkers    = httpWorkers;            this.emitter        = new EventEmitter();        };        /**         * Initialises the MessageHandler for being a handler for a slave (worker) NodeJS process.         */        public initForSlave = (): void =&gt; {            this.emitter        = new EventEmitter();        };        /*-----------------------------------------------------------------------------         ------------------------------------------------------------------------------         --                         MASTER MESSAGE HANDLING                          --         ------------------------------------------------------------------------------         ----------------------------------------------------------------------------*/        //TODO: Separate master and slave message handling?        /**         * Handler function for messages sent by HTTPWorkers.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by an HTTPWorker.         */        public onServerWorkerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from server worker');            this.targetHandler(msg);        };        /**         * Handler function for the messages sent by the IntervalWorker.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by the IntervalWorker.         */        public onIntervalWorkerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from interval worker');            this.targetHandler(msg);        };        /**         * Handler function for the messages sent by the DataBroker.         * Forwards the message to the target.         *         * @param msg The IPCMessage as sent by the DataBroker.         */        public onDataBrokerMessageReceived = (msg: IPCMessage): void =&gt; {            console.log('Message received from data broker');            cluster.workers[msg.workerId].send(msg);        };        /**         * This method is used to direct the IPCMessage to the correct target as specified in the message.         * This handler makes a distinction between messages of the types IPCRequest and IPCReply.         *         * @param msg The IPCMessage that is to be forwarded to the correct target.         */        private targetHandler = (msg: IPCMessage) =&gt; {            if(msg.type == IPCMessage.TYPE_REQUEST) {                let m: IPCRequest = &lt;IPCRequest&gt; msg;                console.log('Master received request');                switch (m.target){                    case MessageTarget.DATA_BROKER:                        this.dataBroker.send(msg);                        break;                    case MessageTarget.INTERVAL_WORKER:                        this.intervalWorker.send(msg);                        break;                    case MessageTarget.HTTP_WORKER:                        let index: number = Math.round(Math.random() * this.httpWorkers.length) - 1;                        index = index === -1 ? 0 : index;                        this.httpWorkers[index].send(msg);                        break;                    default:                        console.error('Cannot find message target: ' + m.target);                }            } else if(msg.type == IPCMessage.TYPE_REPLY) {                let m: IPCReply = &lt;IPCReply&gt;msg;                console.log('Master received reply!');                cluster.workers[m.originalMessage.workerId].send(msg);            }        };        /*-----------------------------------------------------------------------------         ------------------------------------------------------------------------------         --                          SLAVE MESSAGE HANDLING                          --         ------------------------------------------------------------------------------         ----------------------------------------------------------------------------*/        /**         * Handler function for the messages sent by the Master NodeJS process.         * This handler makes a distinction between messages of the types IPCRequest and IPCReply.         *         * @param msg The IPCMessage as passed on by the master process.         */        public onMessageFromMasterReceived = (msg: IPCMessage): void =&gt; {            if(msg.type == IPCMessage.TYPE_REQUEST) {                let m: IPCRequest = &lt;IPCRequest&gt;msg;                console.log('[id:' + cluster.worker.id  + '] Received request from master: routing to: ' + MessageTarget[m.target] + '.' + m.targetFunction);                this.emitter.emit(MessageTarget[m.target] + '', m);            } else if(msg.type == IPCMessage.TYPE_REPLY) {                let m: IPCReply = &lt;IPCReply&gt;msg;                console.log('Slave received reply!');                MessageManager.getInstance().executeCallbackForId(m.originalMessage.callbackId);            }        };    }    Every worker has an instance of the MessageHandler, it in its turn has an event emitter on which events from the messages are broadcast.The actual worker implementations register themselves on the emitter to receive said events.In a future version the message handling should be split up, because now a single file (with an instance on each Node instance) handles both master and slave messages.Final wordsIn conclusion; It is perfectly possible to make a more complex application for NodeJS with TypeScript.By using TypeScript you gain compile time type checking and a more robust and better readable codebase.Fewer errors and strange bugs are encountered because TypeScript ‘forces’ you to write better code.The Node Simple Server application was a great way to learn the ‘new’ TypeScript language.The project is not finished, as some parts could use some more work, but it should stand as a solid starting point.Feel free to fork the codebase, submit issues or start some discussion."
      },
    
      "conference-2017-01-17-oredev2016-html": {
        "title": "Øredev 2016",
        "url": "/conference/2017/01/17/Oredev2016.html",
        "image": "/img/oredev/oredev-logo.png",
        "date": "17 Jan 2017",
        "category": "post, blog post, blog",
        "content": "When arrived in Malmö city, we were welcomed with an exploration tour of the third biggest city in Sweden.The next day Øredev 2016 officially started!It was noticeable that there was a healthy combination for everyone: Microservices, Docker, UX, Management, etc…All these buzzwords were applied in various talks.This blog post will be about the talks I favoured and the extra knowledge I gained which I now get to apply in my nowadays projects.The Overview Wizard  Dockerizing Microservices  Secure your Docker  Kubernetes  Best Practices &amp; Traps to avoid in UX  Linked data API: The future of HTTP API’s?Dockerizing MicroservicesThe Slideless Microservices session - Adam BienAdam Bien is an Expert Group member for the Java EE 6 and 7, EJB 3.X, JAX-RS, and JPA 2.X JSRs. He has worked with Java technology since JDK 1.0 and with Servlets/EJB 1.0 and is now an architect and developer for Java SE and Java EE projects. He has authored several books about JavaFX, J2EE, and Java EE, and he is the author of Real World Java EE Patterns—Rethinking Best Practices and Real World Java EE Night Hacks—Dissecting the Business Tier. Adam is also a Java Champion, Top Java Ambassador 2012, and JavaOne 2009, 2011, 2012, 2013 and 2014 Rock Star.In this session Adam guided us in how to dockerize and communicate between microservices.He says that involving the container technology to the microservices ecosystem is considered the future and inevitable.When looking into the communication between dockerized microservices, there are several ways in doing so.Before communicating, we first make the service dockerized with the help of a Dockerfile.The Dockerfile will be the configuration for your image:FROM openjdk:8-jre                     // Which base image am I using?VOLUME /tmp                            // Is where you will send data toADD service.jar app.jar                // Copies the existing jar in to the imageRUN sh -c 'touch /app.jar'             // This will execute any commands in a new layer on top of the current imageEXPOSE 42001                           // Which ports should I open at runtime?ENTRYPOINT [\"java\",\"-jar\", \"/app.jar\"] // This will allow you to configure a container that will run as an executableAfter we created the Dockerfile we will have to setup a Docker network for connecting these containers.Since Adam is doing this manually, I will explain you how to do it easier with Docker Compose.Docker Compose will run all the containers you setup in the configuration.Since we work with the microservices ecosystem, we will need a lot of containers.To build our image using the Dockerfile, we will add a Docker dependency in our pom.xml.First add a property for naming your repository:&lt;docker.image.prefix&gt;testApp&lt;/docker.image.prefix&gt;And add support for docker:&lt;plugin&gt;  &lt;groupId&gt;com.spotify&lt;/groupId&gt;  &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;  &lt;version&gt;0.4.13&lt;/version&gt;  &lt;executions&gt;    &lt;execution&gt;      &lt;phase&gt;package&lt;/phase&gt;        &lt;goals&gt;          &lt;goal&gt;build&lt;/goal&gt;        &lt;/goals&gt;    &lt;/execution&gt;  &lt;/executions&gt;  &lt;configuration&gt;    &lt;imageName&gt;${docker.image.prefix}/${project.artifactId}&lt;/imageName&gt;    &lt;dockerDirectory&gt;${project.basedir}/src/main/docker&lt;/dockerDirectory&gt;    &lt;resources&gt;      &lt;resource&gt;        &lt;targetPath&gt;/&lt;/targetPath&gt;        &lt;directory&gt;${project.build.directory}&lt;/directory&gt;        &lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;      &lt;/resource&gt;    &lt;/resources&gt;  &lt;/configuration&gt;&lt;/plugin&gt;First we make images of our services using the Maven Docker plugin from Spotify.Every time Mavens builds the project, a Docker image is generated.Docker ComposeTo tell the containers that they have to be under the same network, we will use Docker Compose.First we make a docker-compose.yml file and configure our microservices:version: '2'services:  cars:    image: testApp/cars-service    ports:      - \"8081\"    networks:      - backend  gateway:      image: testApp/zuul-service      ports:        - \"9900:9900\"      networks:        - backendnetworks:  backend:Variables explanation:  image          The image you will be using to create the container, we will be using our microservice image        ports          The port the service will listen to. When you want to expose your port use: “5432:5432”.        build          This will build the Dockerfile inside the same directory        networks as property          This be the network to which you want to be assigned to        networks          This will create one or multiple networks      When you want to add an Eureka service, follow the above steps and configure it inside the docker-compose.yml file.It is important that every container, you want to communicate with, resides under the same network.Secure systems with Docker - Emil KvarnhammerEmil Kvarnhammer is a Security Software Engineer at TrueSec,a leading edge company in IT security and development.He’s been involved in several security-critical projects,developing applications and components that are used by millions of users on a regular basis.Emil has found severe security vulnerabilities in Apple OS X. His recent focus has been securing systems using Docker and Amazon EC2.When entering the world of cloud and distributed systems, security can be challenging for developers and DevOps.Docker is a part of this world and Emil talks about Docker from a security perspective.When looking at it in a secure way, what are the input sources?Where does the base image come from?If we look closer we can see that it retrieves its images from Docker Hub.In addition to that, more complexity comes in when we add new packages on top of the base image that will be retrieved from the package manager server.Finally we have the application package that can come from a CI server and, if it is a fat jar, it can consist of different dependencies through Maven or some kind.The base image is retrieved from Docker Hub every time a Docker image is being updated.A hacker could hijack the image and squeeze in malicious software without anyone noticing.Same goes for the package manager server.What could an attacker do if he gets control of the channel between the Docker image andthe host operating system?The place where the image gets instantiated or the configuration related to the instantiated image.An attacker could infect the image with his own configuration or modify the orchestration tool.The most important issue is when the attacker gets control of code execution within our application.Emil says that it happens quite often and if it isn’t in your code, it is in the third party libraries or the package manager server you’re using.Now you might think that, seeing as the attacker is in a docker container, that he is unable to break out.Think again, he can break out and gain control of the whole operating system.Building secure images  Choose base image carefully  Strategy for security patches - Regularly update your Docker container so it gets the necessary updates  Continuously rebuild and test - Security patches in your build environment  In a secure build environment  Digitally sign Docker images - # docker build -t test --disabled-content-trust=falseInternal private registry - Push and pull the containers internallyThis private registry is insecure by design.If an attacker gains control of the network where the registry is located, he can easily read and modify each image.An attacker pushes a modified image and the orchestration will pull this in.If the attacker gains control of your CI/CD server he can again push modified images to the registry.To avoid this attack, be sure to have these activated inside your registry.  Secure transfer using TLS  User authentication  Limited access rights  Docker Content Trust which will digitally sign your images  Secured development environmentHow does the attacker inject code?  You use whatever Docker image available in the registry and add your backdoor binary in a Dockerfile,then you build the image and point it to the very same image in the registry.Security measurements inside the container  User namespaces          An extra security measure are user namespaces, this will make the root in the container differentiate from the root in the host.        Remote API          If you have no good reason for using the remote API then you should disable this option.      If you have to use it, be sure to use secure transport over TLS and that you have client authentication so you don’t let the world in.        Try to enable SELinux (enforcing) on the host  Drop unused capabilities that are installed by default and install the capabilities that you actually need.          --cap-drop=ALL      --cap-add=needed_cap         Only pull from private registriesThings you should avoid in Docker  --privileged: breakout is trivial and will give you access to different machines          Be sure to check that Kubernetes is not doing this        --insecure-registry : disable TLS requirement  --net=host: use host network stackKubernetes automation in production - Paul BakkerPaul Bakker is working as a software architect for Luminis Technologies, where he’s currently leading the team that develops the CloudRTI,a Kubernetes/Docker based service that takes care of DevOps tasks like blue/green deployment, failover, centralised logging and monitoring.He is the author of “Building Modular Cloud Apps With OSGi” published by O’Reilly and an advocate for modularity in software.He is currently writing his second book for O’Reilly “Java 9 Modularity” which is expected early 2017.He’s a regular speaker at conferences speaking both about modularity/OSGi related topics, and topics related to Kubernetes and cloud architecture in general.Kubernetes is momentarily the best tool to use for orchestrating containers in a clustered production environment.In this technical deep dive you will learn the architecture and production deployment of Kubernetes.Why Kubernetes?  Run Docker in clusters          Scheduling containers on machines - Load balance and failover containers      Networking - How to communicate between containers      Storage - Use a network attached database      Automation - A commit will result in a new deployment      The ArchitectureMasterThe master component is the most important component of Kubernetes, everything that happens in the cluster is handled by the master.Generally there is one master but you can have a failover setup and work with multiple masters.For now we simply see this as a single component with an API.When a master goes down, the rest keeps running, but during that moment you won’t get any scheduling or things like that.Replication ControllerThe controller is there to configure where and how many nodes the container has to run in.It will manage scheduling and monitoring inside the cluster,so if one container crashes for some reason, the replication controller will look at how many replicas you configured and if one is down, it will schedule a new instance into the cluster.It is important that we don’t start our containers ourselves but configure the amount of replicas that are necessary.After our containers are set up, the cluster will take care of it and monitor it.Worker nodesThis is where your docker containers will run in.PodsAn abstraction on top of containers inside the worker node:  May contain multiple containers  Lifecycle of these containers bound together          Don’t place microservices into the same pod        Containers in a pod see each other on localhost  Env vars for servicesEtcdIs a key-value store and contains the configuration of the cluster.You don’t want to lose your etcd otherwise your cluster will go down.If you want fault tolerance, you will need quite a lot of machines.At least one master and three worker nodes to meet the requirements.NetworkThese pods will have to talk to each other so you will have to open ports.  We run many pods on a single machine  Pods may expose the same portsHow to avoid conflicts?  Each pod gets a virtual IP  Ports are not shared with other pods  This can cause some troubles when your process crashes and obtains a new IP address.So how do they keep communicating? Are they tracking each other? Do we have to use something like a service discovery?It would be complex if you want to go there and at the end, you don’t want to depend on those virtual IP addresses.If you want the pods to communicate, we will be using services as a proxy on top of them.ServicesService are basically a proxy on top of the pod and if you look from a technical perspective, we see configured IP tables.When we create a service, it gets a fixed IP address and a DNS name so we can communicate through our services.Finally it will round-robin traffic to the destined paths and keep track of which paths are running.Multi component deployments (microservices):  Each component deployed as a pod  Individually update and scale pods  Use services for component communication  The Frontend talks to the service of the backend pod and this will round robin the call to the right instances.NamespacesIsolated world in Kubernetes where you can have different environments inside the same cluster.If you want to run a TEST, DEV and PRD environment it’s possible with Kubernetes to keep them isolated from each other.Kubernetes in ProductionHTTP load balancingKubernetes does not have support for the following points in production so be sure to have a look at the following points when deploying to PRD.  Expose Kubernetes services to the outside world  SSL offloading  Gzip  Redirects - redirect every HTTP call to HTTPSNodePort  Exposing service ports as ports on physical nodes  Beware of port conflicts!kubectl expose deployment my-nginx --type=NodePortKubernetes Ingress  Level 7 load balancer configuration  SSL offloading  Support for GCE load balancers  Future support for extensions (not quite there yet)  At the moment useful on Google Cloud EngineUsing a custom load balancer  Use HAProxy in front of Kubernetes  Configure HAProxy dynamically  The same works for NGINX and Apache  HAProxy does SSL offloadingHow does ha-proxy know about our services?  HAProxy uses a static config file  Auto-generate it based on data in etcd  ConfdBlue-green deployment  Deployment without downtime  Only one version is active at a time  Rolls back on failed deployment  When updating our cluster we have two options:We want to update our cluster with a rolling update,which updates one pod at a time, rather than taking downthe entire service at the same time or you can do a blue-green deployment where youmake an entirely new cluster of instances and let the load balancer go from blue to green.A developer’s guide to the UX galaxy - Tess FerrandezTess Ferrandez is a full stack developer on the Microsoft Platform.She is equally happy debugging nasty backend issues as she is developing apps or working on UI/UX design.She has been blogging at her blogsite for the better part of her career, and has spoken at lots and lots of conferences around the world.Software is built into two parts, it is what we see and how we do it.In this session Tess talks about what these features should look like.If you design ‘an order’ incorrectly and the user can’t figure out to place that order, you have a bad UX design and it could cost your company a lot of money.Now, two UX researches at Microsoft deep dived into hundred years of research of signs and symbols and listened to what people define as a good interface.Finally they categorized every piece into ‘Tenets’ (attributes of good user interfaces) and ‘Traps’ (bloopers and things to avoid).Tenets: attributes of good useWe will be using a scanner as example that you use in the supermarket.Understandable  Ask yourself, what can I do? Is it understandable?If you look at the scanner in a supermarket, do you know which button you have to press to register a product?It is important to let your users understand what they are interacting with because if they have to read the manual first or ask someone for help, you’re doing something wrong.Physically effortless  Is what I’m holding quick and comfortable to use?When scanning a product you want your users to feel that they can scan the product easily and without difficulty.Responsive  You get immediate feedback, when scanning a product you get an immediate response (ex: Hey! you scanned something)If you don’t get an immediate response, the user will keep trying to scan the product and at the end he will have ten milk boxes.Efficient  What is the most efficient way in a process?The best way to buy ten milk boxes is to scan the milk ten times than to write it down ten times with your other hand.Forgiving  I can undo my actions.If I scan too many items, I have a minus button to unregister the last item.It’s a bad practice if there is no button and you have to ask an employee to help you.Discrete  Don’t over share, don’t overreact.Having a siren on the scanner to point out that you did something wrong is too much.Protective  Always deliver a qualified product with no failure or data loss.A bad practice for example is that after you just filled in a form, clicked through to the next page and tried to go back to the form to add some missing information only to find out that your form has been completely reset.Habituating  Muscle memory, make it a habit to use.When users go to your website and know where to go to buy a specific product automatically after visiting it a couple of times.Try to avoid changing your structure over and over and if you do so, make sure that you remind your users of the change in advance.Beautiful  I find it attractive compelling.Make the user think that you put a lot of effort in it.Use the same buttons, nicely aligned and use the same colors.Traps: attributes to avoidEvery ‘Tenet’ has its own set of traps, each trap is there to teach us to avoid it as much as possible.Understandable TrapsPerceptibleInvisible ElementThe user has learned about a critical hidden capability but cannot see it.NoticeableEffectively Invisible ElementA critical cue goes unseen because it is outside of the user’s vision or unexpected.DistractionSomething is drawing the user’s attention from what they need to see.Comprehensible TrapsUncomprehended ElementIt looks wrong but it is right (ex. If they have to put a sticker on it, you are doing something wrong)Inviting Dead EndIt looks right but it is wrong (ex. Having a music icon on your IPhone and you expect iTunes, but it is not iTunes)Poor GroupingA link between two or more elements is not recognized (ex. Right icon with the wrong button)Memory ChallengeAn unreasonable demand is made on the user’s memory.Confirmatory TrapsAmbiguous FeedbackThe feedback or response to a user action is unclear or absentPhysically effortless Traps - Targeting and readability  Buttons should be big enough to pressAccidental activationTrying to click one button but clicking another.Physical challengeAlways use your design on the end product.When designing a mobile form, be sure you do it on a mobile phone.Invisible element on phoneAccidental activation if shutdownResponsive Traps  Be sure your response takes half a second at most  If the response takes longer you should display a loading circle or show the progression of the processEfficiency TrapsInformation overloadHaving a page that shows you too much information, be sure to keep it as simple as possible.System amnesiaWhen they show you information that you don’t needAutocorrectThe name says it all, when autocorrect changed your word in something unintendedHabituating Traps  Doing things so many times it gets automatic and then it changes.Non redundant gratuitous redundancyToo much of the same action (ex. if you have one button that does multiple different actions on other systems)InconsistencyMessage button on an older version is on another place or it is replaced with textBeautiful TrapsUnattractive designNot appealing design where the user will not stay on the website.Best Practices  Identify common scenarios using the site  Walk through all the different ways the user can complete a certain task  Identify and log any traps you observe  If you can’t find a trap, identify the tenet  Document and discussFeed the links, tuples’ a bag – An introduction to Linked Data APIs - Sebastien LamblaSebastien Lambla is a keen open-source advocate and contributor, a long-time REST proponentand distributed systems aficionado With a career spanning over 20 years on many platforms,he’s a keen speaker and trainer, and has been known to talk a lot about technical things and unicorns.Started with a calm song of tuples’ a bag from Mary Poppins, Sebastien guides us through a possible future of HTTP API’s.When talking about creating our own protocol you know that it is recommended not to not to reinvent the wheel and use HTTP instead.Someone else did the hard work, so why should you?This is called the network effect.How is HTTP designed?  Model things - Resources (Person, Animal, Fruit)  Interact with things - Operations (POST, GET, PATCH, DELETE)  Understand the result of the interaction - Common Responses (200 - OK , 201 - Created, 400 - Bad Request)  Reuse existing libraries  One interface to be taught for everythingCommon Media Types  Expose the structure of data in a common format  Reuse existing libraries (JSON)Hypermedia Media Types  Have structure and links understood and defined commonly  Typed links - A link that knows where to go next  Reuse existing libraries (HAL)Common LanguageJSON-LD will try to use a common language for everyone to understand.It is a lightweight Linked Data format, easy for humans to read and write.It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale.JSON-LD is an ideal data format for programming environments, REST Web servicesand unstructured databases such as CouchDB and MongoDB.Example:{    \"@context\":    {        \"author\": {            \"@id\": \"http://schema.org/url\",            \"@type\": \"@id\"         }    },    \"author\": \"https://serialsev.com/author/\"}In the above example, author is an uri which is the identifier of a resource.If you follow the uri, you will find a resource that is defined as a data type that everybody understands and everyone has agreed upon.Again, here we use the network effect because the last thing we want is to rebuild stuff.If you are interested in playing with the format, there is a playground where you can try out examples:PlaygroundWhen we open the playground, you can see a couple of examples you can look at.{  \"@context\": \"http://schema.org/\",  \"@type\": \"Person\",  \"name\": \"Jane Doe\",  \"jobTitle\": \"Professor\",  \"telephone\": \"(425) 123-4567\",  \"url\": \"http://www.janedoe.com\"}The context defines the uri where to go and the type that tells us which resource we are retrieving.Finally the exact uri link will be http://schema.org/Person.If you visit the uri, you’ll get this:When finding the Person resource we see that there are a lot of properties already defined and documented by several people so you don’t have to.When looking back at our example you can use the property already defined in the resource and use them to initialise.If you reuse the property defined, you will get the proper documentation for it.Some nice tools  Google Structured Data Testing Tool  Hydra console  PlaygroundConclusionØredev is one of the best conferences I attended, not only did it innovate in different talks, it managed to organize so many topics to attend.Not only did it improve my technical skills, it also taught me management skills and a realistic view of what the world is today.Very well organized, interesting talks, great local dishes from Sweden and beautiful sightseeing in Malmö."
      },
    
      "reactive-2016-12-12-reactive-programming-spring-reactor-html": {
        "title": "Reactive Programming with Spring Reactor",
        "url": "/reactive/2016/12/12/Reactive-Programming-Spring-Reactor.html",
        "image": "/img/reactive/reactor_logo.png",
        "date": "12 Dec 2016",
        "category": "post, blog post, blog",
        "content": "Overview  Stephane Maldini @ JOIN  The new normal that is not new  The Reactive Manifesto  Latency &amp; Blocking  The Contract  Reactive Types  Testing &amp; Debuging  Other Changes  RxJava  Spring Framework 5  Conclusion &amp; Do It YourselfStephane Maldini @ JOIN 2016On 5 October 2016, we had the pleasure to welcome Stephane Maldini at our JOIN event.A multi-tasker eating tech 24/7, Stephane is interested in cloud computing, data science and messaging.Leading the Reactor Project, Stephane Maldini is on a mission to help developers create reactive and efficient architectures on the JVM and beyond.He is also one of the main contributors for Reactive support in the upcoming Spring 5 framework, which can be seen as the new standard for reactive applications in the Java world.  You can rewatch his talk on on our Channel on Youtube.The new normal that is not newIt has been around for 30-40 years and boils down to Event-Driven ProgrammingWhat is new is “reactive motion bound to specification”, this means that reactive programming is based on something solid, a specification and no longer some functional concepts.Namely the Reactive Manifesto.Because of this specification, Spring found it the right time to start with Reactor as they could now build something, which would be able to work and where it was clear what people could expect.The Reactive Manifesto  According to the manifesto, reactive systems are  Responsive: respond in a timely manner if at all possible, responsiveness means that problems can be detected quickly and dealt with accordingly.  Resilient: remain responsive in the event of failure, failures are contained with each component isolating components from each other.  Elastic: stay responsive under varying workload, reactive systems can react to changes in the input rate by increasing or decreasing the resources allocated to services.  Message Driven: rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency.This boundary also provides the means to delegate failures as messages.Systems built as reactive systems are thus more flexible, loosely-coupled and scalable. This makes them easier to develop and to allow changes.They are significantly more tolerant of failure and when failure does occur they meet it with elegance rather than disaster.LatencyLatency is also a real issue, the real physical distance of various components and services becomes more important with cloud based systems.This is also a very random number which is difficult to predict because it can depend on network congestion.With Zipkin, you can measure this latency.The same latency can also exist within an application - between the different threads - although the impact will be less severe than between various components.Something needs to be done when latency becomes too big of an issue, especially if the receiver can not process enough.Too much data will fill up the buffer and can result, with an unbounded queue, to the infamous OutOfMemoryException().While you won’t run out of memory with a circular buffer, you risk losing messages as the oldest ones get overwritten.BlockingOne way to prevent out of memory exceptions is to use blocking.But this can be a real poison pill: when a queue is full, it will block a thread and as more and more queues get blocked your server will die a slow death.Blocking is faster and has better performance, than reactive, but reactive will allow for more concurrency.Concurrency is important if you have a microservice based architecture, as there you typically need to be more careful and more exact when allocating resources between services.As in, by being more concurrent you can save a lot of money when using cloud and microservices.ContractReactive is non-blocking and messages will never overflow the queue, see for the standard definition http://www.reactive-streams.org/.  Created by Pivotal, Typesafe, Netflix, Oracle, Red Hat and others.The scope of Reactive Streams is to find a minimal set of interfaces, methods and protocols that will describe the necessary operations and entities to achieve the goal—asynchronous streams of data with non-blocking back-pressure.With back-pressure, a consumer which can not handle the load of events sends towards it, can communicate this towards the upstream components so these can reduce the load.Without back-pressure the consumer would either fail catastrophically or drop events.  This contract defines to send data 0 .. N.Publisher is an interface with a subscribe() method.Subscriber has 4 callback methods:onSubscribe(), onNext() (which can be called 0 to N times), onComplete() and onError().The last two signals (complete and error) are terminal states, no further signals may occur and the subscriber’s subscription is considered cancelled.What is important is the reverse flow and the back-pressure.After subscribing, the subscriber gets a subscription which is a kind of 1 on 1 relationship between the subscriber and the publisher with 2 methods: request and cancel.  Request: this is the more important one, with this method the subscriber will ask the publisher to send x messages (and not more), a so called pull.  Cancel: the subscription is being cancelled.Spring Reactor focuses on the publisher side of the reactive streaming, as this is the hardest to implement and to get right.It provides you with the tools to implement publishers in a back-pressure way.The publisher is a provider of a potentially unbounded number of sequenced elements, publishing them according to the demand received from its Subscriber(s).The Reactive Streams specification has been adopted for java 9.DIY Reactive StreamsImplementing a Reactive Stream framework yourself is very hard to do, for Stephane Maldini this is the 4th or 5th attempt. For Davik Karnok, the tech lead of RxJava, it is attempt 7 or 8.The main difficulty is to make it side effect free.For example:Publisher&lt;User&gt; rick = userRepository.findUser(\"rick\");Note that a publisher is returned instead of directly returning the entity.By doing so it does not block the subscribers when querying for the user and the publisher will produce the user when ready.But by using the specification as is, your publisher might produce 0, 1 or N users, returning an Iterable as result.This is not really practical to work with, as most of the time we are only interested in a single user and not a stream of multiple results.When you would be building the method findOneUser() you also would not want to return an Iterable but just a single User.Also you will have to implement a subscriber to define the action to perform when the result is available.rick.subscribe(new Subscriber&lt;User&gt;(){...});Implementing this subscriber would not be that hard, because the specification has been made so that all complexity lies at the publishers side.Another issue is that you can only subscribe on the publisher, there are no other methods available like map, flatmap, …The other point is that when designing your own API you will also have to deal with the following issues:  Should work with RS TCK (otherwise it might not work with other libraries as well)  Address reentrance  Address thread safety  Address efficiency  Address state  For Many-To-One flows, implement your own merging operation  For One-To-Many flows, implement your own broadcasting operation  …  This is all very hard to do yourself.3 Years to MatureIt took Spring Reactor 3 years to mature.2.0 was not side effect free - also existential questions were raised around the project. At the same time Spring evolved and microservices became the norm.Spring needs to work nicely with these microservices, concurrency is important, can Reactor not be used for that?With 3.0 the team wanted to focus on microservices, take some ideas from Netflix OSS and implement these in a pragmatic way.Actually Reactor 3 was started as 2.5, but so many new features were added that the version had to be changed as well in order to reflect this.Since 3.0 Spring Reactor has been made more modular and consists of several components:    Core is the main library.Providing a non-blocking Reactive Streams foundation for the JVM both implementing a Reactive Extensions inspired API and efficient message-passing support.  IPC: back-pressure-ready components to encode, decode, send (unicast, multicast or request/response) and serve connections.Here you will find support for Kafka and Netty.  Addons: Bridge to RxJava 1 or 2 Observable, Completable, Flowable, Single, Maybe, Scheduler, and also Swing/SWT Scheduler, Akka Scheduler.  Reactive Streams Commons is the research project between Spring Reactor and RxJava as both teams had a lot of ideas they wanted to implement.Lots of effort was put in order to create real working, side-effect free operations.Map and Filtering for example are easy, but mergings, like Flatmap are hard to implement side-effect free.Having a proper implementation in the research project for these operations allowed the team to experiment and make it quite robust.This project contains Reactive-Streams compliant operators, which in turn are implemented by Spring Reactor and RxJava.Both the Spring and RxJava teams are very happy with this collaboration and this is still continuing.When a bug gets fixed in Spring Reactor it will also be fixed in RxJava and vice versa.Everything in Reactor is just reactive streams implementation - which is used for the reactive story of spring 5.There also exists an implementation for .NET, Reactor Core .NET and one for javascript Reactor Core TypeScript.Reactive TypesFlux vs Observable  Observable is not implementing Reactive Streams Publisher which means that if you would like to use the Spring 5 save(Publisher&lt;T&gt;) you first have to convert the Observable to a Flowable as you can see in Observable and Flowable.This was too much noise for the Spring team, they are less dependant on Android developers so they could go all in with Java 8.Flux is a Reactive Streams Publisher with basic flow operations, where you start from a static method which will describe how the data will be generated, just() is the simplest wayAfter that you have other operators like Flatmap(), Map(), … to work with that dataSome of the method names will be different to RxJava2, but the logic behind these methods has been aligned among RxJava and Spring .Flux.just(\"red\", \"white\", \"blue\")       .flatMap(carRepository::findByColor)       .collect(Result:: new, Result::add)       .doOnNext(Result::stop)       .subscribe(doWithResult);Interface CarRepository {    Flux&lt;Car&gt; findByColor(String color);}This Flux will retrieve all cars which match the color “red” then those with the color “white” and finally “blue”.So instead of just three elements, after this Flatmap we are going to have a lot more elements.This is all handled with back-pressure in mind, for example when the flatmap is busy merging data we will not ask for extra recordsIf the Repository implements Flux as a method signature, it will be picked up automatically as a reactive repository.This support for Flux will be part of the whole of Spring 5.Spring Data, Spring Security, Spring MVC, … are all good candidates who will have this kind of support.Mono  None is like a flux, but will return at most 1 result, so it does have less methods.Mono.delayMillis(3000)    .map(d -&gt; \"Spring 4\")    .or(Mono.delayMillis(2000).map(d -&gt; \"Spring 5\"))    .then(t -&gt; Mono.just(t + \" world\"))    .elapsed()    .subscribe()This Mono will wait for 3 seconds on the “call” to Spring 4 or 2 seconds on that of Spring 5.The fastest result will be the one which will be outputted.The Mono has as advantage over an Observable Future of Java 8 that a Mono will only be triggered if you subscribe to it.While with an Observable the call to send() will execute the operation.TestingBlock() exists for very specific use cases and for testing.Never, ever use this in production, as is it blocks your call, which does infer with the Reactive non-blocking statements. ;-)Mono.delayMillis(3000)    .map(d -&gt; \"Spring 4\")    .or(Mono.delayMillis(2000).map(d -&gt; \"Spring 5\"))    .then(t -&gt; Mono.just(t + \" world\"))    .elapsed()    .block()You can also make use of Stepverifier to test Flux, Mono and any other kind of Reactive Streams Publisher.@Testpublic void expectElementsWithThenComplete() {    expectSkylerJesseComplete(Flux.just(new User(\"swhite\", null, null), new User(\"jpinkman\", null, null)));}Use StepVerifier to check that the flux parameter emits a User with “swhite” username and another one with “jpinkman” then completes successfully.void expectSkylerJesseComplete(Flux&lt;User&gt; flux) {    StepVerifier.create(flux)            .expectNextMatches(user -&gt; user.getUsername().equals(\"swhite\"))            .expectNextMatches(user -&gt; user.getUsername().equals(\"jpinkman\"))            .expectComplete();}DebugWhen you use reactive libraries you will quickly realize that step debugging is hard especially when you try to read your stacktraces, there are a lot of recursive calls taking place.Before you invoke your operations you can enable an, expensive, debug mode.Hooks.onOperator(op -&gt; op.operatorStacktrace());try {    Mono.just(\"a\")        .map(d -&gt; d)        .timestamp()        . ...                } catch (Exception e) {    e.printStacktrace()}When an exception is returned it will contain the exact operation that failed and the backtrace to that operation.You must enable this Hooks.onOperator before the operations you want to track.More cool stuffParallelFluxIf you want to stress test your CPU you can use ParallelFlux which will spread the workload in concurrent tasks when possible.Mono.fromCallable( () -&gt; System.currentTimeMillis() )    .repeat()    .parallel(8) //parallelism    .runOn(Schedulers.parallel())    .doOnNext( d -&gt; System.out.println(\"I'm on thread \"+Thread.currentThread()) ).    .sequential()    .subscribe()    This basically avoids that you have to write flatMap(), where after the parallel(x) you will have exactly x number of Rails or Flux.Afterwards you can merge these back into a Flux with sequential().A nice feature is that it keeps the code more readable with everything on a single indentation level.But the cool part is that it is also very performant, with parallel, Reactor is very close to the bare metal of what the JVM can do as you can see in the below comparisation:        https://twitter.com/akarnokd/status/780135681897197568Bridge Existing Async codeTo bridge a Subscriber or Processor into an outside context that is taking care of producing non concurrently, use Flux.create(), Mono.create(), or FluxProcessor.connectSink().Mono&lt;String&gt; response = Mono.create( sink -&gt; {    HttpListener listener = event -&gt; {        if (event.getResponseCode() &gt;= 400) {            sink.error(new RunTimeException(\"Error\"));        } else {            String result = event.getBody();            if (body.isEmpty()) {                sink.succes();            } else {                sink.success(body);            }        }    };    client.addListener(listener);        emitter.setCancellation(() -&gt; client.removeListener(listener));});This create() allows you to bridge 1 result, which will be returned somewhere in the future, to a Mono.If you add a Kafka call, for example, where they have this callback so one can return onSuccess and onError you can use Mono.create(): see Reactor Kafka where this is used a lot.Also exists for Flux of N items but it’s tougher and more dangerous as you must explicitly indicate what to do in the case of overflow; keep the latest and risk losing some data or keep everything with the risk of unbounded memory use. ¯\\(ツ)/¯Create Gateways to Flux and MonoThere also exist some options to bridge the synchronous world with the Flux and the Mono.Like for example the EmitterProcessor which is a signal processor.  EmitterProcessor&lt;Integer&gt; emitter = EmitterProcessor.create();BlockingSink&lt;Integer&gt; sink = emitter.connectSink();sink.next(1);sink.next(2);emitter.subscribe(System.out::println);sink.next(3); //output : 3sink.finish();But you also have:  ReplayProcessor, a caching broadcaster.  TopicProcessor, an asynchronous signal broadcaster  WorkQueueProcessor, which is similar to the TopicProcessor but distributes the input data signal to the next available Subscriber.These are all an implementation of a RingBuffer backed message-passing Processor implementing publish-subscribe with synchronous drain loops.OptimizationsOperation fusion: Reactor has a mission to limit the overhead in stack and message passing.They distinguish 2 types of optimization:  Macro Fusion: Merge operators in one during assembly time, for example, if the user does .merge() - .merge() - .merge() spring reactor is smart enough to put this in a single .merge()  Micro Fusion: Because of the Reactive specification and the asynchronous nature of the response, queues are heavily used, but creating a queue for every request/response is very costly.Spring Reactor will avoid to create queues whenever possible and short circuit during the lifecycle of the request. They are going to merge the queue from downstream with the one from upstream - hence the name fusion.If the parent is something we can pull (an Iterable or a queue) then Reactor is going to use the parent as a queue, thus avoiding to create a new queue.This is very smart to do - but also very complicated to do yourself, because Spring Reactor has this in place you do not have to deal with this hassle..A Simpler APIReactor: a Simpler API, the entire framework just fits in 1 jar: reactor-core jar.Flux and Mono live in the reactor.com.publisher package, reactor.core.scheduler contains the FIFO task executor.By default the Publisher and Subscriber will use the same thread.With publishOn() the publisher can force the subscriber to use a different thread, while the subscriber can do the same with subscribeOn().For Reactor 3.x there will be more focus on the javadoc, as this has been lagging behind compared to the new features which have been developed.RxJavaWhy Reactor when there’s already RxJava2?RxJava2 is java 6 while for Reactor the Spring team decided to go all in and focus only on Java 8.This means that you can make use of all the new and fancy Java 8 features.If you are going to use Spring 5, Reactor might be the better option.But if you are happy with your RxJava2, there is no direct need to migrate to Reactor.Spring Framework 5It will still be backwards compatible. You can just take your Spring 4 application, put Spring 5 behind it and you will be good to go.But with Spring 5 you will be able to make use of the following new components/ Spring Web Reactive and Reactive HTTP.Which under the hood support Servlet 3.1, Netty and Undertow.The annotations are still very similar but you just return a Mono, so the User can now be retrieved in a non-blocking way.@GetMapping(\"/users/{login}\")public Mono&lt;User&gt; getUser(@PathVariable String login) {    return this.repository.getUser(login);}ConclusionSpring Reactor is a very interesting framework, after 3 iterations it has matured and gives you a good base to get started with Reactive Streams.With the upcoming support in Spring 5 it will also start to become more mainstream.Therefore I can see no better way then to get your hands dirty and learn more about Spring Reactor yourself.  reactive-programming-part-I:Provides you with a clear description of what reactive programming is about and its use cases.But also the different ways about how people have implemented reactive programming (actor model, futures, … ) and more specifially the different frameworks which implement reactive programming in java.          Frameworks like: Spring Reactor, Spring Framework 5, RxJava , Akka, Reactive Streams and Ratpack.        reactive-programming-part-II:You will learn the API by writing some code, how to control the flow of data and its processing.      reactive-programming-part-III:Here you will focus on more concrete use case and write something useful, but also on some low level features which you should learn to treat with respect.        reactor-api-hands-on:This hands-on will help you learn easily the lite Rx API provider by Spring Reactor. You just have to make the unit tests green.    On spring.io you can find more interesting blog posts which will give you more background around Spring Reactor and provide you with the resources to start coding."
      },
    
      "conference-2016-11-18-jax-london-2016-html": {
        "title": "JAX London 2016",
        "url": "/conference/2016/11/18/JAX-London-2016.html",
        "image": "/img/jax-london-2016/jax-london-logo.png",
        "date": "18 Nov 2016",
        "category": "post, blog post, blog",
        "content": "  JAX London is a four-day conference for cutting-edge software engineers and enterprise-level professionals. JAX brings together the world’s leading innovators in the fields of Java, microservices, Continuous Delivery and DevOps.For this year’s slogan they decided on: “Create, Innovate, Code”.Ordina was present at JAX London on the 11th and 12th of October 2016 where one of our colleagues, Bart Blommaerts, also presented a talk on The Serverless Cloud.In this blogpost we want to give the highlights of some of the talks we followed.Day 1Introduction to JAX London - Sebastian MeyenTo start off, Sebastian Meyen, program chair of the JAX conferences, gave an introduction to JAX London.He mentioned that only half of the attendants of JAX originate from the UK, making it an international conference.JAX is all about “openness”, we should be celebrating open source and embrace open source thinking as it makes our code smarter.Java is one of the most powerful ecosystems out there according to him and it comes with a unique culture.Sebastian explained that there are different cultures within a company:  Pioneers: They go into the wild and experiment and although they might fail often, they’re looking at what the next big thing might be  Settlers: They make a valid business model based on the results of the research of the pioneers and make stable technology for it  Town builders: They look at the portfolio of the settlers and decide what to industrialise, they want to create volumeHe stresses that innovation happens on all three levels and not just only at the pioneers or settlers.Furthermore he went briefly over the different genres of the talk and the conference app before introducing James Governor.Opening keynote: Java for Cloud Natives - James GovernorJames Governor opens up his talk by showing how Java is still high on the Programming Language rankings despite the “Java is dead”-doomspeak every now and then.Regarding Java frameworks, Spring and Spring Boot are really crushing it.He also mentions that most startups usually start with a new and fancy language but as they mature, a lot of them actually turn into Java shops.Examples of this being: Uber, LinkedIn, Netflix, Twitter, Amazon, Etsy, Facebook, Yahoo and Google.Twitter for example started in Ruby and during the US presidential election in 2012 they migrated to Scala and Java on the JVM for scalability and performance reasons.After the migration they managed to sustain peaks of TPS (Tweets Per Second) for hours, at one point even reaching 15,107 TPS.James went through a couple of companies and the transformations they underwent for staying competitive.Amazon for example started with a messy code base but they did manage to refactor it.Being a top down company, they also managed to create small teams.Netflix is a similar case, at a certain point they had a messy codebase but they really put a lot of effort into refactoring it.They also invested a lot in their software engineers and continue to do so.Open source is the new normal, there are a lot of cool open source frameworks around that you can contribute to such as Zookeeper, Spark, Kafka, Hadoop, Giraph, Jenkins, Cassandra,…It’s awesome when enterprises contribute to open source.  Bosch: They are doing interesting work with Eclipse Foundation, they know they need to do open source so they make open source contributions  Comcast: Everybody hates Comcast in the US, but they are making open source contributionsJames mentions a couple of things he finds important:  Microservices and container based deployment such as Docker and Kubernetes are very hot topics  Break down the technical model and teams  Continuous integration is very important, there are still people not using it!  Make people responsible for their own Quality Assurance  Embrace failure and graceful degradationFinally, he mentioned that there is always the need to deal with the politics and that governance is still important and needed.Oracle needs to give their commitment to Java and the Cloud and we as a community need to encourage Oracle to step up and not only complain about them.Links:  PresentationDeveloping Microservices with aggregates - Chris RichardsonThe goal of Chris Richardson’s presentation was to show how Domain-Driven Design aggregates and microservices are a perfect match.A microservice based architecture tackles complexity through modularisation.A microservice should be seen as a business capability for example a catalog service, review service or order service.By having service boundaries you enforce modularity.Also important to mention is that each microservice needs to have its own database, microservices do not share a database!Finally there should always be an API gateway in front of the microservices, which is the entrypoint for the frontend user interface and mobiles devices.They should never access your microservice directly!Chris strongly suggest reading Domain Driven Design by Evan Evans.The core building blocks of Domain-Driven Design are the following:  Entities  Value objects  Services  Repositories  AggregatesAbout Domain-Driven Design aggregates:  Cluster of objects that can be treated as a unit  Graph consisting of a root entity and one or more other entities and value objects  Typically business entities are aggregates e.g., Customer, Account, Order, ProductChris talked about the problems you have to handle when dealing with microservices and that practicing Domain-Driven Design can help you a lot to address these.He mentioned that you would probably ask yourself how you can enforce invariants if the microservices reside in different JVMs.You would be reliant on ACID transactions to maintain consistency.Transactions violate encapsulation and require 2-Phase Commits (2PC) which is not an option because of the following reasons:  It guarantees consistency but 2-Phase Commit is a single point of failure  It is a chatty protocol: at least O(4n) messages, with retries O(n^2)  Reduced throughput due to locks  Not supported by many NoSQL databases (or message brokers)  Doesn’t fit in a NoSQL mindsetAggregates are a solution to these ACID transactions since they allow you to use eventual consistency.You reference other aggregate roots via an identity, this being the primary key.If an update must be atomic then it must be handled by a single aggregate therefore aggregate granularity is important.You should have your aggregates as fine grained as possible.In an Event-driven architecture you can work with steps where each step publishes an event that trigger the next step in the sequence.You will need to write custom logic to compensate the well known ACID transaction rollback so careful design is required.Using Event Sourcing with Aggregates:  There is no notion of updating the database and then publishing the event but you rather just publish the event  For each Domain-Driven Design aggregate:          Identify state changing domain events (eg with Event Storming)      Define Event classes (for example Order events: OrderCreated, OrderCancelled, OrderApproved, OrderRejected, OrderShipped, …)        Persist events and NOT current state          Store the events themselves in a database      Replay events to recreate state        All Aggregates are storing their events in the Event Store, each aggregate subscribes to events of the other aggregates          When using CQRS, update the view after processing an event      Benefits of Event Sourcing:  Solves data consistency issues in a microservice/NoSQL based architecture  Reliable event publishing needed by predictive analytics, user notifications, etc  Eliminates object relationship mapping problems  Rectifies state changes:          Built in, reliable audit log      Temporal queries      Preserved history      Drawbacks of Event Sourcing:  Requires application rewrite (mechanical transformation)  Learning curve: weird and unfamiliar style of programming  Events can be a historical record of your bad design decisions          You will probably implement a mechanism to deal with that such as versioning        Must detect and ignore duplicate events:          Write idempotent event handlers      Track most recent event and ignore older ones        Querying the event store can be challenging:          Event store might only support lookup of events by identity id      Must use CQRS to handle queries meaning application must handle eventually consistent data      Example application: https://github.com/cer/event-sourcing-examples  Orders and Customers example with Kafka in between as backchannel  Both of them connected with the event store  Written in SpringSummary  Aggregates are the building blocks of microservices  Use events to maintain consistency between aggregates  Event Sourcing is a good way to implement an event-driven architectureSecure by design - Eoin WoodsEoin Woods went over security and that we should care about it for things like protection against malice, mistakes and mischance, theft, fraud, destruction, disruption,…He mentioned OWASP top 10 which he approves and should definitely be checked out if you haven’t seen it.There are four aspects of security practice:  Secure Application Design  Secure Infrastructure Design  Secure Application Implementation  Secure infrastructure deploymentHe explains that there are many sets of security design principles but he notes that there are many similarities between them at a fundamental level:  Viege and McGraw (10)  OWASP (10)  NIST (33)  NCSC (europe)(44)  Cliff Berg’s set (185)Out of all these, Eoin has distilled 10 key principles himself:  Least privilege          Why: Broad privilege allows malicious or accidental access to protected resources      Principle: Limit privileges to the minimum for the context      Tradeoff: It is less convenient, less efficient and adds more complexity      Example: Run a server process with the minimum privileges        Separate responsibilities          Why: Achieve control and accountability, limit the impact of successful attacks, make attacks less attractive      Principle: Separate and compartmentalise responsibilities and privileges      Tradeoff: Development and testing costs, operation complexity, troubleshooting can be more cumbersome      Example: Admins of a submodule should have no access to other module features        Trust cautiously          Why: Many security problems are caused by inserting malicious intermediaries in communication paths      Principle: Assume unknown entities are untrusted, have a clear process to establish trust, validate who is connecting      Tradeoff: Operational complexity, reliability, some development overhead      Example: Don’t accept untrusted RMI connections, use client certificates, credentials or network controls        Simplest solution possible          Why: Security requires understanding of the design, complex design is rarely understood, simplicity allows analysis      Principle: Actively design for simplicity - avoid complex failure modes, implicit behaviour, unnecessary features,…      Tradeoff: Hard decisions on features provided and needs serious design effort in order to be simple      Example: Does the system needs a dynamic runtime config via a custom DSL?        Audit Sensitive Events          Why: Provide record of activity, deter wrongdoing, provide a log to reconstruct the past, provide a monitoring point      Principle: Record all security significant events in a tamper-resistant store      Tradeoff: Performance, operational complexity, development cost      Example: Record all changes to “core” business entities in an append-only store        Secure defaults &amp; fail securely          Why: Default passwords, ports and rules are “open doors”. Failure and restart states often default to “insecure”      Principle: Force changes to security sensitive parameters. Think through failures - Must be secure but recoverable      Tradeoff: Convenience      Example: Don’t allow “SYSTEM/MANAGER” after installation. On failure don’t disable or reset security controls        Never rely on obscurity          Why: Hiding things is difficult - someone is going to find them, accidentally if not on purpose      Principle: Assume that the attacker has perfect knowledge, this forces secure system design      Tradeoff: Designing a truly secure system takes time and effort      Example: Assume that an attacker will guess a “port knock” network request sequence or a password encoding        Defence in-depth          Why: Systems do get attacked, breaches do happen, mistakes are made - need to minimise impact      Principle: Don’t rely on single point of security, secure every level, stop failures at one level from propagating      Tradeoff: Redundancy of policy, complex permissioning and troubleshooting, can make recovery harder      Example: Access control in UI, services, database, OS        Never invent security tech          Why: Security technology is difficulty to create, it’s a specialist’s job, avoiding vulnerabilities is difficult      Principle: Don’t create your own security technology, always use a proven component      Tradeoff: Time to assess security technology, effort to learning it, complexity      Example: Don’t invent your own OSS mechanism, secret storage, crypto libs, use existing proven ones        Secure the weakest link          Why: “paper wall”-problem, common when focus is on technologies and not threats      Principle: Find the weakest link in the security chain and strengthen it, repeat (threat modelling)      Tradeoff: Significant effort required, often reveals problems at the least convenient moment      Example: Data privacy threat met with encrypted communication but with unencrypted db storage and backups      Links:  Presentation  Book: Software Systems Architecture  UK Government NCSC  NIST Engineering Principles for IT Security  OWASP Security by Design PrinciplesEvent-driven Microservices - Jeremy DeaneJeremy Deane starts off telling that within Event-driven architectures (EDA) you have events representing a snapshot in time of an occurrence within a system.We should distinguish the following Enterprise Integration Patterns (EIP):  Event Message  Command Event  Event SourcingAs for possible Middleware solutions:  ActiveMQ (JSM), RabbitMQ (AMQP)  Kafka, ZeroMQ  AkkaThere are a couple of EDA principles to take in mind:  Events are emitted by a Producer and received async, and optionally acted upon, by a stateless Consumer  Streams are sets of related Events  Intermediate Processors can enrich the raw Event  Ideally, Producer and Consumer should be decoupled so they can evolve independently over time  Producer should be a magnanimous writer and consumer should be a tolerant reader  Consumers can listen to Event Queues or subscribe to Event TopicsTo give some EDA examples:  Fraud prevention  Medical Alerting (ER check-in)  Financial Portfolio Management  Supply Chain ManagementMicroservices Architectural Style:  Application as a suite of small services  Each running in its own process  Communicating with lightweight mechanisms  Built around business capabilities and independently deployable by fully automated deployment machinery  Bare minimum of centralized managementJeremy really likes Apache ActiveMQ for several reasons:  Not the fastest but the easiest to implement and maintain  Easy to learn  Great and active community  High Availability via a master/slave approachFinally he did a demo of an application built with ActiveMQ and Apache Camel to show how well and easy they integrate together.The source of the demo is available at the links below.Links:  Presentation  https://github.com/jtdeane/event-driven-microservices  https://github.com/jtdeane/camel-standalone-routerOperating the Spotify backend - Niklas GustavssonNiklas Gustavsson immediately starts off with the culture at Spotify.Spotify made two very nicely animated videos that explain their culture:  Spotify Engineering Culture Part 1  Spotify Engineering Culture Part 2Basically it comes down to having autonomous teams called squads containing of 7-12 team members:  A squad should be fully staffed for their “mission”  A squad can decide how they want to tackle their issue  Each squad is on call for their own microservicesAt Spotify they have hundreds of fairly small services that only do one thing well.Each service is owned by a squad that implements and deploys it into production.Later on, they started putting ops guys in the squads in order to be able to tackle issues a lot faster.Jeremy also stresses that it’s very important to automate as much as possible.At Spotify they have a simple incident process to avoid the same issues from reappearing:  Something explodes, an incident gets created and it gets fixed  (Blameless) post-mortem meeting  Remediations to make sure it never happens again  The incident is closedSystem-Z is the service registry in which all services in production get registered in and the following information is available for each microservice:  Who’s on call for each service  When a service is down  What the dependencies are for each service  The hardware in use  The amount of instancesAlso, in order to encourage squads to register their microservices, if you’re not registering your services in there you’re not getting any hardware and thus you can’t get your service into production.In order to maintain their hardware they utilise the Cortana Pool Manager where they have the following information available:  See hardware available  Specify how much you need like how many instancesThey currently do a lot of self-hosting but are planning to fully migrate to Google Cloud Platform.In order to manage their Docker containers they use Helios which is their own Docker orchestration platform.Links:  PresentationJAX Innovation AwardsThe JAX Innovations Awards reward the most innovative contributions in the shape of technology, idea, documentation, industry impact or community engagement.Most innovative contribution to the Java Ecosystem:  1st: Spring Boot  2nd: Camunda BPM Platform  3rd: SparkInnovation in Software delivery &amp; DevOps:  1st: Docker  2nd: Prometheus  3rd: habitatSpecial Jury Award:  Let’s EncryptExtra: Special Honour Award:  Pieter Hintjes (1962-2016) who passed away last week          “Look at the internet, that’s how software should be, so resilient”      Worth a read: Confessions of a Necromancer      Day 2Opening keynote: Decision-making in the face of disruption - Duncan PearsonDuncan is a Chief Architect at Anaplan.Anaplan is the leading planning and performance management platform for smart business.It provides a mechanism that helps the business understand the consequences of what they intent to do.Duncan first sums up the forms of disruption:  Market replacement          Film -&gt; Digital camera      Digital Camera -&gt; smartphone      GPS -&gt; smartphone        Market change          Free online ad-funded services      Comparison websites        New market          Personal gene technology      Recorded music        Environmental change          Natural disaster      War and civil unrest      Time is of the essence and it is important to act quickly to a window of opportunity when trying to tackle a changing situation over time.There are two perspectives on disruption:  The disruptors          High organisational growth leading to complexity transitions      High sensitivity to: growth rate mismatch and delay effects      For example: DataStax, Facebook, Salesforce, Groupon, Box, OpenDNS        The disrupted          New business models required      Disconnect between business functions and supporting systems      Entrenched organisational systems      For example: Yahoo, Toyota, PWC, Verizon, Vodafone, Cisco      Duncan mentions that there are two kinds of systems: modelling systems and execution systems.A good modelling system has the following characteristics:  Personal  Flexible  General purpose  It has to feel like a spreadsheetWhereas a good execution system tends to be:  Specialist, use-case-specific  Embed the “model” in the software  Secure / multi-user  User-task focusedAn execution system tends to be a system that has already been analysed in-depth.Influence on design:  Grids with pivots, recalculations, access to formulae everywhere, inline changes to names/formulae/structure          It has to support the task-focused application developer                  Easy screen and navigation design          Persistent navigational context          Security &amp; access control                    Completely flexible                  Anything can happen at any time          Remodelling the business should be easy          Fast delivery time                    Scale and perform                  If it looks like a spreadsheet then it had better behave like one even if you are changing billions of numbers                    A business management tool                  No programming          No DB design required                    Influence on the company:  We were the disruptor, on a tight deadline          Success through simplicity      Something that works today is better than something that is right tomorrow      Led by our customers, driven by the features that they need        We are undergoing inevitable complexity transitions          New HR systems, management processes, multiple dev locations      Standard Compliance (security, development process)      The Serverless Cloud - Bart BlommaertsBart Blommaerts starts off explaining that serverless, although as suggested by the name, does not actually mean that you’re not using any server.What it does mean is that the existence of a server is hidden for you.The name might be a bit misplaced and it would be more correct to simply call it Functions as a Service (FaaS).Serverless or FaaS might actually be the next step in the evolution of cloud computing.Going serverless will lead to less worries seeing as you no longer have the server management to take care of.Security updates, scalability and availability is all taken care off for you, by the provider.On the other hand, more trust must be put into the provider.The need for an ops team will not fade since you still need to take care of things such as monitoring, debugging support, memory management, application configuration and more.All these things could be handled by a specialised, outsourced team.Seeing as serverless is rather new, there is also the opportunity for tooling to be built!We can distinguish the following characteristics:  Event-driven  Pay per execution instead of server uptime  The actually invocation cost depends on how long it took to execute the callAn example of the pricing of AWS:  Pay per 100ms minimum each call  3.2 million free tier seconds per month  Outside of the free tier the price per 100ms is $0.000000208Serverless comes with seamless scaling:  No risk of under- or over-provisioning  Short-lived “compute containers” that:          are isolated from other functions      have their resources provided from the function config      may be reused      Serverless is stateless, there is no state persisted in between invocations.In order to preserve state you can still use a db, file system or cross-application cache.Bart then listed a couple of the current providers:  AWS Lambda          Integrates well with all other AWS services        IBM Bluemix OpenWhisk          OpenWhisk is open source and available on Github      Better UI than AWS and also has a dashboard for monitoring        MS Azure Cloud Functions          Mature system      Different payment model compared to AWS that could be more expensive in the long run        Google Cloud Functions          A new initiative by Google, but it benefits from services Google offered over 5 years ago      Currently reimplementing it      Currently in private alpha      Very active community        Auth0 WebTask          CLI      Really simple to use, you can be up and running in 30 seconds      Well documented with examples      Serverless enables experimentation due to how easy it is to get something up and running and the low running costs.It could lead to a more collaborative economy seeing as a lot of companies are sitting on a ton of data currently not being used.All this data could be made public by publishing an API and others could consume the data and combine it with their own data, enriching it.In Belgium there is already a company, Realo, that only combines data and they seem really popular.Bart did a demo using an Arduino measuring the temperature.The Arduino then sends the data off to multiple providers, enriching the data, after which it gets logged in the final step.    During the demo, Bart mentioned that he used the Serverless Framework for development.The Serverless Framework is a CLI, soon to be supplier independent (at the time of writing), with the following features:  Scaffolding  Best practices (grouping of functions)  Lifecycle support (create, deploy, invoke,…)The demo and code samples are available at:  https://github.com/bart-blommaerts/serverless-demo  https://github.com/ordina-jworks/lora-iot-demoBart notes that the size of the function is important as it has an impact on execution time and will affect the cost.For example a service written in Java could be 34MB whereas in NodeJS the exact same function may only be 100KB.Best practices:  Compare the following regarding supplier choice:          Integration      Offering      Tooling available      Execution time is limited so check if it fulfils your needs      SLA available?        Code          Initialise services outside of the function such as making a database connection      Limit function size      Use an external editor and a VCS      Bart also recently wrote a blogpost on The Serverless Cloud, you should definitely check it out if you’re interested on the subject.Links:  PresentationFour Distributed Systems Reference Architectures - Tim BerglundAccording to Tim Berglund, a distributed system is a collection of independent computers that appear to its user(s) as one computer.Independent computers should operate concurrently, fail independently and should not share a global clock.The point of Tim’s talk is to compare the following four systems:  Modern 3-tier Architecture  Sharded Architecture  Lambda Architecture  Serverless ArchitectureEach system has its strengths and weaknesses compared and finally receives a rating for the scalability, hipness and difficulty of use.Modern 3-tier ArchitectureThe classic Presentation, Business and Data tier.  Before: JSP, EJB/Servlet, Oracle DB  Now: React.JS, Node.JS, CassandraStrengths  Rich front-end framework (scale/UX)  Hip, scalable middle tier (such as Spring)  Basically infinitely scalable data tierWeaknesses  State in the middle tierRating  Scalability: 4/5  Hipness: 2/5  Difficulty: 3/5Sharded ArchitectureWith a Sharded Architecture the Business and Data tier is usually sharded.In between the clients and the business tier there’s usually a router that performs loadbalancing and routes the request to the service of the application.The Data tier contains multiple databases.Strengths  Client isolation is easy (data and deployment)  Known, simple technologiesWeaknesses  Complexity  No comprehensive view of data  Oversize shardsRating  Scalability: 3/5  Hipness: 1/5  Difficulty: 4/5Lambda ArchitectureStreaming data versus handling data in batch or even better, unbounded data vs bounded data.Event-driven, events are either stored in a database or processed via an Event Processing framework such as Apache Storm.Storing them in a database includes:  Long-term storage  Bounded analysis  High latency.On the other hand, processing them via an Event Processing framework includes:  Temporary queueing  Unbounded analysis  Low latencyStrengths  Optimises subsystems based on operational requirements  Good at unbounded dataWeaknesses  Complex to operate and maintain  Bad at mutable/update-in-place dataRating  Scalability: 3/5  Hipness: 3/5  Difficulty: 5/5ServerlessDoes not mean that there isn’t any server, just that you no longer maintain it.Also called Functions as a Service (FaaS), these can be seen as “extreme” microservices.Strengths  Potentially low resource cost  Rigorous microservices disciplineWeaknesses  Can be very complex  Requires a different way of thinkingRating  Scalability: 4/5  Hipness: 5/5  Difficulty: 5/5Links:  Presentation"
      },
    
      "cloud-2016-11-12-theserverlesscloud-html": {
        "title": "The Serverless Cloud",
        "url": "/cloud/2016/11/12/TheServerlessCloud.html",
        "image": "/img/serverless.jpg",
        "date": "12 Nov 2016",
        "category": "post, blog post, blog",
        "content": "  In recent years, the uprise of the cloud has brought us a lot of new and disruptive technologies. Everybody is talking about SaaS, PaaS, IaaS and other sorts of aaS. In 2014, Amazon launched AWS Lambda as the pinnacle of the cloud computing. It allows developers to focus on code, without spending time on managing servers.Part 1What?While Microservices have been reigning the Cloud landscape for a couple of years, today the Serverless movement is one of the hottest trends in the industry. Historically, software developers have been pretty bad at naming things and Serverless is no exception. Disregarding what the name suggests, Serverless does not imply the complete absence of servers. It implies that developers who are using the Serverless architectural style, are not responsible for managing or provisioning the servers themselves, but use a vendor-supplied Cloud solution. Serverless means less worrying about servers. Although in the future, it might be possible to install this kind of service on-premise, for example with the open-source IBM OpenWhisk implementation.In regard to this, the definition FaaS: Functions as a Service makes a lot more sense. Functions are short-lived pieces of runtime functionality that don’t need a server that’s always running. Strictly speaking a function can have a longer execution time, but most FaaS providers will currently limit the allowed computation time. When an application calls a function (eg. a calculation algorithm), this function gets instantiated on request. When it’s finished, it gets destroyed. This leads to a shorter “running” time and thus a significant financial advantage. As an example, you can find the AWS Lambda pricing here. FaaS functions are also a great match for event-driven behaviour: when an event is dispatched, the function can be started instantly and ran only for the needed time. A Serverless application is a composition of event chaining. This makes the Serverless style a natural match for API Economy.As a result of being runtime components, FaaS functions are stateless and need to rely on a database (or file system) to store state. Being stateless and short-lived naturally lead to extreme horizontal scaling opportunities and all major FaaS providers support these.NoOpsNoOps (No Operations) is the concept that an IT environment can become so automated and abstracted from the underlying infrastructure that there is no need for a dedicated team to manage software in-house. NoOps isn’t a new concept as this article from 2011 proves. When Serverless started gaining popularity, some people claimed there was no longer a need for Operations. Since we already established that Serverless doesn’t mean no servers, it’s obvious it also doesn’t mean No Operations.It might mean that Operations gets outsourced to a team with specialised skills, but we are still going to need: monitoring, security, remote debugging, … I am curious to see the impact on current DevOps teams though. A very interesting article on the NoOps topic, can be found over here.AWSAWS LambdaAWS Lambda was the first major platform to support FaaS functions, running on the AWS infrastructure. Currently AWS Lambda supports three languages: Node.js, Java, and Python. AWS Lambda can be used both for synchronous and asynchronous services.Currently the tooling for AWS Lambda is still relatively immature, but this is changing rapidly. At the time of writing, the AWS Lambda console offers the possibility to create a Lambda using blueprints. This is already easier than setting up a lambda by hand (using a ZIP-file). Blueprints are sample configurations of event sources and Lambda functions. Currently 45 blueprints are available. To give a short introduction, we’ll select the hello-world blueprint. This blueprint generates a very simple NodeJS function:'use strict';console.log('Loading function');  exports.handler = (event, context, callback) =&gt; {   console.log('value1 =', event.key1);   console.log('value2 =', event.key2);   console.log('value3 =', event.key3);   callback(null, event.key1); };After creating this function, it can be immediately be tested from the console, using a test event. If we want to call this function synchronously, we need to create an API endpoint with the AWS API Gateway. The API Gateway creates API’s that acts as a “front door” to your functions. To make this work with the events in our hello-world example, we need to select the resources of our API:In Integration Request, we add a body mapping template of type application/json with the following template:{ \"key3\": \"$input.params('key3')\",\"key2\": \"$input.params('key2')\",\"key1\": \"$input.params('key1')\"}In ‘Method request’ we add 3 URL String query parameters: key1, key2 and key3. If we then redeploy our API, hitting the Test button gives us an input form to add the 3 query parameters and the function is executed successfully:If you want to test this directly from a browser, you will need to change the Auth to NONE in the ‘Method request’ and do a new deploy of the API. The URL itself can be found in the ‘stage’-menu.This example obviously is not very interesting, so let’s try another blueprint: microservice-http-endpoint. This will generate a CRUD backend, using DynamoDB with a RESTful API endpoint. The code generated, covers all common use-cases:'use strict';letdoc = require('dynamodb-doc');letdynamo = newdoc.DynamoDB();exports.handler = (event, context, callback) =&gt; {   const operation = event.operation;   if(event.tableName) {      event.payload.TableName = event.tableName;   }   switch(operation) {   case'create':      dynamo.putItem(event.payload, callback);      break;   case'read':      dynamo.getItem(event.payload, callback);      break;   case'update':      dynamo.updateItem(event.payload, callback);      break;   case'delete':      dynamo.deleteItem(event.payload, callback);      break;   case'list':      dynamo.scan(event.payload, callback);      break;   case'echo':      callback(null, event.payload);      break;   case'ping':      callback(null, 'pong');      break;   default:      callback(newError(`Unrecognized operation \"${operation}\"`));   }};Obviously you will need a DynamoDB instance with some data in it:You can reference your new table, from your lambda, using the following event:{\"tableName\": \"garage-car-dev\",\"operation\": \"list\",\"payload\": { }} The only difficult part remaining, is finding out the required payload for the different operations :) This is a good start for creating new records:{\"operation\": \"create\",\"tableName\": \"garage-car-dev\",\"payload\": {   \"Item\": {      \"id\": \"1980b61a-f5d7-46e8-b62a-0bbb91e20706\",      \"body\": \"Lamborghini\",      \"updatedAt\": \"1467559284484\"      }   }}The blueprint also generates an API in the API Gateway that we can invoke with the above events as body mapping template in integration request of the method execution, just like the first example.Serverless FrameworkWhile the above approach works as expected, it’s quite cumbersome to get your first function working. Especially since we didn’t write any actual code in the previous examples. Luckily the Serverless Framework (formerly JAWS) is here to make our lives easier. Currently the Serverless Framework only supports AWS Lambda, but support for other IaaS providers is coming. A pull-request for Microsoft Azure already exists and other providers are also working on an implementation. Vendor-neutral FaaS would be a true game-changer!One problem with FaaS, is the (deliberate) mismatch between runtime unit and deploy unit. This is also true for other architectural patterns. It should be possible to deploy one specific function, but often functions will hang out in groups. I’d prefer to deploy a group of functions in one go, when it makes sense, eg. different CRUD operations on the same resource. This way, we benefit from the advantages of functions (scalability, cost, service independence, …) but also ease deployment. This is a key feature of the Serverless Framework.On June 29th, Serverless V1.0-alpha1 was announced. New Alphas and Betas will be released on a regular basis. Currently the documentation can only be found in their v1.0 branch on GitHub. Serverless V1.0 introduces the “Serverless Service” concept, which is a group of functions with their specific resource requirements. In essence Serverless V1.0 is a powerful and easy to use CLI to create, deploy and invoke functions. Serverless V1.0 uses AWS CloudFormation to create AWS resources. It uses the default AWS profile for access to your AWS account. Creating, deploying and invoking a “Hello World” NodeJS function with Serverless is as easy as:serverless create --name cars --provider aws serverless deploy serverless invoke --function hello --path data.jsonThis generates the following lambda:'use strict'; module.exports.hello = (event, context, cb) =&gt; cb(null,    { message: 'Go Serverless v1.0! Your function executed successfully!', event } ); The current version of the Serverless Framework (unfortunately) doesn’t use the region from the AWS config, so you might need to look for your function in a different region.Adding an API Gateway endpoint, is also very easy and can be done by adding this http-event:events:  - http:       path: greet       method: getThe actual URL can be found in the API Gateway in the stages section, as we saw before.Part 2In the first part of these article, I introduced the Serverless architectural style and focused on “market maker” AWS Lambda and on the Serverless Framework. In this part, I want to focus on other Faas providers.Auth0 WebtaskCompared to giants such as Amazon, Google, Microsoft and IBM, Auth0 is a rather small player. However acknowledging their experience with BaaS (Backend as a Service), FaaS is a logical choice for them. Currently Webtask only supports NodeJS.The recommended way of using webtask is through the wt command line interface. Auth0 has put the focus on easy of use. This is really visible by looking at their 30 second example. The wt create command wil generate a function (a webtask) and will automatically return an HTTP endpoint, supporting URL query parameters. Every query parameter is available in your webtask in the form of context.data JavaScript object. With AWS Lambda you need to configure these in the AWS API Gateway, which is both tedious and time-consuming.A very interesting feature of Webtask is the availability of built-in storage.Webtask code can store a single JSON document up to 500KB in size. This data can be stored with ctx.storage.set and retrieved with ctx.storage.get. While I don’t believe your function will often need this, it’s a very nice option.This small example (using Lodash), shows a webtask using a query parameter and built-in storage.module.exports = function (ctx, cb) {    var name = ctx.query.name;     if(name) {        ctx.storage.get(function(err, data){            if(err) cb(err);             data = data || [];             if(_.indexOf(data, name) === -1 ){                data.push(name);                 ctx.storage.set(data, function(err){                    if(err){                        cb(err);                    } else {                        cb(null, data);                    }                })            } else {                cb(null, data);            }        })    } else {        cb(null, \"422\");    }}Deploying this webtask, using the CLI:Webtask created You can access your webtask at the following url: https://webtask.it.auth0.com/api/run/wt-&amp;lt;your username&amp;gt;-0/query_store?webtask_no_cache=1Another way to access your webtask is as a CRON job, using the wt cron command or as a web hook.Contrary to AWS Lambda, you don’t need to bundle the NodeJS modules you want to use. The list of supported modules is available here. An option to bundle other modules is also available. Another difference is the use of query parameters.Not surprisingly, Webtask can be integrated with Auth0 for authentication and authorization.Google Cloud FunctionsGoogle Cloud Functions (GCF) was released early 2016 and is currently in private alpha. Being in private alpha not only means that you specifically need to request access to use the GCF API, but also that you’re limited in sharing information. While this is obviously very unfortunate, it also means that Google is very serious about releasing a complete product. The activity in their (again private) Google Group proves this.Like its competitors, Cloud Functions can be triggered asynchronously by events (from Cloud Pub/Sub and Cloud Storage) or invoked synchronously via HTTPS. Currently GCF only supports NodeJS. Tutorials on common use-cases are available in their documentation. To build functions with GCF, you will first need to download and install the Google Cloud SDK. With the SDK installed, you can create your initial function (replace datastore_gcf with your own staging bucket name):$ gsutil mb gs://datastore_gcfFrom the (very useful) (unofficial) GCF recipes by Jason Polites (Product Manager, GCP), we cloned the datastore example that will persist data to a Google Coud Datastore.From this repository, we deployed 2 functions ‘ds-get’ and ‘ds-set’ by executing:$ gcloud alpha functions deploy ds-set --bucket datastore_gcf --trigger-http --entry-point setThe names of the deployed functions, need to be exported in the Node.js module. These functions can be called with:$ gcloud alpha functions call ds-get --data '{\"kind\": \"test\", \"key\": \"kid\"}'or via the Cloud Functions Console.Your newly added data is also available in the Datastore Entities after selecting a project on the top. After executing a couple of functions, you can also find some metrics of your function (number of calls, execution time, …)Other arguments for the deploy command are listed in the reference documentation. These steps are also available in the Cloud Platform Console.After deployment, your webtrigger URL will be displayed similar to Webtask.Although much information on Google Cloud Functions is not (publicly) available yet, Google is well on its way to become a serious FaaS provider.Azure FunctionsSimilar to Google Cloud Functions, Microsoft Azure Functions is currently in preview stage, meaning it’s not (yet) meant to be used in a production environment. Azure Cloud Functions (ACF) support a variety of languages such as NodeJS, C#, Python, and PHP.Today, it can be used for these common cases:  Events triggered by other Azure services  Events triggered by SaaS services (not limited to Microsoft)  Synchronous requests  WebHooks  Timer based processing (CRON)creating quite a large number of possibilities.Azure Functions are grouped in App Services. This is quite different from AWS Lambda, where the functions are organised independently. Hardware resources are allocated to an App Service and not directly to an Azure Function. It’s important to select a dynamic App Service if you’re aiming for “pay-per-execution”.When creating a new function, you can start from different templates. This can be compared to the blueprints from AWS Lambda. Currently 44 templates are available (but some are very similar). When selecting HttpTrigger for example, Azure Functions will generate a function that is able to use all query parameters passed to the function, similar to Webtask. This short video demonstrates this use case.In the example below, an Azure Cloud Function will store entities in a Storage Table when it receives an HTTP request:function.json: \"bindings\": [    {      \"type\": \"httpTrigger\",      \"direction\": \"in\",      \"name\": \"req\",      \"methods\": [        \"post\"      ],      \"authLevel\": \"function\"    },    {      \"type\": \"http\",      \"direction\": \"out\",      \"name\": \"res\"    },    {      \"type\": \"table\",      \"name\": \"outTable\",      \"tableName\": \"entities\",      \"partitionKey\": \"functions\",      \"rowKey\": \"%rand-guid%\",      \"connection\": \"YOUR_STORAGE\",      \"direction\": \"out\"    }  ],  \"disabled\": false}index.js:    var statusCode = 400;    var responseBody = \"Invalid request object\";     if (typeof req.body != 'undefined' &amp;amp;&amp;amp; typeof req.body == 'object') {        statusCode = 201;        context.bindings.outTable = req.body;        responseBody = \"Table Storage Created\";    }     context.res = {        status: statusCode,        body: responseBody    };     context.done();};To retrieve the added entities:functions.json:  \"bindings\": [    {      \"type\": \"httpTrigger\",      \"direction\": \"in\",      \"name\": \"req\",      \"methods\": [        \"get\"      ],      \"authLevel\": \"function\"    },    {      \"type\": \"http\",      \"direction\": \"out\",      \"name\": \"res\"    },    {      \"type\": \"table\",      \"name\": \"inTable\",      \"tableName\": \"entities\",      \"connection\": \"YOUR_STORAGE\",      \"direction\": \"in\"    }  ],  \"disabled\": falseindex.js:    context.log(\"Retrieved records:\", intable);    context.res = {        status: 200,        body: intable    };    context.done();};What immediately struck me was the quality of their documentation (videos, tours, quickstarts, templates, …) and the user experience from the Azure Portal. The portal can be a little slow sometimes, but the experience is miles ahead of what Amazon and Google are offering. Azure Functions is open source and available on GitHub.Azure Functions will soon be supported by the Serverless Framework, which is a big step towards vendor-neutral FaaS.IBM Bluemix OpenWhiskBluemix OpenWhisk is also an open source service and currently supports NodeJS and Swift. Contrary to other FaaS providers, IBM emphasises on container integration. When an event or an API call invokes an action, OpenWhisk creates a container to run the action in a runtime appropriate to the programming language used. You can even create Docker functions (called actions in OpenWhisk) allowing you to build in any language. OpenWhisk can also run locally on your own hardware, which no other provider currently offers. IBM is very open about this and even provides guidelines on how this can be achieved.As expected, the documentation has a getting started guide to build and run a Hello World action. While working with the CLI works as advertised, it quickly becomes quite cumbersome, especially when integrating with other Bluemix services. After executing your first OpenWhisk function, you can see some metrics in the (pretty) OpenWhisk dashboard. The OpenWhisk dashboard will show all invoked actions, also from actions you didn’t implement yourself. For example when using existing packages.What’s even more impressive is the Openwhisk Editor. This editor only lists the actions you created yourself.As you can see from the screenshot, you immediately get links to the REST Endpoint.ConclusionCurrently it’s too soon to draw any conclusions. These services are constantly changing. What is obvious, is that all major cloud providers want to make sure that they don’t miss the FaaS opportunity. Cloud providers create value by integrating FaaS services with their other offerings. This confirms the value of a Serverless Cloud. The current FaaS solutions have a lot of similar characteristics and choosing one, will likely depend on what other services you already use (or want to use) from a certain provider. It’s important to know the environment your FaaS code lives in and the services available to it. In this phase available documentation also is crucial.Obviously, this high-level introduction doesn’t list all the differences or similarities, but it offers a nice starting point to experience the FaaS (r)evolution first-hand.Part 3In the first part of this article, I introduced the Serverless architectural style. In the second part, I compared all major serverless providers. In this third and last part, I would like to look at serverless as an enabler of collaborative economy.Collaborative EconomyWhat is collaborative ecomomy?  Benita Matofska: The Sharing Economy is a socio-economic ecosystem built around the sharing of human, physical and intellectual resources.It includes the shared creation, production, distribution, trade and consumption of goods and services by different people and organisations.The last part of Benita’s quote: shared creation, production .. of services by different people and organisations makes a very nice use-case for the serverless style of building applications.Your dataIn this day and age, all companies have become IT companies, meaning a lot of data is gathered and stored somewhere. Often the usage of the available data changes over time. If data is not used for the benefit of the enterprise or its employees, does it still hold value? Wouldn’t it be great if we could turn cost into profit?Thanks to its cost model (pay per execution), its focus on scalability (no risk of overprovisioning) and resilience, serverless enables companies to experiment with exposing their data:  Offering an API for others to consume  Enriching existing API’s with their data  …Your ideasServerless also makes a lot of sense for companies that don’t want to expose their data, but have great or new ideas on how to use others data:  Combining data from multiple providers  Filtering and transforming data  New business cases beyond the scope of the original API  …ExampleI implemented a small and simple application that will consume data from different serverless cloud providers. Every “hop” in the system will parse its input and add some new data.Component diagramDescriptionAny client can post a JSON to the first function, made with Auth0 webtask. The body of the post request is simple:{\"temp\":\"42\"}The WebTask will parse that input, add some input of its own and POST request to an IBM OpenWhisk action. The body of this POST request:{  \"hops\": [    {      \"provider\": \"Auth 0 Webtask\",      \"start\": \"2016-08-24T20:32:03.629Z\",      \"temperature\": \"42\",      \"stop\": \"2016-08-24T20:32:03.629Z\"    }  ]}To continue the chain, IBM OpenWhisk will POST the parsed JSON to a function on the AWS Lambda platform after adding a new “hop”:{  \"hops\": [    {      \"provider\": \"Auth 0 Webtask\",      \"start\": \"2016-08-26T18:38:25.021Z\",      \"temperature\": \"44\",      \"stop\": \"2016-08-26T18:38:25.021Z\"    },    {      \"provider\": \"IBM OpenWhisk\",      \"start\": \"2016-08-26T18:38:35.024Z\",      \"temperature\": \"42\",      \"stop\": \"2016-08-26T18:38:35.024Z\"    }  ]}The Lambda, created with Serverless V1.0 Beta 2 will parse the input again and create items in an AWS DynamoDB:The AWS DynamoDB table will stream events to another AWS Lambda that will log the content of the event to the logs of AWS CloudWatch:The source code of all these components is available on GitHub.Best practiceObviously I wouldn’t recommend anyone to use a different cloud provider for every function. Choosing the right one will depend on your specific needs, goals and current cloud landscape. In previous parts of this article, you may find some tips on how to make a reasoned choice.Final noteThis article was originally posted in three parts on the JAX London blog and is also available in German."
      },
    
      "kickstarters-2016-10-31-kickstarters-project-html": {
        "title": "Kickstarter project 2016",
        "url": "/kickstarters/2016/10/31/Kickstarters-Project.html",
        "image": "/img/kicks.png",
        "date": "31 Oct 2016",
        "category": "post, blog post, blog",
        "content": "  On August 1‘st it was D-day for all the kickstarters that had recently joined Ordina. A batch of talented new people were ready to embark on a new adventure. This year around fifty people joined Ordina and participated in the Kickstarter Project. The JWorks kickstarter group consisted of seven people, all of which were eager to get started. Six people joined the JWorks unit and one joined the Security unit. The purpose of the two month long kickstarter project is to broaden the knowledge of and prepare the kickstarters for their first project.Kickstarter project 2016First impressionsYou never get a second chance to make a first impression.– Harlan HoganAnd boy Ordina did a pretty good job!The reception on the first day was really great and pretty informal.First off the kickstarters received a tour of the company.They introduced themselves and got to know eachother in a pretty playful way.FINALLY the moment had arrived that everybody was waiting for !The kickstarters received their highly anticipated company car and laptop.The overal atmosphere is pretty loose, you can ask anyone anything and you can talk to everybody.After a few days the kickstarters also got the chance to go on a teambuilding day in Mechelen.During the course of this day, they had to work together as a team to complete some questions and games.The winners were rewarded with a cup. This enabled them to learn how to communicate in a team and under stress, because some of the tests had to be completed within an certain amount of time.Most of the kickstarters already had the chance to go to one of the Ordina events like JOIN or CC meetings, where they networked with lots of interesting people.August - LearningThe focus was primarily on learning during the first month of the Kickstarters project.The first few days the kickstarters had to improve their softskills by learning how to be more assertive towards the client when necessary.They also learned to introduce themselves properly with the emphasis on their qualities and strengths, to ensure they make a good first impression of themselves when going to the client.The kickstarters were brought up to date with the preferred technologies, editors and best practices used by JWorks.During the first month they received different courses in which they could improve their technical skills about:  backend          Java (7 &amp; 8) + JavaEE      Spring      JPA      Webservices (REST &amp; SOAP)      MongoDB        frontend          HTML &amp; CSS      Javascript &amp; Typescript      Angular      Ionic      These courses were given by the JWorks unit who tried to teach the kickstarters a much as possible with theoretical material and some exercises afterwards.Unfortunately, these technical skills aren’t enough to survive in the forever changing IT world.This is why some extra help was provided in the form of books and courses about how to write clean code, how to work agile and learning how to work in a team while understanding and using the SCRUM principles.By paying attention to technical development with the necessary certification processes and also focusing on the development of soft skills like communication, advising and collaboration,Ordina commits to the personal development of these kickstarters.September - Dev-caseThe focus during the second month was on the implementation of this year’s dev-case.Although they still had to follow a few courses along the way, like the basic priciples of security, GIT and learning how to use MongoDB.SensyMcSenseFace was born– Chosen by popular vote, who would have guessed it…The kickstarters already had learned how to write clean code and how to do this in the best possible way.The purpose of the SensyMcSenseFace project was to give the kickstarters a use case where they could develop an end-to-end IoT solution, in which they could test and use their newly acquired skills.What did the client requestThe kickstarters were given the task to build an application that accepts incoming data, while being able to process this data and output it in a more user friendly way.The data would be sent by three different sensors:  Temperature sensor  Humidity sensor  Motion sensorThese sensors send some data every few seconds to the backend, the backend then processes this data and sends it back to the frontend.Here the frontend developers made sure that all the data has been received and outputted in the correct way.The following picture depicts the two meeting rooms that are equipped with three different sensors, which send their data back to the application’s backend.Each meeting room equipped with the sensors, which have their values read by an Arduino that then sends these across the Proximus LoRa network to the backend. For the initial stages and testing the LoRa part was omitted and a simple node server instance was used to relay the sensor values to the actual backend.This way, the client (Ordina) could figure out when they are using excessive power.For example people leaving the TV on for too long inside of the meeting rooms.Used technologies - sofware  backend          Maven      Spring      Spring Boot      MongoDB      Mockito        frontend          Angular 2      Angular Material 2        extra          Waffle      Cloudfoundry      GitHub      Process of the projectThe first week went pretty well, they divided themselves up into two groups.One group for the frontend and the other one for the backend.At first they started to create different user stories for their SCRUM board.Using the newly acquired scrum techniques during the first month.The kickstarters had to work in four short sprints of one week. During the first sprint they also decided to change their real life SCRUM board into an online version using Waffle.This software would track the pull requests and merges automatically from Github and change the board accordingly.Continuous Integration was pretty important during the course of the project.This way, whenever they made changes to the code and made a pull request to Github that failed to build, they had to fix their code before they could continue.Once the build on codeship succeeded and the pull request was merged. The main development branch and master branch would have their changes (if any) deployed to their Cloudfoundry instance.The process for the backend was pretty simple.They started out with around five people, so some of them started to pair program while others started to program on their own.But, with paying attention to the SCRUM principles and how to write clean code.Their first job was to start with the basic implementations of the sensor, room and notifications classes and writing the JUnit and Mockito tests.While the backend was pretty straight forward and relatively easy to begin with, the frontend team were confronted with some problems.The team consisted of only two people who had to tackle a lot of problems with the use of Angular 2 which was still in Beta at the time.Also the combination with the other frameworks wasn’t quite that easy to work with.Every morning the team did a stand up meeting where they would discuss their changes in eachothers code and what they were going to do next.During these four weeks they tried to work in these short sprints but after a week or two it became clear this wouldn’t be an easy task.A few team members already had left the group because they were assigned to projects, which messed up their sprints completely.During the first two weeks the backend team started with using Spring Data JPA, but soon figured out Spring Data MongoDB was the better alternative because of the large amount of data being pumped into the DB.A lot of time of went into providing REST documentation that covers all the different calls handled by the application.This documentation was created with MockMvc tests, which create code snippets that were easy to use.On the frontend side they didn’t have test cases yet, but nobody in the entire team had ever written frontend tests before.Which caused a little bit of a delay, also the webpages didn’t seem to be responsive at all.Luckily there were some online tutorials available on mocking and writing frontend tests.The responsiveness issue was sovled by using their own components and CSS code instead of using material design.When starting their two final weeks there were only three people left who were able to fully commit to the dev-case.During the last week the kickstarters also had to prepare and give a proper introduction to the management of Ordina.Showing off their newly learned presentation and introduction techniques.At the very end the core of the application was finished.  You are able to watch an overview of the rooms  You are able to check if a room is occupied or available  You are able to check which sensors are in a room and check their last history  You are able to check the sensor history between two timestamps  You are able to get notifications on your cellphone when certain values are exceededpossible future changesBecause there wasn’t enough time to completely finish the project, this project can still evolve in a lot of ways.  Later on it could be possible to add roles or users.  Make adding sensors or rooms more user friendly.  Add predictions to the applications for every room.  Add user management with users and rolesLessons learned during the Kickstarters project  How to be more confident  How to introduce yourself in a professional way  How to be more assertive  You have to keep an eye out for possible changes in your code that encourage clean code  How to work better and agile in a team and how to use SCRUM principles  Pitfalls and difficulties when using and combining new technologies  How to write proper tests (JUnit, Mockito, MvcTests)  How to write proper REST documentationThe new JWorks colleagues"
      },
    
      "conference-2016-10-10-percona-live-amsterdam-2016-html": {
        "title": "Percona Live Amsterdam 2016",
        "url": "/conference/2016/10/10/Percona-Live-Amsterdam-2016.html",
        "image": "/img/2016-10-16-Percona-Live/PLAM-16-01.png",
        "date": "10 Oct 2016",
        "category": "post, blog post, blog",
        "content": "Percona Live Open Source Database Conference 2016, Amsterdam  It was only three weeks before the conference, that I coincidentally discovered (and by Googling) that Percona was organizing one of the biggest Open Source Databases conferences in Amsterdam, Percona Live Europe.Until then I had never heard of Percona.Shame on me! But, Percona is mostly known in the US and according to db-engine.com it’s takes the 47th  place on the popularity list of Relational Databases, and the 97th spot if you consider all database systems.Yet, Percona is celebrating its 10th anniversary this year and among it’s more than 3000 customers worldwide it can count well-known brands like Cisco Systems, Time Warner Cable, Alcatent-Lucent, Groupon, BBC, Flickr, … among  it’s customers.It was at the conference that I uncovered that Percona Server is in fact a fork of the MySQL open source database, just like the more popular fork MariaDB.Percona presents itself on their website as the only company that delivers enterprise-class solutions for both MySQL and MongoDB across traditional and cloud-based platforms.So it was obvious that the focus of the three-day conference was on MySQL and MongoDB.With 16 tutorials on Monday, 48 sessions on Tuesday and even 64 sessions on Wednesday the line-up was impressive.Most of the sessions covered MySQL, MongoDB and PostgreSQL topics but other open source databases like ElasticSearch, Redis, RethinkDB, Clickhouse,… were also discussed.Even with two people of JWorks attending the Conference, picking the right sessions was difficult and the FoMO syndrome was clearly around the corner.While the conference mainly had a technical focus (sometimes in-depth), with subjects as analytics, architecture and design, security, operations, scalability and performance, I was pleased that these sessions were alternated with customer stories.Eventually it is always interesting to see real life uses cases and big names like Facebook, Uber, Dropbox and Booking.com talking about the open source databases they use, and especially how they use them.Facebook was represented by 10 of its employees involved in 7 talks.To pick some : Shared MySQL hosting at Facebook, Massive Schema changes in Facebook, MyRocks Deep Dive: Flash Optimized LSM Database for MySQL, and its Use Case at Facebook, Online Shard Migration at Facebook,…The conference made it also clear that interest in Open Source Databases continues to grow.More and more companies are looking to replace their proprietary databases to open source alternatives.The reasons for that are clear, open source databases maturity have risen to the level of the proprietary databases and some of them have even gone beyond that.And al of that, without the hefty price tag.  Top Internet applications have embraced open source databases a long time ago, and now traditional enterprises are catching up to.So me and my colleague Chris De Bruyne both returned with a backpack full of very useful information that we like to share with you in the coming months.We will definitely try out a lot of stuff, like the different open source monitoring tools for MySQL and MongoDB.The JWorks DBA / NoSQL competence center also advocates the use of open source alternatives when appropriate, and this conference perfectly matches with our ambitions.So we already reserved 25th - 27th of September 2017 in our agendas for the next Percona Live Europe in Dublin."
      },
    
      "conference-2016-09-27-join-2016-html": {
        "title": "JOIN 2016",
        "url": "/conference/2016/09/27/JOIN-2016.html",
        "image": "/img/join16.jpg",
        "date": "27 Sep 2016",
        "category": "post, blog post, blog",
        "content": "  Next week, on the 5th of October 2016, the JWorks Business Unit of Ordina will organize its yearly JOIN event. The purpose of this event is to share knowledge between colleagues and fellow Java, JVM, JavaScript, Cloud and DevOps enthusiasts. Last year, a total of 83 attendees visited Ordina Belgium’s headquarters in Mechelen to learn and talk about the hottest technology trends and developments.This year we expect to have more than 100 attendees, and what’s most exciting is that the event is completely free of charge and everyone is invited. Food and drinks are provided (including a barbecue).JOIN 2016 Schedule  Here are some of the highlights of the day:10AM - 12AM - Docker for Java Developers - Arun GuptaArun has been an avid Docker user for many years and is also one of the Docker Captains, among being a Java Champion and JUG leader.He will bring us a very informative talk about the current state of Docker and how Java Developers can get started with Docker in no time.We’re very much looking forward to this one!  During another talk he will talk about Couchbase, the product company he’s working for at the moment.More information about Arun in his Docker Community Spotlight.5PM - 6PM - The Google Cloud Platform - Koen MaesKoen Maes provides expert advice and development services for Google Cloud Platform and related products.He is a Google Cloud Platform Authorized Trainer &amp; partner and has been working in the software industry since the early nineties and with web/Internet technology since its inception.He designed key applications for several large corporations as well as running a handful of startups of his own, some more successful than others.Since his first encounters with AppEngine in 2009, he never looked back and has been specializing in Google Cloud Platform ever since.  We are currently working for one of our customers on a large scale greenfield microservices system in the Google Cloud Platform, so this will be especially interesting for us.6PM - 7PM - Reactive Programming - Stephane MaldiniA multi-tasker eating tech 24/7, Stephane is interested in cloud computing, data science and messaging.Leading the Reactor Project, Stephane Maldini is on a mission to help developers create reactive and efficient architectures on the JVM and beyond.He is also one of the main contributors for Reactive support in the upcoming Spring 5 framework.  David Karnok, RxJava project lead, identifies the Reactor project as the new standard for reactive applications in the Java world.Most of the developers in our JWorks unit are using Spring (as opposed to JEE), so this talk is going to be very interesting.7PM - 8PM - Typescript: enjoying large scale browser development - Joost De VriesJoost De Vries is one of our Dutch colleagues at Codestar. He will talk about Typescript and how it enables development of large scale front end applications.You can find him on Twitter and Github.FoodWe have foreseen food and drinks during the conference and we will end the night with something special.We hope and believe it will make everyone very happy! Another reason to JOIN us @Ordina, the 5th of October:  Morning reception:          Coffee or Tea      Mini biscuits and chocolates        Lunch:          Luxury sandwiches &amp; subs with salads, French cheese, grey shrimp, prawns or Parma ham      Viking bread with smoked salmon      Grilled chicken wraps      Vegetarian options also available        Afternoon coffee break:          Coffee or Soda      Candy bars or fruit salad        Dinner:          BBQ      Marinated prawn (scampi)      Greenway balls (vegetarian)      Spicy chipolatas      Marinated chicken satés      Steak chimichurri cut and grilled à la minute      Coleslaw, tomatoes, cucumbers, carrots, potato salad, mix of salads, red onions and olives      Bread, potatoes and sauces      "
      },
    
      "monitoring-2016-09-23-monitoring-with-prometheus-html": {
        "title": "Monitoring with Prometheus",
        "url": "/monitoring/2016/09/23/Monitoring-with-Prometheus.html",
        "image": "/img/prometheus.jpg",
        "date": "23 Sep 2016",
        "category": "post, blog post, blog",
        "content": "It is needless to say the world is shifting towards DevOps and microservices.This holy grail we aim for adds a great deal of complexity.Monitoring included.Rather than having to monitor one system,we are suddenly faced with the challenge to oversee our manifold services.There are numerous monitoring systems available,but not all of them are fit for monitoring large, distributed systems.Black box monitoring systems like Nagios allow you to check if an application is alive and healthy.This is done by e.g. pinging the service,checking if there is enough disk space,or monitoring the CPU usage.In a world of distributed architectures where high availability and fast response times are key,it is not sufficient to be only aware if a service is alive.It is crucial to know how a service is working internally as well.How many HTTP requests is it receiving?Are they handled correctly?How fast are requests handled for different endpoints?Are there many errors being logged?How many disk IO operations is the service performing?These are all important questions that need to be monitored to keep a service functional.Prometheus is a white box monitoring and alerting system that is designed for large, scalable environments.With Prometheus,we can answer all these questions,by exposing the internal state of your applications.By monitoring this internal state,we can throw alerts and act upon certain events.For example,if the average request rate per second of a service goes up,or the fifty percent quantile response time of a service suddenly passes a certain threshold,we could act upon this by upscaling the service.Overview  The Rise of Prometheus  Architecture  Data Model  Slice &amp; Dice with the Query Language  Instrumenting Your Services  Exporters  Scraping the Targets  Visualization and Analytics  Alert! Alert!  Monitoring Time!  Final WordsThe Rise of PrometheusAs with most great technologies,there is usually a great story hiding behind them.Nothing is different with Prometheus.Incubated at SoundCloud,the social platform for sharing sounds and music,Prometheus has come a long way.When SoundCloud was just a start-up,they originally developed their application as a single application.Many features later,this resulted in one big, monolithic application called the Mothership.With only a few thousand artists and users sharing music,the application performed sufficiently.However,nowadays,about 12 hours of music is uploaded every minute to SoundCloud.The platform is used by hundreds of millions of users every day.To be able to handle this size of volume,SoundCloud adapted a more scalable approach.Deciding against a complete rewrite of their whole technology stack,they stopped adding new features to the Mothership.Instead, new features were written as microservices,living next to the Mothership.If you want to know more about how SoundCloud moved from one monolithic application to a microservices architecture,you can find a three-partblog postseries on their developer blog (which is an excellent read, by the way).Moving towards a microservices architecture paved the way for many possibilities for SoundCloud,but it also introduced a lot of complexity.Monitoring a single application is easy.Monitoring hundreds of different services with thousands of instances is an entirely different story.SoundCloud’s original monitoring set-up consisted of Graphite and StatsD.This setup did not suffice for the new, scalable microservices architecture.The amount of generated events could not be handled in a reliable way.SoundCloud started looking for a new monitoring tool,while keeping the following requirements in mind:      A multi-dimensional data model,where data can be sliced and diced along multiple dimensions like host, service, endpoint and method.        Operational simplicity,so that you can setup monitoring anywhere you want,whenever you want,without having to have a Ph.D. in configuration management.        Scalable and decentralized,for independent and reliable monitoring.        A powerful query language that utilizes the data model for meaningful alerting and visualisation.  Since no existing system combined all of these features,Prometheus was born from a pet project at SoundCloud.Although the project has been open source from the beginning,SoundCloud did not make any noise about it until the project was mature enough.In January 2015,after 2 years of development and internal usage,the project was publicly announcedand a website was put online.The amount of attention it received was totally unexpected for the team at SoundCloud.After a post on Hacker News,which made it all the way to the top,things got serious.There was a sharp rise in contributions, questions, GitHub issues, conference invites, and all that stuff.The following image depicts the amount of stars the project received on GitHub since its inception.ArchitecturePrometheus’ architecture is pretty straightforward.Prometheus servers scrape (pull) metrics from instrumented jobs.If a service is unable to be instrumented,the server can scrape metrics from an intermediary push gateway.There is no distributed storage.Prometheus servers store all metrics locally.They can run rules over this data and generate new time series,or trigger alerts. Servers also provide an API to query the data.Grafana utilizes this functionality and can be used to build dashboards.Finally,Prometheus servers know which targets to scrape from due to service discovery,or static configuration.Service discovery is more common and also recommended,as it allows you to dynamically discover targets.Data ModelAt its core,Prometheus stores all data as time series.A time series is a stream of timestamped values that belong to the same metric and the same labels.The labels cause the metrics to be multi-dimensional.For example,if we wish to monitor the total amount of HTTP requests on our API,we could create a metric named api_http_requests_total.Now,to make this metric multi-dimensional,we can add labels.Labels are simple key value pairs.For HTTP requests,we can attach a label named method that takes the HTTP method as value.Other possible labels include the endpoint that is called on our API,and the HTTP status returned by the server for that request.The notation for a metric like that could be the following:api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}Now,if we start sampling values for this metric,we could end up with the following time series:            Metrics      Timestamp      Value                  api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}      @1464623917237      68856              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"500\"}      @1464623917237      5567              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"200\"}      @1464624516508      76909              api_http_requests_total{method=\"GET\", endpoint=\"/api/posts\", status=\"500\"}      @1464624516508      6789      One of the great aspects of time seriesis the fact that the amount of generated time series is independent of the amount of events.Even though your server might suddenly get a spike in traffic,the amount of time series generated stays the same.Only the outputted value of the time series is different.This is wonderful for scalability.Prometheus offers four metric types which can be used to generate one or multiple time series.A counter is a metric which is a numerical value that is only incremented,never decremented.Examples include the total amount of requests served,how many exceptions that occur, etc.A gauge is a metric similar to the counter. It is a numerical value that can go either up or down.Think of memory usage, cpu usage, amount of threads, or perhaps a temperature.A histogram is a metric that samples observations.These observations are counted and placed into configurable buckets.Upon being scraped,a histogram provides multiple time series,including one for each bucket,one for the sum of all values,and one for the count of the events that have been observed.A typical use case for a histogram is the measuring of response times.A summary is similar to a histogram,but it also calculates configurable quantiles.Depending on your requirements,you either use a histogram or a summary.Slice &amp; Dice with the Query LanguageA powerful data model needs a powerful query language.Prometheus offers one,and it is also one of Prometheus’ key features.The Prometheus query language,or promql,is an expressive, functional language.One which apparently,by the way,is Turing complete.The language is easy to use.Monitoring things like CPU usage,memory usage, amount of HTTP request served, etc. are pretty straightforward,and the language makes it effortless.Using an instant vector selector,you can select time series from a metric.For example,Continuing with our API example,we can select all the time series of the metric api_http_requests_total:api_http_requests_totalWe can dive a little bit deeper by filtering these time series on their labels using curly braces ({}).Let’s say we want to monitor requests that failed due to an internal server error.We can achieve this by selecting the time series of the metric api_http_requests_totalwhere the label status is set to 500.api_http_requests_total{status=\"500\"}We can also define a time window if we only want to have time series of a certain period.This is done by using a range vector selector.The following example selects time series of the last hour:api_http_requests_total[1h]The time duration is specified as a number followed by a character depicting the time unit:  s - seconds  m - minutes  h - hours  d - days  w - weeks  y - yearsYou can go further back in time by using an offset.This example selects time series that happened at least an hour ago:api_http_requests_total offset 1hWe can use functions in our queries to create more useful results.The rate() function calculates the per-second average rate of time series in a range vector.Combining all the above tools,we can get the rates of HTTP requests of a specific timeframe.The query below will calculate the per-second rates of all HTTP requeststhat occurred in the last 5 minutes an hour ago:rate(api_http_requests_total{status=500}[5m] offset 1h)A slightly more complex example selects the top 3 endpoints which have the most HTTP requestsnot being served correctly in the last hour:topk(  3, sum(    rate(api_http_requests_total{status=500}[1h])  ) by (endpoint))As you can see,Prometheus can provide a lot of useful information with several simple queries that only have a few basic functions and operators.There is also support for sorting, aggregation, interpolation and other mathematical wizardry that you can find in other query languages.Instrumenting Your ServicesOne of the requirements to be able to query data and get results,obviously,is the fact that there must be data that can be queried.Generating data can be done by instrumenting your services.Prometheus offers client libraries forGo,Java/Scala,Python andRuby.There is also a lengthy list of unofficial third-party clients for other languages,including clients for Bash and Node.js.These clients enable you to expose metrics endpoints through HTTP.This is totally different compared to other,more traditional,monitoring tools.Normally,the application is unaware that it is being monitored.With Prometheus,you must instrument your codeand explicitly define the metrics you want to expose.This allows you to generate highly granular data which you can query.However,this technique is not much different than logging.Logging statements are,most of the time,also explicitly defined in the code,so why not for monitoring as well?For short-lived jobs,like cronjobs,scraping may be too slow to gather the metrics.For these use cases,Prometheus offers an alternative,called the Pushgateway.Before a job disappears,it can push metrics to this gateway,and Prometheus can scrape the metrics from this gateway later on.ExportersNot everything can be instrumented.Third-party tools that do not support Prometheus metrics natively,can be monitored with exporters.Exporters can collect statistics and existing metrics,and convert them to Prometheus metrics.An exporter,just like an instrumented service,exposes these metrics through an endpoint,and can be scraped by Prometheus.A large variety of exporters is already available.If you want to monitor third-party software that does not have an exporter publicly available,you can write your own custom exporterScraping the TargetsPulling metrics from instances is called scraping.Scraping is done at configurable intervals by the Prometheus server.Prometheus allows you to configure jobs that fetch time series from instances.global:  scrape_interval: 15s # Scrape targets every 15 seconds  scrape_timeout: 15s # Timeout after 15 seconds  # Attach the label monitor=dev-monitor to all scraped time series scraped by this server  labels:    monitor: 'dev-monitor'scrape_configs:  - job_name: \"job-name\"    scrape_interval: 10s # Override the default global interval for this job    scrape_timeout: 10s # Override the default global timeout for this job    target_groups:    # First group of scrape targets    - targets: ['localhost:9100', 'localhost:9101']      labels:        group: 'first-group'    # Second group of scrape targets    - targets: ['localhost:9200', 'localhost:9201']      labels:        group: 'second-group'This configuration file is pretty self-explanatory.You can define defaults for all jobs in the global root element.These defaults can then be overridden by each job,if necessary.A job itself has a name and a list of target groups.In most cases,a job has one list of targets (one target group),but Prometheus allows you to split these between different groups,so you can add different labels to each scraped metric of that group.Next to your own custom labels,Prometheus will additionally append the job and instance labels to the sampled metrics automatically.Visualization and AnalyticsPrometheus has its own dashboard,called PromDash,but it has been deprecated in favor of Grafana.Grafana supports Prometheus metrics out-of-the-boxand makes setting up metrics visualization effortless.After adding a Prometheus data source,you can immediately start creating dashboards using PromQL:                  Step 1: Create datasource                        Step 2: Profit      Alert! Alert!Prometheus provides an Alert Manager.This Alert Manager is highly configurable and supports many notification methods natively.You can define routes and receivers,so you have fine-grained control over every alert and how it is treated.It is possible to suppress alerts and define inhibition rules,so you can prevent getting thousands of the same alert if a many-node cluster goes down.Alerts can be generated by defining alerting rules.This is done in Prometheus and not in the Alert Manager.Here are a few simple alerting rule examples:# Alert for any instance that have a median request latency &gt;1s.ALERT APIHighRequestLatencyIF api_http_request_latencies_second{quantile=\"0.5\"} &gt; 1FOR 1mLABELS { severity=\"critical\"}ANNOTATIONS {  summary = \"High request latency on {{ $labels.instance }}\",  description = \"{{ $labels.instance }} has a median request latency above 1s (current value: {{ $value }}s)\",}ALERT CpuUsageIF cpu_usage_total &gt; 95FOR 1mLABELS { severity=\"critical\"}ANNOTATIONS {  summary = \"YOU MUST CONSTRUCT ADDITIONAL PYLONS\"  description = \"CPU usage is above 95%\"}After an alert is generated and sent to the Alert Manager,it can be routed using routes.There is one root route on which each incoming alert enters,and you can define child routes to route alerts to the correct receiver.These routes can be configured using a YAML configuration file:# The root route on which each incoming alert enters.route:  # The default receiver  receiver: 'team-X'  # The child route trees.  routes:  # This is a regular expressiong based route  - match_re:      service: ^(foo|bar)$    receiver: team-foobar    # Another child route    routes:    - match:        severity: critical      receiver: team-criticalThere are multiple types of receivers to which you can push notifications to.You can push alert notifications to SMTP,HipChat,PagerDuty,PushOver,Slack and OpsGenie.Additionally,you can use a web hook to send HTTP POST requests to a certain endpoint with the alert as JSON,if you wish to push notifications to somewhere else.Check out this guy’s audio alarm,which alerts him when his internet goes down!The receivers are configured in the same YAML configuration file:receivers:# Email receiver- name: 'team-X'  email_configs:  - to: 'alerts@team-x.com'# Slack receiver that sends alerts to the #general channel.- name: 'team-foobar'  slack_configs:    api_url: 'https://foobar.slack.com/services/hooks/incoming-webhook?token=&lt;token&gt;'    channel: 'general'# Webhook receiver with a custom endpoint- name: 'team-critical'  webhook_configs:    url: 'team.critical.com'Monitoring Time!Do you wish to get your hands dirty quickly with Prometheus?Perfect!I have prepared a project for demonstration purposes,which can be found on the Ordina JWorks GitHub repository.The project can be set up using only one command,leveraging Docker and Make.It covers most of the features discussed in this blog post.First clone the project with Git:$ git clone git@github.com:ordina-jworks/prometheus-demo.gitAfter the project is cloned,run make in the project directory:$ makeThis will compile all applications,build or pull all necessary Docker images,and start the complete project using Docker Compose.The following containers are started:$ docker psCONTAINER ID        IMAGE                              COMMAND                  PORTS                     NAMESc620b49edf4c        prom/alertmanager                  \"/bin/alertmanager -c\"   0.0.0.0:32902-&gt;9093/tcp   prometheusdemo_alertmanager_167b461b6a44b        grafana/grafana                    \"/run.sh\"                0.0.0.0:32903-&gt;3000/tcp   prometheusdemo_grafana_1920792d123bd        google/cadvisor                    \"/usr/bin/cadvisor -l\"   0.0.0.0:32900-&gt;8080/tcp   prometheusdemo_cadvisor_1215c20eb849b        ordina-jworks/prometheus-prommer   \"/bin/sh -c /entrypoi\"   0.0.0.0:32901-&gt;9090/tcp   prometheusdemo_prometheus_1f3cfc2f63f00        tomverelst/prommer                 \"/bin/prommer -target\"                             prometheusdemo_prommer_1574f14998424        ordina-jworks/voting-app           \"/main\"                  0.0.0.0:32899-&gt;8080/tcp   prometheusdemo_voting-app_166f2a00fcbcb        ordina-jworks/alert-console        \"/main\"                  0.0.0.0:32898-&gt;8080/tcp   prometheusdemo_alert-console_14fd707d4e80c        ordina-jworks/voting-generator     \"/main -vote=cat -max\"   8080/tcp                  prometheusdemo_vote-cats_15b876a131ad0        ordina-jworks/voting-generator     \"/main -vote=dog -max\"   8080/tcp                  prometheusdemo_vote-dogs_1As you can see,a lot of containers are started!You can view the public ports of the containers in this list,which you need to access the applications.The project consists of the following components:  Prometheus which scrapes the metrics and throws alerts  Grafana to visualize metrics and show fancy graphs  Alert Manager to collect all alerts and route them with a rule based system  cAdvisor which exposes container and host metrics  Prommer, a custom Prometheus target discovery tool  An alert console which displays the alerts in the console  A voting application which registers and counts votes  A voting generator which generates votesThe voting application exposes a custom metric called voting_amount_total.This metric holds the total amount of votes and is labeled by the type of vote,e.g. voting_amount_total{name=dog}.An alerting rule is configured in Prometheus that checks for the amount of votes.Once it passes a certain threshold,the alert is fired.This alert is sent to the Alert Manager,which in turn routes it to the custom alert console through a webhook.Inactive alertThe alert is firedThe alert console logs the JSON body of the POST request from the Alert Manager.We can check the output of these logs using Docker Compose:$ docker-compose logs -f --tail=\"all\" alert-consoleAttaching to prometheusdemo_alert-console_1alert-console_1  | {\"receiver\":\"alert_console\",\"status\":\"firing\",\"alerts\":[{\"status\":\"firing\",\"labels\":{\"alertname\":\"TooManyCatVotes\",\"instance\":\"172.19.0.5:8080\",\"job\":\"voting-app\",\"name\":\"cat\",\"severity\":\"critical\"},\"annotations\":{\"summary\":\"Too many votes for cats!\"},\"startsAt\":\"2016-09-22T17:09:22.807Z\",\"endsAt\":\"0001-01-01T00:00:00Z\",\"generatorURL\":\"http://215c20eb849b:9090/graph#%5B%7B%22expr%22%3A%22votes_amount_total%7Bname%3D%5C%22cat%5C%22%7D%20%3E%20100%22%2C%22tab%22%3A0%7D%5D\"}],\"groupLabels\":{\"alertname\":\"TooManyCatVotes\"},\"commonLabels\":{\"alertname\":\"TooManyCatVotes\",\"instance\":\"172.19.0.5:8080\",\"job\":\"voting-app\",\"name\":\"cat\",\"severity\":\"critical\"},\"commonAnnotations\":{\"summary\":\"Too many votes for cats!\"},\"externalURL\":\"http://c620b49edf4c:9093\",\"version\":\"3\",\"groupKey\":1012006562800295578}GrafanaThe default credentials for Grafana are admin:admin.After logging in,you must first configure a Prometheus data source.Prometheus is available at http://prometheus:9090 (from within the container).                  Configuring the data source                        Visualizing metrics      cAdvisorcAdvisor also has a simple dashboard which displays most important host and container metrics.Since Prometheus scrapes cAdvisor,these metrics are also available from Grafana.                  Network Throughput                        CPU Usage per Core      Final WordsJust a few months ago,the Prometheus team joined the Cloud Native Computing Foundation.  Today, we are excited to announce that the CNCF’s Technical Oversight Committee voted unanimously to accept Prometheus as a second hosted project after Kubernetes!You can find more information about these plans in the official press release by the CNCF.  By joining the CNCF, we hope to establish a clear and sustainable project governance model, as well as benefit from the resources, infrastructure, and advice that the independent foundation provides to its members.Cloud Native Computing Foundation (CNCF) is a nonprofit, open standardization organisation which commits itself to advance the development of cloud native technologies,formed under the Linux Foundation.It is a shared effort by the industry to create innovation for container packaged, microservices based, dynamically scheduled applications and operations.Prometheus has proven itself to be worthy to be an industry standard in alerting and monitoring.It offers a wide-range of features,from instrumenting to alerting,and is supported by many other tools.If you are looking for a monitoring tool,definitely give it a shot!"
      },
    
      "testing-2016-09-16-automation-testing-with-postman-html": {
        "title": "API Testing with Postman and Newman",
        "url": "/testing/2016/09/16/Automation-testing-with-postman.html",
        "image": "/img/postman.png",
        "date": "16 Sep 2016",
        "category": "post, blog post, blog",
        "content": "PrerequisitesFor the purpose of this tutorial it is mandatory to have Postman installed which is available has native apps for Windows, OS X and Linux.It is also mandatory to create an account at Algorithmia.Creating and selecting an environmentPostman’s environment functionality makes it very easy to switch between different environments.A set of variables can be configured per environment and when switching from one environment to another one these will be replaced accordingly.For example let’s create an environment called “production”.Click the “No environment” dropdown in the header and select “Manage environments”.Select the “Add” button on the popup that is presented to you.Add url https://api.algorithmia.com/v1/algo/ and key simNz9pf7hfAQNifdA224K1GFhs1.Don’t forget to replace the secret by your own key.Finally select the “Production” environment in the environment dropdown and let’s create our first request.Creating a POST requestEnter {{url}}/WayneS/Calculator/0.1.0 in the request field and change the method from GET to POST.We need to add some additional headers as well so add Content-Type application/json, Authorization Simple {{key}}.As you can see,we are using the environment variables {{url}} and {{key}} so when switching environments,those variables will get replaced.The {{...}} format can only be used in the request URL/URL params/Header values/form-data/url-encoded values/Raw body content/Helper fields.Postman also has a few dynamic variables which you can use. For example, {{$guid}} is generating a random v4 style guid,{{$timestamp}} is the current timestamp,{{$randomInt}} a random integer between 0 and 1000.More of those will be added in future releases.But for now,let us just simply enter \"x=log(2)\" as the raw content of our request.Finally let’s hit the “Send” button and if everything goes as expected,we should receive the following response.Next we are going to write our test, but first let us save our request into a collection. By clicking on the create collection button on the collections tab, the following popup will be displayed.  Simply enter “Calculator” as the name of the collection and hit the create button.Now hit the “Save” button next to the request field. Enter “Log” as the name of the request and select “Calculator” from the dropdown menu.Writing a testA Postman test is essentially JavaScript code which sets values for the special ‘tests’ object. To know which other objects and libraries are available while writing your test cases, make sure you check the following link. Let’s copy following code snippet in the Tests sandbox.tests[\"Status code is 200\"] = responseCode.code === 200;var jsonData = JSON.parse(responseBody);tests[\"Verify result\"] = jsonData.result.x === \"0.69314718056\";The test will run each time you hit the “Send” button. Let’s say we need a custom function to set some variables,this can easily be achieved in the pre-request sandbox as shown below:Here we are using the ‘postman’ object and are calling the setEnvironmentVariable function on it,this allows us to assign the result of our function to a variable on the environment scope for later use.Collection RunnerLet’s assume we want to run several tests at once.Postman has a Collection Runner utility that allows us to just do that,even thousands of tests if we want.To access the runner click on “Runner” in the top header then select “Calculator” as the collection and “Production” as the environment.We want the runner to do that 2 times so enter 2 in the iteration inputfield like shown in the screenshot below.Scroll down and hit the blue “Start Test” button. Following test report will be presented to you.Writing a request and tests for each different permutation of data could get tiresome and tedious.On the test runner screen we are given the option to choose a data file.This data file can be either a CSV or a JSON file,but will allow us to set up data in bulk to be run through the test runner.Create a new csv file and copy following snippet into it.input,expected_result2,\"0.69314718056\"224,\"5.41164605186\"3000,\"8.00636756765\"388949,\"12.8712035086\"We need to rewrite the body of our request so it will use the variable of our csv as follows.We also need to rewrite our test.Like you can see we use the ‘data’ object to call our expected_result variable.Back to the runner window.Select the “Calculator” collection and the “Production” environment.Click the “Choose Files” button and select the csv file you just created,click the “Preview” button to check for any inconsistenties.As there are 4 entries in our csv we want to use to feed our test enter 4 in the iteration inputfield.Hit the “Start Test” button and you will now see 12 green tests.Pretty neat, isn’t it?NewmanIntegrating Postman tests with build systems can easily be accomplished with Newman. Newman is the command line tool companion for Postman. It can be installed through the Node.js package manager, npm. You’ll find more information on how the install Newman here.After Newman is installed we can export our previously created collection and environment.Select the ‘Calculator’ collection and hit export and save as my_collection.json.To export the ‘Production’ environment select ‘Manage Environment’ and on the next popup hit export and save as ‘prod_environment.json’.Now run you test with Newman using following command where my_collection.json is the exported collection,my_data.csv the csv, prod_environment.json the environment and -n the number of lines from our csv.newman run my_collection.json -n 4 -d my_data.csv -e prod_environment.jsonSummaryIn this tutorial we saw how to create a request and a test.We saw how to create a collection and how to run it with the collection runnner and Newman.I hope you enjoyed this tutorial and if you have any question feel free to add these as a comment or to email me at gregory.rinaldi@ordina.be.Useful links  Importing Swagger files  Postman Slack invite  Importing cURL commands  Creating cURL commands  Making SOAP requests  Running Newman in Docker  Authentication helpers  Publish Documentation for your Collections  Conditional Workflows in Postman (work in progress)  Newman  Integrating Newman with Jenkins "
      },
    
      "microservices-2016-09-12-microservices-dashboard-1-0-1-html": {
        "title": "Microservices Dashboard",
        "url": "/microservices/2016/09/12/Microservices-Dashboard-1.0.1.html",
        "image": "/img/microservices-dashboard.png",
        "date": "12 Sep 2016",
        "category": "post, blog post, blog",
        "content": "So you’ve jumped on the hype train, built a bunch of microservices, and got your first releases under your belt. Now what?Our experiences taught us this is the easy part.With the newly obtained microservices freedom, teams easily plunge into a world of cowboys and unicorns.The big ball of mud is just around the corner.Panic, mayhem and chaos loom over the organisation, waiting for everything to spin out of control.Especially for any enterprise not residing in Silicon Valley, maintaining some sort of governance and compliancy is essential.What does a microservice architecture mean not just for the developers, but also for analysts and managers?What can we as developers do to offer them peace of mind?Managers like to have a grip on thingsThey want to get a sense of compliancy and maturity of the components part of the ecosystem.In theory a microservices architecture gives developers complete freedom to use whatever tools and frameworks they want inside their microservice.In practice, managers often want to slightly restrict that freedom to avoid complete chaos.It’s not uncommon for managers and architects to impose a set of choices developers can choose from, and goals the teams have to achieve.In order to facilitate recruitment and knowledge transfer, developers could be forced to choose between for instance Java or Javascript.Similarly, architects might enforce every microservice to have a quality gate in place and to have a technical debt less than five days.Aside from the technical aspects inside a microservice, compliancy is even more crucial at the contract level.They should be defined according to an architectural vision and comply to standards across the organisation.Having the ability to track these compliancy regulations and quality assurances is a key enabler for management to push for technical excellence.Too often managers are left clueless on how much effort is required to mature the architecture and which teams they have to chase.Having a dashboard at their disposal indicating where a lack of compliancy and maturity needs their attention can help to ensure budget and priorities are in line with the architectural goals.Aside from compliancy and maturity, managers want some level of change management in place.Oftentimes this is achieved through ticketing systems and cumbersome processes.A microservice architecture goes hand in hand with devops, including full automation and decoupling.In that respect, teams ought to be able to define their own release schedule as there is no need for a waterfall manual testing effort of months on end, and the impact on the ecosystem is contained and managed due to the decoupled nature of microservices.Change management in a devops organisation is much more a read-model instead of a process-heavy model.Managers want to know what is currently out there and what will be out there in the future.This doesn’t require a manual ticketing system, simply a smart dashboard with a timeline.Analysts need to know what functionality is out thereIn order to reuse functionality and avoid duplication, functional analysts have a strong need for an overview of the current functional landscape.Knowing which resources are exposed by what microservices, and which events and messages are being sent back and forth between microservices and queues, can go a long way in helping analysts understand the state of the architecture.Furthermore, impact analysis can significantly improve when an overview of components and how they are linked together is available to the analysts.Not only does it encourage analysts to identify and inform consumers of a changing service, it can help to avoid introducing breaking changes due to negligence or ignorance.During troubleshooting, testers and analysts should be able to find out what services and backends are involved in a certain functional flow.Just like managers, functional analysts are interested in upcoming features and releases.On top of that, analysts can benefit from being able to define the future state of the ecosystem.Especially when multiple teams are working on similar functionality, it can be notoriously difficult to avoid duplication and breaches of bounded contexts.Using a dashboard to define what is coming up, can help to give them an unambiguous view of the current and future landscape.Developers can benefit from a broader view as wellIn a devops organisation, developers have the responsibility to not only build but also run their services.Knowing which versions are deployed where, can assist developers in verifying whether their deployments are successful, but also to determine the versions of their dependencies.A graphical dashboard can go a long way in providing clarity to developers.But most of all, it can act as a hub for other tools and documentation available.Integrations can be made with for instance API documentation, performance tooling, service registries, in-depth instance-specific dashboards and perhaps even reactive insights.The introduction of the Microservices DashboardVisualising the state of the architecture and dependencies in the system can be a huge benefit to all stakeholders in the IT organisation.The Microservices Dashboard is a brand new open source project, which officially launched its first major release at Spring One Platform.Building on top of Spring Boot and Spring Cloud, it visualises your microservice architecture and integrates with tools every microservice architecture benefits from.This ranges from consumer-driven-contract testing over service discovery to hypermedia traversal and more.Microservices Dashboard is a simple application to visualize links between microservices and the encompassing ecosystem.This AngularJS application consumes endpoints exposed by its server component.It displays four columns: UI, Resources, Microservices and Backends.Each of these columns show nodes and links between them.The information for these links come from Spring Boot Actuator health endpoints, Pact consumer-driven-contract tests and hypermedia indexes, which are aggregated in the microservices-dashboard-server project.The architectureThe dashboard currently consists out of an AngularJS 1.x application which communicates over HTTP to a Spring Boot application.The frontend uses D3.js to visualise the nodes in the four columns.We are currently in the process of completely rebuilding the frontend stack.Next version will be running on Angular 2, Typescript and EcmaScript 6.Most of D3.js will be taken care of by Angular 2 itself.Thanks to this refactor we’ll see the introduction of RxJS, making the frontend much more reactive in nature.This aligns our frontend and backend components goals, since the backend is already running RxJava.Our efforts currently focus on replicating all functionalities currently available in the dashboard, albeit with much more attention to quality and testing.Subsequently new features and enhancements will be built on top of a much more mature and extendible frontend application.Our server component is powered by Spring Boot’s auto configuration.It’s a library which, once on the classpath of a regular Spring Boot application, will automatically transform the Spring Boot application into a JSON graph-serving engine.It does so by using aforementioned RxJava.The idea of the server application is that it will fetch information from the microservices ecosystem, with for instance Spring Cloud’s integration of service registries, and collect details of components and their relation within said ecosystem.Needless to say collecting this information requires a lot of outbound calls, and can pose a serious performance burden in case the landscape gets bigger.Making intelligent use of the system’s resources is absolutely necessary, and RxJava does just that.In the future we might migrate to Spring’s Reactor which has a more formal integration of the ReactiveX specification and better integration with Spring itself.Once the frontend’s revamp is completed, the last step towards an end-to-end reactive flow is the HTTP connection between both components.Currently the server still converts the Observable to blocking, undoing a lot of the performance gains we could achieve.Yet even while eventually blocking, we’ve benchmarked a thirty percent performance gain in switching from CompletableFutures to Observables thanks to the more sustained async handling.Aside from its reactive nature, the server component of the dashboard is also built in a very pluggable way.Information is retrieved from the ecosystem through so-called aggregators.Currently four aggregators are provided: the health-indicators aggregator, the index aggregator, the mappings aggregator and the Pact aggregator.We’re looking into supporting Spring Cloud’s recent addition, Spring Cloud Contract, as a source for aggregation.New aggregators can be easily added, and all existing aggregators can be overridden, extended, turned on and off.In the next section we’ll go through these aggregators and their purpose.Collecting information from the ecosystemThe dashboard on its own doesn’t really make a lot of sense when it’s not connected to the architecture it’s supposed to visualise.Aggregators pull in information which eventually gets translated into nodes and links on the dashboard.Health-indicators aggregatorSpring Boot exposes production-ready endpoints through its Actuator module.The health endpoint returns information regarding the current health status of the application.The source of this information is a bunch of health indicators, describing various components and dependencies of the application.For instance, an application can have a dependency on a database, for which a health indicator will usually provide health information to the health endpoint, indicating whether the database is up and the connection pool hasn’t been depleted.Hence, health indicators describe an up-to-date relationship between the application they run on and its dependencies.Spring Cloud ensures health indicators are automatically enabled when you are using service discovery, circuit breakers, a config server or other Spring Cloud services. However, health indicators don’t automatically describe a relationship between an application and another application it calls.Luckily Spring Boot has a very easy way of adding custom health indicators.As such, developers can add a health indicator the moment a remote service call is added to the application.  What about real-time?  Using health indicators we are certain the application calls another application programmatically.This provides clarity in terms of the calls in the code and therefore the dependencies that exist across the applications.However, using this method we aren’t sure whether this remote call is actually being executed at runtime.These concerns are currently provided by other tools such as Twitter’s Zipkin.In the future we will integrate the dashboard with real-time traffic information from Zipkin or similar tooling.Index aggregatorREST over HTTP is arguably the most popular communicational style in microservices architectures.Therefore, gathering information on where and how REST is used can be quite useful.Index and mappings aggregators perform this specific task, albeit each in a different way.The index aggregator relies on a subconstraint of REST called HATEOAS.It stands for Hypermedia As The Engine Of Application State, and describes the idea of adding links in the payload of responses to other resources.This enables discovery of resources, much like we are using the world wide web from its inception.It prevents the need to bookmark URIs to resources, decoupling implementations and enabling independent evolution of the service.Similarly to a regular website, REST APIs using HATEOAS require a homepage or index from which the resource discovery starts.Simply creating an index resource with links to the other resources the service provides, and exposing this index resource at the root of the application takes care of this.Spring HATEOAS provides useful tools to add links to resources.Once every microservice has an index resource, we can use service discovery to discover all the services, and fetch all the index resources to map out the landscape of resources.This is an excellent source for the dashboard, as it shows the relation between microservices and the RESTful resources they expose.Mappings aggregatorOftentimes, RESTful resources are exposed in a more traditional way (using REST level 2) without the added complexity of HATEOAS.While this is not fully REST compliant, it is most common among APIs using JSON over HTTP.Spring Boot offers a handy endpoint in their Actuator module, called the mappings endpoint.It describes all the resources exposed by the application when Spring MVC REST is used.While also describing Spring’s own resources, a simple filter allows us to deduct node and link information from these endpoints to visualise in the dashboard.Pact aggregatorIn a microservices architecture, testing is absolutely crucial.As the primary benefit of microservices is faster time-to-market, changes happen all the time.Not only unit and integration testing is required, but also more advanced contract testing to act as a safety net.Consumer-driven-contract testing allows the consumer (the client) to define what he expects from the producer (the service), and ensure the producer validates that definition every time a change is made to the service.This allows the consumer to rest at ease, knowing the producer will remain backwards compatible or version accordingly, and gives the producer knowledge of who uses exactly which parts of its service.The latter gives the producer the chance to request consumers to update their service in case they are causing too much complexity on the producer’s side due to backwards compatibility.Tests like these document with guaranteed certainty relations between clients and services or services and services.Querying the contracts that define these relations offer a great source of information for the dashboard’s nodes and links between them.When working with the consumer-driven-contract testing framework Pact, a repository called the Pact-broker holds all the available contracts and exposes them through a REST interface.Our Pact aggregator makes use of this interface to pull the information into the dashboard.Spring Cloud recently added the Spring Cloud Contract module to their portfolio, based on Accurest.We’re planning to integrate the Microservices Dashboard with Spring Cloud Contract in the near future as well.ConclusionThe Microservices Dashboard gives managers, analysts and developers peace of mind when working in a microservices architecture.Not only does it map relations between components in a visually attractive manner, it can also be a great tool for compliancy, change management, functional analysis and troubleshooting.The dashboard is currently at version 1.0.1, and can be downloaded through maven central.To quickly get up and running, make sure to check out the reference documentation.Since the project is still fairly new, any feedback is greatly appreciated.You can reach us through Gitter or GitHub."
      },
    
      "conference-2016-08-09-s1p-html": {
        "title": "SpringOne Platform",
        "url": "/conference/2016/08/09/s1p.html",
        "image": "/img/s1p.jpg",
        "date": "09 Aug 2016",
        "category": "post, blog post, blog",
        "content": "  SpringOne Platform is the successor of SpringOne 2GX, with a focus mainly on Spring and Cloud Foundry. Next to these technical topics, SpringOne Platform also offered many sessions on cultural transformation and DevOps. Cultural transformation and DevOps are key to deliver meaningful solutions more quickly. This can be achieved by creating empowered teams, able to make independent decisions. To implement these collaborative teams, leadership buy-in is hugely important. Getting away from legacy thinking will allow enterprises to obtain short feedback cycles and thus continuously improve.From Imperative To Reactive Web Apps - Rossen StoyanchevThe marquee feature of Spring 5, will be first class support for Reactive Web applications. Reactive programming is about non-blocking, event-driven applications with back-pressure. Back-pressure helps to ensure a good collaboration between producers and consumers. The Reactive Manifesto is an interesting read on this topic.To support reactive, Spring 5 will use Project Reactor (led by Stéphane Maldini) through the Spring Web Reactive project. This blogpost, by Rossen Stoyanchev is a nice starting point to learn about Flux, Mono and the Spring Reactive world.Managing Secrets at Scale - Mark PaluchIn a world, where we run large amounts of microservices in orchestrated containers, we can never forget about security, encrypting passwords, storing keys, rotating secrets, etc. Today, applications consume both first and third party APIs and need authentication and authorization to do this in a safe way. Traditional patterns cannot keep the security bar high with dynamic deployment scenarios.As a Secure-By-Design company, this talk immediately caught my attention. In a Spring world, we can use Spring Cloud Vault Config, wrapping Vault. An interesting tutorial on this Spring library is available on spring.io.Slides from the talk are available online.It’s not you, it’s us: Winning over people for yourself and the team - Neha BatraThis was one of the non-technical talks, but aimed to help with the daily management of technical projects. Neha’s session was the most interactive one I attended at SpringOne Platform: about 10 minutes in the session, she wanted us to pair to do a personal SWOT analysis with a stranger in the room and see how we can learn from each other. Everyone participated and I believe this might actually be useful in the context of a project. Something to try out!She ended her session with a tool chest to prevent and mitigate issues as they come up:  SWOT analysis  Personal goals  Inception  Set schedule / cadence  Provide feedback  Provide a “safe haven”  Collect and discuss concerns  Talk in person  Write down useful conversations  Find a way to align first (eg. TDD + pair programming)  Daily retrosThe slidedeck of her talk is available on Slideshare.The five stages of Cloud Native - Casey WestAnother non-technical talk, from the talented and funny Casey West, on how companies are adopting the Cloud Native approach to software development.The talk was very entertaining and resonated with the audience to such an extent, that there was constantly someone laughing. The slides itself don’t say much without explanation so I’ll try to clarify them a bit here.Analogous to the Kübler-Ross model, there are five stages when adopting Cloud Native development:Denial@caseywest immediately grasps the crowd’s attention with these very familiar quotes:  “Containers are just tiny virtual machines”No they’re not. Stop treating them as such. Moving a huge application or database from a virtual machine to a container doesn’t really solve anything.  “We don’t need to automate Continuous Delivery because we already automate our infrastructure with Puppet”The problem is that these measures are not enough and they don’t solve enough of the problem. Managing infrastructure and deploying applications using Puppet scripts already is a great improvement by treating Infrastructure as Code but it still requires too much manual labour.AngerAgain, the goal of these funny quotes is prove a very valid point:  “It works on my machine”The following quote isn’t in the slides but might also sound familiar:  “Let me do a hotfix, I can figure it out”and my favourite:  “DEV is just YOLO-ing sh#t to production”These illustrate the problems, you are likely to get when trying to develop Cloud Native applications without the proper culture in place.This is a clear breakdown in communication and is more a people problem than an IT problem.It just doesn’t work, especially when also considering the compliance or legal aspect. There is a lack of acknowledgement that we need roles and responsibilities.Bargaining  “What if we create microservices that all talk to the same datasource?”Single data model and data ownership are not possible this way.  “We crammed this monolith in a container and called it a microservice”Applications need to adhere to some restrictions to run and scale in the cloud, otherwise you cannot take advantage of the benefits of a platform.Often, there also is the notion of something called bi-modal IT.This is basically dividing your company up into sad mode vs awesome mode.A lot of organizations believe they don’t need to change and prefer to stay in sad mode, they use bi-modal IT as an excuse. Honestly, nobody really wants to work in sad mode.DepressionOnce people actually start creating Cloud Native applications, the depression kicks in:  “We created 200 microservices and forgot to setup Jenkins”A common mistake is not to go for a fully automated CI/CD pipeline from the start.This should be your first action when you start a new project. It is necessary to automate your path to production.  “We have an automated build pipeline but release twice a year”When business is not on board with rapid, iterative delivery, you will never get the desired fast feedback loops.AcceptanceFinally, everyone start realizing the painful, but obvious truth:  All software sucksby which he means that creating software is not easy and will never become easy. But we can try to make it as easy as possible for ourselves.Casey also advises us to respect the CAP theorem, respect Conway’s Law and automate everything.Also, don’t expect to get all of these things right from the start. Taking baby steps and improving gradually over time is certainly possible. An example is to put a monolith inside a container and start breaking it up into more manageable pieces.&amp;TLDR;The (very pretty) slides can be found on Slideshare.OrdinaOrdina was represented at SpringOne Platform with 2 speakers and 3 talks:Writing your own Spring Boot Starter - Dieter HubauDieter Hubau gave a very nice introduction on how to write your own Spring Boot Starter. A Spring Boot Starter is the de-facto standard tool for starting with a greenfield Spring project. He started by explaining the magic behind Spring Boot Starters (and @AutoConfiguration) and ended with a cool game of Josh Long Pokemon, deployed on Cloud Foundry.His slides are available here.Writing Comprehensive and Guaranteed Up-to-date REST API Documentation - Andreas EversAndreas Evers talked about Spring REST Docs to generate documentation that is always up to date. To achieve this, a test-driven approach can be used: generate snippets from integration tests. Combine these snippets with manually written templates and finally generate HTML. Personally, I have always been a huge fan of “documentation-as-code” and Spring REST Docs is a great tool to achieve this goal.His slides are available here. This blogpost by Kevin Van Houtte provides more insight and examples on Spring REST Docs.Ignite: Microservices Dashboard - Andreas EversOn Monday evening, Andreas pitched the Ordina Microservices Dashboard that was released a couple of hours earlier. The Ordina Microservices Dashboard left a big impression:Definitely worth checking out. Expect an in-depth blogpost here soon!Simplifying the Future - Adrian CockroftThe closing keynote at SpringOne Platform was reserved for one of the most influential people in our industry: Adrian Cockcroft. Always at the edge of technology, Adrian often is credited with making Microservices a mature and useful architectural pattern. His talk focussed on:  Simplifying work  Simplify the organization  Simplify things we buildI really recommend watching his entire presentation on YouTube."
      },
    
      "security-2016-07-25-web-of-trusted-commits-html": {
        "title": "A web of trusted commits",
        "url": "/security/2016/07/25/Web-of-trusted-commits.html",
        "image": "/img/digitally-signing-your-json-documents.png",
        "date": "25 Jul 2016",
        "category": "post, blog post, blog",
        "content": "Who Do You Trust?When you’re building software with people from around the world, it’s important to validate that commits and tags are coming from an identified source. By using a distributed revision control system like Git, anyone can have an offline copy of your project’s code repository. In theory having a central repository is not necessary, but it can be used to provide an “official” source from which other developers can clone from and work on. These other floating repositories may contain malicious code because, unfortunately, it is remarkably easy to fake your identity when committing code using Git.The following command allows any individual with bad intentions to commit (malicious) code under your name, meaning that you will get the blame for the backdoor or exploit “you” committed:  # Individual commit.  $ git commit -a -m \"a message\" --author \"Sherlock H. &lt;sherlock.h@bakerstreet.org&gt;\"  # Global settings.  $ git config --global user.name 'Sherlock H.'  $ git config --global user.email sherlock.h@bakerstreet.org  Ensuring TrustThis blog post tells the story of Sherlock H. Sherlock is a witty developer who holds any security-related topic very close to his heart. After a fair amount of pondering about how he could solve the problem of black-hearted developers impersonating his personality, he decided to add a Digital Signature to his commits. By adding a signature Sherlock can finally sleep soundly at night because the signature indicates that he really issued the commit and that it has not been tampered with since he sent it. Moreover it can be used to trace the origin of malicious code that has made its way into a repository. The signature also assures non-repudiation, meaning that it becomes difficult for the signer to deny having signed something because the Digital Signature is unique to both the commit and the signer, and binds them together. Sherlock can now wholeheartedly vouch for the commit.Consider the following scenario:  Sherlock wants to send an urgent message to his fellow developer John W. telling that their application has been compromised by Jim M, a criminal mastermind who only has unkind intentions. John wants the guarantee that the message he received is sent by Sherlock and has not been tampered with by Jim.In order to securely exchange messages, both Sherlock and John will make use of their Key Pairs. A Key Pair consists of a Public and Private Key which are two unique mathematically related cryptographic keys. As its name suggests, the Public Key is made available to everyone by handing out copies or sharing them through a publicly accessible repository. The Private Key however must be kept confidential to its respective owner.Sherlock and John can do the following with the use of their Key Pair:  Signing          The message is still readable to everyone.      Guarantee of the sender’s identity (aka Sherlock).      Guarantee that the message has not been tampered with since it has been signed by the sender (aka Sherlock).        Encryption          The message is only readable by the designated recipient (aka John).      No guarantee of the sender’s identity (aka Sherlock).      Encryption can be done symmetrically by using a Shared Secret Key, a single key is then used for both encryption and decryption. Asymmetrical encryption (aka Public Key encryption) with a Public/Private Keypair uses one key for encryption and another for decryption. Note that the advantages and challenges of using either encryption type is beyond the scope of this blog post.      Enforcing TrustSherlock will combine a digital signature with encryption to convince John that his message is trustworthy.      Sherlock wants to send the following message to John: Data! Data! Data! I can’t make bricks without clay.. He calculates the Hash of this message by applying a publicly known hashing algorithm to the message. The calculated hash by using the SHA-256 hashing algorithm is d6ba26816599a75310c4c263126d4b44979c7026f90e1db8e9b317d6658f3811. The hash value is unique to the hashed data.        Sherlock encrypts the Hash with his Private Key. This encrypted Hash together with a certificate containing additional information about the sender forms the Digital Signature. The reason why the Hash is encrypted and not the entire message, is that a hash function can convert an arbitrary input into a fixed length value which is usually much shorter than the original message. This saves time since hashing is much faster than signing.        Sherlock sends the original message and its Digital Signature to John.        John receives the message and Digital Signature.        Whatever is encrypted with a Public Key can only be decrypted by using its corresponding Private Key and vice versa. Therefore John uses Sherlock’s Public Key to decrypt the Signature.        John also re-calculates the Hash of the original message by applying the same hashing algorithm as Sherlock.        John compares the Hash he calculated himself and the decrypted Hash received with Sherlock’s message.If they’re identical he knows the message has not been tampered with during transit.Should the message been compromised by Jim, then John would have calculated a different Hash than the encrypted Hash that Sherlock has sent along with his message.  Creating An IdentityIn order to sign his commits, Sherlock decided to use Gnu Privacy Guard (GPG) as his weapon of choice. GPG is a complete and free implementation of the OpenPGP standard. It allows to encrypt and sign data and communication, features a versatile key management system as well as access modules for all kinds of public key directories.      Download and install GPG from the official website        Open a command prompt        # Generate a new Key Pair.      $ gpg --gen-key            Sherlock accepted the default RSA and RSA key. RSA is a widely-used asymmetric encryption algorithm and is named after Ron Rivest, Adi Shamir and Len Adleman who invented it in 1977. Should you be interested in more mathematical details how this algorithm works, I can highly recommend watching “Public Key Cryptography: RSA Encryption Algorithm” on YouTube.        Enter the desired key size. I recommend the maximum key size of 4096 bits because they provide far better long-term security. While the default of 2048 bits is secure now, it won’t be in the future. 1024 bit keys are already considered within the range of being breakable and while technology advances 2048 bit keys will also become breakable. Eventually 4096 bit keys will be broken too, but that will be so far in the future that better encryption algorithms will also likely have been developed by then.        Sherlock accepted the default expiration for his key.        He entered his real name and email address. Sherlock provided the verified email address for his GitHub account. This will make it very easy to link his account with his Public Key.        Provide a secure passphrase. Choose wisely and be sure to remember it because else the key cannot be used and any data encrypted using that key will be lost.        Congratulations, a newly fresh Key Pair should be generated now.        # List all keys.      $ gpg --list-keys        pub   4096R/90C3C3DE 2016-07-24        uid     Sherlock H &lt;sherlock.h@bakerstreet.org&gt;        sub   4096R/586B3A7B 2016-07-24        Like many other developers, Sherlock is very active on GitHub and would like to link his Public Key with his account. He therefore will need to create a textual version of his Public Key. After having executed the command below, the content of the generated ‘pubkey.txt’ needs to be added to his account as described in the GitHub Help pages. More details about distributing and registering your Public Key to a key server can be found in the chapter ‘Distributing keys’ of the GPG Users Guide. For other usages like encryption and decryption, please refer to GPG’s Mini HowTo.    # Export the Public Key to a text file.    $ gpg --armor --output pubkey.txt --export 'Sherlock H'    Signing Your WorkOnce Sherlock generated his Key Pair, he can configure Git to use it for signing commits and tags. Following tools can be used to store a GPG key passphrase in a keychain so he doesn’t have to provide it every time he signs a commit: GPG Suite (Mac) or Gpg4win (Windows).  # Set the signing key by taking your Public Key id as parameter.  $ git config --global user.signingkey 90C3C3DE  # Automatically signs every commit.  $ git config --global commit.gpgsign true  # Manually sign a commit.  $ git commit -S -m \"some commit message\"  # Verify whether your commit has been signed.  $ git log --show-signature    commit 81314da640320c65896a4348842d303a754f37d2    gpg: Signature made Sun Jul 24 15:02:25 2016 CEST using RSA key ID 90C3C3DE    gpg: Good signature from \"Sherlock H &lt;sherlock.h@bakerstreet.org&gt;\"    Author: Sherlock H &lt;sherlock.h@bakerstreet.org&gt;    Date:   Sun Jul 24 15:01:52 2016 +0200  # Verify all signatures during merge. If the signatures can not be verified then merge will be aborted.  $ git merge --verify-signatures other_branch  Earlier this year GitHub announced that they now will show when commits and tags are signed and verified using any of the contributor’s GPG keys upload to GitHub. Keep your eyes open for commits and tags labeled with those green verified badges.Secure-By-DesignOrdina’s Secure-By-Design programme encourages to consider and take account of possible security risks as early as possible in a business process.So follow Sherlock’s example by embedding and safeguarding security in your daily work as a developer and Sign Your Work!Resources  GitHub’s Help on GPG  The GNU Privacy Handbook  GPG’s Mini HowTo  “A Git Horror Story” by Mike Gerwitz  “Public Key Cryptography: RSA Encryption Algorithm”"
      },
    
      "conference-2016-07-10-springio16-ddd-rest-html": {
        "title": "Spring I/O 16: Bridging the worlds of DDD &amp; REST",
        "url": "/conference/2016/07/10/SpringIO16-DDD-Rest.html",
        "image": "/img/springio.jpg",
        "date": "10 Jul 2016",
        "category": "post, blog post, blog",
        "content": "SpringIO 2016 in Barcelona was loaded with tons of interesting talks and workshops about Spring Cloud, Spring Boot, Spring Data, Microservices, REST &amp; HATEOAS, Reactive programming, and many many more.In this blogpost I will highlight Oliver Gierke’s 2 hour presentation about bridging the world of Domain Driven Design (DDD) and the world of Representational State Transfer (REST).Oliver Gierke (@olivergierke) is the lead of the Spring Data project at Pivotal and member of the JPA 2.1 expert group. He has been into developing enterprise applications and open source projects for over 10 years. His working focus is centered around software architecture, DDD, REST, and persistence technologies.Domain Driven DesignDDD is an approach to developing software that meets core business objectives by providing on the one hand tactical modeling tools which include well founded patterns and concepts such as entities, repositories and factories. On the other hand DDD also facilitates strategic principles and methodologies for analyzing and modeling domains such as Bounded Contexts and Context Maps.For an in depth understanding of DDD I highly recommend reading “Domain Driven Design - Tackling Complexity in the Heart of Software” by Eric Evans (@ericevans0). There’s also a short, quick-readable summary and introduction to the fundamentals of DDD made available by InfoQ.Oliver’s talk at SpringIO 2016 highlighted a few basic DDD concepts like Entities, Value Objects, Repositories, Aggregates and Bounded Contexts.Value Objects  Avoid Stringly typed codeValue Objects are vital building blocks of DDD. They are small immutable objects that encapsulate value, validation and behaviour. You can use them to group related values together and provide functionality related to what they represent, making implicit concepts explicit.Some common use cases for VOs are: EmailAddress, Money, ZIPCode, Status, … avoid writing these as just plain Strings!Writing VOs can be a cumbersome task but there are some source code generator frameworks out there like Project Lombok and Google’s AutoValue which can handle all the boilerplate code.Entities &amp; RepositoriesIn contrast to Value Objects which are identified by the attributes they carry, Entities are distinguished by their identity. Entity objects have a life cycle because their identity defines their responsibilities and associations. It is this unique identity and their mutability that sets Entities apart from Value Objects. This means that two Value Objects with the same properties should be considered the same whereas two Entities differ even if their properties match.  Aggregates form nice representation boundaries and become the key things to refer to.An Aggregate is a cluster of closely related entities that can be treated as a single unit. The common parent of that cluster is called an Aggregate Root. An example can be an Order and its Line Items, these will be separate objects but it is useful to treat the Order (the Aggregate Root) together with its Line Items as a single Aggregate.When trying to discover Aggregates, we should understand the model’s invariants. An invariant is a business rule that must always be consistent and usually refers to transactional consistency. When a transaction commits then everything inside the Aggregate should be consistent and any subsequent access by any client should return the updated value. In most cases it is a best practice to modify only one Aggregate in a single transaction. For updating multiple aggregates eventual consistency can be used. There will be an inconsistency window during which an access may return either the old or the new value but eventually all accesses will return the last updated value. The duration of the inconsistency window can be calculated based on factors like network delays, number of copies of the object, and the system load.A Repository is an abstraction over a persistence store for Aggregates. It acts like a collection by exposing methods to add and remove objects which encapsulate the actual interaction with the underlying data store. It also has elaborate query capabilities which return fully instantiated Aggregates whose attributes values meet the criteria.Bounded ContextDDD aims to create software models based on the underlying domain. A Bounded Context is the boundary that surrounds a part of a particular domain. This boundary isolates the model and language from other models and therefore helps reducing ambiguity and clarifying the meaning. When the boundaries are chosen well, greater decoupling between systems can be achieved which allows to easily change or replace the internals of a BC. Avoid having transactions across multiple BCs.The language that is structured around the domain model is called the Ubiquitous Language. It is important that this language is used by all team members (developers, analysts, business stakeholders, …) to connect all the activities of the team with the software. The vocabulary on its own does not have any relevance, it only has meaning inside a certain context. For example, an Item has a different meaning in the Orders BC than in the Products BC.Domain EventsA Domain Event is an extremely powerful tool in DDD. It is a type of message that describes something that has happened in the past and that is of interest to the business. (e.g. OrderShipped, CustomerBecamePreferred, …). It is important to model Event names and its properties according to the Ubiquitous Language of the BC where they originated. When Events need to be delivered to interested parties in either a local BC or broadcasted across BCs eventually consistency is generally used.Maturity LevelThe maturity level of the use of Domain Events can be categorized into 4 levels:  Level 0: no events at all          procedural code with just getters and setters      data just goes in and out        Level 1: explicit operations  Level 2: some operations as events          domain events are used as state transition      important domain events are exposed to interested parties via feeds        Level 3: event sourcing - all changes to application state are stored as a sequence of events          only event logs and snapshots are kept (Event Store)      separation of read and write operations (CQRS)      REST  REST ≠ CRUD via HTTP. Representation design matters.ResourcesJust like an Aggregate, a well designed Resource should be identifiable, referable and should have a clear scope of consistency.Exposing the core domain model directly via RESTful HTTP can lead to brittle REST interfaces because each change in the domain model will be reflected in the interface. Decoupling the core domain from the REST interface has the advantage that we can make changes to the domain and then decide in each individual case whether a change is needed in the REST interface and how to map it.Also avoid using HTTP PATCH or PUT for (complex) state transitions of your business domain because you are missing out on a lot of information regarding the real business domain event that triggered this update. For example, changing a customer’s mailing address is a POST to a new “ChangeOfAddress” resource, not a PATCH or PUT of a “Customer” resource with a different mailing address field value.This goes hand in hand with DDD’s concept of Event Sourcing because those state transitions are domain relevant events, not just some changes to the state of some object.HATEOASA RESTful HTTP client can navigate from resource to resource in two different ways. Firstly by being redirected as a result of sending data for processing to the server, and secondly by following links contained in the response of the server. The latter technique is called Hypermedia as the Engine of Application State or HATEOAS.The goal of Hypermedia is to serve not only data but also navigation information at the same time. This has a great impact on the client architecture because now we’re trading domain knowledge with protocol complexity. The client becomes dumber because it no longer needs to know business rules in a sense that its decisions are reduced to checking whether a link is present or not, e.g. whenever there’s a cancel link in the HTTP response, then display the Cancel button. This will make the client’s behavior more dynamic.On the other hand, the client becomes smarter because it needs to handle a smarter and more comprehensive protocol.Maturity levelIn analogy to the maturity level of Aggregates described earlier, Leonard Richardson’s model can be used to determine the maturity or our REST services.  Level 0: Swamp of POX          the HTTP protocol is used to make RPC calls without indication of the application state        Level 1: Resources          exposure of multiple URIs and each one is an entry point to a specific resource, e.g. http://example.org/orders, http://example.org/order/1, http://example.org/order/2      use of only one single method like POST.        Level 2: HTTP verbs          use of HTTP protocol properties (POST, GET, DELETE, …)      use of HTTP response codes, e.g. HTTP 200 (OK)        Level 3: Hypermedia controls          refer to description earlier in this blog post.      Translating domain concepts into web appropriate ones            DDD      REST                  Aggregate Root / Repository      Collection / Item Resource              Relations      Links              IDs      URIs              @Version      ETags              Last Modified Property      Last Modified Header      Sample implementationOliver also prepared a small sample implementation using Spring Boot, Spring Data and Lombok. The project is called Spring RESTBucks and is definitely worth checking out!Resources  “DDD &amp; REST” (slide deck used at SpringIO 2016) by Oliver Gierke  “Spring RESTBucks” (sample project used at SpringIO 2016) by Oliver Gierke  “Benefits of hypermedia” by Oliver Gierke  “Domain Driven Design - Tackling Complexity in the Heart of Software” by Eric Evans  “Implementing Domain Driven Design” by Vaughn Vernon  “Domain Driven Design Quickly” by InfoQ"
      },
    
      "conference-2016-06-30-springio16-spring-rest-docs-html": {
        "title": "Spring I/O 16: Test-driven documentation with Spring REST Docs",
        "url": "/conference/2016/06/30/SpringIO16-Spring-Rest-Docs.html",
        "image": "/img/springio.jpg",
        "date": "30 Jun 2016",
        "category": "post, blog post, blog",
        "content": "Spring IO 2016The main focus this year was definitely about cloud, reactive and microservices.But it is important not to forget other topics, like documentation! Keep calm, you don’t have to do it manually! Spring made it easy for us with Spring REST Docs! This year at Spring IO, Andy Wilkinson himself talked about why, how and when Spring REST Docs are being used. Last but not least, he talked about the new features that came out in version 1.1.Since I implemented Spring REST Docs in a project, I’ll use examples from my experiences.Andy WilkinsonAndy is a Spring Boot, REST docs committer and Spring IO platform lead at Pivotal. You can find him on Twitter using the handle @ankinson.Writing documentation is critical in the world of development. It is used to make an accurate and straight declaration and intent of what the service has to offer. Frontend developers will be able to know which endpoints they have to call and receive the right data. Now, we all know it's tedious for developers to write documentation...It's your lucky day! Spring REST Docs will make your life easier.While you are writing tests, Spring will generate a fully HTML api guide for you and your team. This blog post will take you through the best practices, how to and new features in 1.1.Why Test driven approach  It’s an accurate definition of your application (no side effects)  It describes the specific HTTP request and response  It’s straight forward without repetition  It’s easier to write (no annotations like Swagger)Markup languagesAsciidoctorAsciidoctor is a markup language that processes plain text and produces HTML, completely styled to suit your needs.If you are interested in writing in Asciidoctor be sure to check out the manual.Markdown (New in 1.1)With the newest version of REST Docs, the developer now has more options in terms of markup languages.The Markdown support is not as feature-rich as Asciidoctor, but Markdown can work very well when combined with existing documentation toolchains such as Slate.Here is a good sample working with slate.Andy’s pickAsciidoctor!Since Asciidoctor boasts more features than Markdown, it gives Asciidoctor the edge.Test ToolsWhen we want to use Spring REST Docs, we’ll have to use one of the test tools. Here are the different tools of choice. To use these tools we’ll have to initialise which document, Mockmvc and ObjectWriter we’ll be using.MockMvcA lightweight server-less documentation generation by the Spring Framework that has been the default use in Spring REST Docs.private MockMvc mockMvc;@Autowiredprivate WebApplicationContext context; @Before    public void setup() throws Exception {        this.document = document(\"{method-name}\");        mockMvc = MockMvcBuilders.webAppContextSetup(wac)                .apply(documentationConfiguration(this.restDocumentation).uris().withScheme(\"https\")).alwaysDo(this.document)                .addFilter(new JwtFilter(),\"/*\")                .build();        objectWriter = objectMapper.writer();        authToken = TestUtil.getAuthToken();        TestUtil.setAuthorities();    }RestAssured (New 1.1)As an alternative, you can use RestAssured to test and document your RESTful services. Available in V1.1, RestAssured will be more expandable than MockMvc.private RequestSpecification spec;@Beforepublic void setUp() {    this.spec = new RequestSpecBuilder().addFilter(            documentationConfiguration(this.restDocumentation))             .build();}Andy’s pickThis time he didn’t favor one but he mentioned that RestAssured gives you more functionality and extends your possibilities with HTTP.Snip, snip, snippets everywhere!Default SnippetSnippets are generated by the documented test method.Once you run the test method, you can add these snippets in your Markdown/Asciidoctor file. Be aware, these type of snippet will fail if you don’t have the correct response/request syntax.this.document.snippets(                links(                        halLinks(), linkWithRel(\"self\").description(\"The employee's resource\"),                        linkWithRel(\"employee\").optional().description(\"The employee's projection\")),                        responseFields(                                fieldWithPath(\"username\").description(\"The employee unique database identifier\"),                                fieldWithPath(\"firstName\").description(\"The employee's first name\"),                                fieldWithPath(\"lastName\").description(\"The employee's last name\"),                                fieldWithPath(\"linkedin\").description(\"The employee's linkedin\"),                                fieldWithPath(\"unit\").description(\"The employee's unit\").type(Unit.class),                                fieldWithPath(\"_links\").description(\"links to other resources\")                        )); mockMvc.perform(                get(\"/employees/1\").accept(MediaType.APPLICATION_JSON));Relaxed Snippet (New in 1.1)In contrast to default snippets, relaxed snippets don’t complain when something was neglected in the document.This is an advantage when you only need to focus on a certain scenario or specific part of the response/request.Reusable Snippet (New in 1.1)With the newly introduced reusable snippet, you can define a snippet at the beginning of your test class and reuse it every time you need it. When added to your test method, you can extend it with extra variables.// First we define a snippet for reuseprotected final LinksSnippet pagingLinks = links(        linkWithRel(\"first\").optional().description(\"The first page of results\"),        linkWithRel(\"last\").optional().description(\"The last page of results\"),        linkWithRel(\"next\").optional().description(\"The next page of results\"),        linkWithRel(\"prev\").optional().description(\"The previous page of results\"));// Then you perform the mockMvc and add the snippet to the document.// As you can see, it is expendable.this.mockMvc.perform(get(\"/\").accept(MediaType.APPLICATION_JSON))    .andExpect(status().isOk())    .andDo(document(\"example\", this.pagingLinks.and(             linkWithRel(\"alpha\").description(\"Link to the alpha resource\"),            linkWithRel(\"bravo\").description(\"Link to the bravo resource\"))));Type of Snippets:A snippet can be one of the following:Hypermedia linksWhen documenting your hypermedia application, you’ll have to define your links and where they go to. If you have dynamic links that can disappear at one time, you can use relaxed snippets or optional so it won’t complain. this.document.snippets(                links(                        halLinks(), linkWithRel(\"self\").description(\"The employee's resource\"),                        linkWithRel(\"employee\").optional().description(\"The employee's projection\")),                responseFields(                        fieldWithPath(\"username\").description(\"The employee unique database identifier\").type(String.class),                        fieldWithPath(\"_links\").description(\"links to other resources\")                ));Request fieldsThis defines the fields you request from the client.Normally Spring REST Docs will complain when you neglect a field but with v1.1 we now have support for Relaxed Snippets.Because I use constraints, I made my own method `withPath, this will add an extra column constraint to the documentation.  private static class ConstrainedFields {         private final ConstraintDescriptions constraintDescriptions;         ConstrainedFields(Class&lt;?&gt; input) {             this.constraintDescriptions = new ConstraintDescriptions(input);         }         private FieldDescriptor withPath(String path) {             return fieldWithPath(path).attributes(key(\"constraints\").value(StringUtils                     .collectionToDelimitedString(this.constraintDescriptions                             .descriptionsForProperty(path), \". \")));         }     } @Test public void postEmployee() throws Exception{         Employee employee = employeeRepository.findByUsernameIgnoreCase(\"Nivek\");         employee.setId(null);         employee.setUsername(\"Keloggs\");         String string = objectWriter.writeValueAsString(employee);          ConstrainedFields fields = new ConstrainedFields(Employee.class);          this.document.snippets(                 requestFields(                         fields.withPath(\"username\").description(\"The employee unique database identifier\"),                         fields.withPath(\"firstName\").description(\"The employee's first name\"),                         fields.withPath(\"lastName\").description(\"The employee's last name\"),                         ));          mockMvc.perform(post(\"/employees\").content(string).contentType(MediaTypes.HAL_JSON).header(\"Authorization\", authToken)).andExpect(status().isCreated()).andReturn().getResponse().getHeader(\"Location\");     }                              Response fieldsThis defines the result after consultation of a resource.  this.document.snippets(              responseFields(                       fieldWithPath(\"username\").description(\"The employee unique database identifier\"),                       fieldWithPath(\"firstName\").description(\"The employee's first name\"),                       fieldWithPath(\"lastName\").description(\"The employee's last name\"),                           ));Request/response headersDefines your request/response headers in your API. This is useful when there are extra headers to set. When the request has to involve an authorization header for security reasons, you can add this header to your document.mockMvc.perform(                get(\"/employees/1\").accept(MediaType.APPLICATION_JSON)                .header(\"Authorization\", authToken)                .andDo(document(\"headers\",                \t\t\t\trequestHeaders(                 \t\t\t\t\t\theaderWithName(\"Authorization\").description(                \t\t\t\t\t\t\t\t\"Basic auth credentials\")),                 \t\t\t\tresponseHeaders(                 \t\t\t\t\t\theaderWithName(\"X-RateLimit-Limit\").description(                \t\t\t\t\t\t\t\t\"The total number of requests permitted per period\"),                \t\t\t\t\t\theaderWithName(\"X-RateLimit-Remaining\").description(                \t\t\t\t\t\t\t\t\"Remaining requests permitted in current period\"),                \t\t\t\t\t\theaderWithName(\"X-RateLimit-Reset\").description(                \t\t\t\t\t\t\t\t\"Time at which the rate limit period will reset\")))));Request parametersThe parameters passed by in the uri as a query string are documented with the requestParameters.this.mockMvc.perform(get(\"/users?page=2&amp;per_page=100\")) \t.andExpect(status().isOk())\t.andDo(document(\"users\", requestParameters( \t\t\tparameterWithName(\"page\").description(\"The page to retrieve\"), \t\t\tparameterWithName(\"per_page\").description(\"Entries per page\") \t)));\t\tRequest parts (new in 1.1)The parts of a multipart request can be documenting using requestPartsExampleRestAssured.given(this.spec)\t.filter(document(\"users\", requestParts( \t\t\tpartWithName(\"file\").description(\"The file to upload\")))) \t.multiPart(\"file\", \"example\") \t.when().post(\"/upload\") \t.then().statusCode(is(200));What makes good documentation?Andy’s pickHe told us that the GitHub API is one of the most complete and correct documentation there is. So if you want some guidelines, inspire yourself with this API.Structure and accuracyWhen documenting your application, your accuracy has to be 100% correct and understandable. The structure of your API is the representation of your application, so it better be good.Cross-cutting concernsAndy put forward to document cross-cutting concerns on a general documentation level, avoiding repeating yourself in every single documented API call.Concerns who made it to the top are:  Rate limiting  Authentication and authorisationAnd HTTP verbs/codes (PATCH VS PUT)To be RESTfull, you’ll have to follow the guidelines in having a correct API design. This picture shows you how and when to use the correct verbs and HTTP codes3 main questions if you are working with resources  What do they represent?  What kind of input do they accept?  What output do they produce?Last but not least: do not document uri’s!QuestionsWill constraints be officially added in future releases?The constraints snippets won’t be added in the future.This is because Andy wants to give the developers the choice of what they want to implement.ConclusionSince Spring REST Docs is so effective in bringing documentation to the fun part of development I highly recommend to use this in your future Spring applications. Not only you will be smiling when the API guide is being generated but the Frontend developers will get a more understandable view of the backend.Sources  @ankinson  Spring REST Docs  GitHub API  Verbs &amp; HTTP codes  Asciidoctor manual  Slate example"
      },
    
      "conference-2016-06-20-whats-new-in-docker-112-html": {
        "title": "DockerCon 2016 - What is new in Docker 1.12",
        "url": "/conference/2016/06/20/whats-new-in-docker-112.html",
        "image": "/img/dockercon/dockercon.png",
        "date": "20 Jun 2016",
        "category": "post, blog post, blog",
        "content": "Orchestration Made EasyLast week,I tried out the new orchestration tools that were made available on GitHub.My first impressions were very positive.The setup is easy and it works like a charm.Today,at DockerCon 2016,these new orchestration tools were officially announced during the opening session.There is also an official blog post.Before we start talking about orchestration,let’s take a step back and look at how easy it has become to set up a Swarm cluster.Creating a Swarm manager can be done with one simple command:$ docker swarm initYou can run this command on any Docker 1.12 host.After we created the Swarm manager,we can add additional nodes to the swarm by running the following command on other Docker hosts:$ docker swarm join &lt;IP of manager&gt;:2377That’s it.No messing around with key-value stores or certificates.Docker will automatically configure everything you need out-of-the-box.Under the hood,Docker uses a Raft consensus.There are two types of nodes: manager and worker.The first initial node is a manager.When adding more nodes to the Swarm,these nodes will be worker nodes by default.Manager nodes are responsible for managing the cluster’s desired state.They do health checks and schedule tasks to keep this desired state.Worker nodes are responsible for executing tasks that are scheduled by the managers.A worker node cannot change the desired state.It can only take work and report back on the status.The role of a node is dynamic.We can increment or reduce the amount of managers by promoting or demoting nodes.$ docker node promote &lt;node-id&gt;$ docker node demote &lt;node-id&gt;ServicesDocker 1.12 introduces a new service command.A service is a set of tasks that can be easily replicated.A task represents a workload and is executed by a container.A task does not necessarily have to be a container,but currently that is the only option.In the future,tasks can also be different types of workloads,for example Unikernels.The service command is very similar to the run commandand utilizes a lot of similar flags which we are used to work with.$ docker service create --replicate 3 --name frontend --network mynet --publish 80:80/tcp frontend_image:latestThe above command will create a service named frontend,add it to the mynet network,publish it to port 80,and use the frontend_image for this service.This does not only create the service,but it defines the desired state.The cluster constantly reconciles its state.Upon a node failure,the cluster will automatically self healand converge back to the desired state by scheduling new tasks on other nodes.You can also define a Swarm mode.For example,if you wish to create a service that runs on every node,you can easily do this using the global mode.This will schedule all the tasks of a service on each node.This is great for general services like monitoring.$ docker service create --mode=global --name prometheus prom/prometheusJust like we can put constraints on containers,we can put constraints on services:$ docker daemon --label com.example.storage=\"ssd\"$ docker service ... --constraint com.example.storage=\"ssd\" ...If we want more instances of our service,we can scale our services up and down:$ docker service scale frontend=10 backend=20This will change the desired state of the service(s),and the managers will schedule new tasks (or remove existing tasks) to attain this desired state.We can also apply rolling updates to our services.For example,if we wish to upgrade our service to a newer version without any downtime,we can use the service update command:$ docker service update myservice --image myimage:2.0 --update-parallellism 2 --update-delay 10sThis will update our service by replacing 2 tasks at the time,every 10 seconds.We can also use this command to change environment variables,ports,etc.As you can see,the new service subcommand is very powerful and easy to use.BundlesA Distributed Application Bundle (DAB) file declares a stack of services,including the versioning and how the networking is setup.It is a deployment artifact that can be used in your continuous integration tools,all the way from your laptop to production.Currently,one way to generate a .dab file is by creating the bundle using Docker Compose:$ docker-compose bundleThis command will generate a .dab or .dsb file,which is just a JSON text file.Here’s a partial example:{  \"services\": {    \"db\": {      \"Env\": [        \"constraint:type==backend\",        \"constraint:storage==ssd\"      ],      \"Image\": \"postgres@sha256:f76245b04ddbcebab5bb6c28e76947f49222c99fec4aadb0bb1c24821a9e83ef\",      \"Networks\": [        \"back-tier\"      ]    }  }}This feature is still experimental in Docker 1.12and the specification is still being updated.Docker invites everyone to provide feedback and hopes it will become the de facto standard for deploying applications.Routing Mesh NetworksA problem with load balancers is the fact they are not container-aware,but host-aware.Load balancing containers has been hard up until now,because you have to update the configuration of the load balancers as containers are started or stopped.This is done by overriding the configuration file of the load balancer and restarting it,or by updating the configuration in a distributed key-value store like etcd.Docker now has built in load balancing in the Engine using a container-aware routing mesh.This mesh network can transparantly reroute traffic from any host to a container.For example,publishing a service on port 80 will reserve a Swarm wide ingress port,so that each node will listen to port 80.Each node will then reroute traffic to the container using DNS based service discovery.This is compatible with existing infrastructure.External load balancer no longer need to know where the containers are running.They can just point towards any node and the routing mesh will automatically redirect traffic.Even though this introduces an extra hop,it is still very efficient since it uses IPVS.Security Out Of The BoxDocker now comes with out-of-the-box, zero-configuration security.Docker sets up automatic certificate rotation,TLS mutual authenticationand TLS encryption between nodes.There is no way to turn off security.One of the core principles of Docker is simplicity.Therefor,security must be so simple to use,that you don’t want to turn it off!Container Health Check in DockerfileA new HEALTHCHECK keyword is available for Dockerfiles.This keyword can be used to define the health check of a container.HEALTHCHECK --interval=5m --timeout=3s --retries 3 CMD curl -f http://localhost || exit 1In the above example,health checking is done every 5 minutes.A container becomes unhealthy if the curl command fails 3 times in a row with a 3 second timeout.New Plugin Subcommands (experimental)A new plugin subcommand has been added which allows you to easily manager Docker plugins.$ docker plugin install &lt;plugin-name&gt;$ docker plugin enable &lt;plugin-name&gt;$ docker plugin disable &lt;plugin-name&gt;Plugins also have a manifest file which describes the resources it needs.You can compare it to how a new app on your smart phone asks for access to different resources,like your photos or contacts.Try It Out!As of today,the Docker for Mac/Windows beta,which is already at Docker 1.12,is open for everyone!You can download it at docker.com/getdocker."
      },
    
      "conference-2016-05-13-js-conf-budapest-day-2-html": {
        "title": "JS Conf Budapest Day 2",
        "url": "/conference/2016/05/13/JS-Conf-Budapest-day-2.html",
        "image": "/img/js-conf-budapest-2016.jpg",
        "date": "13 May 2016",
        "category": "post, blog post, blog",
        "content": "From JS Conf Budapest with loveThis year’s edition of JS Conf Budapest was hosted at Akvárium Klub.Located right in the center of the city, below an actual pool, filled with water!  Akvárium Klub is more than a simple bar: it is a culture center with a wide musical repertoire from mainstream to underground.There is always a good concert and a smashing exhibition, performance, or other event happening here, in a friendly scene, situated right in the city center.JS Conf Budapest is hosted by the one and only Jake Archibald from Google.Day 2 started at 9 o’clock.Enough time to drink great coffee and enjoy the breakfast.Day 2: Talks  Suz Hinton: The Formulartic Spectrum  Oliver Joseph Ash: Building an Offline Page for theguardian.com  Nicolás Bevacqua: High Performance in the Critical Rendering Path  Anand Vemuri: Offensive and Defensive Strategies for Client-Side JavaScript  Sam Bellen: Changing live audio with the web-audio-api  Rob Kerr: Science in the Browser: Orchestrating and Visualising Neural Simulations  Stefan Baumgartner: HTTP/2 is coming! Unbundle all the things?!?  Claudia Hernández: Down the Rabbit Hole: JS in Wonderland  Lena Reinhard: Works On My Machine, or the Problem is between Keyboard and ChairDay 2: MorningSuz Hinton: The Formulartic SpectrumSuz is front-developer at Kickstarter. Member of the NodeJS hardware working group. Member of the Ember-A11y Project team.You can find her on Twitter using the handle @noopkat. She blogs on meow.noopkat.com.The physical world is just another binary machine.Data creation, analysis, and corruption combined with JavaScript can make new and unexpected things.Can you programmatically extract joy from the subjectivity it exists in?Can it be translated into intentional forms to hook others in?This session will gently take you along on a personal journey of how you can use code to expose new expressions of the mundane secrets we hold dear.Why are we here  Data &amp; Art  Make a messFeelingsWarning, a lot of feelingsPersonal history1994  Commodore 64 graphics book  Wants to make art on computer  The littlest artist  Accidental programmer (Suz didn’t really want to become a programmer)  Semicolon wars;; It doesn’t matter how you place your semicolon!This story is inspired by the movie Contact by Carl Sagan and makes Suz wonder: what does sound look like?Formulartic spectrum (made up word: art)  Analysing PCM data (Pulse Code Modulation -&gt; raw uncompressed data)  Resulted in only 13-ish lines of codeaudioContext.decodeAudioData(audioData)    .then(function(decoded) {    // just get the left ear, it's fine ;)    let data = decoded.getChannelData(0);    for (let i = 0; i &lt; data.length; i += 1) {        // convert raw sample to within 0-255 range        let hue = Math.ceil((data[i] + 1) * 255 / 2);        // convert HSL to an RGB array        let rgb = hslToRgb(hue, 200, 150);        // create the pixel        imgData.data[i*4] = rgb[0];        imgData.data[i*4+1] = rgb[1];        imgData.data[i*4+2] = rgb[2];        imgData.data[i*4+3] = rgb[3];    }    // put the pixels on a canvas element    canvas.putImageData(imgData, 0, 0);});Suz talked about programming and art.She spent a lot of time on the subway and was wondering if it would be possible to use the sounds of the subway to create art.So she started by taking the sound of the subway doors closing and analysing that part.  Sampling the audio to pixels resulted in 300k pixels  Make it smaller by converting to 16-beat songCheck out the visualisation!  The top section: Stand clear of the closing doors, please.  The mid section: white noise  The bottom section: ding dong!Suz created a visualisation of the sampled audio that resulted in cats sitting on an subway.  Cats can sit on 16 seats in subway car, each seat representing a beat  In total there were 308.728 samples which divided by 16 beats result in 19.295 samples per beat. Suz took the average of the sample values of each ‘beat’  The seats have different colors that represent the drum beat and oscillator note  When a cat is sitting on a chair, we get a guitar strum and noteThe subway example is made using:  SVG images  divs  CSS animationsCheck out the working example!But I’m better at hardwareSo Suz created a subway card with built in speaker!Recap  Creative coding gets you out of your comfort zone and teaches you to use tools you use everyday in another context  Art doesn’t care about your semicolons          Code can be messy      No one cares about semicolons, etc.        Art doesn’t care about perfection          Again, your code doesn’t really matter      Art is about what you learned  Write messy code  Make lots of mistakes  You deserve a break from being judged  Code like no one’s watching  Don’t ‘git rebase -i’          Show the history behind good code      Code evolves from a first idea to a final solution.      At first, code might not be perfect      Don’t rebase to hide this fact      View the slides of Suz’s talk here!Oliver Joseph Ash: Building an Offline Page for theguardian.comOliver is a software engineer working on the team behind theguardian.com.Being passionate about the open web, he aims to work on software that exploits the decentralised nature of the web to solve non-trivial, critical problems.With a strong background in arts as well as engineering, he approaches web development in its entirety: UX, performance, and functional programming are some of the things he enjoys most.You can find him on Twitter using the handle @OliverJAsh.You’re on a train to work and you open up the Guardian app on your phone.A tunnel surrounds you, but the app still works in very much the same way as it usually would, despite your lack of internet connection, you still get the full experience, only the content shown will be stale.If you tried the same for the Guardian website, however, it wouldn’t load at all.Native apps have long had the tools to deal with these situations, in order to deliver rich user experiences whatever the user’s situation may be.With service workers, the web is catching up.This talk will explain how Oliver used service workers to build an offline page for theguardian.com.Oliver talked about the functionality they created with service workers on The Guardian.When offline on The Guardian, you’ll get a crossword puzzle (always the most recent) that you can play.We summarized the key parts of the talk for you.Website vs nativeNative  Content is cached  Experience:          offline: stale content remains      server down: stale content remains      poor connection: stale while revalidate      good connection: stale while revalidate      Website  Experience          offline: nothing      server down: nothing      poor connection: white screen of death      good connection: new content      How it worksService workers  Prototype built in &lt; 1 dayWhat is a service worker?  A script that runs in the background  Useful for features that don’t need user interaction, e.g.:          Listen to push events, useful for pushing notifications      Intercept and handle network requests      Future                  Background sync          Alarms (e.g. reminders)          Geofencing                      A progressive enhancement  Trusted origins only (HTTPS only or localhost)  Chrome, Opera and Firefox stableFor now The Guardian is not yet fully on HTTPS, but they are switching at this time of writing.Some pages have service workers already enabled such as:  theguardian.com/info  theguardian.com/science  theguardian.com/technology  theguardian.com/businessHow did they do it?1. Create and register the service worker&lt;script&gt;if (navigator.serviceWorker) {    navigator.serviceWorker.register('/service-worker.js');}&lt;/script&gt;You can debug service workers in Chrome by selecting Service Workers under the Resources tab.2. Prime the cache  install event: get ready!  Cache the assets needed later  Version the cache. To check if a user has an old version so you can update with newer versions&lt;script&gt;var version = 1;var staticCacheName = 'static' + version;var updateCache = function () {    return caches.open(staticCacheName)        .then(function (cache) {            return cache.addAll([                '/offline-page',                '/assets/css/main.css',                '/assets/js/main.js'            ]);        });};self.addEventListener('install', function (event) {    event.waitUntil(updateCache());});&lt;/script&gt;3. Handle requests with fetch  fetch events          Default: just fetch      Override default      Intercept network requests to:                  Fetch from the network          Read from the cache          Construct your own response                    &lt;script&gt;self.addEventListener('fetch', function (event) {    event.respondWith(fetch(event.request));});&lt;/script&gt;It is possible to use custom responses when using Service Workers. E.g. Use templating to construct a HTML respose from JSON.&lt;script&gt;self.addEventListener('fetch', function (event) {    var responseBody = '&lt;h1&gt;Hello, world!&lt;/h1&gt;';    var responseOptions = {        headers: {            'Content-Type': 'text/html'        }    };    var response = new Response(        responseBody,        responseOptions    );    event.respondWith(response);});&lt;/script&gt;(Im)mutable  Mutable (HTML)          Network first, then cache      Page -&gt; service worker -&gt; server or cache -&gt; Page        Immutable (assets: CSS, JS)          Cache first, then network      Page -&gt; service worker -&gt; cache or server -&gt; Page      4. Updating the crosswordCheck if the cache has been updated and if it’s not up to date, update it and delete old cache.isCacheUpdated().then(function (isUpdated) {    if (!isUpdated) {        updateCache().then(deleteOldCaches);    }});Offline-firstWhy should we be building with offline first?  Instantly respond with a “shell” of the page straight from cache when navigating a website  It improves the experience for users with poor connections  No more white screen of death  Show stale content whilst fetching new contentProblems and caveats  Browser bugs in both Chrome and Firefox  Interleaving of versions in CDN cacheThis can be fixed with a cache manifest.// /offline-page.json{    \"html\": \"&lt;html&gt;&lt;!-- v1 --&gt;&lt;/html&gt;\",    \"assets\": [\"/v1.css\"]}Why? Is this valuable  Fun  Insignificant usage due to HTTPS/browser support          … but plant the seed and see what happens        Iron out browser bugs, pushes the web forward  “If we only use features that work in IE8, we’re condemning ourselves to live in an IE8 world.” — Nolan LawsonConclusion  Service workers allow us to progressively enhance the experience for          Offline users      Users with poor connections        It’s easy to build an offline page  A simple offline page is a good place to startThe slides of Oliver’s talk can be viewed on Speaker Deck.Nicolás Bevacqua: High Performance in the Critical Rendering PathNicolás loves the web. He is a consultant, a conference speaker, the author of JavaScript Application Design, an opinionated blogger, and an open-source evangelist.He participates actively in the online JavaScript community — as well as offline in beautiful Buenos Aires.You can find him on Twitter using the handle @nzgb and on the web under the name ponyfoo.com.This talk covers the past, present and future of web application performance when it comes to delivery optimization.I'll start by glancing over what you're already doing -- minifying your static assets, bundling them together, and using progressive enhancement techniques.Then I'll move on to what you should be doing -- optimizing TCP network delivery, inlining critical CSS, deferring font loading and CSS so that you don't block the rendering path, and of course deferring JavaScript.Afterwards we'll look at the future, and what HTTP 2.0 has in store for us, going full circle and letting us forego hacks of the past like bundling and minification.Getting startedMeasure what is going on and see what is going on!Use the Chrome DevTools Audits.  Per-resource advice  Caching best practicesPageSpeed Insights (Google)developers.google.com/speed/pagespeed/insights/  Insights for mobile  Insights for desktop  Get a rough 1-100 score  Best practices  Practical adviceWebPageTest (webpagetest.org)webpagetest.org  Gives analytics and metrics where you can act on  A lot of statistics  PageSpeed Score  Waterfall View: figure out how to parallelize your download to speed up loading  Makes it easy to spot FOIT  Calculates SpeedIndex: SpeedIndex takes the visual progress of the visible page loading and computes an overall score for how quickly the content painted  Inspect every request  Analyze TCP traffic  Identify bottlenecks  Visualize progressAutomate!But we can automate a lot!  Measure early. Measure often.PageSpeed Insights is available as npm module.npm install psi -gWebpagetest is also available as npm module but is a bit slower.npm install webpagetest-api underscore-cliYSlow is available for different platforms.npm install grunt-yslow --save-devBudgets  Enforce a performance budget  Track impact of every commit  What should I track? More info about this on timkadlec.com/2014/11/performance-budget-metrics          Milestone Timings: Load time, time to interact, “time to first tweet”      SpeedIndex: Average time at which parts of a page are displayed      Quantity based metrics: Request count, page weight, image weight …      Rule based metrics: YSlow grade, PageSpeed score, etc.      Budgeting can also be automated using the grunt-perfbudget plugin.npm install grunt-perfbudget --save-devWhat can we do beyond minification?Minification is usually the first thing developers think of when talking about optimizing your code for speed.But what are the things we can do beyond minification?A lot of best practices on optimizing performance in your app are described in the High Performance Browser Networking book written by Ilya Grigorik.For all the detailed tips and tricks we suggest to view the slides for Nicolás’s talk on ponyfoo.com.Anand Vemuri: Offensive and Defensive Strategies for Client-Side JavaScriptAnand is Senior Application Security Consultant at nVisiumYou can find him on Twitter using the handle @brownhat57.This talk will specifically focus on the other less common client-side vulnerabilities that are not as frequently discussed.Intentionally vulnerable applications developed with client-side JavaScript frameworks will be attacked and exploited live.Remediation strategies will also be discussed so that developers have tools to prevent these vulnerabilities.Through strengthening the security posture of JavaScript applications, we can take strides towards creating a more secure Internet.Break the web together!  They say the best offense is good defense.No. The best offense is offense.Hands-on vulnerability exploitation of medcellarAnand’s talk started by explaining the most common web application vulnerabilities that currently exist.We’re talking about SQL Injection, Cross Site Scripting (XSS) and Cross Site Request Forgery (CSRF).During the talk, Anand used an open source application that contains all of these vulnerabilities and which is available for you as a developer to fool around with.The application is called ‘MedCellar’ and you can find it on github.XSS &amp; CSRFWe saw how to perform XSS attacks and CSRF attacks on the MedCellar Application.These attacks weren’t extremely harmful at first but showed just how they could be exploited.Using the Burp Suite’s proxy, we were able to inspect all requests/responses the application was performing to get more insights in how the app actually worked.Burp Suite is an integrated platform for performing security testing of web applications.XSS  attacks users  JS Injection  Exploits can be bad, really badXSS is a serious vulnerability. It may not seem so for some people or clients but it really is!  How do we exploit apps where users have direct control  How do we attack web apps on a private networkCSRF Attacks!!  “Session Riding”  Attacker sends malicious URL to submit a form to a third party domain  Victim is tricked into interacting with the malicious link and performs undesirable actionsUsing a third party domain you can create a form (you won 1 million dollars) to perform an action like this.BeEFDuring the talk, Anand demonstrated how to perform XSS and CSRF attacks.However, it seemed like you were only able to hack yourself.Things got serious though, when Anand demonstrated how you could exploit these vulnerabilities way more by using a special Linux distro called Kali Linux and BeEF (Browser Exploitation Framework).Kali Linux is a linux distro designed specifically for Penetration Testing and Ethical Hacking.BeEF is a Penetration Testing tool that focusses on the browser and possible vulnerabilities in it and the applications running in it.Combining these two, Anand was able to do basically anything in the users browser and he demonstrated this by running some random audio in the users browser.Playing audio isn’t that harmful, but you could have installed a keyLogger instead and start tracking anything the user types on his computer.That seems to be a little bit worse than playing some audio!  When you enter a coffee shop and see someone using this, disconnect from the internet and run away as fast as you possibly can.” - Quote from AnandMitigate against these attacksImplementation of a CSRF mitigation is Tough!  Method Interchange  Beware of CSRF Token replay  Token must be tied to the user’s session on the server  CSRF Token exposed as GET Param: Could potentially have logs or some other network traffic see the CSRF token and intercept it that way.But, luckily for us, CSRF middleware which implements these mitigations has already been developed for us! You can find these libraries on github:  koajs  crumb  csurfKey takeaways  App Sec vulnerabilities can be used in combination  No state changing operations should be GET requests  Make sure the CSRF token is cryptographically secure          Random !== Cryptographically secure        CSRF Middleware Saves Lives!!Oh… And  Cross Origin Resource sharing (CORS)          Access-control-Allow-Origin: * IS BAD!      Day 2: afternoonSam Bellen: Changing live audio with the web-audio-apiSam is developer at Made with love.You can find him on Twitter using the handle @sambego.As a guitar player, I usually use some effects pedals to change the sound of my guitar.I started wondering: “What if, it would be possible to recreate these pedals using the web-audio-api?”.Well, it turns out, it is entirely possible to do so.This talk takes you through the basics of the web-audio-api and explains some of the audio-nodes I’ve used to change the live sound of my guitar.Presentation can be found here: https://github.com/Sambego/pedalboard-presentationGet the sound in the browser  Create new audio context.  Get the audio input of your computer: navigator.getUserMedia()  Create inputNode from the media stream we just fetched  Connect the inputNode to the audiocontext.destinationAdd effects to the soundVolume pedal  Create a gainNode = audioContext.createGain();  Value of gain is 0 tot 1  So for now we have input -&gt; gain -&gt; output.Distortion pedal  Make the audio sound rough.  Create a waveShaperNode = audioContext.createWaveShaper();  Set a value  So for now we have input -&gt; Waveshaper -&gt; output.Delay pedal  delayNode = audioContext.createDelay();  Set a value delayNode.delayTime.value = 1 (1 second)Reverb pedal  Some kind of echo on your sound  convolverNode = audioContext.createConvolver()  Load impulse-response-file and do some crazy stuffHow to create an oscilator  oscilatorNode = audioContext.createOscilator()  Set Hz valueweb-midi-api  Request access and start doing things with itRob Kerr: Science in the Browser: Orchestrating and Visualising Neural SimulationsRob works at IBM Research Australia.You can find him on Twitter using the handle @robrkerr.My talk will show how the old-school, computationally-heavy software used in science can be set free using the centralized power of cloud resources and the ubiquity of the browser.We'll see real-time, publicly-broadcast, simulations of the electrical activity in brain cells, visualised in 3D using Javascript.Neuroscience introductionThe topic for this talk was quite some heavy material.However, Rob managed to give us a quick, super high-level, introduction to neuroscience and more specifically an introduction to how neurons actually work.Very High level, there are 3 parts in a neuron:  Dendrites  Neuron body (Soma)  Axons.Neurons receive electrical signals through their dendrites, and transmit those to the neuron body, called the Soma.From the neuron body, new electrical signals travel to other neurons.Sending electrical current from one neuron to another is being done through its axons.So the axons actually send electrical signals to other neurons and those other neurons receive these signals trough their dendrites.A better, more thorough explanation of neurons is being described on Wikipedia, but we needed a super simplified explanation of neurons and their main components to further explain what Rob showed us.Science in the browserNeurons and their main components can be ‘encoded’ in special files .swc files.These files contain multiple records with an ID, X, Y, Z, Radius and Parent-link.Using all the records and their properties allows you to visually represent the neurons.There’s already an online repository containing these encoded neurons which you can find here.Now, what does all of this have to do with the browser or JS or anything you would expect at JSConf?Well, while he was working on his Ph.D. thesis, he started playing around with JS and its related technologies.And he continued to do so since then, all in function of the neuroscience domain.As we saw earlier, there’s already a webpage where you can upload swc files with neuron data to visually represent these, but these are rather static images.Instead, Rob decided to create a platform which can also simulate the behaviour of such a neuron when you trigger it with electrical current on its dendrites.Technology stackRob used a combination of tools and technologies to build the platform.Together with his colleagues at IBM research Australia, they built an entire Cloud platform that could perform these complex simulations.On their IBM Bluemix cloud they run Docker Containers running the algorithm that performs the neuron simulations.The algorithm is written in C and is based on mathematic formula which is shown in the below image.Hodgkin-Huxley Model of the Squid Giant AxonThe web application used to render the neurons used a combination of tools, most importantly:  Webgl: Web Graphics API. Javascript API for rendering interactive 3D graphics.  three.js: A Javascript 3D library that uses WebGL.  D3.js: Javascript library for visualizing data using HTML, SVG and CSSThe tool in actionIn the below video you can see what the tool looks and animations look like:\tThe tool enables researchers to replay a scenario where a certain spike is triggered in a branch of the neuron.This gives scientists a lot of knowledge and insights about how neurons behave.Rob gave a really entertaining talk with some really cool visuals of neurons in action.He introduced us to just the right amount of neuroscience to be able to follow what he was actually doing and showing!Stefan Baumgartner: HTTP/2 is coming! Unbundle all the things?!?Stefan is a web developer/web lover based in Linz, Austria.Currently working at Ruxit, making the web a faster place.He is also a co-host at the German Workingdraft podcast.You can find him on Twitter using the handle @ddprrt.In this session, we will explore the major features of the new HTTP version and its implications for todays JavaScript developers.We will critically analyze recommendations for deployment strategies and find out which impact they have on our current applications, as well as on the applications to come.Unbundle all the things?Everybody is saying to not bundle things, minify things, concatenate things, … when moving to HTTP/2.Tools like Browserify, Webpack, etc. would all become obsolete.But why? We need to question this and see if this is actually the truth.The best request is a request not being madeIn HTTP version 1.1 we need to do as few requests possible. Pages like Giphy have 40 TCP connection at a single time!HTTP/2 was made to prevent the bad parts of HTTP/1.1HTTP/2 allows a connection to stay open and transfer multiple things over the same connection.No need for handshakes for each file that needs to be transferred from the server to the client.Rule of thumbA slow website on HTTP/1.1 will still be a slow website on HTTP/2.You need to perform optimisations no matter what.Most important part: do not block the render path.Only serve what you really need.Again, the best request is a request not being made.So, unbundle all the things?So in some way, yes unbundle all the things.Because you don’t want to transfer bytes you don’t need, but there is something more to it.This article about packaging will get you on the way: engineering.khanacademy.org/posts/js-packaging-http2.htm.Create a lot of modules to update as flexible as possible and as small as possible.When using ES6 we can also use Treeshaking.  Create independent, exchangeable components  Create small, detachable bundles  Think about long-lasting applications and frequently of changeUse tools, not rules!Claudia Hernández: Down the Rabbit Hole: JS in WonderlandClaudia is Mexican front-end developer.You can find her on Twitter using the handle @koste4.What even makes sense in Javascript?For a language originally created in 10 days it surely has a lot of quirks and perks many JS developers are unaware of.Sometimes, it might even seem like we fell down the rabbit hole only to find that NaN is actually a Number, undefined can be defined, +!![] equals 1, Array.sort() may not work as you suspected and so much other nonsense that can trip any JS developer’s mind.This talk is a collection of Javascript’s oddities and unexpected behaviors that hopefully will prevent some future headaches and help understand the language that we all love in a more deeper and meaningful way.This talk by Claudia was so much fun! We didn’t write down anything because it was virtually impossible to do. You need to see this with your own eyes!You can view the slides on Speaker Deck.Be sure to check out jsfuck.com for some fun times and jQuery Screwed to get an idea of what you can actually do with JavaScript quirks.Lena Reinhard: Works On My Machine, or the Problem is between Keyboard and ChairLena is teamleader, consultant and photographer.You can find her on Twitter using the handle @lrnrd.In this talk we will look at the many facets that affect our decision making and interactions, and work out how we can change for the better.Together, we will take a look at the effects that our software has on the daily lives of the thousands of people who are using it.You’ll learn what you can do as an individual to support change into a positive direction, and how you can help debug this system and make a difference in the tech industry.You’ll leave knowing about practical things you can do in your daily life to make the tech industry a better, more inclusive and diverse environment that is a better place for everyone.Code debuggingDebugging can be hard and it becomes harder when working with complex software.Spaghetti code is difficult to read and maintain.It can be code that is not organised, has lots of dependencies and is difficult to debug.The Tech Industry is buggedA lot of people already contributed to the tech industry.It has grown very fast and has many flaws.That’s why we need to have a look at it and try to fix the defects.Understanding ourselvesTo be able to fix this we need to understand ourselves. Our flaws, limitations, …  We are privileged and need to understand that.Privilege: The human version of “works on my machine”.Privilege is sitting in your comfy home and not knowing a big thunderstorm is coming that could harm people.Privilege is being able to stand up when attending a standup and not having to sit because you are disabled.We are biasedWe need to understand we are biased.More often we are being objective and often that is not OK.We all have biases and we need to realise and understand.EmpathyWe need to understand that we need to be empathetic.Empathy is the right direction.CreativityCreativity is necessary to design and build good software.DiversityAnd so is diversity and understanding each other.InclusionInclusion means all people in the group are respected for who they are.The lack of inclusion and diversity is a real problem in our industry.The Tech IndustryLet’s look at some key points within our industry.Company  Lack of diversity  Lack of inclusion  HarassmentSociety  Racism  Patriarchy  CapitalismTech industry  Lack of diversity  Lack of inclusion  Harassment  Racism  Patriarchy  CapitalismSoftware can help peopleOur software can help people. A screenreader, accessibility, …But can also ruin livesOur software is racist.Our software (tools like Siri or Cortana or Snapchat) does not correctly recognise skin color, alters skin color and does not recognise harassment or racism.Animations in software can trigger panic attacks or epileptic attacks.  We have a collective responsibility and need to take that very seriously.Technology and our code is not neutral. Our work is political and has consequences on lives.Debugging the systemChange starts with you, starts with all of us.What can we do to debug the system?  Educate yourself, about systemic issues and oppression  Practice empathy, because we need it to be good designers and developers  Work on Humility, because none of us are Unicorns  Understanding Privileges, and use them for good  Address biases, and establish policies to address them  Listen, and actively look for voices outside of your networks  Amplify others’ voices, and speak less  Work on diversity, because it’s our moral obligation  Work on inclusion, to make spaces welcoming and safe  Give, our knowledge, time, technical skills, money  Work on being allies, constantlyQuite a talk on some serious matter to close the second day of JS Conf Budapest.Have you experienced these things yourself in the tech industry?Have you contributed to debugging the tech industry?Day 2: ConclusionJust like day 1, day 2 was one hell of a nice day packed full of great speakers and a superb atmosphere!The talks by Rob Kerr and the one of Lena Reinhart surely got the most attention.Rob’s talk because it was impressive to see what they achieved over a course of 2 years to visualise neurons in the browser.Lena’s talk because we got slammed in the face about how faulty the tech industry is at the moment.This year’s edition was, just like the one we attended last year a very good one!It is nice to see such a diverse community that cares about technology and people.This is something we should be very proud of.A big thank you to the organisers and volunteers to make JS Conf Budapest what it is!Find us on the family photo!Next yearIn 2017, JS Conf Budapest will be held on the 14th and 15th of September.We will surely be present for what will be another great edition! See you next year!JS Conf Budapest 2016, day 1Read our full report on day 1 of JS Conf Budapest here!."
      },
    
      "conference-2016-05-12-js-conf-budapest-day-1-html": {
        "title": "JS Conf Budapest Day 1",
        "url": "/conference/2016/05/12/JS-Conf-Budapest-day-1.html",
        "image": "/img/js-conf-budapest-2016.jpg",
        "date": "12 May 2016",
        "category": "post, blog post, blog",
        "content": "From JS Conf Budapest with loveThis year’s edition of JS Conf Budapest was hosted at Akvárium Klub.Located right in the center of the city, below an actual pool, filled with water!  Akvárium Klub is more than a simple bar: it is a culture center with a wide musical repertoire from mainstream to underground.There is always a good concert and a smashing exhibition, performance, or other event happening here, in a friendly scene, situated right in the city center.JS Conf Budapest is hosted by the one and only Jake Archibald from Google.After waiting in line at 8 o’clock in the morning to get our badges, we were welcomed at the main hall where some companies hosted stands.In another space after the main hall, tables were nicely dressed and people could have breakfast.When going downstairs to the right of the main hall, we entered the room where the talks would be given.For the coffee lovers, professional baristas served the best coffee possible.With a nice heart drawn on top if it.At 9 o’clock the conference would officially start so we went downstairs.After taking our seat, we played the waiting game and all of a sudden, we got this nice intro made with blender and three.js! Check it out for yourself!Day 1: Talks  Laurie Voss: What everybody should know about npm  Safia Abdalla: The Hitchhiker’s Guide to All Things Memory in Javascript  Yan Zhu: Encrypt the web for $0  Denys Mishunov: Why performance matters  Princiya Sequeira: Natural user interfaces using JavaScript  Maurice de Beijer: Event-sourcing your React-Redux applications  Rachel Watson: The Internet of Cats  Nick Hehr: The other side of empathyDay 1: MorningLaurie Voss: What everybody should know about npmLaurie is CTO at npm Inc.You can find him on Twitter using the handle @seldo.The presentation he gave can be found a slides.com/seldo/jsconf-budapest.npm is six years old, but 80% of npm users turned up in the last year.That's a lot of new people! Because of that, a lot of older, core features aren't known about by the majority of npm users.This talk is about how npm expects you to use npm, and the commands and workflows that can make you into a power user.There will be lots of stuff for beginners, and definitely some tricks that even most pros don't know.How does npm look up packages?In contrast to what most people think, npm does not download its modules from GitHub or other version control systems.They would not like it that such an amount of data is transferred on a daily basis.In short npm does this: You -&gt; CLI -&gt; Registry.Let’s dive in.  First, npm will take a look at your local cache and see if the package your are looking for is present.  Next, it will resort to the CDN network and use the server which is the closest as possible to your position.  Finally, if npm can’t find the package in local cache or the CDN network, it will look it up in the registry. The registry is a set of servers all around the world and it will try to match the best version that you are looking for.EACCESS errorA lot of people have issues with EACCESS errors because they used sudo to install things.The easy solution is to always keep on using sudo, BUT we can easily fix npm permission issues.package.jsonDon’t write your package.json yourself. Let NPM do it!It will always do it better. Use npm init, which will ask you some basic questions and generate package.json for you.ScopesA new feature in npm is scopes.These are modules that are “scoped” under an organization name that begins with @.Scopes can be public and private.Here is how to use scopes:npm init --scope=usernamenpm install @myusername/mypackagerequire('@myusername/mypackage')npm-init.jsTo extend the npm init command, it is possible to create an npm-init.js file.This file is a module that will be loaded by the npm init command and will provide basic configurations for the setup.By default the file is placed in the root of your project: ~/.npm-init.js.You can use PromZard to ask questions to the user and perform logic based on the answers.Remember that npm init can always be re-run.Why add stuff in devDependencies.Simple: because production will install faster! A lot of people don’t tend to do this, so please do this!When using this you can simple run the command below on production and be done with it.npm install --productionBundled dependenciesOne of the biggest problems right now with Node.js is how fast it is changing.This means that production systems can be very fragile and an npm update can easily break things.Using bundledDependencies is a way to get round this issue by ensuring that you will always deliver the correct dependencies no matter what else may change.You can also use this to bundle up your own, private bundles and deliver them with the install.npm install --save --save-bundleOffline installsA way to prevent npm to look up the registry, and ensure local installs, is by adding the variable --cache-min and to set it to a high value such as 999999.npm install --cache-min 999999Run scriptsIn package.json it is possible to define default run scripts as shown below.npm startnpm stopnpm restartnpm testOf course it is also possible to define your own run scripts.You can run these scripts like this:npm run &lt;anything&gt;Run scripts get devDependencies in pathDon’t force users to install global tools. That is just not cool.This way you can prevent to get conflicts over global tools, because different projects can use different versions.SemVer for packagesnpm uses Semantic Versioning, which is a standard a lot of projects use to communicate what kind of changes are in a release.It’s important to communicate what kinds of changes are in a release because sometimes those changes will break the code that depends on the package.Let’s take a look at an example.1.5.6Breaking Major . Feature Minor . Fix PatchThis is quite obvious, right?npm allows you to change the version (and to add a comment) by using the commands below.npm version minornpm version majornpm version patchnpm version major -m \"Bump to version %s\"Microservices architectureWhen working with a microservices architecture, it is possible to work with multiple packages for your services.This can be done by using the link function within npm.npm link &lt;dependency&gt;Let’s say we have a package named Alice and we have other packages that depend on this package.We can run npm link.In packages that depend on Alice, say Bob, we simply run npm link alice.All changes made in Alice will be immediately available in Bob without performing any npm update commands.Unpublish a packageBefore the recent events where a package called left-pad got pulled from npm and broke the internet, it was possible to unpublish a package just like that by using npm unpublish.Now this is restricted after the package has been online for 24 hours.To really unpublish the package you will need to contact support.A more friendly way is the use of npm deprecated that will tell users the package has been deprecated.Keeping projects up to dateBefore running npm update, it’s preferred to run npm outdated.This command will check the registry to see if any (specific) installed packages are currently outdated.npm outdatednpm updateBy doing so, you can prevent yourself from breaking the project if certain packages would not be compatible.Stuff everybody should know about npmA lot of things are available for npm that will make your life as a developer easier.  Babel: Transpile all the things! JavaScript, TypeScript, JSX, …  Webpack and Browserify  Greenkeeper (greenkeeper.io) is npm outdated as a service!  Node Security Project: Install by using npm install nsp -g. Use by running nsp check. You can use this to check if your project contains vulnerable modules.Why should I use npm?npm reduces friction.It takes things you have to do all the time and makes things simpler and faster.Safia Abdalla: The Hitchhiker’s Guide to All Things Memory in JavascriptSafia is a lover of data science and open source software.You can find her on Twitter using the handle @captainsafia or on her webpage safia.rocks.Slides and interactive tutorialThe slides of this talk can be found here http://slides.com/captainsafia/memory-in-javascript.Safia also created an interactive tutorial on how to use the Chrome DevTools for memory management.This talk will take beginners through an exploration of Javascript's garbage collector and memory allocation implementations and their implications on how performant code should be written.Attendees will leave this talk having gained insights into the under-the-hood operations of Javascript and how they can leverage them to produce performant code.Why should I care about memory?  It forces us to be (better) more inventive programmers, adds restrictions and forces us to use the best tools to create the best possible experience.  Memory is scarce. A lot of people still use devices that are not packed with a lot of memory.Not everyone has high performant development machines.  It helps us exercise our empathy muscles.What does it mean to manage memory?The Good, The Bad, The UglyHow does JS manage memory?Safia focuses on the V8 JS Engine.We have basic types in JavaScript:  booleans  numbers  stringsMemory is allocated in a heap structure and uses a root node which has references to other ones: booleans, string, etc.So basically: root node -&gt; references -&gt; variables.V8 allocates objects in memory in 6 contiguous chunks, or spaces:  New space: Memory gets allocated here when an object is created immediately.It is small and is designed to be garbage collected very quickly, independent of other spaces.  Old pointer space: Contains most objects which may have pointers to other objects.Most objects are moved here after surviving in new space for a while.  Old data space: Objects that just contain raw data (no reference or pointer) will end up here after surviving in new space for a while.  Large object space: Used to store large object tables.They get stored here so it doesn’t conflict with the store space of the above mentioned spaces.  Code space: Code objects are allocated here. This is the only space with executable memory.  Map space: Contains objects which are all the same size and has some constraints on what kind of objects they point to, which simplifies collection.How does V8 collect garbage memory?V8 uses a ‘stop the world’ technique that enables it to run a short garbage collection cycle.This means it will literally halt the program.V8 has different approaches on how it collects garbage in the new and old space.  New space: Garbage collection by using a scavenging technique.Each scavenging cycle will go through the entire heap starting from the root and will create copies.It will clear out what is currently in new space.Everything that is not reachable will be cleared out of the space.You need double the size of the memory that is available for the new space to use for the copy.  Old space: Mark and sweep technique.Remove unmarked objects on a regular basis.How do I write memory performant applications?Asking yourself the following two question will get you started!  How much memory is my application using?  How often do garbage collection cycles occur in my application?Of course you need to have the tools to work with.The Chrome DevTools HEAP allocation profiler will be our weapon of choice.It allows you to check the retain size and shallow size of objects.  Shallow size of an object is the amount of memory it holds of itself.  Retain size is all of its size and its dependents.Heap dumpHeap dump takes a snapshot of your heap at a specific moment.It will provide a file with .heap extension which enables you to load it in the Chrome DevTools for further inspection.npm install heapdumpLet’s practice!Follow this interactive tutorial on how to use the Chrome DevTools for memory management.Yan Zhu: Encrypt the web for $0Yan is engineer @brave and likes information freedom, breaking shit, cryptography, theoretical physics, free software, infosec, stunt h4cking, and an Internet that respects humans.You can find her on Twitter using the handle @bcrypt.Everyone in 2016 knows that websites should use HTTPS.However, there is a common misconception that TLS and other security measures are expensive and slow down both web developers and page load times.This talk will show you some easy tricks to make your site more secure without sacrificing performance or requiring a lot of effort.Is the web fast yet?Yes. Size of pages is rising. Amount of HTTPS requests is also rising!Is TLS fast yet?Yes. Netflix is going to secure streams this year over HTTPS.  2015: Netflix and chill  2016: Netflix and HTTPS and chillThe numbers aren’t entirely clear, so here they are:  Without encrypted Netflix streams, 65% of internet traffic is unencrypted. Only 29% of internet traffic is encrypted.  With encrypted Netflix streams, unencrypted internet traffic will drop to 26,9% and encrypted traffic will increase to 67,1%.Source: https://www.sandvine.com/downloads/general/global-internet-phenomena/2015/encrypted-internet-traffic.pdf  TLS has exactly one performance problem: it is not used widely enough.Everything else can be optimized.  Data delivered over an unencrypted channel is insecure, untrustworthy, and trivially intercepted.We owe it to our users to protect the security, privacy, and integrity of their data — all data must be encrypted while in flight and at rest.Historically, concerns over performance have been the common excuse to avoid these obligations, but today that is a false dichotomy. Let’s dispel some myths.Keep reading about this matter on istlsfastyet.com.HTTP/2Another technology that can help the adoption of TLS is HTTP/2.HTTP/2 offers:  Binary encoding instead of text encoding  header compression  Server push  multiple requests on single TCP connection!!HTTP/2 allows for requests to be sent in parallel rather than sequentially.Does HTTP/2 require encryption? No. However, Chrome and Firefox will only support HTTP/2 with encryption.Let’s EncryptLet’s Encrypt (a non-profit certificate authority) has left beta stage on the 12th of April and is a new Certificate Authority: It’s free, automated, and open.It is backed by some major sponsors such as Mozilla, Akamai, Cisco Chrome, and so much more.  The objective of Let’s Encrypt and the ACME protocol is to make it possible to set up an HTTPS server and have it automatically obtain a browser-trusted certificate, without any human intervention.This is accomplished by running a certificate management agent on the web server.Interested in Let’s Encrypt? Keep reading on letsencrypt.org.Get HTTPS for free!Manually setting up your free HTTPS certificates from Let’s Encrypt is also an option. You can do that on gethttpsforfree.com.Denys Mishunov: Why performance mattersDenys is frontend developer, speaker. Science aficionado. And writes for @smashingmag.You can find him on Twitter using the handle @mishunov or on his personal website mishunov.me.Performance is not about Mathematics.Performance is about Perception.Perception is what makes a site with very few requests nevertheless feel slow, while a site that delivers search results during tens of seconds can feel fast enough for your user.User’s perception of your website’s speed is the only true performance measure.This talk is about perception, neuroscience and psychology. The time is ripe to understand performance from the user’s perspective.In this talk Denys showed us that performance is not always in the numbers, but that it is most of the time perception.So next time you decide to invest a bunch of money in getting that request 100ms faster, make sure it will have impact!  Performance is about perception! Not mathematics.Houston Airport was used as an example to illustrate this quote.At Houston Airport, there were a lot of complaints about long waiting times at the baggage claim.They decided to optimize the baggage handling process.They managed to get luggage to the baggage claim in about 8 minutes (which is nice!).However, complaints weren’t dropping at all.It turned out that passengers needed only 1 minute to get from the plane to the baggage claim, which meant they needed to wait 7 minutes for their luggage.Eventually they decided to literally taxi and park the airplanes further so passengers now needed to walk 6 minutes from the plane to the baggage claim which reduced waiting times for luggage to 2 minutes.This caused complaints to drastically reduce!Speed!1 second gain will increase revenue by 1% for Company X. 1 second slower will decrease conversions by approximately 5%.The 20% rule.This rule defines that you should make a page load at least 20% faster, otherwise users will not notice.We’re talking about noticeable difference.A big difference with meaningful difference.Noticeable !== MeaningfulWe did a live test on the conference where the crowd needed to decide which of the two pages displayed loaded faster.The first page loaded in 1.6 seconds whereas the second one loaded in 2 seconds.Most of the people thought the second page, with 2 seconds load time was faster. This is all about perception!Another fun fact is when delaying audio on a video, our mind will trick us by syncing the audio with what is visible on the screen. Again perception!Key takeawayDon’t spend to much time optimizing the nitty gritty details of your code, instead try moving the active phase forward.As soon as there is activity being shown (pages being loaded), the brain enters the active phase.The user no longer feels as if he’s waiting (Remember the perception?).You can move the active phase forward by making use of:  async  Service workers  “The perception of performance is just as effective as actual performance in many cases” - Apple quoteDay 1 afternoonPrinciya Sequeira: Natural user interfaces using JavaScriptPrinciya works at Zalando Tech where she uses React and Redux. She’s also a startup enthusiast, teacher, speaker, DataViz Diva and has a love for food and JavaScriptYou can find her on Twitter using the handle @princi_ya and Zalando Tech using the handle @ZalandoTech.The way we interacted with computers on a large scale was stuck in place for roughly 20 years.From mouse to keyboard to joystick, it is game over.Today it is the era of gestures.Today’s gamers can do everything from slice and dice produce in Fruit Ninja to quest for a dragon in Skyrim.We’ve been captivated by these powerful, natural, and intuitive interactions; imagining what it would be like to have that power at our own fingertips.In this recent decade, we’ve seen some staggering advances in technologies bring us closer making these magical experiences a reality.In this talk I will present how we can create new, intuitive, interactions for these novel input devices using JavaScript.This talk takes on a different approach in user interaction, in way that different ways of input can result in the same output.At this moment we know the evolution of Typed, Clicked and Touched but currently we are evolving to Typed, Clicked, Touched, Guestures/Speech/… etc.Evolution of user interfaces.  CLI: Codified, Strict  GUI: Metaphor, Exploratory  NUI (Natural User Interfaces): Direct Intuitive. More natural and more intuitive.NUI + JS = NUIJSAt first, Princiya was trying to build a simulator for motion controlled 3D camera’s.A tool that is not dependent on any platform without using a physical device.The simulator is purely based on JavaScript and easily integrates with the device’s SDKs.Once the simulator was made, she tried to build some apps with it (using leap motion for example) to move a slideshow or any other purpose.The tool can be used for many purposes an a lot of devices are already available (VR, motion, …)  Augmented Reality.  Virtual Reality.  Perceptual Computing: bringing human like behaviour to devicesWhat next?Architecture  Step 1: USB controller reads sensor data  Step 2: Data is stored in local memory  Step 3: Data is streamed via USB to SDKLive demoPrinciya demonstrated a drawing board with a brush, both with mousepointer and LEAP motion.NUIJS will translate the input data from the mouse pointer to the Node.js Web Socket server and this one will process the data and send it back to the LEAP motion SDK.The same code can be used with the LEAP motion itself since it integrates nicely with the device’s SDKs.Other open source tools Princiya mentioned were Webcam Swiper and js-objectdetect.Viola-Jones AlgorithmMost of the tools will use or depend on the Voila-Jones Algorithm which can be used for object detection.Combined with other tools this can be very powerful.  HAAR feature selection  Creating an integral image  Adaboost training  Cascading classifiersMaurice de Beijer: Event-sourcing your React-Redux applicationsMaurice is a freelance developer/trainer and Microsoft Azure MVP.You can find him on Twitter using the handle @mauricedb.With Event-Sourcing every action leading up to the current state is stored as a separate domain event.This collection of domain events is then used as the write model in the CQRS model.These same events are projected out to a secondary database to build the read model for the application.In this session Maurice de Beijer will explain why you might want to use Event-Sourcing and how to get started with this design in your React and Flux applications.What is Event-sourcingEvent-sourcing is a way of capturing changes in the state of an application.The traditional way of doing this would be to just update the existing state of your application to whatever state it should be in.This way you always have the latest state of your data at your disposal.In Event-sourcing, you’ll capture all changes as events.These events will be stored in the sequence they were applied.You now have a complete log of events that happened in your application.This allows for features such as:  Complete Rebuild: Possibility to rebuild the entire application state by re-running all events.  Temporal Query: Determining the state of the application at a given point in time.  Event Replay: Replay incorrect events by reversing it and all subsequent events, then replaying the correct event and re-applying all later events.Common example of systems that use Event Sourcing are Version Control Systems.When to use Event-sourcing?Event-Sourcing is particularly useful in situations where you need to keep an audit trail of all changes that occurred to your data.Accountancy for example is a domain in which Event-Sourcing is very useful, because you need to be able to provide that trail for audit purposes.REPHRASE! -&gt; The immutability of events allows for more scalability in your apps also.CQRS and Event SourcingWhere Event-Sourcing describes the practice of storing all application state changes in individual events, CQRS describes the practice of separating the command from the read side.This means you’ll have a service exposing all write functionality in your application and a separate service exposing all read operations.This model works well with Event-Sourcing as you can use the Events occurring on your system as Commands in the CQRS model.During the talk, Maurice showed some samples of code that were the pieces of the puzzle in setting up Event-Sourcing and CQRS in your React-Redux application.Check out the slides for his talk here to find out more!Rachel White: The Internet of CatsRachel is a front end developer at IBMWatson. A lover of retro graphics &amp; horror &amp; coding &amp; games, but above all, of Cats.You can find her on Twitter using the handle @ohhoe.Find out more about her and her projects on rachelisaweso.me and imcool.onlineEver lose out on a good night's rest because your pesky cats keep waking you up at 4am for food?Rachel has.Many times.For her first project using node, socket.io, microcontrollers, and johnny-five, Rachel built a web-based feeder that delivers tasty cat chow on a configurable schedule or when triggered remotely.She'll walk you through her learning process and get you excited about trying new things in your own projects.Finally, she'll show you how to take the first steps to release your work to the open source community.One thing is for sure, Rachel really, really, really likes cats!Where a lot of people try to create things that improve others peoples lives, Rachel tries to do the same, instead, she does this for cats…One Question is constantly on her mind:  “How can we incorporate cats in technology?”Eventually, she decided to create a feeder bot for her cats and immediately thought of open sourcing “the thing”.The talk was mainly a tour of what she’s learned and encountered along the way.Trying new things is scaryRachel wasn’t exactly familiar with robotics or backend development, so she would be entering a whole new world.She’d have to try out new things and start a project without any idea of whether all of this would actually work out.  Embarking on a new project: will it succeed, will it suck?  Using new technologies for the first time: what will happen, will it work for me?  Contributing to Open Source: putting yourself out there is terrifying!Why so scary?Why is this so scary? It turns out the Open Source developer community can sometimes be quite a harsh environment…  Fear of rejection  Imposter Syndrome  Inclusiveness of Communities  Bad behaviour in General: e.g. Oh you didn’t know about THIS?, e.g. completely ignoring contributions  Your GitHub green timeline is not a representation of what you’re worth. Just opening a PR just for the sake of it sucks.  Don’t insult the contributor. Why on earth …  Vulgar and brutal harassment of the community, seriously, get a life!  PR’s that get ignored (for over a year) and then the maintainer writes the same fixes and says: Oops!Eventually, Rachel set up a Twitter poll asking people about what bad experiences in Open Source Software development they’d already encountered, showing off an entire list of Twitter responses. Which weren’t that positive (euphemism!)One of her Twitter contacts actually created (and open sourced) a tool called echochamber.js, which allows you to include a commenting form in your site that stores the comments only in the local storage.That way, you can be an a**hole and post really offensive comments without actually insulting people.Echochamber.jsProposals for new contributorsKnowing all of these things now, you might wonder if it’s even worth it putting yourself out there.The answer of course is YES, but consider the following tips when doing so!  Find something you are passionate about  Something new you want to try  Make something cool and open source it yourself  First point of contact is your peers  Constructive criticism!Building a cat feeder botNow, let’s talk about the actual Cat Feeder bot, which was most suitably named RoboKitty. Check it out at here! and hereIt’s a node based cat feeder that works over the web.You can use it to instantly feed your cat, or you can feed periodically using cron triggers.After some trial and error on choosing the right combination of hardware, the final list of technologies involved in creating the Cat Feeder Bot looks something like this:  Node.js  Johnny-five: Javascript Robotics &amp; IoT platform.  Particle Photon kit (with breadboard)  4xAA battery pack with on/off switch  Misc hardware accessoriesOther things learned along the way were:  A servo needs external power, so yeah, plugging it in the microcontroller is not enough! :D  No idea how to solder…? Worked out! -&gt; Youtube -&gt; Learn how to solder.Lessons learned  Don’t be afraid of the unfamiliar  Don’t be afraid to ask for help  People really like cat stuff  Don’t downplay your abilities: I mean, it’s a super cool kitty food dispenser!  I like nodebots a lotNick Hehr: The other side of empathyNick is an Empathetic Community Member, Front-End Dev @NamelyHR ,@hoodiehq Contributor, @tesselproject Contributor and @manhattan_js OrganizerYou can find him on Twitter using the handle @HipsterBrown.In an industry that is so focused frameworks &amp; tooling, we tend to lose sight of the people behind the products and how we work with them.I’ve found empathy to be a powerful resource while collaborating with teams inside companies and across the open source community.By breaking down The Other Side of Empathy, I will demonstrate how applying its principles to your development process will benefit the community and the products they create.EmpathyNick Hehr shares Rachels’ point of view on the sometimes rude Open Source communication and communication on Social media in general.In his talk, he addressed the way you should behave when volunteering to contribute or when giving feedback to contributors in Open Source Software (OSS) projects.And Empathy turns out to be key in this process.RantingIt’s all too easy to judge or express prejudice these days, through these social media channels and not think about the people who are actually behind the idea or concept you’re judging.People that decide to Open Source the work on which they’ve spend tons of effort (usually because it’s their passion, but still…) aren’t exactly waiting for trolls or rants from people who like this easy judging.Empathy also plays a huge role in the other way around.It happens all too often that people trying to contribute to OSS for the first time are being ignored (by literally ignoring their pull requests for example), being treated like idiots (instead of being given constructive feedback when there is room for improvement), etc…Saying nice things  “If you don’t have anything nice* to say, don’t say anything at all!”Nice in this context means constructive. Comment on something you think could use improvement and offer a solution.Compliment on certain aspects that really improve the tool.Due to the relative anonymity of social media and other communication channels, we tend to forget these principles.Key take-awaysKey points to take away from this session are:  Give constructive feedback!!!  Always keep in mind the language your using when commenting on Open Source initiatives          Don’t be to blunt or direct in your reactions.        Use the right channels for your communication          meaning, don’t ask for feedback on twitter      Instead turn to platforms such as Slack, IRC, Gitter…      Get (constructive) feedback from people you trust        People that open source their tools don’t owe you anything.          They’re not entitled to give up all their time for you.      They’re not here to start fulfilling all requests from a demanding user base. It’s open source, submit a pull request      Living by these rules will make the (web-)world a little bit of a better place, but won’t prevent other people from still continuing these bad habits.Don’t let these people get to you! Continue doing what you’re passionate about and seek those that will give you that constructive feedback.Afterparty with Beatman and LudmillaAfter a long day, it was time for some party time and since JS Conf Budapest was hosted at a club, this could only be good!We were presented a live set by Breakspoll 2015 winner Beatman and Ludmilla.Day 1: ConclusionDay 1 was packed full of great speakers and the atmosphere was superb! A lot of inspiring talks that gave us a lot of topics to cover for the months to come within the JWorks unit at Ordina Belgium.The after party with Beatman and Ludmilla was a perfect closing of the day. On our walk to the hotel we could only imagine what day 2 would bring.Read our full report on day 2 of JS Conf Budapest here!."
      },
    
      "microservices-2016-05-01-using-jwt-tokens-for-state-transfer-html": {
        "title": "Using JWT for state transfer",
        "url": "/microservices/2016/05/01/Using-JWT-Tokens-for-State-Transfer.html",
        "image": "/img/JWT/jwt-logo.png",
        "date": "01 May 2016",
        "category": "post, blog post, blog",
        "content": "At one of our clients, we have been using Json Web Tokens quite extensively.We even use it to persist state on the client.Why persist state on the client?When building microservices, we need to build so-called “cloud native” applications.One of the key tenets of cloud native application design is keeping your services stateless.The benefit of having stateless applications is foremost the ability to respond to events by adding or removing instances without needing to significantly reconfigure or change the application.More stateless services can easily be added when load suddenly increases, or if an existing stateless service fails, it can simply be replaced with another.Hence, resilience and agility, are easier to achieve with stateless services.Keeping your services stateless means we need to persist our state somewhere else.Since we are transferring state in a REST architectural style, we can use the client to retain our state.For scaling purposes this is a great solution, as the client will only ever have to store its own state, and the server will be relieved of the state of all its clients.At our client we have chosen to use JWT for this state transfer to the client.While JWT is primarily intended for authentication and authorization purposes, the specification allows us to add any data we’d like to verify later on.Looking goodImagine the following scenario:A list of products is fetched from the “products microservice”.The user isn’t allowed to view all products, so only those products the user has access to are returned.When the user wants to order a product, he sends an order request to the “orders microservice” with the id of the product he wants to order.At that moment the “orders microservice” needs to know whether or not the user is allowed to access this product, let alone order it.Since the rights to access and order are the same, we’d like to reuse the information returned from the first call to the “products microservice”.This flow is illustrated below.We could call the “products microservice” from the “orders microservice” and rely on caching, but that would still be an extra network hop and the cache could potentially be invalidated by the time the user orders the product.Using the JWT approach, state is given to the client (the list of product ids the user is allowed to access), and being passed to the server again the moment an order is placed.The signature of the token guarantees us that the state has not been tampered with, while residing on the client.Too good to be trueThis solution prevents the server from having to care about state.It allows the client to store its own state and send it to the server whenever the server requires it - while being guaranteed the data isn’t tampered with.While this might seem like a good idea, it can backfire quickly.CouplingIn distributed systems such as microservices, it’s very important to manage the way we talk between components over the network.Using protocols such as HTTP and especially with the REST architectural style, great care needs to go in defining the contracts between these components.While we can use content-negotiation to version our resources, and JSON for instance as content type, we can build our clients as tolerant readers.Headers don’t have any of these benefits.A header is basically just a key and a value, and in case of JWT, the value is encoded.Therefore it’s hard to do versioning or any kind of content management on the data transferred inside these tokens.In the aforementioned example, the token couples the “products microservice” with the “orders microservice”.If the “products microservice” changes the structure of the token, the “orders microservice” will no longer be able to read it.While this coupling would exist as well when the “orders microservice” would call the “products microservice” directly, we would manage that coupling as part of the contract between these two microservices.In our case we don’t know there is a link between the two microservices since they don’t call each other directly.Yet by transferring the token from one microservice over the client to the other microservice, we are creating a hidden dependency.It’s also hard to have versioning on headers unless we put the version inside the name of the header.ScalingAdding versions to the headernames, documenting which microservices expect which versions of tokens of other microservices, and making sure we implement the tolerant-reader principle when reading the tokens might be a step in the right direction to avoid mass hysteria when tokens have to be adjusted.But what is simply impossible to get around, is the size restriction of headers in HTTP requests and responses.The HTTP specification doesn’t put any restriction on header size (singular or combined).But web servers, reverse proxies, CDNs and other network components do.Why they do this is not entirely clear as the spec allows any size, but the fact of the matter is that these restrictions exist.Putting a list of ids in a header like in our products example, will eventually break as the list could get too long.It’s not even clear how long is too long.AlternativesWe see three possible alternatives to this failed approach to manage state.Instead of passing the state from one microservice over a client to another microservice, we could pass the state as part of the body of the request and response.The downside of this approach is that we can no longer use GET methods for the calls where we need to pass the previously fetched state.The second alternative is to persist the state in a key value datastore on the server.We could asynchronously fetch products data and store it inside a datastore owned by the “orders microservice”.This could get stale, but so could a cache on the “products microservice”.This approach seems most common in the industry and could be well be the most preferable.And when all else fails, we can still simply make a call from the “orders microservice” to the “products microservice” and count on caching.ConclusionUsing Json Web Tokens as a means to transfer state to and from microservices via the client seemed like a good idea, but in the end turned out to be quite an anti-pattern.It introduces hidden coupling which is hard to manage, and can outright fail completely when headers become too big.Transferring state through the body of requests and responses could be a better approach.Using key value datastores to cache data of other microservices on your own microservice feels like the best way to go."
      },
    
      "angular-2016-04-25-component-based-application-architecture-with-angularjs-and-typescript-html": {
        "title": "Component-based application architecture with AngularJS and Typescript",
        "url": "/angular/2016/04/25/component-based-application-architecture-with-angularjs-and-typescript.html",
        "image": "/img/components-angularjs.jpg",
        "date": "25 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Introduction  Ideally, the whole application should be a tree of components that implement clearly defined inputs and outputs, and minimize two-way data binding. That way, it’s easier to predict when data changes and what the state of a component is.  – AngularJS documentationIn this article I will offer some basic guidelines on how to create a scalable AngularJS application with reusable, well encapsulated components that are easy to maintain and refactor.AngularJS (version 1.5.5 at the time of writing) and its latest features offers us the ability to structure our apps as a tree of components.  Each component contains its own controller and templateIt can even have its own (relative) routing configured if you take advantage of the new Component Router.If you are on a team with multiple front-end developers you can easily divide the work by letting each developer focus on a separate component.It also helps in migrating to Angular 2, though I cannot promise it will be an easy task.Another bonus point is you are getting into the mindset of modern front-end development: web components.My preferred toolchain when developing AngularJS applications consists of Typescript, NPM and Webpack.The sample code in this article and the sample application are created together with these tools.You can find the sample application on Github:https://github.com/ryandegruyter/angularjs-componentsWhat is a component?  In AngularJS a component is a directive.More specifically we call it a component directive or template directive.It is an approach to writing your own custom HTML elements which browsers are able to read and render.HTML comes with a set of pre-defined elements, for example the &lt;div&gt;&lt;/div&gt; element or the &lt;span&gt;&lt;/span&gt; element.By combing and nesting these standard HTML tags we can build complex UI widgets.We can change their appearance and behavior dynamically with JavaScript and CSS.True web components can isolate their structure, appearance and behavior.They make use of a technology called the Shadow DOM, which isolates the component in a separate DOM tree.This element will have its styles and scripts encapsulated, they will not conflict with the styles and scripts inside the parent DOM.Angular 2 takes full advantage of this technology, but unfortunately AngularJS, the framework I will be talking about in this article, does not.  Components you write and register inside AngularJS do not get isolated into a separate DOM tree.Lucky for us we are able to mimic the effect of Web components by using directives.We can write a reusable UI element, declare it with a custom tag and configure it by supplying attributes on the element.My advice is to be sure to use correct naming conventions and a module system so styles and scripts will not conflict with each other.Directives and componentsTo create and register a custom element in AngularJS, we can use either methods:  .directive (name, factoryFunction)  .component (name, object)While components are restricted to custom elements, directives can be used to create both elements as well as custom attributes.There are 3 types of directives:  Component directive  Attribute directive  Structural directiveA Component directive is a directive with a template.The .component() method is a helper method which creates a directive set with default properties.An Attribute directive is declared as an element attribute and they can change the appearance or behavior of an element (ng-change, ng-click, …).A Structural directive is an attribute or element that manipulates the DOM by adding or removing DOM elements (ng-if, ng-repeat, …).When do we use .directive(), and when do we use .component()?Custom UI elements should be created with the .component() helper method because it:  enforces best practices and provides optimizations (isolate scope, bindings)  has handy defaults making it easy to create components  makes migration to Angular 2 easier  can take advantage of the new component router which will be the default router in Angular 2  Use the .directive() method when you want to manipulate the DOM by adding or removing elements (Structural directive) or when you want to change the appearance or behavior of an element (Attribute directive).Creating a component-based AngularJS applicationBeginning with a component-based application architecture we need to have a root component.Before creating a component you have to decide if it will be a  Presentational component or a Container component.Presentational componentAlso known as a dumb component.They are used to visualize data and can easily be reused.They don’t manipulate application state nor do they fetch any data.Instead they define a public API in which they can receive inputs (&lt; and @ bindings), and communicate any outputs (&amp; binding) with their direct parent.  Designers can easily work on Presentational components because they don’t interfere with application logic.These components are unaware of any application state, and they only get data passed down to them.A simple presentational root component.export class RootComponent implements IComponentOptions {    static NAME:string = 'app';    bindings:any = {        title: '@',    };    template:string = `        &lt;h1&gt;{{$ctrl.title}}&lt;/h1&gt;        &lt;currency-converter&gt;&lt;/currency-converter&gt;    `;}    angular            .module('currencyConverterApp, []')            .component(Rootcomponent.NAME, new RootComponent());This component has a very simple API with one input -  title - and zero outputs.It doesn’t call a service or fetch any data.It doesn’t update any outside resources or make any requests to manipulate application state.Also notice how easy it was to register this component directive.Let’s create the same component directive but register it with the .directive() method.export class RootComponent implements IDirective {    static NAME:string = 'app';    restrict:string = 'E',    bindToController:any = {        title: '@',    },    scope:IScope = {},    controller:Function = ()=&gt;{},    controllerAs:string = '$ctrl',    template:string = `        &lt;h1&gt;{{$ctrl.title}}&lt;/h1&gt;        &lt;currency-converter&gt;&lt;/currency-converter&gt;     `;    static instance():IDirective {        return new RootComponent();    }}angular.module('currencyConverterApp, []').directive(Rootcomponent.NAME, RootComponent.instance());As you can see, this has a lot more configuration compared to using the .component() helper method.Although it offers more power and flexibility, its more practical to have the .component() method when creating custom UI elements.  bindings are automatically bound to the controller  controllerAs defaults to $ctrl  always creates an isolate scopeContainer componentsAlso known as smart components.This type of component is more tightly coupled to the application and not intended for reusability.It fetches data, manages part of the application state and provides the data to its child components.The child component communicates any update on the data through its output bindings (&amp;).The container component eventually decides what action to take with the data, not the child component.Let’s look at an example of a container component, I will leave out the complete template for brevity’s sake, you can view the complete code in the companion repository.First we start with our component definition:export class CurrencyConverter implements IComponentOptions{    static NAME:string = 'currencyConverter';    template:string = `        ...        &lt;currencies-select            title=\"From\"            on-selected=\"$ctrl.fromSelected(selectedCurrency)\"            show-values-as-rates=\"true\"            currencies=\"$ctrl.fromCurrencies\"        &gt;&lt;/currencies-select&gt;        &lt;currencies-select            title=\"To\"            on-selected=\"$ctrl.toSelected(selectedCurrency)\"            show-values-as-rates=\"true\"            currencies=\"$ctrl.toCurrencies\"        &gt;&lt;/currencies-select&gt;        ...\t\t    `;    controller:Function = CurrencyConverterComponentController;    }The template contains two declarations of a presentational component &lt;currencies-select&gt;.When we look at the attributes of the currencies-select element, the component API consists of three inputs (title, show-values-as-rates and currencies) and one output (on-selected).Our container component can bind a callback method on the on-selected attribute which offers an opportunity for the currencies-select component to communicate with its parent component.Below we define our components controller, here we can set and manipulate our template’s view model.export class CurrencyConverterComponentController {    selectedFromCurrency:Currency;    selectedToCurrency:Currency;    amount:number;    result:number;    fromCurrencies:Currency[];    toCurrencies:Currency[];    static $inject = [CurrenciesDataService.NAME];        constructor(private currencyDataService:CurrenciesDataService) {    }    $onInit():void {        this.fromCurrencies = this.toCurrencies = this.currencyDataService.getCurrenciesByYear(2016);    }    convert(from:number, to:number):void {        this.result = (this.amount / from) * to;    }    fromSelected(currency:Currency):void {        if(this.selectedToCurrency){            this.convert(currency.rate, this.selectedToCurrency.rate);        }        this.selectedFromCurrency = currency;    }    toSelected(currency:Currency):void {        if(this.selectedFromCurrency){            this.convert(this.selectedFromCurrency.rate, currency.rate);        }        this.selectedToCurrency = currency;    }}This component injects a data service to fetch a list of currencies.We pass this list to each &lt;currencies-select&gt; element in the $onInit method.The $onInit is a component lifecycle method that gets called by the framework each time the component gets instantiated.In this method we set our view model properties _fromCurrencies_ and _toCurrencies_ equal to a list of currencies fetched from the data service.The fromSelected and toSelected methods are passed down as callbacks for the &lt;currencies-select on-selected&gt; output.So how does our presentational component definition look like?export class CurrencySelectComponent implements IComponentOptions {    static NAME:string = 'currenciesSelect';    bindings:any = {        title: '@',        currencies: '&lt;',        onSelected: '&amp;',        showSelected: '&lt;',        showValuesAsRates: '&lt;'    };    controller:Function = CurrencySelectComponentController;    template:string = `...`;}export class CurrencySelectComponentController {    public title:string;    public currencies:Currency[];    public onSelected:Function;    public showSelected:boolean;    public showValuesAsRates:boolean;    public selected:Currency;    constructor() {    }    onCurrencyClick(currency:Currency) {        this.selected = currency;        this.onSelected({selectedCurrency: currency});    }}Bindings define the components API, in the above case there are four bindings.Our previous example declared this component but we only noticed three inputs and one output.Apparently there is a fourth input called _showSelected_.We can guess that it’s a flag for showing the selected currency.But as a new developer, we cannot be sure.  This is one of the reasons why it is important to document your components API.It will save new developers and designers a lot of time figuring out how to correctly use your component.Your component will become more transparent and not just an abstract definition.As you can see this component does not inject any data services or manage any outside state.It only receives data through its input bindings:  @ stands for one way string binding  &lt; stands for one way any other primitive/type bindingThe output binding public onSelected:Function; gets called each time the onCurrencyClick method gets called, it passes the selected currency which gets communicated back to the parent component.Make sure the parameter object key matches the parameter name in the parent component’s viewmodel, or the component will not be able to communicate any data.in this case selectedCurrency:onCurrencyClick(currency:Currency) {  this.selected = currency;  this.onSelected({selectedCurrency: currency});}And inside the parent component’s template:&lt;currencies-select on-selected=\"$ctrl.toSelected(selectedCurrency)\" ...Another way of accessing selectedCurrency, is to use $locals.This is useful when you want to send multiple types of data back.The advantage is you don’t have to specify each parameter separately in the component’s template.The disadvantage is $locals is not descriptive.&lt;currencies-select\ton-selected=\"$ctrl.toSelected($locals)\"...To access the selectedCurrency you would use the property on the $locals object with the same name:toSelected($locals:any):void{\tvar selected:Currency = $locals.selectedCurrency ...}Component communicationOutput bindingIn our previous example we saw an example of child to parent communication by mapping an output binding:binding:any = {\tonSelected: '&amp;'}The parent component can pass a method to this binding which the child component can call back and optionally send back any data to.Mapping the require propertyA child component can also require its parent components controller by mapping it in the require property:require:any = {\tparentCtrl: '^parentComponentName'}The ^ symbol is important here.You should replace parentComponentName with the correct component name, you are free to choose a different name for the key, in this case parentCtrl.The parent controller will get bound on to the property parentCtrl.Be aware that this creates a tight coupling between the child and parent component.Using a serviceWe should access and manipulate application state in our container components, but only through services, a component’s controller primary responsibility is to manage the template’s view model.You can implement a custom observer pattern inside the service, or use the rootscope as an eventbus.export class SampleService{    static SERVICE_NAME:string = \"mysampleservice\";    static EVENT_NAME:string = \"sampleEvent\";    static $inject = ['$rootScope'];    constructor(private $rootScope:IRootscopeService){}    subscribe(scope:IScope, callback:Function):void {        var handler = this.$rootScope.$on(SampleService.EVENT_NAME, callback);        scope.$on('$destroy', handler);    },    notify():void {        this.$rootScope.$emit(SampleService.EVENT_NAME);    }}A component controller can get notified by any changes by subscribing to the service:    export class MyComponentController{\t\t        static $inject = ['$scope', SampleService.SERVICE_NAME];        constructor(private isolatescope:IScope, private sampleService:SampleService){}        $onInit():void{            this.sampleService.subscribe(this.isolateScope, ()=&gt;{                ...            });         }    }SummaryStart with a root component and work your way down building components that are composed of either presentational and container components.Data should flow down in one direction (&lt; and @ input bindings), and events should propagate back up (&amp; output binding).  Services manage application state  Controllers manage a templates’ view model  Application state is accessed only by container components through services.  Use .component() when writing custom HTML elements in AngularJS  Use .directive() when you need to manipulate the DOM or need to change the appearance or behavior of a DOM element  Minimize 2 way binding (ngModel and = binding)  Presentational components can contain both container components and presentational components and vice versa  Use the component router, which makes it easy to bind URL paths to components. A component can contain its own relative routes too  Document your component’s API so new developers and designers know how to use it correctly  Keep your controllers clean, their main purpose is to set and manipulate the templates’ view model. Delegate business logic to services"
      },
    
      "microservices-2016-04-22-lagom-first-impressions-and-initial-comparison-to-spring-cloud-html": {
        "title": "Lagom: First Impressions and Initial Comparison to Spring Cloud",
        "url": "/microservices/2016/04/22/Lagom-First-Impressions-and-Initial-Comparison-to-Spring-Cloud.html",
        "image": "/img/lagom.png",
        "date": "22 Apr 2016",
        "category": "post, blog post, blog",
        "content": "  “It’s open source. It’s highly opinionated.   Build greenfield microservices and decompose your Java EE monolith like a boss.” - LightbendTable of Contents  Just the right amount  Design philosophy  Building blocks  Getting started with Lagom  Anatomy of a Lagom project  Example of a microservice  CQRS and Event Sourcing  Lightbend Q&amp;A at the CodeStar launch event  Comparison with Spring  Our advice  Conclusion  Useful linksJust the right amountMeet Lagom, Lightbend’s (formerly Typesafe) new open source framework for architecting microservices in Java.On the 10th of March, Lightbend released the first MVP version of Lagom which is the current version at the time of writing.Although there is currently only a Java API, Scala enthusiasts should not fret because a Scala API is a main priority and well on its way.Lagom is a Swedish word meaning “just the right amount”.Microservices have often been categorised as small services.However, Lightbend wants to emphasize that finding the right boundaries between services, aligning them with bounded contexts, business capabilities, and isolation requirements are the most important aspects when architecting a microservice-based system.Therefore, it fits very well in a Domain-Driven Design focused mindset.Following this will help in building a scalable and resilient system that is easy to deploy and manage.According to Lightbend the focus should not be on how small the services are, but instead they should be just the right size, “Lagom” size services.Lagom, being an opinionated framework, provides a “golden path” from which the developer can deviate if necessary.Being based on the reactive principles as defined in the Reactive Manifesto, Lagom provides the developer a guard-railed approach with good defaults while also allowing to deviate if necessary.This blogpost will cover our initial impression on the framework together with our opinion on the choices made while architecting the framework.Note that we won’t go too deep into detail in all the different aspects of the framework, for more details refer to Lagom’s extensive documentation.As Lightbend is entering the microservices market with Lagom, we feel obliged to make a fair comparison with existing frameworks out there.In the Java world this is predominantly the Spring stack with Spring Boot and Spring Cloud, standing on the shoulders of giants such as the Netflix OSS.In this current stage, it would be a bit too early to make an in-depth comparison between the two, seeing as you would be comparing a mature project to an MVP.What we can share though, are our initial observations.Design philosophyLagom’s design rests on the following principles:  Message-Driven and Asynchronous: Built upon Akka Stream for asynchronous streaming and the JDK8 CompletionStage API.Streaming is a first-class concept.  Distributed persistence: Lagom favours distributed persistence patterns using Event Sourcing with Command Query Responsibility Segregation (CQRS).  Developer productivity: Starting all microservices with a single command, code hot reloading and expressive service interface declarations are some examples of Lagom’s high emphasis on developer productivity.Building blocksThe Lagom framework acts as an abstraction layer upon several Lightbend frameworks and consists of the following core technologies and frameworks:  Scala  Java  Play Framework  Akka and Akka Persistence  sbt  Cassandra  Guice  ConductRSeeing as it acts as an abstraction layer the developer doesn’t need to hold any knowledge of Play Framework and Akka in order to successfully use Lagom.Sbt has been chosen as the build tool because it also acts as a development environment.Lagom relies heavily on the following sbt features:  Fine-grained tasks  Each task may return a value  The value returned by a task may be consumed by other tasksAccording to Lightbend, Scala’s build tool ‘sbt’ offers many handy features to Lagom such as fast incremental recompilation, hot code reloading, starting and stopping services in parallel and automatic injection of configuration defaults.Sbt might be seen as a hurdle by most Java developers since it is Maven and Gradle (and to a lesser extent Ant) that rule most Java projects.Moving towards a microservices framework such as Lagom would already constitute quite a transition so we think that this might hold back Java developers from adopting the framework.Lightbend’s rebranding could be interpreted as a move away from a Scala-oriented company towards a more Java-minded company.In that regard it would make sense to lower the initial learning curve especially for a rather trivial component such as a building tool.After all, the most important thing to achieve adoption is allowing people to easily get started with the new technology. We think that providing integration for Maven or Gradle would have a positive effect on the adoption rate and although it may not be trivial to implement, it should help convince Java developers to give Lagom a go.Google’s Guice has been chosen for dependency injection since it is a lightweight framework.What is remarkable is that Guice is used as well for intermicroservices calls.Lagom acts as a communication abstraction layer and it does so by adding a dependency on the interfaces of remote microservices.Just like a shared domain model and shared datastores being antipatterns for microservices, having code dependencies from one service in another is as well.Changing the code of one microservice should not have an immediate cascading effect on other microservices.This is the very essence of the microservices architecture.In a monolith, having code changes in one component can result in immediate breaking changes in other components of the system.While this may be desired in order to keep technical debt low, this is an inherent characteristic of monolithic systems.One of the reasons microservices were introduced, is to decouple components on all levels, especially binary coupling.Using protocols between components instead of actual binary dependencies allows us to implement the tolerant reader principle and versioning through for instance content negotiation.Lightbend argues that sharing interfaces as code will increase productivity and performance, but we fear the result of this is a distributed monolith instead of an actual decoupled microservices architecture.While we question the default way of communicating between microservices in Lagom, we are enthusiastic that more ways of making intermicroservices calls are becoming available.Using HTTP is possible as well, and one of the upcoming features is a Lagom Service Client.The Guice approach might also be quite favorable for people migrating from monolithic applications to microservices.In the end it is a trade-off, but one that shouldn’t be taken lightly.As a default persistence solution, Apache Cassandra is used due to how well it integrates with CQRS and Event Sourcing.Lagom has support for Cassandra as datastore, both for the reading and writing data.It is possible to use other datastore solutions but this comes at the cost of not being able to take advantage of the persistence module in Lagom.ConductR is an orchestration tool for managing Lightbend Reactive Platform applications across a cluster of machines and is Lightbend’s solution for running Lagom systems in production.Note that ConductR comes with a license fee and is majorly targeted at enterprises.The other option we currently have in order to run our Lagom system in production is to write our own service locator compatible with Lagom.At the time of writing someone already started working on Kubernetes support and we are sure that, given more time, more options will become available.For now though, Lagom is still in an early stage where we either have to pay for the ConductR license, build our own service locator, or wait until someone does the work for us.Getting started with LagomIn order to start using Lagom, Activator must be correctly set up.Currently two Lagom templates exist that can be used for creating a new Lagom application.The Lagom Java Seed template should be the template of choice, the Lagom Java Chirper template is an example of a Twitter-like app created in Lagom.Creating a new Lagom application is as simple as using the following command:$ activator new my-first-system lagom-javaAfterwards the project can be imported in any of the prominent IDEs as an sbt project.In order to boot the system, we first need to navigate to the root of the project and start the Activator console:$ activatorAfter which we can start all our services using a single simple command:$ runAll&gt; runAll[info] Starting embedded Cassandra server.......[info] Cassandra server running at 127.0.0.1:4000[info] Service locator is running at http://localhost:8000[info] Service gateway is running at http://localhost:9000[info] application - Signalled start to ConductR[info] application - Signalled start to ConductR[info] application - Signalled start to ConductR[info] Service helloworld-impl listening for HTTP on 0:0:0:0:0:0:0:0:24266[info] Service hellostream-impl listening for HTTP on 0:0:0:0:0:0:0:0:26230[info] (Services started, use Ctrl+D to stop and go back to the console...)This command starts a Cassandra server, service locator and service gateway.Each of our microservices is started in parallel while also registering them in the service locator.Additionally, a run command to individually start services is available as well.Note that the ports are assigned to each microservice by an algorithm and are consistent even on different machines.The possibility to assign a specific port is available though.Similar to Play Framework, Lagom also supports code hot reloading allowing you to make changes in the code and immediately seeing these changes live without having to restart anything.A feature we’re very fond of.In general, a restart is only required when adding a new microservice API and implementation module in the project.Anatomy of a Lagom projecthelloworld-api           → Microservice API submodule └ src/main/java         → Java source code interfaces with model objectshelloworld-impl          → Microservice implementation submodule └ logs                  → Logs of the microservice └ src/main/java         → Java source code implementation of the API submodule └ src/main/resources    → Contains the microservice application config └ src/test/java         → Java source code unit testslogs                     → Logs of the Lagom systemproject                  → Sbt configuration files └ build.properties      → Marker for sbt project └ plugins.sbt           → Sbt plugins including the declaration for Lagom itself.gitignore               → Git ignore filebuild.sbt                → Application build scriptExample of a microserviceIn order to write a new microservice you create a new API and implementation project.In the API project you define the interface of your microservice:HelloService.javapublic interface HelloService extends Service {  ServiceCall&lt;String, NotUsed, String&gt; hello();    ServiceCall&lt;String, GreetingMessage, String&gt; useGreeting();  @Override  default Descriptor descriptor() {    return named(\"helloservice\").with(        restCall(Method.GET,  \"/api/hello/:id\",       hello()),        restCall(Method.POST, \"/api/hello/:id\",       useGreeting())      ).withAutoAcl(true);  }}A Descriptor defines the service name and the endpoints offered by a service. In our case we define two REST endpoints, a GET and a POST.GreetingMessage is basically an immutable class with a single String message instance variable.On the subject of immutability the Lagom documentation mentions Immutables, a Java library that helps you create immutable objects via annotation processing.Definitely worth a look seeing as it helps you get rid of boilerplate code.In the implementation submodule we implement our API’s interface.HelloServiceImpl.javapublic class HelloServiceImpl implements HelloService {  @Override  public ServiceCall&lt;String, NotUsed, String&gt; hello() {    return (id, request) -&gt; {      CompletableFuture.completedFuture(\"Hello, \" + id);    };  }  @Override  public ServiceCall&lt;String, GreetingMessage, String&gt; useGreeting() {    return (id, request) -&gt; {      CompletableFuture.completedFuture(request.message + id);    };  }}You’ll immediately notice that the service calls are non-blocking by default using CompletableFutures introduced in JDK8.Interesting to know is that Lagom also provides support for the Publish-subscribe pattern out of the box.We also need to implement the module that binds the HelloService so that it can be served.HelloServiceModule.javapublic class HelloServiceModule extends AbstractModule implements ServiceGuiceSupport {  @Override  protected void configure() {    bindServices(serviceBinding(HelloService.class, HelloServiceImpl.class));  }}We define our module in the application.config:play.modules.enabled += sample.helloworld.impl.HelloServiceModuleAnd finally register our microservice in build.sbt with its dependencies and settings:lazy val helloworldApi = project(\"helloworld-api\")  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies += lagomJavadslApi  )lazy val helloworldImpl = project(\"helloworld-impl\")  .enablePlugins(LagomJava)  .settings(    version := \"1.0-SNAPSHOT\",    libraryDependencies ++= Seq(      lagomJavadslPersistence,      lagomJavadslTestKit    )  )  .settings(lagomForkedTestSettings: _*)  .dependsOn(helloworldApi)We can then test our endpoint:$ curl localhost:24266/api/hello/WorldHello, World!$ curl -H \"Content-Type: application/json\" -X POST -d '{\"message\": \"Hello \"}' http://localhost:24266/api/hello/WorldHello WorldSeeing as any good developer writes unit tests for his/her code so should we!public class HelloServiceTest {  private static ServiceTest.TestServer server;  @BeforeClass  public static void setUp() {    server = ServiceTest.startServer(ServiceTest.defaultSetup());  }  @AfterClass  public static void tearDown() {    if (server != null) {      server.stop();      server = null;    }  }  @Test  public void shouldRespondHello() throws Exception {    // given    HelloService service = server.client(HelloService.class);    // when    String hello = service.hello().invoke(\"Yannick\", NotUsed.getInstance()).toCompletableFuture().get(5, SECONDS);    // then    assertEquals(\"Hello, Yannick\", hello);  }  @Test  public void shouldRespondGreeting() throws Exception {    // given    HelloService service = server.client(HelloService.class);    // when    String greeting = service.useGreeting().invoke(\"Yannick\", new GreetingMessage(\"Hi there, \")).toCompletableFuture().get(5, SECONDS);    // then    assertEquals(\"Hi there, Yannick\", greeting);  }}Tests can be executed in Activator via the following command:$ test&gt; test[info] Test run started[info] Test sample.helloworld.impl.HelloServiceTest.testHello started[info] Test sample.helloworld.impl.HelloServiceTest.testGreeting started[info] Test run finished: 0 failed, 0 ignored, 2 total, 16.759s[info] Passed: Total 2, Failed 0, Errors 0, Passed 2[success] Total time: 21 s, completed Apr 14, 2016 10:06:41 AMCQRS and Event SourcingBeing an opinionated framework Lagom suggests to use CQRS and Event Sourcing seeing as it fits well within the reactive paradigm.In this blogpost we are not going to explain CQRS and Event Sourcing in detail seeing as it is very well documented in the documentation of Lagom.The gist of it is that each service should own its own data and only the service itself should have direct access to the database.Other services need to use the service’s API in order to interact with its data.Sharing the database across different services would result in tight coupling.Ideally we want to work with Bounded Contexts following the core principles of Domain-Driven Design where each service defines a Bounded Context.Using Event Sourcing gives us many advantages such as not only storing the current state of data but having an entire journal of events that tell us how the data achieved its current state.With event sourcing we only perform reads and writes, there are no updates nor deletes.All this makes it easy to test and debug and allows us to easily reproduce scenarios that happened in production by replaying the event log from that environment.Note that just because Lagom encourages us to use CQRS and Event Sourcing it isn’t forcing us to use it as it is not always applicable to every use case.It is perfectly possible to, for example, plug in a PostgreSQL database for our persistence layer.Someone has already set up PostgreSQL integration using Revenj persistence.However, Lightbend suggests that for best scalability preference must be given to asynchronous APIs because using blocking APIs like JDBC and JPA will have an impact on that.By default, when launching our development environment, a Cassandra server will be booted without having to do any setup ourselves besides adding the lagomJavadslPersistence dependency to our implementation in our build.sbt.Regarding the code, a persistent entity needs to be defined, combined with a related command, event and state.Note that the following code samples are mainly here to give an idea of the work needed for implementing all this.For more information and a detailed explanation, consult the excellent documentation on the subject.In the persistent entity we define the behaviour of our entity.In order to interact with event sourced entities, commands need to be sent.We therefore need to specify a command handler for each command class that the entity can receive.Commands are then translated into events which will get persisted by the entity.Each event has its own event handler registered.Example of a PersistentEntity:HelloWorld.javapublic class HelloWorld extends PersistentEntity&lt;HelloCommand, HelloEvent, WorldState&gt; {  @Override  public Behavior initialBehavior(Optional&lt;WorldState&gt; snapshotState) {    BehaviorBuilder b = newBehaviorBuilder(        snapshotState.orElse(new WorldState(\"Hello\", LocalDateTime.now().toString())));    b.setCommandHandler(UseGreetingMessage.class, (cmd, ctx) -&gt;      ctx.thenPersist(new GreetingMessageChanged(cmd.message),        evt -&gt; ctx.reply(Done.getInstance())));    b.setEventHandler(GreetingMessageChanged.class,        evt -&gt; new WorldState(evt.message, LocalDateTime.now().toString()));    b.setReadOnlyCommandHandler(Hello.class,        (cmd, ctx) -&gt; ctx.reply(state().message + \", \" + cmd.name + \"!\"));    return b.build();  }}Our PersistentEntity requires a state to be defined:WorldState.java@Immutable@JsonDeserializepublic final class WorldState implements CompressedJsonable {  public final String message;  public final String timestamp;  @JsonCreator  public WorldState(String message, String timestamp) {    this.message = Preconditions.checkNotNull(message, \"message\");    this.timestamp = Preconditions.checkNotNull(timestamp, \"timestamp\");  }  @Override  public boolean equals(@Nullable Object another) {    if (this == another)      return true;    return another instanceof WorldState &amp;&amp; equalTo((WorldState) another);  }  private boolean equalTo(WorldState another) {    return message.equals(another.message) &amp;&amp; timestamp.equals(another.timestamp);  }  @Override  public int hashCode() {    int h = 31;    h = h * 17 + message.hashCode();    h = h * 17 + timestamp.hashCode();    return h;  }  @Override  public String toString() {    return MoreObjects.toStringHelper(\"WorldState\").add(\"message\", message).add(\"timestamp\", timestamp).toString();  }In our command interface we define all the commands that our entity supports.In order to get a complete picture of the commands an entity supports, it is the convention to specify all supported commands as inner classes of the interface.HelloCommand.javapublic interface HelloCommand extends Jsonable {  @Immutable  @JsonDeserialize  public final class UseGreetingMessage implements HelloCommand, CompressedJsonable, PersistentEntity.ReplyType&lt;Done&gt; {    public final String message;    @JsonCreator    public UseGreetingMessage(String message) {      this.message = Preconditions.checkNotNull(message, \"message\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof UseGreetingMessage &amp;&amp; equalTo((UseGreetingMessage) another);    }    private boolean equalTo(UseGreetingMessage another) {      return message.equals(another.message);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + message.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"UseGreetingMessage\").add(\"message\", message).toString();    }  }  @Immutable  @JsonDeserialize  public final class Hello implements HelloCommand, PersistentEntity.ReplyType&lt;String&gt; {    public final String name;    public final Optional&lt;String&gt; organization;    @JsonCreator    public Hello(String name, Optional&lt;String&gt; organization) {      this.name = Preconditions.checkNotNull(name, \"name\");      this.organization = Preconditions.checkNotNull(organization, \"organization\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof Hello &amp;&amp; equalTo((Hello) another);    }    private boolean equalTo(Hello another) {      return name.equals(another.name) &amp;&amp; organization.equals(another.organization);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + name.hashCode();      h = h * 17 + organization.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"Hello\").add(\"name\", name).add(\"organization\", organization).toString();    }  }}And finally we want to define all events that the entity supports in an event interface.It follows the same convention as with commands, specifying all events as inner classes of the interface.HelloEvent.javapublic interface HelloEvent extends Jsonable {  @Immutable  @JsonDeserialize  public final class GreetingMessageChanged implements HelloEvent {    public final String message;    @JsonCreator    public GreetingMessageChanged(String message) {      this.message = Preconditions.checkNotNull(message, \"message\");    }    @Override    public boolean equals(@Nullable Object another) {      if (this == another)        return true;      return another instanceof GreetingMessageChanged &amp;&amp; equalTo((GreetingMessageChanged) another);    }    private boolean equalTo(GreetingMessageChanged another) {      return message.equals(another.message);    }    @Override    public int hashCode() {      int h = 31;      h = h * 17 + message.hashCode();      return h;    }    @Override    public String toString() {      return MoreObjects.toStringHelper(\"GreetingMessageChanged\").add(\"message\", message).toString();    }  }}The HelloServiceImpl.java class will look like the following:public class HelloServiceImpl implements HelloService {  private final PersistentEntityRegistry persistentEntityRegistry;  @Inject  public HelloServiceImpl(PersistentEntityRegistry persistentEntityRegistry) {    this.persistentEntityRegistry = persistentEntityRegistry;    persistentEntityRegistry.register(HelloWorld.class);  }  @Override  public ServiceCall&lt;String, NotUsed, String&gt; hello() {    return (id, request) -&gt; {      PersistentEntityRef&lt;HelloCommand&gt; ref = persistentEntityRegistry.refFor(HelloWorld.class, id);      return ref.ask(new Hello(id, Optional.empty()));    };  }  @Override  public ServiceCall&lt;String, GreetingMessage, Done&gt; useGreeting() {    return (id, request) -&gt; {       PersistentEntityRef&lt;HelloCommand&gt; ref = persistentEntityRegistry.refFor(HelloWorld.class, id);       return ref.ask(new UseGreetingMessage(request.message));    };  }}Lightbend Q&amp;A at the CodeStar launch eventOn the 24th of March we attended the launch event of CodeStar, the new unit from our Dutch Ordina colleagues focused on Full Stack Scala and Big Data solutions.CodeStar also hold a Lightbend partnership.One of the presentations was an introduction to Lagom by Markus Eisele, Developer Advocate at Lightbend.After his talk we had the opportunity to ask Markus and his colleague, Lutz Hühnken, Solutions Architect at Lightbend, several questions regarding Lagom.      What do you guys consider to be the major competitor for Lagom?Spring Cloud and Netflix OSS?          Yes, we would consider that stack to be Lagom’s main competitor. But we believe that with Lagom we have a number of unique features that makes us shine (because otherwise we wouldn’t have built it):1) Lagom’s development environment, in my humble opinion a major productivity boost2) Fostering good practices for building reactive services seeing as Lagom is opinionated, e.g. async communication by default, ES/CQRS, …3) Batteries-included, from development to production4) Streaming is first-class            Lagom suggests that Event Sourcing and CQRS should be used as the default solution for persistence but is it really applicable in the majority of the scenarios?          Lagom is an opinionated framework and will try to suggest using ES &amp; CQRS as the primary solution to use since it fits very well with the reactive mindset.Of course it also depends on the use case.            Don’t you think you encourage code coupling by having microservices depend on the interface of another microservice?          It is true that the default way to do service calls between Lagom services is to use binary dependencies, though of course it is not enforced. We have taken great care to ensure that service calls map down to idiomatic REST and/or websockets. We do have plans in the future to allow simple removal of the binary coupling. To make service interfaces go through a non-binary specification such as Swagger, where Swagger specs will be generated and interfaces will be generated from the Swagger specs.            Does Lagom support REST level 3? Is there support for hypermedia?          Currently not supported but we are open to it. Feel free to create a suggestions ticket at the GitHub project.            Don’t you think it is a bad idea to only support ConductR for production deployments?What about pet projects of single developers? This makes it less appealing to motivate people to pick up Lagom compared to for example Spring Cloud and Netflix OSS.          It is in the strategic planning of Lightbend to push ConductR forward as the main solution for your production environment.Do note that it’s perfectly possible to deploy your Lagom services elsewhere as long as you implement your own service locator (as an example, the integration needed to support Lagom in ConductR is available on GitHub).Looking at our Open Source Position Statement you will notice that one of the differentiators we see between our open source offerings and the commercial products is Time. Open source users tend to invest their time rather than their money. ConductR integration into Lagom could be seen as an example for this. If you would rather spend the money than invest time, buy ConductR. If you would rather invest time instead of money, build your own ServiceLocator implementation and use a different infrastructure.An example of this is the GitHub issue for implementing Kubernetes support.            How do you integrate with other non-Lagom microservices?          Currently you would call them via REST URLs. In the near future the Lagom Service Client could also be used to consume them. Additionally it should also be possible to integrate Eureka in Lagom.            What is the deployment procedure exactly? How do I prepare my Lagom application for deployment into production?          The deployment unit in ConductR is a bundle which is an abstract term that can mean a Docker image or a zip file with a certain structure.By default, when you have multiple services in one project, it will create multiple bundles. You call bundle:dist once on the top level and it will create a separate bundle for each service which can then be deployed to ConductR.You can put multiple components in one bundle so you could have multiple services in one bundle, but we think that it is unusual.Ideally, each service needs to be its own bundle managed in isolation by ConductR, for it to be able to be able to be developed, rolled out, upgraded and failed in isolation.            What about API versioning?          Currently there is no versioning for your services besides the “default” way to do it, e.g. via the header or by versioning your urls.            What do you think about the so-called serverless architectures like AWS Lambda or Google Cloud Functions?          We think that those architectures are part of the future. Lagom can be seen as a step in that direction since it decouples the stateless part of the service (the behavior) from the stateful (persistent entity), allowing the stateless part to be scaled out independently, and automatically by the runtime, in a similar fashion to AWS Lambda. A hosted version of Lagom could give a very similar experience.            About sbt, will you also support a more widely adopted tool such as Maven or Gradle?          Lagom relies on some sbt features, so supporting other build tools is not trivial. While it is probably doable to support Maven, we’d need to do build a proof-of-concept to verify this. This is currently not prioritized. We’ll be watching the community’s feedback on this.            Does the Lagom circuit breaker have a dashboard such as the Hystrix dashboard? Does Lagom in general have operational dashboards?          You could integrate the circuit breaker data with monitoring tools such as Graphite and Grafana.In addition, with Lightbend Monitoring you do get a suite of tools for monitoring your microservices. Lightbend Monitoring is included in the ConductR license.            Is it true that Typesafe rebranded to Lightbend to get a broader adoption than what was possible with a more Scala-orientated reputation attached to Typesafe?          That is correct.This doesn’t mean that we are giving up on Scala, it is still core to all of our technologies.      Comparison with SpringSpring has been out there for more than 10 years and with Spring Boot and Spring Cloud a trend has been set to move to self-contained applications as a basis for microservices development.Spring reaps the fruits of the Netflix OSS while offering Spring’s own components such as Spring Cloud Config and Spring Security as well.The Netflix and Spring stack comes with all the necessary tools to build and run microservices in production.Externalized configuration, out-of-the-box free dashboards for service registries, circuit breaker monitoring and distributed tracing, integration with service registries such as Eureka, Consul and Zookeeper, production-ready monitoring and metrics features with Actuator endpoints, integration with build tools such as Maven and Gradle and extensive security features including upcoming integration with Vault are only a subset of the features Spring has to offer.Seeing as Lagom is still in its early days, it wouldn’t be fair to Lightbend to make an in-depth comparison with the Spring stack.We hope that Lagom will continue to grow towards a more mature framework and a true alternative to Spring on all levels.The first steps we currently see look promising and we hope that they will consider our remarks for how they want to further evolve the framework.It is great to see more microservices frameworks become available and we applaud Lightbend for taking up the competition with Spring.Our adviceOur advice is to keep track of Lagom’s progress closely.If you are currently looking for a mature framework with integration capabilities for just about anything, go with Spring.If you want to use Event Sourcing, Lagom should be a great fit. Additionally, Lagom’s focus on CQRS and its reactive core are truly differentiators with other frameworks.Lagom has great potential and is eager to get community involvement. If you are willing to join forces with Lightbend, Lagom might already be a viable candidate for you.ConclusionWe think that Lagom looks very promising and we will definitely follow it up.Due to Lagom being an opinionated framework everything glues together well.Lagom is just a thin layer on top of Akka and Play, which is very mature and hardened over the years.It might be a bit too early to do an in-depth comparison between Lagom and Spring Cloud since we would be comparing an MVP against a mature technology.We do think that using sbt might be a hurdle for Java developers and it would ease adoption if there would be other ways to use Lagom in production besides ConductR.As it stands right now you would need to write a custom service locator yourself.It would close the gap with Spring if support would already be available for service discovery via for example Eureka or Consul.It is clear that Lagom puts a lot of focus on reactiveness and gaining the best performance. This could come at the cost of binary coupling, seeing as the default way to do service calls between Lagom services is to use binary dependencies. We are looking forward to Lightbend’s plans to go through non-binary specifications in order to reduce coupling on a binary level as well.Given that it is currently an MVP version we are interested in seeing how Lagom matures. Since it is all new and shiny, you will be able to give back to the community by helping to develop parts of this new and exciting framework yourself.Contributing to the framework is easy via pull requests and are actively reviewed by Lightbend developers.The developers are very active on their Gitter channel and they are quick to answer questions. We are also very excited to the release of the Scala API.Our colleague Andreas Evers, who has extensive knowledge on Spring Cloud and Netflix OSS, will soon be participating in a podcast with Markus Eisele hosted by Lightbend to discuss Lagom and microservices trends.The date should be announced soon.Be sure to follow Andreas and Lightbend to catch it!Useful links  Lagom  Lagom documentation  Lagom Twitter  Lagom GitHub  Lagom Gitter  Lightbend  Lightbend Twitter  Markus Eisele Twitter  Lutz Hühnken Twitter"
      },
    
      "microservices-2016-04-09-ordina-becomes-netflix-oss-contributor-html": {
        "title": "Ordina becomes Netflix OSS contributor",
        "url": "/microservices/2016/04/09/Ordina-becomes-Netflix-OSS-contributor.html",
        "image": "/img/netflix.jpg",
        "date": "09 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Netflix has officially added Ordina as active user and contributor to their open source cloud and microservices tools and frameworks.Ordina continues to be a leading force in cloud and microservice architectures in the BeNeLux.Since 2011, Netflix has been releasing more and more components of their cloud platform and utilities as free and open source software. These projects are available to the public through Netflix OSS.Starting around 2009, Netflix completely redefined its application development and operations models. They were driven completely by APIs and riding the initial wave of what we would come to know as microservices. Industry onlookers derided the company with disbelief and uncertainty. While this may work with Netflix, no one else can possibly do this. Fast-forward to 2016, when most of those sentiments changed to commitments of active migration to microservices. The concept is definitely both valid and powerful.Pivotal has embraced these technologies and made them approachable for the masses through the Java-based Spring ecosystem. In the Spring Cloud OSS program, an abstraction layer is added on top of Netflix’s components to ease adoption outside of Netflix. This allows local companies without silicon valley-grade scientists to embrace microservices and their benefits.Ordina has successfully architected and implemented the Netflix and Spring Cloud stack at clients in Belgium. We continue to do so and are proud we can call ourselves Netflix OSS and Spring Cloud contributors. Netflix has added our logo to their contributors page for our continued adoption and contributions to the Netflix and Spring Cloud ecosystem.Our expertise and experience using and contributing to the Netflix and Spring Cloud stack is second to none in the BeNeLux. We are always looking to help new or existing clients to migrate to microservices and make the step to Cloud Native architectures.If this sounds interesting to your company, or these architectures personally excite you, make sure to contact us directly or take a look at jobs.ordina.be."
      },
    
      "ionic-2016-04-07-ionic-protractor-html": {
        "title": "Protractor testing in Ionic app",
        "url": "/ionic/2016/04/07/Ionic-Protractor.html",
        "image": "/img/ionic-protractor.png",
        "date": "07 Apr 2016",
        "category": "post, blog post, blog",
        "content": "Using Protractor in an ionic appSince a few days I’ve been playing around with Protractorand I am also involved on an internal project in which an Ionic app has to be created.So I thought:  Why not use Protractor in my Ionic app?So here we are.It’s not hard to get started and I will explain how I got it working.For the full example please refer to the ionic documentation1. Getting startedFirst of all you need Node.js.When Node.js is installed you should install Ionic and Cordova using npm (Node Package Manager):npm install -g cordova ionicI personally needed an app with tabs, but you can also start a blank app or an app with a side menu:ionic start &lt;your app name&gt; tabsYou can also choose blank or sidemenu instead of tabs2. Running the app in the browserOnce the Ionic app has been generated, you can run t it in the browser using the following command:cd &lt;your app name&gt;ionic serve  It’s worth noting that Ionic has live reload built in by default.So any changes will be immediatelyreflected in the browser.To view the application using the iOS and Android styling applied you can use the following command: ionic serve -lab There are many more awesome things you can do with the Ionic CLI.If you want to know more about the CLI you can find it in the Ionic documentation.3. Structuring the applicationAt this moment you are set up with an Ionic starter app.The first thing I did was refactor the code from technical to functional modules.                    --&gt;                  I strongly advise to use functional modules, it’s easier to work with.Related code should be in one folder and when testing you can use the same structure to test each module separately while coding.  You’ll find yourself navigating less trough your open tabs or a tree-view.Additionally, it makes your code more comprehensible to other developers.The style guide from John Papa on how to structure AngularJS applications is a very good resource.4. Sign-in pageThe one we’ll be testingAfter refactoring, I implemented a sign-in page, which has no access to the tabs.The code can be seen below.If you work in functional modules like I do, it is as easy as referring to the controller and the service from index.html, then pass starter.sign-in as a module to your application.sign-in/sign-in-controller.js:(function () {    angular        .module('starter.sign-in',[])        .controller('SignInCtrl',SignInCtrl);    SignInCtrl.$inject = ['$scope','$state','SignInService'];    function SignInCtrl ($scope,$state,SignInService) {        $scope.user = {};        $scope.signIn = function() {            SignInService.signIn($scope.user)                .then(function(data){                    if(data){                        $state.go('tab.rooms');                    }else{                        $scope.incorrect = true;                    }                })        };    }})();sign-in/signin.html:&lt;ion-view view-title=\"Sign in\"&gt;    &lt;ion-content&gt;        &lt;form name=\"frmLogin\" novalidate ng-cloak&gt;            &lt;ion-list&gt;                &lt;ion-item class=\"item item-input item-floating-label\"&gt;                    &lt;label&gt;                        &lt;span class=\"input-label\"&gt;Username&lt;/span&gt;                        &lt;input type=\"email\" ng-model=\"user.username\" ng-required=\"true\" placeholder=\"Username\" id=\"username\" name=\"username\"&gt;                    &lt;/label&gt;                    &lt;div class=\"assertive\" ng-if=\"frmLogin.$submitted || frmLogin.username.$touched\"&gt;                        &lt;div ng-if=\"frmLogin.username.$error.required\"&gt;Username is required&lt;/div&gt;                        &lt;div ng-if=\"frmLogin.username.$error.email\"&gt;Username is not valid&lt;/div&gt;                    &lt;/div&gt;                &lt;/ion-item&gt;                &lt;ion-item class=\"item item-input item-floating-label\"&gt;                    &lt;label&gt;                        &lt;span class=\"input-label\"&gt;Password&lt;/span&gt;                        &lt;input type=\"password\" ng-model=\"user.password\" ng-minlength=\"4\" ng-required=\"true\" placeholder=\"Password\" id=\"password\" name=\"password\"&gt;                    &lt;/label&gt;                    &lt;div class=\"assertive\" ng-if=\"frmLogin.$submitted || frmLogin.password.$touched\"&gt;                        &lt;div ng-show=\"frmLogin.password.$error.required\"&gt;Password is required&lt;/div&gt;                    &lt;/div&gt;                &lt;/ion-item&gt;                &lt;div class=\"padding\"&gt;                    &lt;div class=\"assertive\" ng-if=\"incorrect\"&gt;                        &lt;div&gt;Username or password is incorrect.&lt;/div&gt;                    &lt;/div&gt;                    &lt;button id=\"btnSignIn\" ng-disabled=\"frmLogin.$invalid\" class=\"button button-full button-positive\" ng-click=\"signIn()\"&gt;                        Sign in                    &lt;/button&gt;                &lt;/div&gt;            &lt;/ion-list&gt;        &lt;/form&gt;    &lt;/ion-content&gt;&lt;/ion-view&gt;sign-in/sign-in.service.js:(function() {    angular        .module('starter.sign-in')        .factory('SignInService', SignInService);    SignInService.$inject = ['$timeout'];    function SignInService ($timeout) {        var _user = {            email: 'yannick@gmail.com',            pass: '1234'        };        function signIn (user) {            return $timeout(function() {                return true;                //return !!(user.username === _user.email &amp;&amp; user.password == _user.pass);            },2000);        }        return {            signIn : signIn        };    }})();Next you need to provide a state, so add the following in app.js:state('signin',{    url:'/signin',    templateUrl: 'sign-in/sign-in.html',    controller: 'SignCtrl')}and change redirect to /signin by default:$urlRouterProvider.otherwise('/signin');For complete authentication you should check the authenticated state when changing pages,but that’s not in the scope of this blog5. Preparing protractorThe sign-in part is the one I am going to test with Protractor.First thing to do, is to install Protractor on your system:npm install -g protractorThe webdriver manager is a helper tool to easily get a Selenium server running.Run the following commands in order to start it:webdriver-manager updatewebdriver-manager startTo keep your code clean, you could put tests in a dedicated folder, but many argue against it.  Since I work in functional modules, tests of these modules should live in the module itself.Next I created a Protractor configuration file in the root of my project called protractor.config.js:touch protractor.config.jsprotractor.config.js:exports.config = {    capabilities: {        'browserName': 'chrome'    },    specs: [        'www/sign-in/sign-in.spec.js',    ],    jasmineNodeOpts: {        showColors: true,        defaultTimeoutInterval: 30000,        isVerbose: true    },    allScriptsTimeout: 20000,    onPrepare: function(){        browser.driver.get('http://localhost:8100');    }};  Don’t forget to set the correct URL to your running app.If not, you’ll see many errors, except that you might be referring to a wrong URL6. Preparing the testsAs you can see, there is already a spec file defined in the protractor config file, so let’s create it:cd www/sign-in/touch sign-in.spec.jsIn the newly created file, you can start writing your tests.If everything went well, you can simply add another test and it should validate to true.It only tests if the first page you see, is the login page:describe('Signing in', function(){    it('should start on sign-in view', function(){        expect(browser.getTitle()).toEqual('Sign in');    });});Basically, we define a describe function which will describe the whole scope of our specs.Every ‘it’ function is called a spec.We only created one for now.As you can see this is a very readable way of testing.We expect the browsers title to be equal to ‘Sign in’.If the expect statement evaluates to true, the spec has passed without failures, otherwise it will have a failure.  Feel free to change Sign in to something else to fail the test.To run the tests, we can execute the following command in the folder of our protractor.config.js file:protractor protractor.config.jsRunning this command will read the config file and run all the spec-files defined.You will get some output in the command line.At the end you’ll get a summary like 1 specs, 0 failures Finished in x.xxx seconds.This is a simple test but it doesn’t show the full potential of Protractor at all.Lets add a new spec as part of the describe.it('should be unable to click Sign-in button when fields are empty', function(){    var button = element(by.id('btnSignIn'));    expect(button.getAttribute('disabled')).toEqual('true');});So here we test the availability of the sign-in button when the fields are empty.Next is to test if the button becomes available if the fields are filled in with valid data.So lets add another test:it('should be possible to click Sign-in button when fields are filled in', function(){    var button = element(by.id('btnSignIn'));    var txtUsername = element(by.id('username'));    var txtPassword = element(by.id('password'));    txtUsername.sendKeys('yannick@gmail.com');    txtPassword.sendKeys('1234');    expect(button.getAttribute('disabled')).toBe(null);});All these tests should pass correctly in protractor.7. Page Object PatternYou might have noticed that your tests run synchronous after each other.In this scenario this might be useful, but sometimes you need to start with a ‘clean page’ which would mean you need to duplicate a lot of code (for finding the button and text-fields).  When you are working in an agile team, it is quite common that requirements or user stories change.This can implicate you’ll have to change a lot of duplicated code.How can we work around that.The solution is called the page object pattern.The general idea is to put your page in a JavaScript object.Lets dive into sign-in.page.js.This file should also be put into the module folder:var SignInPage = function () {    browser.get('http://localhost:8100/signin');};SignInPage.prototype = Object.create({}, {    txtUsername: { get: function () { return element(by.id('username')); }},    txtPassword: { get: function () { return element(by.id('password')); }},    btnSignIn: { get: function () { return element(by.id('btnSignIn')); }},    typeUsername: {value: function (keys) { return this.txtUsername.sendKeys(keys); }},    typePassword: {value: function (keys) { return this.txtPassword.sendKeys(keys); }},    clickSignIn: {value: function (keys) { return this.btnSignIn.click(); }}});module.exports = SignInPage;In the constructor we make sure our browser opens the signin page by passing the correct URL.Then we use the prototype method to link our HTML elements with the object.Finally, it is wise to create helper methods for basic functionality, such as filling in a username, in case you ever would want to change that behaviour.Then you only need to change that line and all your tests will still pass.  Using logical method names keeps your tests readable which is what you’ll want when you look back in a few months.We can now change our sign-in.spec.js to this:var SignInPage = require('./sign-in.page.js');describe('Signing in', function(){    var page;    beforeEach(function () {        page = new SignInPage();    });    it('should be unable to click Sign-in button when fields are empty', function(){        expect(page.txtUsername.getText()).toEqual('');        expect(page.txtPassword.getText()).toEqual('');        expect(page.btnSignIn.getAttribute('disabled')).toEqual('true');    });    it('should be possible to click Sign-in button when fields are filled in', function(){        page.typeUsername('yannick@gmail.com');        page.typePassword('1234');        expect(page.btnSignIn.getAttribute('disabled')).toBe(null);        page.clickSignIn();        expect(browser.getTitle()).toEqual('Rooms');    });});What changed?We created a page variable and before each it we assigned a new SignInPage object to the page variable.This way, your page gets loaded again before running every spec.This means it always returns in the same state.Now you can create your specs as user stories.8. ConclusionProtractor is an awesome way to test your app’s functionality.Using a descriptive syntax you can emulate almost every user action and run trough the whole app in no time, again and again.Using Protractor, you won’t have to spend a lot of time testing your application manually,and you can focus on feature development without having to worry about accidentally breaking some functionality.Protractor will ensure that your user gets a working app without frustrations!"
      },
    
      "ionic-2016-03-31-adding-typescript-to-ionic-framework-html": {
        "title": "Adding TypeScript to Ionic Framework",
        "url": "/ionic/2016/03/31/Adding-TypeScript-to-Ionic-Framework.html",
        "image": "/img/ionic-and-typescript.jpg",
        "date": "31 Mar 2016",
        "category": "post, blog post, blog",
        "content": "Ionic and TypeScript sitting in a treeSo, TypeScript is the all new thing that allows you to use features from ES6 (or ES2015), ES7 and beyond.Say goodbye to loosely typed variables and say hello to modules, classes, interfaces and so much more.In order to use TypeScript in an Ionic Framework project there are a few small things you need to do to get things running.1. Install and configure the gulp package  Install the gulp-tsc package and save it to the development dependencies in package.jsonnpm install gulp-tsc --save-dev  Next, require the package in your gulpfile.js like sovar typescript = require('gulp-tsc');      Add the following line to the paths object: ts: ['./src/*.ts', './src/**/*.ts'].You may have noticed two things here: All my TypeScript files are in a src folder which means I’m not using the www folder that Ionic provides by default.This way I can keep the TypeScript files and JavaScript files separated.Next to that I’m also targeting subfolders in that folder because I’m bundling my logic based on AngularJS modules.You can read more about structuring an AngularJS project in the John Papa AngularJS Style Guide.        Add our compile task  gulp.task('compile', function(){    gulp.src(paths.ts)        .pipe(typescript({ emitError: false }))        .pipe(gulp.dest('www/'));});  Add our task to watchgulp.task('watch', function () {    gulp.watch(paths.sass, ['sass']);    gulp.watch(paths.ts, ['compile']);});  Now change the ionic.project file and add the compile task to the gulpStartupTasks. If the gulpStartupTasks section is not present at all, just add it anyway.\"gulpStartupTasks\": [    \"sass\",    \"compile\",    \"watch\"]2. Add TSDTSD is a TypeScript Definition manager for DefinitelyTyped.TypeScript used TypeScript Definition files so it knows how to handle the TypeScript you are writing and gives you intellisense.Let’s install TSD so we can continue.$ sudo npm install -g tsd$ tsd install ionic cordova --saveThis will create a typings folder which contains a tsd.d.ts file with references to the typings needed for ionic and cordova.In the root of your project a tsd.json file will be created with all the installed definitions.All you need to do to use the typings in your TypeScript file is include it at the top like so:/// &lt;reference path=\"../typings/tsd.d.ts\" /&gt;Note: TSD has been deprecated in favour of Typings to manage and install TypeScript definitions.More info on how to switch from TSD to Typings can be found here.3. Prevent editor from compiling on saveNow to prevent your editor to auto compile TypeScript we add a tsconfig.json file to the src folder with this in it:{    \"compileOnSave\": false}4. Add TypeScript files in src folderNow that we have everything set up it’s time to start refactoring your application.It’s important to know that every JavaScript file is essentially TypeScript because TypeScript is a superset of the current JavaScript implementation.This basically means that you can take your JavaScript files from your www folder, paste them to the src folder and rename them from file.js to file.ts.Of course don’t forget to add the reference on top of your files to the tsd.d.ts file in the typings folder.If you now run ionic serve, you should see a message that looks like this one. “Compiling TypeScript files using tsc version x.x.x”.TypeScript will process these files and write ES5 files to the www folder.ConclusionAs you can see it is fairly simple - just 4 steps - to add TypeScript support to your Ionic project by changing the default gulp setup used by Ionic.It’s nice to know that Ionic 2 will have support for TypeScript built in so you won’t have to do it yourself.By adding a flag --ts to your Ionic 2 project setup it will be enabled.Personally I love using TypeScript and will use it whenever I can.It makes my life as a developer a lot easier by spotting errors before I even hit the browser.What are your thoughts about TypeScript? Feel free to add them in the comments section."
      },
    
      "angular-2016-03-16-angularts-html": {
        "title": "AngularTS: A new look @ Angular",
        "url": "/angular/2016/03/16/AngularTS.html",
        "image": "/img/angularts.jpg",
        "date": "16 Mar 2016",
        "category": "post, blog post, blog",
        "content": "Combining the best of two worlds.Since my introduction to the heroic AngularJS framework at Devoxx around 4 years ago, I was intrigued and set for an adventure.With the upcoming release of Angular 2 we have to prepare ourselves with the migrating road map coming up.One of the core changes in Angular 2 is the focus on using TypeScript.This post will cover the use of Angular components in TypeScript.But what is it?TypeScript is a superset of JavaScript that focuses on strong typing and new ES6 features: classes, interfaces and modules.Like in common object-oriented languages such as Java and C# these features aren’t new.These features give the developer the opportunity to build an object-oriented architecture in JavaScript.With that in mind, let’s see what the advantages are:TranspilingThe DOM can only recognize JavaScript.With this said they had to come up with a way to compile TS (TypeScript).Because TS is a superset of JS it can transpile to plain JavaScript before including it into HTML.Transpilers are integrated in the latest IDE’s. Any valid JavaScript is valid TypeScript.Strongly typedWhen you’re used to plain JavaScript, you notice that every time you need a variable, it is loosely typed.TypeScript gives you the opportunity for each of your variables to have its own type.This comes with great benefits like better refactoring, less bugs and better type checking at compile time.OO architectureTypeScript offers an object-oriented architecture experience, which means all code is defined in classes, interfaces and most of those classes can be instantiated into objects. It also supports encapsulation, which protects the data from unintended access and modification.Learning path of AngularTSIf you’re no stranger to AngularJS you will notice that the structure remains the same. Two way data binding, controllers, services, … But be aware that it has a different syntax in TypeScript. I will show you the different best practices to implement such components.TypeScript Definition FilesWhen using TS we will refer to TSD files.These files describe the types defined in external libraries such as Angular. To install the Angular TSD files we use typings.To use the typings manager we install it with:$ npm install typings --globalAfterwards install Angular with:$ typings install angular --ambient --save--ambient --save enables the flag and persists the selection in ‘typings.json’All the installed TSD files are gathered in the typings folder.In the main.d.ts file you will see the references the application will use for Angular.Since Angular has multiple libraries, you can use the search command to find the required definition.$ typings search AngularIt is possible that you have to declare the reference on top of your file./// &lt;reference path=\"../../typings/main.d.ts\" /&gt;ModulesAngular ModulesModules are here to help us modularize our code.It is a best practice to use one main module as the root of your application. Multiple modules are being used for third-party libraries or common code.To let the module know the existence of every component, they have to register themselves.Below every component declaration you will see a registration to the module. When registering the module you have to add all the libraries you want to depend upon.In this example we inject the routing service for navigation.module JWorks {    \"use strict\";    angular       .module(\"jworks360\", ['ngRoute'])}Internal TypeScript ModulesThese modules are similar to namespaces.You can define an unique namespace around your code.This will encapsulate variables, interfaces and classes. TypeScript supports sub namespaces for further encapsulation.module JWorks {    \"use strict\";}Transpiled JavaScriptvar JWorks;(function(JWorks){})(JWorks || (JWorks = {}));To encapsulate our code, the module will transpile to an IIFE (Immediately-Invoked Function Expression) around our components.This will avoid global code which helps prevent variables and function declarations from living longer than expected in the global scope, which also helps avoid variable collisions.Entity ClassNow that TypeScript supports object-oriented programming, we can analyse our business problem and define the business objects into entity classes.When you analyse and define these entities you can define which properties and methods each entity needs.If you have a couple entities, you can even establish a relationship.This will provide a clear view on what you want to achieve and have the possibility to create multiple instances of these classes. When building an entity class you can optionally define an interface to show what the intention of the class is.module JWorks {    export interface IEmployee {         username:string;         name:string;         eat(food:string):void    }    export class Employee implements IEmployee {           constructor(public username:string, public name:string){}          eat(food:string):void{             //implementation          }    }}To use your entity class in a controller you have to define the export key.This will expose the class to other classes.When exporting the interface you will use it as a data type.Class as propertyEmployee:IEmployee;Instance of the classemployee = new Employee();Access propertyemployee.nameCall methodsemployee.eat(eggs);ControllersAs you know the controller defines the model to the view of your application, methods for every action you require and the scope where you hold a two way binding.Because TS offers an object-oriented architecture, we can use classes and interfaces instead of functions.Interfaces, like in all object-oriented languages, are a contract that must be implemented by classes that use it.When implemented, all methods and properties have to be used.Classes declare and implement the properties and methods exposed to the view.Every class has his own constructor function, in this function we can declare default property values and other initialisation code.Controller Interfacemodule Jworks {    interface IEmployeeController{        person:IPerson;        save(person:IPerson):void;    }}The interface will show you the intent of our controller and declare the properties and methods that will be used.When you look at the syntax, you see that the properties are strongly typed and the type is declared after the colon.If you aren’t certain what type a property should have, you can fall back to the general type ‘any’.When you declare methods in an interface you have to specify the necessary parameters and return types.The parameters have the same syntax as the properties.Controller Classmodule JWorks {    class EmployeeController implements IEmployeeController{        static $inject = [\"EmployeeService\"];        constructor(private employeeService:IEmployeeService,public employee:IEmployee){             this.employee = new Employee(\"Nivek\",\"Kevin\");        }        save():void{             this.employeeService.addEmployee(this.employee).then(function(result){             console.log(\"added successfully!\");        });   }}angular.module(\"jworks360\").controller(\"EmployeeController\", EmployeeController);Dependency Injection in classesWhen a service is needed in your controller, it must to be injected before it can be used.In the above example it is important that you declare the static $injection on top of your constructor.By doing this the constructor will recognize and initialise the injected services.If you inject a custom service you have to reference to the related service./// &lt;reference path=\"../services/employee.service.ts\"/&gt;ConstructorTypeScript supports initialisation of your properties and injections in a constructor.When declaring properties in your class, you can declare them directly into your constructor.Although these two examples are correct you can have issues in your tests with the second example.So this:class Controller {    name:string;    constructor(public name:string){        this.name=name;    }}Becomes:class Controller {    constructor(public name:string){    }}Be sure to notice that we are using access modifiers to tell the controller which properties we want to expose to the view.The best practice is that you put your injections and Angular services private and all your properties you want to use on your view public.When initialising strings, TypeScript makes no distinction between double or single quotes.ControllerAsController classes use the controllerAs feature by default.So it’s important to declare this into your routes and views.In your HTML you will have to prefix your methods and properties with the ControllerAs syntax.module JWorks {   \"use strict\";   function routes($routeProvider) {       $routeProvider           .when('/', {               redirectTo: '/login'           })           .when('/profile', {               templateUrl: 'app/persons/profile.html',               controller: 'EmployeeController',               controllerAs: '$profile',           })           .otherwise({               redirectTo: '/'           });   }   routes.$inject = [\"$routeProvider\"];   angular.module(\"jworks360\")       .config(routes);}ServicesWhen you make a custom service, the code you implement is reusable and can be called in any other Angular component, including controllers and other services.It is important to know that services are singletons, so there will be only one instance for each service.With this in mind we can use the custom service to share data across all components in Angular.Communicating with an HTTP service to collect and share data with any other component by injecting the service.RestangularFor my project I used an Angular service that simplifies common verb requests with a minimum of client code.In my custom services you’ll see examples of Restangular in TypeScript.If you like to checkout what the difference is with $resource, you can check this listmodule JWorks {   export interface IEmployeeService {       username:string;       employee:IEmployee;       getEmployee(): Employee;       setEmployee(employee:IEmployee);       getEmployeeByUsername(username:string):ng.IPromise&lt;{}&gt;;       getEmployeeByLink(href:string):ng.IPromise&lt;{}&gt;;   }   export class EmployeeService implements IEmployeeService {       static $inject = [\"EmployeeRestangular\", \"$location\"];       constructor(private employeeRestService:restAngular.IService, private $location:ng.ILocationService, private employees:restAngular.IElement,public employee:IEmployee,public username:string) {            username =window.sessionStorage.getItem(\"username\");            this.employees = employeeRestService.all(\"employees\");            employees.one(username).get().then((data:any)=&gt; {                   this.setEmployee(data);               });           }       }       getEmployee():Employee {           return this.employee;       }              setEmployee(employee:IEmployee) {           this.employee = employee;       }                    getEmployeeByUsername(username:string):ng.IPromise&lt;{}&gt;            return this.employees.one(username).get()       }       getEmployeeByLink(href:string):ng.IPromise&lt;{}&gt;{           return this.employees.oneUrl(href).get();       }          }          angular.module(\"jworks360\")              .service(\"EmployeeService\", EmployeeService);       }In the above example, to use the Restangular service you have to install the proper typings.For services it is a best practice to declare an interface for data typing and getting a clear view of the intent.The service class will implement all methods related to the data communication with the backend and returns a promise to the controllers or services that will inject this custom service.Restangular has its own configuration you can modify in the .config component to point to the right api call.After the config you can inject the Restangular service and use its services to build up a request to the backend.DirectivesCustom directives allow you to create highly semantic and reusable components.A directive allows Angular to manipulate the DOM and add its own behaviour. These can either be a set of instructions or a JSON representation.To define a directive in TypeScript we use the directive service that Angular provides.module JWorks {   export interface IAnimate extends ng.IAttributes {       jwAnimate:string;   }   class Animate implements ng.IDirective {       restrict = \"A\";       static instance():ng.IDirective {           return new Animate();       }       link($scope, elm:ng.IRootElementService, attr:IAnimate,ngModel:ng.INgModelController):void {           $scope.right = function(){               $(this).animate({                   left: '+=150'               });                elm.fadeOut(\"slow\");           };           var direction = attr['jwAnimate'];           elm.on('click',$scope[direction]);       }   }   angular.module(\"jworks360\").directive(\"jwAnimate\", Animate.instance);}In the interface above we have to tell Angular what name will be used for our directive.The attribute service will be called to add the name to its attributes.Secondly the class has to implement the directive interface to be recognized by the compiler as a directive.Inside the class you have to declare the prefixed properties and override the methods you will be using.The static instance() method has to be declared to let your module know that there is a new directive.At the end you register the directive to your module with the instance as value.Final noteBest practices can change over time. With webpack for example the registry to the module is contained in one file.TypeScript keeps on growing, and in my opinion will be the default language for many future front-end projects.When it comes to testing our code, TypeScript will provide better support because of encapsulation. Finally, this is a nice learning path to take if you want to migrate to Angular 2."
      },
    
      "security-2016-03-12-digitally-signing-your-json-documents-html": {
        "title": "Digitally signing your JSON documents",
        "url": "/security/2016/03/12/Digitally-signing-your-JSON-documents.html",
        "image": "/img/digitally-signing-your-json-documents.png",
        "date": "12 Mar 2016",
        "category": "post, blog post, blog",
        "content": "What is a digital signature?A digital signature is a mathematical scheme for demonstrating the authenticity of a digital message or documents.A valid digital signature gives a recipient reason to believe that the message was created by a known sender, that the sender cannot deny having sent the message (authentication and non-repudiation), and that the message was not altered in transit (integrity).Digital signatures are a standard element of most cryptographic protocol suites.They are commonly used for software distribution, financial transactions, and in other cases where it is important to detect forgery or tampering.Non-repudiation refers to a state of affairs where the author of a statement will not be able to successfully challenge the authorship of the statement or validity of an associated contract.The term is often seen in a legal setting wherein the authenticity of a signature is being challenged.In such an instance, the authenticity is being “repudiated”.Meet JOSEJOSE is a framework intended to provide a method to securely transfer claims (such as authorisation information) between parties.The JOSE framework consists of several specifications to serve this purpose:  JWK – JSON Web Key, describes format and handling of cryptographic keys in JOSE  JWS – JSON Web Signature, describes producing and handling signed messages  JWE – JSON Web Encryption, describes producing and handling encrypted messages  JWA – JSON Web Algorithms, describes cryptographic algorithms used in JOSE  JWT – JSON Web Token, describes representation of claims encoded in JSON and protected by JWS or JWEJWKA JSON Web Key (RFC7517) is a JavaScript Object Notation (JSON) data structure that represents a cryptographic key.    {         \"kty\": \"EC\",        \"crv\": \"P-256\",        \"x\": \"f83OJ3D2xF1Bg8vub9tLe1gHMzV76e8Tus9uPHvRVEU\",        \"y\": \"x_FEzRu9m36HLN_tue659LNpXW6pCyStikYjKIWI5a0\",        \"use\": \"sig\",        \"kid\": \"Public key used to sign our messages\"    }In this example you can see a couple of parameters.The first of them “kty” defines the key type, which is a mandatory field.Depending on the type you’ve chosen other parameters can be set, like you see above.As our type is EC, or Elliptic Curve, we want to specify the type of curve and our point.Next to these parameters we also have the optional “use” to denote intended usage of the key and “kid” as key ID.At the time of writing there are three supported key types: “EC”, “RSA” and “oct”.While “EC” and “RSA” are used for asymmetric encryption, “oct” is used for symmetric encryptionJWSThe JSON Web Signature (RFC7515) standard describes the process of creation and validation of a data structure representing a signed payload.Assume someone wants to transfer an amount of money to his savings account.This action could be represented like the following JSON:    {         \"from\": {            \"name\": \"Tim Ysewyn\",            \"account\": \"Checking account\"        },        \"to\": {            \"name\": \"Tim Ysewyn\",            \"account\": \"Savings account\"        },        \"amount\": 250,        \"currency\": \"EUR\"    }In this example we are using a JSON document, but this is not relevant for the signing procedure.Before we can sign this we need to convert this to base64url encoding, which will be our payload.So actually we might be using any type of data!The result of the base64url encoding of above transaction is:eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9Additional parameters are associated with each payload.One of those is the required “alg” parameter, which indicates what algorithm needs to be used to generate a signature.Here we can also specify “none” to send unprotected messages.All parameters are included in the final JWS.These can either be sent as a protected or unprotected header.The data in the unprotected header is human readable associated data, whereas data in the protected header is integrity protected and base64url encoded.Assume we want to sign our payload using a key like we generated in the previous section.Our data structure would look like this:    {         \"alg\": \"ES256\"    }and base64url encoded this would be:eyAKICAgICAgICAiYWxnIjogIlJTMjU2IgogICAgfQ==The base64url encoded payload and protected header are concatenated with a ‘.’ to form raw data, which is fed to the signature algorithm to produce the final signature.Finally all of this output will be serialized using one the JSON or Compact serialisations.Compact serialisation is simple concatenation of dot separated base64url encoded protected header, payload and signature.JSON serialisation is a human readable JSON object, which for the example in this section would look like this:    {        \"payload\": \"eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9\",        \"protected\": \"eyAKICAgICAgICAiYWxnIjogIlJTMjU2IgogICAgfQ==\",        \"header\": {            \"signature\": \"DtEhU3ljbEg8L38VWAfUAqOyKAM6-Xx-F4GawxaepmXFCgfTjDxw5djxLa8ISlSApmWQxfKTUJqPP3-Kg6NU01Q\"        }    }Before we conclude this section, there is one more thing I would like to share with you.Because we want to sign and protect our messages, we always want to use asymmetric encryption.But, once our private key has been captured, anyone who has this can forge transactions.One way that COULD counter this is to generate a new key pair every session, or even per transaction.Including the public key in the protected header would not only give the server the ability the validate the signature, we will also be sure that it is the correct one since the protected header is integrity protected!JWEJSON Web Encryption (RFC7516) follows the same logic as JWS with a few differences:  by default, for each message a new content encryption key (CEK) should be generated.This key is used to encrypt the plaintext and is attached to the final message.Public key of recipient or a shared key is used only to encrypt the CEK (unless direct encryption is used).  only AEAD (Authenticated Encryption with Associated Data) algorithms are defined in the standard, so users do not have to think about how to combine JWE with JWS.To keep it short: While JWS can be read by everyone because of the simple base64url encoding, we could use JWE to encrypt some or all of our fields.JWAJSON Web Algorithms (RFC7518) defines algorithms and their identifiers to be used in JWS and JWE.The three parameters that specify algorithms are “alg” for JWS, “alg” and “enc” for JWE.Visit following links to view the list of supported algorithms for JWS and JWEJWTJSON Web Token (RFC7519) is used for passing claims between parties in a web application environment.Because the tokens are designed to be compact and URL-safe they are especially usable in a web browser single sign-on (SSO) context.JWT claims can be typically used to pass the identity of authenticated users between an identity provider and a service provider.JWT relies on all previously mentioned JSON standards.The JWT standard defines claims - key/value pairs asserting information about a subject.The claims include  “iss” identifies the principal that issued the token  “sub” identifies the principal that is the subject of the token  “aud” (audience) identifies the intended recipients  “exp” identifies the expiration time on or after which the token MUST NOT be accepted for processing  “nbf” (not before) identifies the time before which the token MUST NOT be accepted for processing  “iat” (issued at) identifies the time at which the token was issued  “jti” (JWT ID) provides a unique identifier for the tokenThese claims are not mandatory to be used or implement in all cases, but they rather provide a starting point for a set of useful, interoperable claims.So, how do we sign this JSON document in code?Ranging from Java and .NET to Node.js, there are already a lot of libraries available on the internet.And even JavaScript has its own implementation of the standard!Because of its fluent API, we are using the Java JWT implementation in this post.Since not all algorithms are implemented in Java, we are also going to use Bouncy Castle as our JCA provider.In our maven configuration we just add following two dependencies:    &lt;dependency&gt;        &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt;        &lt;artifactId&gt;jjwt&lt;/artifactId&gt;        &lt;version&gt;0.6.0&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.bouncycastle&lt;/groupId&gt;        &lt;artifactId&gt;bcprov-jdk15on&lt;/artifactId&gt;        &lt;version&gt;1.54&lt;/version&gt;    &lt;/dependency&gt;If you are working with a gradle project it would be:    runtime 'io.jsonwebtoken:jjwt:0.6.0',            'org.bouncycastle:bcprov-jdk15on:1.54'If we were to implement the examples from the previous sections, we would start of with generating a new public-private key pair.    KeyPair keyPair = EllipticCurveProvider.generateKeyPair(SignatureAlgorithm.ES256);It’s as easy as that!We want to have a key of type “EC” so we use the EllipticCurveProvider, and by specifying SignatureAlgorithm.ES256 we use the P-256 bit curve with SHA-256 hashing.Next we want to sign our base64url encoded payload    Jwts.builder()                .setPayload(\"eyAKICAgICAgICAiZnJvbSI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiQ2hlY2tpbmcgYWNjb3VudCIKICAgICAgICB9LAogICAgICAgICJ0byI6ewogICAgICAgICAgICAibmFtZSI6ICJUaW0gWXNld3luIiwKICAgICAgICAgICAgImFjY291bnQiOiAiU2F2aW5ncyBhY2NvdW50IgogICAgICAgIH0sCiAgICAgICAgImFtb3VudCI6IDI1MAogICAgICAgICJjdXJyZW5jeSI6ICJFVVIiCiAgICB9\")                .signWith(SignatureAlgorithm.ES256, keyPair.getPrivate())                .compact();Since we already encoded our original message in the JWS section, I’m not getting here into detail again.signWith(SignatureAlgorithm.ES256, keyPair.getPrivate()) does a couple of things.First it is going the create a header if not already present and it will add the “alg” key with the value of “ES256”.After that it will base64url encode that header and will append this with a ‘.’ and the encoded payload.This whole blob of data will then be signed using the private key of the previously generated key pair.Last, but not least, is the compact method.This will just output the base64url encoded header and payload with the generated signature, and all parts are separated with a dot.An outcome would be something like:eyJhbGciOiJFUzI1NiJ9.ZXlBS0lDQWdJQ0FnSUNBaVpuSnZiU0k2ZXdvZ0lDQWdJQ0FnSUNBZ0lDQWlibUZ0WlNJNklDSlVhVzBnV1hObGQzbHVJaXdLSUNBZ0lDQWdJQ0FnSUNBZ0ltRmpZMjkxYm5RaU9pQWlRMmhsWTJ0cGJtY2dZV05qYjNWdWRDSUtJQ0FnSUNBZ0lDQjlMQW9nSUNBZ0lDQWdJQ0owYnlJNmV3b2dJQ0FnSUNBZ0lDQWdJQ0FpYm1GdFpTSTZJQ0pVYVcwZ1dYTmxkM2x1SWl3S0lDQWdJQ0FnSUNBZ0lDQWdJbUZqWTI5MWJuUWlPaUFpVTJGMmFXNW5jeUJoWTJOdmRXNTBJZ29nSUNBZ0lDQWdJSDBzQ2lBZ0lDQWdJQ0FnSW1GdGIzVnVkQ0k2SURJMU1Bb2dJQ0FnSUNBZ0lDSmpkWEp5Wlc1amVTSTZJQ0pGVlZJaUNpQWdJQ0I5.MEYCIQCcwunLBiuHu2z_SlDVJyZuQv0NU8X4VYoOFN1EuIvObQIhAJeZuTeZw9k5uhpBc60iT13s3yb01ItSB2MhEd5pUSqCWe split the three parts for better visualisation, the JWS would be one large StringValidating the signatureFirst we will check if the JWS was actually signed.This can be accomplished by executing following line of code.    Jwts.parser().isSigned(jws);To parse the JWS, we use the parse() method.    Jwts.parser()        .setSigningKey(publicKey)        .parse(jws);Depending wether it is signed or not we might need to set the key for validation.In our case we need to specify the public key of our asymmetric key pair.If we would try to parse the JWS without a key an IllegalArgumentException will be thrown.Should a wrong public key have been provided a SignatureException would be thrown, telling us to not trust this JWS.If we were to pass our public key in the protected header like we said in the JWS section, we should use the setSigningKeyResolver() method.This custom resolver would read out the “jwk” field from the protected header and return a public key based on the data that was provided.Our own SigningKeyResolver implementation could look like this:    public class ECPublicSigningKeyResolver implements SigningKeyResolver {        public Key resolveSigningKey(JwsHeader header, Claims claims) {            return getPublicKey(header);        }        public Key resolveSigningKey(JwsHeader header, String plaintext) {            return getPublicKey(header);        }        private Key getPublicKey(JwsHeader header) {            try {                HashMap&lt;String, String&gt; jwk = new ObjectMapper().readValue(header.get(\"jwk\").toString(), HashMap.class);                            String curve = jwk.get(\"crv\");                BigInteger x = new BigInteger(jwk.get(\"x\"), 16);                BigInteger y = new BigInteger(jwk.get(\"y\"), 16);                String keyType = jwk.get(\"kty\");                ECNamedCurveParameterSpec ecNamedCurveParameterSpec = ECNamedCurveTable.getParameterSpec(crv);                            ECCurve curve = ecNamedCurveParameterSpec.getCurve();                ECPoint g = ecNamedCurveParameterSpec.getG();                BigInteger n = ecNamedCurveParameterSpec.getN();                BigInteger h = ecNamedCurveParameterSpec.getH();                ECParameterSpec ecParameterSpec = new ECParameterSpec(curve, g, n, h);                ECPoint ecPoint = curve.createPoint(x, y);                ECPublicKeySpec ecPublicKeySpec = new ECPublicKeySpec(ecPoint, ecParameterSpec);                KeyFactory keyFactory = KeyFactory.getInstance(kty);                return keyFactory.generatePublic(ecPublicKeySpec);            } catch (IOException e) {                e.printStackTrace();            } catch (NoSuchAlgorithmException e) {                e.printStackTrace();            } catch (InvalidKeySpecException e) {                e.printStackTrace();            }            return null;        }    }First we read all our data from the “jwk” field.Next we retrieve the ECNamedCurveParameterSpec based on the “crv” field and assemble a new ECParameterSpec.After that we create a new ECPublicKeySpec with the ECParameterSpec and an ECPoint out of the x and y coordinates.Finally we get a KeyFactory instance for our key type “kty” and generate the public key with our ECPublicKeySpec.ConclusionJOSE is a simple, compact and lightweight framework to sign and encrypt your payload messages.Because of the combination of base64url encoded messages and JSON data structures it is web friendly.With the wide range of libraries this can be used across platforms with native and hybrid applications, even web applications can use this!One particular disadvantage with the use of the compact dot notation is that you can’t send unprotected header data anymore.Final noteAbove examples should only be used as reference. In a production environment we need to use both JWS and JWE.One could embed a public key of an asymmetric key pair in the application.During login a new symmetric key will be generated, encrypted with that public key and sent to the server.This symmetric key can only be decrypted by the server with the private key, and should then be stored in the session.Every time we need to sign a JSON document, we would use the symmetric key to encrypt the JWS using JWE.It doesn’t matter how you encrypt your messages, and which algorithms you use.Once your application has been hacked, the whole system is vulnerable."
      },
    
      "conference-2016-03-10-javaland2016-html": {
        "title": "The 5 key trends of JavaLand 2016",
        "url": "/conference/2016/03/10/JavaLand2016.html",
        "image": "/img/javaland.png",
        "date": "10 Mar 2016",
        "category": "post, blog post, blog",
        "content": "  JavaLand is a software conference, held annually, in Phantasialand, Brühl (Germany). JavaLand focuses on Java enthusiasts, developers, architects, strategists, administrators and project managers. With more than 100 lectures, JavaLand caters to the interest of both beginners and experts. These are, what I believe, the 5 key trends of the conference.1. Microservices stay hot and are maturingJavaLand dedicated an entire track to containers and microservices. This resulted in a large variety of talks on the subject. I attended a couple, but the talk by Ordina’s very own Andreas Evers hit the sweet spot between introducing the microservice concepts and making them tangible. Microservices transfer a lot of the application’s complexity to the interactions between the services. Applying patterns such as circuit breakers and bulkheads are quintessential to building successful distributed systems. Andreas presented all of this in a clear and concise manner to the delight of the audience.2. TypeScript / Angular 2Frontend developers are in for a treat. Currently, nobody exactly knows when Angular 2 is going to be released, but everybody is eagerly looking forward to it. Rumor has it .. release will be very soon. Angular 2 promises to be a faster, more powerful, cleaner, and easier to use tool. The Angular team provides an upgrade path to migrate your old Angular 1 applications. What’s also really interesting is the Angular 2 Style Guide, that contains best practices on how to organize your project, name your components, etc.Angular 2 was migrated to TypeScript, because of the great tooling support. TypeScript isn’t the first language to compile to JavaScript, but supported by Angular 2, it just might be a game changer.3. Cloud-Native JavaJosh Long did a stunning job, giving a whirlwind talk on a large number of Spring tools to support the building of Cloud Native Java applications. He started with Spring Data REST to build a hypermedia-driven REST web service with a Spring Data Repository. Then he introduced Spring Cloud Config to externalize configuration files of the different microservices he was building. When building distributed applications, a config server is essential, in my opinion. Next up was Spring Cloud Netflix, which is a “Springified” collection of tools open sourced by Netflix. Josh demoed:  Eureka for service registry and discovery  Zuul as an API gateway  Hystrix for circuit breakersFinally he used Spring Cloud Sleuth with Zipkin to show us a nice dashboard of the different requests going through his freshly deployed microservices.A talk by Josh Long is always an event and we’re very proud to announce that he will be doing a presentation at Ordina Belgium later this month!4. Reactive“Reactive” is used broadly to define event-driven systems. Reactive Systems are responsive, resilient, elastic and message driven. Details on these key characteristics can be found in the Reactive Manifesto. The most popular Java library to compose asynchronous and event-based programs is RxJava. To build reactive applications, RxJava uses Observable sequences that make it easy to wrap synchronous methods in asynchronous calls.An interesting presentation to learn more about this topic is available on Youtube.While Reactive programming isn’t new, it has been gaining a lot of traction recently. For example thanks to the recently released Lagom framework from Lightbend. Last year the Spring team announced that Spring 5 will also focus on Reactive.5. KubernetesKubernetes is an orchestration system for Docker and Rocket containers, initiated by Google in 2014. In Kubernetes, containers run in Pods. These pods are managed by Replication Controllers (create, destroy, start / stop, failure, scaling, …). Since Replication Controllers can create and destroy Pods dynamically, we can’t rely on their IP addresses to communicate with each other. This can be solved by using Kubernetes Services.Kubernetes can schedule and run containers on clusters of both physical and virtual machines.An interesting discussion, after one of the Kubernetes talks, was about the differences between Kubernetes and a regular PaaS. This post on Stackoverflow provides a lot of input for that discussion, stating that Kubernetes is PaaS-like: Cloud Foundry can be considered an “Application PaaS” and Kubernetes a “Container PaaS”, but the distinction is fairly subtle and fluid, given that both projects change over time to compete in the same markets. The subtlety of the difference is demonstrated by the Kubernetes documentation, explicitly stating Kubernetes is not a PaaS.The Community ActivitiesIt’s impossible to talk about JavaLand, without mentioning The Community Activities. These focus on  Innovation discovery: Humanoid Robots, Virtual Reality (VR), Neural Networks, …  Gamification: Hacking sessions and contests, Dojos, …  Networking: User Groups, Jogging, Tours, …  Phantasialand: The theme park opened its door exclusively for JavaLand on Tuesday evening. What better way to bond with colleagues and leaders in the Java community than in a roller coaster :)"
      },
    
      "spring-2016-03-05-http-public-key-pinning-with-spring-security-html": {
        "title": "HTTP Public Key Pinning with Spring Security",
        "url": "/spring/2016/03/05/HTTP-Public-Key-Pinning-with-Spring-Security.html",
        "image": "/img/spring-security-logo.png",
        "date": "05 Mar 2016",
        "category": "post, blog post, blog",
        "content": "What kind of sorcery is this?HTTP Public Key Pinning, or short HPKP, is a security mechanism which allows HTTPS websites to resist impersonation by attackers using mis-issued or otherwise fraudulent certificates.This was standardized in RFC 7469 and creates a new opportunity for server validation. Instead of using static certificate pinning, where public key hashes are hardcoded within an application, we can now use a more dynamic way of providing this public key hashes.One caveat to remember is that HPKP uses a Trust On First Use (TOFU) technique.How does this work?A list of public key hashes will be served to the client via a special HTTP header by the web server, so clients can store this information for a given period of time.On subsequent connections within previous given period of time, the client expects a certificate containing a public key whose fingerprint is already known via HPKP.I strongly encourage you to read this article by Tim Taubert, where he explains what keys you should pin and what the different tradeoffs are.Imagine you want to terminate the connection between the client and a malicious server for your main domain and all of your subdomains, but also want to be notified when such events happen.In the next paragraph you can find the implementation details.The web server needs to send following header to the connecting client with the first response    Public-Key-Pins:        max-age=5184000;        pin-sha256=\"d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=\";        pin-sha256=\"E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=\";        report-uri=\"https://example.net/hpkp-report\";        includeSubdomainsBy specifying the Public-Key-Pins header the client MUST terminate the connection without allowing the user to proceed anyway. In this example, pin-sha256=”d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=” pins the server’s public key used in production. The second pin declaration pin-sha256=”E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=” also pins the backup key. max-age=5184000 tells the client to store this information for two month, which is a reasonable time limit according to the IETF RFC. This key pinning is also valid for all subdomains, which is told by the includeSubdomains declaration. Finally, report-uri=”https://www.example.net/hpkp-report” explains where to report pin validation failures.So how can we implement this with Spring Security?Retrieving  the list of public key hashesWe first need to get a list of public key hashes. Currently the standard only supports the SHA256 hashing algorithm. The following commands will help you extract the Base64 encoded information:From a key file\topenssl rsa -in my-key-file.key -outform der -pubout | openssl dgst -sha256 -binary | openssl enc -base64From a Certificate Signing Request (CSR)\topenssl req -in my-signing-request.csr -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64From a certificate\topenssl x509 -in my-certificate.crt -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64From a running web server\topenssl s_client -servername www.example.com -connect www.example.com:443 | openssl x509 -pubkey -noout | openssl rsa -pubin -outform der | openssl dgst -sha256 -binary | openssl enc -base64For now we will assume we got 2 public keys:  Our active production certificate: d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=  Our backup production certificate: E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=Configuring Spring SecurityAs of version 4.1.0.RC1, which will be released March 24th 2016, the HpkpHeaderWriter has been added to the security module. The 2 easiest ways to implement this feature is either by Java configuration or by using the older, but still supported, XML configuration. Below you can find both solutions:Java config\t@EnableWebSecurity\tpublic class HpkpConfig extends WebSecurityConfigurerAdapter {\t\t@Override\t\tprotected void configure(HttpSecurity http) throws Exception {\t\t\thttp.httpPublicKeyPinning()\t\t\t\t.addSha256Pins(\"d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=\", \"E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=\")\t\t\t\t.reportOnly(false)\t\t\t\t.reportUri(\"http://example.net/hpkp-report\")\t\t\t\t.includeSubDomains(true);\t\t}\t}XML config\t&lt;http&gt;\t\t&lt;!-- ... --&gt;\t\t&lt;headers&gt;\t\t\t&lt;hpkp\t\t\t\treport-only=\"false\"\t\t\t\treport-uri=\"http://example.net/hpkp-report\"\t\t\t\tinclude-subdomains=\"true\"&gt;\t\t\t\t&lt;pins&gt;\t\t\t\t\t&lt;pin&gt;d6qzRu9zOECb90Uez27xWltNsj0e1Md7GkYYkVoZWmM=&lt;/pin&gt;\t\t\t\t\t&lt;pin&gt;E9CZ9INDbd+2eRQozYqqbQ2yXLVKB9+xcprMF+44U1g=&lt;/pin&gt;\t\t\t\t&lt;/pins&gt;\t\t\t&lt;/hpkp&gt;\t\t&lt;/headers&gt;\t&lt;/http&gt;"
      },
    
      "spring-2016-02-06-generating-spring-rest-docs-without-using-integration-tests-html": {
        "title": "Generating Spring REST Docs without using integration tests",
        "url": "/spring/2016/02/06/Generating-Spring-REST-Docs-without-using-integration-tests.html",
        "image": "/img/spring.png",
        "date": "06 Feb 2016",
        "category": "post, blog post, blog",
        "content": "The problemA couple of days ago I was working on a project of one of our customers.One of their new applications needed to expose a public API, and of course we needed to hand over a set of documentation about those REST endpoints.Some people were already starting to do this manually in Confluence, but after a while (and we’re talking about a timespan just under 2 hours) this became a tedious job. We had to continuously adjust the input &amp; output contracts, the different endpoints,…Using Spring REST Docs I wanted to automatically document all of the public API endpoints, while we were also testing all of the components in the whole application.For some undisclosed reasons we simply couldn’t write integration tests, so we were stuck with our unit tests and mocked objects.The solutionImagine you have following service and controller in a simple Spring Boot application:    @Service    public class DeviceService {        public List&lt;Device&gt; getDevices() {        List&lt;Device&gt; devices = new ArrayList&lt;&gt;();            /*                Some business logic here...            */            return devices;        }    }    @RestController    @RequestMapping(\"devices\")    public class DeviceController {        private DeviceService deviceService;        @Autowired        public DeviceController(DeviceService deviceService) {            this.deviceService = deviceService;        }        @RequestMapping(method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE)        public List&lt;Device&gt; getDevices() {            return this.deviceService.getDevices();        }    }    Since this is a Spring Boot application both classes will automagically be instantiated.Because you need to annotate your unit tests at class level with @WebAppConfiguration and @SpringApplicationConfiguration, we can easily create a new Spring Boot application and use this for our documentation.In this new application we set the base package that needs to be scanned to our controller sub package, and create a mock implementation of our DeviceService.    @SpringBootApplication(scanBasePackages = { \"be.ordina.blog.controller\" } )    public class Application {        @Bean        public DeviceService getDeviceService() {            return EasyMock.createStrictMock(DeviceService.class);        }    }    Our DeviceControllerTests class will then look something like this:    @RunWith(SpringJUnit4ClassRunner.class)    @SpringApplicationConfiguration(classes = { Application.class })    @WebAppConfiguration    public class DeviceControllerTests {        @Rule        public RestDocumentation restDocumentation = new RestDocumentation(\"target/generated-snippets\");        @Autowired        private WebApplicationContext context;        @Autowired        public DeviceService deviceService;        private MockMvc mockMvc;        @Before        public void setUp() {            this.mockMvc = MockMvcBuilders.webAppContextSetup(this.context)                                            .apply(documentationConfiguration(this.restDocumentation))                                            .build();        }        @After        public void cleanup() {            EasyMock.verify(deviceService);            EasyMock.reset(deviceService);        }        @Test        public void getDevices() throws Exception {            Device firstDevice = new Device(\"iPhone 6\");            Device secondDevice = new Device(\"Nexus 5\");                        List&lt;Device&gt; devices = new ArrayList&lt;&gt;();            devices.add(firstDevice);            devices.add(secondDevice);            EasyMock.expect(deviceService.getDevices()).andReturn(devices);            EasyMock.replay(deviceService);            this.mockMvc.perform(get(\"/devices\").accept(MediaType.APPLICATION_JSON))                        .andExpect(status().isOk())                        .andExpect(jsonPath(\"$\", hasSize(2)))                        .andExpect(jsonPath(\"$[0].name\", is(firstDevice.getName())))                        .andExpect(jsonPath(\"$[1].name\", is(secondDevice.getName())))                        .andDo(document(\"device\"));        }    }    So this is how I managed to get rid of the manual, tedious work and keep my unit tests - and got back to the more serious part of my life: coding like a monkey. =)  PS: All of the code above is checked in at our public github repo, so you are free to clone the working application! You can find it here!"
      },
    
      "domain-driven-20design-2016-02-02-a-decade-of-ddd-cqrs-and-event-sourcing-html": {
        "title": "A decade of DDD, CQRS and Event Sourcing",
        "url": "/domain-driven%20design/2016/02/02/A-Decade-Of-DDD-CQRS-And-Event-Sourcing.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "02 Feb 2016",
        "category": "post, blog post, blog",
        "content": "  Command and Query Responsibility Segregation is the most misinterpreted pattern in software design. CQRS doesn’t mean eventual consistency, it’s not about eventing and messaging. It’s not even what most people believe about having separate models for reading and writing.  In his talk A decade of DDD, CQRS and Event Sourcing on DDDEurope 2016, Greg Young gives us a retrospective over the last ten years practicing CQRS and event sourcing.CQRS?Before I go any further, let’s start explaining what CQRS really is. It’s all about applying a design pattern when you notice that your class contains both query- and command methods. It’s not a new principle. Bertrand Meyer described Command-Query Separation in his book Object-oriented Software Construction as follows:  “Every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, Asking a question should not change the answer.”We can apply CQRS principles in many levels of our application, but when people talk about CQRS they are really speaking about applying the CQRS pattern to the object that represents the service boundary of the application. The following example illustrates this.Although this doesn’t seem very interesting to do at first, architecturally we can do many interesting things by applying this pattern:  One example is that the separation is more explicit and programmers will not find it odd to use different data models which use the same data. Reading records from the database must be fast and there’s no problem at all if you can achieve this by using multiple representations of the same data.  CQRS is also an enabler for event-based programming models. It’s common to see CQRS system split into separate services communicating with Event Collaboration. This allows these services to easily take advantage of Event Sourcing.You should be cautious however not to use it everywhere and only in some Bounded Contexts that need it, as everyone agrees that applying the CQRS principle adds complexity.History  “When you searched CQRS on Google a decade ago, it thought it was just Cars misspelled.”CQRS is not a new concept. You might even say that event sourcing has been around for thousands of years. The ancient Sumerians wrote accounting info on clay tablets and baked them. That document stored events in time. Immutable events. And documents are built up of this event information.As I said earlier, Meyer talked about the principle in his book which was released in 1988.It’s QCon San Francisco in 2006 which really gave a boost to the popularity of CQRS and event sourcing. Martin Fowler picked up CommandQuerySeparation in his Bliki and after that, things began to grow.CQRS is more of a stepping stone and you have to put it in its historical context. It was a natural consequence of what was happening with Domain-Driven Design at that time. CQRS is not an end goal, it should be seen as a stepping stone for going to an event sourced system.Good thingsThe community around CQRS and event sourcing is growing to about 3000 people. More and more domains are involved with event sourcing. In other domains, other added values were discovered. These people had breakthroughs by practicing CQRS, eg. in a warehouse system, instead of denying a user’s request because the system couldn’t handle the requests anymore, it accepts an event and processes it a later time.Another good thing about event sourcing, once you model events, you are forced to think about the temporality of systems: what happens at a specific time? how will this object behave in this situation?Event Storming exercises help you to figure out which domains you have in your system and give you a clear view on the different events. You can then formalize events and commands.Ideas about Event Sourcing have been spreading. Functional programming gained popularity in parallel with event sourcing. Event sourcing is a natural functional model. Every state is a left fold of your previous history.A lot of other things also pushed Event Sourcing forward:  Cloud computing  Popularity of Actor Models  MicroservicesBad thingsSome people see CQRS as a full-blown architecture, but it’s not. This is wrong. CQRS and event sourcing is not a top level architecture. You cannot build an Event Sourced system. Instead, you end up into building a monolith which is event sourced internally. Event sourcing is simply not a good solution to every problem. For example, once you deal with immutable events, you need to think about corrections to data. Whenever a user corrects a value and hits the save button again, you would need to have an event for that and it would be too complex to handle.A lot of little things are misinterpreted by the community and this caused dogmas to pop up:  “Value objects can be mutable in some use cases” - It’s not because Eric Evans once said “Value objects are normally immutable” that you have to think that in some situations, you can justify mutable objects. There is never an excuse to create mutable objects and they should be avoided at all times.  “The Write side cannot query the Read side” - There are times that you have to. When you have an invariant that crosses thousands of aggregates, you cannot avoid it.  “Inputs should always equal Outputs” eg. if i have an order command, an order event should be the result - This is not always the case and there are situations where input and output aren’t one on one.  “Must use one-way commands” There’s no such way as fire your command, put it on a queue and forget. One way commands don’t exist! They happened in the real world. They cannot be rolled back. Using commands gives you the opportunity to respond to it. One-way commands can however be changed in events in an event sourced system.Over the years some CQRS frameworks have been created. Greg’s advice is… Don’t write a CQRS framework! It will guaranteed be abandoned after a year. It’s not a framework, it’s more like a reference implementation. We also need to pull back away from Process Manager frameworks. You can probably solve your problem with an Actor Model.A queue of messages doesn’t work for all kinds of systems. You can probably linearize in 90% of the use cases, it will also probably be cheaper. For the other 10%, interesting things are happening. We’re gonna see a push to occasionally connected systems. When you choose availability and high throughput, you’ll have to move to message-driven architectures and linearization is not an option.Future thingsA lot of interesting things are happening in the software world. We’re growing to N-Temporal systems, where multiple things happen at multiple timeframes.Greg concluded with a quote of Ernest Hemingway.  “We are all apprentices in a craft where no one ever becomes a master.”Recommended readingGreg wrote a book about this matter, called Event Centric - Finding Simplicity in Complex Systems. In this book, he explains how to use DDD with Command-Query Responsibility Separation to select the right design solutions and make them work in the real world.Other sources  Martin Fowler on CQRS: http://martinfowler.com/bliki/CQRS.html  Greg Young on CQRS: http://codebetter.com/gregyoung/2010/02/16/cqrs-task-based-uis-event-sourcing-agh/ and http://www.squarewidget.com/greg-young-on-cqrs"
      },
    
      "domain-driven-20design-2016-01-29-dddeu16-odds-and-ends-html": {
        "title": "oDDs and enDs: Vaughn Vernon  on software projects in peril",
        "url": "/domain-driven%20design/2016/01/29/DDDEU16-Odds-and-Ends.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "29 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  There’s an interesting situation you will find in many software development projects.Often there is a team dedicated to keep the software alive.The team patches the system and deals with emergencies day after day.Almost every organization is dealing with this kind of situation.Obviously this is not the situation we want to be stuck in, but how do we alleviate ourselves from this?Vaughn Vernon gave a presentation today at Domain-Driven Design Europe.He started his presentation about the odd things that happen in software projects.Then he shed some light on the (future) solutions using Domain-Driven Design,the so called ends of the problems caused by the odds.oDDsWhat are the odd things that happen in software development projects?Cost centersAn insidious problem is that an IT organization within a company is considered a cost center.The business views software as something that costs a lot of money.You can almost say that the business almost wished they did not have to use computers,or employ software developers.What about the company and you?How does the business view you?Do they view you as a hacker?Someone who is thrilled about technology?If that’s the case, then you may be viewed as a cost center instead of a profit center.Budgets for software development projects are minimalOften a team only contains one senior developer and a lot of juniors developers.The senior developer must keep everything running and moving.Database-DrivenLooking at the business like a database might be a problem.How often does a developer think like this?Data needs to go from a view into a database and out of a database to a view like this.That is how a lot of developers think.We must be careful to be focused so hard on technology and not on business value.Shiny ObjectsSoftware developers are always looking for shiny objects.They want to learn about the latest technologies and work with them.DDD, BDD, TDD, Big Data, Machine Learning, Deep Learning, AI, Reactive, …How do we justify those things to the business?Big Data was a big buzzword 5 or 6 years ago.It is still a buzzword, but if you’re not saying Machine Learning as well, you’re not cool anymore.Are we using technology when it is appropiate?Sometimes the latest technology isn’t always the correct solution for the problem.A Not-So-Ubiquitous Language  It doesn’t matter what you name it. It’s just code.It is very true for the common developer that they think it doesn’t matter.But it does matter.The business wants to talk about something this way,but the developer calls it differently anyway. “It’s just code”.Poor CollaborationHow many organizations use JIRA as a collaboration tool and fail at it?Often someone spends days writing specifications and creating JIRA issues,yet developers don’t use them.Estimates are a big deal.Sometimes it takes longer to estimate than to fix the problem.Task Board ShuffleThis is where software design is entirely comprised of moving sticky notes.You move a sticky note from the Todo column to the In Progress column.After we have done this, we run back to our machine and start coding.Without thoughtful design, the code comes out of our fingertips.If you have a team compromised of a few developers working on the same problem,there will be multiple translations in one day of the same thing.Using the same terms is very important, but often neglected.Big Ball of MudMany organizations are deducing a Big Ball of Mud as software architecture.Everything is part of the same namespace and there are no bounded contexts.The software consists of entangled models that should be separate,but they are all in one place.This is the cause of many problems in the industry.You have to recognize a situation when a Big Ball of Mud is being developed and stay out of it.    Business logic is escaping to everywhereBusiness logic can be found in places outside of the core domain.Business logic in persistence logicYou often see business logic inside of persistence logic.Someone is ready to save an object to the backend storage and there is business logic in the persistence logic.The persistence logic is hiding the important business logic.You lose trace of your business logic because of this.Queries in business logicYou see business logic creating decisions by querying the database.Some part of the decision that is being made is hidden to the business logic, because it is inside that query.These queries can also be broken.Sometimes queries are so expensive, they shut down other operations because the tables are locked.Business logic in the UIThe biggest crime against business logic is putting it inside the UI.The business logic is put inside the view template or model instead of the domain model.CRUDCRUD does not work with complexity.It’s also an insidious problem where software developments teams think they can solve any problem with a database.Anemic Domain ModelThe Anemic Domain Model is one of the most widespread and adapted architectures.Often there is a domain model with objects which are connected with relationships.This all looks nice on paper, but there is no domain logic or any behavior inside these objects!Services live on top of the domain model. This is often called the Service Layer or the Application Layer.They contain most of the domain logic and use the domain objects for data.This is very contrary to object-oriented design.The data and the processes are combined together and it looks very much like procedural programming.This anti-pattern is so common, because most people have not worked with a real domain model.Wrong abstractionsA lot of times developers are thinking too much about abstractionsinstead of getting down to the business.They form a lot of “cool” abstractions that will make it better in the future.  “What if we have this sort of situation in the future?If we come up with this kind of abstraction, then this abstraction will take careof the situation in the future.”We cannot predict the future.The future of software is unknown.Coupled ServicesCoupled services are horrible. What if, for example, a REST controller calls a service, which calls another system.If the other system does not respond, you have a gap in your business logic, even if you use global XA transactions.What to do?It really could be that everybody else is normal.What if writing systems with the odd things actually is the norm?If that is normal, then wouldn’t you like to be the oddball in the crowd?enDsYou want to be the furthest point away from these problems.You want to come up with solutions that work.The business must not view you as a technologist,but as someone who is interested in  the business.You can’t just keep throwing technologies at the problem.You must come up with beneficial business solutions.Developer maturityIf you are a cost center, then you must come up with a way to make your advancesmore economical. You have to develop your maturity.You have to seek other ways to get the rest of your team to maturity.Urge them to go to DDD and software meetups.Do whatever you can, because you can only benefit if those around you are more mature than you.Passion is something we can’t always teach.But you must try to work with people who are passionate about their job.Profit centerYou must try to become a profit center.An entire unit of the business is a profit center.You can only become a profit center if you keep adding business value in a timely manner.Collaborate with the businessDon’t use JIRA to collaborate with the business.You will be amazed what you can learn if you get away from the desk and into a room.The business will tell you what they problems they have hated for years.Use an ubiquitous languageSome things cannot be explained by anybody. “Why do we call it this? Can we call it this instead?”You can learn those interesting and beneficial details by forming an ubiquitous language.Make it your goal to find that ubiquitous language with a bounded context.Concrete ScenarioShow concrete users in a concrete scenario and what goal that has to achieve.As developers we have to chase after deep models as shiny objects.It’s not just technologies.Technology matters.Try to experience with deep modeling through an ubiquitous language.You can use the Gherkin language to achieve this.With these concrete scenarios you can model your domain model and test it.Feature: Coffee Machine    Scenario: Buy Coffee        Given there is coffee left in the machine        When I deposit 1 euro        And I press the coffee button        Then I should be served a coffeeUse bounded contextsTo avoid the Big Ball of Mud, you must introduce bounded contexts and separate models.It is equally important to separate the models as it is to introduce core concepts in the core domain.You have to learn about event storming.You can understand what your bounded contexts are from an event storming event.    Metrics-based EstimatesThe artifacts that come out of an Event Storming event, you can use those as estimation units.If you can’t finish an iteration according what you’ve planned,move these estimation units in a retrospective and encur modeling debt.This modeling debt must be fixed as soon as possible.Know architectureYou must know good architectures, like the hexagonal architecture or CQRS. These architecture solve many of the above problems.They enforce bounded contexts and give the ability to do context mapping.    Decoupled ServicesServices have to be decoupled.A service which calls a peer service directly, is tightly coupled.It cannot work without the other service.What if the other service times out?You can use domain events and messaging systems to fix this problem.MicroservicesThe microservices architecture is another shiny object that a lot of people are chasing.The thing is, they are extremely similar to bounded contexts.Every microservice is master of their own model and usually has one point of access, like an aggregate root in Domain-Driven Design.Actor ModelThe Actor Model is an extremely powerful tool that we need to use in the very near futureby the majority of software development teams.CPU processing power is not increasing, but the amount of cores keeps increasing.The Actor model is a new way to leverage this power because it fully utilizes these cores with threads.    Summary  Many teams are in peril over poorly designed systems  Software development culture is broken  Developers must gain maturity and passion  DDD can be used to make a difference  Use the Actor model to design DDD based microservicesOne more thingVaughn Vernon announced an additional new book called Domain-Driven Design Distilled.It is a 200 page thick book that explains all of the core concepts of DDD.This is very light weight book, intended to rapidly not only teach your team members,but also the business about DDD. This book will be available within a month."
      },
    
      "domain-driven-20design-2016-01-29-ddd-europe-heuristics-from-the-trenches-html": {
        "title": "Heuristics from the Trenches by Cyrille Martraire",
        "url": "/domain-driven%20design/2016/01/29/DDD-Europe-Heuristics-from-the-Trenches.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "29 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  “Communication usually fails, except by accident” - Osmo WiioWith this quote of the Finnish researcher Osmo Wiio, Cyrille opened the second day of the DDD Europe conference. Osmo’s laws of communication are the human communications equivalent of Murphy’s law. Basically if communication can fail, it will and if a message can be understood in several different ways it is quite possible that it will be understood in a harmful way. With the quote Cyrille immediately wants to stress that with Domain-Driven Design, deep conversations with domain experts and careful attention to the language are key.Business domains are often very complex and hard to get into for individuals not familiar with the domain. The conversations’ game with domain experts is a game that takes many years and many failures in order to get better at. Cyrille explained that, even though it’s hard, it’s perfectly possible over time to extract a growing set of techniques, heuristics and best practices to boost the effectiveness of the interviews with domain experts, to learn faster and to converge more quickly to better models.Practices and tricks for talking to the domain experts“Why is it so hard?” you might ask yourself or “We don’t talk the same language as them and they don’t have time for us!”. While it is true that the people with the highest expertise within a certain domain often don’t have a lot of time for interviews or meetings, it is up to us, the developers, to make the necessary preparations prior to seeing them. You should first take some time in order to teach yourself some basic domain knowledge. It all starts with genuine curiosity, successful people are curious about their business domains. You may not believe this but here this will help you too! Without this you will have a bad time! Do your homework: perform the necessary research about the domain on the internet, Google around, read books, check Oasis, … Usage of ubiquitous language is very important.Note taking is also a very important aspect in the whole story. You need to be able to take notes like a pro! Learn to take notes effectively. Listen actively and don’t distort the stories the domain experts tells you. Keep the words as they are. It really is harder than you would think, so you should turn it into a game:  Write down the stories the domain experts tell you  Underline new words and add a definition for yourself, get familiar with all the domain terms  Take note of side remarks  If you think that you’ve encountered a synonym for an existing new word dig into it and ask for more detail  Show your knowledge to the domain user to establish credibility and to challenge them  But… Challenge them respectfully!All this is Domain-Driven Design!Talking to people is hardIt is not easy talking to people and it will often be hard to have productive conversations. However this is also something you can grow into and for which you can develop the right toolset. Have interactive conversations, that way you have control over the conversation and the way you can steer it. Start with “what exactly is the goal?”. Be precise when asking questions, we want to avoid having to reverse engineer the true need from an expressed solution. Be sure to scan the notes you took during a previous conversation and decide where you want the conversation to go next.  “I keep developers out of conversations about the domain because they always want to know ‘why this’ and ‘why that’. Just write your code.” - The Expert BeginnerYou should question everything: ask why but don’t go too far!Combine Domain-Driven Development with Behaviour-Driven Development. Both go hand in hand to interview domain experts. You want to discover the “unknown unknowns” as early as possible and to avoid misunderstandings as often as possible. Be sure to ask for concrete examples and genuine sample documents and data and although this doesn’t always come easy, ask and insist but as mentioned before know your limits and be sure not to push it.People always think that talking abstract is faster and will save time but think about it and ask yourself: “Is it truly faster if we were wrong or missed stuff that matters?”. If the domain expert seems hesitant about something also take note of this to take into account that the feature in question might be eligible for change later on, this way we can model our software design correctly.The domain expert delusionYou might assume that the person, you’re having a conversation with, is an actual system expert within the domain but chances are that he/she is not. The worst expert is the one whose expertise was built from the intricacies of the existing systems. It is therefor also our duty to help out the domain expert where possible:  Have empathy, try to put yourself in their shoes  Build a partnership, it should be a two-way conversation you’re having with the expert  Make it clear that the domain expert is always safe with you, that you have no plan to steal their job  Propose things, it doesn’t always have to be right but this way you will get somewhere  Look for invariants, for example: “Is there any other outcome that is also important?”  Always ask for validation of everything, even if you’re sure about itIt is common in companies that businesses often don’t want all power concentrated into the same hands. This also accounts for domains, you should therefor assume and probe. Get to know the business you are dealing with and their mentality. Not that if you happen to discover that there are multiple domain experts the situation gets a bit more tricky. Having multiple domain experts may lead to more confusion and makes it even more important asking for validation and challenging the experts. Something we want to achieve is that we want to suggest features from our code which could be very useful for the business. Instead of having software to support the domain we want to have software augmenting the domain.Keep improving yourselfGrow into it and build your own toolbox for implementing Domain-Driven Design. You might ask yourself why you should bother so much with all of this, after all we just want to get to it and write code. This however is a wrong approach because the complexity of the domain is there, it is just hiding behind a wall, you just don’t see it yet. You will discover the complexity sooner or later so you may as well want to get into it as early as possible to save precious time. After all, you want to become a domain expert too!Other useful resources mentioned by Cyrille:  Conversation Patterns for Software Professionals by Michael Bartyzel  Analysis Patterns by Martin Fowler  Living Documentation by Cyrille Martraire  Slides: http://www.slideshare.net/cyriux  Blog: http://cyrille.martraire.com  Twitter: @cyriux"
      },
    
      "domain-driven-20design-2016-01-28-evolution-of-ddd-html": {
        "title": "Eric Evans about the evolution of Domain-Driven Design",
        "url": "/domain-driven%20design/2016/01/28/Evolution-of-DDD.html",
        "image": "/img/DDDEurope2015.jpg",
        "date": "28 Jan 2016",
        "category": "post, blog post, blog",
        "content": "  Why is it that DDD only has its own conference after 13 years? Why is this becoming a sudden hype? Why does it gain popularity and is it mentioned so many times in microservices presentations? Eric Evans talks on DDDEurope about the core idea behind Domain-Driven Design and its evolution over the last few years.But what is DDD?The subtitle of Evans’ book, Tackling Complexity in the Heart of Software, bundles two core principles of Domain-Driven Design:  It describes the process of translating complex real-life problems into software  The heart of software entails the domain that we’re working onKey in this activity is finding the core complexity in the critical part of the domain and focus on this and only this piece of complexity. Software developers and domain experts collaborate to develop models, simplified representations of the real-life problem. The written software should eventually explicitly reflect the model. Whenever a brainstorm session occurs, it almost always results in an adaptation of the models within the software.When we encounter multiple complex problems, we must think about them separately. Each problem requires its own model representation.When discussing with others about the domain, we must speak a ubiquitous language. You should use the same vocabulary for describing the problem you’re solving. However, when somebody asks you the meaning of a word, in many cases you have to ask the person: “In what context is it used?”. That’s why the language only means something within a well-defined bounded context.Domain-Driven Design is more like an attitude. Although it gives us principles and terminology to enable talking about it and have discussions, different people will do things differently. Each approach will be slightly different.Bounded contexts?A bounded context is an important principle when applying Domain-Driven Design. As i said earlier, language in itself doesn’t mean anything. It only means something when it’s used within a certain context. eg. Item can be a Stock Item, Sale Item, …Bounded contexts have the following characteristics:  Within a bounded context certain rules apply, eg. validation rules  It needs to be tangible in the software, eg. use packages for each contextAnother benefit of working with separate bounded contexts is that separate teams could take responsibility on separate bounded contexts.ConclusionWhen Erics book was released in 2003, only Java 5 and J2EE were used as a programming language for implementing projects. We only had EJB’s to solve problems and there was no other way of storing data but with SQL. If the technology is so complex and limited to implement something, you can’t focus on the real problem of software design.Nowadays, we have a lot of new tools available to implement a project: We can store data with a NoSQL database or not store it at all and keep it in-memory. We can explore other ways of approaching data with eg. event sourcing. On certain levels, Spring makes the technical aspect of writing software components a breeze. With the upcoming of microservices and each microservice having its own database, bounded contexts are much clearer to the teams working on the software. And there are probably tons of other examples on how today’s tools can help us achieving our goal: write good software.Better tools and a vivid community which masters these tools cause Domain-Driven Design to become more and more popular in ways of designing software. So maybe we can do better now than back in 2003. Maybe… Or maybe we’re not there yet. Fact is that everyday we are learning from mistakes in the past to do better in the future."
      },
    
      "conference-2015-11-09-devoxx15-docker-kubernetes-html": {
        "title": "#Devoxx Arun Gupta talks Docker",
        "url": "/conference/2015/11/09/Devoxx15-Docker-Kubernetes.html",
        "image": "/img/docker.png",
        "date": "09 Nov 2015",
        "category": "post, blog post, blog",
        "content": "DockerDocker is a tool used for container creation for software applications. We have all been aware of the existence of containers for some time, but Docker creates a standard for describing these packages.Docker is used for three things: Build, Ship &amp; Run your software.BuildCreation of a predefined container in a standardized way using the Docker CLI.Use a dockerfile containing a list of commands. The FROM command specifies an OS and additional software packages, eg. FROM jboss/wildfly. All  commands are compressed into one, customized image using the Docker CLI.ShipShare the container via DockerHub or your private repository.  Sharing = caring!RunDocker runs on a minimal operating system and uses the Union File System. On the bottom level, there’s the Bootfs kernel, on which the chosen base image or OS runs and finally, the user images ontop of that.Hosts running Docker are very environment variables oriented, so by using variables in the commands or on the machine itself you configure your application. Any other communication is usually done over HTTP/REST. The Docker images are stored on the Docker host so the actual client is very thin. A Docker app runs on the Docker engine; this is in contrast with regular VMs, running on full-blown operating systems.Docker MachineDocker Machine allows you to get started with Docker on Mac or Windows. It features the docker-machine CLI and uses the boot2docker image (32Mb small) under the hood.  Docker Machine is preferred over boot2docker for development purposes, but it is not production-ready (yet!)Easy way to set up a Docker host with docker-machine:docker-machine create --driver=virtualbox myhostListing all the installed Docker images:docker-machine lsListing all the environment variables of a newly created Docker container:docker-machine env myhostDocker Machine is also used to start, stop or restart containers. It even allows to update Docker itself.Many existing plugins provide support for various cloud platforms.Boot2DockerAn earlier version of docker-machine. As said above, it is being used by docker-machine under the hood.  My advice: migrate to docker-machine, at least for development purposes.Docker ToolboxEasily the best tool to get started with Docker.  Windows  MacOSHands-on Docker  docker helpfor all your docker needs!  docker psto check the running containers  docker imagesto check your images  docker buildfor quick build  docker run -it ubuntu sh for quick running an image in a shell  Docker images are like diapers: if they get shitty, throw’em away and take a fresh one.Docker ComposeAllows you to define and run multi-container applications. It has all the commands the regular Docker has and more.It provides a new way to link containers.Configuration is defined in one or more docker-compose.yml (default) or docker-compose.override.yml (default) files.It is a great tool for setting up Development, Staging and Continuous Integration (CI) environments.  Docker container linking is so passéA problem with container linking was that there was no possible way to work with different hosts. Docker Compose solves this by using volume mapping.It can help with running multi-host environments:  Bridge network span single host  Overlay network spans multiple hosts  Software defined networking is possible and preferred! Docker Compose solves this problem but it should still be used cautiously in production!Starting a set of Docker images using Docker Compose is easy:docker-compose up -f docker-compose.yml -f production.yml -dDocker SwarmDocker Swarm provides native clustering for Docker, fully integrated with Machine &amp; Compose. It either uses Etcd, Consul, Zookeeper or other solutions to store the cluster ID.Whenever you create a Docker Machine, you can add it to the cluster. It also serves the standard Docker API so anything that works on Docker, will work on multi-host environments.  They say the new Docker Swarm v1.0.0 release is production ready: I still have my doubts!References  Docker Docs are the de facto standard reference and are very well documented. They contain information on Docker, Machine, Compose &amp; Swarm: https://docs.docker.com/  Samples: https://github.com/javaee-samples/docker-javaQuestions or Remarks  Contact @arungupta  or @Turbots on Twitter  Create an issue or start a discussion on the Github repository (or on Gitter)"
      },
    
      "microservices-2015-10-13-microservicespatterns-html": {
        "title": "Applying software patterns to microservices",
        "url": "/microservices/2015/10/13/MicroservicesPatterns.html",
        "image": "/img/jax2015_logo.jpg",
        "date": "13 Oct 2015",
        "category": "post, blog post, blog",
        "content": "  During the week of October 12th, my colleagues Andreas Evers and Tim De Bruyn and me attended JAX London 2015. After attending over a dozen talks in three days, we went home with tons of insights about DevOps, microservices, Continuous Delivery, Spring Boot and much more. This is a write-up of Chris Richardson’s talk A pattern language for microservices.Chris Richardson is the author of POJOs in Action and is founder of the original CloudFoundry, which was later acquired by VMWare and then SpringSource. Nowadays he is constantly thinking about microservices and founded a startup that is creating a platform for aiding the development of event-driven microservices (http://eventuate.io/). He also launched http://microservices.io/, describing microservice responsibilities and commonly accepted solutions as patterns.Problems in software engineeringChris started his presentation by pointing out a few problems in software engineering. First, we have a lot of sucks/rocks discussions between developers. JavaScript vs. Java, Spring vs. JEE, Java vs. .NET, functional programming vs. object-oriënted programming, containers vs. virtual machines, … Sounds familiar? But these discussions are usually very subjective and shallow. Back in 1986, Fred Brooks already mentioned in a paper on software engineering that there is no such thing as a silver bullet. So there is no right or wrong answer to which language or framework is better, it depends on the situation.A second problem is that we have a lot of new technologies these days. Typically, these technologies go through the Gartner Hype Cycle.At first, people discover an innovative technology and everybody wants to use it, which drives the technology into the Peak of Inflated Expectations. Docker is a good example of a technology in this phase. Then a huge drop follows, because people didn’t really understand the technology and misused it. When we start to understand the subject, that’s when productivity on the market increases.It’s clear that we need a better way to discuss and think about technology. That’s where software patterns come in.Pattern languagesPatterns help us to describe a reusable solution to a problem occurring in a particular context. The use of pattern languages is a a great way of talking about technology in general. You can see it as an advice around a topic. Describing what you want to solve and its context is much more important than the framework or tool you choose.A pattern description typically contains:  Pattern name  Context  Problem - The issue which we try to solve  Forces, which are a set of indicators why we want to use the pattern, eg. we need to do CD, run multiple instances, …  Solution. What would a pattern be without a solution?  Resulting context. Set of both benefits and downsides which we achieved, but also problems which we then have to solve next.Patterns can be related, they can be alternate solutions, solutions to problems that were caused by another pattern or more specific solutions to a certain problem. When you want to read more about writing and understanding patterns, you can read Martin Fowler’s blog post Writing Software Patterns.Patterns for the microservices worldMicroservices is another one of those new technologies which are in Peak of Inflated Expectations phase. To help us understand the complexity of implementing these kind of systems, Chris founded http://microservices.io/ . Here you can find a collection of microservices patterns.We can group the microservices patterns into several categories:  Core patterns  Deployment patterns  Discovery patternsCore patternsMonolithical applications tend to be simple to develop, test, deploy and scale. You can just run multiple copies of your monolithical application. When the application is large however, you end up in monolithic hell:  Millions of lines of code undermine developer productivity and knowledge of the system.  As one change might affect other parts of the application, there’s a fear of changing and refactoring code  Developer productivity decreases, as your IDE gets slower, startup times of the server take very long, …  Long-term commitment to a technology stackWith X and Z axis scaling, you increase the number of application instancess or you increase server resources. Y axis scaling on the other hand means you break up your application into separate microservices which group business functionality. Some benefits to this are:  Smaller, simpler apps, which are easier to understand  Less classpath hell  Faster to build and deploy  Improve fault isolation  Eliminates the long-term commitment to a single technology stackOf course, each solution always has some drawbacks, to which fortunately solutions exist.  Added complexity of developing a distributed system  We have to handle partial failures  Implement business transactions that span multiple databases  More complex testing: what do we do with transitive dependencies? Do we mock them?  What about managing communication for cross-service development and deployment?Deployment patternsForces to consider when deploying microservices are:  Variety of languages  Building and deploying must be fast  Isolate service instances  Deploying must be cost-effectiveFor these, you can look at the deployment patterns.  Service per VM = Packer.io is a great tool for running each servicein its own VM. Downside is you got the overhead of a whole VM permicroservice. It is very expensive and the deployment itself isrelatively slow. A positive thing though is that the AWSinfrastructure is very mature and reliable.  Service per container = Each service is in its own Docker container, which is started very quickly. A drawback is that these technologies are still very immature.Discovery patternsOne problem we need to address around the area of service discovery is that we need to know the IP address of the server. Simply having configuration files with the IP’s wont work anymore. On top of that, the set of API interfaces can change. And this is just a tip of the iceberg.There are several patterns related to service discovery:  Client-side discovery = The client will query the service registry,pick one from the load balancing configuration, and then use it.Netflix’ Eureka and Ribbon provide this functionality. MultipleEureka’s can be clustered.  Server-side discovery = At some level it’sthe same like client-side discovery. The difference is that the    client makes a request to the router, which then queries t he service registry. You can achieve this with eg. Nginx as the router and an Elastic Load Balancer from AWS. The advantage is that the client code is much simpler. Advantage is that it’s built in in some cloud/container environments, such as AWS ELB, Google’s Kubernetes, Marathon, …ConclusionOver the years, companies like Netflix, LinkedIn, Soundcloud and many others have applied the microservices architecture in their software landscape, with several tools and open-source libraries as a result. But deep down these tools have to tackle the same problems. Chris’ effort to describe these common problems and solutions in software patterns allows us to see the wood for the trees again. Because as I said earlier, knowing what you want to achieve is much more important than the framework or tool you choose."
      },
    
      "conference-2015-09-15-join2015-html": {
        "title": "JOIN 2015 from a developer's perspective",
        "url": "/conference/2015/09/15/JOIN2015.html",
        "image": "/img/2015-09-15-JOIN2015/join2015.jpg",
        "date": "15 Sep 2015",
        "category": "post, blog post, blog",
        "content": "  Last week the Oracle/Java unit held its yearly JOIN Event. The purpose of this event is to share knowledge between colleagues and fellow Java- and JavaScript enthousiasts. A total of 83 attendees visited Ordina Belgium’s headquarters in Mechelen to pack their heads with interesting technology facts.We started off with a couple of Ordina consultants, each giving a lightning talk (a 20 mins. talk) about a hot topic. Amongst others, there were talks about Docker, Polymer, IoT, microservices and Meteor. Afterwards everyone had the chance to attend three keynote talks from international professionals in the Java- and JavaScript world.The past, present and future of ECMAScriptTom Van Cutsem told us about the past and the future of JavaScript and how the ECMAScript standard was born. Tom is a member of the TC39 board, a group of technical people who decide which feature is in or out. There’s no political game going on in this group, which is a good thing. JavaScript was invented in 1995 by Brendan Eich, who worked at Netscape at that time, and quickly standardized as ECMAScript 1st edition in 1997. ECMAScript 5 was released in 2009 and is well supported by all major browsers, as the following compatibility table illustrates: http://kangax.github.io/compat-table/es5/. By introducing strict mode, which makes you as a developer use a more safer and sturdier JavaScript version, the guys at TC39 guaranteed the expansion of ECMAScript 6.This summer a brand new version was released: they did not call it ECMAScript 6 this time, but rather ECMAScript 2015 because they changed their release model to yearly releases. Features that aren’t ready for the release, will be skipped and released a year later. Tom then shortly introduced a subset of the new features available in ECMAScript 2015: Arrow functions are a shorter way to declare anonymous functions, a bit comparable to Java 8’s Lambdas. Second he explained probably the most important feature… classes! Actually it’s just about syntax, because they will be treated like regular functions by the engine. Most notable control flow features were promises and iterators. On the website mentioned before to check ECMAScript 5 compatibility, you can also switch to ECMAScript 2015 and see that browser compatibility is still poor. However, Tom also recommended that you can already use ECMAScript 2015 in your application by using compiling tools like Babel or TypeScript. These tools compile your ECMAScript 2015 code into ECMAScript 5. He also briefly mentioned Nashorn, the new Javacript engine developed by Oracle, released with Java 8.You can have a look at Tom’s presentation on http://t.co/Phwpx3Ig13.Boo(s)t your app developmentStéphane Nicoll enlightened us with a Spring Boot talk. He ex​actly had one slide, but blew us away with a full-packed demo of what Spring Boot’s auto-initialization features can mean for your project. Starting point is https://start.spring.io/, were you can kickstart a Spring project by checking and unchecking technologies. It’s as simple as that. When you’re working with IntelliJ, you can even do it inside your IDE with a wizard. Both result in the same project however.After creating a simple @RestController with a @RequestMapping and a Hello world, ​he added the JPA dependency, created an @Entity and a Spring Data repository. Now we only have to add a database to the project. Just by adding the H2 dependency in the pom.xml file, Spring Boot creates a database for you and attaches the created Repository classes to that database instance. It is able to do this by scanning for DataSource classes on the classpath.Actuator endpoints allow you to monitor and interact with your application. Spring Boot includes a number of built-in endpoints. For example, Spring Boot can automatically create a health status endpoint where you can check the health of your database, JMS queue or any other component that is registered with the Spring Boot system. You can even write your own.He also deployed the application to Cloud Foundry, Pivotal’s cloud platform. He demonstrated the possibility to remotely install another database on that server and bind this database instance to the application. Then he even demonstrated hot code replacement in the cloud… That’s really amazing!  We can conclude that Spring Boot makes Java development as it should be. By following the convention-over-configuration approach, we can achieve very much in very little time.A sneak peek at the new Angular 2.0Finally, Pascal Precht from Thoughtram gave some insights on Angular 2. Attention! You shouldn’t say AngularJS 2, but simply Angular 2. Pascal gave us a quick tour of Angular 2’s new syntax for property- and event binding. You will need to use square brackets around HTML attribute names for property binding and parenthesis for event binding, which looks a bit weird at first. You can read more about Angular 2’s syntax in his blog post Angular 2 Template Syntax Demystified.Angular 2 will also support Web Components, a new standard in developing custom components for web applications.On the question whether Polymer and Angular 2 weren’t tackling the same problems, Pascal replied that Polymer focuses more on Web Components whilst Angular 2 claims to be an end-to-end framework to build applications.Actually, at first the syntax seems frightening, but after hearing the reasoning behind it, it seems to me that the only difference is that Angular will embrace the standard DOM element properties instead of keeping their own in sync like in the previous version… And that’s why you could say that we don’t have two-way databinding anymore in Angular 2. Interesting things, but we’ll have to wait until 2016 to see the final syntax, because everything we saw… can already be different as we speak.​On the question when Angular 2 would be production ready, he opened up his browser and opened Is Angular 2.0 ready?. That tells enough. Pascal’s feeling is that a beta version will be released Q1 2016, but this was a non-official statement."
      },
    
      "spring-2015-05-09-springio15-general-html": {
        "title": "Spring I/O 2015 Barcelona",
        "url": "/spring/2015/05/09/SpringIO15-General.html",
        "image": "/img/springio.jpg",
        "date": "09 May 2015",
        "category": "post, blog post, blog",
        "content": "  Last week Barcelona was the place to be for Spring enthusiasts. With tons of Pivotal speakers and many more community leaders it was a two-day goldmine for anyone looking to update their Spring knowledge. This is my report ranging many different topics, including quite some one on one discussions and hacking sessions with the people behind the Spring ecosystem.Sergi Almar (event organizer), me and Josh Long (Spring developer advocate)One on one talk with Juergen Hoeller:Groovy has now become an Apache Incubator project, as Pivotal decided Groovy isn’t a core project for them. Most of the people behind Groovy are not working for Pivotal anymore but for other companies which means they can only work on it part-time. Juergen doesn’t like this but it wasn’t his decision. Luckily Groovy has a huge userbase and they truly love the language. That’s why Juergen is not too concerned about Groovy dying, but of course the speed of new developments will be a lot slower than the past two or three years.Gradle has strong dependencies on Groovy, as it’s using a Groovy-based DSL language. The story is different here however, as Gradle is backed by Gradleware, a Silicon Valley company with its own vision. Spring is using Gradle for most of its projects, because for one the devs like to use Groovy, but foremost because it offers more flexibility compared to maven which is necessary when dealing with an open-source framework.The Netflix stack is Amazon based as it’s the most important player in the market at the moment. Of course there is support for Cloud Foundry but there are no guarantees it will work on all clouds. Especially Google App Engine is kind of a mess as the team behind it doesn’t really cooperate with Pivotal or anyone else as far as Juergen knows.Currently it’s hard to find decent books about microservices but they should be coming up soon. For cloud there are some books out there but they could feel outdated already. What’s written in 2013 isn’t always completely valid anymore in 2015, especially in a field like Cloud computing.One on one hacking session with Oliver Gierke:Lots of stuff can still be moved from Spring Data REST to Spring Hateoas. This afternoon I’m sitting down with Oliver Gierke to do some hacking on the subject.It’s possible to have a resource with many different embedded resources inside by nesting domain POJOs in eachother. The thing Spring Data REST is missing is the possibility to distinguish between domain model nested classes and resource model nested classes (aka embedded resources). There is no way to embed a Car resource as an embedded resource into a Person resource without actually having Car as a property of Person in the domain model. Having the possibility to manually add embedded resources to a resource would solve this.To achieve this we should have an extension of ResourceSupport with a Set of EmbeddedWrappers inside. Using the EmbeddedWrappers class, we can add embedded resources to our resource in its Assembler. This Wrapper will take care of relation resolving, especially handy when dealing with collections which require plural-forms. The relation value can be annotated in the model or passed along as a second parameter.I will go into more detail with examples as a comment on the GitHub ticket about embedded resources which Oliver will try to follow up. An existing stackoverflow issue about the subject can be found here: http://stackoverflow.com/questions/25858698/spring-hateoas-embedded-resource-supportHaving different representations of the same resource with different fields (e.g. summary and full view) can be achieved using jsonViews. However, those jsonViews can be used for versioning as well, and having both at the same time could interfere. The Projection abstraction is a very nice feature of Spring Data REST that was moved to the Spring Data Commons package. This enables us to use this abstraction without the need of a persistence. We can use it in conjunction with Spring Hateoas without Spring Data REST.The main class to use is the SpelAwareProxyProjectionFactory. This factory can be used to create projections. It’s also possible to use the Page functionality and especially its page.map() function, which can link a projection interface with a domain class. This approach allows us to define an interface with the selection of fields from the domain class as getter methods. This defines the selection of fields which that projection should expose.This pattern is applied as well in the latest Spring MVC where a UserForm is used as a parameter of a POST endpoint method. Defining the UserForm as an interface works exactly by the same principle as the Projections of Spring Data. You can even have default methods in the interface for validation of that form.At my client I am integrating most of the stuff you can find in foxycart’s HAL browser but in a dynamic way. Exposing a graph of resources can be done using Spring Restdocs’ link documents generated in asciidoc. By parsing the results of these asciidocs and merging the results, a resulting json is aggregated and used to generate a graph of resources and their links using d3js. This graph is integrated in the HAL browser, and each resource links to its documentation. That documentation per resource can of course also be reached from the regular HAL browser documentation links using curies. To generate that documentation we are in turn using Spring Restdocs to show examples with their links, request and response fields and error scenarios. All of this is generated and guaranteed up-to-date (if it weren’t, it would have failed the build). The improvement which is still possible to do here, is to have our own hook into Spring Restdocs so we wouldn’t need to go through asciidoc links to generate the full relationship graph. Oliver asked me to open a ticket for Andy Wilkinson to allow hooks in the generation model.Oliver doesn’t fully agree with versioning on resource level. He prefers versioning APIs or not versioning at all, to avoid having a higher cost later due to legacy and lots of old versions we’ll need to support. The initial win of versioning would be insignificant compared to the technical debt it creates.Implementation of the hypertext cache pattern of HAL is quite straight forward according to Oliver. The client should be smart enough to search for the field it needs in the embedded resource, and if that field isn’t there, he should follow the link to the full resource. Keeping track of which representation is shown in which place (embedded vs linked) becomes unnecessary using that approach.More on Spring I/O 2015 Barcelona ….Boot your Search with Spring talk:Speaker: Christoph Strobl - Talk &amp; speaker descriptionSolr feels like an old kitchen sink for anything you want to do. Not exactly a fancy 2015 tool. They are catching up though and documentation is getting better. It’s scheme based. MongoDB does much more out-of-the-box which you have to do manually with xml configuration. Solr schemaless support exists but as long as it’s lucene-based, there’s no such thing as a schema-less index. Their type-guessing only goes so far until you try to add a record with a different type.Spring Data Solr does just what you expect: clean to-the-point interfaces with annotations that do the DAO magic for you. Spring Data Elasticsearch does that as well for the complexity of Elasticsearch. I never really liked the query system that Elasticsearch has so having this abstraction layer could prove really useful.Inside http://spring.io - a production spring reference application &amp; one on one talk with Brian Clozelon this blogReal-time with Spring: SSE and WebSockets talkSpeaker: Sergi Almar - Talk &amp; speaker descriptionSpring WebSockets is better than JSR356 because: there is a fallback with SockJS, there is support for STOMP subprotocol, Spring Security can jump in, and of course flawless integration with messaging components and the Spring messaging style. Security is important because there are no URLs anymore. We have to secure at message level.Spring Data REST - Repositories meet hypermedia talkSpeaker: Oliver Gierke - Talk &amp; speaker descriptionRecommended reading: Domain Driven Design. Although very boring, it introduces vital concepts in the repository world. When combining ALPS and JSON Schema, it should be possible to create a client that is smart enough to discover verbs and even fields of the payload.Building Microservices with Spring Cloud and Netflix OSS talkSpeaker: Dr. Dave Syer  - Talk &amp; speaker descriptionAnother great book is Release It!. It describes a lot of the patterns microservices use such as circuit breaker. It’s definitely a great book for devops.Master Spring Boot autoconfiguration talkon this blogCan Your Cloud Do This? Getting started with Cloud Foundry talk &amp; Building “Bootiful” Microservices with Spring Cloud workshop &amp; One one one talk with Josh Longon this blogEnjoy reading!"
      },
    
      "spring-2015-05-08-springio15-sagan-html": {
        "title": "A production Spring reference application &amp; One on One talk with Brian Clozel",
        "url": "/spring/2015/05/08/SpringIO15-Sagan.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Sagan is the name of the Spring.io website. It’s built by Pivotal Labs and maintained and extended by Brian Clozel. The project uses a best-of-breed set of tools. In Brian Clozel’s talk he sheds a light on which tools are used for which reasons. After the talk I sit down with Brian to discuss some more details.Sagan: A production Spring reference applicationSpeaker: Brian Clozel - Talk &amp; speaker descriptionSagan is the name of the Spring.io website. It’s built by Pivotal Labs and maintained and extended by Brian Clozel. The project uses a best-of-breed set of tools. Of course it uses GitHub as code repository and issue tracking. But GitHub can become a bit confusing and unclear when a lot of issues need to be tackled and tracked. Sagan uses Waffle to link GitHub issues and commits to scrum &amp; kanban practices. Formal communication goes through GitHub issues, and informal conversations are held through HipChat. Travis is used for continuous deployments. Asciidoctor is used for the guides on the website. They are stored in a GitHub repository, fetched by the website and rendered appropriately.Sagan is using a Gradle plugin which is triggering Green-Blue deployment. It calls Cloud Foundry to see which clone is active (green or blue) and deploys on the non-active one. Once deployment is done, the routing is switched automatically to the newly deployed one. In the short moment where the switch occurs, both clones are being routed to avoid a brief moment of downtime.Cloud Foundry takes the console log output of each application and aggregates everything. Either the result is exposed using webockets or it’s bound using a service that can show the logs as well as persist them. Redis is used in conjunction with Spring Session. This creates distributed session management for the cloud. Whenever the database needs to be updated, versioned FlyWayDB scripts are used. The upgrades could be small changes where local tests are sufficient, or a staging environment is used for testing.The slidedeck of this talk can be found here: https://speakerdeck.com/bclozel/spring-dot-io-1One on one talk with Brian ClozelThe Netflix guys have a strong implementation of Conway’s law. The organisation mimics the architecture and vice versa. This closely relates to the microservices pattern, where each microservice can have the best tools for the job, and those can definitely be different for each microservice. In Netflix this applies to teams as well, resulting in different approaches suggested by different people.Regarding database evolution Brian suggests the Netflix guys could have some good ideas about handling backwards-compatibility breaking evolution in a green-blue deployment with zero downtime. In any case there are many cases where teams use feature-switches to make certain user interactions read-only for a short time during migration. This avoids having transactions that have to be forcibly destroyed. After successful deployment the feature is turned on again ensuring the user has a seamless experience.Brian will check with the Netflix guys and let me know what they do for database migration"
      },
    
      "spring-2015-05-08-springio15-microservices-html": {
        "title": "&quot;Bootiful&quot; Microservices in CloudFoundry &amp; One on One with Josh Long",
        "url": "/spring/2015/05/08/SpringIO15-Microservices.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Spring Boot, Spring Cloud and CloudFoundry: a perfect match. Josh Long explains how to build Spring Boot microservices, deploy them in CloudFoundry and manage them using the Netflix OSS stack through Spring Cloud. Including a One on One talk.Can Your Cloud Do This - CloudFoundry talkSpeaker: Josh Long - Talk &amp; speaker descriptionIn Amazon there’s a good chance you’ll encounter “AMIs”. These are basically virtualizations with an operating system, and is perceived as a container. These containers need to be disposable. The moment you remove one, another should be ready to jump in. The idea is to treat your servers as cattle, not as pets. The moment you know the name of a specific server, it’s as if it’s your pet. And you don’t want pets because they’re not disposable.When choosing a cloud platform, it’s important to avoid vendor lock-in. At Google AppEngine, there was a huge community developing applications for the platform before it was even in GA. Once they got there, Google raised the prices significantly. The problem was however that all those applications were using Google-specific APIs and were really tightly coupled to the Google infrastructure. For most companies it was no longer viable to use Google’s platform without incurring debts and were unable to quickly move to another platform. Josh compares it nicely with Hotel California: “You can always check-in, but you can never leave”.It’s possible to deploy a jar to Cloud Foundry, but also a war. The war will be automatically wrapped in a container (using Warden, the linux container, which makes it similar to a docker image), but you can also push your own docker image with your war inside.Once you pushed your application into Cloud Foundry, you have to link it to backend services, such as a database. Doing this is child’s play. One cool backend service in particular is the logging service. You can use Splunk or Papertrail, which you can bind to e.g. your own account on the Papertrail site.An older version of the slidedeck of this talk can be found here: https://speakerdeck.com/joshlong/scaling-with-spring-and-cloud-foundryBuilding “Bootiful” Microservices with Spring Cloud (Workshop)The configprops actuator endpoint is especially useful to figure out what properties are available for certain functionalities. It beats debugging and is a pretty useful form of documentation. For more information check this out: http://docs.spring.io/spring-boot/docs/current/reference/html/howto-properties-and-configuration.html#howto-discover-build-in-options-for-external-propertiesThe configuration server supports configuration which is common to all microservices. If the configuration.yml is inside the resources folder of the configuration server itself, it’s used only as configuration for the configuration server. But if the server finds an application.yml inside the distributed configuration location (e.g. Git), then those configurations are shared for all other yml files, albeit with lowest priority.The Cloud Configuration Bus is interesting if you would like to have a distributed refresh of the configuration of your microservices. Basically instead of POSTing to http://yourmicroservice/refresh, you’d call http://yourmicroservice/bus/refresh on any clone and Spring Cloud Configuration Bus will forward the refresh using AMQP to all the other clones. This way you don’t need to call a refresh on each node separately.It’s also possible to poll for changes instead of pushing them. Simply inject the refresh endpoint as a bean and annotate it with the @Scheduled annotation.Since the 1.0.1 release of Spring Cloud, it’s now even possible to have the client microservice wait and retry until a configuration server is registered. (https://GitHub.com/spring-cloud/spring-cloud-config/issues/129)A cool new annotation is @SpringCloudApplication. It groups the following annotations: @SpringBootApplication, @EnableDiscoveryClient and @EnableCircuitBreaker.The high availability principle in Eureka is considered fulfilled when at least two Eureka instances are registered and peer aware in each zone.Ribbon, the client-side load balancer of Netflix, is integrated nicely into Spring’s RestTemplate. When EurekaClient is enabled on the microservice, Ribbon will be able to resolve the following command: restTemplate.exchange(“http://myservice/books”, …). It will look into Eureka for an application called myservice, and route to the appropriate server.The slidedeck of this talk can be found here: https://speakerdeck.com/joshlong/the-bootiful-microserviceOne on One talk with Josh LongIn many cases there are different access rules for different environments. This applies in a great deal on configuration as well. Most guides showcase one configuration server with all properties for all environments into a single repository. When facing more strict policies, there is no reason why you can’t have a configuration server per environment, or at least for production a dedicated one. That server could have different access rules. It’s an easy solution worth trying out.When you have both public and private microservices, Josh prefers to do the extra security for the private ones on the microservice level instead of in the gateway. It’s important to trust the developers, especially in a devops culture which is a requirement in a microservices architecture.At the company I work, management fears that the developers will introduce security issues, so the enterprise architects are looking into more governance-minded solutions. One of those solutions is having a full blown ESB as the gateway. Such an ESB would require you to register new endpoints - in case of REST these would be resources with their allowed verbs - and different types of handlers and interceptors. This is a very process-heavy solution where the microservice would have to request access through the ESB for every new resource, or verb on a resource, probably to another team in charge of the ESB.This goes completely against the microservices principles. The ESB becomes a bottleneck and increases the time-to-market. It also becomes a heavy single point of failure with lots of logic inside. Josh makes a fitting comparison with conductor versus choreographer. If your gateway is an ESB it acts as an conductor. When the conductor goes offline, the entire orchestra fails. When a choreographer drops out, the dancers can still independently continue. The power lies in the individual units, instead of one governing entity. Microservices need to be in charge of their own decision-making instead of an ESB and its separate team.We’re also facing strong opposition from the infrastructure team against the embedded-container approach. It’s probably not surprising they’re trying to push JEE standards as they’re heavily invested in a certain application server. Josh argues the battle between Spring and JEE was won a long time ago, and by a big margin. Case in point is the new exciting feature of JEE 8: to marshal and unmarshal data between POJOs and JSON. Spring had support for this via Jackson integration already three years ago. Currently there are almost no application servers that implemented JEE 7. It will probably take another five years or so until JEE 8 will be adopted in application servers. That means basic JSON binding on POJO support will only be available eight years after Spring started supporting it. Do you really want to wait that long for such a vital feature? Or should we still use XML-only?Cloud Foundry can be pretty expensive since it’s primarily aimed towards big enterprises. For other purposes, definitely check out the free OSS stack.Pivotal has a division called Pivotal Labs. It’s a super agile company that only takes clients that align with their agile views. They have a proven trackrecord that is pretty much unmatched and allows them to stick to their values. Although super expensive, they’re fast and deliver quality. Aside from taking projects on, the also help companies become more agile.Aside from Eureka, Spring Cloud also supports other registries such as Zookeeper and Consul. The difference between for example Eureka and Zookeeper is that Zookeeper is a shared hierarchical name space of data registers (also called registers znodes). It can be used very well as a service registry, but it offers a lot more features on top of that. One easy example is leader election. Due to the Spring Cloud annotation abstractions it’s possible to switch out Eureka with any other supported service registry."
      },
    
      "spring-2015-05-08-springio15-autoconfig-html": {
        "title": "Master Spring Boot auto-configuration",
        "url": "/spring/2015/05/08/SpringIO15-Autoconfig.html",
        "image": "/img/springio.jpg",
        "date": "08 May 2015",
        "category": "post, blog post, blog",
        "content": "  Spring boot allows you to extend its convention-over-configuration approach by creating your own autoconfigurations. There are some important details you shouldn’t forget.Master Spring Boot auto-configurationSpeaker: Stéphane NicollIn order to create your own autoconfiguration, it’s important to remember the spring.factories file in the META-INF folder of the autoconfiguration project.The autoconfiguration class itself should have @Configuration (of course) and utilise conditional annotations as much as possible. Especially on the bean initializations the rule of thumb is the more conditionals the better. This enables users of your autoconfiguration to override specific elements of the autoconfiguration class. Aside from fully overriding beans, you can also expose properties under your own namespace. Together both these concepts allow small configuration-based modifications and bigger bean-overriding modifications by the user.Spring offers many types of Conditional annotations. The regular ConditionalOnClass or ConditionalOnMissingClass and ConditionalOnBean or ConditionalOnMissingBean are the most common, but also ConditionalOnProperty, OnResource, OnExpression and others are possible. You can even have nested conditions and for the most specific needs you can always write your own conditional annotation with whatever logic you require.By looking how Spring’s autoconfiguration classes are built, it’s easy to figure out how to do it yourself. Especially the ConfigurationProperties are pretty straight forward once you see an example. Don’t forget to put the @EnableConfigurationProperties annotation on your autoconfiguration class however.To expose your configurationproperties to IntelliJ, the trick is in the maven dependencies. You should add the “spring-boot-configuration-processor” dependency. This will generate metadata regarding the properties of your autoconfiguration which IntelliJ uses. Once that’s done, IntelliJ will also autocomplete your properties in the property files.Support for yml configuration autocompletion in IntelliJ is coming by the way.Once your autoconfiguration class is created, it’s a good idea to bundle it into a maven module that can be used as a dependency by other projects. It’s important to use the recommended naming convention for your modules - especially if you’re building autoconfiguration for the community - or it might clash with the modules from spring boot itself. The recommended naming is xyz-spring-boot-autoconfigure and xyz-spring-boot-starter. The former module should contain the autoconfiguration class (and don’t forget the spring.factories file), and the latter should contain the recommended dependencies to enable the autoconfiguration. That way the user can independently have the autoconfiguration and the classes that enable it. The starter is entirely optional though.The spring.factories file should contain your configuration like this: a key being org.springframework.boot.autoconfigure.EnableAutoConfiguration with value the qualified name of your autoconfiguration class.To further allow the user to customize your autoconfiguration, you can expose a customize hook into your autoconfiguration. This should accept an xyzConfigCustomizer interface (you can create whatever interface you want basically) with a single customize method. This customize hook is executed after the autoconfiguration is executed, but before actual instantiation of the beans. The user then just has to create a bean that implements the customizer interface. An example is available on GitHub.One final concern of autoconfiguration is the order in which they are executed. This applies to conditions inside the autoconfiguration but also to combined autoconfigurations. Always make sure the cheapest condition comes first. So expensive SpEL expression conditions should come after conditionalOnBeans. Between autoconfigurations you can use either the annotation @AutoConfigureBefore and After on the autoconfiguration class, or the @Order annotation. When no order or before or after condition is specified, there is no guarantee when the autoconfiguration will be executed.Conditions are being executed in two phases: PARSE_CONFIGURATION phase and REGISTER_BEAN phase. PARSE_CONFIGURATION evaluates the condition when the @Configuration-annotated class is parsed. This gives a chance to fully exclude the configuration class. REGISTER_BEAN evaluates the condition when a bean from a configuration class is registered. This does not prevent the configuration class to be added but it allows to skip a bean definition if the condition does not match (as defined by the matches method of the Condition interface).The slidedeck of this talk can be found here: https://speakerdeck.com/snicoll/master-spring-boot-auto-configuration"
      },
    
    
      "jobs-001-junior-frontend-javascript-developer": {
        "title": "Junior Frontend Developer",
        "url": "/jobs/001-junior-frontend-javascript-developer/",
        "image": "/img/jobs/junior-frontend.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    The current FRONTEND TECHNOLOGIES allow us to create fast, reliable and easy to maintain applications                     that are accessible on a wide set of devices ranging from smartphones to tablets to desktop computers.                     More importantly, they can be created for multiple platforms using a single code base.                     This applies to both web applications and hybrid mobile applications.                     A single code base means less recurring errors and is easier to maintain and test.                     At Ordina JWorks, we embrace technologies such as ANGULAR and IONIC                    and apply them in a number of high-profile projects.                                                    Software development is your true passion                    You contribute in the successful delivery of our projects                    You make sure that our clients are proud of your contribution                    You are interested in the latest trends and evolutions and you want to learn them yourselve                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    You know HTML5 is not just a syntax and have used API's such as Geolocation, Localstorage, ...                    You know CSS3 can do many cool things: animations, transformation, ...                    You know how to write structured Object Oriented JavaScript                    You know jQuery                    Multilingual Dutch and / or French and English                    You work customer focused and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                Nice to have:                                    You have looked at ES6 and know the benefits                    You have used Sass / SCSS                    You have used a framework such as Angular, React, Vue, ...                    You have used TypeScript                    You have experience in writing unit tests                                                                                What does Ordina have to offer?                                    We’ll embrace you and provide top notch coaching so you can grow the best way possible.                     When not working at our clients, you will have the ability to work on internal projects so you can still gain knowledge.                    We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                    No problem, we provide you with the tools to boost your productivity and efficiency.                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Jan De Wilde                        Practice Manager Frontend                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-002-medior-frontend-javascript-developer": {
        "title": "Medior Frontend Developer",
        "url": "/jobs/002-medior-frontend-javascript-developer/",
        "image": "/img/jobs/medior-frontend.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    The current FRONTEND TECHNOLOGIES allow us to create fast, reliable and easy to maintain applications                     that are accessible on a wide set of devices ranging from smartphones to tablets to desktop computers.                     More importantly, they can be created for multiple platforms using a single code base.                     This applies to both web applications and hybrid mobile applications.                     A single code base means less recurring errors and is easier to maintain and test.                     At Ordina JWorks, we embrace technologies such as ANGULAR and IONIC                    and apply them in a number of high-profile projects.                                                    Software development is your true passion                    You contribute in the successful delivery of our projects                    You make sure that our clients are proud of your contribution                    You are interested in the latest trends and evolutions and you want to learn them yourselve                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    At least 3 years of experience with Javascript/HTML5/CSS development                    You know HTML5 is not just a syntax and have used APIs such as Geolocation, Localstorage, ...                    You know CSS3 can do many cool things: animations, transformation, ...                    You know how to write structured Object Oriented JavaScript                    You have deep knowledge of jQuery: writing plugins, templating, ...                    You know ES6 and know the benefits                    You can find your way around TypeScript                    You love to use Sass / SCSS                    You have solid knowledge of a framework such as Angular, React, Vue, ... Preferably Angular                    You have knowledge of RxJS / Redux                    You can write unit tests that result in high code coverage                    You know your way around Git                    The command line is no strange place for you                    Hands-on experience with OO Design and design patterns                    Hands-on experience with Agile development (Scrum) principles & methodologies                    Multilingual Dutch and / or French and English                    You work customer focused and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                    Customer, solution and improvement minded                    Proactive and can-do attitude                                Nice to have:                                    You have deep understanding of UX                    REST APIs are no stranger to you                    Knowledge of automation, continuous integration and unit and integration testing                    You have experience in hybrid app development with Ionic Framework                                                                                What does Ordina have to offer?                                    We’ll embrace you and provide top notch coaching so you can grow the best way possible.                     When not working at our clients, you will have the ability work on internal projects so you can still gain knowledge.                     We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                    No problem.                    Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Jan De Wilde                        Frontend Practice Manager                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-003-senior-frontend-javascript-developer": {
        "title": "Senior Frontend Developer",
        "url": "/jobs/003-senior-frontend-javascript-developer/",
        "image": "/img/jobs/senior-frontend.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    The current FRONTEND TECHNOLOGIES allow us to create fast, reliable and easy to maintain applications                     that are accessible on a wide set of devices ranging from smartphones to tablets to desktop computers.                     More importantly, they can be created for multiple platforms using a single code base.                     This applies to both web applications and hybrid mobile applications.                     A single code base means less recurring errors and is easier to maintain and test.                     At Ordina JWorks, we embrace technologies such as ANGULAR and IONIC                    and apply them in a number of high-profile projects.                                                    Software development is your true passion                    You contribute in the successful delivery of our projects                    You make sure that our clients are proud of your contribution                    You are interested in the latest trends and evolutions and you want to learn them yourselve                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    At least 5 years of experience with Javascript/HTML5/CSS development                    You know HTML5 is not just a syntax and have used API's such as Geolocation, Localstorage, ...                    You know CSS3 can do many cool things: animations, transformation, ...                    You know how to write structured Object Oriented JavaScript                    You have deep knowledge of jQuery: writing plugins, templating, ...                    You know ES6 and know the benefits                    You write TypeScript like no one else                    You love to use Sass / SCSS, and like to dive deeper into mixins, partials, ...                    You have deep knowledge of Angular and solid knowledge of a framework such as React, Vue, ...                    You have deep knowledge of RxJS / Redux                    REST API's are no stranger to you                    Knowledge of automation, continuous integration and unit and integration testing                    You have experience in hybrid app development with Ionic Framework                    You can write unit tests that result in high code coverage                    You know your way around Git                    The command line is no strange place for you                    Hands-on experience with OO Design and design patterns                    Hands-on experience with Agile development (Scrum) principles & methodologies                    You have deep understanding of UX                    Multilingual Dutch and / or French and English                    You work customer focused and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                    Customer, solution and improvement minded                    Pro-active and can-do attitude                                Nice to have:                                    You like to approach things from an architectural standpoint                    You have interest in design systems                    (Public) speaking is exactly your thing                                                                                What does Ordina have to offer?                                    We’ll embrace you and provide top notch coaching so you can grow the best way possible.                     When not working at our clients, you will have the ability work on internal projects so you can still gain knowledge.                     We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                    No problem.                    Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                    Opportunity to work as a team lead for one of our High Performance Teams                                                                                Interested?                                                                                                                                        Jan De Wilde                        Frontend Practice Manager                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-031-junior-cloud-automation": {
        "title": "Junior Cloud Automation Engineer",
        "url": "/jobs/031-junior-cloud-automation/",
        "image": "/img/jobs/junior-cloud-automation.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    Are you passionate about technology? When you hear words like Docker, Kubernetes, and auto-scaling does your heart go aflutter?                    Then this opportunity is right up your alley!  At Ordina we are passionate about our people, our customers, our culture, and especially technology.                                                    This role will focus on support of our client’s production environments, DevOps automation and CI/CD development.                                                    In short, enable extremely high productivity for the development teams and provide a stable production environment for customers.                                Responsibilities                                    As part of a  cloud automation team, you will be responsible for automation, optimization, documentation and support of the infrastructure components.                        These components are hosted in collocated facilities and cloud services such as Azure, Google Cloud or AWS.                                        Deliver cloud computing solutions, hosted services and design underlying software infrastructure.                    Support in monitoring the platform by enabling high availability, performance and notifications.                     Think solution minded towards infrastructure issues in different environments.                                                                                What are the requirements?                Required Skills                                    Bachelor’s Degree or equivalent in Computer Science, Engineering or related field                    Interest in cloud-based services such as Amazon Web Services, Google Cloud and Azure                    Experience with GIT version control systems such as GitHub, GitLab or Bitbucket                    Basic knowledge of CI/CD automation tools such as Jenkins, Concourse or Bamboo                    Excellent troubleshooting and problem solving skills                                Valued Qualifications                                    Excellent written and verbal communication                    Willing to learn many new technologies and keeping that knowledge relevant                    Experience with a scripting language (Bash, LUA, Ansible, Python)                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                    Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                    Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                    No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Dieter Hubau                        Cloud Automation Practice Manager                                                    +32 478 45 81 50                            dieter.hubau@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-032-medior-cloud-automation": {
        "title": "Medior Cloud Automation Engineer",
        "url": "/jobs/032-medior-cloud-automation/",
        "image": "/img/jobs/medior-cloud-automation.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    Did you read Google's book about Site Reliability Engineering?                    Are you passionate about technology? When you hear words like Docker, Kubernetes, and auto-scaling, does your heart go aflutter?                    Then this opportunity is right up your alley!  At Ordina we are passionate about our people, our customers, our culture, and especially technology.                                                    This role will focus on support of our client’s production environments, DevOps automation and CI/CD development.                                                    In short, enable extremely high productivity for the development teams and provide a stable production environment for customers.                                Responsibilities                                    As part of a  cloud automation team, you will be responsible for automation, optimization, documentation and support of the infrastructure components.                        These components are hosted in collocated facilities and cloud services such as Azure, Google Cloud or AWS.                                        Deliver cloud computing solutions, hosted services and design underlying software infrastructure.                    Assist in coaching application developers on proper techniques for building scalable applications in the microservices paradigm.                    Support in monitoring the platform by enabling high availability, performance and notifications.                     Think solution minded towards infrastructure issues in different environments.                    Evaluate new tools, technologies and processes to improve speed, efficiency, and scalability of our client’s continuous integration environments.                                                                                What are the requirements?                Required Skills                                    Bachelor’s Degree or equivalent in Computer Science, Engineering or related field                    Understand pros and cons of cloud technologies                    Experience with Infrastructure as a Service and Platform as a Service technologies.                    Experience and understanding of the SDLC                    Some experience with containerization/orchestration technologies tools like Docker, Kubernetes and optionally Mesos and Swarm                    Basic understanding of GIT version control systems such as GitHub, GitLab or Bitbucket                    Experience with CI/CD automation tools such as Jenkins, Concourse or Bamboo, and artifact repositories like Nexus or Artifactory                    Experience working in an Agile environment                    Excellent troubleshooting and problem solving skills                                Valued Qualifications                                    Excellent written and verbal communication, including mentoring others                    Demonstrate at least one success from current or past technical challenges                    Experience with messaging technologies such as Kafka, RabbitMQ, ActiveMQ, etc                    Experience with NoSQL databases such as MongoDB, Redis, Cassandra, CouchDB, etc                    Experience with log management systems like ElasticSearch, Splunk or Graylog                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                    Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                    Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                    No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Dieter Hubau                        Cloud Automation Practice Manager                                                    +32 478 45 81 50                            dieter.hubau@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-033-senior-cloud-automation": {
        "title": "Senior Cloud Automation Engineer",
        "url": "/jobs/033-senior-cloud-automation/",
        "image": "/img/jobs/senior-cloud-automation.jpg",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    Do you love playing Factorio and get paid for it?                    Are you passionate about technology? When you hear words like Docker, Kubernetes, and auto-scaling does your heart go aflutter?                    Then this opportunity is right up your alley!  At Ordina we are passionate about our people, our customers, our culture, and especially technology.                                                    This role will focus on support of our client’s production environments, automation and CI/CD development with a DevOps mindset.                                                    In short, enable extremely high productivity for the development teams and provide a stable production environment for customers.                                Responsibilities                                    As part of a  cloud automation team, you will be responsible for automation, optimization, documentation and support of the infrastructure components.                        These components are hosted in collocated facilities and cloud services such as Azure, Google Cloud or AWS.                                        Design the proper build tools and frameworks that support deploying and managing our client’s platform.                    Deliver cloud computing solutions, hosted services and design underlying software infrastructure.                    Assist in coaching application developers on proper techniques for building scalable applications in the microservices paradigm.                    Support in monitoring the platform by enabling high availability, performance and notifications.                     Think solution minded towards infrastructural issues in different environments.                    Evaluate new tools, technologies and processes to improve speed, efficiency, and scalability of our client’s continuous integration environments.                    Mentor and coach your teammates.                                                                                What are the requirements?                Required Skills                                    2+ years experience with cloud-based provisioning, monitoring, troubleshooting, and related cloud automation technologies                    Bachelor’s Degree or equivalent in Computer Science, Engineering or related field                    Understand pros and cons of cloud technologies                    Experience with Infrastructure as a Service and Platform as a Service technologies.                    Experience and understanding of the SDLC                    Strong experience with containerization/orchestration technologies tools like Docker, Kubernetes and optionally Mesos or Swarm                    Strong understanding of GIT version control systems such as GitHub, GitLab or Bitbucket                    Experience with CI/CD automation tools such as Jenkins, Concourse or Bamboo, and artifact repositories like Nexus or Artifactory                    Strong experience working in an Agile environment                    Excellent troubleshooting and problem solving skills                    Strong track record of learning new technologies independently                                Valued Qualifications                                    Excellent written and verbal communication, including mentoring others                    Experience with Application operations, including Incident Management, Change Management, Security Management and Capacity Management                    Demonstrate multiple successes from current or past technical challenges                    Experience with multiple provisioning, deployment, cloud computing tools like CloudFormation, Spinnaker and Vagrant                    Experience with messaging technologies such as Kafka, RabbitMQ, ActiveMQ, etc                    Experience with Netflix OSS                    Experience with NoSQL databases such as MongoDB, Redis, Cassandra, CouchDB, etc                    Experience with log management systems like ElasticSearch, Splunk or Graylog                    Master’s Degree or equivalent in Computer Science, Engineering or related field                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                    Opportunity to work as a team lead for one of our High Performance Teams                                                                                Interested?                                                                                                                                        Dieter Hubau                        Cloud Automation Practice Manager                                                    +32 478 45 81 50                            dieter.hubau@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-041-junior-java-software-engineer": {
        "title": "Junior Java Software Engineer",
        "url": "/jobs/041-junior-java-software-engineer/",
        "image": "/img/jobs/junior-java.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    We strive to remain thought leaders in the ever changing technology market.                    JWorks makes this a personal goal of each of our colleagues to stay on top of the game.                     This can be done by following or contributing to numerous WORKSHOPS, coding sessions and webinars we organise,                     but what’s more fun than going to INTERNATIONAL CONFERENCES                    such as Spring I/O in BARCELONA or MongoDB World in NEW YORK?                     Take a couple of days extra to enjoy the surroundings and get to know your colleagues better!                                                    Software development is your true passion                    You contribute in the successful delivery of our project                    You make sure that our clients are proud on your contribution                    Solving complex issues doesn't scare you                    You are interested in the latest trends and evolutions and you want to learn them yourself                    You collaborate in our Competence Centers and you share your knowledge with your colleagues                                                                                What are the requirements?                                    You enjoy programming!                    You have basic Java knowledge                    Basic knowledge of the Spring Framework and its different projects                    Knowledge of Spring Boot, Spring MVC and Spring Data is a plus                    Multilingual Dutch and/or French and English                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-nonsense way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                    Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Ken Coenen                        Backend Practice Manager                                                    +32 472 48 85 01                            ken.coenen@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-042-medior-java-software-engineer": {
        "title": "Medior Java Software Engineer",
        "url": "/jobs/042-medior-java-software-engineer/",
        "image": "/img/jobs/medior-java.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    We strive to remain thought leaders in the ever changing technology market.                    JWorks makes this a personal goal of each of our colleagues to stay on top of the game.                     This can be done by following or contributing to numerous WORKSHOPS, coding sessions and webinars we organise,                     but what’s more fun than going to INTERNATIONAL CONFERENCES                    such as Spring I/O in BARCELONA or MongoDB World in NEW YORK?                     Take a couple of days extra to enjoy the surroundings and get to know your colleagues better!                                                    Software development is your true passion                    You contribute in the successful delivery of our project                    You make sure that our clients are proud on your contribution                    Solving complex issues doesn't scare you                    You are interested in the latest trends and evolutions and you want to learn them yourselve                    You collaborate in our Competence Centers (Cloud Computing, Mobile,…) and you share your knowledge with your colleagues                                                                                What are the requirements?                                    You enjoy programming!                    Bachelor or Master level, or equal by experience                    At least 2 years experience with Java technology                    Knowledge of the Spring Framework, including Spring Boot, Spring MVC and Spring Data                    Knowledge of Spring Cloud is a plus                    Automated testing (unit testing, integration testing and acceptance testing) of applications                     Application Servers and Containers                    Development tools such as Git, Jenkins, Sonar and Maven are familiar to you                    Multilingual Dutch and/or French and English                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Ken Coenen                        Backend Practice Manager                                                    +32 472 48 85 01                            ken.coenen@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-043-senior-java-software-engineer": {
        "title": "Senior Java Software Engineer",
        "url": "/jobs/043-senior-java-software-engineer/",
        "image": "/img/jobs/senior-java.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    We strive to remain thought leaders in the ever changing technology market.                    JWorks makes this a personal goal of each of our colleagues to stay on top of the game.                     This can be done by following or contributing to numerous WORKSHOPS, coding sessions and webinars we organise,                     but what’s more fun than going to INTERNATIONAL CONFERENCES                    such as Spring I/O in BARCELONA or MongoDB World in NEW YORK?                     Take a couple of days extra to enjoy the surroundings and get to know your colleagues better!                                                    Software development is your true passion                    You contribute in the successful delivery of our project                    You make sure that our clients are proud on your contribution                    Solving complex issues doesn't scare you                    You are interested in the latest trends and evolutions and you want to learn them yourselve                    You collaborate in our Competence Centers (Cloud Computing, Mobile,…) and you share your knowledge with your colleagues                                                                                What are the requirements?                                    You enjoy programming!                    Bachelor or Master level, or equal by experience                    At least 5 years experience with Java technology                    Profound knowledge of the Spring Framework, including Spring Boot, Spring MVC, Spring Data and Spring Cloud                    Experience with Netflix OSS is a plus                    REST API's are no stranger to you                    Automated testing (unit testing, integration testing and acceptance testing) of applications doesn't have any secret for you                    Application Servers and Containers                    Tools such as Git, Jenkins, Sonar and Maven are familiar to you                    Knowledge of cloud providers, such as Amazon Web Services (AWS), AWS Lambda, Cloud Foundry, Google Compute Engine or Azure are a plus                    Knowledge of developing applications for cloud platforms, such as Kubernetes, Openshift or Cloud Foundry are a plus                    Affinity with methodologies such as DevOps, Scrum, UML, PRINCE2 and ITIL                    Multilingual Dutch and/or French and English                    You work customer focused and have a service mindset                    You are a team player                    You take initiative and you are very eager to learn                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                    Opportunity to work as a team lead for one of our High Performance Teams                                                                                Interested?                                                                                                                                        Ken Coenen                        Backend Practice Manager                                                    +32 472 48 85 01                            ken.coenen@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-051-java-architect": {
        "title": "Java Architect",
        "url": "/jobs/051-java-architect/",
        "image": "/img/jobs/arch.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    Ordina JWorks is actively transforming companies to cloudnative market disruptors.                     We employ techniques and industry trends such as XaaS, immutable infrastructure and loosely coupled services                    to speed up development and get to market faster than ever. The correct architecture, development methologies and strategy are quintessential to support this.                     Guiding teams through pragmatic leadership and the establishment of a shared vision are enabling strategic and tactic business advantages at an incredible pace.                                       We are the trusted partner of our clients and help them push their organisation forward, while solving difficult and challenging technology problems.                    We believe in evolutionary software delivery through correct service boundaries and APIs.                                                    You play a key role in the successful delivery of our projects                    You make an important contribution in the realization of Java projects, where your architecture is a differentiator                    You create the high-level design for our projects, supporting and steering the customers strategy                    Solving complex problems are a real challenge for you                    You are interested in the latest trends and developments, and can make seasoned decisions on their adoption                    You collaborate in our Competence Centers (Architecture, Cloud, Security, ...) and you share your knowledge with your colleagues                                                                                What are the requirements?                                    Bachelor or Master level, or equal by experience                    Experience with diverse (Java) technology such as Spring (including Spring Boot and Spring Cloud) or JEE                    Thorough knowledge of architectural patterns (event-driven, microservices, n-tier, blackboard, ...), development methodologies, standards, technical risks,...                    Experience with application modernisation or integration platforms is a big plus                    Experience with Application Servers, Containers and Orchestration                    Passion for coaching people and guiding a team                    Strong affinity with setting-up delivery pipelines                    Knowledge of cloud platforms, such as Amazon Web Services (AWS), Cloud Foundry, Google Compute Engine, Azure and microservice architecture are a trump                    Multilingual Dutch and / or French and English                    Strong focus on customer interaction and creating high-level solutions for their needs                    You take initiative and you are very eager to both learn and share experiences                    You are flexible and stress resistant                                                                                What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We’re very fond of our down-to-earth culture and no-bullshit way of working.                     Prefer Gliffy over Enterprise Architect or Archi?                    No problem. Want to make an impact on the architectural choices and tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates and visiting software conferences                    Real possibilities to grow within the company and to clearly define your career on the basis of a Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Ken Coenen                        Backend Practice Manager                                                    +32 472 48 85 01                            ken.coenen@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-061-hybrid-mobile-developer": {
        "title": "Mobile Developer",
        "url": "/jobs/061-hybrid-mobile-developer/",
        "image": "/img/jobs/android.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Who are we looking for?                                    As a Mobile Developer within Ordina you will be part of the team that delivers mobile solutions for leading companies                    Besides the development of applications on mobile platforms for PDAs, smartphones and tablets,                         you will come in touch with peripheral matters that are necessary for setting up a fully functioning circuit for the user                                        These include: setting up and technical management of mobile middleware platforms, setting up database synchronization and integration with existing systems                    Of course security is an important item                    We keep each other sharp and customer satisfaction high. Customer contact and quality are key.                         We also offer you the space for innovative ideas, self-development and entrepreneurship                                                                                                    What are the requirements?                                    Bachelor or Master level                    At least 3 years experience with programming in Java, C++, or C#, or a very broad experience with C                    You have good communication skills, both oral and written                    You have knowledge of modern messaging standards on the Internet, have a thorough knowledge of general web standards and have experience with one or more service architectures                    You have a result-oriented attitude which you combine with independence, creativity and an entrepreneurial attitude                                                                                                    What does Ordina have to offer?                                    We’re growing quickly and oftentimes this can result in limitations and rules prohibiting creativity and freedom.                     Not at JWorks though. We’re very fond of our DOWN-TO-EARTH CULTURE and no-bullshit way of working.                     Prefer a MACBOOK with INTELLIJ over a Windows with Eclipse?                     No problem. Want to make an impact on the architectural choices or tools we use? Jump right in!                                                    Challenging assignments and projects for Belgian top companies                    A dynamic high tech environment with focus on knowledge and innovation                    An open no-nonsense company culture with room for personal initiative                    A great number of workshops, trainings and events in cooperation with Ordina Academy                     Support for obtaining certificates                    Real possibilities to grow within the company and to clearly define your career on the basis of an Human Talent Management program                    Regular team events and social company events                    An attractive salary in accordance to the market situation with a wide remuneration package (company car, laptop, insurance package, meal vouchers,...)                                                                                Interested?                                                                                                                                        Jan De Wilde                        Frontend Practice Manager                                                    +32 498 47 78 53                            jan.dewilde@ordina.be                                                                                                                "
      }
      ,
    
      "jobs-unsolicited-application": {
        "title": "Unsolicited application",
        "url": "/jobs/unsolicited-application/",
        "image": "/img/jobs/unsolicited-application.png",
        "date": "",
        "category": "job, vacancy, jobs, vacancies",
        "content": "                                        Want to suprise us with your talents?                                    While all the listed positions are about a specific role, we are aware that certain other profiles might come in handy.                    JWorks is always on the look-out for like-minded individuals with the right drive to reinforce the unit.                    Tell us about yourself and about the role that you are looking for, and let's get to know each other.                                                                                Interested?                                                                                                                                        Anja Van Acker                        Resource Manager                                                    +32 479 07 18 85                            anja.vanacker@ordina.be                                                                                                                "
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
        </div>
    </section>
</div>


<div id="over"></div>


<footer>
    <div class="contact">
        <div class="address">
            <div class="icon"><i class="fa fa-fw fa-home"></i></div>
            <div class="text">Ordina Belgium<br/>Blarenberglaan 3B,<br/>2800 Mechelen, Belgium</div>
        </div>
        <div class="phone">
            <div class="icon"><i class="fa fa-fw fa-phone"></i></div>
            <div class="text"><a href="tel:003215295858">+32 15 29 58 58</a></div>
        </div>
        <div class="email">
            <div class="icon"><i class="fa fa-fw fa-envelope-o"></i></div>
            <div class="text"><a href="mailto:jworks@ordina.be">jworks@ordina.be</a></div>
        </div>
    </div>
    <ul class="social">
        <li>
            <a href="https://twitter.com/lifeatordinabe" target="_blank">
                <i class="fa fa-fw fa-twitter"></i><span>Twitter</span>
            </a>
        </li>
        <li>
            <a href="https://www.facebook.com/lifeatordinabe" target="_blank">
                <i class=" fa fa-fw fa-facebook"></i><span>Facebook</span>
            </a>
        </li>
        <li>
            <a href="https://www.linkedin.com/company/ordina-belgium" target="_blank">
                <i class=" fa fa-fw fa-linkedin"></i><span>LinkedIn</span>
            </a>
        </li>
        <li>
            <a href="https://plus.google.com/113222464071666722451" target="_blank">
                <i class=" fa fa-fw fa-google-plus"></i><span>Google+</span>
            </a>
        </li>
        <li>
            <a href="/youtube" target="_blank">
                <i class=" fa fa-fw fa-youtube"></i><span>YouTube</span>
            </a>
        </li>
        <li>
            <a href="/github" target="_blank">
                <i class=" fa fa-fw fa-github"></i><span>GitHub</span>
            </a>
        </li>
        <li>
            <a href="/feed.xml" target="_blank">
                <i class=" fa fa-fw fa-rss"></i><span>RSS Feed</span>
            </a>
        </li>
    </ul>
    <div class="copyright">
        &copy; 2019 Ordina JWorks. All rights reserved.
        <br /> Disclaimer: Opinions expressed on this blog reflect the writer's views and not the position of Ordina
        <img id="analyticsImg" src="" width="1" height="1" style="border: 0px"/>
    </div>
</footer>
<!-- Scripts -->
<script src="/js/jquery.min.js"></script>
<script src="/js/jquery.scrollex.min.js"></script>
<script src="/js/jquery.magnific-popup.min.js"></script>
<script src="/js/owl.carousel.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/skel.min.js"></script>
<script src="/js/jquery.pin.min.js"></script>
<script src="/js/util.js"></script>
<!--[if lte IE 8]>
<script src="/js/ie/respond.min.js"></script><![endif]-->



<script src="/js/main.js"></script>

<!-- Google Analytics -->
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o);
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m);
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-70040502-1', 'auto');
    ga('send', 'pageview');
</script>
<script type="text/javascript">
    window.addEventListener('load', function () {
        if (window.ga && ga.create) {
            console.log('GA loaded correctly');
        } else {
            console.log('GA is blocked or failed to load - tracking manually...')
            document.getElementById('analyticsImg').src = 'https://jworks-techblog-analytics.cfapps.io/collect?title=' + document.title;
        }
    }, false);
</script>

<!-- Vertical timeline -->
<script src="/js/vertical-timeline/modernizr.js"></script>
<script src="/js/vertical-timeline/main.js"></script>


</body>
</html>
